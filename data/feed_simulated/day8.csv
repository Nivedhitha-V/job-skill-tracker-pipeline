job_title,company,job_location,job_link,first_seen,search_city,search_country,job level,job_type,job_summary,job_skills,job_skills_raw,job_skills_cleaned
Data Engineer IV,Belcan,"Seattle, WA",https://www.linkedin.com/jobs/view/data-engineer-iv-at-belcan-3783709510,2023-12-17,Kent,United States,Mid senior,Onsite,"Details
#NowHiring #DataEngineer
Job Title: Data Engineer IV
Contract: 9 Months
Belcan is a leading provider of professional IT, Engineering, Workforce Solutions and staffing in the United States, Canada, UK, Europe, and India.
A Data Engineer IV Job in Seattle, WA is currently available through Belcan. In this role you will be working in one of the world's largest and most complex data warehouse environments. If you are interested in this role, Apply Today!
Job Description
Client is looking for a Data Engineer to join the Infrastructure Automation team. Client has over 70 million customers, and developers all over the world rely on our storage, compute, and virtualized services via AWS. Our success depends on our world-class network and hardware infrastructure; we""re handling massive scale and rapid integration of emergent technologies. Our goal is to become 'The Infrastructure Platform' for the world. The Infrastructure Automation team is responsible for delivering the software that powers our infrastructure.
Responsibilities
As a Data Engineer you will be working in one of the world's largest and most complex data warehouse environments.
You will be developing and supporting the analytic technologies that give our customers timely, flexible, and structured access to their data.
You will be responsible for designing and implementing a platform using third-party and in-house reporting tools, modeling metadata, building reports and dashboards in Tableau/Quicksight.
You will work with business customers in understanding the business requirements and implementing solutions to support analytical and reporting needs.
Required Skills & Experience
7+ years of related experience.
Hands-on experience in SQL, ETL/Data pipeline building
Knowledge of scripting/programming language such as Python is required
Experience with AWS cloud services such as S3, AWS Lamda, Redshift, AWS Lake formation, SNS/SQS and others is required.
Very Strong development experience with notable BI reporting tools (Tableau/Quicksight/Alteryx).
Should have experience developing complex and a variety of reports.
Experience in root causing issues in existing code and implement short term/long term fixes
A good candidate has strong analytical skills and enjoys working with large complex data sets.
A good candidate can partner with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies.
Preferred
Knowledge or background working in Finance teams in analytics/reporting space preferred.
Experience in managing Cloud Server (EC2), Tableau and Alteryx is preferred
Location: Seattle, WA
Zip Code: 98121
Keyword""s: #Seattlejobs; #DataEngineerjobs;
Start Date: Right Away
#ZR
Belcan is a global supplier of engineering, technical recruiting, and IT services to customers in the aerospace, industrial, and government sectors. Belcan engineers"" better outcomes through adaptive and integrated services-from jet engines, airframe, and avionics to heavy vehicles, chemical processing, and cybersecurity. Belcan takes a partnering approach to provide customer-driven solutions that are flexible, scalable, and cost-effective. Our unique capabilities have led to continuous growth and success for 63+ years. We are a team-driven Equal Opportunity Employer committed to workforce diversity.
Show more
Show less","AWS Cloud Services, Python, SQL, Data Warehousing, Tableau, ETL, Data Pipeline, Redshift, AWS Lambda, AWS Lake Formation, SNS/SQS, Alteryx, EC2, Finance Analytics","aws cloud services, python, sql, data warehousing, tableau, etl, data pipeline, redshift, aws lambda, aws lake formation, snssqs, alteryx, ec2, finance analytics","alteryx, aws cloud services, aws lake formation, aws lambda, data pipeline, datawarehouse, ec2, etl, finance analytics, python, redshift, snssqs, sql, tableau"
Lead Data Engineer,GoodRx,"Seattle, WA",https://www.linkedin.com/jobs/view/lead-data-engineer-at-goodrx-3771699983,2023-12-17,Kent,United States,Mid senior,Onsite,"GoodRx is America’s healthcare marketplace. Each month, millions of people visit goodrx.com to find reliable health information and discounts for their healthcare — and we’ve helped people save $60 billion since 2011. We provide prescription discounts that are accepted at more than 70,000 pharmacies in the U.S., as well as telehealth services including doctor visits and lab tests. Our services have been positively reviewed by Good Morning America, The New York Times, NBC News, AARP, and many others.
Our goal is to help Americans find convenient and affordable healthcare. We offer solutions for consumers, employers, health plans, and anyone else who shares our desire to provide affordable prescriptions to all Americans.
About The Role
GoodRx is looking for extremely smart and innovative data engineers, who are deft at working with a wide variety of languages, such as Python and SQL, a variety of raw data formats, such as parquet and CSV, in a fast-paced and friendly environment. You will collaborate and work with teams across GoodRx to build an outstanding data platform that supports hundreds of data pipelines which move big data accurately and quickly to guide enterprise data decisions.
Responsibilities
Collaborate with product managers, data scientists, data analysts and engineers to define features needed for a data platform
Provide mentorship and technical leadership for a team
Work closely with other engineers to scale infrastructure, improve reliability and efficiency
Improve developer tooling with a focus on reliability and efficiency
Write good technical documentation
Perform large system upgrades and migrations
Maintenance and improvement of multiple CI/CD pipelines
Act as an in-house data expert who makes recommendations regarding standards for code quality and pipeline architecture
Develop, deploy and maintain data processing pipelines using cloud technology such as AWS, Kubernetes, Lambda, Kafka, Airflow, Redshift, S3, Glue, and EMR
Make smart engineering and infra decisions based on data auditing and collaboration
Lead and architect cloud-based data infrastructure solutions to meet stakeholder needs
Skills & Qualifications
8+ years of professional experience in any one of the Cloud providers such as AWS, Azure or GCP
8+ years experience in engineering data pipelines using data technologies (Python, Databricks, pySpark, Kafka) on large scale data sets
Experience building or maintaining a Data Platform that supports multiple engineering teams and processes big data
Ability to quickly learn complex domains and new technologies
Innately curious and organized with the drive to analyze data to identify deliverables, anomalies and gaps and propose solutions to address these findings
Experience designing data models that have been implemented in production
Strong experience in writing complex SQL and ETL development with experience processing large data sets
Familiarity with AWS services (Redshift, RDS, EKS, S3, EMR, Glue, Lambda)
Experience using GitHub, Docker, Terraform, CodeFresh, Jira
Experience contributing to full lifecycle deployments with a focus on quality and scalability
Good to Have
Experience with customer data platform tools such as Segment
Experience contributing to full lifecycle deployments with a focus on testing and quality
Experience with data quality processes, data quality checks, validations, data quality metrics definition and measurement
AWS/Kafka/Databricks or similar certifications
At GoodRx, pay ranges are determined based on work locations and may vary based on where the successful candidate is hired. The pay ranges below are shown as a guideline, and the successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, and other relevant business and organizational factors. These pay zones may be modified in the future. Please contact your recruiter for additional information.
San Francisco Office
$180,000.00 - $288,000.00
New York And Seattle Offices
$165,000.00 - $264,000.00
Santa Monica Office
$150,000.00 - $240,000.00
Other Office Locations:
$135,000.00 - $216,000.00
GoodRx also offers additional compensation programs such as annual cash bonuses and annual equity grants for most positions as well as generous benefits. Our great benefits offerings include medical, dental, and vision insurance, 401(k) with a company match, an ESPP, unlimited vacation, ""Take Care of Yourself"" days, 11 paid holidays, and 72 hours of sick leave. GoodRx also offers additional benefits like mental wellness and financial wellness programs, fertility benefits, supplemental life insurance for you and your dependents, company-paid short-term and long-term disability, and more!
We’re committed to growing and empowering a more inclusive community within our company and industry. That’s why we hire and cultivate diverse teams of the best and brightest from all backgrounds, experiences, and perspectives. We believe that true innovation happens when everyone has a seat at the table and the tools, resources, and opportunities to excel.
With that said, research shows that women and other underrepresented groups apply only if they meet 100% of the criteria. GoodRx is committed to leveling the playing field, and we encourage women, people of color, those in the LGBTQ+ communities, and Veterans to apply for positions even if they don’t necessarily check every box outlined in the job description. Please still get in touch - we’d love to connect and see if you could be good for the role!
GoodRx is America's healthcare marketplace. The company offers the most comprehensive and accurate resource for affordable prescription medications in the U.S., gathering pricing information from thousands of pharmacies coast to coast, as well as a telehealth marketplace for online doctor visits and lab tests. Since 2011, Americans with and without health insurance have saved $60 billion using GoodRx and million consumers visit goodrx.com each month to find discounts and information related to their healthcare. GoodRx is the #1 most downloaded medical app on the iOS and Android app stores. For more information, visit www.goodrx.com.
Show more
Show less","Python, SQL, Parquet, CSV, AWS, Kubernetes, Lambda, Kafka, Airflow, Redshift, S3, Glue, EMR, Databricks, PySpark, GitHub, Docker, Terraform, CodeFresh, Jira, Segment, RDS, EKS","python, sql, parquet, csv, aws, kubernetes, lambda, kafka, airflow, redshift, s3, glue, emr, databricks, pyspark, github, docker, terraform, codefresh, jira, segment, rds, eks","airflow, aws, codefresh, csv, databricks, docker, eks, emr, github, glue, jira, kafka, kubernetes, lambda, parquet, python, rds, redshift, s3, segment, spark, sql, terraform"
Sr./Staff Software Engineer - Data Development Platform,"HireIO, Inc.","Seattle, WA",https://www.linkedin.com/jobs/view/sr-staff-software-engineer-data-development-platform-at-hireio-inc-3739547070,2023-12-17,Kent,United States,Mid senior,Onsite,"Location: Usa/Usa/California/Sf Bay Area, Seattle
Sponsor Visa? Yes
Language Requirements: English, Mandarin
Requirements
Bachelor's degree in Computer Science or equivalent practical experience
4 years of experience in software development, and with data structures/algorithms
2 years of experience with design and architecture, and testing and launching software products Preferred qualifications:
5 years of experience building and developing large-scale infrastructure, distributed systems or networks, and/or experience with compute technologies, storage, and/or hardware architecture
3 years of experience working in a complex, matrixed organization involving cross-functional, and/or cross-business projects
Experiences in data platform related products development or big data technologies (such as Hadoop, Clickhouse, Flink etc.)
Show more
Show less","Computer Science, Software Development, Data Structures, Algorithms, Design, Architecture, Testing, Launching, Distributed Systems, Networks, Compute Technologies, Storage, Hardware Architecture, Data Platform Development, Big Data Technologies, Hadoop, Clickhouse, Flink","computer science, software development, data structures, algorithms, design, architecture, testing, launching, distributed systems, networks, compute technologies, storage, hardware architecture, data platform development, big data technologies, hadoop, clickhouse, flink","algorithms, architecture, big data technologies, clickhouse, compute technologies, computer science, data platform development, data structures, design, distributed systems, flink, hadoop, hardware architecture, launching, networks, software development, storage, testing"
"Backend Engineer, Data Privacy Technologies",Stripe,"Seattle, WA",https://www.linkedin.com/jobs/view/backend-engineer-data-privacy-technologies-at-stripe-3765543721,2023-12-17,Kent,United States,Mid senior,Onsite,"Who we are
About Stripe
Stripe is a financial infrastructure platform for businesses. Millions of companies—from the world’s largest enterprises to the most ambitious startups—use Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Our mission is to increase the GDP of the internet, and we have a staggering amount of work ahead. That means you have an unprecedented opportunity to put the global economy within everyone’s reach while doing the most important work of your career.
About The Team
Stripe will succeed at our mission of increasing the GDP of the internet only if we prove ourselves worthy of our users’ trust. Software engineers on the Data Privacy Technologies team do work that is a critical part of accomplishing this mission by abstracting away data security and privacy concerns for Stripe and its users.
What you’ll do
The Data Privacy Technologies team builds systems and tools to allow Stripe to deeply reason about and protect its data, at scale. As an engineering team, we leverage data classification, machine learning as well as pseudonymization techniques such as tokenization, redaction, filtering, and masking.
Responsibilities
Design, build, and operate core infrastructure (e.g., APIs, services and frameworks) and tooling used by all of Stripe’s engineering teams, for example to annotate and automatically de-identify sensitive data
Improve engineering standards and processes
Make impactful decisions about systems, productivity, security and privacy — their edge cases, failure modes and tradeoffs
Collaborate closely with other engineering teams and cross-functionally with legal, product, compliance and operations teams to embed best practices for data protection into how products and infrastructure are built
Who you are
We’re looking for someone who meets the minimum requirements to be considered for the role. If you meet these requirements, you are encouraged to apply. The preferred qualifications are a bonus, not a requirement.
Minimum Requirements
2+ years software engineering experience in a high-stakes production environment
Empathy, excellent communication skills and a deep respect for the power of collaboration
A learning mindset, regardless of level or experience
The ability to think creatively and holistically about reducing risk in a complex, fast-changing environment
The ability to drive next steps when encountering ambiguous problems without clear ownership
Preferred Qualifications
Data platform engineering experience
Machine learning experience
Full stack experience
Privacy and/or security experience
Pay and benefits
The annual US base salary range for this role is $146,500 - $198,100. For sales roles, the range provided is the role’s On Target Earnings (""OTE"") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. This salary range may be inclusive of several career levels at Stripe and will be narrowed during the interview process based on a number of factors, including the candidate’s experience, qualifications, and location. Applicants interested in this role and who are not located in the US may request the annual salary range for their location during the interview process.
Additional benefits for this role may include: equity, company bonus or sales commissions/bonuses; 401(k) plan; medical, dental, and vision benefits; and wellness stipends.
Show more
Show less","Software engineering, Machine learning, Data platform engineering, Privacy and security, Full stack development, Data classification, Pseudonymization, Tokenization, Redaction, Filtering, Masking, APIs, Services, Frameworks, Tooling, Engineering standards, Processes, Data protection, Collaboration, Communication, Creativity, Problemsolving, Risk management, Ambiguous problems","software engineering, machine learning, data platform engineering, privacy and security, full stack development, data classification, pseudonymization, tokenization, redaction, filtering, masking, apis, services, frameworks, tooling, engineering standards, processes, data protection, collaboration, communication, creativity, problemsolving, risk management, ambiguous problems","ambiguous problems, apis, collaboration, communication, creativity, data classification, data platform engineering, data protection, engineering standards, filtering, frameworks, full stack development, machine learning, masking, privacy and security, problemsolving, processes, pseudonymization, redaction, risk management, services, software engineering, tokenization, tooling"
Senior Data Engineer,Turnberry Solutions,"Tacoma, WA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-turnberry-solutions-3784260066,2023-12-17,Kent,United States,Mid senior,Remote,"Duration:
6+ months
Location:
100% Remote
Overview:
Every Turnberry consultant belongs to a practice, an internal group of consultants and leaders with shared experience and expertise. Each of these practices aligns to one of the eight core services Turnberry offers to clients. As a Senior Data Engineer, you will join Turnberry's Data& Insights practice and service. Turnberry's Data& Insights service helps clients who seek visibility into company data, lack a single source of truth for accurate data, desire advanced analytics solutions, have data governance needs to ensure data quality, or have specific data challenges that may require an assessment, proof of concept, or custom solution not available in a typical out-of-the-box analytics methodology.
Responsibilities
﻿Design, implement, and manage scalable data pipelines for ingesting, transforming, and storing diverse data sets from various sources into our cloud-based data infrastructure
Develop, test, and deploy code using internal software development toolsets, including the code for deploying infrastructure and solutions for secure data storage, ETL pipelines, data catalogs and data queries
Collaborate with cross-functional teams to understand data requirements and develop solutions that meet business objectives
Optimize data workflows and processes for efficiency, reliability, cost and performance
Ensure data quality, integrity, and security throughout the data lifecycle
Stay updated with emerging technologies and best practices in data engineering, cloud services, and integration methodologies to drive continuous improvement
Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related field
10+ years of IT development experience
5+ years of experience in designing and implementing enterprise data solutions
Proficiency in cloud data platforms such as AWS, Azure, or Google Cloud Platform
Proven experience publishing and managing Kafka topics
Proven experience as a Data Engineer, preferably in the insurance or financial services industry
Strong expertise in data modeling, ETL/ELT processes, and data warehousing concepts
Hands-on experience with programming languages (Python, Java, or Scala) and SQL
Strong understanding of data governance, metadata management, data quality controls, data lineage and data security
Experience working in an Agile environment with strong Scrum framework knowledge
Excellent problem-solving skills and ability to work in a fast-paced, collaborative environment
Preferred Qualifications
Experience with Jupyter Notebooks
Experience with API management
Advanced degree in a relevant field
Industry certifications related to cloud services or data engineering
Experience with specific tools or technologies relevant to insurance data analysis or actuarial science
The salary range for this role is $80,000 to $160,000 or the hourly equivalent. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, Turnberry Solutions offers benefits such as a comprehensive healthcare package (medical, dental, vision), disability and group term life insurance, health and flexible spending accounts, a utilization bonus, 401(k) with match, flexible time off for salaried employees, parental leave for salaried employees, and flexible work arrangements (all benefits are subject to eligibility requirements). No matter where or when you begin a career with Turnberry, you'll find a far-reaching choice of benefits and incentives.
At Turnberry, inclusion is one of our core values. We are fully invested in and focused on hiring and growing a diverse team of high performers. We're committed to creating a positive and connected work environment for all. We believe that uniqueness in ideas, experiences, and backgrounds make us a better Turnberry: Turnberry is an Equal Employment Opportunity/Affirmative Action employer, and recruits, employs, trains, compensates, and promotes regardless of age, ancestry, family medical or genetic information, gender identity and expression, marital, military, or veteran status; national and ethnic origin; physical or mental disability; political affiliation; pregnancy; race; religion; sex; sexual orientation; and any other protected characteristics.
Show more
Show less","Data Engineering, Cloud Data Platforms, AWS, Azure, Google Cloud Platform, Kafka, Python, Java, Scala, SQL, Data Modeling, ETL/ELT Processes, Data Warehousing, Data Governance, Metadata Management, Data Quality Controls, Data Lineage, Data Security, Agile, Scrum, Jupyter Notebooks, API Management","data engineering, cloud data platforms, aws, azure, google cloud platform, kafka, python, java, scala, sql, data modeling, etlelt processes, data warehousing, data governance, metadata management, data quality controls, data lineage, data security, agile, scrum, jupyter notebooks, api management","agile, api management, aws, azure, cloud data platforms, data engineering, data governance, data lineage, data quality controls, data security, datamodeling, datawarehouse, etlelt processes, google cloud platform, java, jupyter notebooks, kafka, metadata management, python, scala, scrum, sql"
Sr. Data Engineer,SHRAPNEL,"Seattle, WA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-shrapnel-3618881609,2023-12-17,Kent,United States,Mid senior,Hybrid,"The Shrapnel team is seeking bright minds from every culture, discipline, and background to join us on a new creative endeavor.
We’re a mission-focused team with industry roots in world-class game development, studio building, and platform creation.
Our goal is empowering the world to share in our playful collaboration at the intersection of gaming, creation, and ownership.
The Shrapnel team is seeking bright minds from every culture, discipline, and background to join us on a new creative endeavor.
We’re a mission-focused team with industry roots in world-class game development, studio building, and platform creation.
Our goal is empowering the world to share in our playful collaboration at the intersection of gaming, creation, and ownership.
This is a core role on our platform team and focuses on the data infrastructure (technologies, schema, queries, ETL) that power aspects of our game, tooling, and web experiences. You will work with multiple data technologies (relational, search) and design, populate and query them with data from our game, marketplace, tooling, and underlying blockchain. You’d be part of a collaborative team with a no ego, “best idea wins” approach to engineering.
Come join our mission to create the future of entertainment.
The Job
Work with the platform team to design and deliver data schema, queries across multiple data technologies (relational, search) to support the needs of Shrapnel’s game and tools.
Work with blockchain engineers to facilitate event and state data updates from Shrapnel’s blockchain subnet.
Partner with backend engineers to deliver optimized data services to power experiences and insights for the game, web experiences, and tooling.
Work with business and technical leadership to define processes, data movement and transformation, and queries to power business and operational dashboards.
Contribute to operational and support process definition and execution for platform data capabilities.
Who Will Be A Great Fit
We welcome your uniqueness. Research shows that diverse teams do the best work. Our customers are all over the world, and from all walks of life. It's important that we’re not all the same, so that we can draw on our differences, what we’ve experienced, and how we think.
Experience with relational database design and implementation.
Proficient in SQL
Experience with Elasticsearch, Apache Solr, or Apache Lucene
Proficient in atleast one scripting such as Python, Typsescript
Experience with public cloud infrastructure. AWS experience a plus.
Experience working in Linux environments
Experience with DevOps practices
Nice to Have
Experience with Snowflake
A mindset of continuous improvement; humble and ambitious.
Prior experience in a startup environment or small business.
Adaptable, with a good sense of humor.
The Perks
Competitive Salary and excellent Benefits.
Flexible hours, with the potential of remote working.
Visibility and input to business strategy, including technology and creative partners.
Autonomy and latitude within your role.
Seattle’s healthy living and vibrant city culture.
Tight knit, highly experienced, fun-loving team.
Additional Information
The base salary range for this full-time position is $120,000 - $200,000 + stocks + tokens + benefits. Our salary ranges are determined by role, level, and location. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training.
Employment Type:
Full-time, On-Site, Not open to remote candidates at this time
Seniority Level:
Mid-Level
Industry:
Video Games, Blockchain
Employment Type:
Full-time – Onsite/Hybrid
Job Functions:
Office support, Communication, Inventory Management
The Fine Print
At Shrapnel we prefer working in-office, together. We believe working in the same space can accelerate progress, improve collaboration, and is definitely more fun. But in-office work doesn't make sense for everyone at this time, and technology makes certain remote work possible. So for certain roles we are open to discussing remote work. Please apply and we’ll decide case by case.
NEON Machine Inc and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law.
The Fine Print
At Shrapnel we prefer working in-office, together. We believe working in the same space can accelerate progress, improve collaboration, and is definitely more fun. But in-office work doesn't make sense for everyone at this time, and technology makes certain remote work possible. So for certain roles and perfect candidates we are open to discussing remote work. Please apply and we’ll decide case by case. In all cases, though, you must be allowed to work in the US without a visa.
We’re following WA state’s health guidelines regarding COVID-19. We make every effort keep our office safe and our employees healthy. No one comes to work sick, not even a sniffle. Our office is big enough to stay 6-feet apart easily, but if you’re in a meeting where distance is not possible and you’re not vaccinated then we require a mask (vaccinated folks might want to wear one too, your choice). Hand sanitizer is always close by, and the nightly janitor crew sanitizes work surfaces for the next day. We believe in the benefits of vaccines, but don’t require it. For now, masks are required in the public areas of our building, whether vaccinated or not, but if you’re working alone in a room you don’t need to wear a mask.https://coronavirus.wa.gov/information-for/business/workplace-safety-guidance
NEON Machine Inc and its subsidiaries are equal opportunity employers. Qualified candidates will receive consideration for employment without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law.
Show more
Show less","SQL, Python, Typescript, Elasticsearch, Apache Solr, Apache Lucene, AWS, Linux, DevOps, Snowflake, Relational database design, Data schema, Data queries, Blockchain, ETL, Data infrastructure, Data services, Data analysis, Data visualization, Data transformation, Data movement, Data governance, Data security, Data privacy","sql, python, typescript, elasticsearch, apache solr, apache lucene, aws, linux, devops, snowflake, relational database design, data schema, data queries, blockchain, etl, data infrastructure, data services, data analysis, data visualization, data transformation, data movement, data governance, data security, data privacy","apache lucene, apache solr, aws, blockchain, data governance, data infrastructure, data movement, data privacy, data queries, data schema, data security, data services, data transformation, dataanalytics, devops, elasticsearch, etl, linux, python, relational database design, snowflake, sql, typescript, visualization"
Data Platform Engineer - Full Stack,eBay,"Bellevue, WA",https://www.linkedin.com/jobs/view/data-platform-engineer-full-stack-at-ebay-3677053777,2023-12-17,Kent,United States,Mid senior,Hybrid,"At eBay, we're more than a global ecommerce leader — we’re changing the way the world shops and sells. Our platform empowers millions of buyers and sellers in more than 190 markets around the world. We’re committed to pushing boundaries and leaving our mark as we reinvent the future of ecommerce for enthusiasts.
Our customers are our compass, authenticity thrives, bold ideas are welcome, and everyone can bring their unique selves to work — every day. We're in this together, sustaining the future of our customers, our company, and our planet.
Join a team of passionate thinkers, innovators, and dreamers — and help us connect people and build communities to create economic opportunity for all.
Core Technology (CT) is a global team responsible for the end-to-end eBay technology platform. This platform runs our entire infrastructure and all the services that come together to form ebay.com.
We are looking for an outstanding and passionate Data Platform Engineer with full stack experience to join the Data and Analytics platform (DAP) team to build data platforms. You will be directly involved in the design and implementation of the core feature set of DAP portfolio products. This is a unique opportunity to be a part of a team building brand-new platforms and have a huge impact on eBay customers.
Open to hiring multiple locations across the U.S. and will consider full time remote**
Responsibilities
Design, implement the next generation of platforms, tools and products
Responsible for detailed architecture, POC, detailed design, advising and often hands-on implementation of complex systems including software and integration with existing systems at eBay
Help in transforming a culture of point data engineering solutions to Repeatable Software engineering platforms and tools
Architect scalable solutions and making it implementable with principles of extensibility, scalability, modular and easy to use with generalization principles
Responsible for operational stability & quality along with Engineering teams
Requirements
Prefer BS, MS, or Ph. D. in Computer Science with 5+ years of proven relevant industry experience
Experience with full stack development using Java, JavaScript and frameworks like Angular, React, Vue.js
Hands-on programmer with a good knowledge of data-structures and CS algorithms
Good technical knowledge of Cloud and open-source technologies
Great understanding of building scalable Data models on Hadoop and Relational systems to solve e2e customer journey problems
Experience in defining and developing high-volume, horizontally scalable web architectures
Experience with system software design and development with an understanding of computer architecture, storage, network, and IO subsystems
Experience with large-scale, distributed systems design and development with an understanding of scaling, replication, consistency, and high availability
Background in performance tuning, identifying performance bottlenecks, and implementing performance optimizations
Experience using projects in the Apache Hadoop ecosystem such as MapReduce, HBase, Hive, Spark, etc.
Experience using Realtime technologies like Apache Kafka, Flink, etc.
Benefits are an essential part of your total compensation for the work you do every day. Whether you’re single, in a growing family, or nearing retirement, eBay offers a variety of comprehensive and competitive benefit programs to meet your needs. Including maternal & paternal leave, paid sabbatical, and plans to help ensure your financial security today and in the years ahead because we know feeling financially secure during your working years and through retirement is important.
Here at eBay, we love creating opportunities for others by connecting people from widely diverse backgrounds, perspectives, and geographies. So, being diverse and inclusive isn’t just something we strive for, it is who we are, and part of what we do each and every single day. We want to ensure that as an employee, you feel eBay is a place where, no matter who you are, you feel safe, included, and that you have the opportunity to bring your unique self to work. To learn about eBay’s Diversity & Inclusion click here: https://www.ebayinc.com/company/diversity-inclusion/
The pay range for this position at commencement of employment in California, Washington, or New York is expected in the range below.
$149,200 - $234,850
Base pay offered may vary depending on multiple individualized factors, including location, skills, and experience. The total compensation package for this position may also include other elements, including a target bonus and restricted stock units (as applicable) in addition to a full range of medical, financial, and/or other benefits (including 401(k) eligibility and various paid time off benefits, such as PTO and parental leave). Details of participation in these benefit plans will be provided if an employee receives an offer of employment.
If hired, employees will be in an “at-will position” and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors.
eBay is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. If you have a need that requires accommodation, please contact us at talent@ebay.com. We will make every effort to respond to your request for accommodation as soon as possible. View our accessibility statement to learn more about eBay's commitment to ensuring digital accessibility for people with disabilities.
Jobs posted with location as ""Remote - United States (Excludes: HI, NM)"" excludes residents of Hawaii and New Mexico.
This website uses cookies to enhance your experience. By continuing to browse the site, you agree to our use of cookies. Visit our Privacy Center for more information.
Show more
Show less","Java, JavaScript, Angular, React, Vue.js, Apache Hadoop, MapReduce, HBase, Hive, Spark, Apache Kafka, Flink, Hadoop, Relational systems, Computer Science, Data structures, CS algorithms, Cloud computing, Opensource technologies, Apache Hadoop ecosystem, Realtime technologies","java, javascript, angular, react, vuejs, apache hadoop, mapreduce, hbase, hive, spark, apache kafka, flink, hadoop, relational systems, computer science, data structures, cs algorithms, cloud computing, opensource technologies, apache hadoop ecosystem, realtime technologies","angular, apache hadoop, apache hadoop ecosystem, apache kafka, cloud computing, computer science, cs algorithms, data structures, flink, hadoop, hbase, hive, java, javascript, mapreduce, opensource technologies, react, realtime technologies, relational systems, spark, vuejs"
Data Analyst,Focus Search LLC,Greater Seattle Area,https://www.linkedin.com/jobs/view/data-analyst-at-focus-search-llc-3764335062,2023-12-17,Kent,United States,Mid senior,Hybrid,"Focus Search is recruiting for a mid-level Data Analyst for our client in Seattle, WA. You'll be working with a growing technical team and collaborating with global outlets. You'll be able to learn new skills and grow with them. They have 13 paid holidays a year and we pay 100% of the benefits (health, dental, vision, etc.) for you and any dependents. It is a full-time, hybrid role with 3 days being in the office weekly.
2 years of experience as a Data Analyst cleaning and redistributing data.
Must have knowledge of database structures.
ETL and Change Management experience is needed.
Knowledge of Snowflake or other database technologies is a plus.
Not a client facing position.
0%-10% travel required.
Show more
Show less","Data Analysis, Data Cleaning, Data Redistribution, Database Structures, ETL, Change Management, Snowflake, Database Technologies","data analysis, data cleaning, data redistribution, database structures, etl, change management, snowflake, database technologies","change management, data cleaning, data redistribution, dataanalytics, database structures, database technologies, etl, snowflake"
Senior BI Data Engineer,Savers | Value Village,"Bellevue, WA",https://www.linkedin.com/jobs/view/senior-bi-data-engineer-at-savers-value-village-3783817176,2023-12-17,Kent,United States,Mid senior,Hybrid,"Pay Range: $101,272.00 - $151,907.00
Savers Benefits
Geographic & job eligibility rules may apply
Healthcare Plans
Comprehensive coverage (medical/dental/vision) at a reasonable cost
Specialized health programs – Improve wellness (quit smoking, counseling, diabetes management, chronic joint pain)
Paid Time Off
Sick Pay
Vacation Pay
Actual amount based on position and hours worked
Increases with length of service
40 to 80 hours annually
Holiday Pay
Based on position and full-time/part-time status
6 paid holidays plus 1 to 2 additional floating holidays
Team member discounts
Up to 50% off store merchandise
Flexible spending accounts
Use pre-tax dollars for eligible health and day care expenses
Employee Assistance Program (EAP)
A whole suite of free tools and resources to manage life’s challenges and maintain a healthy work-life balance
Life insurance
Company provided peace of mind and the option to purchase a supplemental plan
Additional Benefits
Annual Bonus
Performance Merit Increases
Disability Insurance
Parental Leave
Job Title: Senior Software Engineer, Business Intelligence
Who we are:
As one of the largest for-profit thrift operators in the United States, Canada and Australia for value priced pre-owned clothing, accessories and household goods, our mission is to champion reuse and inspire a future where secondhand is second nature. We supply our stores with gently-used, one-of-a-kind items that have been donated by the community to local nonprofit organizations. We purchase these items directly from our nonprofit partners, redirecting billions of pounds of used items away from landfills and on to our store racks and shelves for reuse, and providing our partners with valuable funding for their community-based programs and services. You’ll often hear us say that we are “Thrift Proud.” It’s the millions of loyal customers who visit our 300+ stores and our more than 22,000 team members that make it possible. Learn more about the Savers® family of thrift stores, our impact, and the #ThriftProud movement at savers.com.
Our brands are Savers (in the U.S.), 2nd Ave (in the U.S.), Value Village (in the U.S. and Canada), Unique (in the U.S.), Village des Valeurs (in Quebec) and Savers Australia.
What you can expect :
The opportunity to celebrate uniqueness. We are made up of people from many different backgrounds, experiences, and perspectives, all with something special to contribute.
To work in a purpose-driven company, with a business model that people love for our impact on both the planet and local communities.
An investment in the career path interests of our people. With our aggressive store expansion plans, we recognize the importance of the continued growth and development of our team members.
What you get:
Comprehensive onboarding and training from day one.
In-house expertise! Our training department / Savers University develops and delivers robust training to every team member across our organization — from new hires to senior executives. We develop around 90% of our training internally, and partner with a variety of renowned providers to supplement our offerings.
Benefits offerings including:
Bundled health plans such as medical, Rx, dental and vision
Company-paid life insurance for extra protection and peace of mind
Programs to stop smoking, diabetes management coaching, and on demand care options.
A 401k plan with generous company matching contributions to assist you in saving for a secure financial future.
Paid time off from work for leisure or other hobbies.
A range of mental health services to assist you in managing daily life.
What you’ll be working on:
Summary:
Savers is investing in business intelligence and data-driven decision support. We are looking for a Senior Software Engineer to be part of a team that is responsible for the technical architecture, development and growing the skills and thought leadership around cloud data engineering.
You will have to utilize a broad range of skills and experience ranging from data architecture to ETL, performance analysis, analytics, CICD, DataOps, etc. You will have to use your insight to make connection between our customer’s specific business problem and the BI solution and, the customer-facing skills to communicate that connection and vision to a wide variety of technical and executive audiences.
You are a self-starter and used to a fast-paced and energetic environment. You’re a collaborative person who is excited about technology and driven to get things done. You are not afraid to question the norm and embrace a continuous improvement and learner mentality.
This role reports to the Manager, Software Engineering – Business Intelligence (BI)
Essential Job Functions:
Ability to interpret business requirements and assist in the development of products to meet those requirements.
Develop and maintain scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Ability to translate design document elements into application code and actively participation in code and design reviews.
Collaborate with peers to design scalable and flexible solutions that will grow with business demands.
Outstanding skills presenting to both technical and executive audiences, whether impromptu on a whiteboard or using presentations and demos
Support the BI environment for the enterprise by enabling new features, evangelizing best practices, resolving customer issues and, working with vendors and partners when additional support is needed.
Provide technical guidance on how to resolve customer-specific technical challenges.
Support other team members develop their expertise.
Support developing and documenting standards and practices.
What you have:
Required Knowledge, Skills and Abilities:
Hands-on experience working with modern ETL, BI tools & technology.
Hands-on delivery experience in leading cloud data platforms: Snowflake, Azure Synapse.
Extensive recent experience with SQL and SQL analytics: SQL Server, SSIS, SSRS, SSAS
Hands-on experience implementing ETL pipelines using packaged tools: Attunity, Azure Data Factory
Familiarity and experience with common BI and data exploration tools – Power BI, DAX
Software development experience with C/C++ a definite plus.
Scripting experience with Python, Ruby, Perl, Bash a definite plus.
Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Strong organizational skills.
Excellent written and oral communication skills.
Business / Retail Operations background preferred.
Minimum Required Education, Training and Experience:
Bachelor’s degree in Computer Science or related field is preferred or equivalent experience.
6 – 8 years’ experience in BI Engineering role
Physical Requirements:
Ability to lift and carry up to 25 lbs.
Ability to express or exchange ideas by means of the spoken word.
Ability to receive detailed information through verbal communication, and to make the discriminations in sound.
Ability to receive detailed information visually through written communication (both physical and electronic).
FLSA Status
Exempt
Travel:
10 % or less travel
Work Address : Remote, USA
Savers is an eVerify employer
Show more
Show less","ETL, Cloud data engineering, SQL, SQL analytics, SQL Server, SSIS, SSRS, SSAS, Attunity, Azure Data Factory, Power BI, DAX, C/C++, Python, Ruby, Perl, Bash, Snowflake, Azure Synapse","etl, cloud data engineering, sql, sql analytics, sql server, ssis, ssrs, ssas, attunity, azure data factory, power bi, dax, cc, python, ruby, perl, bash, snowflake, azure synapse","attunity, azure data factory, azure synapse, bash, cc, cloud data engineering, dax, etl, perl, powerbi, python, ruby, snowflake, sql, sql analytics, sql server, ssas, ssis, ssrs"
Data Platform Engineer,Harnham,"Seattle, WA",https://www.linkedin.com/jobs/view/data-platform-engineer-at-harnham-3750647671,2023-12-17,Kent,United States,Mid senior,Hybrid,"Data Platform Engineer
Hybrid- San Francisco, CA and Seattle, WA, Boston, MA
125,000-145,000K with bonus
This role cannot sponsor at this time.
The Company
I am working with a huge player in the biotech and pharma space who is looking for an experienced Data Platform Engineer to join their team.
THE ROLE
Responsibilities Will Include
Automating various data flows.
Support end-to-end code traceability and data provenance.
Partner with tech to modify tools as needed.
Build and manage reusable components and architecture s designed to make it both fast and easy to build robust, scalable, production-grade data products and services.
Your Skills Include
3+ years of relevant experience.
Experience with distributed data tools ( Spark, Kafka, Hive)
Experience with cloud platforms.
Experience with specialized data architecture.
Demonstrated excellence in writing production Python, Java, Scala, Go, and/or C#/C++
Experience building and designing a DevOps first way of working.
The Benefits
competitive salary plus benefits.
How To Apply
Please register your interest by sending your CV to Kyle Margolies via the Apply link on this page.
Show more
Show less","Spark, Kafka, Hive, Python, Java, Scala, Go, C#, C++, DevOps","spark, kafka, hive, python, java, scala, go, c, c, devops","c, devops, go, hive, java, kafka, python, scala, spark"
Senior Data Engineer,Siteimprove,"Bellevue, WA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-siteimprove-3770714400,2023-12-17,Kent,United States,Mid senior,Hybrid,"The Siteimprove Data Platform team is responsible for creating and managing the integrated technologies supporting the end-to-end lifecycle of data across the enterprise. Our engineers are building an event-driven, API-first platform that will enable internal and external business partners to acquire, retain, process, govern, and secure data at a global scale.
Our team is looking for excellent senior data engineers who will play a pivotal role in designing, developing, and enhancing our data pipelines and analytics framework, enabling the efficient processing and analysis of data to drive informed business decisions.
What You Will Be Doing
Lead the design, development, and maintenance of robust and scalable data pipelines to extract, transform, and load data from various sources.
Architect and implement an analytics framework that enables the processing, aggregation, and visualization of data insights.
Collaborate closely with data scientists, data analysts, and cross-functional teams to understand data requirements and use cases.
Build data pipelines that support both batch and real-time processing to ensure timely access to data-driven insights.
Optimize data pipelines for performance, scalability, and reliability while considering data quality and integrity.
Implement automated testing and monitoring strategies to ensure the quality and availability of data pipelines and analytics processes.
Stay updated with emerging technologies and best practices in data engineering and analytics.
Participate in code reviews, providing and receiving constructive feedback to enhance code quality.
Mentor junior engineers, sharing expertise and best practices to support their growth.
Collaborate on defining data governance and data quality standards within the analytics framework.
Contribute to architectural discussions and decisions to improve the overall design of data solutions.
Perform other related duties as assigned.
What We Will Require Of You
Bachelor's degree in Computer Science, Engineering, or a related field; Master's degree is a plus.
6+ years of professional software engineering experience, with a strong background in data engineering and analytics.
Proficiency in programming languages used in data engineering (Python, Java, Scala, etc.).
Strong expertise in building and optimizing data pipelines using ETL/ELT frameworks (e.g., Apache Spark, Apache Beam, etc.).
Experience with cloud-native data warehousing, data lakes, and data modeling concepts.
Familiarity with analytics frameworks and tools (e.g., Pandas, Dask, Jupyter, etc.).
Solid understanding of SQL and NoSQL databases.
Experience with cloud platforms and containerization technologies, including cloud-managed services.
Strong problem-solving skills and a detail-oriented mindset.
Excellent communication skills, both written and verbal.
Agile mindset and familiarity with Agile methodologies.
Travel as needed
What We Will Love About You
Knowledge of machine learning models and systems using Python, TensorFlow, PyTorch, or other frameworks.
Strong communication skills and highly effective collaborator. You articulate your ideas to teammates, peers, and leaders, providing details and supporting your ideas with data where applicable. You incorporate others' input and feedback and strive to find common ground
In addition, we hope you will appreciate:
Rest and relaxation: Open Paid Time Off (OPTO) program for vacation, personal illness, mental health, or to care for a family member, 11 paid holidays, and two Give Back Days
Comprehensive benefits: National medical plan, dental, vision, paid maternity leave, paid paternity leave, HSA, Flex, employer-sponsored short-term, long-term disability, discounts to volunteer plans to meet your family needs, and more!
Prepare for the future: 401(k) with a company match to provide a better future in your retirement years.
$120,000 - $165,000 a year
Base pay will depend on the position, individual qualifications, market, and other operational business needs.
Siteimprove is an equal opportunity employer
All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status.
Siteimprove is a global corporation that has developed data practices to ensure your personally identifiable information is appropriately protected. Please note that personal information may be transferred, accessed, and stored globally as necessary for the uses and disclosures stated in accordance with our Privacy Policy at https://siteimprove.com/en/privacy/ .
Show more
Show less","Data engineering, Data analytics, Data pipelines, ETL/ELT frameworks, Cloudnative data warehousing, Data lakes, Data modeling, Analytics frameworks, SQL, NoSQL databases, Cloud platforms, Containerization technologies, Machine learning models, Python, Java, Scala, Apache Spark, Apache Beam, Pandas, Dask, Jupyter, TensorFlow, PyTorch, Rest API","data engineering, data analytics, data pipelines, etlelt frameworks, cloudnative data warehousing, data lakes, data modeling, analytics frameworks, sql, nosql databases, cloud platforms, containerization technologies, machine learning models, python, java, scala, apache spark, apache beam, pandas, dask, jupyter, tensorflow, pytorch, rest api","analytics frameworks, apache beam, apache spark, cloud platforms, cloudnative data warehousing, containerization technologies, dask, data engineering, data lakes, dataanalytics, datamodeling, datapipeline, etlelt frameworks, java, jupyter, machine learning models, nosql databases, pandas, python, pytorch, rest api, scala, sql, tensorflow"
Data Engineer,"DevSelect, LLC","Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-at-devselect-llc-3614379581,2023-12-17,Lisle,United States,Mid senior,Onsite,"Title: Data Engineer
Location: Chicago, Ill
Openings: 1
Type: Permanent hire
The Client is searching for an experienced data engineer. This person will be responsible for building and managing an array of data acquisition services. The Ideal candidate will demonstrate experience with software development lifecycles, working with cloud architecture, and development within a software team.
Responsibilities include data mining, web scraping, cleaning and organizing the data into both SQL and NoSQL databases. Further responsibilities will include building eﬃcient APIs to communicate the data and managing continual data ingestion.
Required Job Qualifications
At least 4 years experience with python development and database management in a Linux environment
Expertise in relational database architecture and eﬃcient SQL. Experience with managing databases in Azure is a plus as well as Neo4j graph databases.
Development of RESTful APIs
Distributed computing (Spark or similar)
Ability to provide guidance on data security issues
Writing and maintaining clear and reproducible documentation for all services and projects will be required.
Strong general communication and interpersonal skill
Capable of providing guidance to junior developers
Preferred Job Qualifications
Experience in machine learning and general data science
Data visualization
Understanding of the healthcare industry
Azure, Google, AWS or other cloud-platform experience
SDLC Tools & Processes: JIRA, Docker, Git-flow, Puppet, or similar
Education Requirements
B.Sc. in Computer Science, Engineering, or related field.
Show more
Show less","Python, Linux, SQL, NoSQL, Azure, Neo4j, RESTful APIs, Spark, Data security, JIRA, Docker, Gitflow, Puppet, Machine learning, Data science, Data visualization, Healthcare industry, Cloud platforms","python, linux, sql, nosql, azure, neo4j, restful apis, spark, data security, jira, docker, gitflow, puppet, machine learning, data science, data visualization, healthcare industry, cloud platforms","azure, cloud platforms, data science, data security, docker, gitflow, healthcare industry, jira, linux, machine learning, neo4j, nosql, puppet, python, restful apis, spark, sql, visualization"
Data Engineer,LaSalle Network,"Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-at-lasalle-network-3442722431,2023-12-17,Lisle,United States,Mid senior,Onsite,"Job Description
Want to grow your career with one of the fastest growing industries? LaSalle Network is partnered with a top Chicago-based client looking to hire a
Data Engineer
their team.
The
Data Engineer
will be responsible for delivering strategic and tangible solutions through data. We are looking for self-starters that have the ability to work cross-functionally across various teams.
Data Engineer Responsibilities:
Architect, implement and maintain the information data lake and leverage various reporting platforms to enable decision making across the portfolio companies
Build ETLs to ingest data into the data warehouse and data lake, as well as end-user facing reporting applications
Work to deliver flexible, scalable, end-to-end solutions
Work with big data and emerging technologies while driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, reporting and dashboarding
Close coordination with the senior leadership team on a day-to-day basis; ability to clearly communicate complicated data infrastructure concepts is a must
Support for other ad hoc strategic data projects and initiatives
Data Engineer Requirements:
4+ years of experience with Data Modeling, SQL, ETL, Data Warehousing, (Snowflake, Redshift, Azure, AWS)
4+ years of experience with enterprise-class Business Intelligence tools such as Looker, Power BI, Tableau, Oracle BI, MicroStrategy
Python knowledge or experience an added plus
Bachelor’s degree in computer science, information systems, mathematics, statistics or related field
Expertise in the design, creation, management and business use of large datasets
Machine Learning experience a plus
Passionate about working with large unstructured datasets and helping set the vision for the overall data strategy across the growing portfolio
Thank you,
Kelsey Gonzalez
Team Lead
LaSalle Network
LaSalle Network is an Equal Opportunity Employer m/f/d/v.
LaSalle Network is the leading provider of direct hire and temporary staffing services. For over two decades, LaSalle has helped organizations hire faster and connect top talent with opportunities, from entry-level positions to the C-suite. With units specializing in Accounting and Finance, Administrative, Marketing, Technology, Supply chain, Healthcare Revenue Cycle, Call Center, Human Resources and Executive Search. LaSalle offers staffing and recruiting solutions to companies of all sizes and across all industries. LaSalle Network is the premier staffing and recruiting firm, earning over 100 culture, revenue and industry-based awards from major publications and having its company experts regularly contribute insights on retention strategies, hiring trends and hiring challenges, and more to national news outlets.
Show more
Show less","Data Modeling, SQL, ETL, Data Warehousing, Snowflake, Redshift, Azure, AWS, Looker, Power BI, Tableau, Oracle BI, MicroStrategy, Python, Machine Learning, Big Data, Emerging Technologies, Business Intelligence, Reporting, Dashboarding","data modeling, sql, etl, data warehousing, snowflake, redshift, azure, aws, looker, power bi, tableau, oracle bi, microstrategy, python, machine learning, big data, emerging technologies, business intelligence, reporting, dashboarding","aws, azure, big data, business intelligence, dashboard, datamodeling, datawarehouse, emerging technologies, etl, looker, machine learning, microstrategy, oracle bi, powerbi, python, redshift, reporting, snowflake, sql, tableau"
Data Engineer,Truleo,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-at-truleo-3770793671,2023-12-17,Fort Lee,United States,Mid senior,Onsite,"About Truleo
Truleo (www.truleo.co) Truleo is an advanced natural language processing (NLP) platform that automates the review and analysis of body worn cameras (BWC). This enables law enforcement agencies and local governments to gather actionable insights from the thousands of daily interactions captured on BWC. Truleo examines these interactions across proprietary models built by the top 1% of law enforcement, ultimately helping agencies increase professionalism and mitigate risk. Our mission is to support police agencies in building trust between law enforcement and the community they serve. We accomplish this by surfacing risky interactions, identifying trends in officer behavior, and creating a cost effective way for departments to analyze and search through all of their BWC data.
Why
We are a force for good. We have partnered with police departments around the country and are helping them to improve trust with their community. Truleo is a well funded, early-stage startup, with world class technology and culture. We are looking for the best of the best to join our rapidly growing team. We are offering top salary, benefits, equity, the opportunity to work with a first class team on an impactful problem and a chance to make a vertical leap in your career.
Who
Truleo is hiring for a Data Engineer who will help us refine and expand our data approach and infrastructure through scalable data models and pipelines. You will be a critical member of Truleo’s data team and collaboratively contribute to the creation of innovative data ideas and principles. Your expertise will inform designs, timelines, and business and operational decision making, ensuring Truleo can remain at the forefront of technology excellence.
Truleo encourages applications from people of all races, religions, national origins, genders, sexual orientations, gender identities, gender expressions, and ages, as well as veterans and individuals with disabilities.
What You’ll Do
Drive improvements to Truleo’s analytics efforts through the creation of core data models and storage structures.
Strategize, design, and automate high-volume, batch, and real-time ETL pipelines.
Design and execute test plans to validate the accuracy and completeness of data.
Collaborate closely with data scientists, data analysts, BI developers, solution architects, and dev teams to test, troubleshoot and resolve issues and implement technical solutions.
Deploy backend production processes that continuously emphasize high availability, robustness, and monitoring.
Build and improve workflow orchestration tooling to support efficient data pipelines, and employ sustainable source code management tools and repositories.
Who You Are
You have a bachelor's degree in computer science, engineering, data science, or a related discipline; or you have equivalent education and experience.
You have a minimum of 3 years of experience in data science, data processing and modeling, data engineering, data warehousing, Azure, or AWS cloud technologies.
You have a minimum of 3 years of experience with Python, Scala, or Java.
You have experience in debugging and writing complex SQL queries.
You have deep knowledge of Azure Data Factory, Azure SQL Databases, Azure Databricks, AWS Redshift, and/or AWS Glue.
You have experience in cloud-based infrastructure, unit testing and test-driven dev, and querying and manipulating data in relational and/or NoSQL databases.
You possess a competitive understanding of data structures and types, data transformation and validation, and data performance tuning.
You have strong knowledge of Infrastructure as Code (IaC) and the provisioning tools that enable IaC, including but not limited to Ansible, Jenkins, and/or Terraform.
Show more
Show less","Data Engineering, Natural Language Processing, SQL, Python, Java, Scala, Azure Data Factory, Azure SQL Databases, Azure Databricks, AWS Redshift, AWS Glue, AWS Cloud Technologies, Ansible, Jenkins, Terraform, Data structures, Data types, Data transformation, Data validation, Data performance tuning, IaC","data engineering, natural language processing, sql, python, java, scala, azure data factory, azure sql databases, azure databricks, aws redshift, aws glue, aws cloud technologies, ansible, jenkins, terraform, data structures, data types, data transformation, data validation, data performance tuning, iac","ansible, aws cloud technologies, aws glue, aws redshift, azure data factory, azure databricks, azure sql databases, data engineering, data performance tuning, data structures, data transformation, data types, data validation, iac, java, jenkins, natural language processing, python, scala, sql, terraform"
Junior Data Engineer,Phoenix Recruitment LLC,"Baltimore, MD",https://www.linkedin.com/jobs/view/junior-data-engineer-at-phoenix-recruitment-llc-3788770591,2023-12-17,Toccoa,United States,Mid senior,Remote,"This is a remote position.
Title - J
unior Data Engineer
, 1 year of project experience
Employment Type:
Full-time
Base Salary:
$60K-$70K
Phoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.
Position Summary
Join the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance the efficiency, reliability, and performance of CVS Health’s IT operations.
Key Responsibilities include:
Data pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation.
Data modeling: Create and maintain data models ensuring data quality, scalability, and efficiency
Develop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency
Data Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data
Utilize big data technologies such as Kafka to process and analyze large volumes of data efficiently
Implement data security measures to protect sensitive information and ensure compliance with data and privacy regulation
Create/maintain documentation for data processes, data flows, and system configurations
Performance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness
Characteristics of this role:
Team Player: Willing to teach, share knowledge, and work with others to make the team successful.
Communication: Exceptional verbal, written, organizational, presentation, and communication skills.
Creativity: Ability to take written and verbal requirements and come up with other innovative ideas.
Attention to detail: Systematically and accurately research future solutions and current problems.
Strong work ethic: The innate drive to do work extremely well.
Passion: A drive to deliver better products and services than expected to customers.
Required Qualifications
2+ years of programming experience in languages such as Python, Java, SQL
2+ years of experience with ETL tools and database management (relational, non-relational)
2+ years of experience in data modeling techniques and tools to design efficient scalable data structures
Skills in data quality assessment, data cleansing, and data validation
Preferred Qualifications
Knowledge of big data technologies and cloud platforms
Experience with technologies like PySpark, Databricks, and Azure Synapse.
Education
Bachelor’s degree in Computer Science, Information Technology, or related field, or equivalent working experience
Why Phoenix Recruitment LLC?
Phoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization.
Show more
Show less","Data engineering, Data modeling, Data pipelines, Data cleansing, Kafka, Data integration, SQL, Python, Java, ETL tools, Relational databases, Nonrelational databases, Data quality assessment, Data validation, Big data technologies, Cloud platforms, PySpark, Databricks, Azure Synapse, Data security, Performance optimization","data engineering, data modeling, data pipelines, data cleansing, kafka, data integration, sql, python, java, etl tools, relational databases, nonrelational databases, data quality assessment, data validation, big data technologies, cloud platforms, pyspark, databricks, azure synapse, data security, performance optimization","azure synapse, big data technologies, cloud platforms, data engineering, data integration, data quality assessment, data security, data validation, databricks, datacleaning, datamodeling, datapipeline, etl tools, java, kafka, nonrelational databases, performance optimization, python, relational databases, spark, sql"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Villarestaurantgroup,"Cornwall, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-villarestaurantgroup-3756469707,2023-12-17,Cornwall, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, SQL, R, Python, Performance Metrics, Reports, A/B Testing, Data Quality, Data Collection, Data Cleansing, Data Manipulation, Data Visualization, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL Processes","data analysis, statistical techniques, sql, r, python, performance metrics, reports, ab testing, data quality, data collection, data cleansing, data manipulation, data visualization, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, data collection, data manipulation, data quality, dataanalytics, datacleaning, etl, hypothesis testing, performance metrics, powerbi, python, r, reports, sql, statistical modeling, statistical techniques, tableau, visualization"
"Staff Data Solutions Engineer- Madison, WI",findhelp,"Madison, WI",https://www.linkedin.com/jobs/view/staff-data-solutions-engineer-madison-wi-at-findhelp-3748258429,2023-12-17,Windsor,United States,Associate,Onsite,"We’re changing the way people connect to social care.
At findhelp, we’ve built a comprehensive platform of products and services that make it easy for you to connect people to resources, follow them on their journey, and track your impact in a fast and reliable way. Our industry-leading social care network includes more than half a million local, state, and national programs that serve every ZIP Code in the country, from rural areas to major metropolitan centers.
Findhelp is headquartered in Austin, Texas and has been enabling healthcare, government, education, and other organizations to connect people with the social care resources that serve them, with privacy and security, since 2010.
As a mission driven organization, we are focused on creating a positive impact by connecting people in need to the programs that serve them with dignity and ease. Powered by our proprietary technology that enables people to find the resources available in their area, we have helped millions of Seekers find food, health, housing and employment programs.
Job Overview:
We are seeking a highly skilled Staff Data Solution Engineer to join our team. As a Staff Data Solution Engineer, you will coordinate and contribute to the design, development, and implementation of data solutions for our customers and team. You will be responsible for working closely with our product team and customers to understand the data needs of today and tomorrow. This role offers an exciting opportunity to work on diverse projects, solve complex problems, and contribute to the success of our data-driven initiatives.
Responsibilities and Duties:
Client Collaboration: Engage with clients to understand their data requirements and challenges. Serve as a strong technical counterpart and trusted advisor to customer success and government relations teams.
Data Solution Design: Translate business requirements to analytical dashboard designs and operational data delivery solutions that meet the needs of a diverse and growing user audience.
Data Management: Leverage knowledge of relational databases to analyze and optimize schemas, queries, and data movement (ETL).
Problem Solving: Tackle new and complex data-related problems with creativity and a structured approach. Identify opportunities for simplification, process improvement and efficiency gains.
Continuous Learning: Stay up-to-date with industry trends, emerging technologies, and best practices in data engineering, data modeling, and use of AI.
Architecture analysis: Understand data from source to target and inform design, considering optimal methods for data capture, logging, transformation, warehousing, and delivery.
Qualifications:
Bachelor’s degree in Computer Science, Engineering, or related field ( or proven experience (7+ years) in data engineering, dashboard development, or a related role)
Experience with data visualization tools to develop dashboards and reports.
Proficiency in database systems such as MySQL and BigQuery
Strong SQL programming skills with the ability to write complex queries and optimize database performance.
Excellent problem-solving skills and the ability to work on new and challenging data problems.
Exceptional communication and customer-facing skills, with the ability to explain technical concepts to non-technical audiences.
Preferred: Experience with data management, ETL processes, and data movement techniques in Cloud infrastructure.
Preferred: Experience with Python for data manipulation, scripting, or automation.
$89,280 - $133,920 a year
The compensation for this position will be based on a candidate’s job-related skills, experience, education or training, and location.
This job is based on-site in Austin, TX or Madison, WI with the opportunity to work remotely on Fridays.
Perks at findhelp
401k & stock options
Free food and onsite gym at our Austin HQ
Paid parental leave
Competitive PTO & 10 paid holidays
Health, dental, and vision insurance
Pet-friendly office with attached dog park at our Austin HQ
24/7 access to telemedicine and counseling
Book Purchasing Program
We’re building a diverse, inclusive team
You’re welcome here. We want everyone to be able to easily connect to the help they need, and we want our teams to reflect and represent our communities. It is our policy to recruit, hire, train, and promote individuals, as well as administer any and all Company policies, without regard to age, color, disability, ethnicity, family or marital status, gender identity or expression, language, national origin or ancestry, physical and mental ability, political affiliation, race, religion, creed, sexual orientation, socio-economic status, veteran status, or any other protected class, in accordance with applicable laws . Accommodations are available for applicants with disabilities.
Here are some of the ways we support our staff:
Culture Committee
Leadership Development Training
Paid Volunteering Time
Show more
Show less","Staff Data Solution Engineer, Data visualization, MySQL, BigQuery, SQL, Problemsolving, Communication, Customerfacing, Data management, ETL, Python, Cloud infrastructure","staff data solution engineer, data visualization, mysql, bigquery, sql, problemsolving, communication, customerfacing, data management, etl, python, cloud infrastructure","bigquery, cloud infrastructure, communication, customerfacing, data management, etl, mysql, problemsolving, python, sql, staff data solution engineer, visualization"
Data Engineer - Offshore,Compu-Vision Consulting Inc.,"Sun Prairie Town, WI",https://www.linkedin.com/jobs/view/data-engineer-offshore-at-compu-vision-consulting-inc-3769521785,2023-12-17,Windsor,United States,Mid senior,Onsite,"Title: Data Engineer - Offshore
Duration: 4 months +
Location: Remote Role
Interview: Phone + Video
Job Description
Bachelors in Computer Science, Information Systems or related educational background
Excellent working knowledge of SQL & experience querying big data platforms, relational databases, MySQL, Oracle, DB2, etc.
Demonstrated knowledge of and experience in business data definition, data profiling, and data mapping analysis
Excellent understanding of how technology impacts the business
Must have strong business acumen as well as technical solutions expertise
Strong ability to independently perform analysis of business workflow, and technology issues to facilitate decision-making
Multi-tasking along with strong organization and time management skills is a must
Must have strong problem-solving skills
Excellent interpersonal, communication, presentation, documentation skills
Knowledge of Agile methodologies
Experience with Data APIs (Working with RESTful endpoints and/or SOAP)
Knowledge of Microsoft Azure, Databricks, Hadoop 2.0 ecosystems, HDFS, Pyspark, etc. is preferred
Show more
Show less","SQL, Big Data Platforms, MySQL, Oracle, DB2, Data Definition, Data Profiling, Data Mapping Analysis, Business Acumen, Technical Solutions Expertise, Data Analysis, Business Workflow Analysis, Technology Issues Analysis, Multitasking, Organization, Time Management, Problemsolving, Interpersonal Skills, Communication Skills, Presentation Skills, Documentation Skills, Agile Methodologies, Data APIs, RESTful Endpoints, SOAP, Microsoft Azure, Databricks, Hadoop 2.0 Ecosystems, HDFS, Pyspark","sql, big data platforms, mysql, oracle, db2, data definition, data profiling, data mapping analysis, business acumen, technical solutions expertise, data analysis, business workflow analysis, technology issues analysis, multitasking, organization, time management, problemsolving, interpersonal skills, communication skills, presentation skills, documentation skills, agile methodologies, data apis, restful endpoints, soap, microsoft azure, databricks, hadoop 20 ecosystems, hdfs, pyspark","agile methodologies, big data platforms, business acumen, business workflow analysis, communication skills, data apis, data definition, data mapping analysis, data profiling, dataanalytics, databricks, db2, documentation skills, hadoop 20 ecosystems, hdfs, interpersonal skills, microsoft azure, multitasking, mysql, oracle, organization, presentation skills, problemsolving, restful endpoints, soap, spark, sql, technical solutions expertise, technology issues analysis, time management"
Staff Data Engineer,Recruiting from Scratch,"Madison, WI",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744389884,2023-12-17,Windsor,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Deployment, Streamprocessing systems, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, ETL pipelines, Data management tools, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, etl pipelines, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, deployment, dimensional data modeling, docker, etl pipelines, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Madison, WI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748833028,2023-12-17,Windsor,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Warehousing, Kafka, Spark, Snowflake, Python, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, ETL Pipelines, Data Management Tools, Data Classification, Retention","data engineering, data warehousing, kafka, spark, snowflake, python, sql, tdd, pair programming, continuous integration, automated testing, deployment, etl pipelines, data management tools, data classification, retention","automated testing, continuous integration, data classification, data engineering, data management tools, datawarehouse, deployment, etl pipelines, kafka, pair programming, python, retention, snowflake, spark, sql, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Madison, WI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744390732,2023-12-17,Windsor,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Big Data Technologies, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Legal Compliance, Data Management Tools, Data Classification, Data Retention","data engineering, business intelligence, data science, python, big data technologies, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, legal compliance, data management tools, data classification, data retention","airflow, automated testing, big data technologies, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Natural Resources Data Scientist - Senior,State of Wisconsin,"Madison, WI",https://www.linkedin.com/jobs/view/natural-resources-data-scientist-senior-at-state-of-wisconsin-3763378742,2023-12-17,Windsor,United States,Mid senior,Onsite,"Natural Resources Research Scientist - Senior
Location: [Location]
Job Summary:
Join our dynamic team at the Division of Forestry, Forest Ecology and Economics section as a Natural Resources Research Scientist. In this role, you will play a crucial part in managing and analyzing datasets for various research projects, with a key focus on an innovative project aimed at enhancing the complexity of homogeneous stands of northern pine. This position offers extensive collaboration opportunities and the chance to develop a diverse skill set in forest ecology and management.
Responsibilities:
Manage and analyze datasets for research projects, with a particular emphasis on the innovative project related to northern pine stands.
Collaborate with the United States Forest Service (USFS) to reconstruct fire history and evaluate the effects of prescribed burning on heterobasidion root disease (HRD) infected stands.
Assess the efficacy of oak wilt containment measures.
Work independently and as part of a team to conduct statistical analyses and develop and maintain databases.
Design databases and assist with statistical analysis for ongoing projects.
Prepare reports, summarize data, and contribute to manuscript preparation.
Qualifications:
Proven experience as a data scientist, with a focus on forestry research.
Strong skills in statistical analysis, database management, and building databases.
Familiarity with collaborative research projects and applying scientific knowledge to forest ecology and management.
Excellent problem-solving abilities and the capacity to work independently.
Strong written and verbal communication skills.
Proficiency in relevant software, such as statistical analysis tools and database management systems.
Forester - Senior
Location: [Location]
Job Summary:
Join our team at the Division of Forestry, Forest Ecology and Economics section as a Senior Forester. In this position, you will undertake a range of forestry work, including data collection and the establishment and maintenance of study sites and demonstration areas. Collaborating with internal and external partners, you will contribute to various forestry projects and help develop new and innovative silviculture systems.
Responsibilities:
Coordinate and conduct forestry work in the field, including data collection and the setup and maintenance of study sites and demonstration areas.
Collect and maintain spatial data in a Geographic Information System (GIS) and perform spatial data analyses for forestry projects.
Collaborate with the United States Forest Service (USFS), state, and county foresters.
Develop new silviculture systems and establish marking guidelines for managers.
Utilize Arc GIS software and programming languages like R to review and analyze spatial datasets.
Independently build geodatabases and design apps for spatial data collection.
Resolve technical problems and work independently and as a member of a team.
Qualifications:
Extensive experience in forestry work, particularly in data collection and study site management.
Strong skills in spatial data analysis, using software like Arc GIS and programming languages like R.
Familiarity with collaborative work with internal and external partners.
Knowledge of new and innovative silviculture systems and prescribed fire applications.
Excellent problem-solving skills and the ability to work independently.
Strong communication and interpersonal skills.
Proficiency in relevant software and tools related to forestry research and spatial data analysis.
To apply for these positions, please visit our website at [website] and search for Job ID #7064. We look forward to receiving your application.
Please note: Only qualified candidates will be contacted for further consideration.
Employment Type: Full-Time
Show more
Show less","Data analysis, Database management, Statistical analysis, Collaborative research, Forest ecology, GIS, Silviculture, Spatial data analysis, Arc GIS, R, Geodatabase, Spatial data collection, Forestry work, Study site management, Prescribed fire, Communication, Problemsolving, Teamwork","data analysis, database management, statistical analysis, collaborative research, forest ecology, gis, silviculture, spatial data analysis, arc gis, r, geodatabase, spatial data collection, forestry work, study site management, prescribed fire, communication, problemsolving, teamwork","arc gis, collaborative research, communication, dataanalytics, database management, forest ecology, forestry work, geodatabase, gis, prescribed fire, problemsolving, r, silviculture, spatial data analysis, spatial data collection, statistical analysis, study site management, teamwork"
Senior Data Engineer,Advarra,"Madison, WI",https://www.linkedin.com/jobs/view/senior-data-engineer-at-advarra-3782084535,2023-12-17,Windsor,United States,Mid senior,Remote,"Advarra provides integrated solutions that safeguard trial participants, empower clinical sites, ensure compliance, and optimize research performance. Connecting the clinical research ecosystem, Advarra delivers solutions through a site-centric approach that unifies and accelerates the drug development lifecycle, making clinical trials safer, smarter, and faster.
General Summary
This role supports the Data Science team as a Sr Data Engineer for enterprise Data Warehouse (DW) Platform. Will require deep expertise in Snowflake and writing complex data transformations to support Business and Product Analytics. Understanding of security measures provided by Snowflake to secure the data on the platform.
Principal Duties & Responsibilities
Responsible for security and administration of the Snowflake Data Platform.
Build data pipeline using Fivetran to ingest large volume of data from both internal applications and external data source.
Write data transformation for Enablement of BI/ Analytics, Data Sciences, and external customers.
Configure Snowflake data security features to secure the data on the platform
Supports MDM implementation plan and direction toward a single version of truth.
Create, follow, and ensure compliance with database security.
Provide technical recommendations for reporting infrastructure project costing.
Design, maintain and deploy data sources for operational and strategic dashboards.
Education
Bachelor’s degree or equivalent combination of education and related work experience.
Snowflake certification preferred.
Experience
5 years of experience on Snowflake platform.
Experience in writing data transformation with dbt platform.
Experience in building connector with Fivetran platform.
Experience with Sigma Computing platform is a plu.s
Experience in writing test automation scripts.
Data Architecture & Data modeling experience in Clinical trials domain / Life Sciences.
Knowledge of data management tools and process; Data governance tools is a plus.
Working experience with version control platforms, e.g. Github, and agile methodologies and supporting tools JIRA.
Knowledge, Skills, Abilities
Understanding of Snowflake Warehouses, reader accounts, SSO Setup, and data masking policies.
Good understanding of Change Data Capture and Change Data Tracking.
Understanding challenges of ingesting large volume data.
Ability to writ complex data transformation logic using Advanced SQL query skills such as complex joins, sub-queries, grouping, aggregation, as well as stored procedures.
Programming languages, especially Python and Java.
Knowledge of data modeling and transformation tools like dbt, Tableau data prep etc.
Expert in data warehousing concepts, methodologies, and best practices.
Position requires a high level of responsibility regarding confidential information; must maintain confidentiality at all times.
Must be comfortable independently evaluating a situation, exercising good judgment and discretion, and independently making a decision matter of significance.
Excellent oral and written communication skills including the ability to speak in front of large groups.
Comfort working in a geographically distributed team-based environment.
Ability to handle stress and interact with others in a professional manner.
Physical and Mental Requirements:
Sit or stand for extended periods of time at stationary workstation
Occasionally, carry, raise, and lower objects of up to 10lbs
Learn and comprehend basic instructions
Focus and attention to tasks and responsibilities
Verbal communication; listening and understanding, responding and speaking
#mogul
EEO Statement
Advarra provides equal employment opportunity to all individuals regardless of their race, color, religion, creed, sex, sexual orientation, gender identity, national origin, age, disability, veteran, marital, or domestic partner status, citizenship, genetic information or any other status or characteristic covered by federal, state or local law. Further, the company takes affirmative action to ensure that applicants are employed, and employees are treated during employment without regard to any of these characteristics. Discrimination of any type will not be tolerated.
EEO/M/F/Disabled/Vets
Show more
Show less","Snowflake, dbt, Fivetran, Sigma Computing, Java, Python, Tableau, SQL, Data Warehouse, MDM, Data pipeline, Data transformation, Data security, Data governance, Data modeling, Data architecture, Data visualization, Agile methodologies, Data Warehousing, Data Engineering, Clinical Trials","snowflake, dbt, fivetran, sigma computing, java, python, tableau, sql, data warehouse, mdm, data pipeline, data transformation, data security, data governance, data modeling, data architecture, data visualization, agile methodologies, data warehousing, data engineering, clinical trials","agile methodologies, clinical trials, data architecture, data engineering, data governance, data pipeline, data security, data transformation, datamodeling, datawarehouse, dbt, fivetran, java, mdm, python, sigma computing, snowflake, sql, tableau, visualization"
Health Care Data Analyst or Insurance Data Analyst - Remote | WFH,Get It Recruit - Information Technology,"Madison, WI",https://www.linkedin.com/jobs/view/health-care-data-analyst-or-insurance-data-analyst-remote-wfh-at-get-it-recruit-information-technology-3774810589,2023-12-17,Windsor,United States,Mid senior,Remote,"Location: Madison, WI - Candidates should be WI residents or open to relocating to WI. While this role is primarily remote within the state of WI, occasional in-office presence is required on the 1st and 3rd Wednesdays of the month, as well as for board meetings, onsite trainings, or other essential meetings.
Interview Process: Conducted via MS Teams.
About Us
We are seeking a talented professional to join our team. We value diversity and innovation, and we're committed to fostering a positive work environment where everyone can thrive.
Key Required Skill
3+ years of working knowledge in contract development, administration, and performance evaluation.
Primary Responsibilities
Prepare quantitative reports, including the creation of graphs, charts, and tables for effective data display in management decision-making.
Develop dashboards in Tableau.
Utilize Microsoft Office Suite products, demonstrating advanced skills in Excel (pivot tables, link workbooks, plug-ins, formulas, etc.).
Bring 3+ years of experience as a Health Care Data Analyst or in a similar analytical role.
Proficiency in SQL and/or SAS or Alteryx programming.
Nice To Have Skills
Knowledge of information systems, including Electronic Medical Record (EMR), claims payment, medical billing, and phone systems with associated reporting software.
Familiarity with standardized ICD-10 and CPT medical billing codes.
Ability to manage multiple projects simultaneously, work independently, thrive under pressure, and adapt to change.
Inquisitive nature with the ability to seek answers to complex questions without being prompted.
If you are passionate about data analysis, reporting, and contributing to a dynamic work environment, we invite you to apply for this exciting opportunity.
Employment Type: Full-Time
Show more
Show less","Contract Development, Contract Administration, Contract Performance Evaluation, Tableau, Microsoft Office Suite, Excel, SQL, SAS, Alteryx, Electronic Medical Record (EMR), Medical Billing, Phone Systems, ICD10, CPT","contract development, contract administration, contract performance evaluation, tableau, microsoft office suite, excel, sql, sas, alteryx, electronic medical record emr, medical billing, phone systems, icd10, cpt","alteryx, contract administration, contract development, contract performance evaluation, cpt, electronic medical record emr, excel, icd10, medical billing, microsoft office suite, phone systems, sas, sql, tableau"
Senior Cloud Data Engineer,BDO USA,"Madison, WI",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765466999,2023-12-17,Windsor,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data analytics, Business intelligence, Machine learning, Data Lake Medallion Architecture, Batch and/or streaming data ingestion into a data lake, AI Algorithms/Machine Learning, Automation tools, Computer Vision based AI technologies, SQL, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, C#, Python, Java, Scala, Tabular modeling, Git, DevOps deployment technologies, Linux, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, machine learning, data lake medallion architecture, batch andor streaming data ingestion into a data lake, ai algorithmsmachine learning, automation tools, computer vision based ai technologies, sql, data warehousing, data modeling, semantic model definition, star schema construction, c, python, java, scala, tabular modeling, git, devops deployment technologies, linux, cloud, rpa, aws, redshift, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, qlik, athena, data pipeline, glue, star schema, data modeling, ssis, ssas, ssrs, pyspark, microsoft fabric, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql","ai algorithmsmachine learning, athena, automation tools, aws, aws lake formation, batch andor streaming data ingestion into a data lake, bicep, business intelligence, c, cloud, computer vision based ai technologies, data lake medallion architecture, data ops, data pipeline, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops deployment technologies, git, glue, java, kinesis, linux, machine learning, microsoft fabric, pandas, purview, python, qlik, quicksight, redshift, rpa, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema, star schema construction, tabular modeling, terraform"
Sr. Data Analyst,WebMD,"Madison, WI",https://www.linkedin.com/jobs/view/sr-data-analyst-at-webmd-3776262122,2023-12-17,Windsor,United States,Mid senior,Hybrid,"Mercury Healthcare, a WebMD Provider Services company, is a technology and data analytics company that empowers healthcare organizations to engage consumers and optimize provider networks to accelerate growth. Our customers benefit from more than 30 years' experience driving intelligent engagement, aligning physicians, and enabling personalized healthcare journeys. WebMD is an Equal Opportunity/Affirmative Action employer and does not discriminate on the basis of race, ancestry, color, religion, sex, gender, age, marital status, sexual orientation, gender identity, national origin, medical condition, disability, veterans status, or any other basis protected by law.
Position Overview:
WebMD Ignite is focused on providing trusted information that helps consumers and providers make meaningful connections. As a core member of our team, the Senior Data Analyst works to manage various product related data pipelines, ensuring that features and enhancements are fully tested prior to deployment. You will be responsible for understanding and validating the end to end data flows of the Ignite Growth Platform. You will also be responsible for providing guidance to the Product and QA team on how the data flows through the backend system. We value self-starters who are excited and willing to learn new technologies, take accountability for their work and are positive and collaborative.
What you will do:
Conduct detailed data analysis and communicate findings to Product Owners and Engineers.
Understand and define critical path data pipeline items and their impact to the overall platform in a complex multi-tenant environment.
Assist in defining overall data quality standards with engineering and business stakeholders to ensure client implementations are consistent.
Define data requirements for critical path features and communicate to the Product, Engineering and QA teams.
Develop and implement Agile data pipeline test approaches, strategies, plans, and expected results to ensure backend data validation is part of every sprint.
Define, enhance and streamline integration and end to end test cases to be included in the automation test suites.
Perform testing and validation of requirements and/or data management capabilities when/where applicable.
Perform intermediate root cause analysis and resolution of business data issues.
Actively participate in team discussions and contribute insights, suggestions, and innovative ideas to improve data processes and deliverables.
Work closely with the Engineering team to optimize data storage, retrieval, and analysis processes, utilizing industry best practices and technologies.
What you will bring:
Bachelor’s Degree in Computer Science related field
5+ years of experience working with healthcare data
2+ years of data requirements gathering and quality assurance
Strong knowledge of data pipeline development and testing practices
Strong SQL skills and knowledge of backend infrastructure
Experience working in a SaaS environment
Experience with back-end test automation frameworks
Experience with Agile and Kanban SDLC
Show more
Show less","Data Analysis, Agile, SQL, Kanban, Healthcare Data, Data Pipelines, Data Quality, Data Requirements, Automation Testing, Backend Infrastructure, Software as a Service (SaaS), Product Management, Engineering, Communications","data analysis, agile, sql, kanban, healthcare data, data pipelines, data quality, data requirements, automation testing, backend infrastructure, software as a service saas, product management, engineering, communications","agile, automation testing, backend infrastructure, communications, data quality, data requirements, dataanalytics, datapipeline, engineering, healthcare data, kanban, product management, software as a service saas, sql"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Madison, WI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709523,2023-12-17,Windsor,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML DataOps, Data Pipelining, Data Preprocessing, Data Postprocessing, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Statistical Analysis, Data Visualization, Pandas, R, Data Platforms, Data Frameworks, High Volume Data Processing, Real Time Data Processing, Batch Data Processing, Data Governance, Data Compliance, Data Security, NoSQL Databases, ETL Pipelines, Kafka, Storm, Spark Streaming, ML Concepts, Data Management Tools, Kubernetes, Docker, Microservices, Git","data engineering, ml dataops, data pipelining, data preprocessing, data postprocessing, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, data platforms, data frameworks, high volume data processing, real time data processing, batch data processing, data governance, data compliance, data security, nosql databases, etl pipelines, kafka, storm, spark streaming, ml concepts, data management tools, kubernetes, docker, microservices, git","batch data processing, data cleaning, data compliance, data engineering, data frameworks, data governance, data management tools, data mining, data normalization, data platforms, data postprocessing, data preprocessing, data security, datamodeling, datapipeline, docker, etl pipelines, git, high volume data processing, kafka, kubernetes, microservices, ml concepts, ml dataops, nosql databases, pandas, r, real time data processing, spark streaming, statistical analysis, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Madison, WI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773085807,2023-12-17,Windsor,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","ML, Data pipelines, Data mining, Data cleaning, Data normalization, Data modeling, Data governance, SQL, Python, Java, Bash, Git, Snowflake, Airflow, Kubernetes, Docker, Spark, PySpark, DynamoDB, Kafka, Storm, SparkStreaming","ml, data pipelines, data mining, data cleaning, data normalization, data modeling, data governance, sql, python, java, bash, git, snowflake, airflow, kubernetes, docker, spark, pyspark, dynamodb, kafka, storm, sparkstreaming","airflow, bash, data cleaning, data governance, data mining, data normalization, datamodeling, datapipeline, docker, dynamodb, git, java, kafka, kubernetes, ml, python, snowflake, spark, sparkstreaming, sql, storm"
Data Scientist,Surge Technology Solutions Inc,"Illinois City, IL",https://www.linkedin.com/jobs/view/data-scientist-at-surge-technology-solutions-inc-3711181264,2023-12-17,East Moline,United States,Mid senior,Onsite,"Emp Type
: W2 or 1099........
(No C2C)
Visa: H1B, H4EAD, GCEAD, L2, Green Card, US Citizens (Only USA Applicants)
Workplace Type: Hybrid, Morton IL USA ( Once in a Month)
Experience: 5+ Yrs.
Job Description
The PIC Analytics Team uses quantitative methods such as business simulations, data mining, and advanced statistical techniques to solve problems. The Senior Data Scientist contributes to this mission by leveraging his or her quantitative analysis, data management, modeling and/or data visualization skills as an individual contributor to project teams tasked with solving business problems.
Job Duties/Responsibilities May Include, But Are Not Limited To
Statistical modeling and analysis of dealer parts demand & inventory management
Improvement of existing and development of new inventory optimization techniques
Development, execution, and analysis of inventory management simulations
Analytics-based research in both independent and project team environments
Ad-hoc problem solving, data analysis, data processing, and data visualization
Developing/refining large-scale solutions to fit complex situations in multiple unique environments
Building familiarity with PIC system functionality and dealer service parts management practices
Decision-making on routine, medium-risk issues that affect project teams and internal customers
Effective communication of complex concepts and solutions to stakeholders & internal customers
Typical Task Breakdown
Python coding, transitioning sql or saas code, optimizing code, writing code
Problem solving
Research & analytical thinking
Working on improving/establishing Living Machine Logistics (LML) with team.
Support and running simulations.
Education & Experience Required
Bachelor's degree, preferably in statistics, economics, mathematics, or a similar field with quantitative coursework, and 4-5 years of professional experience utilizing quantitative analysis.
Will accept a Master's degree and 2-3 years of experience, or a PhD in one of the associated fields.
Technical Skills (Required)
Experience with analytical tools like SAS, SQL, R, Python, Tableau, Advanced Excel, Power BI (desired).
Demonstrated experience working in MS Office and industry standard statistics and data visualization packages
3+ years of professional experience utilizing quantitative analysis
Knowledge of basic statistical approaches and/or data management techniques
(Desired)
Project management
Soft Skills (Required)
Teamwork
Initiative
Interpersonal Skills
Effective communication (verbal and with digital media)
Experience interacting directly with dealers or external customers.
Exhibited the ability to work on increasingly more complex assignments
Has demonstrated the ability to effectively communicate concepts using data visualization
Please forward your resume and contact details to vahini_b@surgetechinc.com / sahithi_s@surgetechinc.com or can call on 832-990-6448.
Show more
Show less","Python, SQL, SAS, R, Tableau, Advanced Excel, Power BI, MS Office, Statistical analysis, Data visualization, Data management, Project management, Teamwork, Initiative, Interpersonal Skills, Communication, Complex assignments","python, sql, sas, r, tableau, advanced excel, power bi, ms office, statistical analysis, data visualization, data management, project management, teamwork, initiative, interpersonal skills, communication, complex assignments","advanced excel, communication, complex assignments, data management, initiative, interpersonal skills, ms office, powerbi, project management, python, r, sas, sql, statistical analysis, tableau, teamwork, visualization"
IT DATA AND ANALYTICS DEVELOPER,André,"Davenport, IA",https://www.linkedin.com/jobs/view/it-data-and-analytics-developer-at-andr%C3%A9-3645150177,2023-12-17,East Moline,United States,Mid senior,Hybrid,"Description
Our company is currently in search of an IT Data and Analytics Developer to join our GRP&E Information Services (IS) Team and can be located in Pittsburgh, PA; Davenport, IA; or Alcoa, TN
At our company, we are looking for people who share our values of integrity, inclusion, and diversity, and who demonstrate agility, results commitment, and the capability to grow themselves and others. In return, we offer the opportunity to Grow Together with ongoing opportunities for professional growth provided by a constantly changing environment, working alongside employees who value the people they work with just as much as the work they do.
Job Description
Primary Responsibilities
Analyzes, develops business applications spanning multiple computing environments and multiple GRP locations.
Develops integrations and ETLs between environments to populate OLAP databases.
Designs databases and data structures to facilitate summarization and storage of manufacturing information.
Develops business and manufacturing reports, visualizations utilizing the Microsoft reporting tools.
Interacts with systems and BU business process owners to support the deployment of Manufacturing applications projects.
Supports existing application and integration components in the manufacturing environment
Documents key processes and features of deployed solutions
Interacts with team members and contract personnel to deliver timely outcomes.
Key Objectives
Builds an in depth understanding of the business processes in their area(s) of assignment, and a general understanding of the GRP business.
Takes initiative to identify and implement system and/or process improvements to support business objectives.
Uses a combination of leading-edge information technologies along with existing legacy tools to deliver solutions to meet business needs.
Effectively utilizes project methodology and tools to deliver solutions and communicate status.
Pursues continuous learning consistent with changes in technology and uses this to influence IS direction.
Qualifications
Basic Qualifications
Bachelor's Degree from an accredited institution or extensive practical experience.
Minimum 3 year of experience in Data Science, Business Intelligence, Business Analytics, Data Mining and Integration, and Software Development Life Cycle methodologies.
Minimum 3 year of experience in relational database design with Microsoft SQL Server or Oracle
Employees must be legally authorized to work in the United States. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available for this position
This position is subject to the International Traffic in Arms Regulations (ITAR) which requires U.S. person status. ITAR defines U.S. person as an U.S. Citizen, U.S. Permanent Resident (i.e 'Green Card Holder'), Political Asylee, or Refugee.
Preferred Qualifications
Bachelor's Degree in Information Systems, Computer Science, Software Engineering, or IT
Technical experience in Web Development (.NET), SharePoint Designer
Application development experience with Microsoft SQL Server Applications tools (SSIS, SSAS), Microsoft SharePoint, Microsoft Team Foundation Server, Oracle or SAP ERP applications, Rockwell Factory Talk Software suite and OSIsoft PI applications.
Experience with report development and visualization tools: Microsoft SSRS, Microsoft Power BI and Power Apps
Manufacturing functional experience working on quality, planning & scheduling, shipping, or operations management information systems projects.
Show more
Show less","Data Science, Business Intelligence, Business Analytics, Data Mining, Data Integration, Software Development Life Cycle, Relational Database Design, Microsoft SQL Server, Oracle, Web Development, .NET, SharePoint Designer, Microsoft SQL Server Applications tools (SSIS SSAS), Microsoft SharePoint, Microsoft Team Foundation Server, Oracle ERP applications, SAP ERP applications, Rockwell Factory Talk Software suite, OSIsoft PI applications, Microsoft SSRS, Microsoft Power BI, Microsoft Power Apps, Manufacturing","data science, business intelligence, business analytics, data mining, data integration, software development life cycle, relational database design, microsoft sql server, oracle, web development, net, sharepoint designer, microsoft sql server applications tools ssis ssas, microsoft sharepoint, microsoft team foundation server, oracle erp applications, sap erp applications, rockwell factory talk software suite, osisoft pi applications, microsoft ssrs, microsoft power bi, microsoft power apps, manufacturing","business analytics, business intelligence, data integration, data mining, data science, manufacturing, microsoft power apps, microsoft power bi, microsoft sharepoint, microsoft sql server, microsoft sql server applications tools ssis ssas, microsoft ssrs, microsoft team foundation server, net, oracle, oracle erp applications, osisoft pi applications, relational database design, rockwell factory talk software suite, sap erp applications, sharepoint designer, software development life cycle, web development"
Data Engineer - Scala(U.S. remote),Railroad19,"Boston, MA",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782289992,2023-12-17,Brockton,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, Restful APIs, AWS, EMR, S3, Relational databases, Nonrelational databases, Computer science, Computer engineering","scala 212, spark 24, restful apis, aws, emr, s3, relational databases, nonrelational databases, computer science, computer engineering","aws, computer engineering, computer science, emr, nonrelational databases, relational databases, restful apis, s3, scala 212, spark 24"
Senior Principal Consultant – Data and Analytics,Genesys,"Boston, MA",https://www.linkedin.com/jobs/view/senior-principal-consultant-%E2%80%93-data-and-analytics-at-genesys-3781013803,2023-12-17,Brockton,United States,Mid senior,Remote,"Build something new with a world-class team.
At Genesys, we allow our employees to make their mark by entrusting them to make decisions and do what they’ve been hired to do: their very best. Your potential is waiting; why are you?
About Genesys:
GENESYS® powers more than 25 billion of the world’s best customer experiences each year.
In GENESYS, Innovations team builds solutions that enable our customers to get more value from their Genesys platforms. The Innovations team’s primary job is to build new, innovative applications using AI and analytics, with the goal of helping our customers effectively and efficiently deliver experience as a service (XaaS).
Summary: We are seeking a highly skilled and experienced Contact Center Data and Analytics Consultant to join our team. The ideal candidate will have a strong background in data analysis, data visualization, data engineering, and business intelligence. This is a customer facing role and requires the ability to present to both technical and business executive audiences.
Responsibilities:
Present Genesys data and analytics technology and vision to executives and technical contributors at prospects and customers
Work hands-on with prospects and customers to demonstrate and communicate the value of Genesys data and analytics apps throughout the sales cycle, from demo to proof of concept to design and implementation
Immerse yourself in the ever-evolving industry of contact center and experience management SaaS, maintaining a deep understanding of competitive and complementary technologies and vendors and how to position our analytics apps in relation to them
Collaborate with Product Management, Engineering, and Saloes to continuously improve Genesys data and analytics apps and marketing
Requirements:
8+ years direct experience with Genesys Engage Infomart or GCXI or NICE CX Analytics; strong understanding of Contact Center and Customer Experience data and metrics
Strong proficiency in SQL, data modeling, and data engineering
Familiarity with scripting languages for data engineering
Outstanding presenting skills to both technical and executive audiences, whether impromptu on a whiteboard or using presentations and demos.
Hands-on expertise with SQL and SQL analytics
Ability to connect a customer’s specific business problems to Genesys analytics solutions
Experience with data visualization tools such as Tableau or Power BI
Preferred Qualifications:
Experience with Snowflake
Experience with Elastic (ELK stack)
Experience with data governance and data management best practices.
Experience working with remote engineering teams
Familiar with software development and project management frameworks
Experience in consulting or developing products and solutions with contact center technologies like Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya, etc.
Compensation:
This role has a market-competitive salary with an anticipated base compensation range listed below. Actual salaries will vary depending on a candidate’s experience, qualifications, skills, and location. This role might also be eligible for a commission or performance-based bonus opportunities.
$121,500.00 - $238,400.00
Benefits:
Medical, Dental, and Vision Insurance.
Telehealth coverage
Flexible work schedules and work from home opportunities
Development and career growth opportunities
Open Time Off in addition to 10 paid holidays
401(k) matching program
Adoption Assistance
Fertility treatments
More details about our company benefits can be found at the following link: https://mygenesysbenefits.com
If a Genesys employee referred you, please use the link they sent you to apply.
About Genesys:
Every year, Genesys orchestrates billions of remarkable customer experiences for organizations in more than 100 countries. Through the power of our cloud, digital and AI technologies, organizations can realize Experience as a Service™ our vision for empathetic customer experiences at scale. With Genesys, organizations have the power to deliver proactive, predictive, and hyper personalized experiences to deepen their customer connection across every marketing, sales, and service moment on any channel, while also improving employee productivity and engagement. By transforming back-office technology to a modern revenue velocity engine Genesys enables true intimacy at scale to foster customer trust and loyalty. Visit www.genesys.com.
Reasonable Accommodations:
If you require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you or someone you know may complete the Reasonable Accommodations Form for assistance. Please use the Candidate field in the dropdown menu to ensure a timely response.
This form is designed to assist job seekers who seek reasonable accommodation for the application process. Submissions entered for non-accommodation-related issues, such as following up on an application or submitting a resume, may not receive a response.
Genesys is an equal opportunity employer committed to equity in the workplace. We evaluate qualified applicants without regard to race, color, age, religion, sex, sexual orientation, gender identity or expression, marital status, domestic partner status, national origin, genetics, disability, military and veteran status, and other protected characteristics.
Please note that recruiters will never ask for sensitive personal or financial information during the application phase.
Show more
Show less","Data analysis, Data visualization, Data engineering, Business intelligence, SQL, Data modeling, Scripting languages, Tableau, Power BI, Snowflake, Elastic (ELK stack), Data governance, Data management, Software development frameworks, Project management frameworks, Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya, Genesys Infomart, GCXI, NICE CX Analytics","data analysis, data visualization, data engineering, business intelligence, sql, data modeling, scripting languages, tableau, power bi, snowflake, elastic elk stack, data governance, data management, software development frameworks, project management frameworks, genesys engage, genesys cloud, nice, cisco, avaya, genesys infomart, gcxi, nice cx analytics","avaya, business intelligence, cisco, data engineering, data governance, data management, dataanalytics, datamodeling, elastic elk stack, gcxi, genesys cloud, genesys engage, genesys infomart, nice, nice cx analytics, powerbi, project management frameworks, scripting languages, snowflake, software development frameworks, sql, tableau, visualization"
Data Architect,Arcadis,"Boston, MA",https://www.linkedin.com/jobs/view/data-architect-at-arcadis-3749299457,2023-12-17,Brockton,United States,Mid senior,Remote,"Arcadis is the world's leading company delivering sustainable design, engineering, and consultancy solutions for natural and built assets.
We are more than 36,000 people, in over 70 countries, dedicated to improving quality of life. Everyone has an important role to play. With the power of many curious minds, together we can solve the world’s most complex challenges and deliver more impact together.
Overview:
The Information Systems Data Architect will be part of part of a design and development that includes database developers, data architects, data analysts and process automation team. The mission will be to develop and maintain application integration, Data Warehouse by using on-premises MS SQL Server, Azure SQL Managed instance, Azure SQL Database, SSIS, Azure data factory, Azure storage account, Azure App Service, and other data management tools.
Responsibilities:
Lead Data Warehouse design, ETL architecture design, security profile management, data share design and other data architecture application for internal users and corporate transactional applications. Also act as other billable sector data architect consultant.
Act as application solutions architect, lead and design corporation internal application by using Microsoft C#, Azure Web App Service, Web jobs, Azure function App Service.
Admin and monitor corporate service Azure Cloud resources. Based on architecture design, create resources for corporate service application, monitoring resources status and billing status, manage resources access privilege, Suggest new functions in resources for development team.
Act as scrum master in corporate service development teams. Manage sprint by hosting daily standup meeting and bi-weekly sprint meeting, check development activities fit to agile methodology and continuously helping improve teams’ productivity.
Manage a high dynamic development team, and prioritize multiple tasks by working with various stakeholders, maximize team resources utilization to meet each project deadline within limited resources.
Admin Azure DevOps, manage users, user access level, user privileges, processes and its rules, repos for DW, Applications, ETL, etc. Sprints planning, executing, and reviewing.
Act as company Power BI Admin, manage all kinds of Power BI license (such as pro license, premium capacity, embedded capacity) companywide, research and learn Power BI admin parameter in Power BI portal and implement proper setup. Manage embedded Power BI solutions by providing training and guide to development team. Manage Power BI gateways which are used by Power BI, Power Automate.
Qualifications:
Please ensure to provide a description of the projects you have worked on within your resume.
A University bachelor’s degree in computer science with programming experience, or a degree in a related discipline.
Minimum 3 to 5 years of related Data Warehouse, ETL, application integration design and development experience with Microsoft Azure SQL database, data factory, storage, app service, synapse analytics, data lake, network, Azure resource management.
Possess excellent communication, presentation, and leadership skills and be able to work in a dynamic environment with rapidly changing environment. Demonstrated ability to achieve goals by leading team in an innovative and fast paced environment.
Strong experience with SSIS package and pipeline design, development, deployment, monitoring and scheduling.
Strong experience with MS SQL Server design, development, deployment, performance tuning, monitoring.
Strong experience with Azure DevOps in user, project, repo, wiki management. Can observe and improve current process.
Strong experience with Power BI license and parameter admin, can provide user proper Power BI license and proper guide to start with, can research and setup proper parameters under Power BI admin portal, can manage Power BI gateway by applying patch, observing performance, resolving bottle neck.
Strong experience with application integration between corporate service applications such as HRIS, CRM, ERP, CPM by using C# and Azure App Service.
MS Certified: Azure Solution Architect Expert and/or MS Certified: Azure Data Engineer Associate will be an asset.
Previous working experience with extracting data from an ERP application, HRIS application, CRM application is highly preferred.
Why Arcadis?
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills-based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You’ll do meaningful work, and no matter what role, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark, on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Join Arcadis. Create a Legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We are an equal opportunity and affirmative action employer. Women, minorities, people with disabilities and veterans are strongly encouraged to apply. We are dedicated to a policy of non-discrimination in employment on any basis including race, creed, color, religion, national origin, sex, age, disability, marital status, sexual orientation, gender identity, citizenship status, disability, veteran status, or any other basis prohibited by law.
Arcadis offers benefits for full time and part time positions. These benefits include medical, dental, and vision, EAP, 401K, STD, LTD, AD&D, life insurance, paid parental leave, reward & recognition program and optional benefits including wellbeing benefits, adoption assistance and tuition reimbursement. We offer seven paid holidays and potentially up to two floating holidays per calendar year depending on start date, and 15 days PTO that accrue per year. The salary range for this position is $95000 - $110000.
Show more
Show less","MS SQL Server, Azure SQL Managed instance, Azure SQL Database, SSIS, Azure Data Factory, Azure Storage Account, Azure App Service, C#, Azure Web App Service, Web Jobs, Azure Function App Service, Azure DevOps, Power BI, Azure Synapse Analytics, Azure Data Lake, Azure Network, Azure Resource Management, Azure Solution Architect Expert, Azure Data Engineer Associate, ETL, CRM, ERP, CPM, HRIS","ms sql server, azure sql managed instance, azure sql database, ssis, azure data factory, azure storage account, azure app service, c, azure web app service, web jobs, azure function app service, azure devops, power bi, azure synapse analytics, azure data lake, azure network, azure resource management, azure solution architect expert, azure data engineer associate, etl, crm, erp, cpm, hris","azure app service, azure data engineer associate, azure data factory, azure data lake, azure devops, azure function app service, azure network, azure resource management, azure solution architect expert, azure sql database, azure sql managed instance, azure storage account, azure synapse analytics, azure web app service, c, cpm, crm, erp, etl, hris, ms sql server, powerbi, ssis, web jobs"
Data Analyst,Illinois State Board of Education,"Springfield, Illinois Metropolitan Area",https://www.linkedin.com/jobs/view/data-analyst-at-illinois-state-board-of-education-3763597002,2023-12-17,Petersburg,United States,Mid senior,Onsite,"POSITION TITLE:
Business Intelligence Data Analyst (Principal Consultant #1715)
LOCATION: SPRINGFIELD OR CHICAGO
The Illinois State Board of Education is looking for a new business intelligence data analyst to add to our Data Strategies & Analytics team. Reporting to the Supervisor, the position will provide day-to-day development and support of the Agency's business intelligence, data reporting and data visualization projects. The position duties will include working with agency department managers and staff to recommend, build, and support business intelligence, data reporting and data visualizations solutions. Duties will comprise of report development using Microsoft SQL server and Microsoft Power BI.
Duties and Responsibilities:
Performs business intelligence report development using Microsoft Power BI in support of agency projects and initiatives.
Creates and maintains database queries and analyzes stored procedures to gather report data from agency SQL databases.
Participates on business intelligence development team engaged in a wide range of data analytics and metrics optimization for agency's initiatives and projects.
Maintains a high level of individual technical competency.
Stays current with changes in technology and methodology for business intelligence and data visualizations solutions.
Performs other duties as assigned.
Required Qualifications:
Bachelor's degree in Computer Science, Data Science, Statistics, Management Information Systems, or other technical field, or, extensive coursework in technical or quantitative subject matter. (COPY OF TRANSCRIPTS REQUIRED AT TIME OF APPLICATION)
Experience with SQL.
Experience with descriptive statistics.
Interest in public service and improving education for Illinois students.
Excellent listening, oral, written, and interpersonal communication skills.
Ability to efficiently manage simultaneous projects and re-prioritize workload.
Ability to think logically, interpret problems, present creative solutions with clarity and precision, and deliver results.
Ability to work independently, yet quickly assimilate into a team atmosphere to participate on inter-departmental assignments.
Ability to translate technical and complex information and instructions into common, user-friendly terms.
Capable of quickly adapting to change and various agency demands.
Preferred Qualifications:
Master's degree.
At least one year experience with Microsoft SQL server.
At least one year experience with Microsoft Power BI or Tableau.
At least one year experience with SAS, R, or other statistical software.
At least one year experience with business analysis and customer service.
Proficiency with Microsoft Power BI or Tableau.
Show more
Show less","Business Intelligence, Data Analytics, Data Visualization, Microsoft SQL Server, Microsoft Power BI, SAS, R, Tableau","business intelligence, data analytics, data visualization, microsoft sql server, microsoft power bi, sas, r, tableau","business intelligence, dataanalytics, microsoft power bi, microsoft sql server, r, sas, tableau, visualization"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Chatham, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742672954,2023-12-17,Petersburg,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Data Engineering, ETL, ELT, MLOps, Python, Machine Learning, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Apache Beam, Kafka, Big Data, Data Quality, Data Performance, Communication, Collaboration, CrossFunctional Teams","data engineering, etl, elt, mlops, python, machine learning, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, apache beam, kafka, big data, data quality, data performance, communication, collaboration, crossfunctional teams","apache beam, aws, azure, big data, cicd, collaboration, communication, crossfunctional teams, data engineering, data performance, data quality, devops, elt, etl, gcp, iac, kafka, machine learning, mlops, python, scala, spark"
Data / Quality Analyst (Testing/Documentation)- Three positions,Dice,"Springfield, IL",https://www.linkedin.com/jobs/view/data-quality-analyst-testing-documentation-three-positions-at-dice-3788300043,2023-12-17,Petersburg,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, DATAMAXIS, Inc, is seeking the following. Apply via Dice today!
3+ years data warehouse experience
5+ years of complex SQL experience
3+ years of Teradata or Snowflake experience
3+ years of testing and documentation experience including unit, integration, and system testing
Data / Quality Analyst (Testing/Documentation)- Three positions
Show more
Show less","Data Warehouse, SQL, Teradata, Snowflake, Unit Testing, Integration Testing, System Testing, Data Quality","data warehouse, sql, teradata, snowflake, unit testing, integration testing, system testing, data quality","data quality, datawarehouse, integration testing, snowflake, sql, system testing, teradata, unit testing"
Data Center & Technical Operations Analyst (25984),"Levi, Ray & Shoup, Inc. (LRS)","Springfield, IL",https://www.linkedin.com/jobs/view/data-center-technical-operations-analyst-25984-at-levi-ray-shoup-inc-lrs-3760251422,2023-12-17,Petersburg,United States,Mid senior,Hybrid,"Time to get to work! LRS Consulting Services is seeking an experienced Data Center & Technical Operations Analyst for our client in Central Illinois. This is a 6 month contract to hire or direct placement opportunity for the right person. If youre that person and if you have the IT Infrastructure expertise that we need, submit your resume now!
LRS Consulting Services has been delivering IT excellence for over three decades. Our reputation for quality, flexibility, and strong relationships with our clients keeps them turning to us to meet their IT consulting needs. Our contacts, combined with your technical expertise, equals career possibilities for you. So lets get to work!
Job Responsibilities
Work collaboratively across the IT organization to monitor, report and escalate any critical alerts based on the clinics infrastructure monitoring dashboard.
Support, own and manage all aspects of the data center hardware inventory.
Perform the hands-on racking/stacking, cabling, as well as removal of all datacenters and IDF closet equipment across the organizations 2 datacenters and 50+ remote locations.
Maintain documentation on rack, cabling, DRAC & iLO configurations.
Technical background in a client/server environment, including infrastructure networks, hardware, software, and telecommunications.
Produce daily reporting and shift hand-over communications.
Resolve issues, prepare status reviews, and reports.
Coordinate with vendors for on-site maintenance and repairs.
Participate in monitoring and ensuring backup completion within allotted timeframes to avoid production impact.
Travel to the clinics remote sites & locations to assist with infrastructure equipment replacement & installation as necessary.
Potential to work overnight shifts as scheduled or required.
Participate is rotational on-call monitoring.
Provide after-hour support to end users as required.
Must Have The Following Experience
Working knowledge of network, Windows server, and virtualization technologies such as, DNS, DHCP, IP, Firewall, routing, switching, VMware, etc.
Demonstrate basic understanding of troubleshooting methodologies.
Some experience working with general backup technology including VEEAM.
Familiarity working in an Agile or KANBAN style environment is a bonus.
Must have 2+ years of hands-on enterprise-level data center, support and/or infrastructure experience is preferred.
Paessler certification is a plus.
Knowledge, Skills and Abilities:
Working knowledge of network, Windows server, and virtualization technologies such as, DNS, DHCP, IP, Firewall, routing, switching, VMware, etc.
Demonstrate basic understanding of troubleshooting methodologies.
Some experience working with general backup technology including VEEAM.
Familiarity working in an Agile or KANBAN style environment is a bonus.
Detail-oriented, autonomous, and eager to learn and lead alongside the rapid pace of an evolving organization.
Working Environment
Able to lift heavy equipment/items.
Able to perform all essential job functions, including walking, standing, bending, stooping, climbing, lifting and manual dexterity, with or without reasonable accommodation.
Candidate must be able to effectively communicate in English (written & verbal)
Candidate must have permanent authorization to work in the USA for any employer
Corp to Corp candidates will not be considered
LRS is an equal opportunity employer. Applicants for employment will receive consideration without unlawful discrimination based on race, color, religion, creed, national origin, sex, age, disability, marital status, gender identity, domestic partner status, sexual orientation, genetic information, citizenship status or protected veteran status.
Show more
Show less","Data Center Operations, IT Infrastructure, Monitoring, Reporting, Troubleshooting, Server Administration, Hardware Racking/Stacking, Cabling, Documentation, Backup and Recovery, VEEAM, Agile/KANBAN, Network Technologies (DNS DHCP IP Firewall Routing Switching), Virtualization (VMware)","data center operations, it infrastructure, monitoring, reporting, troubleshooting, server administration, hardware rackingstacking, cabling, documentation, backup and recovery, veeam, agilekanban, network technologies dns dhcp ip firewall routing switching, virtualization vmware","agilekanban, backup and recovery, cabling, data center operations, documentation, hardware rackingstacking, it infrastructure, monitoring, network technologies dns dhcp ip firewall routing switching, reporting, server administration, troubleshooting, veeam, virtualization vmware"
Data Engineer,"Professional Project Partners, Inc. (P3)","Des Moines, IA",https://www.linkedin.com/jobs/view/data-engineer-at-professional-project-partners-inc-p3-3783766847,2023-12-17,Des Moines,United States,Mid senior,Hybrid,"** This is a Full-Time position and our client is not offering sponsorship at this time**
**This is a Hybrid position in Des Moines, Iowa, so if a candidate is not located in Des Moines they will need to relocate in order to be considered**
Passionate about crafting innovative data solutions that seamlessly integrate and transform information? Proficient with relational databases like Azure SQL and adept at wielding data integration tools such as Talend and Azure Data Factory? Are you someone who thrives on providing top-notch service while maintaining a keen eye for detail?
We invite you to join one of our esteemed clients as a Data Engineer! In this role, you'll collaborate closely with analysts, developers, and business users, contributing to the construction of data pipelines and marts essential for robust reporting and analytics support.
Key Responsibilities:
Develop Azure-based solutions to harmonize on-premise and cloud data sources, ensuring alignment with data warehouse reporting needs.
Design and implement data integration and automation processes, emphasizing best practices for performance, security, data integrity, and code reusability.
Provide adept maintenance support for existing ETL processes, swiftly addressing issues through root cause analysis and resolution.
Collaborate across functions to comprehend intricate business challenges, proposing and executing technical solutions effectively.
Translate business requirements into functional technical necessities, offering precise estimates regarding effort and timelines.
Continuously enhance understanding of our client’s audiences, systems, processes, and tools, identifying gaps, quality concerns, and performance improvements.
Knowledge:
Proficiency in Azure cloud services including Azure SQL database, Data Lake Gen 2, and Data Factory.
Demonstrated understanding of data modeling, data warehousing, and ETL strategies.
Strong grasp of relational databases (preferably Azure SQL) and Data Governance principles.
Working knowledge of agile methodology and project management tools.
Skills:
Strong experience in developing data flows and relational data stores in Azure cloud services.
Proficiency in Azure Data Factory, Talend, or similar ETL tools (preferably Azure Data Factory and Talend).
Proven background in data design, workflow, integrations, standards, and complex business rules.
Sharp problem-solving and analytical skills to decipher trends and create solutions for complex challenges.
Excellent interpersonal, written, and verbal communication skills with the ability to translate technical aspects to business stakeholders.
Exceptional organizational abilities to manage multiple projects effectively.
Aptitude for troubleshooting and issue resolution in dynamic environments.
Forward-thinking attitude and a proclivity for purposeful change and continual improvement.
Ability to simplify complex data and concepts into easily understandable language and visualizations for business stakeholders.
Education:
Bachelor's degree in computer science, information technology, data science, business, or related field from an accredited college or equivalent experience.
Experience:
3+ years of experience in ETL/ELT development, including expertise with Azure SQL, Talend, Azure Data Factory, and Data Lake Gen 2, or equivalent technologies.
Show more
Show less","Azure SQL, Data Lake Gen 2, Data Factory, Data modeling, Data warehousing, ETL strategies, Relational databases, Data Governance, Agile methodology, Project management tools, Data flows, Relational data stores, Azure Data Factory, Talend, ETL tools, Data design, Workflow, Integrations, Standards, Complex business rules, Problemsolving, Analytical skills, Interpersonal skills, Written communication skills, Verbal communication skills, Troubleshooting, Issue resolution, Business intelligence, Data visualization","azure sql, data lake gen 2, data factory, data modeling, data warehousing, etl strategies, relational databases, data governance, agile methodology, project management tools, data flows, relational data stores, azure data factory, talend, etl tools, data design, workflow, integrations, standards, complex business rules, problemsolving, analytical skills, interpersonal skills, written communication skills, verbal communication skills, troubleshooting, issue resolution, business intelligence, data visualization","agile methodology, analytical skills, azure data factory, azure sql, business intelligence, complex business rules, data design, data factory, data flows, data governance, data lake gen 2, datamodeling, datawarehouse, etl strategies, etl tools, integrations, interpersonal skills, issue resolution, problemsolving, project management tools, relational data stores, relational databases, standards, talend, troubleshooting, verbal communication skills, visualization, workflow, written communication skills"
Data Engineer,Sammons Financial Group Companies,"Des Moines, IA",https://www.linkedin.com/jobs/view/data-engineer-at-sammons-financial-group-companies-3778230866,2023-12-17,Des Moines,United States,Mid senior,Hybrid,"Title:
Data Engineer
Locations:
Des Moines, IA, Chicago, IL, Sioux Falls, SD, Remote
Duties:
Develops and maintains complex automated ETL pipeline architecture using assigned tools and programming languages.
Develops, tests, and deploys code using internal software development toolsets, including the code for deploying infrastructure and solutions for secure data storage, ETL pipelines, data catalogs and data queries.
Excellent ability to build processes supporting data transformation, data structures, metadata, dependency and workload management
Responsible for implementation of ETL projects from requirements to Implementation.
Strong ability to manipulate, process and extract value from large disconnected datasets
Develops complex data objects for business analytics using data modeling techniques.
Develops and optimizes complex data models using best practices for data definition language (DDL), physical and logical tables, data partitioning, compression and parallelization.
Partners with users and stakeholders to understand business requirements and developing new data solutions to meet the business requirements.
Extracts data based on business requirements for data users in the most efficient and effective manner to ensure their data needs are met.
Researches, performs analysis, and proposes effective solutions related to system developments and enhancements.
Analyzes and reviews potential adjustments or modifications for impacts on other programs.
Develops and maintains documentation for ELT processes including data staging and other data integrations.
Provide inputs to continuously improve ETL process discipline and resulting output quality.
Tuning of ETL performance, including query writing and leveraging cloud technology for compute scaling, optimization, and parallelization.
Collaborate with the Architects on POC of new tools and technologies.
Provides guidance to ETL developers.
Maintains legacy data solutions to ensure functionality until replaced with new technology
Excellent verbal and written communication skills
Strong attention to detail, organizational and multi-tasking skills required with the ability to adapt to changing priorities
Technical Experience:
Experience with Extract, Transform and Load and ELT development is required.
Strong SQL knowledge and experience working with relational databases.
Good knowledge of Azure cloud services such as Azure Data Lake, Synapse, and Data Factory.
Deep working knowledge and experience using Snowflake.
Snowflake Snowpark including Python, Scala or Java for development.
Data Vault modeling experience a huge plus.
5+ years of experience in designing and delivering Data Warehouses and reporting platforms.
Strong experience with Database technologies such as SQL server and Snowflake.
Experience with ETL tools like Informatica Power Center or IICS/IDMC.
Data modeling experience with relational and dimensional modeling is required.
Understanding of Metadata, Data Governance, Data Quality and Data security is required.
Should have experience with cloud-based Data Warehouse. (Snowflake experience highly preferred)
Exposure to BI tools like Power BI and Tableau good to have.
Experience with Cloud Analytical platforms and tools good to have along with any exposure to Python.
Additional Experience:
Excellent verbal and written communication skills and the ability to communicate with business partners and other IT staff.
Experience of working in Agile Environment with strong Scrum framework knowledge.
Understanding of Life Insurance or Annuities business is good to have.
Qualifications:
College Degree in the field of computer science, information science, management information systems preferred.
Minimum 5 years' IT development experience or equivalent preferred.
Problem solving skills sufficient to perform research and recommend a proposed solution to problems.
Able to independently complete tasks entirely and meet established deadlines.
Knowledge of computer programming languages as required for the system.
$78,672 - $163,899 - Range includes data points from multiple labor markets. Specific range is dependent on the labor market where the incumbent will be hired to perform the position. Starting salary is dependent on candidate qualifications and experience. For a narrower salary range specific to your labor market, please inquire
Sammons Financial Group offers incentive programs for defined goals subject to eligibility and performance. Monetary rewards are based on individual and/or overall company performance
Our competitive benefit package includes: Health, Dental, Vision, Company Paid Retirement, PTO and Holiday Pay
Criminal background check required.
Show more
Show less","ETL, Python, Scala, Java, SQL, Snowflake, Azure, Data Vault modeling, Database technologies, Informatica Power Center, IICS/IDMC, Data modeling, Metadata, Data Governance, Data Quality, Data security, Cloudbased Data Warehouse, Power BI, Tableau, Agile, Scrum","etl, python, scala, java, sql, snowflake, azure, data vault modeling, database technologies, informatica power center, iicsidmc, data modeling, metadata, data governance, data quality, data security, cloudbased data warehouse, power bi, tableau, agile, scrum","agile, azure, cloudbased data warehouse, data governance, data quality, data security, data vault modeling, database technologies, datamodeling, etl, iicsidmc, informatica power center, java, metadata, powerbi, python, scala, scrum, snowflake, sql, tableau"
Data Analyst,"Professional Project Partners, Inc. (P3)","West Des Moines, IA",https://www.linkedin.com/jobs/view/data-analyst-at-professional-project-partners-inc-p3-3778706004,2023-12-17,Des Moines,United States,Mid senior,Hybrid,"***This role is NOT open for C2C applicants***
We're actively seeking a skilled Data Analyst to join a top client’s team in West Des Moines, Iowa. This position is hybrid, so potential candidates will need reside in the Des Moines Metro.
As a Data Analyst, you'll delve into the heart of our data, uncovering valuable insights and driving impactful decisions.
Your responsibilities will involve:
Interpreting and analyzing data through proficient query execution
Crafting comprehensive Data Design documents with embedded business logic
Proficiency in Snowflake data platform, including experience in data modeling, querying, and optimizing performance within Snowflake, is highly desirable.
Identifying and resolving performance inefficiencies within intricate SQL Queries
Refining and refining data sets by detecting and rectifying code issues based on performance indicators
Proficiency in working with Data Warehouses and adeptness in understanding their design, along with weighing their advantages and limitations
Executing business and IT data requirements through innovative data strategies across various platforms (both relational and dimensional)
Preferably possessing experience in Life Insurance, Annuities, or Actuarial processes
We're looking for candidates who embody:
Exceptional expertise in crafting complex SQL queries
Prior experience in Data Profiling Concepts
Exposure to Data Projects encompassing Data Design and Source-to-Target documentation, including the definition of transformation rules
Proficiency in Snowflake data platform, encompassing hands-on experience in data modeling, SQL querying, optimizing performance, and overall administration within Snowflake's ecosystem.
Knowledge of metadata management, data modeling, and associated tools
Strong verbal and written communication skills
Familiarity with Informatica, Microsoft SQL Server, Snowflake, and PowerBI
If you're passionate about unraveling the stories hidden within data, contributing to transformative projects, and shaping innovative data strategies, we encourage you to apply..
***This role is NOT open for C2C applicants***
Show more
Show less","Data Analysis, Data Warehousing, SQL, Snowflake, Data Profiling, Data Design, SourcetoTarget Documentation, Transformation Rules, Metadata Management, Informatica, Microsoft SQL Server, PowerBI","data analysis, data warehousing, sql, snowflake, data profiling, data design, sourcetotarget documentation, transformation rules, metadata management, informatica, microsoft sql server, powerbi","data design, data profiling, dataanalytics, datawarehouse, informatica, metadata management, microsoft sql server, powerbi, snowflake, sourcetotarget documentation, sql, transformation rules"
Senior Marketing Data Analyst (E-Commerce),Harnham,"Hertfordshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-marketing-data-analyst-e-commerce-at-harnham-3784642699,2023-12-17,Chelmsford, United Kingdom,Mid senior,Onsite,"Senior Marketing Data Analyst (E-Commerce)
Up to £75,000
Hybrid - London
A great opportunity to join a global tech brand providing end-to-end solutions for online grocery as a Senior Data Analyst
THE COMPANY
The business is a global tech brand dedicated to putting the world's retailers online using cloud, robotics, AI, IoT and more. They focus on developing the innovative software and hardware systems that power the retailers website, which is the world's largest online-only grocery retailer as well as their platform too.
Joining as a Senior Data Analyst, you'll specialise in extracting and interpreting Ecommerce data to uncover growth opportunities for the business. With your insight being used to influence commercial decisions, enhancing the shopping experience for hundreds of thousands of global customers.
THE ROLE AND RESPONSIBILITIES
Providing valuable insights and recommendations through querying clickstream data, developing visualisation dashboards, conducting ad-hoc analysis and conducting business reviews
Identify trends and opportunities that drive business growth
Ensure the effective utilisation of e-commerce data for marketing and e-commerce reporting
Identify and close gap in data feeds and e-commerce data layer
Lead on understanding trends and patters in the performance of the business and then presenting findings back to partners
Create data to represent the business objectives and interest to other functions
YOUR SKILLS AND EXPERIENCE
Strong experience in SQL and Data Visualisation - Python and GA desirable
Proven track record in the marketing analytics space
Strong stakeholder management experience
THE BENEFITS
Up to £75,000 + bonuses
Hybrid
Hertfordshire
HOW TO APPLY
If interested in the role please send your CV to Jordan Victor via the Apply Link below
Show more
Show less","SQL, Data Visualization, Python, Google Analytics, Marketing Analytics, Stakeholder Management, Ecommerce, Data Analysis, Clickstream Data, Dashboards, Adhoc Analysis, Business Reviews, Data Feeds, Data Layer, Performance Analysis","sql, data visualization, python, google analytics, marketing analytics, stakeholder management, ecommerce, data analysis, clickstream data, dashboards, adhoc analysis, business reviews, data feeds, data layer, performance analysis","adhoc analysis, business reviews, clickstream data, dashboard, data feeds, data layer, dataanalytics, ecommerce, google analytics, marketing analytics, performance analysis, python, sql, stakeholder management, visualization"
Lead Software Engineer - Big Data,Lever Middleware Test Company 2,"Chicago, IL",https://www.linkedin.com/jobs/view/lead-software-engineer-big-data-at-lever-middleware-test-company-2-3787336465,2023-12-17,East Chicago,United States,Mid senior,Onsite,"About Hopper
At Hopper, we’re on a mission to make booking travel faster, easier, and more transparent. We are leveraging the power that comes from combining massive amounts of data and machine learning to build the world’s fastest-growing travel app -- one that enables our customers to save money and travel more. With over $235M CAD in funding from leading investors in both Canada and the US, Hopper is primed to continue its path toward becoming the go-to way to book travel as the world continues its shift to mobile.
Recognized as the fastest-growing travel app by Forbes and one of the world’s most innovative companies by Fast Company two years in a row, Hopper has been downloaded over 35 million times and has helped travelers plan over 100 million trips and counting. The app has received high praise in the form of mobile accolades such as the Google Play Award for Standout Startup of 2016 and Apple’s App Store Best of 2015.
Take off with us!
THE ROLE
A stellar opportunity to have massive impact: join the rapidly growing team at a company continually redefining how people travel. As a Lead Software Engineer, you will be directly responsible for many of the innovative features we’ll be working on, whether collaborating with our outstanding design and product team or helping our data science group work on the bleeding-edge to personalize travel for our ever expanding population of members.
In This Role You Will
Align and drive team members towards an inspiring vision, yet stage work to deliver value incrementally
Build highly scalable software to ensure data quality and data reliability across our microservices architecture, leveraging Scala and technologies like HBase, Spark, etc.
As a hands-on lead, you will work closely with our team of software engineers and product managers to deliver rapid value.
Collaborate directly with the company’s technology leadership on insuring Hopper’s success
Mentor and coach technical team members
a Perfect Candidate Has
Experience with designing and building large scale data pipelines and data warehouses.
Thrived in complex microservices ecosystem and written robust and well-performing services.
The ability to bring leading industry techniques and technologies to the table.
Excelled in cross-functional teams, working fluidly with Product Managers, Data Scientists, Mobile Engineers, Backend Engineers, and other highly skilled specialists
Experience leading an Agile development team
Experience mentoring and leading people at different stages in their career
About The Team
Our engineering team cares deeply about the growth and success of Hopper, and tries to deliver the best possible experience we can to our more than 35 million users. Work on an award winning product that our customers love and continue to find exceptionally useful in a market and space that is continually growing every year.
Benefits
Well-funded and proven startup with large ambitions, competitive salary and stock options
Dynamic and entrepreneurial team where pushing limits is everyday business
100% employer paid medical, dental, vision, disability and life insurance plans
Access to a 401k (Boston) or Retirement Savings Plan (Montreal)
Easy commute with a paid-for public transportation or parking pass
IATA Travel Agent Card for discounts in the travel industry
Fully stocked kitchen with: coffee/tea, beer, bagels and snacks (both healthy and not-healthy)
In Cambridge, work in a historic factory building near Kendall Square; in Montreal, work in an artist’s loft in the Mile End
Team lunches, off-site activities and much more!
Show more
Show less","Agile, Big data, Data mining, Data pipelines, Data warehouses, HBase, Machine learning, Microservices, Scala, Spark","agile, big data, data mining, data pipelines, data warehouses, hbase, machine learning, microservices, scala, spark","agile, big data, data mining, data warehouses, datapipeline, hbase, machine learning, microservices, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742672917,2023-12-17,East Chicago,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, ETL, ELT, MLOPS, Spark, Scala, PySpark, AWS, Azure, GCP, CI/CD, IaC, Apache Beam, Kafka, DevOps, NFT marketplaces, VR imaging, AI/ML, Data Science, Machine Learning","python, etl, elt, mlops, spark, scala, pyspark, aws, azure, gcp, cicd, iac, apache beam, kafka, devops, nft marketplaces, vr imaging, aiml, data science, machine learning","aiml, apache beam, aws, azure, cicd, data science, devops, elt, etl, gcp, iac, kafka, machine learning, mlops, nft marketplaces, python, scala, spark, vr imaging"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Burbank, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742674875,2023-12-17,East Chicago,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","ETL, ELT, Python, MLOps, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC (Infrastructure as Code), Apache Beam, Kafka, Machine Learning, Crossfunctional teams, NFT Marketplace, VR Imaging, AI/ML Projects, Case Study, Portfolio, Competitions","etl, elt, python, mlops, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac infrastructure as code, apache beam, kafka, machine learning, crossfunctional teams, nft marketplace, vr imaging, aiml projects, case study, portfolio, competitions","aiml projects, apache beam, aws, azure, case study, cicd, competitions, crossfunctional teams, devops, elt, etl, gcp, iac infrastructure as code, kafka, machine learning, mlops, nft marketplace, portfolio, python, scala, spark, vr imaging"
Staff Data Engineer,Recruiting from Scratch,"Chicago, IL",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744391631,2023-12-17,East Chicago,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Kafka, Storm, Spark Streaming, Dimensional Data Modeling, ETL, Data Warehouses, Data Management, Data Classification, Data Retention, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, spark streaming, dimensional data modeling, etl, data warehouses, data management, data classification, data retention, tdd, pair programming, continuous integration, automated testing, deployment","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, spark streaming, sql, storm, tdd"
Health Data Analyst,Insight,"Chicago, IL",https://www.linkedin.com/jobs/view/health-data-analyst-at-insight-3747792158,2023-12-17,East Chicago,United States,Mid senior,Onsite,"We Are Insight
At Insight Hospital and Medical Center Chicago, we believe there is a better way to provide quality healthcare while achieving health equity. Our Chicago location looks forward to working closely with our neighbors and residents, to build a full-service community hospital in the Bronzeville area of Chicago; creating a comprehensive plan to increase services and meet community needs. With a growing team that is dedicated to delivering world-class service to everyone we meet, it is our mission to deliver the most compassionate, loving, expert, and impactful care in the world to our patients. Be a part of the Insight Chicago team that provides PATIENT CARE SECOND TO NONE! If you would like to be a part of our future team, please apply now!
These duties are to be performed in a highly confidential manner, following the mission, values, and behaviors of Insight Hospital and Medical Center. Employees are further expected to provide a high quality of care, service, and kindness toward all patients, staff, physicians, volunteers, and guests.
General Summary
: Reporting to the Director of Quality, Risk and Patient Safety, the Data Analyst is responsible for collecting, organizing, summarizing, presenting and analyzing data so that valid conclusions can be drawn and reasonable decisions made on the basis of this analysis. The Data Analyst also assists others within the organization in the development of databases and quality assessment tools and acts as liaison to the external quality organizations in the transmission of data for public reporting.
Description
Duties and Responsibilities:
Acts as the liaison with departments and the Quality and Safety Committee to ensure that reports flow in the established pattern to the Medical Executive Committee and the Executive Board. This may be accomplished through assisting leadership with accessing accurate data, determining the most appropriate presentation styles, compiling reports, and coordinating meetings.
Presents data and creates various reports, charts and graphs that promote analysis, discussion, and identification of system improvements.
Accesses various databases and produces standard and ad-hoc reports for internal trending and analysis.
Maintains expert knowledge of content and structure of the databases as well as the identification of newer solutions.
Promotes collaborative relationships with Physicians, nursing and leadership.
Assists these individuals with the production of key reports.
Serves as an in-house expert on the development and use of systems/databases.
Provides computer application software support for Quality Management team.
Accurately enters data for the various public-reporting initiatives, especially the CORE measures.
Coordinates the public reporting activities by ensuring that cases are accurately selected, QM staff are provided with due dates for abstraction, and data is entered timely.
Additional Duties And Responsibilities
Contributes to a healthy work environment by using conflict resolution skills, and participating in team problem-solving efforts.
Assumes accountability for creating a culture of safety within the department.
Collaborates with peers and staff in monitoring the environment and performance in the organization to identify potential safety issues and to address them proactively.
Maintains awareness of hospital changes by attending scheduled meetings and hospital town hall meetings.
Carries out other duties as assigned.
Job Specifications
Required Knowledge, Skills and Abilities:
Knowledge
A minimum of three to four years of experience in a data analyst role
Bachelor’s degree in a healthcare related field, computer science or industrial engineering, preferred.
Skills
Demonstrated ability with database systems.
Midas Experience required, Statit experience preferred.
Proficient in the Microsoft suite of products, Word, Excel, PowerPoint; working knowledge of project management and flowchart software, and graphics packages also desired.
Excellent verbal and written communications skills.
Abilities
Demonstrates ability in the establishment of effective work relationships with co-workers and other members of the health care team to achieve results
Excellent problem solving and inductive reasoning skills that manifest themselves in creative and forward thinking solutions in data management.
Commitment to teamwork, service excellence, and interdisciplinary collaboration.
Excellent ability in data analysis and reporting.
Insight Employees are required to be vaccinated for COVID-19 as a condition of employment, subject to accommodation for medical or sincerely held religious beliefs.
Insight is an equal opportunity employer and values workplace diversity!
Show more
Show less","Data Analysis, Data Presentation, Data Summarization, Data Visualization, Database Management, Microsoft Office Suite, Project Management Software, Flowchart Software, Graphics Packages, Verbal Communication, Written Communication, Problem Solving, Inductive Reasoning, Teamwork, Collaboration, Data Reporting","data analysis, data presentation, data summarization, data visualization, database management, microsoft office suite, project management software, flowchart software, graphics packages, verbal communication, written communication, problem solving, inductive reasoning, teamwork, collaboration, data reporting","collaboration, data presentation, data reporting, data summarization, dataanalytics, database management, flowchart software, graphics packages, inductive reasoning, microsoft office suite, problem solving, project management software, teamwork, verbal communication, visualization, written communication"
Senior Data Engineer,Dice,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-dice-3784909641,2023-12-17,East Chicago,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Request Technology, LLC, is seeking the following. Apply via Dice today!
NO SPONSORSHIP
Senior Data Engineer
Salary - 155K - 175K + Bonus
You will drive the data integration solutions in terms of design, build and deployment, DevOps with best-in-class data models, data quality and data architecture standards
Possesses strong data capabilities in terms of data analysis, data models, and hands on expertise in crafting and deploying data pipes using Azure data platform and tools, as well as enterprise ETL tool Talend, leveraging its DQ, DI and Data Catalogue features.
Qualifications & Requirements
Bachelor’s degree in data, computer science or relevant discipline.
8+ years of experience in ETL, ELT and data engineering
At least 3+ years of working experience on Azure data platforms
Experience working in agile delivery, Jira usage and other agile delivery best practices
Data architecture, Data Modeling, and data visualization experience is a plus
Ability to interact with business, other teams to create data mapping documents, ETL architecture/design artifacts, performance improvements, improve delivery & operational excellence.
Technologies/Software
8+ years of end-to-end implementation experience of deploying enterprise data warehouse, data mart and data lake solutions
3+ years of working experience with Azure data solutions including but not limited to ADLS, Data Bricks, ADF, Synapse etc
Azure ADLS/Databricks administration experience
5+ years of Implementation and maintenance experience with Talend DI, DQ capabilities
Demonstrable understanding of Data Governance, and enabling technical tools and technologies
SELLING POINTS: Data analysis data models azure data platform etl tool talend end-to-end implementation deploying enterprise data warehouse data mart data lake solutions azure data solutions ADLS data bricks ADF synapse azure ADLS data bricks administration talend DIDQ understand of data governance.
Show more
Show less","Data Analysis, Data Models, Azure Data Platform, ETL Tool, Talend, EndtoEnd Implementation, Deploying Enterprise Data Warehouse, Data Mart, Data Lake Solutions, Azure Data Solutions, ADLS, Data Bricks, ADF, Synapse, Azure ADLS/Data Bricks Administration, Talend DI/DQ Capabilities, Data Governance, Jira, Agile Delivery, Data Architecture, Data Modelling, Data Visualization, SQL, Python, DevOps","data analysis, data models, azure data platform, etl tool, talend, endtoend implementation, deploying enterprise data warehouse, data mart, data lake solutions, azure data solutions, adls, data bricks, adf, synapse, azure adlsdata bricks administration, talend didq capabilities, data governance, jira, agile delivery, data architecture, data modelling, data visualization, sql, python, devops","adf, adls, agile delivery, azure adlsdata bricks administration, azure data platform, azure data solutions, data architecture, data bricks, data governance, data lake solutions, data mart, data modelling, data models, dataanalytics, deploying enterprise data warehouse, devops, endtoend implementation, etl tool, jira, python, sql, synapse, talend, talend didq capabilities, visualization"
Health Data Analyst,Insight Hospital and Medical Center,"Chicago, IL",https://www.linkedin.com/jobs/view/health-data-analyst-at-insight-hospital-and-medical-center-3747770453,2023-12-17,East Chicago,United States,Mid senior,Onsite,"WE ARE INSIGHT:
At Insight Hospital and Medical Center Chicago, we believe there is a better way to provide quality healthcare while achieving health equity. Our Chicago location looks forward to working closely with our neighbors and residents, to build a full-service community hospital in the Bronzeville area of Chicago; creating a comprehensive plan to increase services and meet community needs. With a growing team that is dedicated to delivering world-class service to everyone we meet, it is our mission to deliver the most compassionate, loving, expert, and impactful care in the world to our patients. Be a part of the Insight Chicago team that provides PATIENT CARE SECOND TO NONE! If you would like to be a part of our future team, please apply now!
These duties are to be performed in a highly confidential manner, following the mission, values, and behaviors of Insight Hospital and Medical Center. Employees are further expected to provide a high quality of care, service, and kindness toward all patients, staff, physicians, volunteers, and guests.
General Summary
: Reporting to the Director of Quality, Risk and Patient Safety, the Data Analyst is responsible for collecting, organizing, summarizing, presenting and analyzing data so that valid conclusions can be drawn and reasonable decisions made on the basis of this analysis. The Data Analyst also assists others within the organization in the development of databases and quality assessment tools and acts as liaison to the external quality organizations in the transmission of data for public reporting.
DESCRIPTION:
Duties and Responsibilities:
Acts as the liaison with departments and the Quality and Safety Committee to ensure that reports flow in the established pattern to the Medical Executive Committee and the Executive Board. This may be accomplished through assisting leadership with accessing accurate data, determining the most appropriate presentation styles, compiling reports, and coordinating meetings.
Presents data and creates various reports, charts and graphs that promote analysis, discussion, and identification of system improvements.
Accesses various databases and produces standard and ad-hoc reports for internal trending and analysis.
Maintains expert knowledge of content and structure of the databases as well as the identification of newer solutions.
Promotes collaborative relationships with Physicians, nursing and leadership.
Assists these individuals with the production of key reports.
Serves as an in-house expert on the development and use of systems/databases.
Provides computer application software support for Quality Management team.
Accurately enters data for the various public-reporting initiatives, especially the CORE measures.
Coordinates the public reporting activities by ensuring that cases are accurately selected, QM staff are provided with due dates for abstraction, and data is entered timely.
Additional Duties and Responsibilities:
Contributes to a healthy work environment by using conflict resolution skills, and participating in team problem-solving efforts.
Assumes accountability for creating a culture of safety within the department.
Collaborates with peers and staff in monitoring the environment and performance in the organization to identify potential safety issues and to address them proactively.
Maintains awareness of hospital changes by attending scheduled meetings and hospital town hall meetings.
Carries out other duties as assigned.
JOB SPECIFICATIONS:
Required Knowledge, Skills and Abilities:
Knowledge:
A minimum of three to four years of experience in a data analyst role
Bachelor’s degree in a healthcare related field, computer science or industrial engineering, preferred.
Skills:
Demonstrated ability with database systems.
Midas Experience required, Statit experience preferred.
Proficient in the Microsoft suite of products, Word, Excel, PowerPoint; working knowledge of project management and flowchart software, and graphics packages also desired.
Excellent verbal and written communications skills.
Abilities:
Demonstrates ability in the establishment of effective work relationships with co-workers and other members of the health care team to achieve results
Excellent problem solving and inductive reasoning skills that manifest themselves in creative and forward thinking solutions in data management.
Commitment to teamwork, service excellence, and interdisciplinary collaboration.
Excellent ability in data analysis and reporting.
Insight Employees are required to be vaccinated for COVID-19 as a condition of employment, subject to accommodation for medical or sincerely held religious beliefs.
Insight is an equal opportunity employer and values workplace diversity!
Show more
Show less","Microsoft Word, Microsoft PowerPoint, Microsoft Office Suite, Midas, Statit, Project Management Software, Flowchart Software, Graphics Packages, Data Analysis, Data Reporting, Verbal Communication, Written Communication, Conflict Resolution, Teamwork, ProblemSolving, Inductive Reasoning, Data Management","microsoft word, microsoft powerpoint, microsoft office suite, midas, statit, project management software, flowchart software, graphics packages, data analysis, data reporting, verbal communication, written communication, conflict resolution, teamwork, problemsolving, inductive reasoning, data management","conflict resolution, data management, data reporting, dataanalytics, flowchart software, graphics packages, inductive reasoning, microsoft office suite, microsoft powerpoint, microsoft word, midas, problemsolving, project management software, statit, teamwork, verbal communication, written communication"
ground. Our data platform spans from the SDKs that our teams use to emit and consume data in front- and back-end applications to the integration with various third-party systems, including everything in between (data ingestion, event brokers, data persistence, etc.). To be successful in your role you’ll need to be proficient in both. Having worked with distributed, event-driven systems is a plus.,,,,,,,
Experience operating at scale. You have worked on data systems that power a product that serves hundreds of millions of active users.,,,,,,,,,,,,
Linktree is committed to providing a competitive compensation package. Our cash compensation amount for this role is targeted at $175,000-$225,000 in the San Francisco Bay or Los Angeles area. Final offer amounts are determined by multiple factors including candidate expertise, the scope of role and level, and may vary from the amounts listed above.,,,,,,,,
P.S. If you don’t tick every box in this ad, please don’t rule yourself out. We take pride in inclusion and hiring incredible human beings with great potential over ticking boxes – so if this role resonates with you, hit that apply button!,,,,,,,,,,
Where And How We Work,,,,,,,,,,,,
We are a global and diverse group offering a truly flexible and family friendly work environment. Kids, pets," and the occasional delivery person are all actively encouraged to appear on our Zoom screens. All of us at Linktree work either fully remote or a hybrid """"remote"," but in-office sometimes"""" approach.",,,,,,,,,
We currently have offices in Melbourne, Sydney and LA, but our team is spread across Australia, United States, and New Zealand. As our team approaches 200 people, our company will be 10x the size we were in 2020.,,,,,,,
"We offer autonomy and flexibility in how you structure your days and weeks. There will be the need for some collaboration outside of a """"normal"""" 9-5 being a global company", but we aim to work asynchronously where possible.,,,,,,,,,,,
Our Culture And Benefits,,,,,,,,,,,,
Linktree's company culture and values are based around collaboration, diversity, inclusion, and flexibility. Those are all nice words but to give you some more specific examples:,,,,,,,,,
We recognize that our team are individually unique and have designed our benefits with this in mind. Each person has an annual allowance ($7,000 AUD / $4,900 USD) to use on things like (but not limited to) fitness memberships, development courses, childcare, travel, charitable donations, pet insurance, home office set up - the choice is yours!,,,,
We provide top-flight medical, dental, vision, disability and life insurance - we cover 100% of your monthly premiums (and 80% for your dependents).,,,,,,,,,
Employee Stock Option Program - we want each and every employee to share in the company’s success as we go further together.,,,,,,,,,,,,
To learn more about our benefits, including our parental leave program, volunteering leave, DE&I initiatives, and more, click here!,,,,,,,
Our Story,,,,,,,,,,,,
"We're on a mission to empower anyone to curate and grow their digital universe. We created the """"link in bio"""" category and are trusted by some of the world's biggest brands and celebrities including TikTok", The UN Environmental Program, The White House, F1, Manchester United, Selena Gomez, Alicia Keys, and Dwayne “The Rock” Johnson. With a flexible work environment and a team spread across multiple time zones, we offer autonomy and flexibility. Join us in empowering people to control their online presence!,,,,
At Linktree, we celebrate and support everyone’s perspective and background, and we’re proud to be an equal opportunity workplace. We aim to foster a diverse and inclusive environment where all team members have a sense of belonging, because we believe in,,,,,,,,,
going further together.,,,,,,,,,,,,
Linktree welcomes all people regardless of sex, gender identity, race, ethnicity, disability, pregnancy, age, or other lived experience. If you require accommodations to fully participate in our opportunities, please don't hesitate to reach us at,,,,
recruiting@linktr.ee,,,,,,,,,,,,
– your needs are important to us.,,,,,,,,,,,,
Show more,,,,,,,,,,,,
"Show less""","Data Platform Development, Software Engineering, Data Engineering, Analytics Engineering, Data Driven Systems, Data analysis, Data Discovery, Data Governance, Distributed Systems, Eventdriven Systems, Data warehousing, Software Development Kits (SDKs), Data Integration, Instrumentation, Event ingestion, Event brokers, Experimentation platforms, Product analytics, Marketing tooling, Data pipelines, Cloud Providers, Scalability, AWS, GCP, Azure","data platform development, software engineering, data engineering, analytics engineering, data driven systems, data analysis, data discovery, data governance, distributed systems, eventdriven systems, data warehousing, software development kits sdks, data integration, instrumentation, event ingestion, event brokers, experimentation platforms, product analytics, marketing tooling, data pipelines, cloud providers, scalability, aws, gcp, azure","analytics engineering, aws, azure, cloud providers, data discovery, data driven systems, data engineering, data governance, data integration, data platform development, dataanalytics, datapipeline, datawarehouse, distributed systems, event brokers, event ingestion, eventdriven systems, experimentation platforms, gcp, instrumentation, marketing tooling, product analytics, scalability, software development kits sdks, software engineering",,,,,,,,,
Senior Data Engineer,Michael Page,"Dandenong, Victoria, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-michael-page-3687412346,2023-12-17,Victoria, Australia,Mid senior,Onsite,"Global organisation with Hybrid working|Professional growth and development
About Our Client
Our client is a global Wholesaler
Job Description
The successful applicant will be responsible for:
Collaborate closely with cross-functional teams to understand data requirements and deliver tailored data solutions.
Develop and maintain scalable ETL processes, data pipelines, and data workflows.
Implement data quality and validation procedures to ensure accurate and reliable data.
Work on data integration projects, connecting various data sources to create a unified data ecosystem.
Optimise and tune data pipelines for performance, scalability, and reliability.
Contribute to the design and implementation of our master data management strategy.
Stay up-to-date with industry trends and emerging technologies, suggesting improvements to current processes.
The Successful Applicant
The Successful Applicant Will Ideally Possess The Following
Bachelor's or higher degree in Computer Science, Data Science, Engineering, or related field.
Proven experience as a Data Engineer, with a track record of successfully delivering complex data projects.
Strong proficiency in data integration, ETL processes, and data pipeline development.
Experience with relevant tools and technologies such as SQL, Python, Spark, Kafka, Airflow, etc.
Familiarity with master data management concepts and practices is a plus.
Excellent problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Strong communication skills to interact with technical and non-technical stakeholders.
What's on Offer
Opportunity to work on greenfield projects and shape data solutions from inception.
Competitive salary
Flexible work arrangements
Collaborative and inclusive work culture that values innovation and growth.
Access to cutting-edge technologies and a dynamic learning environment.
Contact: Kristian Childs
Quote job ref: JN-082023-6143991
Show more
Show less","Data Engineering, ETL, Data Pipelines, Data Quality, Data Integration, Master Data Management, SQL, Python, Spark, Kafka, Airflow, Data Science, Computer Science, Engineering","data engineering, etl, data pipelines, data quality, data integration, master data management, sql, python, spark, kafka, airflow, data science, computer science, engineering","airflow, computer science, data engineering, data integration, data quality, data science, datapipeline, engineering, etl, kafka, master data management, python, spark, sql"
Data Analyst,Luxury Escapes,"Southbank, Victoria, Australia",https://au.linkedin.com/jobs/view/data-analyst-at-luxury-escapes-3754787525,2023-12-17,Victoria, Australia,Mid senior,Onsite,"Luxury Escapes is one of the world's fastest growing travel businesses, reaching up to 6 million subscribers each month with exclusive limited time offers at 'insider' prices.
Here at Luxury Escapes, our mission is to delight our customers with incredible deals for the world's best holidays.
We are a collective of forward-thinking people, driven to challenge the travel industry and ourselves to bring ever better travel experiences to our customer community.
Our award winning employee experience is king here having been recognised as ""One of the Best Places to Work' by the AFR in both 2021 and 2023, with weekly events, monthly recognition programs and significant professional development opportunities we do what we can to make you look forward to waking up every day!
We are now seeking a highly skilled and analytical
Data Analyst
who will be support our marketing activities.
Playing a crucial role in driving data-driven decision-making, measuring marketing campaign effectiveness, and optimizing marketing strategies; your expertise in data analysis and reporting will be instrumental in supporting our performance marketing initiatives, enabling Luxury Escapes to maximize ROI and achieve business goals.
Please note, it is essential that you have extensive commercial SQL experience to be successful in this role.
You will be responsible for:
Data analysis in relation customers and performance marketing channels including conversion data, customer journeys, funnel analysis, and attribution as well as the modern data and analytics technology stack with emphasis on Google Analytics, AppsFlyer, Snowplow, Tableau and Looker (Data Studio).
Collect, clean, and analyse marketing data to extract valuable insights and identify trends.
Develop and maintain dashboards and reports to track key performance indicators (KPIs) for marketing campaigns.
Provide regular performance reports to stakeholders, highlighting key findings and actionable recommendations.
Conduct A/B testing and multivariate analysis to optimize ad creatives, targeting, and bidding strategies.
Analyse user behaviour across marketing channels to identify opportunities for conversion rate optimization.
Track and report on funnel performance metrics, including acquisition, activation, retention, and revenue.
Collaborate with cross-functional teams to implement strategies for improving funnel efficiency.
Ensure data accuracy and integrity by implementing data governance processes and performing regular data audits.
Generate actionable insights and recommendations based on data analysis to support strategic decision-making.
What we are looking for:
Bachelor's degree in Mathematics, Statistics, Economics, Computer Science, or a related field. A postgrad degree would be highly regarded .
Commercial experience as a Data Analyst or similar role within the performance marketing domain.
Strong analytical skills with expertise in data manipulation, analysis, and visualization using tools such as SQL, Python, R, or Excel.
Proficiency in data visualization tools: Tableau, Google Looker (Data Studio).
Experience with performance marketing platforms, such as Google Ads, Facebook Ads Manager, or Adobe Analytics.
Strong attention to detail and a commitment to data accuracy.
Excellent communication and presentation skills, with the ability to effectively convey complex information to both technical and non-technical stakeholders.
Please note we use SQL extensively, so require excellent SQL skills
What's in it for you?
Competitive remuneration package
First class health and well-being program including annual vaccinations, yoga, meditation, gym discounts a, daily breakfasts and lunches provided 4 days a week.
Fun working environment: Friday afternoon onsite gatherings (wine included!) and monthly celebration & recognition programs
Discounted travel, birthday leave and a whole lot more!
If this sounds like you and you feel you have the skills and experience to meet the requirements of the role, please apply online today!
At Luxury Escapes we embrace diversity in our people and make hiring decisions based on experiences and skills. We do not discriminate based on gender identity, religion, race, national origin, sexual orientation, age, marital status, or disability status. You'll find a collaborative, inclusive, and respectful workplace here at Luxury Escapes, and we are proud to celebrate who you are. Please let us know if you require any reasonable adjustments during the interview process
.
We are a 2023 Circle Back Initiative Employer – we commit to respond to every applicant.
Show more
Show less","Data Analysis, Data Visualization, SQL, Python, R, Excel, Tableau, Google Looker (Data Studio), Google Analytics, AppsFlyer, Snowplow, Google Ads, Facebook Ads Manager, Adobe Analytics, Funnel Analysis, Attribution Modelling, Marketing Campaign Effectiveness, Performance Marketing, Conversion Rate Optimization, Data Governance, Data Accuracy, Data Integrity, Business Intelligence, Reporting","data analysis, data visualization, sql, python, r, excel, tableau, google looker data studio, google analytics, appsflyer, snowplow, google ads, facebook ads manager, adobe analytics, funnel analysis, attribution modelling, marketing campaign effectiveness, performance marketing, conversion rate optimization, data governance, data accuracy, data integrity, business intelligence, reporting","adobe analytics, appsflyer, attribution modelling, business intelligence, conversion rate optimization, data accuracy, data governance, data integrity, dataanalytics, excel, facebook ads manager, funnel analysis, google ads, google analytics, google looker data studio, marketing campaign effectiveness, performance marketing, python, r, reporting, snowplow, sql, tableau, visualization"
Data Engineer,Kaliba - Tech & Sales Talent,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-kaliba-tech-sales-talent-3765161862,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"An Australian-founded AI platform that is reshaping the marketing landscape. Founded less than five years ago, this dynamic venture has swiftly emerged as a trailblazing force in the industry.
Crafted to empower marketers, the cutting-edge platform provides unparalleled insights into customer journeys, media effectiveness, and the foresight to predict the outcomes of strategic levers.
As they continue to scale up, they are looking for a skilled Data Engineer to join their Melbourne team. Ideally, this role would be perfect for someone
who has had strong commercial experience of R - ideally used it for stats and regression.
Responsibilities:
Create and implement processes for integrating new data sources into our data ecosystem, ensuring impeccable data quality and consistency.
Work closely with data scientists and statisticians to expose predictive models through scalable and robust APIs.
Harness the power of Python and R for tasks like data processing, transformation, and integration.
Establish and uphold security protocols to safeguard our data and models against unauthorized access and potential breaches.
Experience needed:
3+years of proficiency in ETL processes, data cleansing techniques, and data quality assurance.
Strong experience
particularly in R
and Python for data analysis, statistical modeling, and data cleansing.
Hold a Bachelor's or Master's degree in Marketing Analytics, Data Science, or a related field.
Familiarity with data manipulation libraries and frameworks (e.g., Pandas, NumPy, dplyr) and machine learning libraries (e.g., scikit-learn, TensorFlow, Keras).
Visualization: Showcase data visualization skills using tools such as Matplotlib, ggplot2, or similar.
Bring strong problem-solving skills and an ability to collaborate effectively in a small, tight-knit team.
Benefits:
$110 000 - $150 000 + Super + Benefits
Show more
Show less","Data Engineering, R, Python, ETL, Data Cleansing, Data Quality Assurance, Data Analysis, Statistical Modeling, Pandas, NumPy, dplyr, scikitlearn, TensorFlow, Keras, Matplotlib, ggplot2, Machine Learning","data engineering, r, python, etl, data cleansing, data quality assurance, data analysis, statistical modeling, pandas, numpy, dplyr, scikitlearn, tensorflow, keras, matplotlib, ggplot2, machine learning","data engineering, data quality assurance, dataanalytics, datacleaning, dplyr, etl, ggplot2, keras, machine learning, matplotlib, numpy, pandas, python, r, scikitlearn, statistical modeling, tensorflow"
Data Engineer,Nuage Technology Group,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-nuage-technology-group-3775168516,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"Nuage are currently seeking an experienced Azure Data Engineer to join an engineering team working on a global SaaS medical platform. Your primary focus will be building and maintaining efficient and scalable data pipelines. They are looking for candidates with a strong software engineering background, proficiency in .NET development, and hands-on experience in building data pipelines using Azure services, including Databricks, Synapse, and Azure Data Factory (ADF).
Any experience in DevOps practices will be highly regarded. Join our team and contribute to the advancement of our global SaaS medical platform.
Responsibilities:
Develop, deploy, and maintain data ingestion, transformation, and storage processes using Azure services, such as Databricks, Synapse, and Azure Data Factory (ADF).
Ensure data pipelines are efficient, reliable, and adhere to data quality and security standards.
Optimize and monitor data pipelines to ensure timely and accurate data delivery.
Work closely with software engineers to integrate data pipelines into the overall architecture of the SaaS medical platform.
Implement DevOps practices, including CI/CD, automated testing, and infrastructure as code, to streamline deployment and maintenance processes.
Stay updated with the latest Azure data technologies and trends, and provide recommendations for improvement.
Proven experience as a Data Engineer or a similar role, with a strong software engineering background and proficiency in .NET development.
This position offers the chance to contribute to the development and enhancement of a global SaaS medical platform that is making a positive impact on healthcare worldwide. To apply you must be based in Australia with full working rights.
Follow the Nuage page for updates on market trends and our available openings.
Show more
Show less",".NET, Azure, Databricks, Synapse, Azure Data Factory (ADF), DevOps, CI/CD, Automated Testing, Infrastructure as Code, Data Engineering, Software Engineering","net, azure, databricks, synapse, azure data factory adf, devops, cicd, automated testing, infrastructure as code, data engineering, software engineering","automated testing, azure, azure data factory adf, cicd, data engineering, databricks, devops, infrastructure as code, net, software engineering, synapse"
Data Engineer,Angle Auto Finance,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-angle-auto-finance-3775810040,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"Angle Auto Finance is an independent financier in Australia, bringing a modern platform and a flexible business approach to help more Australians with their car purchase.
The Data Engineer, reporting to the Data Engineering lead, is responsible for building our data engineering solutions, including data models, data ingestion, ETLs, data storage, and data delivery. The role works closely with internal and external stakeholders across the business to design and deliver technical data engineering solutions.
Key Accountabilities & Responsibilities
Core responsibilities;
Contribute to the refinement and delivery of the Data Roadmap
Provide technical advice, development and mentorship within the Data Platform Team
Lead data engineering projects end-to-end, including requirement gathering, solution design, leading the team to deliver while managing stakeholders.
Implement and champion Data Engineering ways of working including and not limited to automated testing, CB/CI, and continuous deployment.
Lead the end-to-end release and deployment process for the Data Warehouse
Work with the infrastructure team to maintain all Data Warehouse Environments
Provide support to the Data Platforms Lead by applying a technical lens in the assessment of team priorities and work effort
Act as a champion for continuous improvement and optimisation
Help design, then build, implement, and maintain cloud-first Data Pipelines and Integrations, for existing and new data sources across a variety of platforms
Perform any required data profiling, cataloguing, and mapping work for technical design and construction of technical data flows
Maintain, refine, or create all ETL processes related to the data stack
Assist other technical teams with deployment of data aggregation tools and tuning performance of scripts, views, and queries.
Ensure all Compliance obligations related to Data are understood and followed, including those related to privacy, security, and regulatory compliance
Key Stakeholders;
BA’s and Data Engineers across the Data team
BA’s and Architects in the technology team
Business teams
Technology partners
Experience;
Relevant IT related degree or certification/s. A degree in a related discipline is preferred
Hands-on data engineering expertise. Experience in building a data ingestion, transformation, and egress framework will be highly advantageous
Knowledge of best practices for data warehousing, data management and architecture, with significant experience in data pipelines, frameworks, tools and technologies and their use in business contexts
Exposure to developing technical designs, including data profiling, cataloguing, and mapping exercises
Experience in the design and build of APIs (REST/SOAP) to push and pull data from various data systems and platforms
Familiar working with DevSecOps tools, methodologies such as CI/CD with GitHub, and Azure DevOps
Ideally you will have good knowledge of the Azure Data platform (Azure Synapse, Data Factory, Data Bricks, Data Lake, Power BI) and Azure cloud data technologies (Spark, ADLS2, CosmosDB, AKS, AEH).
Strong SQL background particularly with capability to write performant queries and troubleshoot performance
Our Principles
Do what’s right​
We have good intent and we do the right thing
Bold and courageous​
We challenge, evolve and innovate for sustained success
Own it and deliver​
We’re accountable
Win together​
We collaborate and encourage each other to thrive
Celebrate success​
We appreciate and celebrate our achievements
How to apply
Select the
'Apply'
button above to send your resume through to the Talent Acquisition team. Application close date
- Thursday, 7 December 2023.
Angle Auto Finance is committed to attracting, retaining and strengthening the performance of a diverse, equitable and inclusive workforce. It is our commitment to ensure equal employment opportunities without discrimination or exclusion on the basis of cultural heritage, colour, religion, citizenship status, age, sex, sexual orientation, gender identity, or expression, marital or domestic/civil partnership status, disability, veteran status, genetic information or any other basis protected by law.
Show more
Show less","Data engineering, Data ingestion, ETLs, Data storage, Data delivery, Data modeling, Data pipelines, Data architecture, Data warehousing, Data management, Data profiling, Data cataloguing, Data mapping, API design, Azure Data platform, Azure Synapse, Data Factory, Data Bricks, Data Lake, Power BI, Azure cloud data technologies, Spark, ADLS2, CosmosDB, AKS, AEH, SQL, CI/CD, GitHub, Azure DevOps","data engineering, data ingestion, etls, data storage, data delivery, data modeling, data pipelines, data architecture, data warehousing, data management, data profiling, data cataloguing, data mapping, api design, azure data platform, azure synapse, data factory, data bricks, data lake, power bi, azure cloud data technologies, spark, adls2, cosmosdb, aks, aeh, sql, cicd, github, azure devops","adls2, aeh, aks, api design, azure cloud data technologies, azure data platform, azure devops, azure synapse, cicd, cosmosdb, data architecture, data bricks, data cataloguing, data delivery, data engineering, data factory, data ingestion, data lake, data management, data mapping, data profiling, data storage, datamodeling, datapipeline, datawarehouse, etls, github, powerbi, spark, sql"
AWS Data Engineer,InfoCentric,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/aws-data-engineer-at-infocentric-3777883316,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"InfoCentric is currently looking for
AWS Data Engineer
to join our growing team.
This is a full-time, permanent role based out of Melbourne.
Working with a leading Australian enterprise client, you will be creating innovative Data Solutions, building Data Warehouses, and creating Ingestion Pipelines.
Qualifications
Good Data Warehousing experience
AWS Cloud Experience
Good Redshift experience
Strong Python and SQL experience
Stakeholder management experience
Strong financial services or banking experience
Databricks experience would be an advantage
Who Are We?
InfoCentric is one of the largest specialist Cloud, Data and Analytics Consulting firms
Data is in our DNA –
Harnessing the latest technology, delivering thought-leading outcomes in AI, Machine Learning, API development and Cloud Platform delivery
We translate the best ideas into awesome outcomes that make a difference to Australia's largest organisations.
Embrace diversity
- We work to understand and celebrate differences to embrace diversity, drive smarter decisions, create new ideas and build a better culture
Thrive together
- We work together in innovative teams to share knowledge and help each other grow
Realise opportunity
- We offer opportunities to develop skills and leadership with paid certifications and team trainings while offering great career growth
Empower clients
- We strive to understand our clients to deliver hands-on solutions to Australia’s largest companies across a diverse industries
Think big picture
- We act in fairness to look after our people, our clients and our community
What We Offer
Hands-on, onshore experience with Australia’s largest organizations
Paid trainings and certifications on the latest technologies including AWS, Azure and Snowflake
Great on-site support to help you settle in and progress your career
Regular social events and company parties. Previous events include private cinema screenings, bubble soccer, minigolf, bowling and social drinks
Due to the high number of applicants we receive, you will be contacted if you are successful. We thank you for your interest in InfoCentric and working with us. If you're unsuccessful for this role we will keep your details and reach out to you with any suitable roles in the future.
Show more
Show less","AWS, Data Engineering, Data Warehousing, Redshift, Python, SQL, Stakeholder Management, Databricks, Cloud Platform, AI, Machine Learning, API Development","aws, data engineering, data warehousing, redshift, python, sql, stakeholder management, databricks, cloud platform, ai, machine learning, api development","ai, api development, aws, cloud platform, data engineering, databricks, datawarehouse, machine learning, python, redshift, sql, stakeholder management"
Data Engineer,Open Universities Australia,"Docklands, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-open-universities-australia-3784117527,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"Our mission is to remove barriers, enhancing access to quality education for all Australians – irrespective of age, location, life circumstance or previous experience.
As a not-for-profit organisation, we’ve got our students’ best interests at heart. We have guided more than 500,000 students to explore, choose, and enrol in one of the 1,000 and growing courses listed on the OUA marketplace from 27 leading Australian universities.
We embrace the principles of autonomy, alignment, and agility to guide our operations. Our culture is built on values that prioritise Acting for Customers, Continuously Improve, and Caring for Each Other.
About the team
The Customer Insights team works with teams across the student journey, collecting, connecting, and analysing performance, voice of customer and external datasets. Our primary objective is delivery of reliable, actionable, and timely insights to enhance student experience and outcomes.
About the role
You will be responsible for connecting and collecting data from various sources, constructing and managing data pipelines to build data marts and models to support all analytics activities. Stay updated with cloud-oriented languages and latest technologies to ensure our data flows smoothly and securely through of AWS stack.
Here’s what a normal day might look like for you
Connecting and gathering data from diverse sources like our website, Salesforce CRM, telephony provider, and Google. Your primary focus is on maintaining a robust and reliable data lake, ensuring seamless integration with both new and existing data sources.
You'll be constructing and managing data marts and models to support analytics activities and business-facing applications. Your work plays a crucial role in providing a consolidated view of organisational data for descriptive and diagnostic analysis.
In addition, you'll facilitate connectivity and accessibility across the data landscape. Your goal is to ensure that business and analytics teams can easily and securely access the necessary tools for effective data utilization.
Here are some ways you will bring your best to this role
Experience in Data Engineering and CI/CD principles.
Experience with API based transaction and bulk integrations with cloud-based platforms
Proven experience in data modelling, data warehousing, business intelligence capacity
Strong in SQL and Python scripting and familiar with programming languages Python
Knowledge of database management practices, experience with AWS stack, including Redshift, AppFlow, Glue etc.
Ability to engage with platform development and architects to improve current data integrations.
Exposure to Business Intelligence and experience management solutions such as GoodData, MarketingCloud, Tealium, Qualtrics.
What do we offer?
Not-for-profit salary packaging benefits, including novated leasing
Competitive market salary along with annual individual and company incentives
Discounted gym memberships and health insurance
Learning and training opportunities
Subsidised study opportunities
Paid subscription to Calm as part of our commitment to health and wellbeing
Inclusive leave policy to swap and recognise significant events that fall outside of our public holidays
Workation leave – work remotely anywhere for 10 days of the year
If you're passionate about data integration, management, and creating a streamlined data environment for analytical purposes, this role offers an opportunity to make a significant impact. Join us and contribute to transforming data into valuable insights for our organisation.
OUA is committed to fostering an inclusive and diverse workplace, reflecting the diversity of the marketplace we serve. We encourage candidates from all backgrounds to apply, even if they don't meet all the requirements listed. We value unique perspectives and experiences that can contribute to our mission of helping students transform their lives.
Show more
Show less","Cloudoriented languages, AWS, Salesforce CRM, Google Cloud Platform, Data Engineering, CI/CD, API, Data modelling, Data warehousing, Business intelligence, SQL, Python, Database Management, Redshift, AppFlow, Glue, GoodData, MarketingCloud, Tealium, Qualtrics","cloudoriented languages, aws, salesforce crm, google cloud platform, data engineering, cicd, api, data modelling, data warehousing, business intelligence, sql, python, database management, redshift, appflow, glue, gooddata, marketingcloud, tealium, qualtrics","api, appflow, aws, business intelligence, cicd, cloudoriented languages, data engineering, data modelling, database management, datawarehouse, glue, gooddata, google cloud platform, marketingcloud, python, qualtrics, redshift, salesforce crm, sql, tealium"
Azure Data Engineer,"Workforce Solutions, a division of NTT Ltd.","Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/azure-data-engineer-at-workforce-solutions-a-division-of-ntt-ltd-3773082808,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"Is innovation part of your DNA? Do you want to enable a connected future for people, organizations, and society?
Join our growing global NTT team and you’ll be part of the world’s largest ICT company (by revenue). We’ve combined the capabilities of 28 remarkable companies to become one, leading technology services provider. Together, we help our people, clients, and communities do great things with technology to create a more secure and connected future. We employ 40,000 people across 57 countries. By bringing together the world’s best technology companies and emerging innovators, we work together to deliver sustainable outcomes to businesses and the world. Innovation is part of our DNA. We believe it’s key to what makes us different. So, we strive to move forward, challenge the status quo, and drive excellence through the technologies we integrate and the services we deliver around the world. The result is connected cities, connected factories, connected healthcare, connected agriculture, connected conservation, connected mobility, and connected sport. Together we enable the connected future.
Want to be part of the team?
I am seeking an experienced Azure Data Engineer for 6 months with a view of extension in Melbourne.
Your day at NTT
Azure Data factory
Data Pipelines
SQL
Azure Integration
Analzying of source system of data
Mandatory: SQL, Power BI
What would make you a good fit for this role?
Ideally have DevOps + Teams
More data warehouse focsed than analytics focused. About building data warehouses and enabling pipelines
Needs to be senior enough to work independently
Need to have built data warehouses in the past
If you think you match this requirement then email me your CV NOW!
Show more
Show less","Azure Data Factory, Data Pipelines, SQL, Azure Integration, Analyzing Source System of Data, Power BI, DevOps, Teams, Data Warehouse, Data Warehouse Building, Data Pipeline Enabling","azure data factory, data pipelines, sql, azure integration, analyzing source system of data, power bi, devops, teams, data warehouse, data warehouse building, data pipeline enabling","analyzing source system of data, azure data factory, azure integration, data pipeline enabling, data warehouse building, datapipeline, datawarehouse, devops, powerbi, sql, teams"
Data Center Engineer,NTT,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-center-engineer-at-ntt-3763838274,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"Key skills:
Minimum of 5 years’ experience in storage, design, and engineering
Experience with vendor products such as Veeam, Dell EMC (VM ware), NetApp, Commvault and Hitachi
VMware certification, IT degree or equivalent, and any other desired certifications with NetApp, Commvault, or EMC
General knowledge of Active Directory infrastructure
Experience in troubleshooting and root cause analysis
Understanding of WAN/LAN/SAN constraints related to centralized storage in virtualized environments.
Capability to convert storage architecture requirements into effective solutions.
Sound communication skills with technical leadership
Comprehensive knowledge of storage deployment, configuration, and monitoring
Familiarity of Backup & Archiving Solutions including Commvault and Veeam
Demonstrated excellence in teamwork, including collaborating with virtual and geographically dispersed teams.
Experience with Disaster Recover solutions.
Show more
Show less","Storage engineering, Troubleshooting, Root cause analysis, WAN/LAN/SAN, VMware certification, IT degree, NetApp certification, Commvault certification, EMC certification, Active Directory infrastructure, Backup & Archiving Solutions, Virtualization, Disaster Recovery solutions","storage engineering, troubleshooting, root cause analysis, wanlansan, vmware certification, it degree, netapp certification, commvault certification, emc certification, active directory infrastructure, backup archiving solutions, virtualization, disaster recovery solutions","active directory infrastructure, backup archiving solutions, commvault certification, disaster recovery solutions, emc certification, it degree, netapp certification, root cause analysis, storage engineering, troubleshooting, virtualization, vmware certification, wanlansan"
Lead Data Engineer,TelstraSuper,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/lead-data-engineer-at-telstrasuper-3777019650,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"At TelstraSuper our employee experience is created through our caring and collaborative community, the deep and rewarding career experiences on offer and our close connection to our members. With diverse and inclusive teams, it’s the difference that counts that helps us deliver a great member experience and engagement among our people.
TelstraSuper is an award-winning fund with more than $24 billion funds under management and recently received the 2023 Chant West Award for Pension Fund of the Year, for the second year running.
The Opportunity
Exciting opportunity for a Lead Data Engineer to join the TelstraSuper’s Technology team. Reporting to the Head of IT Solution Design & Delivery, this role will lead the end-to-end design, development, delivery and operation of solutions that support the delivery of trusted, consistent and timely information to the business. This role will also be responsible for the tools, processes, and technologies required to manage data and contribute to broader tactical and strategic programs in the areas of technical data architecture and data integration.
Key responsibilities:
Lead the architectural design of the data warehouse environment ensuring adherence to frameworks and solutions are fit for purpose
Lead the development and operation of information assets that deliver trusted information to the business that meets reporting, analysis and insight needs
Elicit data requirements, define conceptual, logical and physical data models and workshop solution designs in order to continuously improve on the business intelligence asset in meeting information needs
Facilitate and drive the education and communication of D&A Power users and other key stakeholders including workshops, meetings, presentations, demos etc.
Act as the gatekeeper for ensuring data warehouse information is fit for purpose and that data within the warehouse meets the quality consistency and timeliness expectations of the business
Guide, support, coach and mentor Data Provisioning and stakeholders, encouraging collaboration
Ensure data warehouse data is correct and available through ongoing monitoring, troubleshooting and remediation of issues in a timely manner
About You
Tertiary qualifications in Computer Science systems or relevant disciplines
7+ years of commercial experience within heavy data warehouse environments
Advanced data analysis and visualisation skills
Strong background and applied knowledge in Kimball methodologies
Experience querying Microsoft SQL Server, creating Views and Stored Procedures’
Strong background in the following toolsets: MS Azure, PowerBI, Tableau (data sourcing/ customised source queries, views, interactive dashboards, data visualisations, table calculations, hierarchies, trending, global and dynamic filtering)
Additionally, you have:
Experience working in the superannuation industry with an understanding of administration and operations
Excellent advisory, consultative and stakeholder management skills
Proven ability to design and manage complex technical solutions from both strategic and tactical perspectives
TelstraSuper Experience
We’re an Employer of Choice for Gender Equality (WGEA) and believe you should be able to bring your whole self to work so you can be your best every day. We offer a range of benefits to support your work and life including:
Flexible working and leave options
Development and learning opportunities to grow yourself and your career
Health and wellbeing support including flu shots and skin checks
Discounts through our partners including Telstra
To be eligible to apply, you are an Australian or New Zealand citizenship or have permanent residency status.
Applications close:
Monday 8th of January 2024
No agencies please. TelstraSuper will not accept unsolicited resumes from any source other than directly from a candidate.
www.telstrasuper.com.au
Show more
Show less","Data Warehouse Architecture, Data Analysis, Data Visualisation, Kimball methodologies, Microsoft SQL Server, PowerBI, Tableau, Azure, Stakeholder Management, Complex Solution Design","data warehouse architecture, data analysis, data visualisation, kimball methodologies, microsoft sql server, powerbi, tableau, azure, stakeholder management, complex solution design","azure, complex solution design, data visualisation, data warehouse architecture, dataanalytics, kimball methodologies, microsoft sql server, powerbi, stakeholder management, tableau"
Data Engineer,Spotlight Retail Group (SRG),"South Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-spotlight-retail-group-srg-3769597673,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"Iconic Australian owned and operated global retail brand
Excellent Package + Profit Share + Employee Discounts + Work Flexibility & more
A vibrant, team-oriented culture where we embrace pace and encourage new ideas
About the Role
We are seeking a skilled and experienced Data Engineer to join our dynamic team and play a pivotal role in shaping our modern data engineering practice. As a Data Engineer, you will be responsible for designing, developing and implementing data engineering solutions that enable our organization to derive valuable insights from its vast data assets.
What You Will Be Doing
Establish and maintain a modern data engineering practice in collaboration with IT stakeholders, business stakeholders, and partners
Implement and maintain applications and systems for accessing, processing, enriching, cleaning data and orchestrating data analytics as requested by business teams
Gather and document business requirements, prepare functional designs and technical specifications covering all aspects of data engineering from ETL to presentation of information
Administer, manage and support SRG's data analytics landscape and associated application assets
Develop and manage technology frameworks encompassing comprehensive documentation of data collection design, data pipeline establishment, data movement with pre-processing and transformation, data storage strategies and data governance
Implement techniques such as aggregates, indexing, partitioning, clustering, archiving, and compression to enhance performance
Proactively monitor and identify potential issues with application performance, availability, or data integrity
Lead and support data analytics application landscape enhancements, upgrades, and patches
Assist end-users to understand and interpret the information on the landscape
Provide appropriate on-call after-hours support in line with SLAs
About You
Must have expertise in ETL/ELT processes, data warehousing, and data modelling
Tertiary qualifications in Computer Science, Data Science or a related field
At least 3+ years of experience as a Data Engineer or a similar role
Proven experience in designing, developing, and implementing data engineering solutions
Strong understanding of data governance principles and practices
Experience with cloud-based data platforms such as AWS or Azure, cloud infrastructure services (ideally AWS) and SQL programming
Proficiency in scripting languages such as Python and JavaScript
Excellent analytical, problem solving, consulting and communication skills (back-of-house & front-of-house)
Proven ability to engage with internal customers to gather and document business requirements, functional requirements and design of solutions
Good understanding of retail business processes
Demonstrated ability to produce high quality documentation with Ability to explain technical systems, problems & outcomes to non-technical staff
Demonstrated ability to work in a fast-paced environment, meeting deadlines & multi-task under limited Supervision Formal training or certification in IaaS, PaaS, SaaS areas such as AWS services, Azure services, Snowflake, PowerBI, Apache Airflow, DBT or equivalent technology is highly desirable.
What’s in it for you?
Along with the incredible career- growth opportunity of being part of our talented IT team, we can offer you
Great salary base + Super + Profit Share + Hybrid Role
Progression & personal development
A culture that rewards initiative and creativity
Centrally located office, close to south Melbourne market, for when we return to work
Benefits- team discount, EAP, gym discounts and more
Who We Are
Spotlight Group Holdings (“SGH”) is looking to grow their talented, highly entrepreneurial team. SGH is Australia's largest privately-owned and operated family business, comprising of the Spotlight Retail Group, the Spotlight Property Group, Spotlight Direct Investments and the Spotlight Charitable Foundation.
Comprising of the long term existing iconic brands in
Spotlight, Anaconda
and
Mountain Designs
, together with the exciting addition of
Harris Scarfe
, SRG is acknowledged as the market leaders within the Australian retail Space.
To apply, please complete our online application form and submit an up-to-date resume.
You must have unrestricted working rights in Australia to be considered for this position
Only shortlisted candidates will be contacted
No agency enquiries or applications please
Show more
Show less","Data Engineering, ETL/ELT processes, Data warehousing, Data modelling, Cloudbased data platforms, AWS, Azure, SQL programming, Python, JavaScript, Apache Airflow, DBT, Snowflake, PowerBI, IaaS, PaaS, SaaS","data engineering, etlelt processes, data warehousing, data modelling, cloudbased data platforms, aws, azure, sql programming, python, javascript, apache airflow, dbt, snowflake, powerbi, iaas, paas, saas","apache airflow, aws, azure, cloudbased data platforms, data engineering, data modelling, datawarehouse, dbt, etlelt processes, iaas, javascript, paas, powerbi, python, saas, snowflake, sql"
Data Analyst,Codex,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-analyst-at-codex-3780817926,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"Melbourne,
with (flexible work arrangements)
Attractive remuneration $600 – 750 per day!
Ongoing contract role
with a growing organisation where you can influence our strategy and direction.
Codex is a value-driven Australian technology consultancy that works with our clients to discover, design and deliver digital and data solutions that have meaningful impact on their customers and communities.
We are proud to lead with empathy, prioritise sustainability, and celebrate authenticity. These values are behind everything we do: they influence our decision-making; they define our target market; and ultimately, they guide the way we engage with each other.
Are you our next hire?
We have an exciting opportunity for a Data Analyst.
In this role you will be challenged to deliver consistently at a high standard, providing demonstrable capability.
The Market Data & Customer Analytics (MDCA) team is seeking a skilled individual to contribute significantly to our data analytics function, aiding us in driving customer-focused analytics for Marketing and enhancing strategic initiatives across our client's organisation. In this role, you will have the opportunity to partner and collaborate with various stakeholders, lead data analytics projects, and provide analytical expertise and support to stakeholders.
Collaborate with various stakeholders across key projects and priorities, driving solutions aimed at increasing Marketing and/or Business ROI.
Lead projects to build scalable data analytics solutions in support of Marketing and broader customer insight needs, encompassing all aspects of delivery from strategy to continuous improvement.
Maintain and execute regular outputs with high reliability and accuracy, including ad hoc analysis, customer campaigns, reporting, models, segmentation, and data tools.
Provide analytical expertise that enables stakeholders to utilise analytic techniques and team outputs confidently.
Identify and lead learning opportunities across the stakeholder group and mentor team members, fostering skills and expertise development.
To be successful in this role, you will need:
Essential - Experience in a senior role in Data Analytics, within a
Marketing division/ function
, with strong strategic application of data and insights.
Advanced proficiency in R is essential.
Experience with SQL and/or visualisation tools.
Strong communication and influencing skills at all levels, with an ability to translate complex data analytics into understandable insights and actions for the business. Comfortable collaborating and building strong/credible relationships across business units.
Experience in preparing & presenting data insights & analysis to senior business stakeholders.
Previous experience in a client facing consulting role will be viewed favourably.
Working at Codex
We are one of Australia’s fastest growing consultancies, and we are a for-impact, for-profit business. This means we define our success not only by financial metrics, but by our positive impact on our people, clients and community. This means it is not by chance that we work primarily in healthcare, energy and infrastructure, sports, fitness and wellness, and other high-impact, value-aligned industries.
You can probably tell by now that we aren’t your average consultancy, so…
What sets us apart?
An equal voice policy
. We foster an authentic and collaborative environment where the best idea wins.
A genuine commitment to sustainability.
We invest time, money and services into environmental and social sustainability initiatives.
We lead with empathy.
This empowers us to better help our people, clients and community by first understanding their problems.
We prioritise YOUR career.
We work with you to understand your goals, and then equip you with the essentials for success, investing in your growth and empowering your career development.
Codex is an Equal Opportunity Employer.
Codex embraces who we are as individuals, and believes that our differences in background, opinion, culture and experience all contribute to the strength of our organisation. We assess each application on merit and merit only.
Thank you for considering Codex for your next employment opportunity.
We look forward to reviewing your application and will be in contact with candidates successful in moving through to the next stage of our hiring process.
By submitting your application you understand that we will keep your resume on file for other suitable opportunities.
Show more
Show less","Data Analyst, R, SQL, Visualization Tools, Analytics, Data Insights, Communication, Influencing, Collaboration, Business Insights, Data Analytics Solutions, Stakeholder Relationships, Strategic Application of Data, Advanced Proficiency in R, Experience with SQL, Data Models, Segmentation, Consulting Role","data analyst, r, sql, visualization tools, analytics, data insights, communication, influencing, collaboration, business insights, data analytics solutions, stakeholder relationships, strategic application of data, advanced proficiency in r, experience with sql, data models, segmentation, consulting role","advanced proficiency in r, analytics, business insights, collaboration, communication, consulting role, data analytics solutions, data insights, data models, dataanalytics, experience with sql, influencing, r, segmentation, sql, stakeholder relationships, strategic application of data, visualization tools"
Senior Data Engineer (Postgres to AWS RDS Migration),Nuage Technology Group,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-postgres-to-aws-rds-migration-at-nuage-technology-group-3775942164,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"6-Month Rolling Contract
Greenfield Data Migration Project
High impact Data Engineering Team
$850-900 per day
About Us:
Nuage has partnered with a leading player in the financial services sector, who have committed to an enterprise grade migration project. The project is fully signed off from the business to migrate their data infrastructure from Postgres to AWS RDS. As part of this exciting venture, we are seeking highly skilled and experienced Data Engineering Specialists to join the clients team.
Project Overview:
This is a complete greenfield project, with budgets signed off and architectural designs in progress. While the initial architecture has started, the actual solution designs, building and migration phases are yet to commence, providing a fantastic opportunity for you to make a significant impact from the ground up.
Key Responsibilities:
- Work closely with the business to Lead the migration from Postgres to AWS RDS, leveraging core technologies including Airflow, Jenkins, AWS, Postgres, and RDS.
- Design and implement RBAC, Data Masking, and Data Governance principles to ensure data security and compliance.
- Utilize Python programming and SQL for data manipulation and analysis.
- Hands-on experience with AWS services such as SQS, SNS, EMR, EC2, etc.
- Familiarity with DBT and Apache Airflow is highly advantageous.
About you:
- Must-have technology skills in AWS RDS, Postgres SQL, and Apache Spark.
- Proficiency in Python programming and SQL.
- Extensive hands-on experience with AWS services.
- Familiarity with DBT and Apache Airflow.
- Strong problem-solving skills and the ability to thrive in a fast-paced, collaborative environment.
- Excellent communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.
- Advanced certifications in AWS or related technologies are an added advantage.
MUST be based in Melbourne, and able to commit to 3 days a week in the CBD minimum. MUST have full working rights with a minimum of Australian Permanent Residency or Australian Citizens.
Essentially if you have depth in Postgres, and have done an AWS RDS migration project from a Data Engineering perspective – Would love to chat to you.
MUST
have full working rights for Australia (with a minimum of Permanent Residency or Australian Citizenship)
For more information on the Australian Technology market, please feel free to follow us on LinkedIn: https://www.linkedin.com/company/nuage-technology-group/?viewAsMember=true or visit our website www.nuagetg.io
Show more
Show less","AWS RDS, Postgres SQL, Apache Spark, Python, SQL, AWS services (SQS SNS EMR EC2), DBT, Apache Airflow, RBAC, Data Masking, Data Governance, Problemsolving, Communication","aws rds, postgres sql, apache spark, python, sql, aws services sqs sns emr ec2, dbt, apache airflow, rbac, data masking, data governance, problemsolving, communication","apache airflow, apache spark, aws rds, aws services sqs sns emr ec2, communication, data governance, data masking, dbt, postgres sql, problemsolving, python, rbac, sql"
Senior Data Engineer,Culture Amp,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-culture-amp-3763317941,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"Join us on our mission to make a better world of work.
Culture Amp revolutionizes how over 25 million employees across 6,000 companies create a better world of work. As the global platform leader for employee experience, Culture Amp empowers companies of all sizes and industries to transform employee engagement, develop high performing teams, and retain talent via cutting-edge research, powerful technology, and the largest employee dataset in the world. The most innovative companies across the globe, such as Salesforce, PwC, KIND, SoulCycle, Celonis and BigCommerce depend on Culture Amp every day.
Culture Amp is backed by 10 years of innovation, leading capital venture funds, and offices in the U.S, U.K, Germany and Australia. Culture Amp is recognized as one of the world’s top private cloud companies by Forbes and one of the most innovative workplace companies by Fast Company.
Learn more about how Culture Amp can help you create a better world of work at cultureamp.com .
How you can help make a better world of work
We are looking for a Senior Data Engineer to join our People Analytics team. We sit within the Data Intelligence Camp, where we build reporting and analytics tools for our platform, leveraging data engineering, ML engineering and data science. We're working on a brand new set of features to help HR leaders utilise data from across the Culture Amp platform to maximise and measure the impact they have on their organisations.
In part of this team of amazing humans,
You will
Lead the design and build of the data pipelines that underpin our analytics products.
Ensure those pipelines are tested, performant and observable.
Monitor end-user performance and optimise the schemas and queries.
Play a key role in promoting data best practices across the team and camp and support others to develop their own knowledge and skills.
Collaborate with stakeholders to ensure we’re responsibly managing our data in line with our privacy, security and compliance requirements.
You have
A high level of proficiency with cloud data warehouses such as Redshift.
Expert skills in modelling and querying in SQL and Python.
Proven experience with ETL/ELT tooling including DBT and Airflow.
Desirable but not essential - experience with CI/CD and infrastructure-as-code, ideally within AWS cloud.
Also desirable - familiarity with AWS’ data tools, including Athena, Glue, SageMaker, EMR.
You are
A driver of technical excellence in a team environment. You’re an expert in your domain and are able to develop the expertise and knowledge of those around you.
Someone who loves collaboration - our teams are cross functional and you’ll be working with other engineers, team leads and product managers to deliver great outcomes together.
Keenly interested in understanding and solving customer problems. You think about how your work impacts our customers and improves their experience
A pragmatic engineer. You ask and understand what we're optimizing for before jumping into solutions, and can comfortably balance long-term objectives with short-term tradeoffs.
Aligned with our values, check them out here: https://www.cultureamp.com/company#values and demonstrate them through your working practice
Self-motivated and able to work independently, comfortable dealing with ambiguity when necessary. You take the initiative to ensure that you have everything you need to work effectively and ask for support when required.
Thank you for taking the time to read this advert. If you decide to apply, as part of your application, we will ask you to complete voluntary diversity questions (excluding Germany). Please watch this video from our amazing DEI Leader, Aubrey Blanche to share more on why we collect the data and how we will use it.
Show more
Show less","Data Engineering, ML Engineering, Data Science, Python, SQL, DBT, Airflow, AWS, Athena, Glue, SageMaker, EMR, Redshift, CI/CD, InfrastructureasCode","data engineering, ml engineering, data science, python, sql, dbt, airflow, aws, athena, glue, sagemaker, emr, redshift, cicd, infrastructureascode","airflow, athena, aws, cicd, data engineering, data science, dbt, emr, glue, infrastructureascode, ml engineering, python, redshift, sagemaker, sql"
Senior Consultant - Databricks and Microsoft Fabric Data & Analytics Engineer,BI & DW Australia,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/senior-consultant-databricks-and-microsoft-fabric-data-analytics-engineer-at-bi-dw-australia-3751662988,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"Senior Consultant - Databricks and Microsoft Fabric Data & Analytics Engineer
A leading Data & Analytics Professional Services consultancy is looking for two Microsoft Data Engineer Consultants for their practice.
What's In It for You
A competitive permanent salary package with the opportunity to work with large enterprise clients in many verticals including financial services, utilities, government and more!
The role
This role is the epitome of the archetypal IT business consultant where you will apply your proven Databricks and Microsoft Data & Analytics development expertise within top ASX 500 style organisations. You will be working exclusively on high level projects within the Melbourne market and will utilise the Microsoft Azure Fabric Services technologies, Synapse, OneLake, Data Factory and Power BI. You will become a key member of the team assisting with the growth of the Microsoft Practice.
The skills needed
The ideal candidate for this true technical consulting role will have some or all of the following experience:
Databricks
Microsoft Platform knowledge of Azure Fabric Azure Data Services, the Microsoft Data cloud stack and on prem Microsoft BI technologies
Power BI experience
Azure Data Factory experience is a huge advantage
Synapse Analytics experience is a huge advantage
Data modelling experience
DBT, Snowflake, AWS, GCP experience all valuable
Ideally previous Professional services experience with niche Data consultancies
Previous on prem SSIS, SSAS (tabular and modular) and SSRS, Power BI experience
High level client liaison skills
Documentation (business focused, technology focused and project focused)
As mentioned salary levels are excellent and remuneration will match your experience.
Show more
Show less","Databricks, Azure Fabric Services, Synapse, OneLake, Data Factory, Power BI, Microsoft Data, Power BI, Azure Data Factory, Synapse Analytics, Data modeling, DBT, Snowflake, AWS, GCP, SSIS, Power BI, Client liaison, Documentation","databricks, azure fabric services, synapse, onelake, data factory, power bi, microsoft data, power bi, azure data factory, synapse analytics, data modeling, dbt, snowflake, aws, gcp, ssis, power bi, client liaison, documentation","aws, azure data factory, azure fabric services, client liaison, data factory, databricks, datamodeling, dbt, documentation, gcp, microsoft data, onelake, powerbi, snowflake, ssis, synapse, synapse analytics"
Senior Data Engineer,Bupa,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-bupa-3773090290,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"A full time, permanent opportunity has become available in our Bupa ADF Health Services Contract (ADFHSC) Data & Analytics team. This is a hybrid role and will be based out of our Melbourne, Sydney or Brisbane office locations when not working from home.
As the Senior Data Engineer, you will be part of a centralised technology team of specialist collectively working to build the future state of Bupa’s data platforms on Microsoft Azure services, and to enable the business to work strategically with data to maximise competitive performance.
As a senior member of the team, you will provide technical mentorship to other team members, lead workshops, coach on engineering practices, and play a role in Engineering governance.
How will I help?
Key accountabilities include but are not limited to:
Support the Head of Data & Analytics in providing thought leadership and consulting to business stakeholders around the use of data and data technology
Mentor and coach team members on engineering practices and technology
Work directly with the business in understanding detailed data requirements and translating the requirements into the form of Epics, Features, and Stories, that technical deliver teams can understand and build from
Lead and produce estimation of effort for the delivery of stories and lead team level design and code review processes
Produce detailed design deliverables based on established architectures and design standards and identify and contribute to the evolution of data design standards, guidelines, frameworks, and practices
Engineering of data pipelines from data acquisition and ingestion though the data lake, various means of consumption including, reports, dashboards, applications, models, and API’s
Engineering of data features in support of advanced analytical processes and data science teams as well as advanced analytical solutions including ML and ML ops processes and CI/CD processes and execution of releases
What’s in it for me?
As well as a competitive salary, a range of Bupa benefits and flexible working/ work from home arrangements, you’ll be challenged and encouraged to innovate. You will partner with colleagues who are dedicated to delivering exceptional experiences. We respect and consider everyone, knowing your difference will make the difference.
Perk highlights:
Discount on our health insurance, travel, car, home, contents and pet insurance products as well as discounts when you attend a Bupa Dental clinic or purchase glasses from a Bupa Optical store. A dedicated internal wellbeing team provides you with a range of services, such as a global resilience program, regular training opportunities on a range of wellbeing topics and access to a variety of wellbeing discounts.
What do I need?
5 to 10 years’ experience as a software or data engineer
Cloud Data Platform experience in AZURE preferred, but AWS & GCP also considered
An expert level knowledge of SQL server, SQL/TSQL, Python
A background in DataBricks & Spark
Advanced engineering practices and CI/CD experience
Experience running code reviews
A passion for mentoring engineers for skill development
Strong knowledge of Agile frameworks
Strong project management skills and stakeholder management skills
Excellent presentation, communication, and interpersonal skills
This is a Security Cleared Position. The occupant must be an Australian Citizen and must be eligible to obtain and uphold a Baseline Security Clearance
About Bupa
Bupa is an international healthcare group which has a purpose to deliver longer, healthier, happier lives and making a better world for more than 70 years. In Australia and New Zealand, Bupa assists more than 5 million customers through a broad range of health and care services including health insurance, aged care, rehabilitation, dental, optical, medical, hearing and medical visa services.
Our people reflect the diversity of our community. At Bupa, your wellbeing, identity and own story is respected and valued.
Show more
Show less","Data Engineering, Microsoft Azure Services, SQL Server, SQL/TSQL, Python, DataBricks, Spark, CI/CD, Agile, Project Management, Stakeholder Management, Presentation, Communication, Interpersonal Skills, Security Clearance, Baseline Security Clearance","data engineering, microsoft azure services, sql server, sqltsql, python, databricks, spark, cicd, agile, project management, stakeholder management, presentation, communication, interpersonal skills, security clearance, baseline security clearance","agile, baseline security clearance, cicd, communication, data engineering, databricks, interpersonal skills, microsoft azure services, presentation, project management, python, security clearance, spark, sql server, sqltsql, stakeholder management"
Senior Data Engineer,Blinq,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-blinq-3767969061,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"WHAT IS BLINQ?
🤝 The first interaction two people have is the bedrock of all strong business relationships. If you can make that experience special, you can start to build a great second interaction, and so on. Blinq is the tool to help people do that. We're building a platform that allows you to share a snapshot of who you are with anyone, anywhere via digital business cards, dynamic email signatures and virtual backgrounds. Join us on our mission to help the world connect.
(We will get to the fun perks part at the bottom, keep going!)
Let's Dive Into What Makes Blinq An Extraordinary Product
🚀 We're on an incredible growth trajectory, doubling our ARR every few months. Get ready to soar to new heights as we make waves in the industry!
😃 Our app is trusted and loved by employees at renowned companies like Patreon, Tesla, Uber, and Google. Rub shoulders with industry leaders and be part of the Blinq revolution.
🙌 Backed by Australia's top venture capitalists, Blackbird and Square Peg, we've brought together their investment prowess since Canva's seed round. It's a testament to our potential and the caliber of our vision.
❤️ With over 50K reviews and a stellar 4.9/5 star rating on the App Store, we've become one of the top 65 Business apps. Join a team that's making waves and be a part of our success story!
The Role
We are looking for a Data Engineer to take ownership of Blinq’s data analytics infrastructure. This role provides you with the opportunity to create and develop Blinq’s data practices to maximise business effect. With a great sense of ownership over our data and analytics platforms, you will be positively affecting team member happiness and productivity!
At this exciting stage of our growth, we want to ensure our data is scalable and spick and span! You would be working closely with Product and Engineering and liaising with technical and non-technical peers to make things happen.
Be part of a start-up business and product-led growth at Blinq!
What You Will Own
In this role you would become the go-to owner of our data and analytics platforms
You will own and maintain our analytics infrastructure and dashboards, using a variety of tools
You will be helping a small product company make best-informed data-driven decisions
You will be aligning our data strategy to our broader business goals
You will be critiquing our data to ensure its’ accuracy
You will be working closely with Product and Engineering teams to determine where best to measure success
You will drive cross-functional efforts related to data quality, reporting, insights and remediating data inconsistencies and incompleteness to answer business questions.
You will be delivering on data infrastructure projects, such as new data pipelines, database integrations, or data warehouse enhancements
What We Are Looking For
You are a great collaborator who can work to understand the needs of different stakeholders and develop a strategy which considers each of these
You are a strong communicator and are confident at being an evangelist and driving internal education around your data
You have worked on a modern event data stack and are familiar with Segment, Amplitude, Apache Superset, Google Analytics and AWS/GCP (desirable)
You have a strong vision of how an industry-leading modern analytics stack looks like, and can develop a clear strategy for building this out
Now, Let's Talk About Our Inspiring Work Environment
🇦🇺 Based in Melbourne, Australia, our vibrant team of 28 (and growing rapidly) is making waves in the industry.
🍺 Fun-fact: Our office overlooks the oldest building in Australia, an enchanting old Irish pub. Experience history and innovation coming together!
🎲 We believe in fostering a healthy work-life balance, board games, and top-notch stand-up desk workstations. It's all about creativity and collaboration.
🏡 Autonomy is our guiding principle, which is why we embrace hybrid work. Come in when you need to, or work at your optimal hours—whether that's burning the midnight oil or rising with the sun.
And Here's What We Offer
😎As an early member of the Blinq family, you'll enjoy a one-of-a-kind chance to influence the company's direction in a dynamic, self-managed, and results-driven startup environment. Say goodbye to corporate nonsense and micro-management because here, you are your own boss. We believe in empowering our team members to unleash their full potential.
💸Equity in the business and a competitive salary: We value our team members and want to ensure they share in our success.
✨But here's the real magic: at Blinq, we're not just creating innovative solutions – we're creating a culture that thrives on transparency, autonomy, collaboration, and big ideas. We believe in celebrating individuality and encouraging out-of-the-box thinking. With us, you'll be inspired to push boundaries, drive innovation, and ultimately leave a lasting impact on our users on both B2C and B2B realms
🚨 If you do not check all the boxes above, that is okay - we enthusiastically encourage you to apply!
We welcome individuals at all experience levels and take pride in being an equal opportunity employer committed to creating an inclusive and diverse workforce. Join us on this remarkable journey as we reshape the way people connect and network
Show more
Show less","Data analytics, Data infrastructure, Data pipelines, Data warehouse, Data strategy, Data quality, Data reporting, Data insights, Data visualization, Apache Superset, Google Analytics, AWS, GCP, Segment, Amplitude","data analytics, data infrastructure, data pipelines, data warehouse, data strategy, data quality, data reporting, data insights, data visualization, apache superset, google analytics, aws, gcp, segment, amplitude","amplitude, apache superset, aws, data infrastructure, data insights, data quality, data reporting, data strategy, dataanalytics, datapipeline, datawarehouse, gcp, google analytics, segment, visualization"
Senior Engineer - Employee Data Enrichment,Culture Amp,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/senior-engineer-employee-data-enrichment-at-culture-amp-3747537817,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"Join us on our mission to make a better world of work.
Culture Amp revolutionizes how over 25 million employees across 6,000 companies create a better world of work. As the global platform leader for employee experience, Culture Amp empowers companies of all sizes and industries to transform employee engagement, develop high performing teams, and retain talent via cutting-edge research, powerful technology, and the largest employee dataset in the world. The most innovative companies across the globe, such as Salesforce, PwC, KIND, SoulCycle, Celonis and BigCommerce depend on Culture Amp every day.
Culture Amp is backed by 10 years of innovation, leading capital venture funds, and offices in the U.S, U.K, Germany and Australia. Culture Amp is recognized as one of the world’s top private cloud companies by Forbes and one of the most innovative workplace companies by Fast Company.
Learn more about how Culture Amp can help you create a better world of work at cultureamp.com .
How you can help make a better world of work
As part of this team of amazing humans,
You will
Join a cross-functional team that is responsible for evolving and maintaining our pipeline for ingesting, enriching and sharing employee data across the Culture Amp platform.
Working with domain driven design, event sourcing and CQRS - you’ll help the team build new features to enrich employee data, allowing the Culture Amp platform to provide more meaningful insights across our product modules.
As a
Senior
Software Engineer
on one of our teams you can expect to:
Tackle a wide variety of technical problems across our codebases as you build features and help us move to a distributed and event-driven architecture.
Collaborate with other engineers in your team through pair programming, writing solution designs, or giving feedback on pull requests, to build shared understanding of your work.
Work with the product managers and designers in your team to understand customer problems and help to develop, refine, and then implement solutions.
Maintain and support the services your team is responsible for. Ensure our dependencies are up-to-date, observe and understand services in production, refactor code, and attend to issues raised by our customers.
You have
You have experience in building web-based software applications. You understand fundamental engineering concepts, scalability, security, and maintenance.
Your process involves documenting your work. You write code that is consistent, maintainable, and understood by your teammates.
You are comfortable working across the full stack of web service architecture, including frontend, backend, and devops.
You are proficient with Ruby, Go or TypeScript (or are willing to learn).
You understand the value of test-driven development and know how and when to add tests.
You’re self-motivated and can work independently. You take the initiative to ensure that you have everything you need to work efficiently and call for support when required.
You’re aligned with our values and demonstrate them through your working practice.
You are
You are a proactive and clear communicator. You value empathy and kindness and are able to articulate your ideas and feelings when writing or speaking, and are open to the perspectives others bring.
You seek to understand and solve customer problems. You think about how your work – at all levels – impacts our customers and improves their experience.
It’s not expected that any single candidate would check every box here — what’s important is that you arrive with a strong set of fundamentals, and that you’re committed to building the skills you’ll need to do great work. Similarly, while we use a range of specific technologies — Go, AWS, TypeScript, React, Kafka, Rails, Kotlin — you don’t need to have direct experience with them for this position and there’ll be opportunities to learn as part of the role.
We believe that inclusive businesses are better, not just for “company results”, but for the world. We have a strong commitment to Anti-Racism, and endeavor to lead by example. Every step we make as a business towards anti-racism is another step we can take to support our customers in making a better world (of work). You can see our current commitments to Anti-Racism here .
We ensure you have the tools you need to thrive both in and out of work.
MacBooks for you to do your best work
Share Options - it’s important to us that everyone is an owner and can share in our success
Excellent parental leave and in work support programme, - for those families to be
Flexible working schedule - where we can, let’s make work, work for you
Fun and inclusive digital, and in-person events
Most importantly, an opportunity to really make a difference in people’s lives.
Please keep reading...
Research shows that candidates from underrepresented backgrounds often don't apply for roles if they don't meet all the criteria – unlike majority candidates meeting significantly fewer requirements.
We strongly encourage you to apply if you’re interested: we'd love to know how you can amplify our team with your unique experience!
Thank you for taking the time to read this advert. If you decide to apply, as part of your application, we will ask you to complete voluntary diversity questions (excluding Germany). Please watch this video from our amazing DEI Leader, Aubrey Blanche to share more on why we collect the data and how we will use it.
Show more
Show less","Ruby, Go, TypeScript, Web development, Software design, Software architecture, Eventdriven architecture, DevOps, Distributed systems, Scalability, Security, Maintenance, Domaindriven design, Event sourcing, CQRS, Testdriven development, Pair programming, Pull requests, Documentation, AWS, Kafka, Rails, Kotlin","ruby, go, typescript, web development, software design, software architecture, eventdriven architecture, devops, distributed systems, scalability, security, maintenance, domaindriven design, event sourcing, cqrs, testdriven development, pair programming, pull requests, documentation, aws, kafka, rails, kotlin","aws, cqrs, devops, distributed systems, documentation, domaindriven design, event sourcing, eventdriven architecture, go, kafka, kotlin, maintenance, pair programming, pull requests, rails, ruby, scalability, security, software architecture, software design, testdriven development, typescript, web development"
Data Science Lead - Hawthorn East,Coles Group,"Hawthorn East, Victoria, Australia",https://au.linkedin.com/jobs/view/data-science-lead-hawthorn-east-at-coles-group-3742415274,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"We’ve been trusted to serve Aussie communities since 1914 and grown to become a top 30-listed on the ASX with 120,000 team members and a portfolio of iconic brands. At Coles Group, you’ll not only get to make a difference to millions of Aussie lives—you’ll also get to see your impact.
What You’ll Be Doing
The role is responsible for leading a world class team of data science practitioners to deliver best in class products and solutions to deliver customer, team member, and supplier experience while driving year on year EBIT improvement and innovation.
The role is required to ensure all products align with key design decisions, standards and patterns to assure scalability, pace and progressive attainment of intelligent automation across the retail value chain.
Key Responsibilities Will Include
Coach and develop direct reports, creating a cultural environment in which employees are empowered to continuously grow and learn by doing
Mentor team members in their personal and professional growth objectives
Effectively identify talent for development and manage under-performance when necessary, ensuring employees understand the expectations required of them, are provided with opportunities to develop, and improve
Role model an engaged and healthy culture within the team
Promote a culture of innovation in areas of machine learning, statistics and artificial intelligence to identify new solutions and new opportunities
Build the analytical expertise of the broader Advanced Analytics and Artificial Intelligence team by sharing novel analytical approaches and applications
Deliver high quality Data Science Products and solutions in line with leading practice
Research new opportunities in areas of prediction, regression, machine learning, text mining etc to identify new opportunities for future solutions.
Deliver a seamless and proactive service to stakeholders articulating customer opportunity, commercial benefit, analytical findings, and data recommendations
Showcase, benchmark and quantify the value and outcomes of intelligent automation and analytics-based decision making
Good things you’ll need
Analytics professional with a proven track record (5+ years) in advanced analytics / data science / Measurement science and/or Econometrics
Extensive Track record in designing and delivering AA solutions
Deep experience in key analytical techniques, e.g. regression, clustering, decision trees, machine learning, text mining etc.
Highly conversant with analytical applications -R, Python, Scala, Hive, Spark, SQL, Excel
Extensive experience with productivity applications - Git, Jira, Confluence
Extensive experience in experimental design
Masters or PhD degree in Computer Science, Artificial Intelligence, Machine Learning, Econometrics, Applied Statistics, Physics, Engineering or related field.
Taking on new opportunities and tough challenges with a sense of urgency, high energy, and enthusiasm
Creating new and better ways for the organisation to be successful
Consistently achieving results, even under tough circumstances
Business and commercial acumen
Good scientific research skills, regular literature search and evidence of ongoing investment
keep up to date with industry development and leading practices
Take your next step into something bigger, apply now
With us it’s not about the discounts (although you do get those), it’s about joining a team where your wellbeing and professional development is invested in and celebrating your contributions is the norm. And because everyone leads unique lives, we offer flexible work including work from home, additional leave and parental leave entitlements.
We’re continuing to build a gender equitable team, and a culture that’s just as diverse, inclusive and welcoming as the communities we serve. We encourage applications from people of all ages, cultures (including Aboriginal and Torres Strait Islander peoples), abilities, sexual orientation and gender identities.
We’re happy to adjust our recruitment process to support candidates with disability. Find out more in the ‘Our Recruitment Process’ section of our careers site.
Job ID 98674
Employment Type Full time
Show more
Show less","Data Science, Machine Learning, Statistics, Artificial Intelligence, Econometrics, Analytical Applications, R, Python, Scala, Hive, Spark, SQL, Excel, Git, Jira, Confluence, Experimental Design, Business Acumen, Commercial Acumen, Scientific Research, Regression, Clustering, Decision Trees, Text Mining","data science, machine learning, statistics, artificial intelligence, econometrics, analytical applications, r, python, scala, hive, spark, sql, excel, git, jira, confluence, experimental design, business acumen, commercial acumen, scientific research, regression, clustering, decision trees, text mining","analytical applications, artificial intelligence, business acumen, clustering, commercial acumen, confluence, data science, decision trees, econometrics, excel, experimental design, git, hive, jira, machine learning, python, r, regression, scala, scientific research, spark, sql, statistics, text mining"
"Senior Analyst, Investment Data",AustralianSuper,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/senior-analyst-investment-data-at-australiansuper-3774114471,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"We make a difference, delivering outcomes for millions of members.
Join a fast paced and growing team with a global reach.
Flexible and inclusive work environment for all
At AustralianSuper, we truly care about our colleagues.​ We know work and life are intertwined. That's why we support the diverse needs of everyone and have policies that enable us all to thrive and be truly flexible. We ensure diversity is celebrated for the opportunity it provides us all to learn and grow and deliver better outcomes for members.
Your new role
The role supports Investment Data management related activities, including the investment process to capture, maintain and improve Investment Data in order to produce complete, accurate and timely investment and different books of record (ABOR, IBOR and TBOR) to effectively manage the Fund's exposures on a daily basis.
The role works closely with the Manager, Investment Data to assist with leading and coaching Data Analysts. In addition, to lead initiatives and Data requests, including workflow design and implementation support.
Some key responsibilities but not limited to;
Ensure security set ups in the Data systems have accurate attributes by validating on data vendor platforms, translating term sheets, loan agreements and updating the Data systems with the correct values
Enrich security master file information as required, including analytical calculations
Ensure portfolio set up & maintenance is completed, including maintenance of other internal structures, such as fund & pricing hierarchies
Provide timely query resolution and escalate to manager or stakeholders, where appropriate
Undertake daily control tasks and monitor daily database updates to ensure accurate and timely data delivery
Demonstrate a deep and current understanding of investment instruments.
Establish and maintain strong working relationships with software & market data vendors, custodians, technology team and the investment department to resolve data and reporting issues, including the members of the internal management and dealing teams, to achieve best outcomes for the Fund.
Support the daily hand-off process between regions (Europe; America ; APAC) to ensure a smooth transition of responsibilities across the global business day
What you'll need
Minimum 5 years' experience in investment management or asset owner with investment operations capability.
Broad and deep understanding of financial markets and financial instruments across multiple asset classes
Strong understanding of data management functions and usage of investment data in its various forms, structures and outcomes to meet end user requirements.
Sound understanding of the nuances between the investment (IBOR) and accounting (ABOR) books of record and how these might be applied in an Asset Management and Asset Owner context
Experience with process improvement and ability to continually identify better ways of working
Ability to influence internal and external stakeholders by having strong attention to detail and deep knowledge of the processes.
Experience in Aladdin is essential
Experience in Bloomberg and Refinitiv highly desirable
Experience in data tools such as Power BI and Python desirable
Life at AustralianSuper
AustralianSuper is committed to colleague development, and we support our people with ongoing learning, coaching and training, as well as career opportunities across our expanding global organisation. We offer generous leave entitlements and promote a blended working environment in which all roles can flex, and we're happy to discuss what this looks like for you.
We are committed to supporting our diverse workforce in a way that is inclusive and embraces diversity in all its forms. If you require any reasonable adjustments to the recruitment process or the role, please let our recruitment team know.
What's next
Apply now, if you share our values of Energy, Integrity, Generosity of Spirit and Excellent Outcomes and would like the opportunity to work in a challenging, growing and rapidly evolving team to deliver outstanding results.
Australian or New Zealand citizenship or Australian permanent residency status is required.
Progress powered by purpose.
Show more
Show less","Investment Data Management, Investment Process, Data Analysis, Data Systems, Data Security, Data Quality, Analytical Calculations, Financial Markets, Financial Instruments, Investment Data, Data Management, Investment Accounting, Process Improvement, Attention to Detail, Bloomberg, Refinitiv, Power BI, Python, Generosity of Spirit, Excellent Outcomes, Energy, Integrity","investment data management, investment process, data analysis, data systems, data security, data quality, analytical calculations, financial markets, financial instruments, investment data, data management, investment accounting, process improvement, attention to detail, bloomberg, refinitiv, power bi, python, generosity of spirit, excellent outcomes, energy, integrity","analytical calculations, attention to detail, bloomberg, data management, data quality, data security, data systems, dataanalytics, energy, excellent outcomes, financial instruments, financial markets, generosity of spirit, integrity, investment accounting, investment data, investment data management, investment process, powerbi, process improvement, python, refinitiv"
Senior Data Engineer - AWS,Mantel Group,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-aws-at-mantel-group-3785134490,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"About Us
Mantel Group is an Australian-owned technology consulting business with capabilities across Cloud, Digital, Data & Security. Since our inception in November 2017, we have experienced remarkable growth across Australia & New Zealand and are honoured to be recognised as a Great Place to Work for 4 years in a row!
We hire smart and talented people and get out of their way. As a principle-based organisation we have a flat structure with no hierarchy. By focusing on our five principles and not getting caught up in red tape, we trust you to get the job done!
Cloud
Mantel Group’s dedicated cloud capability means you are able to work with a team of passionate people and all the latest tech in your cloud of choice whether you want to help businesses reach their full potential with Microsoft products, be part of the full cloud transformation on AWS or an expert in Google cloud - there’s an opportunity here for you.
About The Role
As a key member of our AWS Data Engineering team, you’ll be responsible for designing and deploying client environments through code, developing data pipelines, building data models and advanced queries to help our clients harness the power of data powered by AWS.
As a Senior Data Engineer - AWS you'll be:
Solution designing and delivering using AWS tools within the Data and Analytics domain.
Building strong trusted relationships with new and existing clients through thought leadership, strong delivery and great relationships which align to Mantel Group’s principles
Building and deploying data lakes using AWS and AWS data-focused services such as Kinesis, Glue, Athena, S3 and RedShift
Developing complex data pipelines using Python, EMR and Lambda functions
Solving complex data engineering issues to help our clients advance their data and analytics solution
Providing guidance and mentorship to others within the team through on-the-job teaching, hands-on training and other similar activities
Representing Mantel Group within the data engineering community
We want to hear from you if you have:
Previous experience in a data engineering role at a consulting organisation
AWS data services experience in particular CloudFormation and/or Terraform), EMR, Glue RDS, S3, EC2, Redshift and other data infrastructure automation services
Infrastructure automation through DevOps scripting using - Python, R, SparkQL or PowerShell
Querying and managing data - SQL
Data modelling - Relational, Dimensional, NoSQL, Snowflake
Code Management System - GitHub, BitBucket or Gitlab
Experience with CI/CD practices & tools - Gitlab, Jenkins
Ability to engage with senior level stakeholders and having a broad influence in the internal and client’s teams
Experience supporting technical projects and project teams varying in size
What you can expect from us:
We know you won’t have one job for life. At Mantel Group we believe in supporting our team to take their career in a direction that aligns with their passions. We have internal opportunities across Cloud, Data, Security and Digital.
You’ll get all the tools you need to hit the ground running including a new phone, laptop & swag.
We believe in unique experiences for all. Our My Deal program allows you to tailor your yearly plan, with the support of your Leader, to decide on what’s most important to you. That might be extra professional development, extra annual or parental leave, time to work on your side hustle, or something else completely different! One size does not fit all.
You’ll be genuinely supported by an organisation that cares about not only you but your family as well, Mantel Group offers Flexible Personal Leave options for those unplanned moments in life.
We support a flexible hybrid approach to working which is guided by our principles; we trust each other to “make good choices” about the best workplace locations for the requirements of the project, role and client. This can change based on our client needs.
Click ‘Apply’ to be considered for this role and our Talent team will be in touch.
We value a diverse workplace and strongly encourage people from all backgrounds and minority groups to apply.
#EAST
Show more
Show less","Cloud, AWS, Microsoft, Google, Data Engineering, Data Pipelines, Data Models, Data Lakes, Kinesis, Glue, Athena, S3, RedShift, Python, EMR, Lambda, DevOps, SparkQL, PowerShell, SQL, Snowflake, GitHub, BitBucket, Gitlab, Gitlab CI/CD, Jenkins CI/CD, Terraform, CloudFormation, RDS, EC2","cloud, aws, microsoft, google, data engineering, data pipelines, data models, data lakes, kinesis, glue, athena, s3, redshift, python, emr, lambda, devops, sparkql, powershell, sql, snowflake, github, bitbucket, gitlab, gitlab cicd, jenkins cicd, terraform, cloudformation, rds, ec2","athena, aws, bitbucket, cloud, cloudformation, data engineering, data lakes, data models, datapipeline, devops, ec2, emr, github, gitlab, gitlab cicd, glue, google, jenkins cicd, kinesis, lambda, microsoft, powershell, python, rds, redshift, s3, snowflake, sparkql, sql, terraform"
Data Center Switching and Wireless Network Engineer,Accenture Australia,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-center-switching-and-wireless-network-engineer-at-accenture-australia-3775604807,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"About Accenture
Accenture is a global professional services company with leading capabilities in digital, cloud and security. Combining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services and Accenture Song — all powered by the world’s largest network of Advanced Technology and Intelligent Operations centers. Our 738,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. We embrace the power of change to create value and shared success for our clients, people, shareholders, partners and communities. Visit us at www.accenture.com.
Role Responsibilities
As a Data Center Switching and Wireless Network Engineer, you will be responsible for managing the data center switching infrastructure and wireless networks to ensure high availability, performance, and security. Your role will involve working with cross-functional teams to support network operations, troubleshoot issues, and optimize network performance within the data center environment.
Skills Required
Possess 5-8 years of experience
Experience on Network - DC Switching ( Arista/ACI), Campus Switching/Wireless (Cisco/Aruba), Routing. Knowledge of Google cloud platform with network operations.
Knowledge of F5 and ACE Load Balancers (nice to have)
Prefer with CCNA/CCNP certified.,
Excellent troubleshooting skills
Diagnosis and Investigation of incidents
Participate at MIM (Major Incidents)
Coordination if multiple teams are involved during investigation and service restoration (e.g. infrastructure application support teams and/or multiple vendors or applications support teams)
Regular case/ticket updates (in agreed intervals)
Communications with the customer during the Incident Management process
Escalation (Technical and Management) to Service Measures
Resolution & Service Recovery to Service Measures, including organising (equipment) faulty parts replacement
Closure to Service Measures
Excellent customer focus.
Proven ability to manage multiple projects
Proactive work ethic
Available to work after hours as required
Ability to work with minimal supervision
Location:
Melbourne preferred. Adelaide, Canberra and Sydney considered
At Accenture, we recognise that our people are multi-dimensional, and we create a work environment where all people feel like they can bring their authentic selves to work, every day.
Our unwavering commitment to inclusion and diversity unleashes innovation and creates a culture where everyone feels they have equal opportunity. Our range of progressive policies support flexibility in ‘where’, ‘when’ and ‘how’ our people work to ensure that Accenture is an organisation where you can strive for more, achieve great things and maintain the balance and wellbeing you need.
We encourage applications from all people, and we are committed to removing barriers to the recruitment process and employee lifecycle. All employment decisions shall be made without regard to age, disability status, ethnicity, gender, gender identity or expression, religion or sexual orientation and we do not tolerate discrimination. If you require adjustments to the recruitment process or have a preferred communication method, please email exectalent@accenture.com and cite the relevant Job Number, or contact us on +61 2 9005 5000.
To ensure our workplace is inclusive and diverse we are setting bold goals and taking comprehensive action. To achieve these goals, we collect information that allows us to track the effectiveness of our Inclusion and Diversity programs. Learn how Accenture protects your personal data and know your rights in relation to your personal data. Read more about our Privacy Statement.
Equal Employment Opportunity Statement for Australia:
At Accenture, our philosophy is anchored in recognising that our people are multi-dimensional. We take an intersectional human approach to create a work environment where all people feel like they can bring their authentic selves to work, every day.
We believe that equality drives innovation. Our commitment to accelerating equality starts at the top with our board and CEO and extends across every part of the company. This comes to life when our people own the equality agenda, making it part of their jobs – every decision, every day – and feel free to speak up and to act.
We do not tolerate discrimination because of differences, such as age, ability, ethnicity, gender, gender identity or expression, religion, or sexual orientation. We want a workplace that is inclusive and diverse to that end we are setting bold goals and taking comprehensive action. To achieve these goals, we collect information that allows us to track the effectiveness of our Inclusion and Diversity programs.
Show more
Show less","Data Center Switching, Wireless Networks, Network Operations, Troubleshooting, Network Performance Optimization, Google Cloud Platform, F5 Load Balancers, ACE Load Balancers, CCNA, CCNP, Incident Management, Customer Focus, Project Management, Proactive Work Ethic, AfterHours Availability, Minimal Supervision","data center switching, wireless networks, network operations, troubleshooting, network performance optimization, google cloud platform, f5 load balancers, ace load balancers, ccna, ccnp, incident management, customer focus, project management, proactive work ethic, afterhours availability, minimal supervision","ace load balancers, afterhours availability, ccna, ccnp, customer focus, data center switching, f5 load balancers, google cloud platform, incident management, minimal supervision, network operations, network performance optimization, proactive work ethic, project management, troubleshooting, wireless networks"
Data Engineer (Senior - Lead),Mantel Group,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-senior-lead-at-mantel-group-3752405384,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"Mantel Group is an Australian-owned technology consulting business with capabilities across Cloud, Digital, Data & Security. Since our inception in November 2017, we have experienced remarkable growth across Australia & New Zealand and are honoured to be recognised as a Great Place to Work for 4 years in a row!
We hire smart and talented people and get out of their way. As a principle-based organisation we have a flat structure with no hierarchy. By focusing on our five principles and not getting caught up in red tape, we trust you to get the job done!
Data at Mantel Group
Working in data at Mantel Group means there’s opportunities to align to a particular cloud platform or remain cloud agnostic. Our teams are made up of awesome people who specialise in numerous areas of data so there’s endless opportunities for you including data engineering, machine learning, data analytics and data science.
Here Are Some Things You’ll Be Doing
Building and deploying data lakes and data warehouses on GCP using services including BigQuery, Dataflow, DataFusion, Pub/Sub, Composer(Airflow), CloudSQL, Cloud Functions and GCS
Developing complex code-based ETL/ELT data pipelines with performance optimised data modelling
Massive volumes of data ingestion from a variety of sources (DBs, APIs, streaming data feeds, IOT, and other clouds, etc.)
Event driven processing
BigData processing - handling PII, data formatting & wrangling, data preparation, etc.
Analytics - BigQuery, denormalisation of data, large views, driving analytics & reporting
BI - deep & interactive visualisations with Looker, DataStudio, PowerBI, etc.
Enabling ML & AI - Natural Language Processing, Google's ML APIs to work with audio/video files, language translations, image processing, and much more
What’s On Offer
We know you won’t have one job for life. At Mantel Group we believe in supporting our team to take their career in a direction that aligns with their passions. We have internal opportunities across Cloud, Data, Security and Digital.
You’ll get all the tools you need to hit the ground running including a new phone, laptop & swag.
We believe in unique experiences for all. Our My Deal program allows you to tailor your yearly plan, with the support of your Leader, to decide on what’s most important to you. That might be extra professional development, extra annual or parental leave, time to work on your side hustle, or something else completely different! One size does not fit all.
You’ll be genuinely supported by an organisation that cares about not only you but your family as well, Mantel Group offers Flexible Personal Leave options for those unplanned moments in life.
We support a flexible hybrid approach to working which is guided by our principles; we trust each other to “make good choices” about the best workplace locations for the requirements of the project, role and client. This can change based on our client needs.
Keen to chat?
Click “
Apply
” to share your details with our talent team, and we’ll be in touch.
We value a diverse workplace and strongly encourage people from all backgrounds and minority groups to apply.
In the meantime, check out ‘
how we hire
’ (scroll to the bottom!) to find out what’s in store if you’re successful and get to know us better by visiting our website and following Kasna on LinkedIn.
We keep this role open and continue to advertise because we're always on the lookout for awesome individuals. We understand that finding the right fit is also about the right timing!
#EAST
Show more
Show less","Cloud, Digital, Data, Security, BigQuery, Dataflow, DataFusion, Pub/Sub, Composer, Airflow, CloudSQL, Cloud Functions, GCS, ETL, ELT, BigData, PII, Looker, DataStudio, PowerBI, Machine learning, Natural Language Processing, Google's ML APIs","cloud, digital, data, security, bigquery, dataflow, datafusion, pubsub, composer, airflow, cloudsql, cloud functions, gcs, etl, elt, bigdata, pii, looker, datastudio, powerbi, machine learning, natural language processing, googles ml apis","airflow, bigdata, bigquery, cloud, cloud functions, cloudsql, composer, data, dataflow, datafusion, datastudio, digital, elt, etl, gcs, googles ml apis, looker, machine learning, natural language processing, pii, powerbi, pubsub, security"
Data Engineer - Data Platforms,Merkle Australia,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-data-platforms-at-merkle-australia-3784296583,2023-12-17,Victoria, Australia,Mid senior,Hybrid,"Merkle
is a leading data-driven Customer Experience Management (CXM) company that specializes in the delivery of unique, personalized customer experiences across platforms and devices.
We are a team of dreamers, doers, and deliverers. Our collaborative and agile ways of working mean you’ll develop versatile skills and be exposed to new and exciting thinking. Combine that with a pioneering spirit aimed at making a positive impact on brands, society and each other – and you’ll soon realise why we’re at the forefront of the digital economy.
The Merkle Data and Analytics Team now has an opportunity for our next
Data Engineer – Data Platforms.
This role could be based in Melbourne or Sydney.
The role:
As our next
Data Engineer
you will demonstrate strong data development and analysis skills to help deliver outstanding outcomes for our customers through the use of data. The role requires strong SQL skills and a core understanding of cloud computing services such as AWS, Azure, GCP and Snowflake. It also requires a fundamental understanding of core data concepts such as data integration or Extract/Transform/Load (ETL), data modelling/transformation and data visualisation.
Core responsibilities include:
Develop data solutions based on a set of requirements
Deliver required data project work streams
Develop relationships with the customers you work with through delivering solutions and proving value
Identify opportunities at clients to improve efficiencies or drive more business for them
A bit about you:
Excellent stakeholder engagement style with genuine passion for consulting
Strong knowledge of SQL Scripting
Strong Knowledge of Cloud Services such as AWS/Azure/GCP/Snowflake. Ideally certified on one or more of these platforms
Understanding of core data warehouse concepts such as data modelling, ETL/integration and data visualisation
Thirst for knowledge/improvement
What’s in it for you:
Real flexibility to work when and where you feel most productive with our ‘Be the Best You’ policy which includes a $250 contribution to your home office set up
The opportunity to give back with dedicated volunteer leave and our whole-company event, One Day for Change
5 weeks annual leave after 2 years of service and 3 whole company Wellness days off per year for you to switch off and take your day, your way
Career Development and Learning & Development opportunities, including access to our global online dentsu University
This role will see you maintain, improve, clean and manipulate data in the business’s operational and analytics databases. You will work with the business’s software engineers, data analytics teams, data scientists and data warehouse engineers in order to understand and aid in the implementation of database requirements, analyse performance, and troubleshoot any existent issues.
Become a champion for meaningful progress:
Our diverse and inclusive culture enables our employees to bring their whole selves to work and be proud of doing so. For us, this is the foundation for great innovation which, in turn, generates better outcomes for our people, partners and communities. This is why we encourage applications from people with disabilities, and of all ages, nationalities, backgrounds and cultures.
If you want to belong to a one global network where opportunities are limitless – we look forward to welcoming you.
Show more
Show less","AWS, Azure, Snowflake, GCP, SQL, Cloud Services, ETL, Data Visualization, Data Modelling, Data Integration, Data Warehousing, Stakeholder Engagement, Database Management, Database Troubleshooting, Data Analysis","aws, azure, snowflake, gcp, sql, cloud services, etl, data visualization, data modelling, data integration, data warehousing, stakeholder engagement, database management, database troubleshooting, data analysis","aws, azure, cloud services, data integration, data modelling, dataanalytics, database management, database troubleshooting, datawarehouse, etl, gcp, snowflake, sql, stakeholder engagement, visualization"
Data Analyst,"Informatic Technologies, Inc.","Chicago, IL",https://www.linkedin.com/jobs/view/data-analyst-at-informatic-technologies-inc-3783925563,2023-12-17,Brookfield,United States,Mid senior,Onsite,"This is onsite role in Chicago, IL only on w2
You will be working within the Marketing Insights team and working closely with other individuals within the marketing department as well as individuals from Data Science and Business Intelligence. You will be responsible for documenting and reviewing the digital journey of Client clients, across multiple databases. The databases will contain a mix of related and unrelated data and it will be your responsibility to map clients across both.
Responsibilities
Utilize various databases and tools to identify individuals across different data sources
Compare and cross-reference data from multiple databases to identify potential matches
Analyse and evaluate the data to determine the accuracy of potential matches
Continuously improve matching techniques and processes to enhance efficient and accuracy
Communicate effectively with team members and stakeholders to provide updates and share insights
Skills
Analytical mindset with the ability to interpret and evaluate complex data sets
Knowledge of data analysis tools, including advanced skills in Excel
Experience in handling large data sets and relational databases
Excellent attention to detail and the ability to accurately identify patterns and matches
Strong verbal and written communication skills to effectively share findings with shareholders
Experience with SQL and Tableau is desirable
Show more
Show less","Data Analysis, Excel, SQL, Tableau, Relational Databases, Data Matching, Data Evaluation, Communication, Attention to Detail, Pattern Recognition","data analysis, excel, sql, tableau, relational databases, data matching, data evaluation, communication, attention to detail, pattern recognition","attention to detail, communication, data evaluation, data matching, dataanalytics, excel, pattern recognition, relational databases, sql, tableau"
Data Integration Engineer (REMOTE),Sears,"Hoffman Estates, IL",https://www.linkedin.com/jobs/view/data-integration-engineer-remote-at-sears-3771222423,2023-12-17,Brookfield,United States,Mid senior,Remote,"Job Description
JOB SCOPE:
This position will report directly to the Sr Dir, Technology for Sears Home Services and will support financial platform technology integrations across the Sears Home Services organization.
Job Summary
The Data Integration Engineer is responsible for designing, developing, testing, and deploying data integrations across multiple platforms both within middleware and through API connections. They will work with stakeholders to understand data requirements and develop data models. They will also select and implement the appropriate data integration tools and technologies.
Responsibilities/Skills/Experience Requirements
JOB DUTIES/RESPONSIBILITIES:
Designs, develops, tests, and deploys data integration solutions with a focus on financial data accuracy and integrity
Collaborates with stakeholders and cross-functional teams in the Finance Business Unit to understand financial data requirements and develops financial data models that can be used for further analysis
Evaluates, selects, and implements appropriate data integration tools and technologies, with a focus on financial data security and efficiency
Develops and executes comprehensive testing plans to ensure the accuracy and reliability of financial data integration solutions;
Continuously monitors and troubleshoots data integrations if issues arise
Creates and maintains documentation for financial data integration processes, ensuring compliance with regulatory requirements and facilitating knowledge transfer within the team
Stays updated on industry best practices and emerging technologies in financial data integration
Ensures compliance with financial data regulations and industry standards
Communicates effectively with both technical and non-technical team members to ensure successful implementation of financial data integration projects
REQUIRED SKILLS:
Bachelor's degree in Computer Science, Information Systems, or a related field
3+ years of experience in data integration
Experience with SQL, NoSQL, and data warehousing technologies
Experience with data modeling and data quality
Experience with cloud computing platforms (AWS, Azure, GCP)
Strong programming skills (Python, Java, etc.)
Excellent problem-solving and analytical skills
Ability to work independently and as part of a team
Experience with cloud-based data integration platforms such as AWS Glue or Azure Data Factory
Experience with machine learning and artificial intelligence
PREFERRED SKILLS:
Experience with data integration tools such as Boomi, Informatica, or MuleSoft
Experience with big data technologies such as Hadoop and Spark
Years Experience
2 - 5 Years Experience
Travel Requirements
On Occasion (Less than 5%)
Country
United States
Work-In Address 1
REMOTE
Work-In Address 2
REMOTE
Work-In City
REMOTE
Work-In State
REMOTE
Work-In Postal Code
REMOTE
Business
Transformco Home Services - Support
Job Function
Information Technology
Employment Category
Regular, Full-time
Compensation Range
NA
Additional Compensation Explanation
N/A
EEO/EOE Footer
Equal Opportunity Employer / Disability / Vet.
Posting Tags
#HSCorporate
Company Brand
Sears Home Services
Location City
HOFFMAN ESTATES
Show more
Show less","Data Integration, Data Modeling, Data Warehousing, Cloud Computing, Programming (Python Java), Data Quality, Cloud Platforms (AWS Azure GCP), Data Integration Platforms (AWS Glue Azure Data Factory), Machine Learning, Artificial Intelligence, Boomi, Informatica, MuleSoft, Hadoop, Spark","data integration, data modeling, data warehousing, cloud computing, programming python java, data quality, cloud platforms aws azure gcp, data integration platforms aws glue azure data factory, machine learning, artificial intelligence, boomi, informatica, mulesoft, hadoop, spark","artificial intelligence, boomi, cloud computing, cloud platforms aws azure gcp, data integration, data integration platforms aws glue azure data factory, data quality, datamodeling, datawarehouse, hadoop, informatica, machine learning, mulesoft, programming python java, spark"
Data Mining and Analytics Engineer (Senior),ICF,"Pensacola, FL",https://www.linkedin.com/jobs/view/data-mining-and-analytics-engineer-senior-at-icf-3710533480,2023-12-17,Pensacola,United States,Mid senior,Hybrid,"ICF International seeks an experienced Senior Data Mining and Analytics Engineer to support the research and development of new cyber analytic capabilities that will help the US protect and defend its networks and critical information systems. The successful cleared candidate will act as a Senior Data Mining and Analytics Engineer to support a large federal cyber security analytic program. Your work will contribute to the knowledge of how cyber-attacks work, how vulnerabilities are exploited, and the way hostile cyber actors operate.  Utilize your skills to help experiment and prototype future cyber capabilities for implementation at large scale.
As the Senior Data Mining and Analytics Engineer, your exceptional skillset will create useful and actionable insight for the customer through the development of analytic solutions (hardware, analytics, tools, techniques, practices, deployment, standards, performance specifications, etc.) for analytic use cases developed during the performance of this project. You will work closely with the Analytics Research team to identify platform enhancements that support the forward-looking analytics under consideration.
The ideal candidate has extensive knowledge of a wide variety of systems and networks including high-volume/high-availability systems. You are focused on results, a self-starter, and have demonstrated success in using analytics to drive the understanding, growth, and success of the analysis. This is an opportunity to contribute to an important project from its beginning, work with the latest and emerging technologies, and all while building a great career at ICF!
This role is primarily telework-based with occasional meetings at client locations (Arlington, VA or Pensacola, FL) or ICF facilities within the National Capital Region.
What You Will Be Doing
Perform knowledge elicitation from customer subject matter experts and convert that to build analytic solutions
Design, engineer, and optimize sustainment of large-scale distributed computation platforms and supporting environments (ecosystems) for various stakeholders, business owners, and industry partners
Manage teams of system administrators, prioritize work, identify high-risk critical problems, and dedicate appropriate resources
Oversee the transition of services from third-party vendors to the analytic environment and be responsible for ad hoc and formal end-user training
Identify applicable data to perform analytics and create solutions to acquire, transform, and load or correlate data components to and from the analytic environment
Develop custom data modeling procedures to assist with data mining, modeling, and production
Assess the effectiveness and accuracy of new data sources and data gathering techniques
Develop processes and tools to monitor and analyze model performance and data accuracy
Interpret and communicate results to non-technical customers
What You Must Have
Active high-level security clearance required as part of client contract requirements
US Citizenship required as part of client contract requirements
Bachelor’s degree with 12+ or Master’s degree with 10+ years of experience in Computer Science, Mathematics, Engineering, or related field
Practical working experience and advanced knowledge of cyber threats, tools, techniques, and processes.
Strong experience in data modeling and working with datasets of all sizes using a variety of data mining and data analysis methods/tools
Preferred Skills/Experience
Interpersonal skills and the ability to communicate effectively with various clients in order to explain and elaborate on technical details
Experience in developing analytic tools, processes, and governance for storing, modeling, capturing, and delivering data to the client’s enterprise
Experience with computational notebook software such as Zeppelin or Jupyter
Experience with the application of visual analytics to computational analytic results
Fluency in one or more programming languages (e.g., Python, JavaScript, R, etc.)
Experience with database querying like SQL
Readiness to collaborate with engineering teams, product teams, and customers to develop prototypes and software products
Scaled Agile Framework (SAFe) experience
Amazon Web Services (AWS) Certified Cloud Practitioner or higher desired
CompTIA Security+ or higher cybersecurity certification preferred
Working at ICF
ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our EEO & AA policy.
Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: Know Your Rights and Pay Transparency Statement.
Pay Range
- There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
$85,153.00 - $144,760.00
Virginia Client Office (VA88)
Show more
Show less","Data Mining, Analytics, Cyber Security, Cyber Threats, Data Modeling, Machine Learning, Python, JavaScript, R, SQL, Zeppelin, Jupyter, AWS, SAFe, Cloud Computing","data mining, analytics, cyber security, cyber threats, data modeling, machine learning, python, javascript, r, sql, zeppelin, jupyter, aws, safe, cloud computing","analytics, aws, cloud computing, cyber security, cyber threats, data mining, datamodeling, javascript, jupyter, machine learning, python, r, safe, sql, zeppelin"
Lead Database Engineer,Leidos,"Pensacola, FL",https://www.linkedin.com/jobs/view/lead-database-engineer-at-leidos-3744504939,2023-12-17,Pensacola,United States,Mid senior,Hybrid,"Description
Leidos is seeking an experienced Lead Database Engineer for the role of Core Infrastructure Senior – Linux in support of the Defense Information Systems Agency (DISA) Unified Cyber Situational Awareness (UCSA) program in Pensacola, FL. The position is primarily responsible for the planning, development, implementation, maintenance, troubleshooting and recovery of mission critical enterprise Databases hosted on VM, Cloud, and physical Linux systems. Additionally, the successful candidate will also be responsible for analyzing operational requirements, affecting mission critical systems supporting the Department of Defense (DoD).
Primary Responsibilities
Administer and Support existing Databases and the migration of these applications and services into a new integrated network of cyber capabilities.
Provide operational Core Team support functions for UCSA Enterprise systems. These Include but are not limited to:
MySQL, PostgreSQL, and AWS cloud based databases
Automation platforms, such as Ansible
Proficient in Red Hat Enterprise Linux 7/8
Security Compliance:
OS Patching and Security Technical Implementation Guidelines (STIGs)
Database Patching as well as STIGs and SRGs
Experience with Enterprise Storage, such as Dell VNX series storage arrays
Disaster Recovery and Continuity of Operations (COOP) testing
Others as directed
Support the Core Services Manager and PMO in scheduling service efforts and aid in effective decision making.
Provide ongoing monitoring and reporting, meeting support, communications and document support, information sharing, and cross-program coordination.
Maintain quality service by enforcing organization standards, security posture, and compliance.
Basic Qualifications
Active Secret level security clearance and a current DoD 8570 approved certification.
Bachelor's degree in Computer Science or a related field and ten (10) years of direct experience.
Significant experience with Systems/Network Integration, Administration and Deployment of new Hardware, Software, and Services, Back-up Recovery Change & Configuration Management support, and Maintenance in support of all Database Services.
Ability to Lead, Collaborate, and Support the Core Infrastructure Team and UCSA Enterprise as a Database Engineering Subject Matter Expert.
Experience with Defensive Cyber Systems and Operations
Experience with DISA Defensive Cyber Platforms
Familiarity with SAFe Agile
Pay Range
Pay Range $101,400.00 - $183,300.00
The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.
Original Posting Date
12/11/2023
While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.
Show more
Show less","MySQL, PostgreSQL, AWS, Ansible, Red Hat Enterprise Linux, OS Patching, Security Technical Implementation Guidelines (STIGs), Database Patching, Dell VNX series storage arrays, Disaster Recovery, Continuity of Operations (COOP), SAFe Agile, Systems Integration, Network Integration, Administration, Deployment, Hardware, Software, Services, Backup Recovery, Change & Configuration Management, Support, Database Services","mysql, postgresql, aws, ansible, red hat enterprise linux, os patching, security technical implementation guidelines stigs, database patching, dell vnx series storage arrays, disaster recovery, continuity of operations coop, safe agile, systems integration, network integration, administration, deployment, hardware, software, services, backup recovery, change configuration management, support, database services","administration, ansible, aws, backup recovery, change configuration management, continuity of operations coop, database patching, database services, dell vnx series storage arrays, deployment, disaster recovery, hardware, mysql, network integration, os patching, postgresql, red hat enterprise linux, safe agile, security technical implementation guidelines stigs, services, software, support, systems integration"
Lead Database Engineer with Security Clearance,ClearanceJobs,"Pensacola, FL",https://www.linkedin.com/jobs/view/lead-database-engineer-with-security-clearance-at-clearancejobs-3753458931,2023-12-17,Pensacola,United States,Mid senior,Hybrid,"R-00121113 Description Leidos is seeking an experienced Lead Database Engineer for the role of Core Infrastructure Senior
– Linux in support of the Defense Information Systems Agency (DISA) Unified Cyber Situational Awareness (UCSA) program in Pensacola, FL. The position is primarily responsible for the planning, development, implementation, maintenance, troubleshooting and recovery of mission critical enterprise Databases hosted on VM, Cloud, and physical Linux systems. Additionally, the successful candidate will also be responsible for analyzing operational requirements, affecting mission critical systems supporting the Department of Defense (DoD). Primary Responsibilities
Administer and Support existing Databases and the migration of these applications and services into a new integrated network of cyber capabilities.
Provide operational Core Team support functions for UCSA Enterprise systems. These Include but are not limited to:
MySQL, PostgreSQL, and AWS cloud based databases
Automation platforms, such as Ansible
Proficient in Red Hat Enterprise Linux 7/8
Security Compliance:
OS Patching and Security Technical Implementation Guidelines (STIGs)
Database Patching as well as STIGs and SRGs
Experience with Enterprise Storage, such as Dell VNX series storage arrays
Disaster Recovery and Continuity of Operations (COOP) testing
Others as directed
Support the Core Services Manager and PMO in scheduling service efforts and aid in effective decision making.
Provide ongoing monitoring and reporting, meeting support, communications and document support, information sharing, and cross-program coordination.
Maintain quality service by enforcing organization standards, security posture, and compliance. Basic Qualifications
Active Secret level security clearance and a current DoD 8570 approved certification.
Bachelor's degree in Computer Science or a related field and ten (10) years of direct experience.
Significant experience with Systems/Network Integration, Administration and Deployment of new Hardware, Software, and Services, Back-up Recovery Change & Configuration Management support, and Maintenance in support of all Database Services.
Ability to Lead, Collaborate, and Support the Core Infrastructure Team and UCSA Enterprise as a Database Engineering Subject Matter Expert.
Experience with Defensive Cyber Systems and Operations
Experience with DISA Defensive Cyber Platforms
Familiarity with SAFe Agile Pay Range: Pay Range $101,400.00 - $183,300.00 The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law. Original Posting Date: 12/11/2023 While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.
Show more
Show less","Linux, MySQL, PostgreSQL, AWS, Ansible, Red Hat Enterprise Linux, Security Compliance, STIGs, SRGs, Dell VNX series storage arrays, Disaster Recovery, Continuity of Operations, Database Engineering, Defense Cyber Systems, DISA Defensive Cyber Platforms, SAFe Agile","linux, mysql, postgresql, aws, ansible, red hat enterprise linux, security compliance, stigs, srgs, dell vnx series storage arrays, disaster recovery, continuity of operations, database engineering, defense cyber systems, disa defensive cyber platforms, safe agile","ansible, aws, continuity of operations, database engineering, defense cyber systems, dell vnx series storage arrays, disa defensive cyber platforms, disaster recovery, linux, mysql, postgresql, red hat enterprise linux, safe agile, security compliance, srgs, stigs"
Systems Developer / Data Analyst,The ONE Group Ltd,"Kettering, England, United Kingdom",https://uk.linkedin.com/jobs/view/systems-developer-data-analyst-at-the-one-group-ltd-3765764016,2023-12-17,Leicester, United Kingdom,Associate,Onsite,"The One Group are collaborating with a dynamic team in
Kettering
to fill a key position for a
Junior-Mid Level Systems Developer
/
Data Analyst
. This is an excellent opportunity to join a close-knit team, and the role is based in our client's office.
Our client stands as a leading force in the manufacturing and marketing of top-tier tools and equipment designed for professional use in the transportation industry. Their reach spans across automotive, heavy-duty, equipment, marine, aviation, and railroad sectors.
As a
Systems Developer/Data Analyst
, you will play a crucial role in enhancing business value. Your responsibilities will include generating data insights, streamlining processes, and boosting overall efficiency. This involves the creation, review, analysis, and evaluation of reporting and applications. A comprehensive understanding of end-to-end processes across various business units is key to your success in this role. Please take a look at the core requirements below and apply for a more detailed conversation.
The Role
Solution Development:
Explore alternative solutions to MS Database, including Office 365 and web-based options for global business units.
Technical Support and Application Management:
Serve as primary technical support for custom integration applications.
Support and develop business reporting and scheduling servers.
Data Management and Security:
Manage data sets to ensure integrity and security policies are followed.
Conduct open data queries for process analysis and maintain SQL queries and MS Access scripts.
Documentation and Change Management:
Collaborate with stakeholders to document and enhance business processes and reports.
Create SOPs, FAQs, and guides for internal applications, enforcing change management processes.
Reporting, Training, and Efficiency:
Design reports using Tableau and SSRS.
Provide training for self-servicing of management information.
Work with IT infrastructure teams to ensure efficient data and reporting structures align with business strategy.
About You
Essential:
Strong process logic skills.
Experience with Tableau , Power BI , SSRS , and SQL server technologies.
Creative problem-solving ability.
Proficient in data analysis, process mapping, and Microsoft applications.
Desirable:
Experience in business requirement documentation.
Database development in MS Access / VBScript.
Familiarity with Microsoft Power Platform.
Expertise in migrating MS Access solutions to Power Platform, SQL Server, or Tableau.
The Benefits
£28-35k salary dependent on skills/experience
Company events
Company pension
Cycle to work scheme
Health & wellbeing programme
Life insurance
Keywords:
SQL / Power Bi / SSRS / SSIS / Tableau / Data / Database
For more information, and a full description please contact
Sam Payne
at
The One Group
for an informal conversation. Please note this role does not offer sponsorship.
IND1
Show more
Show less","MS Database, Office 365, Tableau, SSRS, SQL Server, Power BI, MS Access, VBScript, Microsoft Power Platform, Process Mapping, Data Analysis","ms database, office 365, tableau, ssrs, sql server, power bi, ms access, vbscript, microsoft power platform, process mapping, data analysis","dataanalytics, microsoft power platform, ms access, ms database, office 365, powerbi, process mapping, sql server, ssrs, tableau, vbscript"
Data Analyst,Aldi UK,"Atherstone, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-aldi-uk-3504168071,2023-12-17,Leicester, United Kingdom,Associate,Hybrid,"A role in the National Supply Chain function is a great opportunity to play a key part in shaping the future department and its ways of working. You will gain valuable experience working on a global transformation project and have a significant role in being a source of analytical expertise within your function. You will form relationships with across all domains and across the Aldi global network of countries.
You will support the Supply Chain Team and will work with representatives from across all business and IT domains. As the department and the transformation project evolves, so will the role. As such you will require a proactive approach and an ability to be resilient, whilst managing multiple responsibilities. This is an excellent opportunity for someone with a passion for data, reporting and analytics, whilst working within the National Supply Chain Function, at the heart of the business.
The role would suit a data scientist with knowledge of R Studio, Python and analytics software, confidently working with Big Data and using analytical skills to make recommendations. Having the ability to effectively present this information, through visualization tools will be key. The individual will also be able to communicate effectively and share their reports and data sets both clearly and meaningfully to different stakeholders.
If you’re looking for a career with a business that means more to it’s colleagues, join #TeamAldi today!
Your New Role:
Managing a workload of data analyses sprints and report creation tasks
Collaborating with multiple departments to ensure that our Supply Chain teams have the tools they need to operate effectively
Solving complex problems with data insights
Working with IT and Supply Chain counterparts to successfully implement a key area of the global transformation project
To identify and communicate project information necessary for senior management decision making
Supports creation and maintenance of deployment project plans
About You:
Data analysis and maintenance, using a range of skills
Scripting and/or automation capabilities
Ability to create sophisticated business reports
A clear passion for and capability to solve problems
Create and deliver presentations to illustrate processes and share insights
What you'll get in Return:
Salary starting from £50,945 rising to £58,680
Monday to Friday, 8am to 4:30pm, with the opportunity of 2 days a week remote working
5 weeks’ annual leave plus Bank Holidays
In office flexi-time
Full training provided
Pension scheme
Private employee medical insurance after 4 years
Company sick pay scheme
Company maternity, paternity and adoption leave pay after 2 years
Long service rewards
Access to MyBenefits, where you can find a wide range of benefits, including our bike to work scheme, shopping and cinema discounts, health cash plan, a FREE will, travel and discounted gym memberships, as well as a host of benefits to support your financial and wellbeing needs.
Access to a free,24/7, confidential virtual GP service for all Aldi Colleagues and their children (under the age of 16)
Aldi is an equal opportunities employer. We’re committed to maintaining a diverse and inclusive workforce and are dedicated to promoting a culture of inclusion, providing an environment in which collaboration, respect and fairness are essential. We value diversity and are dedicated to treating all of our colleagues and prospective colleagues fairly and with respect.
If you’re looking for a career that gives you more, apply today!
Show more
Show less","Data analysis, Data maintenance, Scripting, Automation, R Studio, Python, Big Data, Visualization tools, Data insights, Business reports, Presentation skills, Project management, Collaboration, Problemsolving","data analysis, data maintenance, scripting, automation, r studio, python, big data, visualization tools, data insights, business reports, presentation skills, project management, collaboration, problemsolving","automation, big data, business reports, collaboration, data insights, data maintenance, dataanalytics, presentation skills, problemsolving, project management, python, r studio, scripting, visualization tools"
Data Analyst Assistant,Aldi UK,"Atherstone, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-assistant-at-aldi-uk-3504163676,2023-12-17,Leicester, United Kingdom,Associate,Hybrid,"We are on the lookout for an individual with an understanding of supply chain and data analytics or a graduate with a passion for working within supply chain.
In conjunction with key stakeholders, you will be responsible for delivering a high quality and cost efficient inbound supply chain both through operational and project management. You will be working as part of a growing and developing supply chain team, looking for continuous improvement within your role to simplify processes and maximise efficiencies.
This is a fantastic opportunity for you to develop within the business and play a key part in shaping the future of Aldi globally. Ready for more? Apply to join #TeamAldi today!
Your New Role
Maintain overall control and responsibility for all demand forecasts for all Key Business Stakeholders
Analyse sales trends and product behaviours to build future forecast profiles
Supervise large data sets for accuracy and consistency, whilst drawing meaningful conclusions from data trends
Management and control of multiple supply chain projects and tasks, working with Buying, RDCs, Stores and suppliers to deliver to agreed timeframes.
Internal and external stakeholder engagement and management of all levels of business.
Management and control of supply chain forecasting processes, KPI’s, development and continual improvements, to maximise availability of product into the RDC’s from suppliers, and DCs to stores.
Continuous improvement within role, simplifying processes and operations to maximise efficiencies.
Deliver successful Christmas and key events.
Support the delivery of the department strategy as required.
About You
GCSE or equivalent
Supply Chain work experience OR a recent graduate with a degree in any of the following - Supply Chain, Forecasting, Data Analytics, Business, Logistics, Maths, Economics or any other Supply Chain or Logistics degree.
Demonstrate understanding of supply chain processes
Experience in data analytics
Strong written and verbal communication skills with the ability to build effective relationships with stakeholders
Highly proficient with Microsoft Office Suite; especially Microsoft Excel.
Ability to manage a varied workload, conflicting deadlines and multiple projects simultaneously.
A strong desire to improve and simplify processes and ensure consistency wherever possible.
Strong attention to detail
What you'll get in Return
Starting salary from £34,690 rising to £40,510
Monday- Friday, 8am to 4:30pm with the opportunity of 2 days a week remote working.
5 weeks’ annual leave plus Bank Holidays
In-Office flexi time
Full training provided
Pension Scheme
Private employee medical insurance after 4 years
Company sick pay scheme
Company maternity, paternity and adoption leave pay after 2 years.
Long service rewards.
Access to MyBenefits, where you can find a wide range of benefits, including our bike to work scheme, shopping and cinema discounts, health cash plan, a FREE will, travel and discounted gym memberships, as well as a host of benefits to support your financial and wellbeing needs.
Access to a free, 24/7, confidential virtual GP service for all Aldi Colleagues and their children (under the age of 16)
Aldi is an equal opportunities employer. We’re committed to maintaining a diverse and inclusive workforce and are dedicated to promoting a culture of inclusion, providing an environment in which collaboration, respect and fairness are essential. We value diversity and are dedicated to treating all of our colleagues and prospective colleagues fairly and with respect.
If you’re looking for a career that gives you more, apply today!
Show more
Show less","Supply Chain Management, Data Analytics, Demand Forecasting, Data Visualization, Microsoft Office Suite, Excel, Stakeholder Management, Project Management, Communication Skills, Attention to Detail","supply chain management, data analytics, demand forecasting, data visualization, microsoft office suite, excel, stakeholder management, project management, communication skills, attention to detail","attention to detail, communication skills, dataanalytics, demand forecasting, excel, microsoft office suite, project management, stakeholder management, supply chain management, visualization"
Senior Data Analyst,BML Digital,"Coventry, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-bml-digital-3779200984,2023-12-17,Leicester, United Kingdom,Mid senior,Onsite,"Provide advanced analytical capabilities to multiple departments across the organisation including Digital, Marketing, Supply Chain, Finance, and HR. Explore data and provide actionable insights and recommendations to help the business make data-driven decisions.
Youll be responsible for identifying inefficiencies and input to designing business solutions to tackle critical business problems.
Salary and ways of working
£55,000/year
Hybrid working with 4 days per month on site.
Skills And Experience
SQL expert with over 2 years’ experience
Good PowerBI experience from designing and creating dashboards
Strong communication skills to present complex data topics to a non-data savvy audiences and across all stakeholder levels.
Commercial awareness and ability to see the ‘bigger picture’ scenarios and implications.
Nice To Have
Experience in design of experiments, inferential statistics, regression analysis, statistical modelling is a strong plus, but you don’t need to be a data scientist.
Experience with Azure Data Platform, Azure Synapse Notebook, and Databricks
A retail background will be useful, but no specific industry experience is required
Responsibilities
Lead meetings with internal customers to identify data analytics needs and scope projects including small scale statistical analyses, descriptive analytical projects.
Establish project strategy in collaboration with the team including internal other data analysts, data scientists, data engineers).
Participating in the definition of the requirements in terms of data format and structure and identifying the relevant statistical tools and techniques building on guidance from Lead Data Analyst / Lead Data Scientist to ensure the project is delivered when required.
Clean and manipulate data and employ analytical models and statistical methods to generate valuable insight based on the analytical / data science project lead’s recommendations.
Present results from analysis in a clear and actionable way with tangible recommendations to project stakeholders and management.
Participate in automation projects and assist Lead Data Scientist / Data Scientist with project delivery when necessary.
Operate in an agile and innovative team with flexible working conditions, share knowledge openly and have a very collaborative spirit, even while working remotely.
As a subject matter expert in Data Analytics, youll collaborate with Data Analysts and stakeholders across the business to showcase the value of data and promote a data-driven culture.
Show more
Show less","SQL, Power BI, Communication skills, Commercial awareness, Azure Data Platform, Azure Synapse Notebook, Databricks, Agile, Python, R, Statistical analysis, Regression analysis, Data mining, Data warehousing","sql, power bi, communication skills, commercial awareness, azure data platform, azure synapse notebook, databricks, agile, python, r, statistical analysis, regression analysis, data mining, data warehousing","agile, azure data platform, azure synapse notebook, commercial awareness, communication skills, data mining, databricks, datawarehouse, powerbi, python, r, regression analysis, sql, statistical analysis"
Senior Mission Data Engineer,BAE Systems,"Warton, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-mission-data-engineer-at-bae-systems-3774787963,2023-12-17,Leicester, United Kingdom,Mid senior,Hybrid,"Job title:
Senior Mission Data Engineer
Location:
Warton
Salary:
£52,084 – £57,344 (Commensurate with skills and experience)
What You’ll Be Doing
The Air Service Electronic Warfare & Test Solutions team design, develop and integrate a suite of electronic warfare products and provision Mission Data for the Eurofighter partner nations. These products allow the nations’ air forces to develop, test and validate their Typhoon mission data before being loaded to the aircraft.
The successful candidate will Lead the specification, design, development, qualification and delivery of Mission Data Sets for delivery to various export Customers.
The role is primarily based at Warton but the successful candidate will also be required to travel to supplier, partner and customer locations within the UK as well as internationally when required.
Your Main Responsibilities Will Involve
Extraction of Emitter Data from an RF Emitter Database.
Performance of RF Emitter Ambiguity Analysis.
Creation of DASS Receiver Search Strategies.
Programming of DASS Mission Data utilising the DASS Mission Data Preparation Programme Toolsets.
Mission Data Development and Test
Supporting Customer Trials / Mission Data validation
Providing responses to In Service Support Tasks
Supporting Rigs, Flight Test and Problem Investigation
Essential
Your skills and experiences:
Familiar with Electronic Warfare and its products
Competent in the use of Microsoft Office Products (Word, Excel & PowerPoint)
Ability to forge partnerships with internal and external customers.
Experience with specification, design, development, qualification and delivery of Mission Data Sets
Desirable
A degree educated in a STEM subject, HND/HNC with sound experience of the Engineering Lifecycle, or equivalent relevant experience
Benefits
You’ll receive benefits including a competitive pension scheme, enhanced annual leave allowance and a Company contributed Share Incentive Plan. You’ll also have access to additional benefits such as flexible working, an employee assistance programme, Cycle2work and employee discounts – you may also be eligible for an annual leave incentive.
Why BAE Systems?
This is a place where you’ll be able to make a real difference. You’ll be part of an inclusive culture that values diversity, rewards integrity, and merit, and where you’ll be empowered to fulfil your potential. We welcome candidates from all backgrounds and particularly from sections of the community who are currently underrepresented within our industry, including women, ethnic minorities, people with disabilities and LGBTQ+ individuals.
We also want to make sure that our recruitment processes are as inclusive as possible. If you have a disability or health condition (for example dyslexia, autism, an anxiety disorder etc.) that may affect your performance in certain assessment types, please speak to your recruiter about potential reasonable adjustments.”
Show more
Show less","Electronic Warfare, Data Analysis, Microsoft Office Suite, Data Preparation, Mission Data Development, DASS Mission Data Preparation Programme Toolsets, DASS Mission Data, Engineering Lifecycle, MATLAB, R, Python","electronic warfare, data analysis, microsoft office suite, data preparation, mission data development, dass mission data preparation programme toolsets, dass mission data, engineering lifecycle, matlab, r, python","dass mission data, dass mission data preparation programme toolsets, data preparation, dataanalytics, electronic warfare, engineering lifecycle, matlab, microsoft office suite, mission data development, python, r"
Senior Data Engineer,Boots UK,"Nottingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-boots-uk-3747628070,2023-12-17,Leicester, United Kingdom,Mid senior,Hybrid,"This is an exciting opportunity for experienced Data Engineer to join our team as Senior Data Engineer. Based out of Nottingham in our International Data Office, you will be responsible for the analytics engineering of data products within a specified data domain.
Who we are
International Technology & Advanced Analytics (IT2A) is a multi-national function of Walgreens Boots Alliance based in the UK, Europe, Asia, South America and the USA. We leverage emerging technology and robust data to transform our pharmacy and retail offerings, improve experiences for customers and patients and enhance operational effectiveness for team members. Partnering with some of the finest tech providers and utilising a cutting-edge tech stack, we support two of the UK’s most trusted and established brands - Boots and No7 – in personalising their customer interactions. We’re an innovative team of specialists spanning security, hosting, architecture, software engineering, networks, project delivery and more. A proud equal opportunity employer, we passionately embrace team member diversity and provide a positive and inclusive working environment for all.
About the role
Within our recently formed Data Office, the Data Platform team builds and maintains our self-serve data platform for our international divisions. Seeking to better align ourselves with our organisation needs, we take a business domain-oriented approach to organising our data engineering and data products. Data domains are very broad across our business across Retail, Pharmacy, Healthcare, Marketing, Product, Merchandising, Opticians etc
As a Senior Data Engineer you will lead a domain-focused, multi-disciplinary squad focused on building products that drive value from our data. Working with our business you’ll design and implement products that allow us to maximise the value of our data to our customers and patients.
Using your data engineering expertise, you’ll coach and support your team building them into a highly performing squad.
Hybrid Working – 3 days office & 2 days home
We offer our Support Office team members a flexible, hybrid working approach based upon empowerment, accountability, care and trust which includes a balanced mix of remote and office-based working - this will be discussed with you as part of the application process
.
What you’ll need to have
Hands-on’ experience of data engineering within analytics and/or data science environment.
Expert knowledge of one or more data languages including SQL, Python and Spark.
Experience of coaching and developing your peer’s knowledge and understanding of modern data engineering principles and practices.
Experience of data warehousing techniques such as Kimball dimensional modelling.
It would be great if you also have
Experience of developing large scale data engineering solutions in Microsoft Azure cloud technologies (preferably Databricks too)
Experience of developing data warehouses, Business Intelligence/Management Information, ML and AI solutions
Experience of line management of engineers
Knowledge of agile processes such as Scrum
Knowledge of Supply Chain or Pharmacy data domains
Our benefits
Boots Retirement Savings Plan
Generous employee discount plus enhanced discounts for Boots brands, Boots Opticians and Boots Hearingcare
Excellent onsite facilities including staff shop, opticians (including free eye tests for team members), gym, cafeteria, and outdoor seating spaces.
What’s next?
If your application is successful, our recruitment team will be in touch to arrange an interview and to answer any initial questions you have. If you have not been successful on this occasion, you will be notified by email.
We are always open to discussing possible flexible working options and what this may look like for you, including job share and reduced hours. If you require additional support as part of the application and interview process, we are happy to provide reasonable adjustments to enable you to be at your best.
Show more
Show less","Data engineering, Analytics, SQL, Python, Spark, Kimball dimensional modelling, Data warehousing, Business Intelligence, Management Information, ML, AI, Agile, Scrum, Supply Chain, Pharmacy","data engineering, analytics, sql, python, spark, kimball dimensional modelling, data warehousing, business intelligence, management information, ml, ai, agile, scrum, supply chain, pharmacy","agile, ai, analytics, business intelligence, data engineering, datawarehouse, kimball dimensional modelling, management information, ml, pharmacy, python, scrum, spark, sql, supply chain"
Data Modelling Analyst - Proposition & Financial,Aldi UK,"Atherstone, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-modelling-analyst-proposition-financial-at-aldi-uk-3782736395,2023-12-17,Leicester, United Kingdom,Mid senior,Hybrid,"We are seeking a highly motivated individual to join our dynamic team as a Data Modelling Senior Analyst for Customer Strategy based at our Tamworth Office in the UK.
This is a great opportunity for the successful candidate to develop within the business and play a key part in shaping the future of ALDI globally. Your role will support the provision of market / competitor and customer intelligence to ALDI countries and Global teams. This includes details about events and developments in the CI market. This role will work closely with the strategy team to help manage ALDI’s responses to any changes in the market.
You will develop an excellent understanding of our competitors and our customers through forecasting and modelling future propositions. This will be used to support the development of our CI strategy including our response to evolving markets and customer trends. It will also drive how we prioritise our digital initiatives, ensuring what our customers want is at the heart of this decision.
The ideal candidate will be a strong, forward thinking professional with an approachable but assertive presence in the working environment. Expertise in the concepts and principles of data modelling is required. Previous experience of this in a grocery retail environment would be advantageous.
You will need enthusiasm and drive, with proven experience of working to deadlines in a fast-paced environment. You will need to think on your feet, be self-reliant, and be a confident individual with a positive attitude.
The role is UK based, but part of a global team, therefore some travel on demand should be expected, as per the needs of the project, or upon request of the Strategy Manager. All travel, accommodation and food expenses will be paid for.
If you would like to join our dynamic team and thrive working in a fast-paced environment, apply to join #TeamAldi today!
Your New Role
Identify, analyse and interpret trends or patterns in complex data sets.
Acquire data from primary or secondary data sources and maintain databases/data systems.
Design, build, and deliver Anaplan models to support the planning, forecasting, and operational processes.
Perform integration between internal systems and Anaplan; monitor data flows between countries and onto Anaplan platform to ensure all data is up-to-date and accurate.
Produce enhancements to the models and where required, provide upskilling to end users on Anaplan models and processes.
Regularly collaborate with stakeholders and countries to design and maintain impactful operational and financial forecasting by understand new initiatives and supporting strategic opportunities.
Maintain and manage pipeline of future enhancements and build whilst having an instrumental involvement with the implementation of Anaplan.
Deliver expert commercial data model advice to internal stakeholders (e.g. Research Analysts, Strategy Analysts), demonstrating subject matter expertise
Identify and quantify new data sources to enrich commercial data models
Provide constant monitoring and analysis of commercial data (e.g. competitor performance, profits, organisational FTE, market penetration, etc)
Establish a base set of commercial data models that can be refreshed periodically to inform research needs
Design and / or establish an appropriate metadata repository and present changes to existing metadata repositories (where relevant)
Work closely with Markey & Customer Research team on 3rd party data (e.g. from Kantar, Nielsen, GfK) to develop insightful data models for input to research outputs
Utilise relevant data sets (e.g. Ascential, Crunchbase & Statista) on market assessment activities
Ensure that commercial data is fit for purpose for any onward uses
About You
Able to reverse engineer data models from a live system
Expertise in the concepts and principles of data modelling, specifically financial forecasting of propositions
Able to produce relevant data models across multiple subject areas
Experience in using industry recognised data modelling patterns / standards and knows when to apply them
Understands a range of tools for storing and working with metadata
Able to compare and align different data models
Working in team environments – being a team player
Able to provide simple clarity on large volumes of data
Good presentation materials development skills
Experience in and comfort with working in fast-paced environments
Good interpersonal skills and is confident in dealing with clients/users across multiple International locations.
Excellent communication skills, with the ability to present to a range of audiences, including senior audiences
A systematic, analytical approach to problem solving
The ability to effectively prioritise own workload and execute tasks
A logical and efficient work ethic with a keen attention to detail
What you will get in Return
Salary starting from £50,945 rising to £58,680
Monday to Friday, 8am to 4:30pm, with the opportunity of 2 days a week remote working
The opportunity to travel to ALDI South countries as required
5 weeks’ annual leave plus Bank Holidays
Flexible working hours
Full training provided with ongoing personal development opportunities
Company sick pay scheme
Company maternity, paternity and adoption leave pay after 2 years
Long service rewards
A variety of Aldi Perks including bike to work scheme, discounted gym passes, accommodation, travel, cinema and family day out tickets
Access to a free, 24/7, confidential virtual GP service for all Aldi Colleagues and their children (under the age of 16)
If you’re ready to join a forward thinking business with a focus on your development as well as the business, apply today!
Aldi is an equal opportunities employer. We’re committed to maintaining a diverse and inclusive workforce and are dedicated to promoting a culture of inclusion, providing an environment in which collaboration, respect and fairness are essential. We value diversity and are dedicated to treating all of our colleagues and prospective colleagues fairly and with respect
Show more
Show less","Data Modelling, Financial Forecasting, Anaplan, Data Analysis, Data Interpretation, Database Management, Data Systems, Metadata, Data Integration, Data Enhancement, Upskilling, Collaboration, Communication, Problem Solving, Prioritisation, Attention to Detail, Presentation Skills, Teamwork, Interpersonal Skills, Time Management, Work Ethic, Microsoft Office Suite, Proficiency in English","data modelling, financial forecasting, anaplan, data analysis, data interpretation, database management, data systems, metadata, data integration, data enhancement, upskilling, collaboration, communication, problem solving, prioritisation, attention to detail, presentation skills, teamwork, interpersonal skills, time management, work ethic, microsoft office suite, proficiency in english","anaplan, attention to detail, collaboration, communication, data enhancement, data integration, data interpretation, data modelling, data systems, dataanalytics, database management, financial forecasting, interpersonal skills, metadata, microsoft office suite, presentation skills, prioritisation, problem solving, proficiency in english, teamwork, time management, upskilling, work ethic"
Data Engineer,BSI People Group - Resourcing today’s talent for your ongoing success,"Canberra, Australian Capital Territory, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-bsi-people-group-resourcing-today%E2%80%99s-talent-for-your-ongoing-success-3778333662,2023-12-17,Australian Capital Territory, Australia,Mid senior,Onsite,"Data Engineer - Multiple
About The Project
The Enterprise Integration Team is responsible for designing, building, and supporting integrations between on-premises and cloud-based enterprise applications, utilising Microsoft Azure tools such as Azure APIM, Azure Service Bus and Azure Data Factory.
The team provides ongoing support, performance monitoring and troubleshooting, while also collaborating with other software development teams to design and build APIs and data pipelines supporting multiple modernisation projects. Additionally, the team works with Master Data Management (MDM) and Identity and Access Management (IDAM) to ensure data integrity and security, streamlining data flow and enhancing productivity across the agency.
Requirements
Proven experience in data engineering, including data integration, ETL processes, and data modelling.
Strong SQL skills and experience with data warehousing concepts.
Knowledge of data governance, security, and compliance best practices.
Excellent problem-solving skills and the ability to work effectively in a collaborative team environment.
Effective communication skills for collaborating with technical and non-technical stakeholders.
Demonstrated ability to build and maintain effective working relationships.
Ability to work independently and be a self-motivated individual. Desirable:
Proficiency in Microsoft Azure tools and services, such as Azure Data Factory, and Azure SQL Database.
Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent experience).
Relevant certifications (e.g., Microsoft Azure Data Engineer, AWS Certified Data Analytics) are advantageous.
Must be an Australian Citizen
To register interest and find out more (complete JD), please send your CV (
in Word format only)
to the following link below.
YOUR APPLICATION WILL BE TREATED WITH THE STRICTEST CONFIDENCE.
Show more
Show less","Data Engineering, Data Integration, ETL, Data Modelling, SQL, Data Warehousing, Data Governance, Data Security, Data Compliance, Problem Solving, Collaboration, Communication, Azure Data Factory, Azure APIM, Azure Service Bus, Azure SQL Database, Master Data Management, Identity and Access Management","data engineering, data integration, etl, data modelling, sql, data warehousing, data governance, data security, data compliance, problem solving, collaboration, communication, azure data factory, azure apim, azure service bus, azure sql database, master data management, identity and access management","azure apim, azure data factory, azure service bus, azure sql database, collaboration, communication, data compliance, data engineering, data governance, data integration, data modelling, data security, datawarehouse, etl, identity and access management, master data management, problem solving, sql"
Postgresql Database Engineer,Leidos Australia,"Canberra, Australian Capital Territory, Australia",https://au.linkedin.com/jobs/view/postgresql-database-engineer-at-leidos-australia-3785121843,2023-12-17,Australian Capital Territory, Australia,Mid senior,Onsite,"Company Description
About Us
Our name isn’t the only thing that’s unique about Leidos Australia. We’re a complex systems integration company building world-class solutions across Government and Defence. We’re trusted by our customers to deliver the most innovative answers to their most complex challenges. Seriously interesting work that benefits and safeguards every Australian.
Here in Australia, we’re agile and growing fast. Our 1,400 employees are busy building an enviable reputation for innovation and delivery. And, as part of a US-based organisation with 37,000 people and a 50 year history, we are able to draw upon world-leading technical expertise to help us provide the best solutions to our customers.
We invest in our people and promote internally, so if you’re seeking growth and/or a varied career path, then you’ve come to the right place.
Job Description
Your New Role
You'll form part of a highly skilled team of Database Administrators providing support and maintenance for a large enterprise environment spanning multiple locations within Australia and overseas.
The successful candidate will use their skills to enhance the design, implementation, testing and maintenance of Postgresql Databases
Responsibilities Will Include But Not Limited To The Following
Install, create, configure, and monitor databases to ensure optimal performance
Evaluate, recommend, manage and execute upgrades and enhancements as required
Provide support to application teams for incidents relating to Postgresql Databases
Implement monitoring and health reports that reduce and eliminate issues effecting end users
Be process driven and take ownership of issues through to resolution
Contribute to the creation and maintenance of technical system documentation
Work within an ITIL framework for all work related activities (Change, Incident, & Problem management)
Ensure adherence to processes, procedures and security requirements
Participate in the team on call roster and be part of a team roster covering 7am – 7pm weekdays
Qualifications
About You and What You'll Bring
Education
Coupled with your education and practical experience, you will demonstrate a pro-active approach with the ability to understand the business, identify issues and develop relationships to achieve the company’s objectives. You will also have:
Relevant experience working with Postgresql Databases in a complex environment
Proven experience administering, configuring and managing large database environments
Experience in Service Management, Operations and Disaster Recovery
Experience managing backup and restoration processes
Ability to build and maintain strong relationships with other teams
Additional experience working with Oracle, Microsoft SQL or IBM DB2 databases would be ideal
Additional Information
This role does require the successful applicant to be an Australian Citizen and hold a current NV-1 security clearance, with the ability obtain an NV-2 clearance in the future.
This is a team that cares for its people, focuses on building culture, and is high-performing all while maintaining a healthy work-life balance.
At Leidos you’ll enjoy 12 weeks’ paid parental leave as a primary carer, flexible work practices, discounted health insurance, novated leasing and more. You will also have an option for an additional 12 Days leave when you enrol into the Life Days Program. Foster your career through complete access to learning and development and mentoring opportunities, we have a strong track record of internal promotion and career transitions
Show more
Show less","PostgreSQL, Database Administration, ITIL framework, Microsoft SQL, IBM DB2, Oracle, Service Management, Operations, Disaster Recovery, Backup and restoration processes","postgresql, database administration, itil framework, microsoft sql, ibm db2, oracle, service management, operations, disaster recovery, backup and restoration processes","backup and restoration processes, database administration, disaster recovery, ibm db2, itil framework, microsoft sql, operations, oracle, postgresql, service management"
Postgresql Database Engineer,Leidos,"Canberra, Australian Capital Territory, Australia",https://au.linkedin.com/jobs/view/postgresql-database-engineer-at-leidos-3785140449,2023-12-17,Australian Capital Territory, Australia,Mid senior,Onsite,"Company Description
About Us
Our name isn’t the only thing that’s unique about Leidos Australia. We’re a complex systems integration company building world-class solutions across Government and Defence. We’re trusted by our customers to deliver the most innovative answers to their most complex challenges. Seriously interesting work that benefits and safeguards every Australian.
Here in Australia, we’re agile and growing fast. Our 1,400 employees are busy building an enviable reputation for innovation and delivery. And, as part of a US-based organisation with 37,000 people and a 50 year history, we are able to draw upon world-leading technical expertise to help us provide the best solutions to our customers.
We invest in our people and promote internally, so if you’re seeking growth and/or a varied career path, then you’ve come to the right place.
Job Description
Your New Role
You'll form part of a highly skilled team of Database Administrators providing support and maintenance for a large enterprise environment spanning multiple locations within Australia and overseas.
The successful candidate will use their skills to enhance the design, implementation, testing and maintenance of Postgresql Databases
Responsibilities will include but not limited to the following:
Install, create, configure, and monitor databases to ensure optimal performance
Evaluate, recommend, manage and execute upgrades and enhancements as required
Provide support to application teams for incidents relating to Postgresql Databases
Implement monitoring and health reports that reduce and eliminate issues effecting end users
Be process driven and take ownership of issues through to resolution
Contribute to the creation and maintenance of technical system documentation
Work within an ITIL framework for all work related activities (Change, Incident, & Problem management)
Ensure adherence to processes, procedures and security requirements
Participate in the team on call roster and be part of a team roster covering 7am – 7pm weekdays
Qualifications
About You And What You'll Bring
Coupled with your education and practical experience, you will demonstrate a pro-active approach with the ability to understand the business, identify issues and develop relationships to achieve the company’s objectives. You will also have:
Relevant experience working with Postgresql Databases in a complex environment
Proven experience administering, configuring and managing large database environments
Experience in Service Management, Operations and Disaster Recovery
Experience managing backup and restoration processes
Ability to build and maintain strong relationships with other teams
Additional experience working with Oracle, Microsoft SQL or IBM DB2 databases would be ideal
Additional Information
This role does require the successful applicant to be an Australian Citizen and hold a current NV-1 security clearance, with the ability obtain an NV-2 clearance in the future.
This is a team that cares for its people, focuses on building culture, and is high-performing all while maintaining a healthy work-life balance.
At Leidos you’ll enjoy 12 weeks’ paid parental leave as a primary carer, flexible work practices, discounted health insurance, novated leasing and more. You will also have an option for an additional 12 Days leave when you enrol into the Life Days Program. Foster your career through complete access to learning and development and mentoring opportunities, we have a strong track record of internal promotion and career transitions
Show more
Show less","Postgresql, Database Administration, SQL, Oracle, Microsoft SQL, IBM DB2, ITIL, Service Management, Operations, Disaster Recovery, Backup and Restoration, Change Management, Incident Management, Problem Management","postgresql, database administration, sql, oracle, microsoft sql, ibm db2, itil, service management, operations, disaster recovery, backup and restoration, change management, incident management, problem management","backup and restoration, change management, database administration, disaster recovery, ibm db2, incident management, itil, microsoft sql, operations, oracle, postgresql, problem management, service management, sql"
Data Analyst and Power BI specialist,Peoplebank,"Canberra, Australian Capital Territory, Australia",https://au.linkedin.com/jobs/view/data-analyst-and-power-bi-specialist-at-peoplebank-3778459829,2023-12-17,Australian Capital Territory, Australia,Mid senior,Onsite,"Location: Canberra
Job Type: Contract
Posted: 23 days ago
Contact: Param Kaur
Discipline
Business Intelligence / Data Analytics
General IT
Reference: 259390
Our Federal Government Client is seeking a Data Analyst and Power BI Specialist
This is a long-term contract role for 12 months with an extension term of 6 months ,located at our client's office in Canberra, ACT.
As The Selected Candidate, You Will
Support the Security and Business Continuity team in developing automated reporting to strengthen the department's security maturity with dynamic, accurate and highly visible security clearance/security incident information to support evidence-based decision making and Protective Security Policy Framework (PSPF) compliance.
Develop a dashboard to track the PSPF uplift workplan and the delivery of activities identified in the PSPF maturity assessment report 2022-23.
Provide expert knowledge in relation to Power BI capabilities, system compatibility and processes.
Take responsibility for the achievements of own outcomes, monitor progress and follow through to deliver quality outcomes.
Manage workload with limited supervision and work remotely when required.
Be proficient in the use of technology and the Microsoft Suite of tools.
Collaborate with relevant system administrators and line areas including in the SDO, People Branch and ICT areas where relevant.
Align and streamline data from multiple sources and legacy systems including SAP, myClearance.
Build team capability through training and transfer of technical skills, so team members can maintain reporting suite created during contract.
To Be Successful In This Role You Should Have
Demonstrated experience in data analysis and Power BI reporting.
Strong understanding of data modelling and data quality concepts.
Experience with Microsoft Power BI, including DAX and data modelling.
Experience with aligning and streamlining data from multiple sources and legacy systems.
Excellent communication and interpersonal skills.
It Is Desirable If You Have
Experience in the Australian Government security environment.
Experience with SAP and myClearance.
Knowledge of the Protective Security Policy Framework (PSPF).
Due to security clearance requirements for this role, candidates must be Australian citizens with Baseline security clearance.
Apply now for immediate consideration - contact Param Kaur on 0262689781 quoting Job Reference: # 259390
The applications for the role closes on Wednesday, 06 December 2023.
Please note: Only candidates that meet the above criteria will be contacted. Thank you for your interest in the position.
Diversity and inclusion are strongly supported at Peoplebank. People of all nationalities, gender identities, and cultural backgrounds, including Aboriginal and Torres Strait Islander Peoples, are encouraged to apply.
Show more
Show less","Data Analysis, Power BI, Data Modelling, Data Quality, DAX, Data Alignment, Data Streamlining, Microsoft Suite, Communication, Interpersonal Skills, Security Clearance, SAP, myClearance, Protective Security Policy Framework","data analysis, power bi, data modelling, data quality, dax, data alignment, data streamlining, microsoft suite, communication, interpersonal skills, security clearance, sap, myclearance, protective security policy framework","communication, data alignment, data modelling, data quality, data streamlining, dataanalytics, dax, interpersonal skills, microsoft suite, myclearance, powerbi, protective security policy framework, sap, security clearance"
Senior Data Analyst,Workforce Australia for Individuals,"Canberra, Australian Capital Territory, Australia",https://au.linkedin.com/jobs/view/senior-data-analyst-at-workforce-australia-for-individuals-3779190727,2023-12-17,Australian Capital Territory, Australia,Mid senior,Onsite,"One Senior Data Analyst role with a Government Department for an initial contract to 30 June 2024 + 12 months.
Your New Company
You will be working with a Government Department as a Senior Data Analyst in the Data Analytics/Support Section.
Your new role
As a Senior data Analyst, you will be responsible for analysing and producing data visualisations and analytics tools to allow users to explore data and personalised insights. Undertake curation, quality assurance, and the automation of datasets to power online tools and information. Representing the department in negotiations to access and agree usage parameters for data sources. Performing additional duties or assuming responsibility of functions as directed by the supervisor from time to time.
What You'll Need To Succeed
To be considered, you will need:
Experience in data compilation, management, analysis, visualisation and publication, including proficiency in using tools such as R and Power BI.
Enjoys working with data and creating data and insights to inform good decision making.
Good attention to detail, the ability to work independently, and manage processes, outputs and deadlines.
Experience in using datasets to provide data, insights, briefing and analysis to stakeholders, including for non-technical audiences.
Demonstrated experience on end-to-end data analysis and/or calculation solution designs, capability to deliver a high-quality technical solution.
Good verbal and written communication skills including the ability to communicate and summarise complex information from a range of sources.
You must be an Australian citizen and will possess or be willing to go through the process to obtain and maintain a (Baseline) personnel security clearance.
What You'll Get In Return
In return you will secure a competitive hourly rate with an initial contract to 30 June 2024 with a 12-month extension option.
What You Need To Do Now
If you are interested in this role, please contact Antonio Musolino on (02) 6112 7621 or email Antonio.Musolino@hays.com.au.
If this job isn't quite right for you, but you're looking for more opportunities in the IT industry, please contact me for a confidential discussion on your career.
Please click here to apply.
Show more
Show less","Data Analytics, Data Visualization, Data Management, Data Analysis, Data Publication, R, Power BI, Data Compilation, EndtoEnd Data Analysis, Data Quality Assurance, Data Automation, Datasets, Data Insights, Data Exploration, Technical Writing, Verbal Communication, Written Communication","data analytics, data visualization, data management, data analysis, data publication, r, power bi, data compilation, endtoend data analysis, data quality assurance, data automation, datasets, data insights, data exploration, technical writing, verbal communication, written communication","data automation, data compilation, data exploration, data insights, data management, data publication, data quality assurance, dataanalytics, datasets, endtoend data analysis, powerbi, r, technical writing, verbal communication, visualization, written communication"
Data Scientist,Whizdom,"Canberra, Australian Capital Territory, Australia",https://au.linkedin.com/jobs/view/data-scientist-at-whizdom-3779314777,2023-12-17,Australian Capital Territory, Australia,Mid senior,Onsite,"Overview
We are looking for an experienced Data Scientist who will be responsible for working under limited direction as a member of a team responsible for developing advanced analytics, tools, and techniques to exploit client’s data holdings in support of business requirements. The role will need extensive data analysis experience, the ability to architect and develop software/analytic solutions, and a proven ability to work within an integrated team of technical/non-technical personnel.
Major Responsibilities:
Evaluates the need for analytics, assesses the problems to be solved and what internal or external data sources to use or acquire.
Specifies and applies appropriate mathematical, statistical, predictive modelling or machine-learning techniques to analyse data, generate insights, create value and support decision-making. Manages reviews of the benefits and value of analytics techniques and tools and recommends improvements.
Takes technical responsibility across all stages and iterations of software/analytic development. Plans and drives software/analytic construction activities. Adopts and adapts appropriate software/analytic development methods, tools and techniques.
Contributes to and supports leadership efforts to improve data science processes and techniques.
Contributes to the development of solution architectures in data analysis and exploitation. Identifies and evaluates alternative architectures and the trade-offs in cost, performance and scalability.
Location of work:
Canberra ACT.
Contract Term:
11 months with 2x12 months extension option
SFIA security level required
: The Specified Person will be expected to demonstrate attributes of SFIA Level of Responsibility 5.
Security Requirements
: A minimum Australian Government Security Vetting Agency (AGSVA) Top Secret Positive Vetting (TSPV) security clearance is required.
How to Apply
: Please upload your resume to apply. Please note you will need to complete selection criteria to complete this application process. Candidates will need to be willing to undergo pre-employment screening checks which may include, ID and work rights, security clearance verification and any other client requested checks. We will be in touch with instructions for suitably skilled candidates.
Applications open until
5:00pm on 15/01/2024.
Call
Salina on 0480 002 449 or salinas@whizdom.com.au
for any further information
Show more
Show less","Data Analysis, Predictive Modelling, Machine Learning, Software Development, Data Science, Solution Architecture, Data Exploitation","data analysis, predictive modelling, machine learning, software development, data science, solution architecture, data exploitation","data exploitation, data science, dataanalytics, machine learning, predictive modelling, software development, solution architecture"
Data Engineer,Tech Savvy Consulting (TSC),"Canberra, Australian Capital Territory, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-tech-savvy-consulting-tsc-3778466519,2023-12-17,Australian Capital Territory, Australia,Mid senior,Hybrid,"About the role
We are looking for a
Data Engineer
to join our diverse and collaborative team.
You'll join a flexible, progressive company with a flat org structure and vibrant team culture.
We are a strong, experienced, and supportive team and you will gain exposure to a broad range of exciting work.
Location:
The roles are based in
ACT
offering flexible remote work policies.
Role Type:
This is a full-time permanent or contract opportunity with ongoing extensions.
Mandatory:
Proven experience in data engineering, including data integration, ETL processes, and data modelling.
Strong SQL skills and experience with data warehousing concepts.
Knowledge of data governance, security, and compliance best practices.
Excellent problem-solving skills and the ability to work effectively in a collaborative team environment.
Effective communication skills for collaborating with technical and non-technical stakeholders.
Demonstrated ability to build and maintain effective working relationships.
Ability to work independently and be a self-motivated individual.
Desirable:
Proficiency in Microsoft Azure tools and services, such as Azure Data Factory, and Azure SQL Database.
Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent experience).
Relevant certifications (e.g., Microsoft Azure Data Engineer, AWS Certified Data Analytics) are advantageous.
Benefits:
Remote working with flexible working arrangements
Receive competitive pay, employee referral bonus, and flexible leave offerings
Be a thought leader and advocate for the customer
Opportunity to join a team who are highly engaged and supportive
Join a progressive, open-minded company with a flat org structure
Work rights:
Please note that the candidate
must be able to obtain NV1 Security Clearance.
Only shortlisted candidates will be contacted.
Show more
Show less","Data Engineering, Data Integration, ETL, Data Modelling, SQL, Data Warehousing, Data Governance, Data Security and Compliance, ProblemSolving, Team Collaboration, Communication, Microsoft Azure Tools, Azure Data Factory, Azure SQL Database, AWS Certified Data Analytics, Microsoft Azure Data Engineer, Computer Science, Information Technology, NV1 Security Clearance","data engineering, data integration, etl, data modelling, sql, data warehousing, data governance, data security and compliance, problemsolving, team collaboration, communication, microsoft azure tools, azure data factory, azure sql database, aws certified data analytics, microsoft azure data engineer, computer science, information technology, nv1 security clearance","aws certified data analytics, azure data factory, azure sql database, communication, computer science, data engineering, data governance, data integration, data modelling, data security and compliance, datawarehouse, etl, information technology, microsoft azure data engineer, microsoft azure tools, nv1 security clearance, problemsolving, sql, team collaboration"
Data Engineer - Contract - Canberra,Hastha Solutions,"Canberra, Australian Capital Territory, Australia",https://au.linkedin.com/jobs/view/data-engineer-contract-canberra-at-hastha-solutions-3778479706,2023-12-17,Australian Capital Territory, Australia,Mid senior,Hybrid,"Urgent requirement of
Data Engineer - Contract - Canberra
Requirements
Collaborate with team members and stakeholders to design data integration solutions that meet business requirements, utilising Microsoft Azure tools such as Azure Data Factory and SQL.
Develop, implement, and maintain ETL pipelines to ensure efficient and secure data transfer between various systems, adhering to best practices for data processing and transformation.
Design and maintain data models, ensuring data accuracy, consistency, and availability while considering scalability and performance optimisations.
Implement data validation and quality checks to identify and rectify data anomalies, errors, and inconsistencies within integrated systems.
Continuously monitor and optimise data integration processes to enhance performance, reduce latency, and minimise resource consumption.
Create and maintain comprehensive documentation for data integration solutions, including technical specifications, data flow diagrams, and operational procedures.
Work closely with cross-functional teams, including developers, and system analysts, to understand data requirements and ensure integrations align with business objectives.
Troubleshoot and resolve data integration issues, working collaboratively with the team to implement effective solutions and prevent recurrence.
Stay updated on emerging technologies and industry trends related to data engineering and integration, applying this knowledge to improve integration processes and tools.
Duration: 12 months
and possible extension
Eligibility:
Baseline Holders OR Ability to obtain Baseline Clearance
Email: jobs@hasthasolutions.com
Show more
Show less","Azure Data Factory, SQL, ETL pipelines, Data integration, Data modeling, Data validation, Data quality checks, Performance optimization, Resource consumption, Documentation, Crossfunctional collaboration, Troubleshooting, Emerging technologies, Industry trends","azure data factory, sql, etl pipelines, data integration, data modeling, data validation, data quality checks, performance optimization, resource consumption, documentation, crossfunctional collaboration, troubleshooting, emerging technologies, industry trends","azure data factory, crossfunctional collaboration, data integration, data quality checks, data validation, datamodeling, documentation, emerging technologies, etl pipelines, industry trends, performance optimization, resource consumption, sql, troubleshooting"
Data Engineer,Certus Recruitment Group,"Canberra, Australian Capital Territory, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-certus-recruitment-group-3774190850,2023-12-17,Australian Capital Territory, Australia,Mid senior,Hybrid,"Data Engineer
Negotiable Package
Canberra, ACT
Our established client, a leading government agency acts as an independent statutory authority managing federal elections. They currently have an exciting opening in Canberra for an experienced Data Engineer.
Data Engineer Responsibilities:
Design, develop and maintain data integration solutions
Leverage Azure tools and technologies
Implement and maintain ETL pipelines
Design and maintain data models
Implement data validation and quality checks
Monitor and optimise processes to drive performance
Prepare and manage comprehensive documentation and technical specifications
Collaborate with cross-functional teams
Troubleshoot
Monitor emerging technologies and industry trends
Qualifications and Experience:
Computer Science degree
Data engineering, data integration, data modelling, and ETL processes
SQL and data warehousing
Azure tools, azure data factory Azure SQL
Data governance and security
Problem solving
Strong communication and collaboration skills
Azure and AWS certifications (advantageous)
NV1 clearance
Benefits for the Successful Candidate:
A generous package
Flexible working
Professional development and ongoing learning opportunities
A collaborative team culture
Certus Recruitment Australia, a specialist provider of sales, marketing, and IT recruitment services to the business community, is part of the Certus Recruitment Group, operating throughout Australia, the UK, Europe, and North America.
Show more
Show less","Data engineering, Data integration, Data modelling, ETL processes, SQL, Data warehousing, Azure tools, Azure data factory, Azure SQL, Data governance, Security, Problem solving, Communication, Collaboration, Azure certification, AWS certification","data engineering, data integration, data modelling, etl processes, sql, data warehousing, azure tools, azure data factory, azure sql, data governance, security, problem solving, communication, collaboration, azure certification, aws certification","aws certification, azure certification, azure data factory, azure sql, azure tools, collaboration, communication, data engineering, data governance, data integration, data modelling, datawarehouse, etl, problem solving, security, sql"
Data Engineer - Contract - Canberra,Hastha Solutions,"Russell, Australian Capital Territory, Australia",https://au.linkedin.com/jobs/view/data-engineer-contract-canberra-at-hastha-solutions-3779511323,2023-12-17,Australian Capital Territory, Australia,Mid senior,Hybrid,"Urgent requirement of
Data Engineer - Contract - Canberra
Requirements
Collaborate with team members and stakeholders to design data integration solutions that meet business requirements, utilising Microsoft Azure tools such as Azure Data Factory and SQL.
Develop, implement, and maintain ETL pipelines to ensure efficient and secure data transfer between various systems, adhering to best practices for data processing and transformation.
Design and maintain data models, ensuring data accuracy, consistency, and availability while considering scalability and performance optimisations.
Implement data validation and quality checks to identify and rectify data anomalies, errors, and inconsistencies within integrated systems.
Continuously monitor and optimise data integration processes to enhance performance, reduce latency, and minimise resource consumption.
Create and maintain comprehensive documentation for data integration solutions, including technical specifications, data flow diagrams, and operational procedures.
Work closely with cross-functional teams, including developers, and system analysts, to understand data requirements and ensure integrations align with business objectives.
Troubleshoot and resolve data integration issues, working collaboratively with the team to implement effective solutions and prevent recurrence.
Stay updated on emerging technologies and industry trends related to data engineering and integration, applying this knowledge to improve integration processes and tools.
Duration: 12 months
and possible extension
Eligibility:
Baseline Holders OR Ability to obtain Baseline Clearance
Email: jobs@hasthasolutions.com
Show more
Show less","Azure Data Factory, SQL, ETL pipelines, Data processing, Data transformation, Data models, Data accuracy, Data consistency, Data availability, Scalability, Performance optimization, Data validation, Data quality checks, Data anomalies, Data integration, Performance enhancement, Latency reduction, Resource consumption, Technical specifications, Data flow diagrams, Operational procedures, Data requirements, Business objectives, DevOps, Emerging technologies, Data engineering","azure data factory, sql, etl pipelines, data processing, data transformation, data models, data accuracy, data consistency, data availability, scalability, performance optimization, data validation, data quality checks, data anomalies, data integration, performance enhancement, latency reduction, resource consumption, technical specifications, data flow diagrams, operational procedures, data requirements, business objectives, devops, emerging technologies, data engineering","azure data factory, business objectives, data accuracy, data anomalies, data availability, data consistency, data engineering, data flow diagrams, data integration, data models, data processing, data quality checks, data requirements, data transformation, data validation, devops, emerging technologies, etl pipelines, latency reduction, operational procedures, performance enhancement, performance optimization, resource consumption, scalability, sql, technical specifications"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Ropesgray,"Revelstoke, British Columbia, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-ropesgray-3752011685,2023-12-17,Revelstoke, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical techniques, SQL, R, Python, Datadriven decisionmaking, Collaboration, Model building, Algorithm development, Optimization, A/B testing, Data quality, Data integrity, Data accuracy, Data completeness, Data collection, Data cleansing, Data manipulation, Data presentation, Datadriven reports, Tableau, Power BI, Statistical modeling, Hypothesis testing, ETL processes","data analysis, statistical techniques, sql, r, python, datadriven decisionmaking, collaboration, model building, algorithm development, optimization, ab testing, data quality, data integrity, data accuracy, data completeness, data collection, data cleansing, data manipulation, data presentation, datadriven reports, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, algorithm development, collaboration, data accuracy, data collection, data completeness, data integrity, data manipulation, data presentation, data quality, dataanalytics, datacleaning, datadriven decisionmaking, datadriven reports, etl, hypothesis testing, model building, optimization, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau"
Data Quality Engineer,Zortech Solutions,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-quality-engineer-at-zortech-solutions-3667473747,2023-12-17,Burlington, Canada,Mid senior,Onsite,"Data Quality Engineer- Mississauga, ON - Contract
Employment Duration:
6 months initial with high possibility of extension based on performance
Key Responsibilities
Understanding the flow of data between the systems and its Data model.
Works directly with Data Owners and Data Stewards: to assess effectiveness of subject area data
to fulfill business requirements, needs or desires
Establish quality rules and data quality standards to align data quality requirements at the
enterprise level; Test the accuracy of the data, and its completeness
Perform enterprise-level data quality impact analysis due to data-related changes required by
the business
Data maintenance/clean up - Work with the report team on creating additional reports to
monitor data for integrity and consistency
Research quality deficiencies perceived by data consumers. Analyzes data life cycle, Meta Data,
data quality metrics, etc. to detect quality ""bottlenecks"" in business processes
Identify key ETL mapping scenarios and create SQL queries that simulate them
Validate the canned and custom reports from the HRIS system
Understanding project documents, demands, and business objectives to create the Test Plan,
create and execute test cases
Performing System Integration Testing (SIT/QA) at all the transformation layers which includes
Source Data Check, Data Mapping between different systems
Move the enterprise from reactive to proactive quality management
Work collaboratively with a team and in an Agile-Scrum environment.
Required Experience
B.S. or B.A. in Math, Statistics, Comp Sci, Engineering, or other quantitative field required;
graduate degree preferred
Ability to understand and articulate data quality requirements at all levels of the organization
Ability to develop data quality standards, policies and procedures that represent business
Requirements
Experience with Data Mapping, Gap Analysis, Source target mapping document, source fields,
target fields, documenting transformation rules
Experience in creating test plans to perform data validation, accuracy, and completeness
covering various business flows
Preferred
Experience with validation of BI reports Cognos preferred
Experience with data platforms including Power BI, Azure and SQL Server
Experience on Azure Data Factory or Data Bricks would be nice to have
7+ years of meaningful work experience in analyst role in a data/analytics team
Minimum 5 years of experience in end-to-end data validation of complex systems
Minimum 5 years of experience in ETL & BI testing
TFS/ADO tool experience and exposure to defect life cycle
Ability to create RTM and flag the risks
Strong knowledge on ETL and BI processes
Should be quick learner, and should be able to work independently, co-ordinate with PM and dev team
Ability to keep learning and adapting to new technology & tools
Feel free to reach out to me for any further details.
Show more
Show less","Math, Statistics, Comp Sci, Engineering, Data Quality, Data Modeling, SQL Queries, ETL Mapping, Gap Analysis, Source Target Mapping, Data Validation, Accuracy, Completeness, Business Flows, BI Reports, Cognos, Power BI, Azure, SQL Server, Azure Data Factory, Data Bricks, TFS/ADO, Defect Life Cycle, RTM, Risk Management, ETL Processes, BI Processes","math, statistics, comp sci, engineering, data quality, data modeling, sql queries, etl mapping, gap analysis, source target mapping, data validation, accuracy, completeness, business flows, bi reports, cognos, power bi, azure, sql server, azure data factory, data bricks, tfsado, defect life cycle, rtm, risk management, etl processes, bi processes","accuracy, azure, azure data factory, bi processes, bi reports, business flows, cognos, comp sci, completeness, data bricks, data quality, data validation, datamodeling, defect life cycle, engineering, etl, etl mapping, gap analysis, math, powerbi, risk management, rtm, source target mapping, sql queries, sql server, statistics, tfsado"
Senior Software Engineer- Big Data & Java,PointClickCare,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-software-engineer-big-data-java-at-pointclickcare-3771455802,2023-12-17,Burlington, Canada,Mid senior,Hybrid,"For more than 20 years, PointClickCare has been the backbone of senior care. We’ve amassed the richest senior care dataset making our market density untouchable and our connections to the healthcare ecosystem exponentially more powerful than those of any other platform.
With Collective Medical & Audacious Inquiry, we’ve become the most expansive, full-continuum care collaboration network, offering care teams immediate, point-of-care access to deep, real-time insights at every stage of a patient’s journey.
For more information on PointClickCare, please connect with us on Glassdoor and LinkedIn.
What Your Day-to-day Will Look Like
Identify, prioritize and execute tasks in the software development life cycle.
Work with business to iterate over software requirements.
Develop tools and applications by producing clean, efficient code.
Automate tasks through appropriate tools and scripting.
Analyze and debug systems.
Perform validation and verification testing in a test-driven manner
Review the work of others, and invite others to review your work.
Collaborate with internal teams and vendors to fix and improve products.
Ensure software is up-to-date with latest technologies.
Work with distributed computing systems like Apache Hudi and Trino for big data processing.
What Qualifications We’re Looking For
Experience with distributed computing tools like Apache Hudi, Trino, Map Reduce and other big data technologies.
Experience with distributed storage systems like HDFS, S3, etc.
Familiarity with Hadoop, Spark, or other distributed computing systems.
Understanding of data partitioning and sharding techniques.
Knowledge of distributed computing principles and how they apply to large-scale data processing.
Experience writing clean code that performs well at scale using languages such as Java/Kotlin/C#/Go.
Experience in scripting languages such as Python.
Knowledge of relational databases (e.g. Microsoft SQL Server, MySQL).
Solid experience writing RESTful API endpoints.
Absolutely love TDD and have working knowledge of it.
Proficient in GIT.
Experience using system and performance monitoring tools (e.g. New Relic, DataDog).
Excellent organization, critical-thinking and personal leadership skills
Self-starter with the ability to deliver with minimal supervision.
Being okay with the uncomfortable feeling that comes from learning new things.
Team player.
Analytical mind with problem-solving aptitude.
Proven experience as a great Engineer.
BSc/BA in Computer Science or a related degree.
Bonus Points For
Experience with distributed computing
Experience writing code/scripts in Python
Experience with Spring Boot.
Nice to have React, Selenium automation and cloud experience.
Experience with Docker, Kubernetes and Istio.
Experience with Ansible.
Experience with Jenkins CI/CD pipeline.
Linux and IP networking knowledge.
Experience with AWS/Azure cloud services or equivalent.
Experience with SAML, OAuth and OpenID Connect.
Experience working on a SaaS product.
Experience with Service Oriented Architecture.
Knowledge of scripting languages such as Python, Bash or Groovy.
On-call experience with production grade systems.
Has mentored others in a professional setting.
It is the policy of PointClickCare to ensure equal employment opportunity without discrimination or harassment on the basis of race, religion, national origin, status, age, sex, sexual orientation, gender identity or expression, marital or domestic/civil partnership status, disability, veteran status, genetic information, or any other basis protected by law. PointClickCare welcomes and encourages applications from people with disabilities. Accommodations are available upon request for candidates taking part in all aspects of the selection process. Please contact recruitment@pointclickcare.com should you require any accommodations.
When you apply for a position, your information is processed and stored with Lever, in accordance with Lever’s Privacy Policy. We use this information to evaluate your candidacy for the posted position. We also store this information, and may use it in relation to future positions to which you apply, or which we believe may be relevant to you given your background. When we have no ongoing legitimate business need to process your information, we will either delete or anonymize it. If you have any questions about how PointClickCare uses or processes your information, or if you would like to ask to access, correct, or delete your information, please contact PointClickCare’s human resources team: recruitment@pointclickcare.com
PointClickCare is committed to Information Security. By applying to this position, if hired, you commit to following our information security policies and procedures and making every effort to secure confidential and/or sensitive information.
Show more
Show less","Software Development Life Cycle, Apache Hudi, Trino, Map Reduce, Hadoop, Spark, Data Partitioning, Sharding, Relational Databases, RESTful APIs, TDD, GIT, System and Performance Monitoring, Linux, AWS, Azure, SAML, OAuth, OpenID Connect, SaaS, Service Oriented Architecture, Python, Bash, Groovy, Docker, Kubernetes, Istio, Ansible, Jenkins CI/CD","software development life cycle, apache hudi, trino, map reduce, hadoop, spark, data partitioning, sharding, relational databases, restful apis, tdd, git, system and performance monitoring, linux, aws, azure, saml, oauth, openid connect, saas, service oriented architecture, python, bash, groovy, docker, kubernetes, istio, ansible, jenkins cicd","ansible, apache hudi, aws, azure, bash, data partitioning, docker, git, groovy, hadoop, istio, jenkins cicd, kubernetes, linux, map reduce, oauth, openid connect, python, relational databases, restful apis, saas, saml, service oriented architecture, sharding, software development life cycle, spark, system and performance monitoring, tdd, trino"
Master Data Analyst,Gay Lea Foods,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/master-data-analyst-at-gay-lea-foods-3781742165,2023-12-17,Burlington, Canada,Mid senior,Hybrid,"Status
: Full-time
Reporting to:
Supervisor, Master Data Services
Job Scope:
Reporting to the Supervisor of Mater Data Services (MDS), the primary responsibility of this role is to support the company’s master data accuracy, ensuring overall data integrity is maintained in Gay Lea’s business applications, including the ERP and Warehouse Management Systems by analyzing business request for compliance and accuracy prior to data entry. This position specializes in identifying, investigating, and resolving master data discrepancies and providing master data support to all Gay Lea departments as required (e.g. audits, balancing accounts, reporting).
Key Responsibilities:
Review, analyze and consult with the business to ensure data/field accuracy for the intended data location
Input, monitor and maintain consistency and integrity of master data
Identify, analyze and resolve data issues and/or system errors
Apply data audit and validation methods and procedures to ensure data quality
Utilized internal Master Data field library to collaborate with the data owners on ensure data and field compliance. Escalate to data governance council when required
Understand end-to-end business processes to provide data management and integrity expertise
Meet service level agreements as agreed upon with the business
Complete daily, weekly and monthly data integrity tasks
Provide daily support of the ERP and Warehouse Management Systems; manage and resolve support tickets
Contribute to the continuous improvement of the department
Qualifications:
Post-Secondary education in Business or Computer Programming
Excellent attention to detail with a high level of accuracy
Strong analytical and problem-solving skills
Proficient with computers (Microsoft Word, Excel, PowerPoint, Access, Outlook)
Advanced knowledge of Microsoft Excel for data manipulation
SQL experience is considered an asset
Solid organizational and multi-tasking skills with the ability to work in a fast-paced environment
Effective communication and interpersonal skills (written and verbal)
Previous experience with D365 /other ERP or Warehouse Management System is an asset
Gay Lea Foods
is a proud Canadian-owned dairy food and beverage company with a robust portfolio that includes 7 established brands such as Gay Lea Butter, Nordica Cottage Cheese, Ivanhoe Cheese, Salerno Cheese, and Hewitt’s Dairy. Our wide range of products are sold in Retail, Foodservice, Industrial, Co-pack, and Export markets, developed and produced out of our 13 manufacturing facilities in Ontario and Alberta.
Delivering high-quality and innovative products - from the consumer favourite Spreadables and North America’s first Smooth™ Cottage Cheese, to Nothing But Cheese™, an innovative snack made with 100% cheese, is what makes us market leaders in our categories. Our grassroots approach to business is what guides our direction and differentiates us from our competitors.
Our employees inspire our culture of excellence and entrepreneurship. We hire innovative and passionate professionals who know that greatness is achieved through people, partnerships, and acting with a business owner mindset.
For more about us visit: http://www.gaylea.com
Gay Lea Foods is an equal-opportunity employer and values diversity in our workplace.
Reasonable accommodation is available upon request throughout our recruitment and selection process.
Thank you for your interest in Gay Lea Foods.
Show more
Show less","Data Management, Data Analysis, Data Integrity, Data Warehousing, Data Mining, Data Validation, Data Entry, Data Governance, Data Quality, Microsoft Office Suite, Microsoft SQL, D365, ERP Systems, Warehouse Management Systems, Business Process Management, Communication, Interpersonal Skills, ProblemSolving, Analytical Thinking, Attention to Detail, Organizational Skills, MultiTasking, FastPaced Environment, Business Administration, Computer Programming","data management, data analysis, data integrity, data warehousing, data mining, data validation, data entry, data governance, data quality, microsoft office suite, microsoft sql, d365, erp systems, warehouse management systems, business process management, communication, interpersonal skills, problemsolving, analytical thinking, attention to detail, organizational skills, multitasking, fastpaced environment, business administration, computer programming","analytical thinking, attention to detail, business administration, business process management, communication, computer programming, d365, data entry, data governance, data integrity, data management, data mining, data quality, data validation, dataanalytics, datawarehouse, erp systems, fastpaced environment, interpersonal skills, microsoft office suite, microsoft sql, multitasking, organizational skills, problemsolving, warehouse management systems"
P2 - Sr. SAS Data Analyst,Randstad Canada,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/p2-sr-sas-data-analyst-at-randstad-canada-3756049019,2023-12-17,Burlington, Canada,Mid senior,Hybrid,"Are you a Sr. SAS Data Analyst looking for a new opportunity?Are you looking for a new contract opportunity?
We are pleased to offer you a new contract opportunity for you to consider: Sr. SAS Data Analyst
Start: ASAP
Estimated length: 6 months
Location: Mississauga
Hybrid remote work
Actively seeking for candidates with below requirements.
Advantages
You will have an opportunity to work with a leading employer in the local market.
Responsibilities
The day starts with a stand-up meeting where we review the current tickets assigned for the current sprint and address roadblocks
After the stand-up, the top ticket priority would have time set aside to develop on.
After a few hours of coding you have a meeting with a stakeholder to confirm some cosmetic choices and showcase the report built so far
Some revisions are made and you’re ready to send the code to the production team to deploy
Suddenly a stakeholder alerts you that an existing report failed to go out on its scheduled time
You pull the job log from the server and deep dive the error code
After patching and redeploying the report code you follow up with your manager and stakeholder that the issue has been resolved
Your original report is deployed and you move the ticket ahead on our Kanban ready for tomorrow’s stand up
Working with a team of BI analysts whilst helping to maintain and build new reporting analytical capabilities
Analysis, delivery of business requests/issues/requirements, and conducting quality assurance testing for all BI deliverables supporting Sales & Marketing business functions
Excellent leadership and communication skills required to work well with business users, other analysis teams, development teams, and IT teams
Deliver on key projects and data strategies that directly support key business drivers
Work with and opportunity to present to various management levels
Qualifications
You have a Bachelor's degree, or Masters with minimal work experience, in a discipline such as:
Business analytics / business intelligence
applied math
computer science
applied science / engineering
economics/econometrics
management science / operations research
You have knowledge of, and experience with, SQL, SAS, and data visualization tools
You are highly analytical and understand the basic principles of working with large and complex data sets
You have the ability to leverage insights and opportunities from data and metrics to build strategies and make recommendations
You have the ability to work with a team towards common goals
You are able to manage multiple projects and priorities.
Nice to Have:
Experience with data systems such as Oracle, SQL Server, Teradata, SAS, MicroStrategy or others
Experience with BI and data warehouse technologies
Basic working knowledge of project management steps
Experience in Telecommunications
Summary
Do you have this experience? If you answer YES, then please apply IMMEDIATELY to so we can then discuss your experience and interest in this opportunity!
Randstad Technologies Group
Canada's largest provider of IT Staffing Solutions, offering hundreds of permanent and contract opportunities across all roles, levels and platforms. Our Web based tools help you see and apply for jobs matched automatically to your skills and preferences. When you're ready to interview we meet with you in person to help you build the technology career path you've always wanted. Visit www.randstad.ca to get started!
Randstad Canada is committed to fostering a workforce reflective of all peoples of Canada. As a result, we are committed to developing and implementing strategies to increase the equity, diversity and inclusion within the workplace by examining our internal policies, practices, and systems throughout the entire lifecycle of our workforce, including its recruitment, retention and advancement for all employees. In addition to our deep commitment to respecting human rights, we are dedicated to positive actions to affect change to ensure everyone has full participation in the workforce free from any barriers, systemic or otherwise, especially equity-seeking groups who are usually underrepresented in Canada's workforce, including those who identify as women or non-binary/gender non-conforming; Indigenous or Aboriginal Peoples; persons with disabilities (visible or invisible) and; members of visible minorities, racialized groups and the LGBTQ2+ community.
Randstad Canada is committed to creating and maintaining an inclusive and accessible workplace for all its candidates and employees by supporting their accessibility and accommodation needs throughout the employment lifecycle. We ask that all job applications please identify any accommodation requirements by sending an email to accessibility@randstad.ca to ensure their ability to fully participate in the interview process.
Show more
Show less","SAS, SQL, Data visualization tools, Oracle, SQL Server, Teradata, MicroStrategy, Project management, Telecommunications, Agile, Kanban, Analysis, Reporting, Communication, Leadership","sas, sql, data visualization tools, oracle, sql server, teradata, microstrategy, project management, telecommunications, agile, kanban, analysis, reporting, communication, leadership","agile, analysis, communication, data visualization tools, kanban, leadership, microstrategy, oracle, project management, reporting, sas, sql, sql server, telecommunications, teradata"
Senior Data Engineering Advisor,Spirit Omega Inc.,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineering-advisor-at-spirit-omega-inc-3787729614,2023-12-17,Calgary, Canada,Mid senior,Onsite,"Engineering and Technical – Calgary, AB, Canada – Contracted
Reference Number:
HSKYJP00008090
Company Overview
Office Location:
Calgary, AB, Canada.
Contract Duration:
1 year, 40 hours per week, 8 hours per day
Language:
English.
Description:
You’ll be joining the Data Engineering team in the Corporate Data Office (CDO). Our vision is to enable all staff to be more creative through insights derived from our trusted data and analytical capabilities leading to improved safety, increased production, lower GHG emissions, and actionable efficiencies. Collaborating with stakeholders across the organization, the CDO provides the capabilities, services, and governance to protect, deploy, and exploit value.
The Data Engineering team endeavors to deliver trusted, secure, high-quality, and easily consumable data sets to data consumers and practitioners across the organization. You’ll be joining a fun, exciting team comprised of highly motivated members working towards the common goal of democratizing our data.
Key Responsibilities:
You will apply your skills to help engineer data that provides business insights for our company.
Work with various data stakeholders throughout the organization to identify opportunities for leveraging company data that drives business solutions.
Building, fostering, and participating in a data community to connect data producers with data consumers and develop data literacy.
Work with various data stakeholders across the organization in the utilization of a modern data architecture that enables the collection, transformation, storage, modeling, and delivery of data to consumers and practitioners.
Apply data governance practices and procedures in the design and development of data pipelines.
Collaborate with cross-functional teams to design and develop data pipelines for visualization or analytic solutions.
Perform maintenance and support activities for data pipelines.
Support the build of big data environments that enable analytics solutions on a variety of big data platforms, including assessing the usefulness of new technologies and advocating for their adoption.
Education/Experience:
Undergraduate degree in Computer Science, Engineering, Applied Mathematics, or Commerce with a Business Analytics concentration, or related Information Science.
5 - 10+ years of experience in a data-related role
Good knowledge of oil and gas operations
Familiarity with common oil & gas applications (Well and land, Subsurface, Production operations, Drilling, Engineering, etc.)
Competently communicates both verbally and in writing with strong listening skills.
Willingness to learn and offer new creative ways to work that makes our team more agile and adaptable.
Commitment to being a team player who values and respects the differences in working within a diverse and inclusive team.
Technical expertise in developing data integrations, logical and physical data models, data mining, and data warehousing.
Proficiency in SQL
Deep understanding of architectures of traditional data architecture practices (MDM, ODS, data warehousing, and others), as well as transition to Next-Gen platforms such as Lakehouse Architecture, Cloud, distributed ledgers, etc.
Hands-on experience with various relational and NoSQL database technologies, including SQL Server, DB2, Oracle, HANA, Azure DB, AWS RDS, etc.
Understanding of Data Governance strategy, practices, and procedures to effectively deliver secure, trusted, and high-quality data.
Experience creating and maintaining data pipelines that operate on a variety of sources, such as APIs, FTP sites, cloud-based blob stores, databases (relational and non-relational), unstructured data, etc.
Experience with big data platforms, cloud technologies, and server-less environments (such as AWS, Azure, Redshift, EMR, etc.)
Proficiency in one or more of the following data modeling/ data munging/ data management toolsets such as Databricks, Data Virtualization, SQL Server, Stream sets, Azure Synapse, Redshift, Azure Data factory, AWS Glue, Alation Data Catalog, etc.
Proficiency in big data technologies including querying, creating, and populating data lakes.
Proficiency in any of the following programming languages, frameworks, and technologies: Python, R, and open-source databases such as PostgreSQL, MySQL, and MongoDB.
If you would like to apply to this position or any others, please send your resume and cover letter to
**We thank all applicants for applying, however, only those considered for an interview will be contacted directly**
Spirit Omega is committed to a diverse and inclusive workplace. We welcome applications from anyone, including members of visible minorities, women, Indigenous peoples, persons with disabilities, persons of minority sexual orientations and gender identities, and others with the skills and knowledge to productively engage with diverse communities.
Powered by JazzHR
VJX7my7nym
Show more
Show less","Data Engineering, Data Analytics, SQL, MDM (Master Data Management), ODS (Operational Data Store), Data Warehousing, Lakehouse Architecture, Cloud Computing, Distributed Ledgers, SQL Server, DB2, Oracle, HANA, Azure DB, AWS RDS, Data Governance, AWS, Azure, Redshift, EMR, Databricks, Data Virtualization, Stream sets, Azure Synapse, Azure Data Factory, AWS Glue, Alation Data Catalog, Python, R, PostgreSQL, MySQL, MongoDB","data engineering, data analytics, sql, mdm master data management, ods operational data store, data warehousing, lakehouse architecture, cloud computing, distributed ledgers, sql server, db2, oracle, hana, azure db, aws rds, data governance, aws, azure, redshift, emr, databricks, data virtualization, stream sets, azure synapse, azure data factory, aws glue, alation data catalog, python, r, postgresql, mysql, mongodb","alation data catalog, aws, aws glue, aws rds, azure, azure data factory, azure db, azure synapse, cloud computing, data engineering, data governance, data virtualization, dataanalytics, databricks, datawarehouse, db2, distributed ledgers, emr, hana, lakehouse architecture, mdm master data management, mongodb, mysql, ods operational data store, oracle, postgresql, python, r, redshift, sql, sql server, stream sets"
Sr. Performance Data Analyst,Energy Jobline,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/sr-performance-data-analyst-at-energy-jobline-3773712830,2023-12-17,Calgary, Canada,Mid senior,Onsite,"Job Title:
Sr. Performance Data Analyst
Location:
Calgary, AB
Schedule:
M-F (40 hours)
Job Type:
Hybrid
Contract length:
12 months
Anticipated Start date:
Jan 3rd, 2024
Description
As a senior role, the job requires a degree of influence, mentorship, coaching and guidance to our lesser experienced team members.
In addition, the role requires collaboration cross functionally while demonstrating an ability to effectively communicate and influence decision making.
This position requires the ideal candidate to be able to provide strong governance over the finance business processes at the individual manufacturing sites while supporting key business function stakeholders in collecting, consolidating, and reporting on key financial KPI’s and performance metrics. An essential part of the role is to maintain networks within the broader downstream operational teams, finance teams and performance networks.
The position requires an ability to identify, plan and implement key improvement initiatives in Finance to improve analysis quality, reduce reporting redundancies, increase productivity, and improve cycle time resulting in significant business. The ideal candidate should be able to help the organization identify opportunities for improvement by analyzing data and conducting research.
Key Deliverables
Strong business analysis skills:
Business process improvements (Striving towards standardization and simplification in the reporting cycles)
Support the delivery of various transformational improvement initiatives for the company, Finance and Downstream.
Actively participate and often lead the manufacturing Finance Modernization & digital initiatives.
Conduct wide-ranging business analysis in identifying and documenting requirements across our key manufacturing finance business processes.
Strong Data analytical Skills:
Develop actionable data models and data wrangling processes by integrating data extraction considerations into business solutions.
Analyze data to identify trends, patterns, and insights that inform business process improvements and efficiencies across manufacturing financial reporting.
Communicate effectively with all stakeholders in ensuring transparent and clear understanding of financial reporting as well as simple and easy to understand reporting visualizations.
Support back-end data models in Power BI and Spotfire to ensure stability in digital analytical reporting tools.
Support designing financial data model using SQL and DAX coding to help drive decision making and improve business performance transparency.
Project Management skills:
Actively engage and project manage large financial modernization initiatives.
Actively seek opportunities of deploying AI and digital analytics in various facets of financial reporting (i.e. data wrangling, data modelling, data gathering etc.)
Qualifications
Bachelor’s degree in computer science, Finance, or Engineering
CPA/MBA considered an asset.
Advanced knowledge in Power BI, SQL, and other digital analytics software
Excellent oral & written communications skills to work collaboratively with various stakeholders across the organization.
Hands-on experience building complex Power BI reports using various data sources and data integrations.
Proficiency in creation of reporting data visualization solution and experience in building technical & process documentation requirements.
SAP Knowledge considered an asset.
Apply here or send your resume to
mengmeun.young@nesfircroft.com
With over 90 years' combined experience, NES Fircroft (NES) is proud to be the world's leading engineering staffing provider spanning the Oil & Gas, Power & Renewables, Chemicals, Construction & Infrastructure, Life Sciences, Mining and Manufacturing sectors worldwide. With more than 80 offices in 45 countries, we are able to provide our clients with the engineering and technical expertise they need, wherever and whenever it is needed. We offer contractors far more than a traditional recruitment service, supporting with everything from securing visas and work permits, to providing market-leading benefits packages and accommodation, ensuring they are safely and compliantly able to support our clients.
Show more
Show less","Power BI, Spotfire, SQL, Data Visualization, Data Wrangling, Data Analysis, Business Process Improvement, Project Management, Financial Reporting, Digital Analytics, Data Modeling, DAX, SAP, AI","power bi, spotfire, sql, data visualization, data wrangling, data analysis, business process improvement, project management, financial reporting, digital analytics, data modeling, dax, sap, ai","ai, business process improvement, data wrangling, dataanalytics, datamodeling, dax, digital analytics, financial reporting, powerbi, project management, sap, spotfire, sql, visualization"
"Marketing Business Analytics Data Analyst, Marketing – Lakeland",Publix Super Markets,"Lakeland, FL",https://www.linkedin.com/jobs/view/marketing-business-analytics-data-analyst-marketing-%E2%80%93-lakeland-at-publix-super-markets-3766959082,2023-12-17,Lake Wales,United States,Mid senior,Onsite,"Description
Are you interested in working for a Fortune 100 retailer? Do you want to be able to apply your analytical knowledge and experience to help solve constantly changing business and customer facing opportunities? As a Data Analyst on the Business Analytics team you will be supporting Publix with Marketing measurement and data science methodologies that support decisions and create solutions related to the financial performance of Publix, its products and services, Publix Marketing campaigns, and customer behavior data.
The Data Analyst is responsible for developing financial analytical solutions supporting and measuring operational performance across the organization. Specifically, the role includes defining business problems, identifying key performance metrics, researching and gathering data, performing financial analyses and communicating insights to business partners. This position leverages data analytics to uncover sales and customer trends that drive decision making that improves profitability and performance of our stores, products, and services. This position requires an agile software development mindset and the ability to deliver end-to-end full-stack solutions, including ideation/brainstorming, data/feature engineering and cleaning, data wrangling, coding, testing, deploying, data visualization, and maintenance.
Responsibilities Include
Responding to analytical requests that provide data driven business insights and recommendations (including development of interactive dashboards and leadership reporting) across Publix business areas, including: Marketing, Corporate Purchasing, Real Estate, and other support areas
Developing descriptive analytical models to provide business insights and recommendations across Publix business areas, including Marketing, Corporate Purchasing, Real Estate, and other support areas
The development, execution, deployment, and improvement of team data asset production processes, weekly reporting, automation, QC/data validation, gathering info from SMEs
Participating as the primary analytical support for data driven cross-functional strategic initiatives
Identifying and engaging in opportunities to grow knowledge and skills related to best practices in analytical technologies, methodologies, verification, knowledge of data sets, drawing insights, and communicating learnings
Important: Please note that Publix will not sponsor any hire for this position for an H-1B visa or permanent residence.
[#video#https://youtu.be/BfHbfPMQy-U{#400,300#}#/video#]
Required Qualifications
Bachelor’s degree in Data Analytics, Economics, Finance, Geography & GIS, Computer Science, Statistics, Data Science, Engineering, Marketing, Business, or another analytical discipline from an accredited college or university or equivalent experience
1-year in an analytical role specializing in data manipulation, interpretation, report writing, and data insight summarization/reporting
1-year analytical experience:
Conducting Financial performance analysis; OR
Writing SQL; OR
Writing R code or similar functionality within other statistical tools; OR
Using a Geographic Information System (GIS) specializing in demographic analyses
For recent college graduates, these requirements can be satisfied by relevant coursework
Advanced knowledge of mathematics and statistics
Accounting and/or Finance knowledge (basic knowledge of accounting principles)
The ability to work effectively without close supervision or guidance
The ability to self-direct work activities and take action on tasks without direction from a supervisor
The ability to work productively and effectively when faced with stressful work situations and time pressures
The ability to manage time so that the priority of the activities determines the timing and amount of attention they receive
Arithmetic computation and reasoning
The ability to inspect, cleanse, transform and model data from various sources with the goal of discovering useful information/trends, informing conclusion and supporting decision-making
The ability to interpret, analyze, and explain financial information
The ability to understand and apply new information, procedures, or principles to perform job duties
The ability to understand the “big picture” and the interrelationships among various parts of the organization
Innovation and creativity
Problem solving skills
The ability to develop and maintain professional, trusting, positive, and productive working relationships with associates, managers, customers, and vendors
Customer focused
The ability to persuade, convince or achieve buy-in from others
Negotiation skills
Active listening skills
Presentation skills
Verbal and written communication skills
The ability to collate data extracts and summarize key findings into concise relevant and easy to digest reports/deliverables for decisions makers
Advanced skills in Microsoft Word, Excel
Basic skills in Microsoft PowerPoint
Intermediate skills in Microsoft Access
Skilled in SSMS or other SQL based application
Preferred Qualifications
MBA or Master’s degree in Analytics, Economics, Finance, Computer Science, Statistics, Data Science, Engineering, Marketing, Business, Geographic Information Systems, GIScience, or another analytical discipline
3+ years of analytics work related experience
Data Science Certificate Or Business Analytics Certificate, Salesforce, Geographic Information System Professional
Recent completions of Coursera or similar courses to demonstrate ability to stay on trend with analytics
Knowledge of Data analysis techniques, including: inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision making
In depth understanding of Information Systems design methodologies and testing processes
Skills in Data visualization/story telling
Expert skills in Microsoft Word, Excel
Intermediate skills in Microsoft PowerPoint
Advanced skills in Microsoft Access
Skills in Alteryx, Tableau, PowerBI, and other Business Intelligence Tools (ETL)
Skills in Salesforce
Skills in ArcGIS/ESRI
Benefits
Employee stock ownership plan that contributes Publix stock to associates each year at no cost
An opportunity to purchase additional shares of our privately-held stock
401(k) retirement savings plan
Group health, dental and vision plans plan
Paid Time Off
Paid Parental Leave
Short- and long-term disability insurance
Tuition reimbursement
Free hot lunches (buffet-style) at facilities with a cafeteria
Visit our website to see all of our benefits: Benefits – Jobs (publix.com)
Additional Information
Your application may have additional steps that you will need to complete in order to remain eligible for consideration. Please be sure to monitor your email, including your spam folder, on a daily basis for critical, time-sensitive emails that could require action within 24-48 hours.
Please do not use your Publix email address when applying. Once your application has been successfully submitted you will receive a confirmation email.
In Publix’s corporate offices, we value in-person interactions, similar to those our store associates have with customers. Many of Publix’s corporate offices offer work from home up to two days a week. Each business area implements their policy differently and should be discussed during interviews.
Show more
Show less","Data Analytics, Data Science Methodologies, Financial Analysis, Sales and Customer Trends, Agile Software Development, Software Development Mindset, Data Engineering, Data Wrangling, Coding, Testing, Deploying, Data Visualization, Maintenance, SQL, R, GIS, Geographic Information System, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Access, SSMS, Data Analysis Techniques, Information Systems Design Methodologies, Data Visualization, Alteryx, Tableau, PowerBI, Business Intelligence Tools, Salesforce, ArcGIS/ESRI","data analytics, data science methodologies, financial analysis, sales and customer trends, agile software development, software development mindset, data engineering, data wrangling, coding, testing, deploying, data visualization, maintenance, sql, r, gis, geographic information system, microsoft word, microsoft excel, microsoft powerpoint, microsoft access, ssms, data analysis techniques, information systems design methodologies, data visualization, alteryx, tableau, powerbi, business intelligence tools, salesforce, arcgisesri","agile software development, alteryx, arcgisesri, business intelligence tools, coding, data analysis techniques, data engineering, data science methodologies, data wrangling, dataanalytics, deploying, financial analysis, geographic information system, gis, information systems design methodologies, maintenance, microsoft access, microsoft excel, microsoft powerpoint, microsoft word, powerbi, r, sales and customer trends, salesforce, software development mindset, sql, ssms, tableau, testing, visualization"
IT Delivery Manager - Data and AI Services,Dice,"Lakeland, FL",https://www.linkedin.com/jobs/view/it-delivery-manager-data-and-ai-services-at-dice-3787364371,2023-12-17,Lake Wales,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Publix, is seeking the following. Apply via Dice today!
Publix Super Markets, Inc. is the largest privately-owned food retailer in the nation with more than 1,200 stores and more than 200,000 associates throughout the Southeast. We are associate-owned, proud of our family atmosphere, and consistently named as one of the best companies to work for in America. We are largely debt-free and renowned for our financial performance as well as our premier customer service. Publix?s Information Services (I/S) department is located in Lakeland, Florida and Alpharetta, Georgia and employs over 1000 associates. I/S provides the information technology required for all Publix business units including retail stores and all office and warehouse environments. The Publix I/S mission is to maximize the value of information technology to Publix associates and customers. The IT Delivery Manager of the Data and AI Services team is responsible for ensuring the maximum return on Data Platforms such as NoSQL, Data Lake, and Snowflake to meet the needs of the business while providing the governance, security, and compliance of those systems. They will also manage AI development, assets, and a governance program to ensure responsible AI practices.
Lead a team of Data Engineers working to develop and manage Data Platforms and Data Pipelines to provide enterprise data for the business areas
Work with Data Architects, Financial Analysts, and Auditors to improve data validity and consistency for business analysis and reporting
Lead a team of AI Developers and Engineers to consult, develop, implement, and support AI systems using Azure OpenAI and Cognitive Services and other AI models, SDKs, or services
Own the AI Innovation Center and Governance Committee while working with a cross-functional team to review, advise, and approve AI standards, solutions, and training
Plan projects for the team, including resource allocation and milestones to meet business area needs and IT requirements
Foster and maintain good working relationships with business area representatives to remain in sync on direction and future solution requests
Manage relationships with software vendors as needed
Provide oversight of support processes to ensure timely defect resolution and appropriate escalation of system issues
Provide communication and escalation to IT and business area management on critical incidents and problems
Monitor production environments to ensure systems are running efficiently and accurately
Perform human resource management duties, including hiring, training, performance monitoring, mentoring, career counseling, pay reviews, promotions, terminations, and work prioritization for assigned associates
Participate in establishing departmental processes, KPIs, and measurement reporting
Ensure availability for 24/7 tier 3 system support as needed
Bachelor?s degree in Computer Science, Business, or other analytical disciplines or equivalent work experience
8 years of IT experience, including design, implementation, and maintenance of data, application, or AI systems
5 years of leadership experience in one or more of the following:
Demonstrated leadership in IT project management; experience in organizing, planning, and executing large-scale projects from vision through implementation, involving internal personnel, contractors, and vendors; ability to analyze project needs and determine resources needed to meet objectives
Successfully developed and implemented or supported applications in new computing architecture environments using new and emerging technologies
Demonstrated ability to manage multidiscipline, high-performance work teams
Developed efficient and effective solutions to diverse and complex business problems
3 years of experience hands-on experience delivering data solutions as a data engineer, data analyst, or database administrator
Solid understanding of big data concepts, including data lakes, structured streaming, structured vs unstructured data, micro-batch processing, etc.
Strong analytical, problem-solving, and conceptual skills
Ability to make sound decisions and solve complex business and technical issues
Strong teamwork and interpersonal skills (the ability to work well with others and be a team member)
Ability to communicate verbally and in writing with all levels of management and thrive in a cross-functional environment
High degree of confidence, maturity, self-motivation, commitment, and integrity
Enthusiasm, initiative, pride in work, and commitment to Publix and our Mission
10 years of IT experience, including designing, developing, and implementing Data, Applications, and AI systems
2 years of experience working with big data solutions using technical capabilities such as Hadoop, Apache Spark / Databricks
2 years of experience working in the public cloud using different data solutions such as AWS RDS, Azure Synapse, or Google Cloud Platform Big Query
Experience implementing or contributing to data or AI governance programs
Experience developing, implementing, and supporting AI systems using Azure OpenAI and Cognitive Services and other AI models, SDKs, or services
Experience with supporting large-scale applications within a 24/7 environment; willingness to work extended/off hours unexpectedly if critical issues arise
Experience with human resource management, including knowledge of team dynamics, team building concepts, and human resource policies
Basic knowledge of financial management and budgeting
Employee stock ownership plan that contributes Publix stock to associates each year at no cost
An opportunity to purchase additional shares of our privately-held stock
401(k) retirement savings plan
Group health plan (with prescription benefits)
Group dental plan
Group vision plan
Sick pay
Paid Parental Leave
Long-term disability insurance
Company-paid life insurance (with accidental death & dismemberment benefits)
Tuition reimbursement
Vacation pay
Free hot lunches (buffet-style) at facilities with a cafeteria
Paycheck direct deposit
Credit union
Access to over 50 discount offers including discounts on computer, vehicle and wireless purchases
6 paid holidays (associates can exchange the following holidays with their manager?s approval: New Year?s Day, Memorial Day, Fourth of July, and Labor Day)
Show more
Show less","Data Platforms, Data Lake, Snowflake, NoSQL, AI, Azure OpenAI, Cognitive Services, AI models, SDKs, Data Pipelines, Data Architects, Financial Analysts, Auditors, AI Developers, AI Engineers, Business analysis, Azure Synapse, Google Cloud Platform Big Query, Hadoop, Apache Spark, Databricks, AWS RDS, Data governance, Human resource management, Team building, Financial management, Budgeting","data platforms, data lake, snowflake, nosql, ai, azure openai, cognitive services, ai models, sdks, data pipelines, data architects, financial analysts, auditors, ai developers, ai engineers, business analysis, azure synapse, google cloud platform big query, hadoop, apache spark, databricks, aws rds, data governance, human resource management, team building, financial management, budgeting","ai, ai developers, ai engineers, ai models, apache spark, auditors, aws rds, azure openai, azure synapse, budgeting, business analysis, cognitive services, data architects, data governance, data lake, data platforms, databricks, datapipeline, financial analysts, financial management, google cloud platform big query, hadoop, human resource management, nosql, sdks, snowflake, team building"
Data Engineer,HMGCC,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-hmgcc-3779346827,2023-12-17,Luton, United Kingdom,Associate,Onsite,"Salary: £56,384
Location: Milton Keynes
This role is available full-time for 37 hours per week or part-time with a minimum coverage of 25 hours / Flexible or compressed working patterns available.
This role is based in the office.
Do you have Data Engineering expertise? Are you interested in Cyber Security? Do you want to work where you can make a difference and use your Data Engineering skills, in cyber security research to protect National Security in the UK and overseas?
Your expertise in developing and constructing data products and services and integrating them into systems and business processes will be applied to high priority and cutting-edge cyber security research. You will be required to take raw data acquired from a diverse range of source and transform them to make them available for exploitation by our Data Scientists and Security Researchers.
Our work is intellectually stimulating and varied. You will be part of a highly collaborative and creative team environment; focused on solving cyber security problems. We use Red Team methodology in our security research, and work in small teams. We secure a wider variety of technologies, and you’ll work on projects that range from short-term security assessments to long-term in-depth analysis of complex systems.
Our investment in your continued professional development is world-class, and throughout you will develop your career through technical training, professional accreditation, mentoring, and most importantly, on the job training. There are also opportunities for overseas travel for conferences and training too.
More About You
To apply for this role, you will already have:
Data modelling, designing, testing and writing code* and data services (*we use a variety of programming languages and development environments)
Creating ingest pipelines for standard datasets; identifying and investigating data quality or compliance issues; providing support to analytics.
Planning and leading development of data services to meet user needs.
Following a test-driven approach, writing code that is clean, secure, and well tested, following best practice.
Operating the services, you build, and identifying and correcting issues in production.
Collaborating and communicating with a variety of technical and non-technical stakeholders.
To strengthen your application, it would be beneficial if you also have:
Using data engineering and manipulation techniques to produce/improve data performance by selecting the most appropriate tools and technologies.
The application of data engineering to cyber security problems
Computer networking and communication protocol fundamentals.
Your Personal Skills
You will be the type of person who:
Has an eye for detail – and is conscious of security risks and implications of using various technologies.
Considers security an integral pat of any data engineering build.
Is open-minded and demonstrates strong intellectual curiosity.
Is a quick learner and leans in to learning a range of new tools and technique, as required – in order to stay current with technological advancements.
Is technically curious, with excellent problem-solving abilities and the ability to adapt to unplanned changes, overcome obstacles and see the task through to completion.
Takes pride in being the ‘go-to person’, establishing your credibility equally with technical peers and not technical end users.
Enjoys actively maintaining your technical knowledge, making sure tar you keep up with the all the latest developments.
Feels confident with uncertainty, helping others to reduce and manage ambiguity around potential ways forward.
Putting your skills to use
Using your diverse range of skills within the team you will:
Develop and innovate various data engineering processes, to serve the requirements of a range of customers.
Enjoy working with data and various pipelining technologies to understand where Data Engineering can add value to support Data Scientists and other researchers.
Enjoy working with a range of experts, knowing when to draw on their expertise to help deliver project requirements.
Be able to communicate detailed technical information to a wide range of audiences including non-technical staff.
What we Offer
Now that we’ve explained a bit about what you will we be doing, here’s what we can offer you:
25 days annual leave, rising to 30 after 5 years’ service.
8 Bank Holiday days off in addition to your annual leave.
2 ½ additional days leave (set dates through the year).
Paid overtime or time off in lieu. Your salary is for your contracted hours. If you work more, we believe you should be rewarded for that.
Training. Lots and lots of training. This can be anything from a ½ day internal course to a distant learning degree. If it’s relevant to your career, we will aim to support you.
Pension. Our Pension scheme is Alpha. It’s competitive.
Sports & Social Society. Whether its classic cars or baking, golf or crafting, we have a society for you.
On site nursery (subject to availability).
3 days paid leave a year to support a registered charity of your choice.
Free parking
Free onsite gym, open 24 hours a day
Relocation Package*
Affinity Groups – whether you are disabled, LGBTQ and/or from an Ethnic Minority. We provide an inclusive environment and support for everyone.
You’ll be working under a capability framework, which recognises and rewards specialist expertise.
Training and Development:
We offer our commitment in developing our colleagues and recognise this may look different for everyone. We’ll use this capability framework to provide guidance and in turn the tools to support you in your career at HMGCC. Not only that but there’ll be opportunities to gain qualifications and pursue and number of specialist pathways, as well as undertaking training tailored to your skillset, coaching and mentoring.
As you develop, grow, and maintain your technical skills, you will have the opportunity to progress through capability framework pay points. You’ll need to regularly demonstrate and endorse your skills based on the set assessment criteria. If you find yourself in a position where you aren’t maintaining the required skills, you’ll be given the opportunity and support to rectify this, as you could find yourself reverting to a lower level of capability and pay.”
About HMGCC
We are HMGCC, the engineering specialists at the heart of the UK’s national security.
We work with the National Security Community, UK Government, academia, private sector partners and international allies to bring engineering ingenuity to the national security mission, creating tools and technologies that drive us ahead and help to protect the nation.
We invest in our people, not only with training but also support throughout your career with us; to allow you to reach your full potential at work, as well as enjoy the work/life balance many aspire to.
To find out more about us and if we’re right for you, please
visit our website.
Equal Opportunities
Inclusion & Diversity really matters to us. It's not just about numbers or visible differences.
HMGCC are committed to providing equal opportunities and encourage applications from candidates of all backgrounds, ethnicities, gender identities, sexual orientations, and those with disabilities or who are neurodiverse.
We’re not looking for any one type of person. We’re looking for talent from all different backgrounds to join our organisation, where we genuinely respect and value each other's differences, as well as our similarities. It makes us even better at what we do.
We are proud to offer candidates the opportunity to apply through the Offer Of Interview Scheme.
The scheme ensures eligible candidates are selected for an interview as long as their application meets the minimum criteria for the role. To find out more and how you can apply under this scheme, please see the
inclusion and diversity information
on our website.
Show more
Show less","Data Engineering, Cyber Security, Data Modelling, Data Design, Data Quality, Data Compliance, Data Analytics, Data Services, Data Pipelining, TestDriven Development, Secure Coding, Production Support, Stakeholder Communication, Data Manipulation, Data Performance, Computer Networking, Communication Protocols, Technical Curiosity, ProblemSolving, Adaptability, Communication Skills","data engineering, cyber security, data modelling, data design, data quality, data compliance, data analytics, data services, data pipelining, testdriven development, secure coding, production support, stakeholder communication, data manipulation, data performance, computer networking, communication protocols, technical curiosity, problemsolving, adaptability, communication skills","adaptability, communication protocols, communication skills, computer networking, cyber security, data compliance, data design, data engineering, data manipulation, data modelling, data performance, data quality, data services, dataanalytics, datapipeline, problemsolving, production support, secure coding, stakeholder communication, technical curiosity, testdriven development"
Data Scientist,HMGCC,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-scientist-at-hmgcc-3779342820,2023-12-17,Luton, United Kingdom,Associate,Onsite,"Salary: £63,312
Location: Milton Keynes
This role is available full-time for 37 hours per week or part-time with a minimum coverage of 25 hours / Flexible or compressed working patterns available.
Do you have Data Science expertise? Are you interested in Cyber Security? Do you want to work where you can make a difference, and use your Data Scientist skills, in cyber security research to protect National Security in the UK and overseas?
As a Data Scientist, you will be using your experience in mathematics and computer science, you will apply scientific methods, process, and algorithms, to extract knowledge and insights from noisy, structured and unstructured data.
Our work is intellectually stimulating and varied. We use Red Team methodology in our security research, and work in small teams. We secure a wide variety of technologies, and you’ll work on projects that range from short-term security assessments to long-term in-depth analysis of complex systems.
Our investment in your continued professional development is world-class, and throughout you will develop your career through technical training, professional accreditation, mentoring, and most importantly – on the job training. There are also opportunities for overseas travel for training and conferences.
More About You
To apply for this role, you will already have experience in and be able to show evidence in your application of:
Using programming experience to build scalable data products for a range of users.
Using a range of data sources, analytical tools, and techniques throughout the lifecycle to develop and deploy robust data science solutions.
Analysing and interpreting data.
Applying data science techniques to present, communicate and disseminate complicated data science products clearly, to achieve and measure impact to a range of audiences, including non-technical staff.
To strengthen your application, it would be beneficial if you also have experience in and be able to show evidence in your application of:
The application of data science to cyber security problems, in anything from vulnerability analysis support, to debugging software and/or network traffic analysis.
Understanding the ethical considerations of data science approaches and the policy and legislation applicable in this area
Computer science fundamentals and digital forensics (including memory and instructions sets – x86/ARM/MIPS/etc.)
Domain experience is a preferable but not essential.
Your Personal Skills
You will be the type of person who:
Is technically curious.
Enjoys problem solving.
Has the ability to adapt to unplanned changes and overcome obstacles.
Enjoys seeing tasks through to completion.
Is open minded.
Demonstrates strong intellectual curiosity.
Has excellent problem-solving skills.
Takes pride in being the ‘go to’ person, establishing your credibility equally with technical peers and non-technical end users.
Enjoys continuously promoting personal development by expanding data science knowledge and sharing best practice across departments/industry.
Feels confident with uncertainty, helping others to reduce and manage ambiguity around potential ways forward.
Putting your skills to use
Using your diverse range of skills within the team, you will:
Enjoy working with Policy and operations, to understand where Data Science can add value to support strategic and operational decision-making, in order to create impact.
Sources, accesses, manipulates, and engineers data processes with data that typically have characteristics of volume, velocity and variety, and veracity.
Builds credible statistical models from the data and uses the best coding practices to generate reproducible work.
Proactively uses an evolving range of data analysis tools and techniques, and the ability to adapt to unplanned changes, overcome obstacles, and see tasks through to completion.
Enjoys working with a range of experts, knowing when to draw on their expertise to help deliver project requirements.
Has an interest in mentoring Junior data scientists and other engineers and sharing knowledge and experience with the team.
Identifying efficient and effective ways to use Data Science to tackle business and organisational challenges, applying innovative techniques.
What we offer
Now that we’ve explained a bit about what you will we be doing, here’s what we can offer you:
25 days annual leave, rising to 30 after 5 years’ service.
8 Bank Holiday days off in addition to your annual leave.
2 ½ additional days leave (set dates through the year).
Paid overtime or time off in lieu. Your salary is for your contracted hours. If you work more, we believe you should be rewarded for that.
Lots and lots of training. This can be anything from a ½ day internal course to a distant learning degree. If it’s relevant to your career, we will aim to support you.
Our Pension scheme is Alpha. It’s competitive.
Sports & Social Society. Whether its classic cars or baking, golf or crafting, we have a society for you.
On site nursery (subject to availability).
3 days paid leave a year to support a registered charity of your choice.
Free parking
Free onsite gym, open 24 hours a day
Relocation Package*
Affinity Groups – whether you are disabled, LGBTQ and/or from an Ethnic Minority. We provide an inclusive environment and support for everyone.
You’ll be working under a capability framework, which recognises and rewards specialist expertise.
Training and Development Section:
We offer our commitment in developing our colleagues and recognise this may look different for everyone. We’ll use this capability framework to provide guidance and in turn the tools to support you in your career at HMGCC. Not only that but there’ll be opportunities to gain qualifications and pursue and number of specialist pathways, as well as undertaking training tailored to your skillset, coaching and mentoring.
As you develop, grow, and maintain your technical skills, you will have the opportunity to progress through capability framework pay points. You’ll need to regularly demonstrate and endorse your skills based on the set assessment criteria. If you find yourself in a position where you aren’t maintaining the required skills, you’ll be given the opportunity and support to rectify this, as you could find yourself reverting to a lower level of capability and pay.”
About HMGCC
We are HMGCC, the engineering specialists at the heart of the UK’s national security.
We work with the National Security Community, UK Government, academia, private sector partners and international allies to bring engineering ingenuity to the national security mission, creating tools and technologies that drive us ahead and help to protect the nation.
We invest in our people, not only with training but also support throughout your career with us; to allow you to reach your full potential at work, as well as enjoy the work/life balance many aspire to.
To find out more about us and if we’re right for you,
please visit our website.
Equal Opportunities
Inclusion & Diversity really matters to us. It's not just about numbers or visible differences.
HMGCC are committed to providing equal opportunities and encourage applications from candidates of all backgrounds, ethnicities, gender identities, sexual orientations, and those with disabilities or who are neurodiverse.
We’re not looking for any one type of person. We’re looking for talent from all different backgrounds to join our organisation, where we genuinely respect and value each other's differences, as well as our similarities. It makes us even better at what we do.
HMGCC is proud to hold the status of Disability Confident Employer. In line with this, we offer the Disability Confident - Offer of Interview. To find out more and how you can apply under this scheme, please see the
inclusion and diversity information
on our website.
Show more
Show less","Data Science, Cyber Security, Mathematics, Computer Science, Scientific Methods, Algorithms, Red Team Methodology, Programming, Data Analysis, Data Interpretation, Python, R, Java, Scala, SQL, NoSQL, Hadoop, Spark, Machine Learning, Artificial Intelligence, Cloud Computing, Big Data, Ethics, Policy, Legislation, Computer Science Fundamentals, Digital Forensics, Memory, Instructions Sets, x86, ARM, MIPS, Technical Curiosity, Problem Solving, Adaptability, Obstacle Overcoming, Task Completion, Openmindedness, Intellectual Curiosity, Critical Thinking, Credibility, Personal Development, Knowledge Sharing, Uncertainty Management, Policy and Operations, Data Engineering, Data Manipulation, Statistical Modeling, Coding Practices, Data Analysis Tools, Mentoring, Innovation, Training, Development, Capability Framework, Qualifications, Coaching","data science, cyber security, mathematics, computer science, scientific methods, algorithms, red team methodology, programming, data analysis, data interpretation, python, r, java, scala, sql, nosql, hadoop, spark, machine learning, artificial intelligence, cloud computing, big data, ethics, policy, legislation, computer science fundamentals, digital forensics, memory, instructions sets, x86, arm, mips, technical curiosity, problem solving, adaptability, obstacle overcoming, task completion, openmindedness, intellectual curiosity, critical thinking, credibility, personal development, knowledge sharing, uncertainty management, policy and operations, data engineering, data manipulation, statistical modeling, coding practices, data analysis tools, mentoring, innovation, training, development, capability framework, qualifications, coaching","adaptability, algorithms, arm, artificial intelligence, big data, capability framework, cloud computing, coaching, coding practices, computer science, computer science fundamentals, credibility, critical thinking, cyber security, data analysis tools, data engineering, data interpretation, data manipulation, data science, dataanalytics, development, digital forensics, ethics, hadoop, innovation, instructions sets, intellectual curiosity, java, knowledge sharing, legislation, machine learning, mathematics, memory, mentoring, mips, nosql, obstacle overcoming, openmindedness, personal development, policy, policy and operations, problem solving, programming, python, qualifications, r, red team methodology, scala, scientific methods, spark, sql, statistical modeling, task completion, technical curiosity, training, uncertainty management, x86"
Data Privacy Analyst,Barclay Simpson,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-privacy-analyst-at-barclay-simpson-3771416864,2023-12-17,Luton, United Kingdom,Associate,Hybrid,"Data Protection Analyst / Assistant is sought for a market leading, global company based in Milton Keynes.
The role will require you to work on complex Data Subject Access Requests. It is a pre-requisite that you have good previous experience of handling DSARs which involve a significant amount of data.
The ideal candidate will have an interest in GDPR and will be interested in developing their career further in Data Protection. The role offers excellent progression and development prospects.
You will be required to work in Milton Keynes twice a week, so you must live locally or be willing to relocate to the local area. Base salary up to £30,000 plus benefits.
Show more
Show less","Data Protection, Data Subject Access Requests, GDPR","data protection, data subject access requests, gdpr","data protection, data subject access requests, gdpr"
Connected Car Data Analyst,Expleo Group UK Jobs,"Bedfordshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/connected-car-data-analyst-at-expleo-group-uk-jobs-3742681014,2023-12-17,Luton, United Kingdom,Associate,Hybrid,"An opportunity has arisen for a Connected Car Data Analyst to support our global automotive client, based in Bedfordshire, in their vision to deliver the most distinct and innovative range of road vehicles, all equipped with state-of-the-art technology. The role is for an ongoing contract.
The successful Connected Car Data Analyst will be responsible for:
• Investigating Market issues related to connected services and provide appropriate solutions
• Utilise analytical tools to understand trends and find the root cause
• Use advanced MS Excel techniques including Power Query, Power Pivot and Power Automation to manipulate cloud data and produce reports
• Liaise with Project Quality, Management and Engineering teams to ensure early response to concerns
Background and experience required for the role of Connected Car Data Analyst:
• Ideally Degree qualified or equivalent in a relevant engineering discipline
• Basic understanding of automotive development cycle and telematics / connected services
• Experience on platform health monitoring in a cloud platform system
• Experience of MS Azure OMS & Kusto Query is an advantage but not essential
• Excellent communication skills.
• Valid UK / European driving licence is essential
To meet with current legislation, right to work checks will be carried out to ensure candidates are eligible to work within the UK. We are unable to support candidates requiring sponsorship to work in the UK.
To make an application for the Connected Car Data Analyst role or require further information, please contact:
Ansa Varghese
ansa.varghese@expleogroup.com
Show more
Show less","Connected Car Data Analysis, Market Research, Trend Analysis, Root Cause Analysis, MS Excel, Power Query, Power Pivot, Power Automation, Cloud Data Manipulation, Report Generation, Project Management, Engineering, Automotive Development Cycle, Telematics, Cloud Platform System Monitoring, MS Azure OMS, Kusto Query, Communication Skills","connected car data analysis, market research, trend analysis, root cause analysis, ms excel, power query, power pivot, power automation, cloud data manipulation, report generation, project management, engineering, automotive development cycle, telematics, cloud platform system monitoring, ms azure oms, kusto query, communication skills","automotive development cycle, cloud data manipulation, cloud platform system monitoring, communication skills, connected car data analysis, engineering, kusto query, market research, ms azure oms, ms excel, power automation, power pivot, power query, project management, report generation, root cause analysis, telematics, trend analysis"
Senior Data Engineer,Movement8,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-movement8-3773793123,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"Movement8 has teamed up with a renowned consultancy seeking to expand its Azure Data Specialist team in collaboration with an Energy company. This opportunity offers a dynamic environment that emphasizes collaboration, growth, and continuous learning.
As an Azure Data specialist specialising in Real-Time Data, your role will involve constructing and managing a sophisticated data platform on Azure. You'll be tasked with developing solutions for Data Lakes and Warehouses, collaborating with teams focused on Reporting and Data Visualization to deliver comprehensive data visualization solutions.
Key responsibilities include employing Data Vault and Dimensional modeling techniques for data model construction and establishing both batch and streaming data pipelines that are automated, efficient, and reusable. Additionally, you'll coordinate with teams dedicated to Reporting and Data Visualization to ensure the delivery of comprehensive solutions.
The ideal candidate will possess proficiency in Azure Data Lake Storage, familiarity with Azure Data Factory (including the creation and management of metadata-driven pipelines), and a deep understanding of SQL and Data Streaming Technologies.
The package includes:
£80,000 (depending on experience)
10% Bonus
Learning and Development Budget
Full Remote Working
Holidays
Pension
If you are an Azure Data Specialist with a strong focus on Real-Time Data, please reach out for more information at (url removed)
Show more
Show less","Azure Data Specialist, RealTime Data, Data Lake, Data Warehouse, Data Vault, Dimensional Modeling, Batch Data Pipeline, Streaming Data Pipeline, Azure Data Lake Storage, Azure Data Factory, SQL, Data Streaming Technologies","azure data specialist, realtime data, data lake, data warehouse, data vault, dimensional modeling, batch data pipeline, streaming data pipeline, azure data lake storage, azure data factory, sql, data streaming technologies","azure data factory, azure data lake storage, azure data specialist, batch data pipeline, data lake, data streaming technologies, data vault, datawarehouse, dimensional modeling, realtime data, sql, streaming data pipeline"
Data Cabling Engineer,Digital Waffle,"Wembley Park, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-at-digital-waffle-3775219018,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"Position: x3 Data Cabling Engineer (Temp/Contract/Freelance)
Department: Information Technology / Network Infrastructure
Location: Wembley, Greater London, UK
Job Summary: We are seeking a trio of skilled and dedicated Data Cabling Engineers to join our Information Technology team. The successful candidates will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
We are after a team of 3, with x2 out right engineers and x1 to take lead as a senior/charge hand - so it is beneficial to have SSSTS, SMSTS, Prince2 etc. All 3 will be on the tools.
If you are a motivated team of Data Cabling Engineers with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Jake on jake@digitalwaffle.co
Show more
Show less","CAT6, CAT6a, Copper Cables, Cable installation, Cable routing, Cable pathways, Cable connectors, Cable outlets, Termination panels, Cable labeling, Cable testing, Troubleshooting, Cable certification, Quality Assurance, Network expansion, Safety protocols, Technical drawings, Schematics, Cabling diagrams, Problemsolving, Communication skills, RJ45, Punchdown tool, Cable certifier, Cable labels, Markers, Label printer, Tape measure, Ruler, Level, Cable ties, Velcro straps, Cable clips, Mounts, Power drill, Screwdrivers, Wall anchors, Safety glasses, Work gloves, Documentation, Pen, Notepad, Mobile device, Steeltoed boots, Hard hat, SSSTS, SMSTS, Prince2","cat6, cat6a, copper cables, cable installation, cable routing, cable pathways, cable connectors, cable outlets, termination panels, cable labeling, cable testing, troubleshooting, cable certification, quality assurance, network expansion, safety protocols, technical drawings, schematics, cabling diagrams, problemsolving, communication skills, rj45, punchdown tool, cable certifier, cable labels, markers, label printer, tape measure, ruler, level, cable ties, velcro straps, cable clips, mounts, power drill, screwdrivers, wall anchors, safety glasses, work gloves, documentation, pen, notepad, mobile device, steeltoed boots, hard hat, sssts, smsts, prince2","cable certification, cable certifier, cable clips, cable connectors, cable installation, cable labeling, cable labels, cable outlets, cable pathways, cable routing, cable testing, cable ties, cabling diagrams, cat6, cat6a, communication skills, copper cables, documentation, hard hat, label printer, level, markers, mobile device, mounts, network expansion, notepad, pen, power drill, prince2, problemsolving, punchdown tool, quality assurance, rj45, ruler, safety glasses, safety protocols, schematics, screwdrivers, smsts, sssts, steeltoed boots, tape measure, technical drawings, termination panels, troubleshooting, velcro straps, wall anchors, work gloves"
Lead Data Engineer,easyJet,"Luton, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-easyjet-3766159802,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"From the price of tickets and the planning of flights, to the way we manage our corporate functions – data is an incredibly important part of how we run our business here at easyJet. And thanks to the success and growth of our current team we’re now looking for a new Lead Data Engineer to deliver IT change and help us in our ambition to become the leading data driven airline.
Requirements
Managing up to three Data Engineers and working with a wider team of Data Scientists, Analysts, and Leaders, you’ll apply your technical expertise and inspirational leadership skills to design, deliver and implement data solutions that meet functional and non-functional requirements and standards. You’ll also:
Champion our distributed data platform and drive data product adoption
Identify and implement internal process improvements along with other opportunities to deliver end-to-end data product solutions that drive business value
Identify new and emerging technology, methods, trends and techniques in the Data Engineering space
What You’ll Need To Do The Role
Naturally for this role you’ll already have significant experience designing and building data solutions on a cloud based, big data distributed system – along with a solid understanding of data concepts and data modelling principles; applied across traditional warehousing, OLTP and modern data lakes.
And as well as the leadership and team management skills you’ll need to inspire your team you’ll also have the communication and relationship management skills to work with technical and non-technical audiences alike.
From a technical skills point of view you’ll need experience of:
Python along with modern software development and release engineering practices (e.g. TDD, CI/CD)
Apache Spark or any other distributed data programming frameworks (e.g. Flink, Arrow, MapR)
Writing efficient SQL
Working with a range of cloud-based big data technologies (e.g. HDFS, Blob Storage, Spark, Kafka, Delta, Hive, Oozie, NiFi)
Linux and containerisation
Enterprise scheduling tools (e.g. Apache Airflow, Spring DataFlow, Control-M)
Data Science Tooling for ML (e.g. MLFlow, Sagemaker), Data Analytics Tooling for dashboarding/visualisation (e.g. Tableau, Looker, Power BI) and interactive API’s/Web Apps (e.g. R-Studio, Shiny, Flask, Django)
Enterprise scheduling tools (e.g. Apache Airflow, Spring DataFlow, Control-M)
What You’ll Get In Return
Competitive base salary
Up to 20% bonus
25 days holiday
BAYE, SAYE & Performance share schemes
7% pension
Life Insurance
Flexible benefits package
Excellent staff travel benefits
About EasyJet
At easyJet our aim is to make low cost travel easy – connecting people to what they value using Europe’s best airline network, great value fares, and friendly service.
It takes a real team effort to carry over 90 million passengers a year across 35 countries. Whether you’re working as part of our front line operations or in our corporate functions, you’ll find people that are positive, inclusive, ready to take on a challenge, and that have your back. We call that our ‘Orange Spirit’, and we hope you’ll share that too.
Apply
Complete your application on our careers site.
We encourage individuality, empower our people to seize the initiative, and never stop learning. We see people first and foremost for their performance and potential and we are committed to building a diverse and inclusive organisation that supports the needs of all. As such we will make reasonable adjustments at interview through to employment for our candidates.
Business Area
Data
Primary Location
United Kingdom-London-London Luton Airport
Organisation
Data
Schedule
Full-time
Unposting Date
Ongoing
Show more
Show less","Python, Linux, Data Engineering, Data Science, Data Modeling, Apache Spark, Flink, Arrow, MapR, SQL, HDFS, Kafka, Containerization, Apache Airflow, MLFlow, Sagemaker, Tableau, Looker, Power BI, RStudio, Shiny, Flask, Django","python, linux, data engineering, data science, data modeling, apache spark, flink, arrow, mapr, sql, hdfs, kafka, containerization, apache airflow, mlflow, sagemaker, tableau, looker, power bi, rstudio, shiny, flask, django","apache airflow, apache spark, arrow, containerization, data engineering, data science, datamodeling, django, flask, flink, hdfs, kafka, linux, looker, mapr, mlflow, powerbi, python, rstudio, sagemaker, shiny, sql, tableau"
Data Analyst,Big Red Recruitment,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-big-red-recruitment-3782723931,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"Do you want to make a difference to world health?
Would totally greenfield data projects and the setup of a new department be of interest?
As a Data Analyst you’ll join the technology department for an organisation who specialise in biomedical research for the academic, pharmaceutical and biotech community. This organisation was at the forefront of sample processing during the Covid pandemic.
Your role will sit in a newly established data team where you’ll work closely with developers and project managers to understand data requirements, conduct data collections, execute data processing, and manage historical data cleansing activities. You’ll perform trend analysis and troubleshooting using all data variables, implement data capture controls & automated data monitoring processes.
You will join a team of dynamic and likeminded individuals, who work in a collaborative and supportive culture.
Location: Milton Keynes - 5 days a week in office.
Salary: £45 000 - £55 000 / 26 days holiday +2 days every 2 years till a 40 day maximum is reached / pension 5%/6% or 8%/12%.
Ideally You Will Have
Strong SQL and Excel skills.
Experience in data integrations and automations
Good problem solving skills.
The ability to troubleshoot issues using data and other relevant monitoring tools.
If you feel as though you have the experience to succeed in this opportunity, please apply/send through your CV as soon as possible as we have interview slots available!
Unfortunately, we will not be able to provide sponsorship for this role and the successful candidate will need to live in the UK and be commutable to Milton Keynes.
We are an equal opportunity recruitment company. This means we welcome applications from all suitably qualified people regardless of race, sex, disability, religion, sexual orientation or age.
We are particularly invested in Neurodiversity inclusion and offer reasonable adjustments in the interview process. Reasonable adjustments are changes that we can make in the interview process if your disability puts you at a disadvantage compared with others who are not disabled. If you would benefit from a reasonable adjustment in your interview process, please call or email one of our recruiters.
Show more
Show less","Data Analysis, SQL, Excel, Data Integration, Automation, Troubleshooting, Data Monitoring","data analysis, sql, excel, data integration, automation, troubleshooting, data monitoring","automation, data integration, data monitoring, dataanalytics, excel, sql, troubleshooting"
Senior Data Engineer,CityFibre,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-cityfibre-3781195057,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"Job Description
Our goal at CityFibre is to unlock the digital potential of Britain by creating a broadband network that works seamlessly for everyone, every day. Powerful, gigabit-speed connections that enable people across entire communities to enjoy life in full flow. To do this, we make sure that our people’s voices are heard, their ideas are welcomed, and everyone is given the opportunity to make a real impact.
We encourage change, have a challenger mindset, foster innovation and nurture a welcoming, inclusive and supportive community. In this environment we allow you to bring your best-self to work every day, helping you to achieve your goals, grow your career and reach your full potential.
We’re a Times Top 50 Employer for Women; are endorsed by WORK180; have pledged our commitment to the Armed Forces Covenant; and are a Disability Confident Employer. Working together with our Employee Networks, we’re committed to changing the industry for everyone.
What You Can Expect From Us
Everyone at CityFibre has an important part to play, and we reflect that in our benefits. With
a competitive salary
depending on experience, when you join us, you’ll also enjoy 25 days’ holiday, a day off on your birthday, a day off to support a charity or organisation of your choice, a performance-related bonus scheme, and private healthcare.
We believe we deliver our best when we work together. It’s the way we do things, trusting and giving our people the autonomy to manage their own day-to-day, promoting wellbeing, whilst still prioritising time together with our colleagues to connect, collaborate and share our ideas.
How You’ll Change It Up
You’ll oversee the organisation, maintenance, and use of data across the business, playing a crucial role in ensuring that the data is accurate, reliable, and easily accessible. Day to day, this some of what you can expect to be doing:
Supporting the entire data lifecycle, from data creation to data retirement
Developing and implementing data management policies and procedures
Developing and implementing data governance policies, standards, and procedures to ensure data quality, integrity, and security
Establishing guidelines for data usage, access, and retention
Implementing data quality controls and validation processes to monitor and improve data accuracy and completeness
What You’ll Bring To The Role
You’ll have significant experience in data management, with a deep understanding of telecom data, data warehouses and data lakes. You’ll also bring the following:
AWS and Snowflake
Knowledge of data lineage and how it is essential for governance, quality assessment and compliance with regulations
Strong communication skills and the ability to translate between non-technical stakeholders and technical teams
Any experience with Erwin software would be a bonus!
We’re looking for people who don’t settle for ‘good enough’ and push things forward. If you’ve just found yourself thinking, ‘I’d love to do this’, we want to hear from you. Let’s change it up.
Diversity, Inclusion and Belonging
Our people have a wide range of experiences and perspectives – and it’s this diversity of thought that makes us who we are.
CityFibre is working hard to change up the telecoms industry. This means we’re committed to providing equal opportunities to people from all backgrounds and commitments, including your carer and family status, ethnicity, sexual orientation, religion, age, disability, gender and gender identity. We’re also offering training programmes and placement opportunities to support the skilled, career changers, service-leavers, and returning workers to become part of our teams.
Additional Information
If there’s any additional support or adjustment you require in the application process to help you achieve your best, please get in touch by emailing
careers@cityfibre.com
. We’re here to support you in changing up your career.
Show more
Show less","AWS, Snowflake, Erwin, Telecom data, Data management, Data warehouse, Data lake, Data governance, Data quality, Data security, Data lineage, Data retention, Data validation","aws, snowflake, erwin, telecom data, data management, data warehouse, data lake, data governance, data quality, data security, data lineage, data retention, data validation","aws, data governance, data lake, data lineage, data management, data quality, data retention, data security, data validation, datawarehouse, erwin, snowflake, telecom data"
Business Intelligence Developer - Data Analyst,Scania UK,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/business-intelligence-developer-data-analyst-at-scania-uk-3784827039,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"Company Description
SGB
Job Description
Business Intelligence Developer – Data Analyst
Salary: £45,000 - £55,000 plus excellent benefits package including competitive pension scheme, 25 days holiday, access to car schemes and many more!
As a Business Intelligence Developer, you have an analytical approach to your work with experience and knowledge of Power BI. You are a team player, has strong problem-solving abilities and driven by data to meet the needs of various stakeholders throughout the business.This role requires you to work closely with the rest of the data teams and all IT areas to assess and provide solutions that consist of data management, data accuracy and visualisation layers.
Key Responsibilities
Maintain SGB data platform reporting layer and support business with needs.
Ensure documentation and checks are in place to ensure knowledge base stays up to date.
Help business Data Champions to create and maintain their self-serve workspaces.
Assist end users with training and education of self-serve platform.
Create MI / BI reports as per the specifications of business requirements.
Create and maintain reports and visuals in Power BI Platform.
Create and maintain rigid frameworks / templates for visuals that follow Scania branding and best practices.
Provide input on best practices and trends to ensure the best quality of supporting systems.
Through detailed monitoring ensure data quality of reports.
Create and maintain monitoring tasks that ensure data quality.
When we ask our manager what they love about working for Scania, they tell us it’s all about the people. We can foster a culture where learning and development is valued. This is an excellent opportunity to join the IT department whilst it grows and evolves. We want you to come and enjoy Scania’s journey with us.
Working for Scania is not just about the job. It’s about you too, and this is where the company goes the extra mile and provides an industry leading employment package. We offer an excellent benefits package which includes a pension scheme, complementary life insurance, financial incentive schemes and discounts on major retail outlets including groceries.
At Scania we invest considerably in colleague development, and you can expect to receive comprehensive training and career progression not only in the UK but throughout our Global organisation.
We have a strong and supportive culture, where each individual is seen, respected and has the potential to contribute. We trust each other to act and make decisions, and we believe in a more flexible future workplace based on individual needs. For us, diversity and inclusion is a strategic necessity. By having colleagues with the widest possible range of skills, knowledge, backgrounds, and experiences, we ensure we have the right people and together with an inclusive corporate culture, this drives our business forward. We want our colleagues to feel proud and happy to work for us, no matter where they are from or who they are, and we strive to achieve an inclusive and family-friendly environment for everyone.
Scania is dedicated to delivering exceptional products and services to our customers and the successful candidate will reflect our core values; CUSTOMER FIRST, RESPECT, ELIMINATION OF WASTE, RESPONSIBILITY and TEAM SPIRIT in all we do.
Profile Description
﻿
Show more
Show less","Power BI, Data management, Data accuracy, Data visualization, MI / BI reporting, Business intelligence, Data analysis, Reporting, Monitoring, Data quality assurance, Training, Education, Selfserve platform, Visuals, Scania branding, Best practices, Monitoring tasks","power bi, data management, data accuracy, data visualization, mi bi reporting, business intelligence, data analysis, reporting, monitoring, data quality assurance, training, education, selfserve platform, visuals, scania branding, best practices, monitoring tasks","best practices, business intelligence, data accuracy, data management, data quality assurance, dataanalytics, education, mi bi reporting, monitoring, monitoring tasks, powerbi, reporting, scania branding, selfserve platform, training, visualization, visuals"
Data Analyst,Big Red Recruitment,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-big-red-recruitment-3782726984,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"Do you want to make a difference to world health?
Would totally greenfield data projects and the setup of a new department be of interest?
As a Data Analyst you’ll join the technology department for an organisation who specialise in biomedical research for the academic, pharmaceutical and biotech community. This organisation was at the forefront of sample processing during the Covid pandemic.
Your role will sit in a newly established data team where you’ll work closely with developers and project managers to understand data requirements, conduct data collections, execute data processing, and manage historical data cleansing activities. You’ll perform trend analysis and troubleshooting using all data variables, implement data capture controls & automated data monitoring processes.
You will join a team of dynamic and likeminded individuals, who work in a collaborative and supportive culture.
Location: Milton Keynes - 5 days a week in office.
Salary: £45 000 - £55 000 / 26 days holiday +2 days every 2 years till a 40 day maximum is reached / pension 5%/6% or 8%/12%.
Ideally you will have:
Strong SQL and Excel skills.
Experience in data integrations and automations
Good problem solving skills.
The ability to troubleshoot issues using data and other relevant monitoring tools.
If you feel as though you have the experience to succeed in this opportunity, please apply/send through your CV as soon as possible as we have interview slots available!
Unfortunately, we will not be able to provide sponsorship for this role and the successful candidate will need to live in the UK and be commutable to Milton Keynes.
Show more
Show less","SQL, Excel, Data integration, Automation, Troubleshooting, Data analysis, Data monitoring, Data cleansing, Data capture, Trend analysis","sql, excel, data integration, automation, troubleshooting, data analysis, data monitoring, data cleansing, data capture, trend analysis","automation, data capture, data integration, data monitoring, dataanalytics, datacleaning, excel, sql, trend analysis, troubleshooting"
Lead Data Engineer,Addition+,"Watford, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-addition%2B-3774763203,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"We are currently working on behalf of our trusted client who are looking for a
Lead Data Engineer.
Join our trusted clients, market leaders in property finance for investors, landlords, homeowners, and developers!
What You Can Expect
Work Type - Permanent
Salary - £75k
Location - Watford
Plus, amazing employee benefits including flexible working arrangements and more!
Main Responsibilities As a Lead Data Engineer
Be point of contact - the most technically astute in the data engineering team.
Core Experience & Skills Required
Extensive experience with SQL, Amazon Redshift & ideally MySQL.
Extensive experience with visualisation tools, ideally with Amazon Quick Sight.
Experience with AWS Glue and the ability to write Python scripts, e.g. for ETL.
Knowledge of SAP Business Objects would be helpful but not necessary - currently in use but in the process of migrating to Redshift & Quick Sight.
What’s In It For You
Great company to work for!
Flexible hybrid working options.
Discretionary bonus scheme.
For further information on this Lead Data Engineer
role apply below
We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, colour, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
By applying, you confirm you are happy to be added to the Addition mailing list regarding future suitable positions. You can opt out of this at any time simply by contacting one of our consultants.
Show more
Show less","SQL, Amazon Redshift, MySQL, Amazon Quick Sight, AWS Glue, Python, ETL, SAP Business Objects","sql, amazon redshift, mysql, amazon quick sight, aws glue, python, etl, sap business objects","amazon quick sight, amazon redshift, aws glue, etl, mysql, python, sap business objects, sql"
Data Engineering Consultant,Nigel Frank International,"Luton, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineering-consultant-at-nigel-frank-international-3740000926,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"I am working with a Microsoft Partnered consultancy who are looking for a Data Engineering Consultant to join their growing team. You will have the opportunity to work on a variety of client projects across a number of different sectors such as retail, finance and the public sector.
In this role you will utilise your experience with the latest Azure technologies to provide guidance to clients. As a hands-on consultant you use your experience as a data engineer to ensure the timely delivery on clients data driven projects.
You will be joining a people centred business, who invest heavily in its team. You will be given the opportunity to explore other areas of technologies that may be of interest to you, including data science, machine learning and AI. You will have time designated purely to training and development, with internal development schemes as well as funded Microsoft learning, training courses and certifications.
This is an exciting time to join a hugely successful and growing business who have a number of exciting roadmap of projects planned all the way into 2024!
As part of this role, you will be responsible for some of the following areas.
Offer guidance to clients on data driven projects
Take the lead on data engineering projects focused around the Microsoft and Azure tech stack
Deliver on all aspects of the project including requirements gathering, testing, implementation and maintenance
This is a salaried role paying up to £55,000 per annum depending on experience and a company benefits package. This is a home based role with occasional, fully expensed visits to client sites as and when required. You will also need to commute to company retreats once per quarter to meet other members of the team.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Coding experience with languages such as SQL and Python.
Strong knowledge of Databricks for data ingestion and transformation would be beneficial
Excellent communication skills
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Data Engineering, Azure, Data Factory, Synapse, Azure Data Lake, ETL, SQL, Python, Databricks, Data Ingestion, Data Transformation, Communication Skills","data engineering, azure, data factory, synapse, azure data lake, etl, sql, python, databricks, data ingestion, data transformation, communication skills","azure, azure data lake, communication skills, data engineering, data factory, data ingestion, data transformation, databricks, etl, python, sql, synapse"
Senior Data Analyst,Keech Hospice Care,"Luton, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-keech-hospice-care-3784663132,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"“MAKING THE DIFFERENCE WHEN IT MATTERS THE MOST”
Would you like to join a top 100 Best Large Company rated outstanding to work for? An opportunity to deliver real value to the wider community?
As one of the leading charities, Keech Hospice Care are recruiting for a Senior Data Analyst (12 month FTC) who is ready to embark on a new journey with us. Supporting the charity to ensure the voices of our supporters, service users and wider community are at the heart of everything we do. This is a new role, within our data team, one where you will have the scope to shape and continue to deliver real lasting impact to all our stakeholders.
What You’ll Do
Creating insightful thematic reports and presentations to provide valuable insights to drive informed decision-making
Work closely with the Data Team Manager and other cross-functional teams to understand current and future data needs of the business from care staff, to marketing and fundraising teams
Build and shape an ongoing programme of work to place our supporters, service users and stakeholders at the heart of the organisation
Translate data findings into actionable recommendations for stakeholders
Make sure to read the full description below, and please apply immediately if you are confident you meet all the requirements.
What You Need To Do It
3+ years of hands-on experience in data analysis, Bachelor’s or Master’s degree in Data Science, Statistics, or a related field is highly desirable
Strong data communication skills, with the ability to tell compelling stories through data
Proficiency in establishing data dashboards to monitor key metrics
Ability to extract, manage and manipulate data for analysis purposes from various data sources and platforms.
Experience in visualizing data using tools such as Power BI, Tableau, or equivalent platforms
Familiarity with customer relationship management tools and/or marketing and communications data is highly desirable
If you are a dynamic individual with a solid background in data analysis and a passion for driving data-driven decision-making, join us in shaping a future where data is not just a tool, but a strategic advantage for Keech Hospice Care!
We thrive on seeing you flourish in your role and invest in continuous growth with supportive staff and experts on hand. We offer great benefits which include hybrid working an option to continue an NHS pension, generous annual leave amongst many more.
Ready to apply, simply click the button and complete our online application.
Not sure or have question please refer to our FAQ alternatively reach out to our recruitment team to arrange an informal chat .
We are committed to promoting equality of opportunity across our workforce and encourage all to apply.
Keech provide free, specialist care for Adults in Luton and South Bedfordshire, and children from Bedfordshire, Hertfordshire and Milton Keynes living with a life limiting illness. Keech supports adults and children to live pain and symptom free, to spend untroubled time with their loved one, to understand what’s happening to them, to stay out of hospital and to make the most of the time they have.
Show more
Show less","Data Analysis, Data Visualization, Data Management, Data Communication, Data Dashboards, Power BI, Tableau, Customer Relationship Management, Marketing, Communications, DataDriven DecisionMaking","data analysis, data visualization, data management, data communication, data dashboards, power bi, tableau, customer relationship management, marketing, communications, datadriven decisionmaking","communications, customer relationship management, data communication, data dashboards, data management, dataanalytics, datadriven decisionmaking, marketing, powerbi, tableau, visualization"
Senior Data Analyst,IRIS | Networx | Recruitment Software & Services,"Luton, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-iris-networx-recruitment-software-services-3782405314,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"“MAKING THE DIFFERENCE WHEN IT MATTERS THE MOST”
Would you like to join a top 100 Best Large Company rated outstanding to work for? An opportunity to deliver real value to the wider community?
As one of the leading charities, Keech Hospice Care are recruiting for a Senior Data Analyst (12 month FTC) who is ready to embark on a new journey with us. Supporting the charity to ensure the voices of our supporters, service users and wider community are at the heart of everything we do. This is a new role, within our data team, one where you will have the scope to shape and continue to deliver real lasting impact to all our stakeholders.
What You’ll Do
Creating insightful thematic reports and presentations to provide valuable insights to drive informed decision-making
Work closely with the Data Team Manager and other cross-functional teams to understand current and future data needs of the business from care staff, to marketing and fundraising teams
Build and shape an ongoing programme of work to place our supporters, service users and stakeholders at the heart of the organisation
Translate data findings into actionable recommendations for stakeholders
What You Need To Do It
3+ years of hands-on experience in data analysis, Bachelor’s or Master’s degree in Data Science, Statistics, or a related field is highly desirable
Strong data communication skills, with the ability to tell compelling stories through data
Proficiency in establishing data dashboards to monitor key metrics
Ability to extract, manage and manipulate data for analysis purposes from various data sources and platforms.
Experience in visualizing data using tools such as Power BI, Tableau, or equivalent platforms
Familiarity with customer relationship management tools and/or marketing and communications data is highly desirable
If you are a dynamic individual with a solid background in data analysis and a passion for driving data-driven decision-making, join us in shaping a future where data is not just a tool, but a strategic advantage for Keech Hospice Care!
We thrive on seeing you flourish in your role and invest in continuous growth with supportive staff and experts on hand. We offer great benefits which include hybrid working an option to continue an NHS pension, generous annual leave amongst many more.
Ready to apply, simply click the button and complete our online application.
Not sure or have question please refer to our FAQ alternatively reach out to our recruitment team to arrange an informal chat .
We are committed to promoting equality of opportunity across our workforce and encourage all to apply.
Show more
Show less","Data analysis, Data visualization, Power BI, Tableau, Data communication, Data storytelling, Customer relationship management, Data dashboards, Data extraction, Data manipulation, Data science, Statistics","data analysis, data visualization, power bi, tableau, data communication, data storytelling, customer relationship management, data dashboards, data extraction, data manipulation, data science, statistics","customer relationship management, data communication, data dashboards, data extraction, data manipulation, data science, data storytelling, dataanalytics, powerbi, statistics, tableau, visualization"
Data Engineering Consultant,Nigel Frank International,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineering-consultant-at-nigel-frank-international-3739799993,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"I am working with a Microsoft Partnered consultancy who are looking for a Data Engineering Consultant to join their growing team. You will have the opportunity to work on a variety of client projects across a number of different sectors such as retail, finance and the public sector.
In this role you will utilise your experience with the latest Azure technologies to provide guidance to clients. As a hands-on consultant you use your experience as a data engineer to ensure the timely delivery on clients data driven projects.
You will be joining a people centred business, who invest heavily in its team. You will be given the opportunity to explore other areas of technologies that may be of interest to you, including data science, machine learning and AI. You will have time designated purely to training and development, with internal development schemes as well as funded Microsoft learning, training courses and certifications.
This is an exciting time to join a hugely successful and growing business who have a number of exciting roadmap of projects planned all the way into 2024!
As part of this role, you will be responsible for some of the following areas.
Offer guidance to clients on data driven projects
Take the lead on data engineering projects focused around the Microsoft and Azure tech stack
Deliver on all aspects of the project including requirements gathering, testing, implementation and maintenance
This is a salaried role paying up to £55,000 per annum depending on experience and a company benefits package. This is a home based role with occasional, fully expensed visits to client sites as and when required. You will also need to commute to company retreats once per quarter to meet other members of the team.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Coding experience with languages such as SQL and Python.
Strong knowledge of Databricks for data ingestion and transformation would be beneficial
Excellent communication skills
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure, Azure Synapse, Azure Data Factory, Azure Data Lake, ETL, SQL, Python, Databricks","azure, azure synapse, azure data factory, azure data lake, etl, sql, python, databricks","azure, azure data factory, azure data lake, azure synapse, databricks, etl, python, sql"
Data Analyst Fixed Term Contract (Mid Level) - Ocado Solutions,Ocado Solutions,"Hatfield, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-fixed-term-contract-mid-level-ocado-solutions-at-ocado-solutions-3775993375,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"Please note: This mid level Data Analyst position is for a Fixed Term, 12 month contract.
Data Analyst FTC (Mid Level)
| Ocado Solutions | Hatfield | Hybrid (2 days office)
About Us
“We are on a mission to transform the future of grocery retail through sustained technology innovation.”
Ocado Group is putting the world’s retailers online using the cloud, robotics, AI, and IoT. We develop the innovative software and hardware systems that power Ocado.com, the world’s largest online-only grocery retailer as well as the global ‘Ocado Smart Platform’. With everything from websites to fully autonomous warehouses that we design in-house, our employees need to be specialists in a wide range of technologies to help drive our business. We architect and build groundbreaking, game-changing technology solutions.
About The Team
This role is within Ocado’s Solutions division.Supporting our Partners around the world in a consistent way is essential to our long-term relationship with them; and to our commercial success. The Solutions teams provide all of our Ocado Smart Platform (OSP) Partners with wide-ranging remote and local support for their platform; so they can deliver their retail customer promises every day.
The mission of the Solutions Analytics team is to
“Provide the insight, advice and services to help our partners and prospects succeed and grow with OSP”.
We have four teams working across different parts of the platform: eCommerce, Fulfilment, Last Mile and External Reporting & Process Mining.
What would I be doing?
As an Analyst you will specialise in extracting and interpreting data to identify efficiency and growth opportunities within the businesses of our OSP Partners, directly impacting their commercial decisions and enhancing the shopping experience for hundreds of thousands of end customers around the world. We have billions of data points at our disposal, mapping across all elements of the OSP platform, from the website to the robots which power our CFCs.
You will be helping to answer questions such as “What is the cost to our OSP Partners of a late customer delivery?”
You will also be helping to drive our profitability whilst continuously improving the services we deliver. Your insights will have a direct contribution to Ocado’s long term goal of changing the way the world shops. You’ll be working on things like;
From day one, you will be working with cross-functional partners to support our Partners’ global operations, by way of building KPI dashboards, ad-hoc data pulls, and business reviews
Partner with other teams to identify trends and opportunities in driving growth for our OSP Partners
Partner with Ocado Technology to identify and close gaps in the data that we provide to our Partners
Problem Solving, with our department and business growing rapidly, and with even bigger growth plans ahead, new challenges and insights will develop for you to tackle
Creating data to represent Ocado Solutions’ objectives and interests to other functions (e.g. Product Continuous Improvement)
Project Management and Support. Whether it is your own idea or a project managed by another department, you will be involved in supporting different projects across the business
What We’re Looking For
Accomplished analyst with experience working with Big Data, ideally with 2 or more years analytical experience
Comfortable communicating with technical and non-technical senior management and key stakeholders
Extensive experience with data analysis and visualisation tools (e.g. SQL, Excel/Google Sheets, Looker)
Able to work independently using data to create compelling stories and achieve positive outcomes
Experience adapting to a fast-changing environment
Strong communication skills and able to communicate clearly with partners that may not have English as their first language
Willingness to travel internationally to meet with our OSP Partners
Additional Nice To Have
Python experience - able to utilise Google Colab
Experience using Celonis process mining platform
Experience using GIS software and practices
Experience in FMCG retail, online grocery or automated fulfilment environment
Language skills (e.g. Spanish, French, Japanese, Korean)
What Do I Get In Return
30 Day ‘work from anywhere’ policy +
Remote working for the month of August & 50% of December
25 days annual leave, rising to 27 days after 5 years service (plus optional holiday purchase)
Pension scheme (various options available including employer contribution matching up to 7%)
Private Medical Insurance
22 weeks paid maternity leave and 6 weeks paid paternity leave (once relevant service requirements complete)
Train Ticket loan (interest-free)
Cycle to Work Scheme
Opportunity to participate in Share save and Buy as You Earn share schemes
15% discount on Ocado.com and free delivery for all employees
Income Protection(can be up to 50% of salary for 3 years) and Life Assurance(3 x annual salary)
About Ocado Group
When our journey started, we were all about changing the way people shop for groceries. 20 years later, we’re developing our own ground breaking technology, opening multiple sites at an accelerating rate and engaging in exciting new business partnerships around the world!
Transforming an industry also means transforming the way we do business. As we continually develop new technologies and customer propositions, we’re looking for the Everyday Pioneers to lead us into the next stage of our evolution: enhancing our capabilities, inspiring our teams and developing new ways of working.
Changing the way the world shops, for good.
Show more
Show less","Big Data, Data Analysis, Data Visualization, SQL, Excel/Google Sheets, Looker, Python, Google Colab, Celonis, GIS, FMCG, Retail, Online Grocery, Automated Fulfilment, KPI","big data, data analysis, data visualization, sql, excelgoogle sheets, looker, python, google colab, celonis, gis, fmcg, retail, online grocery, automated fulfilment, kpi","automated fulfilment, big data, celonis, dataanalytics, excelgoogle sheets, fmcg, gis, google colab, kpi, looker, online grocery, python, retail, sql, visualization"
Principal Engineer - Vehicle Road Load Data (RLD),Energy Jobline,"Flitwick, England, United Kingdom",https://uk.linkedin.com/jobs/view/principal-engineer-vehicle-road-load-data-rld-at-energy-jobline-3773343495,2023-12-17,Luton, United Kingdom,Mid senior,Onsite,"Principal Engineer – Vehicle Road Load Data (RLD)
Permanent, based on site in Bedford
Salary: up to £65,000 depending on experience and other benefits including eligibility to take part in the bonus which will target 7.5% of salary at OTE, Life Cover, Healthcare cashback plans and many more.
Are you a technical leader? Do you enjoy reviewing test data and developing test procedures? Would you like the experience of traveling in Europe to test in different environments? If “yes”… could this newly created role be the opportunity you’ve been looking for?
We’re looking for a Principal Engineer to join a leading Automotive Test facility. The successful Engineer will join a company offering customisable vehicle testing solutions, which include; proving grounds and laboratories for vehicle engineering and homologation for transport, agricultural and defence industries (amongst others!)
The Durability and Measurement activity conducts whole vehicle testing on test tracks and public roads. Tests include structural and powertrain durability, corrosion, QA, NVH, road load data, dynamics, and many others.
What would the Principal Engineer do?
Provide technical guidance and leadership to test engineers
Lead/support customer discussions from a technical standpoint
Validation of testing methodologies and analysis
Develop test procedures to meet ISO17025 standards and maintain uncertainty budgets
Develop department capability for upcoming regulatory or industry demands
Lead integration and cross-pollination of Durability and Measurement tests in UK, France, Morocco, and Finland
Support quotation and specification of new and bespoke test and measurement activities
Identify and lead improvement activities
Undertake analysis of results, including statements of conformity, opinions, and interpretations
To report, review and provide authorisation of results
What skills and experience will the Principal Engineer need?
Relevant engineering degree or extensive experience in similar field(s)
Vehicle testing and development
ISO17025 compliance
Detailed knowledge and experience in some or all of the following areas would be advantageous
Advanced data processing (nCode Glyphworks desirable)
Data acquisition and instrumentation (DeweSoft desirable)
Strain gauging
Wheel force transducers
Load cells
Accelerometers
Fatigue and damage calculation
Test profile correlation (road to track, track to track, track to rig)
Essential requirements?
Willing to undergo a Criminal Disclosure check
SC clearance or ability to achieve this (UK national with residency in the UK for the last 5 years)
Full UK driving license
For further information on this excellent opportunity, please call Kym on (phone number removed)
This advert represents a summary of the full role description. To apply, please submit a copy of your up to date CV clearly indicating your relevant experience. Applicants must have an existing right to work in the UK and evidence of eligibility will be required. Suitable candidates will be contacted
Show more
Show less","Vehicle testing, ISO17025 compliance, nCode Glyphworks, DeweSoft, Advanced data processing, Strain gauging, Wheel force transducers, Load cells, Accelerometers, Fatigue and damage calculation, Test profile correlation, Durability testing, Measurement techniques, Data acquisition, Instrumentation, Engineering degree","vehicle testing, iso17025 compliance, ncode glyphworks, dewesoft, advanced data processing, strain gauging, wheel force transducers, load cells, accelerometers, fatigue and damage calculation, test profile correlation, durability testing, measurement techniques, data acquisition, instrumentation, engineering degree","accelerometers, advanced data processing, data acquisition, dewesoft, durability testing, engineering degree, fatigue and damage calculation, instrumentation, iso17025 compliance, load cells, measurement techniques, ncode glyphworks, strain gauging, test profile correlation, vehicle testing, wheel force transducers"
Data Engineer,BJSS,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-3439444635,2023-12-17,Luton, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
About the Role
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Please note: any applicants must be able to gain valid SC clearance.
Show more
Show less","Data engineering, Cloud computing (AWS Azure GCP), Data services (Databricks Data Factory Synapse Kafka Redshift Glue Athena BigQuery S3 Cloud Data Fusion), Coding best practices, Design patterns, Code and data versioning, Dependency management, Code quality and optimisation, Error handling, Logging, Monitoring, Validation, Alerting, Python, Objectoriented programming, CI/CD tooling, Data storage, Data processing, Parallel computing, Workflow programming, Complex queries, Relational data stores, Nonrelational data stores","data engineering, cloud computing aws azure gcp, data services databricks data factory synapse kafka redshift glue athena bigquery s3 cloud data fusion, coding best practices, design patterns, code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation, alerting, python, objectoriented programming, cicd tooling, data storage, data processing, parallel computing, workflow programming, complex queries, relational data stores, nonrelational data stores","alerting, cicd tooling, cloud computing aws azure gcp, code and data versioning, code quality and optimisation, coding best practices, complex queries, data engineering, data processing, data services databricks data factory synapse kafka redshift glue athena bigquery s3 cloud data fusion, data storage, dependency management, design patterns, error handling, logging, monitoring, nonrelational data stores, objectoriented programming, parallel computing, python, relational data stores, validation, workflow programming"
Data Engineer,Solutions Through Knowledge,"Bedford, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-solutions-through-knowledge-3774390182,2023-12-17,Luton, United Kingdom,Mid senior,Hybrid,"Job title:
Data Engineer
Location:
Hybrid / Bedford
Salary:
£60,000 per annum + 20% Bonus + double-match pension contributions up to 10%
Standard hours:
Monday to Friday 8am-4.30pm / 9am-5.30pm
Benefits:
25 days holiday + statutory holiday
Role
We are assisting a successful well-established organisation who are searching for a talented Data Engineer to join a newly created department.
They are just about to embark on a massive digital transformation project which will impact all of their systems - a truly massive data overhaul - as part of fulfilling their ambition to develop a data enabled business for the future.
Working in a Hybrid capacity from their Bedford office, you will be welcomed into an extremely friendly and high-achieving team.
As Data Engineer, you will play an integral part in achieving this transformation by providing the following:
Responsibility for developing accurate, efficient data transformations which meet customer needs to agreed timescales
Ensuring the stability, robustness, and resilience of the projects you design and build, enjoying substantial autonomy while working within agreed standards
Align activities to the Group wide data transformation programme both current future orientated
Duties
Design and build of reliable, robust, and accurate data pipelines based on agreed best practices
Transformation of source data to meet business requirements
Support the team in consolidating manual processes into a managed data environment
Work with key stakeholders and other data consumers to gather requirements
Ad-hoc support on database administration tasks as needed
Skills & Experience
As an experienced Data Engineer, ideally you possess relevant experience in ingesting, transforming and supporting data processes within Snowflakes ecosystem and be comfortable with the following:
Understanding of Snowflake architecture, data modelling and administration
Experience in designing and implementing efficient ETL/ELT pipelines
Experience with AWS Data Services - AWS S3
Comfortable working with a range of data sources and formats e.g. JSON, XML, Flat files, API Integration
Understanding of dimensional modelling for Data Warehousing (Kimball)
Proficient in writing SQL, Stored procedures and views. Creating and optimising complex queries, analysing query performance, use of partitioning and clustering.
Experience in effectively coaching novice developers
Good problem solving and data analysis skills
Excellent written and oral communication skills, ability to communicate complex concepts
Ability to translate business requirements into technical solutions
Other Technology
Databases MS SQL Server, Oracle
Informatica
Python
Data Lakes
Jira, BitBucket and Confluence
Company
A fantastic organisation that is able to combine a large corporate presence with a caring, friendly and personable dynamic.
They value their employees and are keen to develop & promote staff from within, which has led to excellent staff retention rates.
Show more
Show less","Data Engineering, Data Transformation, Snowflake, ETL/ELT pipelines, AWS Data Services, AWS S3, JSON, XML, Flat files, API Integration, Dimensional modelling, Data Warehousing, SQL, Stored procedures, Views, Query performance, Partitioning, Clustering, Coaching, Problem solving, Data analysis, Communication, MS SQL Server, Oracle, Informatica, Python, Data Lakes, Jira, BitBucket, Confluence","data engineering, data transformation, snowflake, etlelt pipelines, aws data services, aws s3, json, xml, flat files, api integration, dimensional modelling, data warehousing, sql, stored procedures, views, query performance, partitioning, clustering, coaching, problem solving, data analysis, communication, ms sql server, oracle, informatica, python, data lakes, jira, bitbucket, confluence","api integration, aws data services, aws s3, bitbucket, clustering, coaching, communication, confluence, data engineering, data lakes, data transformation, dataanalytics, datawarehouse, dimensional modelling, etlelt pipelines, flat files, informatica, jira, json, ms sql server, oracle, partitioning, problem solving, python, query performance, snowflake, sql, stored procedures, views, xml"
"Lead Azure Data Engineer - London/Hybrid - GBP90,000",Nigel Frank International,"Luton, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-azure-data-engineer-london-hybrid-gbp90-000-at-nigel-frank-international-3779651245,2023-12-17,Luton, United Kingdom,Mid senior,Hybrid,"Lead Azure Data Engineer - London/Hybrid - £90,000
I am working with a well-established high street retailer based who are looking for a Lead Data Engineer to join their team. In this role you will be hands on leading on the development of the organisations Azure data platform whilst also managing the close-knit data engineering team.
You will join the organisation at a significant point in their data journey and a time of significant investment in its data function. They are looking to revolutionise the way they use data, and you will have the opportunity to implement best practice across the department. You will be responsible for designing and implementing the technical roadmap as the business look to develop their Azure Data Platform
This is a salaried role paying up to £90,000 per year depending on experience. This is a hybrid role based out of the organisation's office in London. This role is also eligible for the organisation's benefits package which includes generous holidays allowance, competitive pensions scheme and other benefits.
To be successful in this role you will have the following experience.
Previous experience within a Lead or Senior Data Engineer role
A strong understanding of data architecture concepts
Experience working with Azure technologies such as Data Lakes, Data Factory, Databricks, Synapse Analytics
Strong ETL experience
Experience as a team lead or mentoring other members of the team
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now!
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure, Data Lakes, Data Factory, Databricks, Synapse Analytics, ETL, Data architecture","azure, data lakes, data factory, databricks, synapse analytics, etl, data architecture","azure, data architecture, data factory, data lakes, databricks, etl, synapse analytics"
"Lead Azure Data Engineer - London/Hybrid - GBP90,000",Nigel Frank International,"Stevenage, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-azure-data-engineer-london-hybrid-gbp90-000-at-nigel-frank-international-3779645942,2023-12-17,Luton, United Kingdom,Mid senior,Hybrid,"Lead Azure Data Engineer - London/Hybrid - £90,000
I am working with a well-established high street retailer based who are looking for a Lead Data Engineer to join their team. In this role you will be hands on leading on the development of the organisations Azure data platform whilst also managing the close-knit data engineering team.
You will join the organisation at a significant point in their data journey and a time of significant investment in its data function. They are looking to revolutionise the way they use data, and you will have the opportunity to implement best practice across the department. You will be responsible for designing and implementing the technical roadmap as the business look to develop their Azure Data Platform
This is a salaried role paying up to £90,000 per year depending on experience. This is a hybrid role based out of the organisation's office in London. This role is also eligible for the organisation's benefits package which includes generous holidays allowance, competitive pensions scheme and other benefits.
To be successful in this role you will have the following experience.
Previous experience within a Lead or Senior Data Engineer role
A strong understanding of data architecture concepts
Experience working with Azure technologies such as Data Lakes, Data Factory, Databricks, Synapse Analytics
Strong ETL experience
Experience as a team lead or mentoring other members of the team
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now!
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure, Data Lakes, Data Factory, Databricks, Synapse Analytics, ETL, Data Architecture, Data Engineering","azure, data lakes, data factory, databricks, synapse analytics, etl, data architecture, data engineering","azure, data architecture, data engineering, data factory, data lakes, databricks, etl, synapse analytics"
"Lead Azure Data Engineer - London/Hybrid - GBP90,000",Nigel Frank International,"Watford, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-azure-data-engineer-london-hybrid-gbp90-000-at-nigel-frank-international-3779643974,2023-12-17,Luton, United Kingdom,Mid senior,Hybrid,"Lead Azure Data Engineer - London/Hybrid - £90,000
I am working with a well-established high street retailer based who are looking for a Lead Data Engineer to join their team. In this role you will be hands on leading on the development of the organisations Azure data platform whilst also managing the close-knit data engineering team.
You will join the organisation at a significant point in their data journey and a time of significant investment in its data function. They are looking to revolutionise the way they use data, and you will have the opportunity to implement best practice across the department. You will be responsible for designing and implementing the technical roadmap as the business look to develop their Azure Data Platform
This is a salaried role paying up to £90,000 per year depending on experience. This is a hybrid role based out of the organisation's office in London. This role is also eligible for the organisation's benefits package which includes generous holidays allowance, competitive pensions scheme and other benefits.
To be successful in this role you will have the following experience.
Previous experience within a Lead or Senior Data Engineer role
A strong understanding of data architecture concepts
Experience working with Azure technologies such as Data Lakes, Data Factory, Databricks, Synapse Analytics
Strong ETL experience
Experience as a team lead or mentoring other members of the team
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now!
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure Data Platform, Data Architecture, Azure Technologies, Data Lakes, Data Factory, Databricks, Synapse Analytics, ETL, Team Lead, Mentoring, Data Engineering, SQL, Power BI, Python, Power Platform","azure data platform, data architecture, azure technologies, data lakes, data factory, databricks, synapse analytics, etl, team lead, mentoring, data engineering, sql, power bi, python, power platform","azure data platform, azure technologies, data architecture, data engineering, data factory, data lakes, databricks, etl, mentoring, power platform, powerbi, python, sql, synapse analytics, team lead"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Watford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3736354380,2023-12-17,Luton, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader organisation within the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on the latest technology like Databricks. The organisation has big plans for expansion within the next couple of years, as they are winning more Azure-based projects.
Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too. You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks. There is great room for progression within this role. The team really values the culture, and support and training to further develop your skills-set will be provides..
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, SQL, Python, PySpark, Azure Data Factory, Azurebased projects, Data Engineering, Big data","databricks, sql, python, pyspark, azure data factory, azurebased projects, data engineering, big data","azure data factory, azurebased projects, big data, data engineering, databricks, python, spark, sql"
Data Protection Analyst - OneTrust,Barclay Simpson,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-protection-analyst-onetrust-at-barclay-simpson-3773332050,2023-12-17,Luton, United Kingdom,Mid senior,Hybrid,"Leading automotive firm seeking Data Protection Analyst. The role will be centred on the implementation of OneTurst and ensuring the firm meets its data protection obligations when collecting and processing personal data.
Responsibilities
Optimise and enhance OneTrust implementation through process engineering and service optimisation
Assist with the development and delivery of OneTrust training
Create and publish OneTrust awareness and educational materials
Support strategic team objectives through organisation of workstreams, delivery of key tasks and completion of actions
Provide advice on how the organisation’s internal data protection policies, standards and guidelines should be standardised and automated
Conduct PIA, LIA, DPIA and other risk assessments as required to ensure that data protection is embedded into new products, services and business changes involving the collection or use of personal data
Assist with the documentation and ongoing maintenance of Records of Processing Activities involving personal data
Identify and monitor risks of non-compliance with data protection controls and assist with the development and continual improvement of controls to ensure risks are managed effectively
Work across all initiatives to deliver the data protection strategy and improve the firm’s data protection maturity
Produce reports and metrics concerning the status of data protection compliance
Skills/ Experience
Experience implementing or optimising OneTrust modules
Previous data protection experience
Excellent understanding of the GDPR and Data Protection Act 2018
Proactive attitude, with a willingness to support all data protection initiatives
Team orientated and able to work collaboratively with other colleagues to deliver results
Self-sufficient and able to prioritise and carry out tasks with minimum supervision
The ability to influence decisions at the management level
Excellent communication and presentation skills
As an ideal candidate, you will have a data protection certification such as BCS or IAPP and have strong knowledge of the insurance industry. You will also have expert knowledge of OneTrust and preferably have a OneTrust Professional or Expert Certification.
Show more
Show less","OneTrust, GDPR, Data Protection Act 2018, Data protection, Risk assessment, Privacy impact assessment, Legal impact assessment, Data protection impact assessment, Records of Processing Activities, Data protection controls, Data protection strategy, Data protection maturity, Data protection compliance","onetrust, gdpr, data protection act 2018, data protection, risk assessment, privacy impact assessment, legal impact assessment, data protection impact assessment, records of processing activities, data protection controls, data protection strategy, data protection maturity, data protection compliance","data protection, data protection act 2018, data protection compliance, data protection controls, data protection impact assessment, data protection maturity, data protection strategy, gdpr, legal impact assessment, onetrust, privacy impact assessment, records of processing activities, risk assessment"
Senior Data Engineer,Movement 8,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-movement-8-3775225150,2023-12-17,Luton, United Kingdom,Mid senior,Hybrid,"Job Description
Movement8 has teamed up with a renowned consultancy seeking to expand its Azure Data Specialist team in collaboration with an Energy company. This opportunity offers a dynamic environment that emphasizes collaboration, growth, and continuous learning.
As an Azure Data specialist specialising in Real-Time Data, your role will involve constructing and managing a sophisticated data platform on Azure. You'll be tasked with developing solutions for Data Lakes and Warehouses, collaborating with teams focused on Reporting and Data Visualization to deliver comprehensive data visualization solutions.
Key responsibilities include employing Data Vault and Dimensional modeling techniques for data model construction and establishing both batch and streaming data pipelines that are automated, efficient, and reusable. Additionally, you'll coordinate with teams dedicated to Reporting and Data Visualization to ensure the delivery of comprehensive solutions.
The ideal candidate will possess proficiency in Azure Data Lake Storage, familiarity with Azure Data Factory (including the creation and management of metadata-driven pipelines), and a deep understanding of SQL and Data Streaming Technologies.
The Package Includes
£80,000 (depending on experience)
10% Bonus
Learning and Development Budget
Full Remote Working
Holidays
Pension
If you are an Azure Data Specialist with a strong focus on Real-Time Data, please reach out for more information at ndoherty@movement8.co.uk!
Show more
Show less","Azure, Data Lake Storage, Data Factory, SQL, Data Streaming Technologies, Data Vault, Dimensional modeling, Data Lakes, Data Warehouses, Reporting, Data Visualization","azure, data lake storage, data factory, sql, data streaming technologies, data vault, dimensional modeling, data lakes, data warehouses, reporting, data visualization","azure, data factory, data lake storage, data lakes, data streaming technologies, data vault, data warehouses, dimensional modeling, reporting, sql, visualization"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739503192,2023-12-17,Zion,United States,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Upwork, Hybrid Workforce Solutions, Relational database management (Postgres/MySQL/Oracle), SQL, PL/SQL, pgPL/SQL, Python, Shell scripting, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Data federation engines, Presto, Trino, Dremio, Athena, Snowflake, Clickhouse, Greenplum","upwork, hybrid workforce solutions, relational database management postgresmysqloracle, sql, plsql, pgplsql, python, shell scripting, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, data federation engines, presto, trino, dremio, athena, snowflake, clickhouse, greenplum","athena, clickhouse, data federation engines, dremio, elasticsearch, greenplum, hybrid workforce solutions, kafka, kinesis, memcache, mongo, opensearch, pgplsql, plsql, presto, python, redis, relational database management postgresmysqloracle, shell scripting, snowflake, sql, trino, upwork"
Senior Data Scientist (US),Tiger Analytics,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/senior-data-scientist-us-at-tiger-analytics-3730242095,2023-12-17,Zion,United States,Associate,Remote,"Tiger Analytics is looking for experienced Data Scientists to join our fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world.
As a Senior Data Scientist, you will play a pivotal role in designing, solutioning, and implementing data solutions while leveraging your analytical prowess to extract meaningful insights. You will also be responsible for presenting your findings to both technical and non-technical stakeholders and leading collaborative efforts within cross-functional teams.
Key Responsibilities
Develop scalable and efficient data pipelines to facilitate the collection, storage, and processing of large volumes of structured and unstructured data. While data engineering is not the core aspect of the role, because this role needs to get intimate with data, they need to have basic data engineering skills in databricks, pyspark environment
Apply advanced statistical analysis, machine learning, and data mining techniques to derive actionable insights from complex datasets
Develop and implement predictive, prescriptive and inferential models to solve business challenges and drive decision-making processes
Must be good with collaborating with technical and non-technical stakeholders to effectively understand the business challenge, use case and structure the discussion in succinct format for driving next steps
Collaborate with domain experts to understand business requirements and translate them into data-driven solutions
Communicate complex technical concepts and analytical findings effectively to both technical and non-technical stakeholders
Create visually compelling presentations, reports, and dashboards to convey insights and recommendations
Lead workshops, meetings, and discussions to drive alignment and understanding across teams
Provide strategic guidance and contribute to the development of data-driven strategies within the organization
Requirements
Bachelor's or Master's degree in Computer Science, Data Science, Statistics, or a related field
Proven experience (6 to 8+ years) as a data engineer and data scientist, preferably in a fast-paced technology or analytics-driven environment
Experience in leveraging Graph data science concepts to solve business use cases a must. Experience with any graph tools/packages like Neo4J, MemGraph, NetworkX, GraphFrame, iGraph etc. is important
Strong proficiency in data engineering tools and languages (e.g., SQL, Python, Scala, ETL frameworks, data warehousing)
Comfortable and experienced working in Databricks, Spark environment
Extensive experience with machine learning frameworks (e.g., TensorFlow, H2O) and data science libraries (e.g., SparkML)
Ability to develop visualizations, as necessary to present insights, metrics and how this informs addressing the business challenge
Excellent presentation and communication skills, with the ability to convey technical concepts to non-technical audiences
Experience in a networking or communications domain is a plus
Experience interacting with other technical and non-technical stakeholders to collaborate on efforts like solutioning, understanding other data domains in order to source data is a must
Benefits
This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.
Show more
Show less","Data Science, Machine Learning, AI, Data engineering, SQL, Python, Scala, ETL frameworks, Data warehousing, TensorFlow, Graph data science, H2O, SparkML, Databricks, Spark, Neo4J, MemGraph, NetworkX, GraphFrame, iGraph, Data pipelines","data science, machine learning, ai, data engineering, sql, python, scala, etl frameworks, data warehousing, tensorflow, graph data science, h2o, sparkml, databricks, spark, neo4j, memgraph, networkx, graphframe, igraph, data pipelines","ai, data engineering, data science, databricks, datapipeline, datawarehouse, etl frameworks, graph data science, graphframe, h2o, igraph, machine learning, memgraph, neo4j, networkx, python, scala, spark, sparkml, sql, tensorflow"
Data Engineer - Reference Genome,Damco Solutions,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/data-engineer-reference-genome-at-damco-solutions-3766295974,2023-12-17,Zion,United States,Mid senior,Onsite,"Job Description
- Should be able to create data management pipeline in Airflow
- Should have good working knowledge of AWS
- Should have domain knowledge of Genomics versioning, bioinformatics, and life sciences
Show more
Show less","Apache Airflow, Amazon Web Services (AWS), Genomics, Bioinformatics, Life sciences","apache airflow, amazon web services aws, genomics, bioinformatics, life sciences","amazon web services aws, apache airflow, bioinformatics, genomics, life sciences"
"Snowflake Data Engineer-FT WASINGTON, PA- Hybrid Role-Need Locals-Only GC, GC EAD, Citizen, TN","ActiveSoft, Inc","Pennsylvania, United States",https://www.linkedin.com/jobs/view/snowflake-data-engineer-ft-wasington-pa-hybrid-role-need-locals-only-gc-gc-ead-citizen-tn-at-activesoft-inc-3787737082,2023-12-17,Zion,United States,Mid senior,Onsite,"Position: Snowflake Data Engineer
Location : FT WASINGTON, PA- Hybrid Role-Need Locals
Duration:3 to 6 Months Plus
Only GC, GC EAD, Citizen, TN
Job Purpose:
The Enterprise Data Warehouse (EDW) is the corporate repository of integrated operational data. It is moving from on-premises management of it’s Data Warehouse and Business Intelligence to cloud Data Management and reporting. Working with EIS and offshore resources, this role will lead the development of the cloud DW while supporting the current DW environment.
Interaction with Others:
External Contact: This position will work with external data and system vendors to exploit cutting edge Data Management technologies and integrate/export data following security guidelines.
Internal Contact: This position will have significant contact with
Offshore development resources
EIS physical IT resource management and Architecture staff
Internal business sponsors and subject matter experts.
Essential Job Responsibilities
:
Working with IT and business resources, support the Data Warehouse strategy and Business Intelligence initiatives:
Lead development activities to migrate the current on-premises SQL Server DW to Snowflake on AWS.
Gather requirements for Data Warehouse improvements and translate them to high level design documents including Physical Data Models. Provide technical architecture vision and recommend strategy/solutions.
Design and develop transformation processes and data structures for the Data Warehouse following best practice procedures.
Lead resolution of Data Warehouse load issues.
Support the analytical needs of the business users. Build strong relationships to help identify opportunities to enhance the analytical capabilities of the Data Warehouse.
Partnering with business stakeholders and technical report developers, establish, maintain, and promote consistent methodology for reporting and analytics deployment. Perform ad hoc data analysis to meet business unit data validation needs.
Participate in Data Quality initiatives and lead the data transformation component design to improve and maintain high quality data. Support performance tuning.
Foster teamwork through cooperative interactions with co-workers. Where needed, ensure project integration by communicating activities and status to project manager, appropriate project team members, and business users.
Job Qualifications
:
BS degree, preferably in Information Technology, Management Information Systems, Computer Science or similar discipline.
Post-college training in Data Management and vendor tool use
4-6+ years of Data Warehousing/ BI experience.
This role serves as the primary technical resource for managing and moving data in and out of the Data Warehouse. The candidate must:
Be well versed in Data Warehouse design concepts including Kimball and Inmon methodologies
Have hands-on experience with ETL and ELT methodologies and tools.
Have experience with SQL Server 2016 and SSIS.
Experience with business reporting requirements analysis.
Familiarity with Data Management and Business Intelligence tools, such as Business Objects, Microsoft Power BI, QlikSense, Tableau, Looker, AWS S3 and Redshift, or Snowflake.
Demonstrated ability to clearly communicate with all levels within an organization.
Have experience leading small or offshore development teams
understanding and experience with building and deploying Business Intelligence and analytics applications.
Familiarity with Financial, Sales, Marketing, and Logistics reporting environments.
Strong sense of leadership, strong analytical skills, excellent communication. Have expert skills and good technical abilities.
Preferred candidates will have experience with:
SSAS.
Analytics design and development is preferred.
Hands on development experience with .Net and/or Java is a plus.
Comfortable working in a fast-paced team. Capable of grasping new concepts quickly and acting with a sense of urgency.
Preferred candidate may have certifications like:
CBIP (Certified Business Intelligence Professional)
CDMP (Certified Data Management Professional)
Vendor-specific certifications
Powered by JazzHR
YuznpQC6bK
Show more
Show less","Snowflake, SQL Server, SSIS, ETL, ELT, Kimball methodology, Inmon methodology, Business Objects, Microsoft Power BI, QlikSense, Tableau, Looker, AWS S3, Redshift, SSAS, .Net, Java, CBIP, CDMP","snowflake, sql server, ssis, etl, elt, kimball methodology, inmon methodology, business objects, microsoft power bi, qliksense, tableau, looker, aws s3, redshift, ssas, net, java, cbip, cdmp","aws s3, business objects, cbip, cdmp, elt, etl, inmon methodology, java, kimball methodology, looker, microsoft power bi, net, qliksense, redshift, snowflake, sql server, ssas, ssis, tableau"
Data Integration Developer,"Liberty Personnel Services, Inc.","Pennsylvania, United States",https://www.linkedin.com/jobs/view/data-integration-developer-at-liberty-personnel-services-inc-3623672205,2023-12-17,Zion,United States,Mid senior,Onsite,"Job Details:
Data Integration Developer
I'm seeking a Data Integration Developer for a Remote job opportunity with a stable and growing company in the local area. Interested candidates should have the following experience.
Perform development and implementation of Oracle PL/SQL
.Net development
Data integration and/or data warehousing
Implement data solutions using Azure Cloud data components
Azure Synapse, Azure NoSQL, Azure ADF
Good knowledge of .Net, C#, APIs. windows scripting, etc.
If you would like to learn more about this exciting work from home position, please send your resume to bf@libertyjobs.com. You can also connect with me on LinkedIn as I work in all areas of IT, and I would be glad to help you or someone you know learn about better job opportunities.
Brian Patrick Feeley
bf@libertyjobs.com
www.linkedin.com/pub/brian-patrick-feeley/5/442/498/
484-690-9609
www.libertyjobs.com
#IT
#midsenior
Show more
Show less","Oracle PL/SQL, .Net development, Data integration, Data warehousing, Azure Cloud data components, Azure Synapse, Azure NoSQL, Azure ADF, .Net, C#, APIs, Windows scripting","oracle plsql, net development, data integration, data warehousing, azure cloud data components, azure synapse, azure nosql, azure adf, net, c, apis, windows scripting","apis, azure adf, azure cloud data components, azure nosql, azure synapse, c, data integration, datawarehouse, net, net development, oracle plsql, windows scripting"
IT Data Architect,Fulton Bank,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/it-data-architect-at-fulton-bank-3744198351,2023-12-17,Zion,United States,Mid senior,Onsite,"Our values define us and our culture inspires us to change lives for the better. Our employees are the heart and soul of our company, and every success we experience begins with them. Together we are committed to making a positive impact in our local communities. We champion a culture of continuous learning, work-life integration, and inclusion. We promote a digitally enabled work environment to continuously enhance the experience of our employees and customers.
Responsible for data modeling and source to target mapping for database development projects, design and user support for business intelligence implementation, design and ownership of the corporate data architecture and related standards, design and maintenance of reference and master data management initiatives, support of a comprehensive data quality program, establishment of meta data management policies and procedures, and development and enforcement of data management standards and controls.
Data Architects work on a broad range of data governance projects. Providing detailed data modeling and design for all data oriented development projects. Design logical and physical data models and maintain documentation. Conduct source to target analysis and mapping and maintain documentation. Create and update data dictionary. Participate in acceptance testing as required.
Support implementation of reference data and master data programs. Define and maintain Match rules, Data cleansing rules, Hierarchies and affiliations, Golden records. Manage changes to reference and master data by identifying sources and contributors and establishing processes for managed change.
Support the data quality program. Profile, analyze and assess data quality problems. Determine root cause of data quality issues. Define data quality metrics and business rules.
Support metadata management policies and procedures. Develop, maintain and enforce standards. Manage metadata repositories. Create and manage widely accessible data dictionaries. Enforce policies and processes needed to improve and maintain the quality and ownership of data including data owners, stewards and administrators.
Develop and monitor conformance with data standards, policies and procedures. Manage and resolve data related issues and conflicts. Monitor and ensure regulatory compliance. Establish and monitor documentation standards and repositories.
Assist with defining the corporate data architecture and related standards. Data integration architecture. Data warehouse and business intelligence architecture. Taxonomies and namespaces. Data modeling tools and standards. Metadata standards and requirements.
Education
Bachelor Degree or the equivalent experience. Specialty Computer Science, Computer Information Science. (Required)
Experience
3 or more years Data Engineer, Integration ETL architect or data modeler, big data architect. (Required)
1 Or More Years Of Azure Experience (Preferred)
This role may perform other job duties as assigned by the manager. Each employee of the Organization, regardless of position, is accountable for reading, understanding and acting on the contents of all Company-assigned and/or job related Compliance Programs, regulations and policies and procedures, as well as ensure that all Compliance Training assignments are completed by established due dates. This includes but is not limited to, understanding and identifying compliance risks impacting their department(s), ensuring compliance with applicable laws or regulations, and escalating compliance risks to the appropriate level of management.
As a condition of employment, individuals must be authorized to work in the United States without sponsorship for a work visa by Fulton Bank currently or in the future.
Fulton Bank (“Fulton”) is an equal opportunity employer and is committed to providing equal employment opportunity for all qualified persons. Fulton will recruit, hire, train and promote persons in all job titles, and ensure that all other personnel actions are administered, without regard to race, color, religion, creed, sexual orientation, national origin, citizenship, gender, gender identity, age, genetic information, marital status, disability, covered veteran status, or any other legally protected status.
Show more
Show less","Data Modeling, ETL, Database Development, Business Intelligence, Data Architecture, Data Quality, Metadata Management, Data Governance, Data Integration, Data Warehouse, Taxonomies, Namespaces, Data Modeling Tools, Metadata Standards, Azure, Data Dictionary, Logical and Physical Data Models, Source to Target Analysis, Match Rules, Data Cleansing Rules, Golden Records, Data Quality Metrics, Data Owners, Data Stewards, Data Administrators, Data Standards, Data Policies, Data Procedures, Regulatory Compliance, Documentation Standards, Repositories, Big Data Architecture","data modeling, etl, database development, business intelligence, data architecture, data quality, metadata management, data governance, data integration, data warehouse, taxonomies, namespaces, data modeling tools, metadata standards, azure, data dictionary, logical and physical data models, source to target analysis, match rules, data cleansing rules, golden records, data quality metrics, data owners, data stewards, data administrators, data standards, data policies, data procedures, regulatory compliance, documentation standards, repositories, big data architecture","azure, big data architecture, business intelligence, data administrators, data architecture, data cleansing rules, data dictionary, data governance, data integration, data modeling tools, data owners, data policies, data procedures, data quality, data quality metrics, data standards, data stewards, database development, datamodeling, datawarehouse, documentation standards, etl, golden records, logical and physical data models, match rules, metadata management, metadata standards, namespaces, regulatory compliance, repositories, source to target analysis, taxonomies"
Lead Analytic Data Analyst,Highmark Health,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/lead-analytic-data-analyst-at-highmark-health-3762974571,2023-12-17,Zion,United States,Mid senior,Onsite,"Company :
Highmark Health
Job Description :
JOB SUMMARY
Due to confidentiality restrictions, only US citizens will be eligible to fill this role.
This job understands healthcare data from end-to-end and analyzes raw and analytic data for the enterprise. The incumbent cleanses, validates, audits, and compares data from various sources. Provides business process and industry domain knowledge to projects while working with engineering and business teams on requirements and outcomes. The incumbent delivers data profiling, conducts testing and systems validation to troubleshoot data anomalies and monitors data management metrics and data loads. Working with data quality teams, ensures business alignment and analytic value of derived data is met. Provides guidance and education to Senior, Intermediate and Associate staff. Responsible for customer relationship management.
ESSENTIAL RESPONSIBILITIES
Verify analytic data for the enterprise by conducting end-to-end analysis of data, including cleansing, validating, auditing, and comparing data from various sources.
Confirm the accuracy of new data sources used by the analytic entities of the enterprise. This includes all aspects of the process from testing of source data to the interconnections of this data to internal sources.
Routinely monitor data sources used for analytics to assure the continued updates of data are meeting quality standards
Drive functional efforts for programs across multiple projects, including guidance and education of Senior and Intermediate individual contributors. Also responsible for management of relationships with customers of the function or domain of analytics.
Responsible for requirements solicitation from key stakeholders leveraging expert knowledge.
Other duties as assigned.
EDUCATION
Required
Bachelor's Degree in Computer Systems Analysis, Data Processing, Healthcare Informatics or Management Information Systems, or closely related discipline
Substitutions
None
Preferred
Master's Degree in Management Information Systems, or Healthcare Informatics
EXPERIENCE
Required
7 - 10 years of Business Analyst Experience, or
7 - 10 years of Data Analytics Experience
5 -7 years of Healthcare Industry Experience
Preferred
5 - 7 years of Data Warehousing Experience, or
7 - 10 years of Healthcare Industry Experience
Familiarity with the Google Cloud Platform (GCP)
LICENSES AND CERTIFICATIONS
Required
None
Preferred
None
SKILLS
SQL
SAS
Microsoft Office
Problem-Solving
Communication Skills
Requirements Analysis
Testing Skills
Language (other Than English)
None
Travel Requirement
0% - 25%
PHYSICAL, MENTAL DEMANDS and WORKING CONDITIONS
Position Type
Office-Based
Teaches / trains others regularly
Constantly
Travel regularly from the office to various work sites or from site-to-site
Rarely
Works primarily out-of-the office selling products/services (sales employees)
Never
Physical work site required
Yes
Lifting: up to 10 pounds
Occasionally
Lifting: 10 to 25 pounds
Rarely
Lifting: 25 to 50 pounds
Never
Disclaimer:
The job description has been designed to indicate the general nature and essential duties and responsibilities of work performed by employees within this job title. It may not contain a comprehensive inventory of all duties, responsibilities, and qualifications required of employees to do this job.
Compliance Requirement:
This position adheres to the ethical and legal standards and behavioral expectations as set forth in the code of business conduct and company policies.
As a component of job responsibilities, employees may have access to covered information, cardholder data, or other confidential customer information that must be protected at all times. In connection with this, all employees must comply with both the Health Insurance Portability Accountability Act of 1996 (HIPAA) as described in the Notice of Privacy Practices and Privacy Policies and Procedures as well as all data security guidelines established within the Company’s Handbook of Privacy Policies and Practices and Information Security Policy. Furthermore, it is every employee’s responsibility to comply with the company’s Code of Business Conduct. This includes but is not limited to adherence to applicable federal and state laws, rules, and regulations as well as company policies and training requirements.
Pay Range Minimum:
$78,900.00
Pay Range Maximum:
$146,000.00
Base pay is determined by a variety of factors including a candidate’s qualifications, experience, and expected contributions, as well as internal peer equity, market, and business considerations. The displayed salary range does not reflect any geographic differential Highmark may apply for certain locations based upon comparative markets.
Highmark Health and its affiliates prohibit discrimination against qualified individuals based on their status as protected veterans or individuals with disabilities, and prohibit discrimination against all individuals based on their race, color, age, religion, sex, national origin, sexual orientation/gender identity or any other category protected by applicable federal, state or local law. Highmark Health and its affiliates take affirmative action to employ and advance in employment individuals without regard to race, color, age, religion, sex, national origin, sexual orientation/gender identity, protected veteran status or disability.
EEO is The Law
Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled/Sexual Orientation/Gender Identity (
https://www.eeoc.gov/sites/default/files/migrated_files/employers/poster_screen_reader_optimized.pdf
)
We endeavor to make this site accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact number below.
For accommodation requests, please contact HR Services Online at HRServices@highmarkhealth.org
California Consumer Privacy Act Employees, Contractors, and Applicants Notice
Show more
Show less","Data Analysis, Data Profiling, Data Warehousing, Data Cleansing, Data Validation, Data Auditing, Data Management, Microsoft Office, SQL, SAS, ProblemSolving, Communication Skills, Requirements Analysis, Testing Skills, Google Cloud Platform, Healthcare Industry, Business Analysis","data analysis, data profiling, data warehousing, data cleansing, data validation, data auditing, data management, microsoft office, sql, sas, problemsolving, communication skills, requirements analysis, testing skills, google cloud platform, healthcare industry, business analysis","business analysis, communication skills, data auditing, data management, data profiling, data validation, dataanalytics, datacleaning, datawarehouse, google cloud platform, healthcare industry, microsoft office, problemsolving, requirements analysis, sas, sql, testing skills"
Senior Data Analyst - 100% Remote,Radian,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/senior-data-analyst-100%25-remote-at-radian-3787310814,2023-12-17,Zion,United States,Mid senior,Remote,"See yourself at Radian? We see you here too.
At Radian, we see you. For the person you are and the potential you hold. That’s why we’ve embraced a new way of working that lets our people across the country be themselves, be their best and be their boldest. Because when each of us is truly seen, each of us gives our best – and at Radian, we’ll give you our best right back.
Studies have shown that job seekers may hesitate to apply for jobs unless they meet every single qualification listed. We strive to see the potential in each applicant, so if you’re excited about this role but your experience or education level doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.
See Yourself as Sr Data Analyst
The Sr. Data Analyst proactively participates in developing artifacts that are consumed by the data engineering team such as source to target mappings, data quality rules, and data transformation rules for the various EIM projects. The Sr. Data Analyst must be a critical thinker who can investigate an issue, review the results, and provide a suggested solution to the problem. The Sr. Data Analyst works closely with a variety of partners including the data architects, the data engineering team, the application development team, the QA team, and business partners. Therefore the Sr. Data Analyst must be able to effectively communicate to ensure others understand the information that is provided to them as well as any request that is made of them.
Primary Duties And Responsibilities
Work closely with the team, business teams and data governance to define and develop data requirements, mapping specifications to deliver key business information needs, data analysis and issues.
Analyze and profile source system data, outline data relevance for business intelligence solutions for analysis and reporting needs.
Partner with Product Owners, other Business Analysts, and stakeholders to discover data requirements, document functional and non-functional requirements, and user acceptance testing scenarios.
Collaborating with business partners and the broader analytics organization to lead value-driven data insights.
Identify data quality issues while performing data profiling and testing and validation.
Collaborate to develop user acceptance testing (UAT) specifications.
Ability to design, drive, implement scalable and efficient data architecture solutions and recommendations to support analytics and reporting.
Identify opportunities for data pipeline process improvements and automation. Continuously assesses and improves data collection and reporting processes.
Data Integration and Processing: Process, clean, and integrate large data sets from various sources to ensure that the data is accurate, complete, and consistent.
Collaboration: Work closely with cross-functional teams, including data analysts, data engineers and business stakeholders
Interprets, analyzes, and provides recommendations regarding data accuracy and data collection needs and processes.
Job Specifications
Knowledge
5+ years of experience in Business Intelligence, Analytics, Data Visualization, Data Warehousing.
5+ years of experience reviewing and writing Technical & Business Requirements in relation to Business Intelligence, data Management or software development projects.
Ability to build and run complex SQL queries.
Experience in data quality and data governance.
Excellent analytical thinking, problem solving, organizational and time management skills.
Strong initiative, initiative-taking and ability to meet deadlines.
Ability to be flexible and to manage multiple projects in an organized and timely manner.
Ability to innovate and to effectively deal with rapid change in a positive manner.
Skills And Abilities
Strong experience with ETL development, data modeling, and data warehouse design
5+ years of experience with SQL
Strong diligence and demonstrated ability to work with substantial amounts of data, extract relevant information, identify patterns, and draw accurate conclusions.
Excellent organization skills, meticulous to ensure accuracy and reliability of data and information needs are captured and documented.
Strong understanding of and experience with Guidewire application including Policy and Billing Center is a plus.
Experience in analyzing data and present outcomes in a way for gathering data requirements, documenting, and communicating with the team.
Experienced working in a fast-paced, high-tech environment and comfortable navigating conflicting priorities and ambiguous problems
Advanced ability to draw insights from data and clearly communicate to the team and other stakeholders.
Must be an adaptive learner and possess strong analytical skills to identify patterns and insights from large and complex data sets.
Experience in working with both agile methodologies.
Should be an adaptive person and must be able to work with minimal supervision in a demanding environment.
Excellent critical thinking skills.
Extremely detailed oriented with high quality standards.
Prior Work Experience
Technical: 5 - 8 years
Supervisory: None
Education and Credentials
Required: Bachelor’s Degree or Equivalent Experience
Other Certifications: Master's degree in business analytics would be a plus
See Your Location
Radian is committed to a flexible work environment for many of our roles. This is a *
Work From Anywhere
* role meaning you have the flexibility to work from home (or another designated workspace that fits your needs).
This role provides additional flexibility should you want to work on-site at a Radian office. Explore our office locations here and let your Talent Acquisition Partner know you would be interested in working on-site.
Work From Anywhere is subject to Radian’s Alternative Work Policy and business needs.
See Why You Should Work With Us
Competitive Compensation: anticipated base salary from $86,000 to $142,000 based on skills and experience. This position is eligible to participate in an annual incentive program.
Rest and Relaxation. This role is eligible for 25 days of paid time off annually, which is prorated in the year of hire based on hire date. In addition, based on your hire date, you will be eligible for 9 paid holidays + 2 floating holiday in support of our DEI culture. Parental leave is also offered as an opportunity for all new parents to embrace this exciting change in their lives.
Our Company Makes an Impact. We’ve been recognized by multiple organizations like Bloomberg’s Gender-Equality Index, HousingWire’s Tech 100, and The Forum of Executive Women’s Champion of Board Diversity. Radian has also pledged to PwC’s CEO Action for Diversity & Inclusion commitment.
Comprehensive Health Benefits. Multiple medical plan choices, including HSA and FSA options, dental, vision, and basic life insurance.
Prepare for your Future. 401(k) with a top of market company match (did we mention the company match is immediately vested?!) and an opportunity to participate in Radian’s Employee Stock Purchase Plan (ESPP)
Homebuyer Perks. Our Homebuyer Perks program helps employees navigate the home searching, buying, selling, and refinancing processes and provides valuable financial benefits to encourage, enable, and support home ownership.
Additional Benefits. To learn more about our benefits offerings, visit our Benefits Page.
See More About Radian
Radian is a fintech servicing the mortgage and real estate services industry. As a team, we pride ourselves on seeing the potential of every person, every idea and every day.
Seeing each other at Radian goes far beyond our open, flexible culture. It means seeing our people’s potential – and creating inspiring career paths that help them get there. Or seeing new pathways and innovating for the future of our industry. It means seeing each other for all that we are. And it means seeing our purpose as one that extends beyond the bottom line – having an impact on communities across the country to help more people achieve the American Dream of homeownership.
We hope you’ll see yourself at Radian. See more about us at Radian.com.
Defining Roles for Radian's Future
Understanding the qualities and characteristics that define a Leader and an Employee is important to building our future-fit workforce. Radian's future is only as bright as its people. For that reason, our People Plan includes profiles to support the qualities and characteristics that each Leader as well as each Employee should embody upon hire or via development.
EEO Statement
Radian complies with all applicable federal, state, and local laws prohibiting discrimination in employment. All qualified applicants will receive consideration for employment without regard to gender, age, race, color, religious creed, marital status, gender identity, sexual orientation, national origin, ethnicity, ancestry, citizenship, genetic information, disability, protected veteran status or any other characteristic protected by applicable federal, state, or local law.
Equal Opportunity Employer Details
To learn more about Radian’s Code of Conduct and Ethics and workplace conduct, please click [here]. Radian participates in E-Verify [Link] (en español [Link]). Learn more about your rights under immigration laws [Link] (en español [Link]). View the ""EEO is the Law"" poster [Link]. View the ""EEO is the Law"" Supplement [Link]. View Pay Transparency Nondiscrimination Provision [Link].
Accommodation
Whether you require an accommodation for the job application or interview process, Radian is dedicated to a barrier-free employment process and encourages a diverse workforce. If you have questions about the accommodation process, please e-mail careers@radian.com.
Show more
Show less","SQL, Data Warehousing, Data Mining, Data Modeling, Data Governance, Data Quality, Data Integration, Data Visualization, Business Intelligence, Analytics, Agile Methodologies, ETL Development, UAT Development, Data Profiling, Data Analysis, Data Reporting, Data Architecture, Data Pipelines, Big Data, Cloud Computing, Hadoop, Spark, Hive, Machine Learning, Python, R, Tableau, Power BI, Looker","sql, data warehousing, data mining, data modeling, data governance, data quality, data integration, data visualization, business intelligence, analytics, agile methodologies, etl development, uat development, data profiling, data analysis, data reporting, data architecture, data pipelines, big data, cloud computing, hadoop, spark, hive, machine learning, python, r, tableau, power bi, looker","agile methodologies, analytics, big data, business intelligence, cloud computing, data architecture, data governance, data integration, data mining, data profiling, data quality, data reporting, dataanalytics, datamodeling, datapipeline, datawarehouse, etl development, hadoop, hive, looker, machine learning, powerbi, python, r, spark, sql, tableau, uat development, visualization"
"Senior Systems Test, Data Analysis",NVIDIA,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/senior-systems-test-data-analysis-at-nvidia-3759874736,2023-12-17,Zion,United States,Mid senior,Remote,"The Automotive Vehicles team is searching for a creative and experienced Systems Engineers to help us bring NVIDIA's autonomous vehicle solution out to the world. You will participate in a focused effort to develop and productize ground-breaking solutions that will redefine the world of transportation and the growing field of self-driving cars. You will work with hardworking and dedicated multi-functional engineering development teams across various vehicle subsystems to integrate their work into our AV SW platform, while achieving or exceeding all meaningful NVIDIA and automotive standards & guidelines. You'll find the work is exciting, fun, and relevant. We have deadlines, customers, and competition.
We are leading the technology revolution, shaping the future of transportation through our commitment to developing self-driving vehicles! As a data analyst/engineer, your primary role will be to construct robust and scalable data infrastructure that will enable us to track, analyze and validate our self-driving technology using billions of data points.
What You Will Be Doing
Design, develop, and maintain data infrastructure that supports large-scale, complex analysis of autonomous driving systems.
Develop and enhance tools that automate the data analysis and KPI reporting process, fostering a culture of data-driven decision making across the company.
Create robust and sharable dashboards and reports using data visualization tools.
Collaborate across engineering, data science, testing, and release teams to identify needs, build and scale analytical solutions to address gaps and reduce risks to the project.
Contribute to improve the overall process for improving quality of products.
Identify, troubleshoot, and resolve system-level issues that affect the performance and reliability of our data infrastructure.
What We Need To See
BS (or equivalent experience), MS, or PhD in Engineering, Computer Science, or related field.
6+ years of data engineering and analysis experience, preferably in the field of simulation, autonomous systems, robotics, artificial intelligence or related areas.
Deep technical proficiency with standard data analytics tools such as Python (Pandas, Jupyter Notebooks, Python-based visualization, etc.) and SQL.
Experience with writing clean and maintainable code in a collaborative environment.
Strong understanding of scalable data modeling, ETL processes, and data visualization techniques.
Experience with distributed computing and cloud platforms.
Excellent problem-solving, analytical skills, and technical troubleshooting abilities.
Strong communication and collaboration skills.
Demonstrated ability to perform in situations with high ambiguity and competing priorities.
Curiosity and a desire to “dive deep” into data to derive insights.
Ways To Stand Out From The Crowd
Experience with designing KPIs and dashboards for complex systems such robotics
Knowledge of AI/ML systems, or a background in working with AI-native systems.
Experience in large-scale data engineering for hardware or software validation of any robotics solution.
The base salary range is 176,000 USD - 333,500 USD. Your base salary will be determined based on your location, experience, and the pay of employees in similar positions.
You will also be eligible for equity and benefits .
NVIDIA accepts applications on an ongoing basis.
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.#deeplearning
Show more
Show less","Python, SQL, Data Analysis, Data Engineering, Scalable Data Modeling, ETL Processes, Data Visualization, Distributed Computing, Cloud Platforms, ProblemSolving, Communication, Collaboration, Artificial Intelligence, Machine Learning, Data Infrastructure, KPI Design, Dashboard Design, Robotics","python, sql, data analysis, data engineering, scalable data modeling, etl processes, data visualization, distributed computing, cloud platforms, problemsolving, communication, collaboration, artificial intelligence, machine learning, data infrastructure, kpi design, dashboard design, robotics","artificial intelligence, cloud platforms, collaboration, communication, dashboard design, data engineering, data infrastructure, dataanalytics, distributed computing, etl, kpi design, machine learning, problemsolving, python, robotics, scalable data modeling, sql, visualization"
Strategic Data Analyst III - 100% Remote,Radian,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/strategic-data-analyst-iii-100%25-remote-at-radian-3787310812,2023-12-17,Zion,United States,Mid senior,Remote,"See yourself at Radian? We see you here too.
At Radian, we see you. For the person you are and the potential you hold. That’s why we’ve embraced a new way of working that lets our people across the country be themselves, be their best and be their boldest. Because when each of us is truly seen, each of us gives our best – and at Radian, we’ll give you our best right back.
Studies have shown that job seekers may hesitate to apply for jobs unless they meet every single qualification listed. We strive to see the potential in each applicant, so if you’re excited about this role but your experience or education level doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.
See Yourself as a Strategic Data Analyst III
The Strategic Data Initiatives team is tasked with ensuring Radian is fully leveraging all available data to enable insightful analytics, reporting and efficient/smart operational processes. Individuals on this team have exposure to all aspects of the mortgage insurance business and are responsible for steering research on data questions/issues as well as providing direction on data related decisions. This role is for a hands-on analyst who is expertly adept at manipulating and analyzing data from disparate datasets while leveraging their business knowledge to independently arrive at meaningful conclusions with actionable recommendations. This analyst will be involved in acquiring, managing, understanding, standardizing and analyzing the internal and external data that is available on Radian’s mortgage insurance portfolio as well as for driving projects that facilitate data usage, data consistency and data quality.
See Your Primary Duties And Responsibilities
Manipulate large data files using SAS and SQL Server to identify trends, make business recommendations, and to provide data extractions to business partners.
Use strong experience base to identify trends, proactively research anomalies and offer fully developed solutions to problems.
Work well with external and internal customers and provide clear directions on objectives, outreach strategy and next steps.
Lead efforts to design databases and ETL processes using SQL Server, SSIS, SAS and/or Python
Leverage knowledge of the business, data and analytics to arrive at recommendations on opportunities to leverage data and to identify opportunities to fill in data gaps.
Creates presentations, reports and communicates strategic business findings to senior management.
See the Job Specifications
Your Knowledge
The successful candidate will have a Bachelor’s degree and a minimum of 5-8 years of experience. A Master’s degree with fewer years of prior work experience may also be sufficient.
Expert analytical, problem solving and data management skills with abilities to digest vast amounts of data, manipulate it, and identify correlations though a variety of statistical techniques
Experience in Financial Services - Mortgage industry experience preferred
Your Skills And Abilities
Exhibits passion about making a difference and able to perform responsibilities with a great deal of autonomy.
Key competencies in: critical thinking, problem solving skills, and planning and organizing work
Proficient with SQL, SAS and / or other data analysis software (Python, R, etc.)
Experience with BI tools like Tableau, MicroStrategy, QlikView
Excellent presentation skills (including oral and written) and ability to create meaningful management reports from various sources
Excellent oral and written communication skills and show a demonstrated ability to absorb information quickly.
Training / experience with Microsoft Office products; very high-level of proficiency with Microsoft Excel
Other Position Parameters
Exceptional time management skills, able to respond to changing priorities
Ability to balance and prioritize requests from multiple managers and stakeholders
Your Prior Work Experience
Technical: 5 - 8 years
Your Education and Credentials
Required: Bachelor’s Degree, Concentration in Business, Economics, Finance, Math, Computer Science
Preferred: Master’s Degree, Concentration in Business, Economics, Finance, Math, Computer Science
See Your Location
Radian is committed to a flexible work environment for many of our roles. This is a *
Work From Anywhere
* role meaning you have the flexibility to work from home (or another designated workspace that fits your needs).
This role provides additional flexibility should you want to work on-site at a Radian office. Explore our office locations here and let your Talent Acquisition Partner know you would be interested in working on-site.
Work From Anywhere is subject to Radian’s Alternative Work Policy and business needs.
See Why You Should Work With Us
Competitive Compensation: anticipated base salary from $77,500 to $120,000 based on skills and experience. This position is eligible to participate in an annual incentive program.
Our Company Makes an Impact. We’ve been recognized by multiple organizations like Bloomberg’s Gender-Equality Index, HousingWire’s Tech 100, and The Forum of Executive Women’s Champion of Board Diversity. Radian has also pledged to PwC’s CEO Action for Diversity & Inclusion commitment.
Rest and Relaxation. This role is eligible for 20 days of paid time off annually, which is prorated in the year of hire based on hire date. In addition, based on your hire date, you will be eligible for 9 paid holidays + 2 floating holiday in support of our DEI culture.
Health & Welfare Benefits. Multiple medical plan choices, including HSA and FSA options, dental, vision, and basic life insurance.
Prepare for your Future. 401(k) with a top of market company match (did we mention the company match is immediately vested?!) and an opportunity to participate in Radian’s Employee Stock Purchase Plan (ESPP)
Paid Parental Leave. An opportunity for all new parents to embrace this exciting change in their lives.
Employee Assistance and Discount Programs. From helping you navigate the healthcare system, to providing resources and assistance to parents and caregivers of children with development disabilities, to scoring discounts with thousands of retailers.
Pet Insurance. To help protect our furry family members.
See More About Radian
Radian is a fintech servicing the mortgage and real estate services industry. As a team, we pride ourselves on seeing the potential of every person, every idea and every day.
Seeing each other at Radian goes far beyond our open, flexible culture. It means seeing our people’s potential – and creating inspiring career paths that help them get there. Or seeing new pathways and innovating for the future of our industry. It means seeing each other for all that we are. And it means seeing our purpose as one that extends beyond the bottom line – having an impact on communities across the country to help more people achieve the American Dream of homeownership.
We hope you’ll see yourself at Radian. See more about us at Radian.com.
Defining Roles for Radian's Future
Understanding the qualities and characteristics that define a Leader and an Employee is important to building our future-fit workforce. Radian's future is only as bright as its people. For that reason, our People Plan includes profiles to support the qualities and characteristics that each Leader as well as each Employee should embody upon hire or via development.
EEO Statement
Radian complies with all applicable federal, state, and local laws prohibiting discrimination in employment. All qualified applicants will receive consideration for employment without regard to gender, age, race, color, religious creed, marital status, gender identity, sexual orientation, national origin, ethnicity, ancestry, citizenship, genetic information, disability, protected veteran status or any other characteristic protected by applicable federal, state, or local law.
Equal Opportunity Employer Details
To learn more about Radian’s Code of Conduct and Ethics and workplace conduct, please click [here]. Radian participates in E-Verify [Link] (en español [Link]). Learn more about your rights under immigration laws [Link] (en español [Link]). View the ""EEO is the Law"" poster [Link]. View the ""EEO is the Law"" Supplement [Link]. View Pay Transparency Nondiscrimination Provision [Link].
Accommodation
Whether you require an accommodation for the job application or interview process, Radian is dedicated to a barrier-free employment process and encourages a diverse workforce. If you have questions about the accommodation process, please e-mail careers@radian.com.
Show more
Show less","SAS, SQL Server, SSIS, Python, Tableau, MicroStrategy, QlikView, R, Microsoft Office, Microsoft Excel, SQL, Data Analysis Software, BI Tools, Financial Services, Data Management, Problem Solving, Statistical Techniques, Critical Thinking, Planning, Organizing Work, Presentation Skills, Communication Skills, Time Management, Data Extraction, Data Standardization, Data Analysis, Data Quality, Data Usage, Data Consistency, ETL Processes, Database Design, Project Management, Data Gaps, Strategic Business Findings, Analytical Skills, Autonomy, Decision Making, Correlations, Budgeting","sas, sql server, ssis, python, tableau, microstrategy, qlikview, r, microsoft office, microsoft excel, sql, data analysis software, bi tools, financial services, data management, problem solving, statistical techniques, critical thinking, planning, organizing work, presentation skills, communication skills, time management, data extraction, data standardization, data analysis, data quality, data usage, data consistency, etl processes, database design, project management, data gaps, strategic business findings, analytical skills, autonomy, decision making, correlations, budgeting","analytical skills, autonomy, bi tools, budgeting, communication skills, correlations, critical thinking, data analysis software, data consistency, data extraction, data gaps, data management, data quality, data standardization, data usage, dataanalytics, database design, decision making, etl, financial services, microsoft excel, microsoft office, microstrategy, organizing work, planning, presentation skills, problem solving, project management, python, qlikview, r, sas, sql, sql server, ssis, statistical techniques, strategic business findings, tableau, time management"
Data Engineer IV - Max Digital (Data Operations),ACV Auctions,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-operations-at-acv-auctions-3762880111,2023-12-17,Zion,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack (SSIS, C#, Web APIs) outages during our operational support window.
Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required.
Leverage DMVs and monitoring tools to ensure system performance; work with data operations and engineering to improve as required.
Ensure existing HADR (availability groups) solution is functional and meets requirements.
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively. This may include being part of the emergency after-hours on-call rotation.
Mentor junior data engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert in SQL Query optimization
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (e.g., MongoDB)
Experience developing Windows services in C#
Experience writing unit and integration testing
Expert SQL and data-layer development experience; OLTP schema design.
Experience using and integrating with cloud services, specifically: AWS RDS, S3, SQS, SNS.
Nice to Have
Experience with Airflow
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","Computer Science, Information Technology, Computer Information Systems, Management Information Systems, SQL Query optimization, ETL workflow implementation, SSIS, Airflow, C#, Python, Cloud Services, AWS RDS, S3, SQS, SNS, NoSQL data stores, MongoDB, Windows services, Unit and integration testing, OLTP schema design, DBT","computer science, information technology, computer information systems, management information systems, sql query optimization, etl workflow implementation, ssis, airflow, c, python, cloud services, aws rds, s3, sqs, sns, nosql data stores, mongodb, windows services, unit and integration testing, oltp schema design, dbt","airflow, aws rds, c, cloud services, computer information systems, computer science, dbt, etl workflow implementation, information technology, management information systems, mongodb, nosql data stores, oltp schema design, python, s3, sns, sql query optimization, sqs, ssis, unit and integration testing, windows services"
Fraud Data Analytics,Capgemini,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/fraud-data-analytics-at-capgemini-3767714677,2023-12-17,Zion,United States,Mid senior,Hybrid,"Job Title:
Fraud Data Analytics
Job Location:
Pennsylvania, Arizona
Key Responsibilities
Experience with SQL to create complex functions and stored procedures.
Analyze queries, develop security protocols, and resolve problems and design dashboard.
Write complex queries for applications and business intelligence reporting, and create, maintain complex mathematical models.
Outstanding communications skills and ability to clearly communicate with both senior leaders and staff.
Exercise judgment within defined procedures and practices to efficiently implement appropriate action.
Ability to synthesize large quantities of data into concise presentations with sound business conclusions.
Required Skills
Detailed knowledge of PC computing environment, experience with database/data warehouses.
Experience using reporting/query tools, such as Brio, MS Query, MS Access, Excel, Crystal Reports or Business Objects and Falcon system/scoring.
Able to work in a high-productive, deadline oriented environment.
Must be flexible to handle various tasks simultaneously.
Strong analytical, interpretive, and problem-solving skills with the ability to interpret large amounts of data and its impact in both operational and financial areas.
Must have fraud operations/analytics experience.
Experience with Python (or comparable system) fraud strategy and rule writing.
Life At Capgemini
Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer:
Flexible work
Healthcare including dental, vision, mental health, and well-being programs
Financial well-being programs such as 401(k) and Employee Share Ownership Plan
Paid time off and paid holidays
Paid parental leave
Family building benefits like adoption assistance, surrogacy, and cryopreservation
Social well-being benefits like subsidized back-up child/elder care and tutoring
Mentoring, coaching and learning programs
Employee Resource Groups
Disaster Relief
About Capgemini
Capgemini is a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided everyday by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get The Future You Want | www.capgemini.com
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Show more
Show less","SQL, Python, Business Intelligence, Data Warehouses, Reporting Tools, MS Query, MS Access, Excel, Crystal Reports, Business Objects, Falcon System, Brio, Dashboard Design, Fraud Analytics, Fraud Detection, Rule Writing, Data Interpretation, Financial Analysis, Operational Analysis","sql, python, business intelligence, data warehouses, reporting tools, ms query, ms access, excel, crystal reports, business objects, falcon system, brio, dashboard design, fraud analytics, fraud detection, rule writing, data interpretation, financial analysis, operational analysis","brio, business intelligence, business objects, crystal reports, dashboard design, data interpretation, data warehouses, excel, falcon system, financial analysis, fraud analytics, fraud detection, ms access, ms query, operational analysis, python, reporting tools, rule writing, sql"
Fraud Data Analytics,Capgemini,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/fraud-data-analytics-at-capgemini-3768639065,2023-12-17,Zion,United States,Mid senior,Hybrid,"Job Title:
Fraud Data Analytics
Job Location:
Pennsylvania, Arizona
Key Responsibilities
Experience with SQL to create complex functions and stored procedures.
Analyze queries, develop security protocols, and resolve problems and design dashboard.
Write complex queries for applications and business intelligence reporting, and create, maintain complex mathematical models.
Outstanding communications skills and ability to clearly communicate with both senior leaders and staff.
Exercise judgment within defined procedures and practices to efficiently implement appropriate action.
Ability to synthesize large quantities of data into concise presentations with sound business conclusions.
Required Skills
Detailed knowledge of PC computing environment, experience with database/data warehouses.
Experience using reporting/query tools, such as Brio, MS Query, MS Access, Excel, Crystal Reports or Business Objects and Falcon system/scoring.
Able to work in a high-productive, deadline oriented environment.
Must be flexible to handle various tasks simultaneously.
Strong analytical, interpretive, and problem-solving skills with the ability to interpret large amounts of data and its impact in both operational and financial areas.
Must have fraud operations/analytics experience.
Experience with Python (or comparable system) fraud strategy and rule writing.
Life At Capgemini
Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer:
Flexible work
Healthcare including dental, vision, mental health, and well-being programs
Financial well-being programs such as 401(k) and Employee Share Ownership Plan
Paid time off and paid holidays
Paid parental leave
Family building benefits like adoption assistance, surrogacy, and cryopreservation
Social well-being benefits like subsidized back-up child/elder care and tutoring
Mentoring, coaching and learning programs
Employee Resource Groups
Disaster Relief
About Capgemini
Capgemini is a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided everyday by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get The Future You Want | www.capgemini.com
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Show more
Show less","SQL, Stored procedures, Complex queries, Business intelligence reporting, Mathematical models, PC computing environment, Database/data warehouses, Reporting/query tools, Python, Fraud strategy, Rule writing, Brio, MS Query, MS Access, Excel, Crystal Reports, Business Objects, Falcon system","sql, stored procedures, complex queries, business intelligence reporting, mathematical models, pc computing environment, databasedata warehouses, reportingquery tools, python, fraud strategy, rule writing, brio, ms query, ms access, excel, crystal reports, business objects, falcon system","brio, business intelligence reporting, business objects, complex queries, crystal reports, databasedata warehouses, excel, falcon system, fraud strategy, mathematical models, ms access, ms query, pc computing environment, python, reportingquery tools, rule writing, sql, stored procedures"
Principal Data Engineer,Walmart,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-walmart-3726087493,2023-12-17,Menlo Park,United States,Mid senior,Onsite,"Position Summary...
What you'll do...
About Team
The mission of Walmart Display Ads is to connect brand owners and shoppers with relevant display ads. We bring brand awareness and rich product information to shoppers and help brand owners grow their business with advanced targeting and optimization techniques. This is a fast-growing business never lack ing opportunities ! We are looking for an experienced tech lead to spearhead advertising data analytics related to deman d, supply, and overall marketplace healt h. You will be responsible for extracting meaningful insights about campaign performance, marketplace efficiency, and gaps in systems and products , surfacing marketplace health metrics via dashboards , and working with cross-functiona l partners to move the needle in display advertising.
What You Will Do
Building data pipelines and dashboards to monitor display ads serving funnel and marketplace health.
Leading analytics for display ads marketplace and presenting findings and recommendations to engineering teams and cross functional partners.
Collaborating with engineering and data science leads to improve data quality, build ing data warehouse, and delivering predictive models for market place insights and anomaly detection.
Bringing data driven culture to the engineering team to assure product and system quality.
What You'll Bring
Bachelor's degree in data science , computer science, statistics, operation research, or related fields
6 years' experience of building data pipelines, extracting signals from noisy data, and establishing metrics for monitoring.
Proficiency in data analysis tools (e.g., as Python, R, SQL) and data visualization tools (e.g., Tableau, Superset, Looker ) .
Analytical thinking and detail-oriented mindset. Familiar with S QL and N on- SQL database s.
String communication skills. Ability to convey complex ideas and findings to non-technical stakeholders .
Knowledge of A/B testing.
Preferred Qualifications
Advanced degree in data science, computer science, statistics, operation research, or related fields
6 + years' experience in programmatic advertising, on-line content distribution, or e-commerce.
Knowledge of optimization, auction, and ML applications.
People management experience is a big plus.
About Walmart Global Tech
Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That is what we do at Walmart Global Tech. We are a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity and those looking for the opportunity to define their career. Here, you can kickstart a distinguished career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.
Flexible, Hybrid Work
We use a hybrid way of working that is primarily in the office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.
Benefits
Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO , multiple health plans, and much more.
Equal Opportunity Employer
Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas, and opinions - while being inclusive of all people.
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
At Walmart, we offer competitive pay as well as performance-based incentive awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more.
You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices .
Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.
Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. For information about benefits and eligibility, see One.Walmart at https://bit.ly/3iOOb1J .
The annual salary range for this position is $168,000.00-$252,000.00
Additional Compensation Includes Annual Or Quarterly Performance Incentives.
Additional compensation for certain positions may also include:
Regional Pay Zone (RPZ) (based on location)
Stock equity incentives
Minimum Qualifications...
Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.
Option 1: Bachelor's degree in Computer Science and 5 years' experience in software engineering or related field. Option 2: 7 years' experience in software engineering or related field. Option 3: Master's degree in Computer Science and 3 years' experience in software engineering or related field.
4 years' experience in data engineering, database engineering, business intelligence, or business analytics.
Preferred Qualifications...
Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.
Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 5 years' experience in software engineering or related field
Primary Location...
840 W CALIFORNIA AVE, SUNNYVALE, CA 94086-4828, United States of America
Show more
Show less","Data pipelines, Dashboards, Python, R, SQL, Tableau, Superset, Looker, SQL, NonSQL databases, A/B testing, Optimization, Auction, ML, Data analysis tools, Data visualization tools","data pipelines, dashboards, python, r, sql, tableau, superset, looker, sql, nonsql databases, ab testing, optimization, auction, ml, data analysis tools, data visualization tools","ab testing, auction, dashboard, data analysis tools, data visualization tools, datapipeline, looker, ml, nonsql databases, optimization, python, r, sql, superset, tableau"
Data Engineer III,WinMax,"Cupertino, CA",https://www.linkedin.com/jobs/view/data-engineer-iii-at-winmax-3617894359,2023-12-17,Menlo Park,United States,Mid senior,Onsite,"Title:
Data Engineer III, Req# 16765591
Location:100% Remote must be in the US
Contract:12 Month
Job Description
Dev-Ops, Analytics & Cognitive
Will Work With Clients To
Implement large-scale data ecosystems including data management, governance and the integration of structured and unstructured data to generate data collection insights leveraging cloud-based platforms
Leverage automation, cognitive and science-based techniques to manage data, predict scenarios and prescribe actions during the daily operations, across SW and HW
Drive operational efficiency by maintaining their data ecosystems and providing As-a-Service offerings for continuous data collection improvements
Monitor, log and generate the operational insights quantitatively, report to client on daily basis.
Work you'll do
Support the implementation of data collection, integration & monitoring requirements and develop the pipeline of data from raw to curation layers including the cleansing, transformation, derivation and aggregation of data.
Communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work
Support in the development of technical solutions to business problems
Required Qualifications
Bachelor's degree, preferably in Computer Science, Information Technology, Computer Engineering, or related IT discipline; or equivalent experience
5+ years of hands-on experience as a Data Engineer or Big Data developer
5+ years of experience in Core JAVA and SQL, foundational to advanced knowledge including relational database experience (e.g. Teradata, Vertica) and Hive or Presto/Trino on a Big Data Platform
3+ years or experience in Python / Spark / Unix Shell Scripting, including performance tuning, working with Dataframes, and code optimization
Experience Defining, Analyzing And Documenting Functional And Technical Requirements
Hands on big data/ Hadoop performance tuning and optimization experience
Has developing or applying Dev-Ops platform experience
Basic machine learning knowledge
Strong customer facing skills with the ability to articulate business value and communicate features of the end-product
Demonstrates thought leadership and expertise in the field as well as the domain
Ability to communicate effectively (written and spoken) and work with the multi-location development teams and self-manage own work
Experience identifying a business problem and developing end-to-end technical solutions to address such problems
Demonstrates good time management skills as well as ability to project manage work independently and handle issue management tracking
Additional Requirements
Software development experience is a plus.
Show more
Show less","Data Engineering, Data Management, Data Governance, Cloud Computing, Automation, Cognitive Computing, Data Collection, Data Integration, Data Monitoring, Data Processing, Data Cleansing, Data Transformation, Data Derivation, Data Aggregation, Python, Spark, Unix Shell Scripting, Performance Tuning, Dataframes, Code Optimization, SQL, Java, Hive, Presto/Trino, Teradata, Vertica, Hadoop, DevOps, Machine Learning, Business Communication","data engineering, data management, data governance, cloud computing, automation, cognitive computing, data collection, data integration, data monitoring, data processing, data cleansing, data transformation, data derivation, data aggregation, python, spark, unix shell scripting, performance tuning, dataframes, code optimization, sql, java, hive, prestotrino, teradata, vertica, hadoop, devops, machine learning, business communication","automation, business communication, cloud computing, code optimization, cognitive computing, data aggregation, data collection, data derivation, data engineering, data governance, data integration, data management, data monitoring, data processing, data transformation, datacleaning, dataframes, devops, hadoop, hive, java, machine learning, performance tuning, prestotrino, python, spark, sql, teradata, unix shell scripting, vertica"
Senior Data Engineer,Tencent,"Palo Alto, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-tencent-3759959531,2023-12-17,Menlo Park,United States,Mid senior,Onsite,"Work Mode:
Onsite
Responsibilities:
【About Level Infinite】
Level Infinite is a global gaming brand dedicated to delivering high-quality and engaging interactive entertainment experiences to a worldwide audience, wherever and however they choose to play. It operates from bases in Amsterdam and Singapore with staff around the world.
To learn more about Level Infinite, visit www.levelinfinite.com, and follow on Twitter,Facebook, Instagram and YouTube
Responsibilities:
Design, build and own the core data models and systems that able to support petabyte of data pipeline across the Tencent's overseas games.
Analyze large amounts of players’ data with business use cases to bridge the gaps of data-driven lifecycles.
Partner with multiple internal teams and external partners to collect requirements and deliver large-scale data solutions for various analytics and business applications.
Collaborate with product team to define, create, and maintain data applications and platform such as real-time platforms, data pipeline, telemetry implementation, and machine learning deployment to help achieve Tencent's publishing goals.
The challenges range from building real-time and offline distributed data processing pipelines for feature engineering and automation as well as building systems and frameworks to deploy, scale, test, monitor real-time machine learning models and user acquisition algorithms.
Requirements:
Advanced degree in Computer Science, Engineering, Mathematics or equivalent technical field.
Programming and scripting language (e.g. SQL, Python, Java/Scala, Golang).
2.4+ years of professional software development: experience in backend development experience with interest in work involving data pipelines, distributed systems, and large-scale data processing.
Experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, Flink, Kafka, Impala, HBase etc.) for building efficient and large-scale data pipelines.
Experience shipping scalable data solutions in the cloud (AWS, Azure, GCP) and database technologies such as Snowflake, Redshift, SQL/NoSQL and or columnar databases.
Dynamic team player with ability to handle numerous requests concurrently and strategically, prioritizing when necessary.
Strong problem-solving skills with an ability to isolate, deconstruct and resolve complex data engineering challenges.
Ability to communicate with worldwide business stakeholders regard data products and technical requests.
Proven track record to clearly form and communicate ideas to both technical and non-technical worldwide audiences.
[DEI Statement]
Diversity, Equity & Inclusion at Tencent:
Diversity, equity and inclusion are important, interdependent components of our workplace. As an equal opportunity employer, we firmly believe that diverse voices fuel our innovation and allow us to better serve our users and the community. We foster an environment where every employee of Tencent feels supported and inspired to achieve individual and common goals.
Location State(s)
California
The base pay range for this position in the state(s) above is $124,420 to $227,770 per year. Actual pay is based on market location and may vary depending on job-related knowledge, skills, and experience. A sign on payment, relocation package, and restricted stock units may be provided as part of the compensation package, as well as other medical, financial, and/or other benefits, dependent on the specific position offered.
Show more
Show less","SQL, Python, Java, Scala, Golang, Hadoop, MapReduce, Spark, Flink, Kafka, Impala, HBase, AWS, Azure, GCP, Snowflake, Redshift, SQL/NoSQL, Data pipelines, Distributed systems, Largescale data processing, Machine learning, Telemetry, Realtime platforms, Feature engineering, Automation, Data engineering","sql, python, java, scala, golang, hadoop, mapreduce, spark, flink, kafka, impala, hbase, aws, azure, gcp, snowflake, redshift, sqlnosql, data pipelines, distributed systems, largescale data processing, machine learning, telemetry, realtime platforms, feature engineering, automation, data engineering","automation, aws, azure, data engineering, datapipeline, distributed systems, feature engineering, flink, gcp, golang, hadoop, hbase, impala, java, kafka, largescale data processing, machine learning, mapreduce, python, realtime platforms, redshift, scala, snowflake, spark, sql, sqlnosql, telemetry"
Senior Data Engineer,Enertek Group,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-enertek-group-3782128983,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Job Title:
Senior Data Engineer
Location:
United States (Must be a US Citizen)
Model:
Fully Remote
Company:
Financial Services and Payment Integration
About the Company:
Our partner are a dynamic and rapidly growing mid-sized enterprise specializing in financial services and payment integration. They create innovative solutions to facilitate effective and timely decision-making for some of the world's largest organizations. With self-financed and profitable growth of around 30% annually for the past decade, we have successfully expanded into the North American market and recently secured a $110 million growth investment from a private growth equity firm.
Job Description:
As a Senior Data Engineer you will play a crucial role in understanding analytics needs and developing robust, sustainable, and scalable data pipelines. Working with a combination of Azure (ADF, Databricks), Microsoft SQL, Microsoft SSIS, and proprietary integration tools, you will design and build data integrations and migrations for our financial services and payment integration applications. Your responsibilities will include interfacing between our integration team and customers' IT and project members, actively supporting and improving integration processes, and converting business requirements into analytics capabilities
.
Responsibilities:
Collaborate with team members and clients to understand analytics needs.
Work with Azure (ADF, Databricks), Microsoft SQL, Microsoft SSIS, and proprietary integration tools.
Design and build robust, sustainable, and scalable data pipelines.
Investigate and understand clients’ data on cloud, on-premises ERPs, and third-party data.
Interface between our integration team and customers' IT and project members.
Provide active support and improvement ideas for existing integration processes.
Convert business requirements into analytics capabilities.
Challenge the status quo and contribute to continuous improvement.
Requirements:
Minimum 7 years of ETL experience.
Minimum 3 years of Azure or other cloud-based Data Engineering experience.
Independence and ability to take ownership.
Ability to work under pressure and take time off as needed.
Strong analytical and problem-solving skills.
Desire for continuous learning and skill development.
Interest in automating processes.
Good communication and time-management skills.
Knowledge of data best practices with a focus on resource efficiency.
Advanced SQL skills.
Experience with financial services and payment integration highly valued.
Fluency in English (other language skills appreciated).
Desirable:
Scripting knowledge (PowerShell or Python).
Experience working with APIs (RESTful and SOAP) integrations.
Industry experience in financial services or payment integration.
Client-facing experience.
Consulting or project management experience.
Employment:
Full-time Permanent Employment Position
Base Salary Band: $165,000 to $215,000
Metric-based Bonus: 15% Per Annum
Equity: Stock Option Exercise
Mandatory Start Date: Q2 of 2024
** Please Note This Position Is Not Available For C2C or C2H **
Hiring contact: ali@enertek-group.com
Show more
Show less","Data Engineering, ETL, Azure, Databricks, Microsoft SQL, SSIS, ERPs, RESTful, SOAP, PowerShell, Python, Data pipelines, Advanced SQL, Data best practices, Resource efficiency, Analytical skills, Problemsolving skills, Communication skills, Timemanagement skills","data engineering, etl, azure, databricks, microsoft sql, ssis, erps, restful, soap, powershell, python, data pipelines, advanced sql, data best practices, resource efficiency, analytical skills, problemsolving skills, communication skills, timemanagement skills","advanced sql, analytical skills, azure, communication skills, data best practices, data engineering, databricks, datapipeline, erps, etl, microsoft sql, powershell, problemsolving skills, python, resource efficiency, restful, soap, ssis, timemanagement skills"
Data Engineer - Lead,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/data-engineer-lead-at-steneral-consulting-3742097886,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Remote role, someone from GA preferred, if not in GA, then must be from FL, NC, AL, CA, NV, IL, UT, MO, TX, PA, NH, PA
Candidates need to take a video screen with prime vendor prior to end client submittal
Need 2 strong candidates on this role
Need strong knowledge of PySprak. 80% of the role will be using Pyspark.
They need to be able to be very hands-on and have experience coding within Pyspark not just enabling the tool.
Candidates need to have experience with large-scale projects that take 1-2 years versus small projects or proof of concept.
Candidates need to have more thought leadership skills. They have a lot of doers currently and need someone who can help guide the team.
Top skills:
Pyspark
Azure Databricks and/or Data Factory
Experience leading and managing offshore teams.
ETL
Must Have
Azure Cloud
Azure Data Brick
ETL
High volumes of data
Pyspark
Python
Experience Leading or managing offshore teams
Data en
gineer has ownership of designing and developing large scale data solutions. You will have deep technical skills in a variety of technologies to play an important role in developing and delivering proof of concepts and product implementation.
Responsibilities
Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data Factory
Working with event based streaming technologies to ingest and process data
Design and Implement Data Lakehouse
Working with other members of the project team to support delivery of additional project components (API interfaces, Search)
Evaluating the performance and applicability of multiple tools against customer requirements
Working within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.
Work with contractors to deliver products and services for the bottlers
Conduct Pilots with bottlers and measure value
Transition successful pilots into commercial applications
Ensure design and implementation of technologies that comply with security standards and application architecture principles (in alignment with platform architects and respective review boards) and that consider state of the art data and analytics concepts
Review code/solutions developed by team prior to migration to production
What makes you a good fit?
Strong knowledge of data management principles
Experience in building ETL pipelines
Hands on experience designing and delivering solutions using the Azure including Azure Storage, Azure Data Factory, PySpark, Python, Databricks, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics
Experience working with structured and unstructured data
Experience working in DevOps environment
3+ years of relevant experience
Ability to balance high customer orientation and service attitude with business priorities
Analytic thinking, and problem-solving skills
High energy, with strong will and ambition to learn and work on new things
Outstanding proven verbal, written and interpersonal communication skills
Ability to adapt quickly to changing product scope and priorities (demand driven)
Proven ability to influence and collaborate effectively with cross-functional teams
Demonstrates noticeable commitment to foster and preserve a culture of diversity and inclusion by creating environments where people of diverse backgrounds are excited to bring all of who they are and do their best work
Constant role model of culture: Integrity, Accountability, Passion, Collaboration, Innovation
Bachelor’s degree (or equivalent)
Show more
Show less","PySpark, Azure Databricks, Data Factory, ETL, Azure Cloud, Python, Data Lakehouse, DevOps, Azure Storage, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics, Structured data, Unstructured data","pyspark, azure databricks, data factory, etl, azure cloud, python, data lakehouse, devops, azure storage, azure data lake, azure cosmos db, azure stream analytics, structured data, unstructured data","azure cloud, azure cosmos db, azure data lake, azure databricks, azure storage, azure stream analytics, data factory, data lakehouse, devops, etl, python, spark, structured data, unstructured data"
Data Engineer,BigRio,"Massachusetts, United States",https://www.linkedin.com/jobs/view/data-engineer-at-bigrio-3780009515,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Data Engineer
Boston, MA- Remote
Contract to Hire
Must hold or be eligible for Public Trust
About BigRio:
BigRio is a remote technology consulting firm headquartered in Boston. We deliver a range of solutions including custom machine learning/AI integrations and data warehousing and processing solutions. Our comprehensive approach serves clients from a variety of industries as a result of our ability to consistently, and quickly deliver cutting-edge and cost-conscious software solutions.
You will join our client's team as a Data Engineer who will work with our clients.
Position Summary:
As a Data Engineer, you will be responsible for developing and maintaining data pipelines using cloud technologies, architecting modern data warehousing platforms, and ensuring the efficient flow of data across the organization. Your expertise in cloud services, data engineering, and collaboration with offshore teams will be instrumental in achieving our data-driven goals.
Key Responsibilities:
Develop and maintain data pipelines using cloud technologies, with a minimum of 7 years of experience.
Architect and optimize modern data warehousing platforms using technologies such as Snowflake or Redshift (5+ years of experience).
Utilize cloud services, including S3, Step Functions, Glue, and Airflow, to streamline data processes.
Proficient in Python for data transfers and extractions (ETL and ELT).
Develop and deploy ETL solutions, including tools like Informatica or similar technologies.
Collaborate within an agile development process (Scrum, Kanban, etc.).
Familiarity with CI/CD concepts and implementation.
Create comprehensive technical documentation to ensure knowledge sharing and transfer.
Utilize Airflow and DAG development to manage workflow orchestration.
Exhibit proficiency in BI and data analysis, contributing to end-to-end development in data platform environments.
Proactively identify and address potential issues to prevent data pipeline disruptions.
Write high-quality, fully tested code to build ETL/ELT data pipelines on cloud platforms.
Coordinate and collaborate with offshore teams, demonstrating prior experience in the onsite-offshore model.
Qualifications
:
Bachelor's degree in Computer Science, Engineering, or a related field (Master's preferred).
7+ years of experience in data engineering and pipeline development.
5+ years of experience in modern data warehousing architecture, specifically with Snowflake or Redshift.
Strong knowledge of cloud services, including S3, Step Functions, Glue, and Airflow.
Proficiency in Python for data transfers and extractions.
Experience in ETL development and deployment, including tools like Informatica.
Familiarity with agile development methodologies and CI/CD concepts.
Demonstrated ability to create clear and comprehensive technical documentation.
Experience with Airflow and DAG development.
Strong BI and data analysis capabilities.
Proven track record of proactively addressing issues and preventing data pipeline disruptions.
Excellent coding skills with a focus on testing and quality.
Experience coordinating with offshore teams and working in the onsite-offshore model.
Equal Opportunity Statement
BigR.io is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, religion, national origin, sex, sexual orientation, gender identity, age, pregnancy, status as a qualified individual with disability, protected veteran status, or other protected characteristic as outlined by federal, state, or local laws. BigR.io makes hiring decisions based solely on qualifications, merit, and business needs at the time. All qualified applicants will receive equal consideration for employment.
Show more
Show less","Data Engineering, Cloud Technologies, Data Warehousing, Snowflake, Redshift, Python, ETL, ELT, Informatica, Agile Development, Scrum, Kanban, CI/CD, Technical Documentation, Airflow, DAG Development, BI, Data Analysis, Computer Science, Engineering","data engineering, cloud technologies, data warehousing, snowflake, redshift, python, etl, elt, informatica, agile development, scrum, kanban, cicd, technical documentation, airflow, dag development, bi, data analysis, computer science, engineering","agile development, airflow, bi, cicd, cloud technologies, computer science, dag development, data engineering, dataanalytics, datawarehouse, elt, engineering, etl, informatica, kanban, python, redshift, scrum, snowflake, technical documentation"
Senior Software / Data Engineer,DataTribe,"Columbia, MD",https://www.linkedin.com/jobs/view/senior-software-data-engineer-at-datatribe-3787753817,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Do you want to help build the next generation network security planning solution?
Company Overview:
Sixmap is working on leading edge network intrusion detection technology that enables enterprises and network operators to gain insights into their complete network attack surface and identify network vulnerabilities at unheard of speed and comprehensiveness. Sixmap’s platform can complete IPv4 scans with deep and configurable service interrogation that is orders of magnitude faster than anything currently available. The team is building the world’s first platform to perform comprehensive IPv6 scans, previously thought to be impossible.
Position Summary:
We are looking for a data-oriented senior software engineer to help build the core network mapping and interrogation engine. Candidates should have deep hands-on experience working on data pipelines, ETL, data analysis processes, and database technologies in addition to a solid understanding of TCP/IP networking. The ideal candidate should be a well-rounded developer but be particularly strong in backend business-logic-oriented software development. Come join us, if you are ready to change the world of network security while having some fun along the way.
Position Requirements:
To be considered for this position, you must:
Be a development athlete with at least 5 years’ experience and have a passion in understanding users’ needs and system requirements and turning them into working software
Have a BS degree or higher in computer science, electrical/computer engineering, or related technical field
Be fully fluent in Python and common data analysis Python libraries, C++, SQL, Airflow or other ETL / data pipeline tools, and preferably be a polyglot comfortable in many additional programming languages.
Be an expert in using relational databases and NoSQL data stores - PostgreSQL experience is a must.
Be experienced with Linux environments.
Have experience working on container-based cloud infrastructure frameworks such as Docker or Kubernetes within common cloud service providers such as AWS, GCP, or Azure.
Be experienced using Agile methodologies, operating cloud dev-ops, and coordinating with product development teams
Have the ability to thrive when presented a complex challenge in a fast-paced, performance-oriented culture with intelligent people
Have exceptional level of integrity, raw intelligence, creativity, energy and passion
Operate efficiently with individual responsibility in a highly collaborative environment
Powered by JazzHR
4lnILZIcTW
Show more
Show less","Network security, Network intrusion detection, IPv4/IPv6 scanning, TCP/IP networking, Data pipelines, ETL (Extract Transform Load), Data analysis, Database technologies, Python, Data analysis Python libraries, C++, SQL, Airflow, PostgreSQL, NoSQL data stores, Linux, Docker, Kubernetes, AWS, GCP, Azure, Agile methodologies, Cloud devops, Product development","network security, network intrusion detection, ipv4ipv6 scanning, tcpip networking, data pipelines, etl extract transform load, data analysis, database technologies, python, data analysis python libraries, c, sql, airflow, postgresql, nosql data stores, linux, docker, kubernetes, aws, gcp, azure, agile methodologies, cloud devops, product development","agile methodologies, airflow, aws, azure, c, cloud devops, data analysis python libraries, dataanalytics, database technologies, datapipeline, docker, etl extract transform load, gcp, ipv4ipv6 scanning, kubernetes, linux, network intrusion detection, network security, nosql data stores, postgresql, product development, python, sql, tcpip networking"
AWS Data engineer,IVY TECH SOLUTIONS INC,"Washington, DC",https://www.linkedin.com/jobs/view/aws-data-engineer-at-ivy-tech-solutions-inc-3787780205,2023-12-17,Menlo Park,United States,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
AWS Data engineer
Location: Washington, DC
Duration: 12+Months
Initially Remote
Please send the resume to
or 847- 350-1008
Must-haves:
Familiarity with AWS data services and modules.
5+ years of hands-on experience with AWS services (Lambda, S3, RDS, Aurora, DynamoDB, Kinesis, AWS Glue, AWS Data Pipeline)
3+ years of experience with data migration, data analysis, and SQLs
3+ years of experience with informatica
Experience with Structured Query Language (SQL), should be able to analyze, compare and profiling data sets
Ability to work in globally distributed teams
Knowledge of IT processes, including quality assurance, release management, and production support
Excellent analytical, troubleshooting, and problem-solving skills
Excellent communicator (written and verbal, formal, and informal).
Flexible and proactive/self-motivated working style with strong personal ownership.
Ability to multi-task and prioritize under pressure.
Ability to work independently with minimal supervision as well as in a team environment.
Undergraduate or graduate degree in Computer Science, Data Science, or equivalent education/professional experience is required
Warm Regards,
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
PH.
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
BimNQHf5mR
Show more
Show less","AWS, Lambda, S3, RDS, Aurora, DynamoDB, Kinesis, AWS Glue, AWS Data Pipeline, Informatica, SQL, Data migration, Data analysis","aws, lambda, s3, rds, aurora, dynamodb, kinesis, aws glue, aws data pipeline, informatica, sql, data migration, data analysis","aurora, aws, aws data pipeline, aws glue, data migration, dataanalytics, dynamodb, informatica, kinesis, lambda, rds, s3, sql"
Lead Data Engineer-Azure Databricks- US,Zortech Solutions,United States,https://www.linkedin.com/jobs/view/lead-data-engineer-azure-databricks-us-at-zortech-solutions-3745133198,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Role:
Lead Data Engineer-Azure Databricks
Location: Remote/US
Duration: 6+ Months
Job Description
Ability to design and develop Azure framework which should be able to pull data from any type of unstructured/ structured sources of data, run transformation based on business rules and load in various types of targets
Candidate need strong Java programming and troubleshooting skills.
Expertise with Azure Databricks Services
Should have BigData, K8 / ETL experience
Show more
Show less","Azure framework, Java programming, Azure Databricks Services, BigData, K8, ETL","azure framework, java programming, azure databricks services, bigdata, k8, etl","azure databricks services, azure framework, bigdata, etl, java programming, k8"
"Data Engineer (AWS, Azure, GCP)",CapTech,"Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-aws-azure-gcp-at-captech-3774199172,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Company Description
CapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.
Job Description
CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.
Specific responsibilities for the Data Engineer – Cloud position include:
Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)
Advising clients on specific technologies and methodologies for utilizing cloud resources to efficiently ingest and process data quickly
Utilizing your skills in engineering best practices to solve complex data problems
Collaborating with end users, development staff, and business analysts to ensure that prospective data architecture plans maximize the value of client data across the organization.
Articulating architectural differences between solution methods and the advantages/disadvantages of each
Qualifications
Typical experience for successful candidates includes:
Experience delivering solutions on a major cloud platform
Ability to think strategically and relate architectural decisions/recommendations to business needs and client culture
Experience in the design and implementation of data architecture solutions
A wide range of production database experience, usually including substantial SQL expertise, database administration, and scripting data pipelines
Ability to assess and utilize traditional and modern architectural components required based on business needs.
A demonstrable ability to deliver production data pipelines and other data products. This could be hands on experience, degree, certification, bootcamp, or other learning.
Skills
Successful candidates usually have demonstrable experience with technologies in some of these categories:
Languages: SQL, Python, Java, R, C# / C++ / C
Database: SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle
DevOps: git, docker, subversion, Kubernetes, Jenkins
Additional Technologies: Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR
Popular Certifications: AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer
Additional Information
We want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs.
CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clients
Learning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths
Modern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs
Carrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care
Fringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and more
Employee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations
Philanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities.
401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.
At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
#LM-Remote
Show more
Show less","Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), SQL, Python, Java, R, C#, C++, C, SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle, git, docker, subversion, Kubernetes, Jenkins, Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR, AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer","amazon web services aws, microsoft azure, google cloud platform gcp, sql, python, java, r, c, c, c, sql server, postgresql, snowflake, redshift, aurora, presto, bigquery, oracle, git, docker, subversion, kubernetes, jenkins, spark, databricks, kafka, kinesis, hadoop, lambda, emr, aws cloud practitioner, microsoft azure data fundamentals, google associate cloud engineer","amazon web services aws, aurora, aws cloud practitioner, bigquery, c, databricks, docker, emr, git, google associate cloud engineer, google cloud platform gcp, hadoop, java, jenkins, kafka, kinesis, kubernetes, lambda, microsoft azure, microsoft azure data fundamentals, oracle, postgresql, presto, python, r, redshift, snowflake, spark, sql, sql server, subversion"
GCP Data Engineer,Accroid Inc,United States,https://www.linkedin.com/jobs/view/gcp-data-engineer%C2%A0-at-accroid-inc-3774786009,2023-12-17,Menlo Park,United States,Mid senior,Remote,"GCP Data Engineer
GCP Experience
Must have Banking and Payments experience
Must have good communication
Show more
Show less","GCP, Banking, Payments, Communication","gcp, banking, payments, communication","banking, communication, gcp, payments"
Data Engineer,Xoriant,"Dallas, TX",https://www.linkedin.com/jobs/view/data-engineer-at-xoriant-3775952368,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Position Title: Data Engineer
Location: Sunnyvale, CA / Dallas, TX / REMOTE (need to work CST hours)
Duration: 12 Months contract with possibility of extension
Position Description:
Skill Set Required:
Spark, Scala, Google Cloud, SQL, Hive, Hadoop and any scheduling tool.
Experience level:
Minimum 4-5 years of experience as Big Data Engineer
Show more
Show less","Spark, Scala, Google Cloud, SQL, Hive, Hadoop, Scheduling tools, Big Data Engineering","spark, scala, google cloud, sql, hive, hadoop, scheduling tools, big data engineering","big data engineering, google cloud, hadoop, hive, scala, scheduling tools, spark, sql"
Senior Data Engineer,Infotek Consulting Inc.,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-infotek-consulting-inc-3779823685,2023-12-17,Menlo Park,United States,Mid senior,Remote,"We are searching for several Senior Data Engineers - this is a REMOTE role working on EST - must be on W2
SUMMARY OF DAY-TO-DAY RESPONSIBILITIES:
The main function of a Data Analyst is to coordinate changes to computer databases, test, and implement the database applying knowledge of database management systems. A typical Data Analyst is responsible for planning, coordinating, and implementing security measures to safeguard the computer database. Job Responsibilities: Review and approve the testing of programs or databases. Review existing databases and database management systems and direct programmers and analysts to make changes. Coordinate database development and determine project scope and limitations. Review project requests describing database user needs to estimate time and cost required to accomplish project. Approve, schedule, plan and supervise the installation and testing of new products and improvements to computer systems. Plan, coordinate, and implement security measures to safeguard information in computer files against accidental or unauthorized damage, modification, or disclosure. Develop standards and guidelines to guide the use and acquisition of software and to protect vulnerable information.
MUST HAVE:
SQL experience
Databricks
Banking experience
Implementing data warehousing in banking
Data Analyst/Business Analyst experience
Azure
Tableau
NICE TO HAVE
AML experience
PMP Certification
Business and technology experience
Show more
Show less","SQL, Databricks, Azure, Tableau, Data Analyst, Business Analyst, Data warehousing, Banking, AML, PMP Certification","sql, databricks, azure, tableau, data analyst, business analyst, data warehousing, banking, aml, pmp certification","aml, azure, banking, business analyst, dataanalytics, databricks, datawarehouse, pmp certification, sql, tableau"
Data Engineer,Insight Global,United States,https://www.linkedin.com/jobs/view/data-engineer-at-insight-global-3779389659,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Job Description:
This position reports to the Data Partnerships Director of Data and Analytics Platforms. This position will be responsible for the management and execution of the deidentication processes applied to the data assets to be included in the Federated Clinical Applications Platform (FCAP), and the management and administration of the FCAP deidentification cloud environment .
The position will be a member of the Data Parternships Data Engineering team and will additionally provide expertise in the development of data integration and delivery pipelines to deliver new data modalities into the FCAP and Data Lake. These solutions will capitalize on technologies to improve the value of analytical data, improve effectiveness of information stewardship, and streamline the flow of data in the organization. Solutions will focus on using state of the art data and analytics tools including traditional and near real-time data warehousing, big-data, relational and document based databases using both extract, load, transform (ELT) toolsets as well as REST APIs and FHIR. The ideal candidate will be comfortable with data science platforms with proven experience leveraging DevOps and Automation/Orchestration tools.
Must Haves:
5+ years of experience in a hands on Data Engineer role
Expertise in a Linux & Azure environment
Bachelor's degree in a related field
Experience with relational SQL and NoSQL databases
Writing and executing Python programs and shell scripts
on Linux
Linux administration experience
Data Engineering on Microsoft Azure
Experience with data pipeline and orchestration tools such as Azure Data Factory and SQL Server Integration Services
Developing on cloud-based analytic platforms such as Azure Synapse
Plusses:
Prior experience in health care IT
Working knowledge of Azure DevOps & Automation/Orchestration
Knowledge of open source software solutions and open source as a business model
Technical breadth across application development, enterprise architecture, or application integration
Understanding of Agile methodology
Knowledge of APIs, API Integration, and API Management
Job Responsibilities:
Create and follow defined procedures in the deidentification of patient medical information
Maintain and tune the deidentification environment to perform optimally and comply with policies and standards
Collaborate with partners on improving the deidentication programs and processes, and work with the partner and the Cloud Team on troubleshooting issues
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Recommend design of analytics solutions which improves data integration, data quality, and data delivery with an eye towards re-useable components
Create and maintain an optimal data pipeline architecture
Show more
Show less","Data Engineering, Linux, Azure, SQL, Python, Shell Scripting, Azure Data Factory, SQL Server Integration Services, Azure Synapse, Agile Methodology, APIs, API Integration, API Management, Deidentification, Cloud Computing, DevOps, Automation, Orchestration, Open Source Software, Enterprise Architecture, Data Integration, Data Quality, Data Delivery, Scalability","data engineering, linux, azure, sql, python, shell scripting, azure data factory, sql server integration services, azure synapse, agile methodology, apis, api integration, api management, deidentification, cloud computing, devops, automation, orchestration, open source software, enterprise architecture, data integration, data quality, data delivery, scalability","agile methodology, api integration, api management, apis, automation, azure, azure data factory, azure synapse, cloud computing, data delivery, data engineering, data integration, data quality, deidentification, devops, enterprise architecture, linux, open source software, orchestration, python, scalability, shell scripting, sql, sql server integration services"
Data Engineer III,Millennium Corporation,United States,https://www.linkedin.com/jobs/view/data-engineer-iii-at-millennium-corporation-3778855882,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Millennium Corporation is hiring a
Data Engineer to work 100% remote. All candidates MUST have an active DoD Secret Clearance to qualify for consideration.
The selected candidate will:
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
May serve as lead on projects.
Qualifications:
Candidate MUST have an active Secret Clearance
Bachelor’s Degree in Business, Computer Science, Engineering, Finance, Accounting, Mathematics, Statistics, or Economics or related field and six (6) years of related experience including project management experience and best practices OR High School Diploma with at least eighteen (18) years of relevant experience.
Show more
Show less","Data Engineering, Data Pipeline Architecture, Data Extraction, Data Transformation, Data Loading, SQL, AWS Big Data Technologies, Analytics Tools, Data Visualization, Data Security, Data Management, Data Infrastructure, Project Management, Business Intelligence, Data Analytics, Data Science, Machine Learning, Statistics, Economics, Mathematics, Finance, Accounting","data engineering, data pipeline architecture, data extraction, data transformation, data loading, sql, aws big data technologies, analytics tools, data visualization, data security, data management, data infrastructure, project management, business intelligence, data analytics, data science, machine learning, statistics, economics, mathematics, finance, accounting","accounting, analytics tools, aws big data technologies, business intelligence, data engineering, data extraction, data infrastructure, data loading, data management, data pipeline architecture, data science, data security, data transformation, dataanalytics, economics, finance, machine learning, mathematics, project management, sql, statistics, visualization"
Big Data Engineer (Databricks),Eliassen Group,United States,https://www.linkedin.com/jobs/view/big-data-engineer-databricks-at-eliassen-group-3770190396,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Our client, an industry leader in industrial transportation, has an excellent opportunity for a Big Data Engineer to work on a 12-month contract. Work will be remote, must be able to work EST. Big Data Engineers are responsible for architecture, design and build of Big Data applications to support business strategies and deliver business value. A data engineer participates in all phases of the Data Engineering life cycle and will independently and collaboratively write project requirements, architect solutions, and perform data ingestion development and support duties.
This position is a contract opportunity, offering a comprehensive benefits package that includes medical, dental, vision, disability, and life insurance, plus 401(k), and weekly payment. No C2C.
Responsibilities of the Big Data Engineer:
The successful candidate must have Big Data engineering experience and must demonstrate an affinity for working with others to create successful solutions.
Use Databricks to process, store, clean, share, analyze, model, and monetize their datasets with solutions from BI to machine learning. Use the Databricks platform to build and deploy data engineering workflows, machine learning models, analytics dashboards, and more
Participate in all phases of the Data Engineering life cycle and will independently and collaboratively write project requirements, architect solutions and perform data ingestion development and support duties.
Data Engineers require ability to communicate effectively, both written and verbal, and have extensive experience working with business areas to translate their business data needs and data questions into project requirements.
Requirements of the Big Data Engineer:
Must have hands-on experience with Databricks
Must have hands-on experience with high-velocity high-volume stream processing: Apache Kafka and Spark Streaming
o Experience with real-time data processing and streaming techniques using Spark structured streaming and Kafka
o Deep knowledge of troubleshooting and tuning Spark applications
Must have hands-on experience with Python and/or Scala i.e. PySpark/Scala-Spark
Must have hands-on experience building, testing, and optimizing ‘Big Data’ data ingestion pipelines, architectures and data sets
Experience in successfully building and deploying a new data platform on Azure/ AWS
Experience in Azure / AWS Serverless technologies, like, S3, Kinesis/MSK, lambda, and Glue
Knowledge of Messaging Platforms like Kafka, Amazon MSK & TIBCO EMS or IBM MQ Series
Experience with Databricks UI, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL, Delta Live Tables, Unity Catalog
Experience with data ingestion of different file formats across like JSON, XML, CSV 11. Experience with NoSQL databases, including HBASE and/or Cassandra
Knowledge of Unix/Linux platform and shell scripting is a must
Experience with Cloud platforms e.g. AWS, GCP, etc. Experience with database solutions like Kudu/Impala, or Delta Lake or Snowflake or BigQuery
Preferred Skills:
Bachelor’s degree in Information Systems, Computer Science, or a related field
Strong SQL skills with ability to write intermediate complexity queries
Strong understanding of Relational & Dimensional modeling
Experience with GIT code versioning software
Experience with REST API and Web Services
Good business analyst and requirements gathering/writing skills
Job ID:
381115
Show more
Show less","Big Data engineering, Databricks, Apache Kafka, Spark Streaming, Spark structured streaming, PySpark/ScalaSpark, Python, Scala, Azure, AWS, Serverless technologies, S3, Kinesis/MSK, Lambda, Glue, Kafka, Amazon MSK, TIBCO EMS, IBM MQ Series, Delta Lake, JSON, XML, CSV, HBASE, Cassandra, Unix/Linux, Shell scripting, Cloud platforms, Kudu/Impala, Snowflake, BigQuery, SQL, Relational & Dimensional modeling, GIT, REST API, Web Services","big data engineering, databricks, apache kafka, spark streaming, spark structured streaming, pysparkscalaspark, python, scala, azure, aws, serverless technologies, s3, kinesismsk, lambda, glue, kafka, amazon msk, tibco ems, ibm mq series, delta lake, json, xml, csv, hbase, cassandra, unixlinux, shell scripting, cloud platforms, kuduimpala, snowflake, bigquery, sql, relational dimensional modeling, git, rest api, web services","amazon msk, apache kafka, aws, azure, big data engineering, bigquery, cassandra, cloud platforms, csv, databricks, delta lake, git, glue, hbase, ibm mq series, json, kafka, kinesismsk, kuduimpala, lambda, pysparkscalaspark, python, relational dimensional modeling, rest api, s3, scala, serverless technologies, shell scripting, snowflake, spark streaming, spark structured streaming, sql, tibco ems, unixlinux, web services, xml"
Data Engineer,Harnham,United States,https://www.linkedin.com/jobs/view/data-engineer-at-harnham-3775618186,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Job Title: Data Engineer
Location: Los Angeles, CA or New York, NY (Hybrid - 2 days in-office)
Position Type: 6-month
Pay Rate: $60-70/hr
About the Opportunity:
My client, a prominent entertainment company, is actively seeking a skilled Data Engineer to join their innovative Data Technology team. This exciting opportunity involves working on cutting-edge projects related to advertising, marketing, and data solutions. The team is currently engaged in building an enterprise data platform and a state-of-the-art data management platform that prioritizes user consent and privacy.
Key Responsibilities:
Technical Proficiency:
Apply hands-on experience with AWS, with proficiency in Python.
Data Transition:
Contribute to the recent transition from Redshift to Snowflake.
Computation Expertise:
Work on large computations, primarily shifting to Snowflake, while managing legacy projects on Spark.
Collaborative Environment:
Collaborate with a diverse team, including analysts, engineers, product, legal, and external vendors.
Project Leadership:
Play a pivotal role in the migration to a fully functional data management platform.
Desired Skills and Experience
Technical Skills:
Solid experience with AWS (Lambda, Step Functions, API Gateway), databases (Snowflake, Redshift, etc.), and proficiency in SQL and Python.
Desirable Skills:
Previous exposure to marketing, advertising, consent, or commercial projects is advantageous.
Adaptability:
Ability to thrive in a fast-paced environment and contribute to scoping, discovery, and building new platforms.
Qualifications:
4-5 years of experience in a similar role.
Previous exposure to media companies is a plus.
Software engineering background preferred.
Show more
Show less","Data Engineer, AWS, Python, Redshift, Snowflake, Spark, SQL, Lambda, Step Functions, API Gateway, Databases","data engineer, aws, python, redshift, snowflake, spark, sql, lambda, step functions, api gateway, databases","api gateway, aws, databases, dataengineering, lambda, python, redshift, snowflake, spark, sql, step functions"
Sr. Data Engineer I,Pax8,"Colorado, United States",https://www.linkedin.com/jobs/view/sr-data-engineer-i-at-pax8-3554228541,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Pax8 is the leading cloud-based technology marketplace, simplifying the cloud journey for our partners by integrating technology, business intelligence and proactive service to deliver an unparalleled experience. Serving thousands of partners through the indirect sales channel, our mission is to be the world’s favorite place to buy cloud products. We are a fast-growing, dynamic and high-energy startup organization, allowing you to make a meaningful impact on the business. Culture is important to us, and at Pax8, it’s business, and it
IS
personal. We are passionate, creative and humorously offbeat. We work hard, keep it fun, and expect the best.
We Elev8 each other. We Advoc8 for our partners. We Innov8 continuously. We Celebr8 life.
No matter who you are, Pax8 is a place you can call home. We know there’s no such thing as a
“perfect""
candidate, so we don’t look for the right ""
fit
"" – instead, we look for the add. We encourage you to apply for a role at Pax8 even if you don’t meet 100% of the bullet points. We believe in cultivating an environment with a diversity of perspectives, in hopes that we can all thrive in an inclusive environment.
We are only as great as our people. And we have great people all over the world. No matter where you live and work, you’re a part of the Pax8 team. This means embracing hybrid- and remote-work whenever possible.
Position Summary:
Are you a wizard with Data? At Pax8, we are looking to the future and how data will help get us there. You will join our growing Data Science team who are committed to the creation and expansion of clean accessible data to our internal Pax8 partners.
As a Sr. Data Engineer, you will work amongst a collaborative team working with data from inception to modeling.This team works with ETL processes in order to pull data from across various data sources and consolidate it into our newly created and centralized Redshift data warehouse. The data you will work with and be creating a central point of truth for will aid several different Pax8 teams stretching across the organization that will be aimed at improving our customers' experience within our marketplace.
For you to successfully perform in this role, you will need to be equipped with experience to run with data, end-to-end, including everything from inception to modeling. Strong Python and SQL skills will be needed to navigate and deliver on the architectural areas and testing. You'll work with a vast amount of data, one crucial area will be within real-time streaming data harnessed by the use of Kafka. Having a solutions mindset will also be a key element to your contribution to our team approach to projects.
Meet the team and hear what they have to say about our Product & Engineering group: Meet the Team
Essential Responsibilities:
Builds pipelines to ingest new data sources
Transforms data to support varied use cases
Includes testing in all aspects of the development process
Mentors junior and mid-level Engineers
Optimizes existing data pipelines and improves existing code quality
Makes updates and improvements to deployment processes
Participates in project planning and architecture discussions
Analyzes potential problems and finds solutions to pressing data issues
Participates in on-call rotation
Ideal Skills, Experience, and Competencies:
At least Four (4) years of relevant data engineering experience
Advanced experience with Python
Expert experience with SQL
Intermediate experience with a JVM language
Exposure to other software development languages
Advanced experience with Apache Spark or other distributed processing engines
Advanced experience with Apache Kafka or other stream processing frameworks
Intermediate experience with Terraform, Docker, Kubernetes, or other similar infrastructure tooling
Advanced experience with cloud data tools such as S3, Glue, and Athena
Intermediate experience with building CI/CD pipelines
Effective problem solving and troubleshooting abilities
Ability to consistently achieve results, even under tough circumstances
Effective technical leadership abilities
Excellent verbal and written communication skills
Experience with innovative application design and implementation
Ability to make sense of complex, high quantity, and sometimes contradictory information to effectively solve problems
Required Education & Certifications:
B.A./B.S. in related field or equivalent work experience
M.S./M.A. in related field or equivalent work experience
Compensation:
Qualified candidates can expect a salary beginning at $140,000 or more depending on experience
#Dice-J #BI-Remote
Note: Compensation is benchmarked on local Denver Metro area market rates. Qualified candidates in other locations can expect a salary package that may be adjusted based off applicable cost of wages in their respective location.
At Pax8 we believe that your Total Rewards should include a benefits package that shows how much we value our greatest assets. All
FTE
Pax8 people enjoy the following benefits:
Non-Commissioned Bonus Plans or Variable Commission
401(k) plan with employer match
Medical, Dental & Vision Insurance
Employee Assistance Program
Employer Paid Short & Long Term Disability, Life and AD&D Insurance
Flexible, Open Vacation
Paid Sick Time Off
Extended Leave for Life events
RTD Eco Pass (For local Colorado Employees)
Career Development Programs
Stock Option Eligibility
Employee-led Resource Groups
Pax8 is an EEOC Employer.
Show more
Show less","Data Engineering, Data Science, Data Transformation, Data Warehousing, Data Quality, Data Integration, Data Modeling, Data Streaming, Data Pipelining, Python, SQL, JVM, Terraform, Docker, Kubernetes, CI/CD Pipelines, Apache Spark, Apache Kafka, S3, Glue, Athena, Cloud Data Tools","data engineering, data science, data transformation, data warehousing, data quality, data integration, data modeling, data streaming, data pipelining, python, sql, jvm, terraform, docker, kubernetes, cicd pipelines, apache spark, apache kafka, s3, glue, athena, cloud data tools","apache kafka, apache spark, athena, cicd pipelines, cloud data tools, data engineering, data integration, data quality, data science, data streaming, data transformation, datamodeling, datapipeline, datawarehouse, docker, glue, jvm, kubernetes, python, s3, sql, terraform"
Senior Data Engineer,Big Cloud,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-big-cloud-3774584522,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Do you enjoy working with stakeholders to solve challenging technological problems?
Would you like to join a flat hierarchy of elite tech consultants to manage end-to-end software and AI projects?
I'm recruiting for a flexible, remote-first software company solving major problems across healthcare AI, telecoms, edge computing and much more.
This company prioritizes a healthy work-life balance, allowing the team the freedom to excel in their projects while embracing remote work options. As a Data Engineer, you'll have access to state-of-the-art technology (both in-house and industry software), offering lots of opportunities to up-skill and expand your tech stack.
The founder and CEO of this company sold his previous AI startup to one of the world's largest retail and supply chain giants. In this new venture, he and the team are looking to solve even more data-driven challenges.
Responsibilities:
working with stakeholders on end-to-end long-term and short-term technical projects
shaping, managing and executing your own projects
helping the tech team scale, potentially making contributions to long-term strategy-making
knowledge sharing, mentoring and learning from a team of experienced specialists
keeping updated with the latest research in software, generative AI and LLMs
Requirements:
3+ years of data engineering experience
Experience with big data processing (Kafka/Hadoop/Spark)
Masters or Bachelors in computer science or equivalent
If you're excited about the idea of shaping smart features that improve the world of AI and cloud data, I'd love to hear from you.
The company is paying a competitive salary to secure the best talent (no contract work).
Big Cloud is a machine learning recruiting firm. We’re lucky enough to recruit the best candidates in the most exciting companies all over the world. We try to reply to all applications, but we’re only human, for now! So, you may only hear from us if you are successful.
Check out www.bigcloud.io/jobs to see what else we’re recruiting for.
Show more
Show less","Data Engineering, Software Development, AI, Machine Learning, Cloud Computing, Big Data Processing, Kafka, Hadoop, Spark, Software, Generative AI, LLMs","data engineering, software development, ai, machine learning, cloud computing, big data processing, kafka, hadoop, spark, software, generative ai, llms","ai, big data processing, cloud computing, data engineering, generative ai, hadoop, kafka, llms, machine learning, software, software development, spark"
Data Engineer,Edge & Node,United States,https://www.linkedin.com/jobs/view/data-engineer-at-edge-node-3768704344,2023-12-17,Menlo Park,United States,Mid senior,Remote,"Edge & Node stands as the revolutionary vanguard of web3, a vision of a world powered by individual autonomy, shared self-sovereignty and limitless collaboration. Established by trailblazers behind The Graph, we’re on a mission to make The Graph the internet’s unbreakable foundation of open data. Edge & Node invented and standardized subgraphs across the industry, solidifying The Graph as the definitive way to organize and access blockchain data. Utilizing a deep expertise in developing open-source software, tooling, and protocols, we empower builders and entrepreneurs to bring unstoppable applications to life with revolutionary digital infrastructure.
Edge & Node acts on a set of unwavering principles that guide our journey in shaping the future. We champion a decentralized internet—free from concentrated power—where collective consensus aligns what is accepted as truth, rather than authoritative dictation. Our commitment to censorship resistance reinforces our vision of an unyielding information age free from the grasp of a single entity. By building for open-source, we challenge the stagnant landscape of web2, recognizing that true innovation thrives in transparency and collaboration. We imagine a permissionless future where the shackles imposed by central gatekeepers are not only removed, but relegated to the dustbin of a bygone era. And at the foundation of it all, our trust shifts from malevolent middlemen to trustless systems, leveraging smart contracts to eliminate the age-old vulnerabilities of misplaced trust.
The Data Science team works closely with teams across Edge & Node to deliver high quality data for product research & development and go to market, as well as business analytics. We work across the data lifecycle from infrastructure to data analytics.
We are looking for an early-career Data Engineer to be focused on developing and maintaining data science pipelines. Ideally, the team would like to bring on someone who has experience with the current tools being used by the team which include, but are not limited to, Redpanda, Materialize, and GCP. In this role, you will monitor and maintain reliability of the Redpanda cluster, streaming database, DBT jobs, QoS oracle, and other data engineering systems. You’ll be expected to learn Materialize and help migrate BigQuery models to reduce costs. In addition, you will help establish and maintain good standards around documentation and internal educational tools and respond to data engineering/devops requests in our incident management process.
What You’ll Be Doing
Learning our infrastructure and data engineering toolset
Partnering closely with our Data Science and SRE teams to perform various data warehouse jobs and periodic RedPanda/streaming database devops tasks
Manage historical data models in BigQuery/DBT
Develop pipelines to support dashboards and perform devops tasks to support dashboards
What We Expect
Experience with one or more of the following: BigQuery, ETL automation/workflow tools (DBT), BI/dashboarding tools (Apache Superset/Metabase), streaming data platforms (Apache Kafka, Redpanda, or Confluent), or other data engineering and data warehouse toolsets/environments
Some experience or knowledge of container orchestration tools such as Kubernetes and Kustomize preferred
Some experience or knowledge of monitoring and alerting (Grafana dashboards) preferred
Some experience or knowledge of SQL–able to create and manage tables within a SQL database
Proficiency in one or more programming languages, such as Python, R, or Rust
Must be able to to serve on-call shifts and support devops needs
Ability to create documentation and communicate with a a variety of audiences
Clear communication skills (written and verbal) to document processes and architectures
Ability to work well within a multinational team environment
Preference to be physically located in The Americas, however the team is open to candidates in European time zones or other locations
About the Graph
The Graph is the indexing and query layer of web3. The Graph Network’s self service experience for developers launched in July 2021. Developers build and publish open APIs, called subgraphs, that applications can query using GraphQL. The Graph supports indexing data from multiple different networks including Ethereum, NEAR, Arbitrum, Optimism, Polygon, Avalanche, Celo, Fantom, Moonbeam, IPFS, and PoA with more networks coming soon. To date, tens-of-thousands of subgraphs have been deployed on the hosted service, and now subgraphs can be deployed directly on the network. Over 28,000 developers have built subgraphs for applications such as Uniswap, Synthetix, KnownOrigin, Art Blocks, Balancer, Livepeer, DAOstack, Audius, Decentraland, and many others.
If you are a developer building an application or web3 application, you can use subgraphs for indexing and querying data from blockchains. The Graph allows applications to efficiently and performantly present data in a UI and allows other developers to use your subgraph too! You can deploy a subgraph to the network using the newly launched Subgraph Studio or query existing subgraphs that are in the Graph Explorer. The Graph would love to welcome you to be Indexers, Curators and/or Delegators on The Graph’s mainnet. Join The Graph community by introducing yourself in The Graph Discord for technical discussions, join The Graph’s Telegram chat, and follow The Graph on Twitter, LinkedIn, Instagram, Facebook, Reddit, and Medium! The Graph’s developers and members of the community are always eager to chat with you, and The Graph ecosystem has a growing community of developers who support each other.
The Graph Foundation oversees The Graph Network. The Graph Foundation is overseen by the Technical Council. Edge & Node, StreamingFast, Messari, Semiotic and The Guild are five of the many organizations within The Graph ecosystem.
Show more
Show less","Python, R, Rust, BigQuery, ETL Automation & Workflow Tools, DBT, BI/Dashboarding Tools, Apache Superset, Metabase, Streaming Data Platforms, Apache Kafka, Redpanda, Confluent, Kubernetes, Kustomize, Grafana Dashboards, SQL, Devops, Documentation, Communication, Containers, Cloud Native, Software Development","python, r, rust, bigquery, etl automation workflow tools, dbt, bidashboarding tools, apache superset, metabase, streaming data platforms, apache kafka, redpanda, confluent, kubernetes, kustomize, grafana dashboards, sql, devops, documentation, communication, containers, cloud native, software development","apache kafka, apache superset, bidashboarding tools, bigquery, cloud native, communication, confluent, containers, dbt, devops, documentation, etl automation workflow tools, grafana dashboards, kubernetes, kustomize, metabase, python, r, redpanda, rust, software development, sql, streaming data platforms"
Data Platform Engineer,Green Light Worldwide,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-platform-engineer-at-green-light-worldwide-3763825969,2023-12-17,New South Wales, Australia,Associate,Onsite,"Initial 6-months Contract
Hybrid Set-up
Sydney Office
About the Company:
Green Light is one of Asia Pacific’s most successful and fastest growing IT services businesses. We offer a wide range of IT services in Security, infrastructure and Systems Integration to a diverse customer base throughout Australia, New Zealand, SE Asia and Europe.
Green Light are looking to bring on-board a talented
Data Platform Engineer
to work with our delivery team in Sydney. The Data Integration Engineer will develop and implement data integration solutions required to integrate IT systems and data flows in order to meet business requirements.
The Data Platform Engineer is additionally responsible for ensuring all the platforms are kept up to date with the latest software and security. Your strong and broad technical background encompassing both cloud native and data center hosted solutions along with the underlying operating system, database, network, security, and other various development and system integration principles and techniques will enhance your ability to be successful in this role.
Responsibilities
:
Develop and troubleshoot quality and maintainable code base.
Interact with business domain subject matter experts to understand requirements.
Perform data modelling, data extracts, data translation, and data clean-up as part of larger projects
Analyse and migrate large complex data sets.
Create and document scripts/programs to facilitate the extraction, transformation and loading of data.
Troubleshoot and resolve issues in operational environments.
Implement software development lifecycle best practices in data protection.
Requirements
:
Strong technical experience using
ETL tools
such as
DTS/ ODI/ Informatica PowerCentre
Working knowledge of data base design principles, performance tuning, and solution scalability in a data driven environment.
Demonstrated experience with coding best practices and guidelines.
Past experience working in an Agile environment.
Proven ability to thrive in a fast-paced environment and make good decisions
Minimum required skills:
Experience working with a variety of cross-functional teams
Able to work in a fast-paced, constantly evolving environment and manage multiple priorities
Pragmatic and capable of solving complex issues
Service-oriented, flexible team player
Attention to detail & technical intuition
Excellent written, verbal, and interpersonal skills for executive-level communication and collaboration
Experience in the bio/pharmaceutical industry is a plus
Minimum preferred skills
Strong Regulatory Experience (GxP, Sarbanes-Oxley, French Data Protection Act (FDPA), General Data Protection Regulation (GDPR)
Good knowledge of AWS/Cloud Computing (Knowledge of Azure or GCP a plus)
Experience (and enjoyment) working with emerging technologies
How to Apply:
Click APPLY or contact LIZ – Resource Manager at lizc@greenlightsworldwide.com for a confidential discussion with Green Light Worldwide.
Show more
Show less","Data integration, Data migration, ETL tools (DTS ODI Informatica PowerCenter), Data modeling, Data extraction, Data transformation, Data loading, Data cleansing, Data protection, Software development lifecycle, Database design principles, Performance tuning, Scalability, Agile development, Cloud computing (AWS Azure GCP), Regulatory compliance (GxP SarbanesOxley FDPA GDPR)","data integration, data migration, etl tools dts odi informatica powercenter, data modeling, data extraction, data transformation, data loading, data cleansing, data protection, software development lifecycle, database design principles, performance tuning, scalability, agile development, cloud computing aws azure gcp, regulatory compliance gxp sarbanesoxley fdpa gdpr","agile development, cloud computing aws azure gcp, data extraction, data integration, data loading, data migration, data protection, data transformation, database design principles, datacleaning, datamodeling, etl tools dts odi informatica powercenter, performance tuning, regulatory compliance gxp sarbanesoxley fdpa gdpr, scalability, software development lifecycle"
Data & Integration Analyst,Aegros,"Macquarie Park, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-integration-analyst-at-aegros-3649793833,2023-12-17,New South Wales, Australia,Associate,Onsite,"Who We Are…
Aegros is Latin for ‘Patient’. We chose this name because it speaks to our focus on the patient. Every day we strive to place the patient first by providing therapeutic plasma products they would not otherwise be able to access.
At Aegros we believe access to life saving therapeutic plasma drugs is a human right and not a privilege. In a nutshell our mission is to put the patients first through innovation.
Over 30 years we have developed our HaemaFrac™ process which enables countries to turn human plasma they collect into lifesaving hyperimmune products.
Here’s the high-level…
Due to our rapid expansion plans at Aegros, the IT Team is experiencing double digit growth…!!!!
As part of our growth strategy, we now require a dedicated Data & Integration Analyst to join the team at Macquarie Park, Sydney. Reporting to the CIO, you will be responsible for leading and providing specialist support in the development and implementation of data integration, reporting and dashboarding between and within the suit of new technology platforms being implemented at Aegros to enable the business vision.
You will work with business stakeholders to define and deliver how Aegros collects, stores, integrates, and represents data between and across platforms and systems. Is this the next step in your career?
What You Will Be Doing…
Developing and maintaining data assets and integrations, including data modelling, classification, development of business rules to support data quality and integrity, and integration with systems, analysis tools and visualisation tools
Developing and maintaining data standards, integration playbooks, and roadmaps to meet the organisations target state goals
Undertaking projects in collaboration with business areas across the organisation, including scoping, planning, and delivering a range of project tasks against agreed timelines
Engaging with internal stakeholders to scope the opportunities to leverage data and lift data capability within the organisation
Developing different ways to visualise and report results of data analysis to internal clients and senior managers and maintain regular reporting/visualisations
Working collaboratively with stakeholders to find and develop solutions to business problems & opportunities, by communicating with staff and partners to understand their requirements and capabilities
Designing and delivering reporting and visualisation capabilities to support the organisations digitisation and continuous improvement goal.
Requirements
You will bring...
Bachelor of Computing Science or relevant degree
2 years Data Integration Analyst experience
Biopharma and/or pharmaceutical industry experience including GMP Data Management and Data Integrity (DMDI) regulatory requirements.
Knowledge and experience in Business Information systems such as Manufacturing Execution Systems ERP platforms (e.g. SYSPRO, SAP)
Experience with Microsoft Technology Stack, M365, Azure, SharePoint, SQL Server, and the Power Platform (PowerApps, Power Automate, and Power BI)
Strong working knowledge of Data Warehousing, Data Lakes, Business Information query and analysis systems and report writing
Report writing skills with Crystal Reports, Power Bi, SQL Server scripting or similar Integration Analys Experience in system implementation projects
Ability to deal with ambiguity, work in fast paced environment where you are required to adapt quickly to changing priorities
Ability to work both independently but also within a collaborative team environment
Excellent communication, influencing and collaboration skills to build consensus for improvement plans and lead their delivery
Benefits
What we can offer you…
Brand new role within the company where you can define the future!
Working with Aegros Leadership Team and Senior Stakeholders
Career development and advancement opportunities
A multicultural environment where every voice is heard
Competitive remuneration package including Bonus and Employee Share Options.
Interested? Apply today…!!!!!
Show more
Show less","Data Warehousing, Data Lakes, Business Information Systems, Data Analysis and Reporting, Crystal Reports, Power BI, Azure, SharePoint, SQL Server, Manufacturing Execution Systems, Enterprise Resource Planning Platforms, SYSPRO, SAP, PowerApps, Power Automate, Business Intelligence, Agile Development, Data Integration, Data Integrity, Data Modeling, Data Classification, Data Quality, Data Visualization, ETL tools, Reporting Tools","data warehousing, data lakes, business information systems, data analysis and reporting, crystal reports, power bi, azure, sharepoint, sql server, manufacturing execution systems, enterprise resource planning platforms, syspro, sap, powerapps, power automate, business intelligence, agile development, data integration, data integrity, data modeling, data classification, data quality, data visualization, etl tools, reporting tools","agile development, azure, business information systems, business intelligence, crystal reports, data analysis and reporting, data classification, data integration, data integrity, data lakes, data quality, datamodeling, datawarehouse, enterprise resource planning platforms, etl tools, manufacturing execution systems, power automate, powerapps, powerbi, reporting tools, sap, sharepoint, sql server, syspro, visualization"
Data Engineer - Hadoop Bigdata Platform,Virtusa,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineer-hadoop-bigdata-platform-at-virtusa-3761037085,2023-12-17,New South Wales, Australia,Associate,Hybrid,"Implementation experience in BigData Platform ; preferably in Cloudera Hadoop platform
Minimum 2 years of Development experience using Hadoop eco system tools & utilities: MapReduce, Spark, Kafka, Sqoop, Impala, Hive etc
Ability to work independently and also contribute to overall architecture and design
Experience in writing Shell scripts in Linux Platform
Knowledge on API management concepts and design
Developed Apache Spark applications and comfortable developing in Python. (Preferred)
Performed debugging and performance tuning of Spark applications.
Show more
Show less","Cloudera Hadoop, MapReduce, Spark, Kafka, Sqoop, Impala, Hive, Shell scripting, Linux, API management, Apache Spark, Python, Debugging, Performance tuning","cloudera hadoop, mapreduce, spark, kafka, sqoop, impala, hive, shell scripting, linux, api management, apache spark, python, debugging, performance tuning","apache spark, api management, cloudera hadoop, debugging, hive, impala, kafka, linux, mapreduce, performance tuning, python, shell scripting, spark, sqoop"
Data Quality Engineer,NGM Group,"Newcastle, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-quality-engineer-at-ngm-group-3785869211,2023-12-17,New South Wales, Australia,Associate,Hybrid,"Purpose of the role
As a Data Quality Engineer reporting to the Data Governance Manager, you will be an integral part of a close-knit team dedicated to ensuring the highest standards of data quality at NGM Group is maintained. In this hands-on role, your primary focus will be on developing, configuring, and using tools to maintain data lineage, conduct control testing, and monitor data quality. You will work with stakeholders across NGM Group on resolving data quality issues to ensure data quality outcomes essential for informed decision-making and regulatory compliance.
What will your key responsibilities include?
Create and maintain data lineage for Critical Data Elements (CDEs). Configure available Data Governance System connectors for automated metadata ingestion, develop and maintain custom metadata ingestion solutions in accordance with NGM Group’s Software Development Lifecycle (SDLC).
Perform data quality control testing along the CDE lineage and deliver results to stakeholders.
Capture requirements for and implement data quality rules using off-the-shelf and custom solutions. Profile data, develop, and maintain data quality rules in alignment with NGM Group’s Data Risk Management Framework and SDLC.
Monitor data quality rule execution results to detect, assess, and rectify data quality issues. Develop and maintain data quality dashboards.
Assist Data Governance Managers in administering the Data Governance System by automating the maintenance of appropriate configurations across the environments.
Support Data Governance Managers in running processes necessary for bringing critical data under governance.
What are we looking for?
Tertiary qualifications in computer science, information technology, mathematics, or business, or equivalent work experience.
Background in software engineering in an agile environment using Java or Python programming languages.
Working knowledge of integration with REST APIs.
Experience with data profiling using SQL (experience with other information retrieval languages such as XQuery and data analysis packages such as Pandas is a bonus).
Experience reverse-engineering solutions that use data storage, data integration and data analytics technologies (experience with Microsoft SQL Server, Oracle Database, Microsoft SSIS, Power BI is a bonus).
Exposure to metadata management, data catalogues and business glossaries (experience with Microsoft Purview and Apache Atlas is a bonus).
Attention to detail, focus on accuracy and passion for problem solving.
Genuine interest in learning about and understanding the business, and the data required to run it.
What can you expect from us?
NGM Group employees can take advantage of the following work perks:
Health and wellbeing
: Fitness passport, corporate health insurance and annual flu vaccination.
Lifestyle and giving back:
Hotel discounts, 14 weeks paid parental leave, two community volunteer days and three recreational leave days per year.
Professional advancement:
Flexible work arrangements, recognition programs and employee referral program.
About us
We’re NGM Group, and we offer retail banking services to more than half a million Australians under the brands Greater Bank and Newcastle Permanent.
We have fresh energy and big goals for our customers, and our people are key to us achieving this success. If you’re ready to be part of a team that puts the customer first and enjoys a challenge as the path to growth and innovation, then … we want you!
Collectively, we’re the largest customer-owned bank based on net assets and the 10th largest Australian-owned bank for household deposits. A financial powerhouse headquartered in the Hunter, we have a workforce of more than 1,600 people and total assets of more than $20 billion.
NOTE:
Applications to be assessed in January 2024
Show more
Show less","Data Governance, Data Lineage, Data Quality, Data Profiling, Data Integration, Data Analytics, Data Catalogues, Data Glossaries, Metadata Management, Data Quality Rules, Data Quality Dashboards, REST APIs, Java, Python, SQL, XQuery, Pandas, Microsoft SQL Server, Oracle Database, Microsoft SSIS, Power BI, Microsoft Purview, Apache Atlas","data governance, data lineage, data quality, data profiling, data integration, data analytics, data catalogues, data glossaries, metadata management, data quality rules, data quality dashboards, rest apis, java, python, sql, xquery, pandas, microsoft sql server, oracle database, microsoft ssis, power bi, microsoft purview, apache atlas","apache atlas, data catalogues, data glossaries, data governance, data integration, data lineage, data profiling, data quality, data quality dashboards, data quality rules, dataanalytics, java, metadata management, microsoft purview, microsoft sql server, microsoft ssis, oracle database, pandas, powerbi, python, rest apis, sql, xquery"
Data Engineer,Practiv,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-practiv-3711944272,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"As a Practiv Data Engineer you will join a team of passionate data-cloud technologists who help deliver end to end data engineering solutions for our customers from building data pipelines, warehousing the data and visualisation/presentation. Practiv Data Engineers understand both data pipeline development, operations and management as well as delivery of automated data solutions that are ready for production.What you’ll do:
Solving our clients' most critical and difficult problems with advanced technologies
Being an integral part in helping to deliver Modern Data & Analytics solutions and help move our customers to the cloud
Architecting data platforms to support our clients helping them fully leverage and organise their enterprise data
Implementing data lakes as streaming platforms using the latest cloud and big data technologies
Working with a fun, collaborative team where everyone can be themselves and are provided the opportunity to love their life
What you’ll bring:
Strong professional services background along with well-rounded experience to offer across different disciplines within data and analytics
Experience with data services on any or all cloud platforms (Amazon Web Services, Azure, and Google Cloud)
Proficiency in modern data architectures and relational database design and development
Proficiency and hands-on experience with big data technologies
Experience with agile engineering and product development lifecycles and ability to manage agile engineering client engagements
Analytical approach to problem-solving; ability to use technology to solve business problems
Requirements
Strong SQL Skills
5+ years of commercial database experience
Experience with Cloud Data Warehouses (Amazon Redshift, Snowflake, Google BigQuery)
Modern Data Workflows (Apache Airflow, dbt, Dagster) and experience with technologies such as Snowflake, Matillion, DBT, Tableau CRM, Fivetran etc
Big Data Platforms (Apache Spark, Presto, Amazon EMR)
Object Oriented Coding (Java, Python)
NoSQL Databases (DynamoDB, Cosmos DB, MongoDB)
Container Management Systems (Kubernetes, Amazon ECS)
Artificial Intelligence / Machine Learning (Amazon Sagemaker, Azure ML Studio)
Streaming Data Ingestion and Analytics (Amazon Kinesis, Apache Kafka)
Visual Analytics (Tableau, PowerBI)
Experience with object-oriented/object function scripting languages: SQL, Python, PySpark, Scala, etc
Show more
Show less","Data Engineering, Cloud Computing, Data Warehousing, Data Visualization, Data Pipelines, Data Architecture, Data Lakes, Streaming Platforms, Agile Development, SQL, Cloud Data Warehouses, Modern Data Workflows, Big Data Platforms, Object Oriented Coding, NoSQL Databases, Container Management Systems, Artificial Intelligence, Machine Learning, Streaming Data Ingestion, Visual Analytics, Python, PySpark, Scala","data engineering, cloud computing, data warehousing, data visualization, data pipelines, data architecture, data lakes, streaming platforms, agile development, sql, cloud data warehouses, modern data workflows, big data platforms, object oriented coding, nosql databases, container management systems, artificial intelligence, machine learning, streaming data ingestion, visual analytics, python, pyspark, scala","agile development, artificial intelligence, big data platforms, cloud computing, cloud data warehouses, container management systems, data architecture, data engineering, data lakes, datapipeline, datawarehouse, machine learning, modern data workflows, nosql databases, object oriented coding, python, scala, spark, sql, streaming data ingestion, streaming platforms, visual analytics, visualization"
Data Engineer,Tata Consultancy Services,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-tata-consultancy-services-3777032981,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"About TCS:
Join Tata Consultancy Services, Asia Pacific and be part of an organization committed to sustainable development for our future. TCS follows the Tata group philosophy of building sustainable businesses that are rooted in the community and demonstrate care for the environment. Our unique values position us to combine a purpose-driven worldview with digital innovation, collaborating with customers, communities and governments to lead and shape innovative solutions for a sustainable future. TCS has been carbon neutral in its operations across 11 countries, 12 delivery centres and 18 offices in Asia Pacific since 2022. This is only the initial stage in TCS’ journey as we strive to achieve long-term net zero emissions by 2030.
Corporate sustainability is embedded in our triple-bottom-line, focusing on people, the planet, and our purpose. Our offices are designed with eco-friendly features that significantly reduce our carbon footprint and enhance energy efficiency. We actively champion green initiatives, such as promoting paperless operations, implementing energy-efficient practices, and fostering employee engagement in sustainability efforts. When you become part of the TCS family, you will play an essential role dedicated to innovation, excellence, and crafting a brighter, greener future together. Join us and be a part of our mission to drive sustainability through technology and talent at Tata Consultancy Services, APAC today
Please find the job description below
We are looking for engineers with strong understanding of banking.
Desired Competencies:
This is a cloud native build with AWS technologies such as S3, Glue, Dynamo, Lambda, Kafka, PySpark, Aurora and Iceberg. Knowledge of tools such as Jenkins, Terraform, Sonarqube etc well regarded. Data design experience necessary, not just a teckie/programmer.
Must have Skills:
PySpark,Java,RESTful APIs, Kafka
Roles and Responsibilities:
The Senior Software Engineer will be required to design, develop and support implementation of a cloud native data platform.
Providing technical input to the strategic direction of technology investments.
Developing & implementing technology solutions in line with best practices and to agreed timeframes.
Engaging with Architects, Consultants, Infrastructure and Security Specialists to provide a key link between the technical needs of the business and the solution design.
Participating in customer's Engineering communities, and providing constructive feedback to platform teams
Thank you for your interest in applying for this position with TCS. All applicants who meet the requirements will be considered equally and fairly for employment. We will review your application and will get back to you if we are considering your interest in this opportunity
Show more
Show less","AWS, S3, Glue, Dynamo, Lambda, Kafka, PySpark, Aurora, Iceberg, Jenkins, Terraform, Sonarqube, Java, RESTful APIs","aws, s3, glue, dynamo, lambda, kafka, pyspark, aurora, iceberg, jenkins, terraform, sonarqube, java, restful apis","aurora, aws, dynamo, glue, iceberg, java, jenkins, kafka, lambda, restful apis, s3, sonarqube, spark, terraform"
Senior Data Engineer,Quantexa,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-quantexa-3751416426,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Description
Founded in 2016 with only a handful of individuals, Quantexa was built with a purpose that through a greater understanding of context, better decisions can be made. 7 years, 15 locations and 650+ employees later we still believe that today. We connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data.
Due to the continuous success and high demand from our customers, we are looking for a Senior Data Engineer with a proven track record to join the Quantexa family. 🚀
What does a Senior Data Engineer role at Quantexa look like?
In order to be a successful Senior Data Engineer at Quantexa, you’ll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.
Being Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. You’ll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service.
We want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala, Data Fusion and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but don’t worry If that’s not your strongest language or if you haven’t used it before, we make sure that every Quantexan goes through our training academy so they’re comfortable and confident with using our platform.
Requirements
What do I need to have?
We’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.
The desire to learn and code in Scala
Experience in working in an Agile environment
Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.
A strong coding background in either Java, Python or Scala
Experience of building data processing pipelines for use in production “hands off” batch systems, including either traditional ETL pipelines and/or analytics pipelines.
Passion and drive to grow within one of the UK’s fastest growing scale-ups.
Consulting or business facing skills and a desire to work with customers.
Benefits
Why join Quantexa?
We know that just having an excellent glass door rating isn’t enough, so we’ve put together a competitive package as a way of saying thank you for all your hard work and dedication.
We Offer
Competitive salary 💰
Company bonus
11% Superannuation
Private healthcare with Medibank
Dental, Orthodontics & Optics Cover
Free Calm App Subscription #1 app for meditation, relaxation and sleep 🧘‍♀️
20 days annual leave, 13 national holidays + birthday off! 🌴
Ongoing personal development
Great WeWork Office Space & Company wide socials
Our mission
We have one mission. To help businesses grow. To make data easier. And to make the world a better place. We’re not a start-up. Not anymore. But we’ve not been around that long either. What we are is a collection of bright, passionate minds harnessing complexities and helping our clients and their communities. One culture, made of many. Heading in one direction – the future.
It's All About You
Quantexa is proud to be an Equal Opportunity Employer. We’re dedicated to creating an inclusive and diverse work environment, where everyone feels welcome, valued, and respected. We want to hear from people who are passionate about their work and align with our values. Qualified applications will receive consideration for employment without regard to their race, colour, ancestry, religion, national origin, sex, sexual orientation, gender identity, age, citizenship, marital, disability, or veteran status. Whoever you are, if you’re a curious, caring, and authentic human being who wants to help push the boundaries of what’s possible, we want to hear from you.
Internal pay equity across departments is crucial to our global compensation philosophy. Grade level and salary ranges are determined through interviews and a review of experience, education, training, knowledge, skills, and abilities of the applicant, equity with other team members, and alignment with market data.
Quantexa is committed to providing reasonable accommodations in our talent acquisition processes. If you require support, please inform our Talent Acquisition Team.
Show more
Show less","Apache Spark, Apache Hadoop, Scala, Data Fusion, Elasticsearch, Google Cloud Platform, Data Processing Pipelines, ETL Pipelines, Agile, Scrum, Business Analytics, Data Science, Java, Python, SQL, NoSQL, Big Data","apache spark, apache hadoop, scala, data fusion, elasticsearch, google cloud platform, data processing pipelines, etl pipelines, agile, scrum, business analytics, data science, java, python, sql, nosql, big data","agile, apache hadoop, apache spark, big data, business analytics, data fusion, data processing pipelines, data science, elasticsearch, etl pipelines, google cloud platform, java, nosql, python, scala, scrum, sql"
Data Engineer x 2 positions available,The Recruitment Company Pty Ltd,Greater Sydney Area,https://au.linkedin.com/jobs/view/data-engineer-x-2-positions-available-at-the-recruitment-company-pty-ltd-3777814658,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"The Opportunity...
Unique role for an experienced
Data Engineer
with a desire to gain more exposure to Snowflake and Azure Factory
You will report directly into the Data Engineering manager partner with the Data Lead on designing and building new data products.
This role represents a great opportunity to learn about data architecture while remaining hands on with Data Engineering.
Make your job work around you - the business is all about putting yourself and your mental health first - work from home and work flexible hours.
Work just 1 day in the Sydney office - the rest can be office or remote. Super flexible! Applicants must be in NSW
Experience Required
Strong understanding of SQL, with the ability to write complex SQL queries to extract and manipulate data
Experience with data modelling and ETL: Able to design and implement data models and ETL processes to extract, transform, and load data from various sources.
Solid commercial experience working with Cloud tech Azure is desirable but will consider AWS or GCP too
Bonus Skills
Snowflake - Big Bonus!
Azure Data Factory
Architecture experience
Consulting experience
You
Good communication skills
You enjoy working as part of a collaborative, solutions focused team.
You have strong attention to detail but can still see the big picture.
Friendly and not too serious!
This is an excellent opportunity to join a friendly and stable organization- in a growth period.
To be considered please APPLY!! Interviews next week.
Or for a confidential discussion please text Anna-Maria on 0426 503 822.
Candidates must be based in NSW with Citizenship or PR.
Show more
Show less","SQL, Data modeling, ETL, Data architecture, Data products, Snowflake, Azure Data Factory, Azure, AWS, GCP","sql, data modeling, etl, data architecture, data products, snowflake, azure data factory, azure, aws, gcp","aws, azure, azure data factory, data architecture, data products, datamodeling, etl, gcp, snowflake, sql"
Senior Data Engineer,Simplyai,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-simplyai-3737498474,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Senior Data Engineers
Simplyai is working with a number a customers in Australia delivering advanced data solutions using the latest technologies.
We have a number of projects where we are looking to hire a number of new Data Engineers to deliver these projects. We are happy to consider both contractors and people looking for a permanent role with Simplyai.
Preferred Skills in: AWS, Azure, Snowflake, Databricks, SQL, Informatica, Python, DBT, Fivetran, Coalesce, PySpark, Kafka, Glue, Apache Iceberg, MongoDB, etc
Show more
Show less","AWS, Azure, Snowflake, Databricks, SQL, Informatica, Python, DBT, Fivetran, Coalesce, PySpark, Kafka, Glue, Apache Iceberg, MongoDB","aws, azure, snowflake, databricks, sql, informatica, python, dbt, fivetran, coalesce, pyspark, kafka, glue, apache iceberg, mongodb","apache iceberg, aws, azure, coalesce, databricks, dbt, fivetran, glue, informatica, kafka, mongodb, python, snowflake, spark, sql"
Senior Data Engineer,ALOIS Solutions,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-alois-solutions-3781855833,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Role:
Senior Data Engineer
Job Type:
Contract
Location:
Sydney
Job Description:
12+ Years Data Engineering Experience.
Completed Data Engineering Professional certification.
Minimum 2-3 projects delivered with hands-on experience in Databricks.
Completed Apache Spark Programming with Databricks Data Engineering with Databricks Optimizing Apache Spark™ on Databricks.
Experience in Spark and/or Hadoop Flink Presto other popular big data engines.
Familiarity with Databricks multi-hop pipeline architecture.
Show more
Show less","Data Engineering, Databricks, Apache Spark, Hadoop, Flink, Presto, Big data engines, Multihop pipeline architecture","data engineering, databricks, apache spark, hadoop, flink, presto, big data engines, multihop pipeline architecture","apache spark, big data engines, data engineering, databricks, flink, hadoop, multihop pipeline architecture, presto"
Senior Data Engineer,Net2Source Inc.,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-net2source-inc-3783626493,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Job description:
Here you go:
· 12+ Years Data Engineering Experience
· Completed Data Engineering Professional certification
· Minimum 2-3 projects delivered with hands-on experience in Databricks
· Completed Apache Spark Programming with Databricks Data Engineering with Databricks Optimizing Apache Spark™ on Databricks
· Experience in Spark and/or Hadoop Flink Presto other popular big data engines
· Familiarity with Databricks multi-hop pipeline architecture.
Show more
Show less","Data Engineering, Databricks, Apache Spark, Spark Programming, Apache Spark Optimizing, Hadoop, Flink, Presto, Big data engines, Multihop pipeline architecture","data engineering, databricks, apache spark, spark programming, apache spark optimizing, hadoop, flink, presto, big data engines, multihop pipeline architecture","apache spark, apache spark optimizing, big data engines, data engineering, databricks, flink, hadoop, multihop pipeline architecture, presto, spark programming"
Senior Data Engineer,Atlassian,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-atlassian-3774342291,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Overview
Atlassian is looking for a Senior Data Engineer to join our Data Experience Enablement team. This team owns the customer facing data model available to Atlassian customers in the Atlassian Analytics product offering. Building a high quality, easy to use, and insightful data model for our Enterprise customers involves ingesting, modeling, and enhancing the data in the Atlassian Data Lake.
That’s where you come in. We’re looking for a senior data engineer with experience using data pipelines, analysis, and modeling to help us expand and improve our customer facing data offerings. You’ll be working in a team of software and data engineers within the larger Analytics and Visualization Platform group. We are excited to have you join our team!
Responsibilities
Launch new data capabilities to Atlassian customers through our Atlassian Analytics product suite.
Build tooling to empower the business to launch new data capabilities faster and easier.
Collaborate with teams across Atlassian to build high quality data models for our customers.
Work alongside a team of engineers, contributing to the team culture through your passion, creativity, and experience.
Learn and grow as an engineer through various career development opportunities.
Qualifications
BS in Computer Science or equivalent experience with 5+ years of data engineering experience
Ability to lead small projects involving multiple team members from start to finish
Fluency in variety of programming languages including Python
SQL skills that enable advanced analysis and data modeling
Experience with data pipeline tooling and warehouses, specifically DBT, AWS data services (Redshift, Athena, EMR), and Apache projects (Spark, Flink, Hive, and Kafka)
Familiarity with modern software development practices (Agile, TDD, CICD) applied to data engineering
Understanding of cloud environments such as AWS or GCP
Show more
Show less","Data Engineering, Data Modeling, Data Analysis, Data Pipelines, Python, SQL, DBT, AWS Data Services (Redshift Athena EMR), Apache Projects (Spark Flink Hive Kafka), Agile, TDD, CICD, Cloud Environments (AWS GCP)","data engineering, data modeling, data analysis, data pipelines, python, sql, dbt, aws data services redshift athena emr, apache projects spark flink hive kafka, agile, tdd, cicd, cloud environments aws gcp","agile, apache projects spark flink hive kafka, aws data services redshift athena emr, cicd, cloud environments aws gcp, data engineering, dataanalytics, datamodeling, datapipeline, dbt, python, sql, tdd"
Senior Data Engineer,Atlassian,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-atlassian-3782246669,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Overview
Our Data Engineering Team is comprised of data experts. We build world-class data solutions and applications that power crucial business decisions throughout the organisation. We manage multiple analytical data models and pipelines across Atlassian, covering finance, growth, product analysis, customer analysis, sales and marketing, and so on. We maintain Atlassian's data lake that provide a unified way of analysing our customers, our products, our operations, and the interactions among them.
We're hiring a Senior Data Engineer, reporting to the Data Engineering Manager based in Sydney. Here, you'll enable a world-class engineering practice, drive the approach with which we use data, develop backend systems and data models to serve the needs of insights, and help build Atlassian's data-driven culture. You love thinking about the ways the business can consume data and then figuring out how to build it.
Responsibilities
You'll partner with the product analytics and data scientist team to build the data solutions that allow them to obtain more insights from our data and use that to support important business decisions.
You'll work with different stakeholders to understand their needs and architect/build the data models, data acquisition/ingestion processes and data applications to address those requirements.
You'll add new sources, code business rules, and produce new metrics that support the product analysts and data scientists.
You'll be the data domain expert who understand all the nitty-gritty of our products.
You'll own a problem end-to-end. Requirements could be vague, and iterations will be rapid
You'll improve data quality by using & improving internal tools/frameworks to automatically detect DQ issues.
Qualifications
A BS in Computer Science or equivalent experience with 5+ years of professional experience as a Sr. Data Engineer or in a similar role.
Strong programming skills using Python
Working knowledge of relational databases and query authoring (SQL).
Experience designing data models for optimal storage and retrieval to meet product and business requirements.
Experience building scalable data pipelines using Spark (SparkSQL) with Airflow scheduler/executor framework or similar scheduling tools.
Experience working with AWS data services or similar Apache projects (Spark, Flink, Hive, and Kafka).
Understanding of Data Engineering tools/frameworks and standards to improve the productivity and quality of output for Data Engineers across the team.
Well versed in modern software development practices (Agile, TDD, CICD)
Show more
Show less","Python, SQL, Data Engineering, Data Analysis, Data Modeling, Data Lakes, Data Pipelines, Spark (SparkSQL), Airflow scheduler/executor framework, AWS data services, Apache projects (Spark Flink Hive and Kafka), Agile, TDD, CICD, Data quality, Data governance, Data analysis tools, Data visualization tools, Cloud computing, Software development, Project management","python, sql, data engineering, data analysis, data modeling, data lakes, data pipelines, spark sparksql, airflow schedulerexecutor framework, aws data services, apache projects spark flink hive and kafka, agile, tdd, cicd, data quality, data governance, data analysis tools, data visualization tools, cloud computing, software development, project management","agile, airflow schedulerexecutor framework, apache projects spark flink hive and kafka, aws data services, cicd, cloud computing, data analysis tools, data engineering, data governance, data lakes, data quality, data visualization tools, dataanalytics, datamodeling, datapipeline, project management, python, software development, spark sparksql, sql, tdd"
*Data Engineers - Databricks*,Accenture Australia,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineers-databricks-at-accenture-australia-3775607805,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Accenture
is a global professional services company with leading capabilities in digital, cloud and security. Combining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Interactive, Technology and Operations services — all powered by the world’s largest network of Advanced Technology and Intelligent Operations centers. Our 738,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. We embrace the power of change to create value and shared success for our clients, people, shareholders, partners and communities. Learn more about how we are measuring value in all directions with Accenture's 360 degree Value Integrated Reporting.
About you:
As a Databricks Data Engineer, you will play a key role in designing, implementing, and maintaining data pipelines on the Databricks platform that support business and technology objectives. Your primary focus will be on building scalable and efficient data ingestion, transformation, and processing solutions to support our data-driven initiatives. You enjoy working both autonomously and as part of a team and have the confidence to influence and communicate with stakeholders at all levels, and to work in a fast-paced complex environment.
About the role:
You will be working on client projects with teams from across our Modern Data Platform and Applied Intelligence practice alongside our industry, functional and technology SMEs on some of our clients most challenging projects. From day one, you will be involved in the design and implementation of complex data solutions ranging from batch to streaming and event-driven architectures, across cloud, on-premise and hybrid client technology landscapes. We are looking for 3+ years of experience in data engineering in a customer or business facing capacity and experience in the following:
Preferred
Ability to understand and articulate requirements to technical and non-technical audiences
Stakeholder management and communication skills, including prioritising, problem solving and interpersonal relationship building
Experiences with design, develop, and implement end-to-end data engineering solutions using Databricks for large-scale data processing and data integration projects.
Build and optimize data ingestion processes from various sources, ensuring data quality, reliability, and scalability.
Perform data transformation tasks, including data cleansing, aggregation, enrichment, and normalization, using Databricks and related technologies.
Monitor and troubleshoot data pipelines, identifying and resolving performance issues, data quality problems, and other technical challenges.
Implement best practices for data governance, data security, and data privacy within the Databricks environment.
Strong SQL, Python, PySpark knowledge
Collaborate with DevOps and infrastructure teams to optimize the performance and scalability of Databricks clusters and resources.
Provide guidance and mentorship to junior data engineers, fostering a culture of knowledge sharing and continuous learning within the team.
Desirable
Knowledge in DataOps and experience in delivering CI/CD and DevOps capabilities in a data environment
Experience with advanced analytics and machine learning frameworks such as Apache Spark MLlib, TensorFlow, or PyTorch.
Proficiency in data visualization tools such as Tableau, Power BI, or Looker
Certification in Databricks Engineer Professional would be a plus.
Location
: Sydney, Brisbane, Melbourne
Equal Employment Opportunity Statement for Australia:
At Accenture, we recognise that our people are multi-dimensional, and we create a work environment where all people feel like they can bring their authentic selves to work, every day.
Our unwavering commitment to inclusion and diversity unleashes innovation and creates a culture where everyone feels they have equal opportunity. Our range of progressive policies support flexibility in ‘where’, ‘when’ and ‘how’ our people work to ensure that Accenture is an organisation where you can strive for more, achieve great things and maintain the balance and wellbeing you need.
We encourage applications from all people, and we are committed to removing barriers to the recruitment process and employee lifecycle. All employment decisions shall be made without regard to age, disability status, ethnicity, gender, gender identity or expression, religion or sexual orientation and we do not tolerate discrimination. If you require adjustments to the recruitment process or have a preferred communication method, please email exectalent@accenture.com and cite the relevant Job Number, or contact us on +61 2 9005 5000.
To ensure our workplace is inclusive and diverse we are setting bold goals and taking comprehensive action. To achieve these goals, we collect information that allows us to track the effectiveness of our Inclusion and Diversity programs. Learn how Accenture protects your personal data and know your rights in relation to your personal data. Read more about our Privacy Statement.
n/a
Show more
Show less","Databricks, SQL, Python, PySpark, Tableau, Power BI, Looker, Apache Spark MLlib, TensorFlow, PyTorch","databricks, sql, python, pyspark, tableau, power bi, looker, apache spark mllib, tensorflow, pytorch","apache spark mllib, databricks, looker, powerbi, python, pytorch, spark, sql, tableau, tensorflow"
GCP Data Engineer,Zone IT Solutions,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/gcp-data-engineer-at-zone-it-solutions-3769063981,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"We are looking for GCP Data Engineer. You will be a part of a Global Consulting Company for one of their prestigious clients. This is a Permanent Role based in Sydney Location.
Requirements
5+ years of experience working as a systems automation engineer with strong Linux experience and having responsibilities of Site Reliability Engineer
Resource should have excellent knowledge and experience in Datastage (preferably in 11x),Able to design and code complex Datastage jobs
Experience on Google Cloud Platform (primarily on Google Storage, BigQuery)
Experience with container security platforms like Aqua, Twistlock, etc.
Experience in SQL with Database knowledge (Vertica / Oracle / Teradata ) & DevOps (CICD)
Experience utilizing automation tools like Terraform & Ansible and monitoring using Grafana and Prometheus
Strong understanding of Security, Monitoring, and Performance aspects of cloud-native platform and application architectures
Provide and implement solutions on microservices to support custom functionalities
Great communicator and problem solver. Unconventional solutions and box-busting thinking should not deter you
Benefits
About Us
Zone IT Solutions is Australia based Recruitment Company. We specialize in Digital, ERP and larger IT Services. We offer flexible, efficient and collaborative solutions to any organization that requires IT, experts. Our agile, agnostic and flexible solutions will help you source the IT Expertise you need. Our delivery Offices are in Melbourne, Sydney and India. If you are looking for new opportunities your profile at Careers@zoneitsolutions.com or contact us at 0434189909
Also follow our LinkedIn page for new job opportunities and more.
Zone IT Solutions is an equal opportunity employer and our recruitment process focuses on essential skills and abilities. We welcome applicants from a diverse range of backgrounds, including Aboriginal and Torres Strait Islander peoples, people from culturally and linguistically diverse (CALD) backgrounds and people with disabilities.
Show more
Show less","Linux, Datastage, Google Cloud Platform, Google Storage, BigQuery, Aqua, Twistlock, SQL, Vertica, Oracle, Teradata, DevOps, CICD, Terraform, Ansible, Grafana, Prometheus, Microservices, Cloudnative platform, Application architectures","linux, datastage, google cloud platform, google storage, bigquery, aqua, twistlock, sql, vertica, oracle, teradata, devops, cicd, terraform, ansible, grafana, prometheus, microservices, cloudnative platform, application architectures","ansible, application architectures, aqua, bigquery, cicd, cloudnative platform, datastage, devops, google cloud platform, google storage, grafana, linux, microservices, oracle, prometheus, sql, teradata, terraform, twistlock, vertica"
Senior Data Engineer,endeavourX,"Surry Hills, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-endeavourx-3763153117,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Company Description
Experience the endeavourX difference
endeavourX brings the best of data, design and tech to make the world of drinks easier & more exciting, in-store and online. Our innovations enable personalised, digital experiences to empower memorable moments for our customers and team members across Australia, and our portfolio of brands including Dan Murphy’s, BWS, ALH Hotels, Pinnacle Drinks, Langton’s, Shorty’s and Jimmy Brings.
Job Description
You’re customer focused and passionate about being part of a tribe of great people and you actively contribute to your workplace community.
With a growth mindset, you orient around doing what’s meaningful, purposeful and delivers value. You’re excited about being part of a data-led transformation at one of Australia’s biggest companies.
You’re curious and empathetic. You have an eye for the bigger picture while also ensuring the details are in order. You deal with complexity by collaborating with colleagues, listening to customers, testing ideas and continuously learning. You take authorship for what you create.
Sound good? Read on.
Here is a taster of what you can expect in this role:
Analysis
Work with squad members and business SMEs to understand the requirements including business problem or opportunity, data feed/pipeline requirements, model design, features and/or reporting requirements
Contribute to documentation of requirements and user stories in Jira/Confluence, as required
Delivery
Build and deploy data pipelines required for the relevant initiative - could be to support analytics, reporting, machine learning, data migration and/or data archiving requirements
Document relevant information required for knowledge sharing across the team and for operational support handover
Knowledge sharing amongst the data engineering chapter and squad team members
Collaborate with team and stakeholders to ensure successful delivery
Contribute and adhere to technology standards, data governance & privacy policies and IT security standards
Adhere to corporate governance policies
Attend/contribute to squad agile ceremonies (e.g. standups, sprint planning etc)
Where required, support ML Engineers to implement and deploy scalable ML models and features (as designed by data scientists), adhering to enterprise CI/CD and devops practices
Where required, support ML Engineers to optimise execution performance (time and cost) of models
Operations
Contribute and assist with handover documentation to by used by operations team for new builds/enhancements
3rd level support for incidents relating to productionised data pipelines and/or ML models
Comply with the change process and provide inputs, as required, and attend CAB, where required
Qualifications
Must Have
Cloud data pipeline build, deploy and maintain/support skills
Python, DBT, Airflow and SQL skills
Experience with distributed programming (e.g. Apache Spark)
An in-depth understanding of common databases and data structures.
Back-end development skills, such as APIs
Experience working with big data cloud platforms (Azure, Google Cloud Platform, AWS)
Experience working in agile environment
DevOps and. CI/CD
Google Cloud Platform experience (Composer, Cloud Run, Cloud Functions, GCS, BigQuery, etc.)
Experience working with Personally Identifiable Information (PII) data
Desired
Databricks experience
Experience working with retail offline (e.g. store sales) and online data sets (e.g. ecommerce)
Experience working with unstructured data
Data science knowledge
Retail industry experience
Software engineering experience/skills
Experience with Infrastructure as code
Experience with Java and/or C#
Experience with Object-Oriented programming (OOP)
Experience with Unit Testing, TDD
Leadership and behaviours
Ability to think customer first and relate solutions back to the customer problem and outcomes
Enthusiastic and positive team member with a self-directed and self-motivated attitude
Comfortable with ambiguity
Excellent collaboration skills with an ability to see past problems to solutions
Able to work in a fast-paced environment
Effective communication skills to articulate technical concepts to both technical and non-technical stakeholders
Effective time management to prioritise workload
Ability to see the big picture
One team mindset
Strong commercial experience
The benefits are good too!
We offer flexible working in every sense
An exclusive discount card for BWS, Dan Murphy’s, Woolworths, BIG W and other Endeavour Group brands, including our ALH pubs
Monthly meeting-free days
Your health and wellbeing is your most important asset, and as one of our valued team members, it’s our first priority. You will have a range of free services to help you live well and support your physical, mental and financial wellbeing
Endeavour Group is full of opportunities - use our dedicated learning and development options to grow an idea, yourself, and your career. This is just the start, so dream big.
At Endeavour, we value being a workplace where everyone’s welcome - if you meet a number of the requirements (and not all), we encourage you to apply.
Additional Information
We are together creators
With a portfolio that includes Dan Murphy’s, BWS, ALH Hotels, Pinnacle Drinks and more, Endeavour Group is big on sociability. Together we create the moments that bring millions of people together. And together we have more fun, create more opportunities, and score a lot more goals. We’re serious about creating a safe, inclusive and fun place to rock up to where equal opportunity is key, and flexibility is part of how we roll.
We’re all about creating a more sociable future - for our customers and each other. If this job excites you - and you’re close-enough on the requirements, reach out, we’d love to hear from you.
You can learn more about working with us on LinkedIn or at endeavourgroupcareers.com.au.
Our Talent Team and Hiring Leaders kindly request no unsolicited resumes or approaches from Recruitment Agencies. Endeavour Group is not responsible for any fees related to unsolicited resumes.
#WeAreTogetherCreators #ComeAsYouAre #DreamBig #FeelTheEnergy #LeaveYourMark #EndeavourGroup
Show more
Show less","Cloud data pipeline, Python, DBT, Apache Airflow, SQL, Distributed programming, Apache Spark, Databases, Data structures, Backend development, APIs, Big data cloud platforms, Azure, Google Cloud Platform, AWS, Agile environment, DevOps, CI/CD, Google Cloud Platform, Composer, Cloud Run, Cloud Functions, GCS, BigQuery, Databricks, Retail offline data, Online data, Unstructured data, Data science, Retail industry, Software engineering, Infrastructure as code, Java, C#, ObjectOriented programming (OOP), Unit Testing, TDD, Leadership, Collaboration, Fastpaced environment, Communication, Time management, Commercial experience","cloud data pipeline, python, dbt, apache airflow, sql, distributed programming, apache spark, databases, data structures, backend development, apis, big data cloud platforms, azure, google cloud platform, aws, agile environment, devops, cicd, google cloud platform, composer, cloud run, cloud functions, gcs, bigquery, databricks, retail offline data, online data, unstructured data, data science, retail industry, software engineering, infrastructure as code, java, c, objectoriented programming oop, unit testing, tdd, leadership, collaboration, fastpaced environment, communication, time management, commercial experience","agile environment, apache airflow, apache spark, apis, aws, azure, backend development, big data cloud platforms, bigquery, c, cicd, cloud data pipeline, cloud functions, cloud run, collaboration, commercial experience, communication, composer, data science, data structures, databases, databricks, dbt, devops, distributed programming, fastpaced environment, gcs, google cloud platform, infrastructure as code, java, leadership, objectoriented programming oop, online data, python, retail industry, retail offline data, software engineering, sql, tdd, time management, unit testing, unstructured data"
*Data Engineers - Databricks*,Accenture Australia,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineers-databricks-at-accenture-australia-3775613018,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Accenture is a global professional services company with leading capabilities in digital, cloud and security. Combining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Interactive, Technology and Operations services — all powered by the world’s largest network of Advanced Technology and Intelligent Operations centers. Our 738,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. We embrace the power of change to create value and shared success for our clients, people, shareholders, partners and communities. Learn more about how we are measuring value in all directions with Accenture's 360 degree Value Integrated Reporting.
About you:
As a Databricks Data Engineer, you will play a key role in designing, implementing, and maintaining data pipelines on the Databricks platform that support business and technology objectives. Your primary focus will be on building scalable and efficient data ingestion, transformation, and processing solutions to support our data-driven initiatives. You enjoy working both autonomously and as part of a team and have the confidence to influence and communicate with stakeholders at all levels, and to work in a fast-paced complex environment.
About the role:
You will be working on client projects with teams from across our Modern Data Platform and Applied Intelligence practice alongside our industry, functional and technology SMEs on some of our clients most challenging projects. From day one, you will be involved in the design and implementation of complex data solutions ranging from batch to streaming and event-driven architectures, across cloud, on-premise and hybrid client technology landscapes. We are looking for 3+ years of experience in data engineering in a customer or business facing capacity and experience in the following:
Preferred
Ability to understand and articulate requirements to technical and non-technical audiences
Stakeholder management and communication skills, including prioritising, problem solving and interpersonal relationship building
Experiences with design, develop, and implement end-to-end data engineering solutions using Databricks for large-scale data processing and data integration projects.
Build and optimize data ingestion processes from various sources, ensuring data quality, reliability, and scalability.
Perform data transformation tasks, including data cleansing, aggregation, enrichment, and normalization, using Databricks and related technologies.
Monitor and troubleshoot data pipelines, identifying and resolving performance issues, data quality problems, and other technical challenges.
Implement best practices for data governance, data security, and data privacy within the Databricks environment.
Strong SQL, Python, PySpark knowledge
Collaborate with DevOps and infrastructure teams to optimize the performance and scalability of Databricks clusters and resources.
Provide guidance and mentorship to junior data engineers, fostering a culture of knowledge sharing and continuous learning within the team.
Desirable
Knowledge in DataOps and experience in delivering CI/CD and DevOps capabilities in a data environment
Experience with advanced analytics and machine learning frameworks such as Apache Spark MLlib, TensorFlow, or PyTorch.
Proficiency in data visualization tools such as Tableau, Power BI, or Looker
Certification in Databricks Engineer Professional would be a plus.
Equal Employment Opportunity Statement for Australia:
At Accenture, we recognise that our people are multi-dimensional, and we create a work environment where all people feel like they can bring their authentic selves to work, every day.
Our unwavering commitment to inclusion and diversity unleashes innovation and creates a culture where everyone feels they have equal opportunity. Our range of progressive policies support flexibility in ‘where’, ‘when’ and ‘how’ our people work to ensure that Accenture is an organisation where you can strive for more, achieve great things and maintain the balance and wellbeing you need.
We encourage applications from all people, and we are committed to removing barriers to the recruitment process and employee lifecycle. All employment decisions shall be made without regard to age, disability status, ethnicity, gender, gender identity or expression, religion or sexual orientation and we do not tolerate discrimination. If you require adjustments to the recruitment process or have a preferred communication method, please email exectalent@accenture.com and cite the relevant Job Number, or contact us on +61 2 9005 5000.
To ensure our workplace is inclusive and diverse we are setting bold goals and taking comprehensive action. To achieve these goals, we collect information that allows us to track the effectiveness of our Inclusion and Diversity programs. Learn how Accenture protects your personal data and know your rights in relation to your personal data. Read more about our Privacy Statement.
n/a
Show more
Show less","Databricks, Data engineering, Data pipelines, Scalable, Efficient, Data ingestion, Data transformation, Data processing, SQL, Python, PySpark, DataOps, CI/CD, DevOps, Apache Spark MLlib, TensorFlow, PyTorch, Tableau, Power BI, Looker","databricks, data engineering, data pipelines, scalable, efficient, data ingestion, data transformation, data processing, sql, python, pyspark, dataops, cicd, devops, apache spark mllib, tensorflow, pytorch, tableau, power bi, looker","apache spark mllib, cicd, data engineering, data ingestion, data processing, data transformation, databricks, dataops, datapipeline, devops, efficient, looker, powerbi, python, pytorch, scalable, spark, sql, tableau, tensorflow"
Senior Data Engineer,Morgan McKinley,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-morgan-mckinley-3693896505,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"A leading Australian company are seeking to add a Senior Data Engineer to their team as they continue their real time data journey. You will be joining a well know ASX listed organisation who have an award winning culture as well as a praised learning and development scheme. In addition to working in a hybrid environment with a high end office, there is also a range of market leading perks on offer.
The ideal candidate will have
Strong data engineering skills with ideally at least 5 years of experience
Hands on cloud computing experience in this instance AWS (essential)
Possess end to end data pipelines experience
Strong communication skills, with the ability to break down complex technical elements to non technical senior stakeholders
A passion for creating and maintaining best in class data practices in a cloud environment
End to end Software development lifecycle expertise (requirements, design, architecture, development, testing, deployment, release and support)
Experience with large scale datasets , data lake and data warehouse technologies and strong on Snowflake
Strong experience in server-less technologies, and you are expected to use that to build highly scalable data platforms.
Experience with CI/CD
If this sounds like the right role for you please apply now. Please note only candidates with PR or citizenship and based in Sydney can be considered.
Recruitment consultant: Leon Young
Show more
Show less","Data Engineering, Cloud Computing (AWS), Data Pipelines, Communication Skills, Data Practices, Software Development Lifecycle, Snowflake, Data Lake, Data Warehouse, Serverless Technologies, CI/CD","data engineering, cloud computing aws, data pipelines, communication skills, data practices, software development lifecycle, snowflake, data lake, data warehouse, serverless technologies, cicd","cicd, cloud computing aws, communication skills, data engineering, data lake, data practices, datapipeline, datawarehouse, serverless technologies, snowflake, software development lifecycle"
Senior Data Engineer - AWS - $140k,Robert Walters,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-aws-%24140k-at-robert-walters-3770906482,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Drive the technical vision for existing data assets and complex projects and empower and guide a high performing, collaborative team.
Responsibilities
Build data assets that are aligned to DET strategy and architectural roadmaps
Promote and champion Data Platform and Engineering ways of working via continuous build, continuous integration and continuous delivery
Lead detailed solution design, data modelling, estimation and specification process
Promote test driven development and automated unit test frameworks within the teams
Promote and champion Automated Deployment via Jenkins
Coordination of deployment and release activities for the team
Challenge the development team on their delivery of quality, unit test coverage and assistance in the creation of development artefacts
Contribute to risk analysis and utilise that information to plan development activities
Identify technical and process improvements that will benefit the business
Optimising code and ensuring we are getting the best out of the Data Platform
Key Requirements
Tertiary qualification in Software Engineering, Computer Science, or Information Technology
Minimum 6 years’ experience within Data Engineering with minimum 2 years in a similar role
Experience supporting high availability production systems is highly desirable
Strong SQL skills and experience in Data Warehousing
Experience developing data assets and optimising data pipelines in Massively Parallel Processing (MPP) Databases (Preferably Redshift)
Strong skills and experience in cloud environments, especially AWS
Comfortable working within conceptual, logical and physical data model following Inmon, Kimball or data vault patterns
Experience in data migration, building, enhancing and maintaining cloud data warehousing
Strong understanding of BI data structures and how to optimise data delivery for different BI Reporting tools (Tableau & Cognos)
Ability to analyse data and source system to identify root cause and ability on problem solving
Ability to rapidly acquire an understanding of complex business problems/requirements to develop solutions and designs, regardless of existing areas of expertise or specialisation
Experience using ETL / ELT is preferable but not mandatory
Aboriginal and Torres Strait Islander Peoples are encouraged to apply.
To apply please click apply or call Joe Spencer-Lewis on 02 8289 3148 for a confidential discussion.
Show more
Show less","Data Engineering, Software Engineering, Computer Science, Information Technology, Data Warehousing, Massively Parallel Processing (MPP) Databases, Amazon Web Services (AWS), SQL, Redshift, Data Migration, Cloud Data Warehousing, Business Intelligence (BI), Tableau, Cognos, ETL / ELT, Jenkins, Data Modeling, Data Pipelines, Data Structures, Prototyping, Unit Testing, Continuous Integration / Continuous Delivery, Risk Analysis","data engineering, software engineering, computer science, information technology, data warehousing, massively parallel processing mpp databases, amazon web services aws, sql, redshift, data migration, cloud data warehousing, business intelligence bi, tableau, cognos, etl elt, jenkins, data modeling, data pipelines, data structures, prototyping, unit testing, continuous integration continuous delivery, risk analysis","amazon web services aws, business intelligence bi, cloud data warehousing, cognos, computer science, continuous integration continuous delivery, data engineering, data migration, data structures, datamodeling, datapipeline, datawarehouse, etl elt, information technology, jenkins, massively parallel processing mpp databases, prototyping, redshift, risk analysis, software engineering, sql, tableau, unit testing"
Senior Data Engineer,Woolworths Group,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-woolworths-group-3785922807,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Harness huge datasets and work on projects to create innovative retail tools
Experiment with new ideas, taking a test and learn approach to your work
Work collaboratively with other data professionals in an Agile squad setup
We strive to create better experiences together for a better tomorrow for our customers and team, bringing the best of Woolworths Group and Quantium together to make data driven decisions. By partnering with Quantium, a world-class data science and advanced analytics business we’re excited about the future as we unlock the power of data to reimagine retail.
Through this partnership, we’ve created a new business called wiq who will harness the best of Woolworths Group and Quantium to reimagine retail by solving the most complex problems using data and analytics. As a team, we leverage Agile ways of working to help our team develop their skills while also making sure our team is focusing on solving the most complex problems for both our customers and team.
What You’ll Do
Design, develop, implement, scale and maintain Advanced Analytics solutions using predominantly Google Cloud (GCP) products. This includes for example building data pipelines, logging and monitoring. This work is done in multidisciplinary teams in close partnership with Data Scientists, with engineers having the primary responsibility for productionising and scaling solutions and developing cutting edge retail tools.
Develop Cloud based data pipelines to solve a broad range of business problems.
Grow the Engineering Guild capability through pioneering new tech and approaches to improve time to market, reliability and performance of Data Science applications
Create, present and seek inputs and feedback on architecture, solution designs and foster alignment and support for solutions
Apply relevant testing frameworks to ensure the robustness of solutions
Communicate analysis, insights, solution designs and showcase working solutions
What You’ll Bring
We are looking for an experienced Senior Data Engineer, with a problem solving mindset, able to take sometimes nebulous and abstract challenges, break it down into components and develop solution hypotheses for a range of business problems across Woolworths’ ecosystem.
Bachelor or above degree in Computer Science, Engineering or related field
Solid Python and SQL; ideally experience with Google Cloud or other cloud technologies
Experience building data pipelines with airflow or other orchestration tools preferred
Proven experience in Data Engineering is essential
Ability to mentor, coach and help to develop more junior members of the engineering team
What You’ll Experience
Our Team Members are at the heart of everything we do and we’re always looking for ways to support your career journey and reward great work:
Team discounts across our range of Woolworths Group brands you know and love and a robust rewards program that celebrates and incentivises purpose-driven work.
A global business with endless career possibilities around every corner and across every discipline – with valuable exposure to a vast and exciting business network.
High impact role supporting our teams who enrich our communities.
A range of programs to help you prioritise and manage your wellbeing, including 24/7 access to the Sonder app.
A progressive and competitive leave policy that gives you more space for what matters to you.
Endless possibilities with Woolworths Group
We’re a proud part of the Woolworths Group – 200,000+ bright minds, passionate hearts and unique perspectives working together connected by a shared Purpose – ‘to create better experiences together for a better tomorrow.’
Here you’ll be supported and empowered to explore new ideas, make brave commitments and innovate better ways to meet the food and everyday needs of more than 24 million customers every week. Backed by a culture of genuine care, a flexible approach to work and the support to grow your career and make a meaningful impact, the possibilities for what we can achieve together are endless.
Everyone belongs at Woolworths Group
Diversity, equity, inclusion, and belonging are key to realising our purpose of better together for a better tomorrow. We recognise the value our team’s diversity brings to our business, customers, and communities and that teams with diverse experiences and backgrounds enrich our group and are better able to innovate and solve problems. As one of the largest employers in Australia and New Zealand, we aim to create a truly inclusive workplace where everyone feels that they belong, can be their best selves, and reach their full potential.
We encourage all candidates to apply; please let us know in your application if we can support you with any adjustments in the hiring process.
You can learn more about working with us on LinkedIn or via www.wowcareers.com.au.  #work180. #Hybrid
Our Talent Acquisition Team and Hiring Leaders kindly request no unsolicited resumes or approaches from Recruitment Agencies. Woolworths Group is not responsible for any fees related to unsolicited resumes.
Show more
Show less","Google Cloud, Python, SQL, Data pipelines, Airflow, Data engineering, Agile, Cloud computing, Data analytics, Data orchestration, Testing frameworks, Solution design, Mentoring, Coaching","google cloud, python, sql, data pipelines, airflow, data engineering, agile, cloud computing, data analytics, data orchestration, testing frameworks, solution design, mentoring, coaching","agile, airflow, cloud computing, coaching, data engineering, data orchestration, dataanalytics, datapipeline, google cloud, mentoring, python, solution design, sql, testing frameworks"
Senior Data Engineer,Woolworths Group,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-woolworths-group-3774259777,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Senior Data Engineer - Splunk
Bring your SPLUNK expertise to be the SME in the team.
Hybrid working and flexibility to set you up for success
Make a big impact in a critical role
We are Woolworths Group
We are Woolworths Group. 200,000+ bright minds, passionate hearts and unique perspectives connected by a shared Purpose – ‘to create better experiences together for a better tomorrow.’ It’s that Purpose that fuels our ambition to explore new ideas, make brave commitments and innovate better ways to meet the food and everyday needs of more than 24 million customers every week.
If you’re excited to turn today’s blue sky hinking into a better tomorrow for future generations, you’ll find yourself supported and enriched in an dynamic, inclusive and empowering workplace that reflects the diverse communities we serve. With a culture of genuine care, a flexible approach to work and opportunities across the group to grow your career and make a meaningful impact, the possibilities for what we can achieve together are endless.
Welcome to Group Enablement
Our Group Enablement Teams are the Technology, Business Enablement, Value Chain, and Replenishment experts developing new capabilities and platforms for a better retail future.
We’re the crucial link between our stores and the stock they need, the IT engine moving our business forward, the strategic connectors streamlining the way we work, and the forward thinkers using tech to transform the retail experience.
Uniting cutting edge technology, data, and retail smarts, you’ll work behind the scenes, in the margins, and across every corner of the business – to uplift our capability, amplify our collective impacts, and revolutionise the customer experience.
What You'll Do
As the Senior Data Engineer, you will be providing expert oversight and maintenance of a wide variety of knowledge objects & data that power our monitoring and operational insights platform. This includes products like the Splunk Cloud & Splunk Observability Suite. You will play a Senior role as subject matter expert given the specialised nature of the role
Provide data architecture expertise to the users of Splunk and support analytics business functions
Provide expertise and a specialised Splunk related focus on enterprise-level machine data analytics, while supporting mission-critical information systems
Ensuring compliance with organisational policies and procedures and overall data/ information management and analytics
strategy.
Contribute to the ongoing development and growth of the monitoring discipline
Deliver cost-effective, quality, innovative technology solutions and services that support business strategy.
What You’ll Bring
You will bring a growth mindset and have proven capabilities to deliver in a virtual agile environment. You will have demonstrated experience managing relationships and be able to influence within matrix organisation,across a range of stakeholders.
You will be a curious, self starter, always looking to learn with strong problem solving skills.
Broad knowledge and confidence in many data and automation applications and tools.
10+ years experience developing and operating applications and tools in a large enterprise environment.
Extensive experience working with Splunk - Splunk Cloud, Splunk ITSI, Splunk ES
Experience working with Big Data platforms, complex data integrations and cloud platforms such as Azure & GCP
Experience with process definition, optimisation and automation
What You’ll Experience
Our Team Members are at the heart of everything we do and we’re always looking for ways to support your career journey and reward great work:
Team discounts across our range of Woolworths Group brand you know and love and a robust rewards program that celebrates and incentivises purpose-driven work.
A global business with endless career possibilities around every corner and across every discipline – with valuable exposure to a vast and exciting business network.
A range of programs to help you prioritise and manage your well-being, including 24/7 access to the Sonder app.
A progressive and competitive leave policy that gives you more space for what matters to you.
Everyone belongs at Woolworths Group
Diversity, equity, inclusion, and belonging are key to realising our purpose of better together for a better tomorrow. We recognise the value our team’s diversity brings to our business, customers, and communities and that teams with diverse experiences and backgrounds enrich our group and are better able to innovate and solve problems. As one of the largest employers in Australia and New Zealand, we aim to create a truly inclusive workplace where everyone feels that they belong, can be their best selves, and reach their full potential.
We encourage all candidates to apply; please let us know in your application if we can support you with any adjustments in the hiring process.
You can learn more about working with us on LinkedIn or via www.wowcareers.com.au.  #work180.
Our Talent Acquisition Team and Hiring Leaders kindly request no unsolicited resumes or approaches from Recruitment Agencies. Woolworths Group is not responsible for any fees related to unsolicited resumes.
Show more
Show less","Splunk, Splunk Cloud, Splunk Observability Suite, Enterpriselevel machine data analytics, Data architecture expertise, Data management and analytics strategy, Automation applications and tools, Big Data platforms, Complex data integrations, Cloud platforms, Azure, GCP, Process definition, Optimisation, Automation","splunk, splunk cloud, splunk observability suite, enterpriselevel machine data analytics, data architecture expertise, data management and analytics strategy, automation applications and tools, big data platforms, complex data integrations, cloud platforms, azure, gcp, process definition, optimisation, automation","automation, automation applications and tools, azure, big data platforms, cloud platforms, complex data integrations, data architecture expertise, data management and analytics strategy, enterpriselevel machine data analytics, gcp, optimisation, process definition, splunk, splunk cloud, splunk observability suite"
Principal Data Engineer,Workforce Australia for Individuals,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/principal-data-engineer-at-workforce-australia-for-individuals-3781321827,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"The State Insurance Regulatory Authority (SIRA) are currently looking for a Principal Data Engineer to join our team!
Benefits
1 x ongoing clerk grade 11/12 opportunity.
Salary range: $139,787 - $161,663 Read More... Click here to view more detail / apply for Principal Data Engineer
Show more
Show less",Principal Data Engineer,principal data engineer,principal data engineer
Senior Data Engineer,Correlate Resources,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-correlate-resources-3776261678,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"Join a fantastic team within a challenging environment, having access to cutting edge technology and continual career development and growth.
An inclusive, team-first company, our client is most known for their work in the retail and eCommerce space operating Australia wide.
About The Role
Design, develop, implement, scale and maintain Advanced Analytics solutions using predominantly Google Cloud (GCP) products. This includes for example building data pipelines, logging and monitoring. This work is done in multidisciplinary teams in close partnership with Data Scientists, with engineers having the primary responsibility for productionising and scaling solutions.
Key Responsibilities
Develop Cloud based data pipelines to solve a broad range of business problems.
Grow the Engineering Guild capability through pioneering new tech and approaches to improve time to market, reliability and performance of Data Science applications
Work collaboratively with Data Scientists, ML Engineers, Application Developers & Data Engineers to develop cutting edge retail tools in an agile manner
Create, present and seek inputs and feedback on architecture, solution designs and foster alignment and support for solutions
Apply relevant testing frameworks to ensure the robustness of solutions
Communicate analysis, insights, solution designs and showcase working solutions
Mentor, coach and help to develop more junior members of the engineering team
Who We Are Looking For
We are looking for an experienced Senior Data Engineer, with a problem solving mindset, able to take sometimes nebulous and abstract challenges, break it down into components and develop solution hypotheses for a range of business problems.
Requirements
Bachelor or above degree in Computer Science, Engineering or related field
Solid Python and SQL
Experience building data pipelines with airflow or other orchestration tools preferred
Experience with Google Cloud preferred
Proven experience in Data Engineering is essential
SALARY:
Up to 170k + Super
How To Apply
Please click on the 'Apply' button below or email your resume to kyle@correlateresources.com
Show more
Show less","Advanced Analytics, Google Cloud (GCP), Data Pipelines, Logging, Monitoring, Multidisciplinary Teams, Data Scientists, Agile, Architecture, Solution Designs, Testing Frameworks, Mentoring, Python, SQL, Airflow, Orchestration Tools","advanced analytics, google cloud gcp, data pipelines, logging, monitoring, multidisciplinary teams, data scientists, agile, architecture, solution designs, testing frameworks, mentoring, python, sql, airflow, orchestration tools","advanced analytics, agile, airflow, architecture, data scientists, datapipeline, google cloud gcp, logging, mentoring, monitoring, multidisciplinary teams, orchestration tools, python, solution designs, sql, testing frameworks"
Senior Data Engineer (Kafka),Talenza,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-kafka-at-talenza-3771070048,2023-12-17,New South Wales, Australia,Mid senior,Onsite,"$180,000 + Super
Sydney, NSW
Leading ASX-listed business based in Sydney, Australia, at the forefront of data and integration solutions are seeking a a Senior Data Engineer who will be responsible for taking ownership of their distributed streaming application suite, messaging, and integration components.
Key Responsibilities
Develop, configure, deploy, and operate distributed streaming platforms using Kafka in a hybrid environment (on-prem and cloud services).
Configure, deploy, and manage observability components for the streaming platform, utilizing tools such as Grafana and ELK, to enable operational reporting and capacity management.
Operate and maintain Kafka platform, TIBCO EMS, Java Springboot services, and BusinessWorks services.
Collaborate with business stakeholders and engage with project teams to deliver streaming and integration solutions.
Requirements
Key Requirements
Successfully deployed and operated Kafka platform in a Production environment.
Have experience in building, deploying and managing services in the cloud.
Have experience in managing messaging and integration platforms. - Kafka - admin support BAU operations/ Managing current Kafka cluster
GCP Cloud - Platform exp
Preferred Qualifications And Skills
Familiarity with monitoring and dashboarding tools such as Grafana, Telegraf, InfluxDB, and ELK.
Knowledge of trading systems or applied knowledge of financial industry systems.
Database experience, including MySQL and Oracle.
Proficiency in scripting languages such as Shell, Perl, and Python.
If you feel you possess the relevant skills to the above, apply now with most updated CV!
Show more
Show less","Kafka, TIBCO EMS, Java Springboot services, BusinessWorks services, Grafana, ELK, Telegraf, InfluxDB, MySQL, Oracle, Shell, Perl, Python, Cloud computing, Data streaming, Messaging, Integration, Observability, Capacity management, Distributed systems, Monitoring, Dashboarding, DevOps, Data engineering","kafka, tibco ems, java springboot services, businessworks services, grafana, elk, telegraf, influxdb, mysql, oracle, shell, perl, python, cloud computing, data streaming, messaging, integration, observability, capacity management, distributed systems, monitoring, dashboarding, devops, data engineering","businessworks services, capacity management, cloud computing, dashboard, data engineering, data streaming, devops, distributed systems, elk, grafana, influxdb, integration, java springboot services, kafka, messaging, monitoring, mysql, observability, oracle, perl, python, shell, telegraf, tibco ems"
Senior Data Engineer,Publift,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-publift-3782813405,2023-12-17,New South Wales, Australia,Mid senior,Remote,"Company Description
About us
We lift ad revenue for publishers with our award-winning, programmatic adtech and a team of awesome people.
At Publift, we maximise revenue through making advertising simple and easy for our customers. By letting our technology platform do the heavy lifting in the background, our publishers can experience enormous growth while focusing on what matters most to them; building incredible content.
Did we mention we were an award-winning business? Read below to learn about a few things we are extra proud of.
Consistent 40% YoY with current projections at the 100% YOY growth mark. Well done team!
Autonomy to choose - Office, Hybrid or Remote (from Australia and New Zealand)
Winners of The Drum APAC Best Sell-Side Programmatic Technology 2019
Deloitte 10th fastest-growing tech business in Australia 2019
We’re proud to be recognised as one of Australia’s Best Workplaces™ by Great Place to Work®, an AFR BOSS Best Place to Work, a Family Friendly Workplace, and WGEA Employer of Choice for Gender Equality.
The support of a highly engaged, high-performing team including a leadership team that you’ll actually have face time with.
18 weeks paid parental leave for primary caregivers, four weeks paid secondary caregivers leave, six weeks paid gender transition leave.
Regular Hackathons, endless learning and development opportunities, and a range of activities that will help support your mental, emotional, and physical wellbeing.
Job Description
What you will be doing
We're looking for a Senior Data Engineer to help build the infrastructure that powers the optimisation loop at the heart of our business: analytics and machine learning.
You'll be joining an awesome team of engineers and world-class ad-tech experts.
Building ETL pipelines (Airflow, Apache Beam, SQL, etc) to ingest data.
Designing and maintaining our data lake and associated infrastructure using Terraform.
Working with business and technical teams to interpret data.
Identifying data quality issues and proactively developing quality strategies.
Building pipelines for machine learning models.
Qualifications
What you bring to the role
Experience with cloud data engineering platforms. We use GCP, but Databricks, AWS, etc are all good experience.
Experience with the Python programming language: you are able to define pipelines in Python, and maintain simple webservices where required.
Ideally, you have experience deploying your pipelines on an Infrastructure-as-Code basis. We use Terraform, but CloudFormation/CDK are a good background.
Good understanding of SQL, ideally with some data warehousing/data lake experience.
Additional Information
A few other things we want to let you know about...
Publift is an equal opportunity employer who prides themselves on fostering a diverse and inclusive workplace. We encourage everyone of all ages, genders and backgrounds to apply. If you’re a candidate with a disability or you’ll need some adjustments to be at your best, let us know how we can provide you with additional support.
We believe work satisfaction comes from two things: working in an unbelievably great team and solving customer problems that matter.
Our flat organisation structure allows us to be nimble and innovative to drive both customer and personal growth.
We know how important keeping that balance between work/life is and have a range of flexible working options on offer, including part time. We are open to this conversation during our recruitment process if you want to know more just ask!
We are a proud Circle Back Initiative Employer which means we are committed to responding to every job applicant.
Successful candidates joining the publift are required to undergo a National Police Record check and must have full-time Australian working rights on an ongoing basis.
Show more
Show less","Python, Webservices, SQL, Data warehousing, Data lake, Data engineering, Cloud computing (GCP Databricks AWS), Machine learning, Airflow, Apache Beam, Terraform, CloudFormation, CDK, ETL","python, webservices, sql, data warehousing, data lake, data engineering, cloud computing gcp databricks aws, machine learning, airflow, apache beam, terraform, cloudformation, cdk, etl","airflow, apache beam, cdk, cloud computing gcp databricks aws, cloudformation, data engineering, data lake, datawarehouse, etl, machine learning, python, sql, terraform, webservices"
Senior Software Engineer (Data Visualisation),Neara,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-software-engineer-data-visualisation-at-neara-3767237066,2023-12-17,New South Wales, Australia,Mid senior,Remote,"Neara is a high-growth, venture-backed Series B, tech company headquartered in Sydney. We work with 75% of the utilities in Australia and New Zealand and are growing rapidly across the US and Europe. Our mission is to revolutionise the utilities industry by helping them future-proof their infrastructure and navigate the challenges of the clean energy transition. Our 3D digital network model enables utilities to simulate various scenarios and make informed decisions to protect their assets and communities.
We are seeking a Senior Software Engineer (Data Visualisation). You will play a key role in designing and implementing product features and services that leverage machine learning in our critical infrastructure, digital twin platform. You will collaborate closely with our product and engineering teams to enhance the platform's user interface and user experience. Additionally, as a senior member of our team, you will also mentor and guide other engineers in best practice.
Some of the things you will contribute to:
Designing and implementing innovative and efficient front end solutions that integrate machine learning capabilities into our digital twin platform
Collaborating with product managers and designers to create intuitive and visually appealing user interfaces
Ensuring the scalability, performance, and security of the front end codebase
Mentoring and guiding junior engineers, promoting best practices and maintaining high code quality standards
Staying up to date with the latest trends and advancements in front end development technologies and share your knowledge with the team
The essential experience you'll have:
Full stack software engineering background
Experience in gaming / graphics manipulation / 3D digital twin or geospatial mapping software development would be highly advantageous
Proficiency in JavaScript/TypeScript, HTML, CSS, and SASS plus a sound working knowledge of d3.js / three.js / other visualization framework
Some attributes we highly regard:
Contributing to complex technical projects or products
Strong problem-solving skills, algorithmic thinking, and understanding of data structures
Clear communicator and collaborator
Some of our technologies:
JavaScript/TypeScript, HTML, CSS, and SASS
React, Angular, or Vue
Swift /Kotlin/Java (For our mobile apps)
Carbon Design System, Tailwind, or Material UI
Three.js, Canvas, WebGL, Google Maps, and Tile APIs
OAuth 2.0, OWASP top ten, Mixpanel, and Figma
Some of the benefits we have on offer:
Competitive salary
Meaningful Equity
Hybrid Max Environment (work from office / home or a blend of the two). We have a fully stocked office (and impressive snack collection) in Redfern.
Regular office events
The real benefit is working on a genuinely complex, innovative and industry leading product making a genuine difference in the world around us.
To apply, please use the online application link below. Neara values diversity, belonging and equal employment opportunities. We encourage individuals from all backgrounds to apply. We will endeavour to give you an update on your application within 7-10 working days.
Please note, you will require current and unrestricted working rights to be considered for the role. Unfortunately we are not in a position to offer visa sponsorship at this time.
No agencies or third party service providers please **
Questions? Email talent@neara.com.
Show more
Show less","JavaScript, TypeScript, HTML, CSS, SASS, React, Angular, Vue, Swift, Kotlin, Java, Carbon Design System, Tailwind, Material UI, Three.js, Canvas, WebGL, Google Maps, Tile APIs, OAuth 2.0, OWASP top ten, Mixpanel, Figma, Machine learning, Frontend development, User interface design, User experience design, Data visualization, Software engineering, Agile development, Problemsolving, Algorithmic thinking, Data structures, Communication, Collaboration","javascript, typescript, html, css, sass, react, angular, vue, swift, kotlin, java, carbon design system, tailwind, material ui, threejs, canvas, webgl, google maps, tile apis, oauth 20, owasp top ten, mixpanel, figma, machine learning, frontend development, user interface design, user experience design, data visualization, software engineering, agile development, problemsolving, algorithmic thinking, data structures, communication, collaboration","agile development, algorithmic thinking, angular, canvas, carbon design system, collaboration, communication, css, data structures, figma, frontend development, google maps, html, java, javascript, kotlin, machine learning, material ui, mixpanel, oauth 20, owasp top ten, problemsolving, react, sass, software engineering, swift, tailwind, threejs, tile apis, typescript, user experience design, user interface design, visualization, vue, webgl"
Senior Data Analyst,IntelliPro,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-analyst-at-intellipro-3787739844,2023-12-17,New South Wales, Australia,Mid senior,Remote,"Core Requirements
Strong SQL (must-have)
Strong data analysis (must-have)
Support CN Hours (11am Beijing Time - 7pm Beijing Time) (must-have)
Python
Experience in data pipelining /ETL
Experience working with cross-functional stakeholders
3+ years of experience
Preferred
Bilingual proficiency in Mandarin and English is an advantage for international collaboration
Experience with A/B testing
Experience in Ecommerce
Proactive and independent
Other Requirements
Meet client compliance requirements
Powered by JazzHR
NjkOpV8F42
Show more
Show less","SQL, Data analysis, Python, Data pipelining, ETL, A/B Testing, Ecommerce","sql, data analysis, python, data pipelining, etl, ab testing, ecommerce","ab testing, dataanalytics, datapipeline, ecommerce, etl, python, sql"
Senior Data Engineer,TheDriveGroup.,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-thedrivegroup-3777020599,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"A rare opportunity to work as a Senior Data Engineer and lead internal and external projects.
As the Senior, you will have ownership of client relationships, as well as the opportunity to mentor the team as you build out the Data Engineering Consulting practice.
What you will be doing
Undertaking detailed analysis of data held in various formats and identification of data fields required for models
Data extraction and transformation ready for ingestion into customised models
Implementing and supporting ETL
Working directly with clients and other internal consultants, providing advisory and consulting services.
Working with various business and technical teams to interpret data and resolve data issues
Building and optimising SQL queries for various projects
Identifying data quality issues and designing suitable remediation strategies
What you bring to the role
Strong experience in Snowflake and Azure Cloud is essential.
Experience in data profiling, data analysis, and data mapping
Experience in Consulting and strong stakeholder engagement skills
Understanding of Data Engineering concepts, including ETL and DWH
Extensive SQL knowledge and experience
Experience implementing and supporting ETL
You will be joining a fast-growing sector of the business, that offers ongoing training and support, regular incentives, training courses, conferences and a host of other benefits. This will be a challenging role, however, you will be rewarded with a strong remuneration package, as well as a bonus reflecting the value you have added to the business.
Please note - candidates must be based in Sydney as you will be in the office 3 days per week with the other 2 WFH. Full and unrestricted working rights are also required for this role.
If you have any questions about this role or others we have available you can email our team directly via data@thedrivegroup.com.au
For The Latest Jobs, Tech News or to 'Introduce A Friend' for a $1000 Referral Fee - Follow Us On Linkedin:
https://www.linkedin.com/company/thedrivegroup
TheDriveGroup is 100% committed to improving meaningful diversity in the technology industry.
We partner with clients who embrace diversity and seek candidates across all backgrounds, abilities, genders, sexualities, cultures, and faiths.
If you have the right skills and attitude we would love to hear from you!
Show more
Show less","Data Engineering, Snowflake, Azure Cloud, Data profiling, Data analysis, Data mapping, Consulting, Stakeholder engagement, ETL, DWH, SQL, Data quality","data engineering, snowflake, azure cloud, data profiling, data analysis, data mapping, consulting, stakeholder engagement, etl, dwh, sql, data quality","azure cloud, consulting, data engineering, data mapping, data profiling, data quality, dataanalytics, dwh, etl, snowflake, sql, stakeholder engagement"
Data Engineer,luvo Talent,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-luvo-talent-3775345805,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"As a Data Engineer, you'll design, build and optimise Python data pipelines for ingestion, and identify trends and innovations in data platforms to inform the future development of our solutions. You'll have the opportunity to grow your responsibilities build your skillset with the support of our experienced Data Team, and contribute to the development of new tools, products and services.
What We Are Looking For
In-depth knowledge and experience building data pipelines in Python
Proven experience developing data pipelines that use unit-testing frameworks
Proven experience deploying data pipelines using common CI and CD techniques
Hands-on experience in building ETL/ELT solutions for large-scale data pipelines
Attention to detail with strong communication and problem-solving skills
Desirable skills include working knowledge of AWS data services, data platform concepts such as big data cloud storage, data warehouse processing, scheduling and monitoring of ETL/ELT jobs, and SQL skills within cloud architectures.
What We Do
Come and join a rapidly growing Tech company, primarily based out of Sydney with a platform that serves some of Australia's largest employers, enabling them to get their wages and employee entitlements right, quickly.
They are passionate about their products, innovating to do things better, and creating a work environment that values tenacity, respect, and a sense of fun.
If you have the skills and this sounds like a job for you APPLY or REFER TODAY!
Please note you must have full work rights (PR/Citizen) for Australia to be considered for this role and here in Sydney.
Show more
Show less","Python, Data pipelines, Data platforms, Unit testing, CI/CD, ETL/ELT, AWS data services, Big data, Cloud storage, Data warehouse, Monitoring","python, data pipelines, data platforms, unit testing, cicd, etlelt, aws data services, big data, cloud storage, data warehouse, monitoring","aws data services, big data, cicd, cloud storage, data platforms, datapipeline, datawarehouse, etlelt, monitoring, python, unit testing"
Data Engineer,Hays,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-hays-3777880621,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Are you looking for your next Data Engineering contract opportunity within wealth management?
This client has a wealth of knowledge in financial solutions and is looking to bring on a Data Engineer to join their highly engaged technology team.
You will be working as part of an engaging and highly focused team, collaborating to deliver onsite DWH to the new cloud based solution (Azure). You will also provide database design, analysis, visualisation skills utilising MS stack including but not limited to SSIS, SSRS, SQL, ADF, Synapses and Power BI.
What you'll need to succeed
SQL development and database administration, management and warehousing;
DWH migrations from SQL (SISS) to Azure Data Factory.
Reverse and forward engineer data assets.
Write, test, and validate data transformation and ETL pipelines development.
Ability to work within an agile environment collaborating and supporting each other.
What you need to do now
If you're interested in this role, click 'apply now' to forward an up-to-date copy of your CV, or call us now.
If this job isn't quite right for you, but you are looking for a new position, please contact us for a confidential discussion on your career.
Show more
Show less","Data Engineering, Azure, DWH, SSAS, SSIS, SSRS, SQL, ADF, Synapse, Power BI, ETL, Agile","data engineering, azure, dwh, ssas, ssis, ssrs, sql, adf, synapse, power bi, etl, agile","adf, agile, azure, data engineering, dwh, etl, powerbi, sql, ssas, ssis, ssrs, synapse"
Data Engineer,Coverforce,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-coverforce-3775355107,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"At Coverforce, we pride ourselves on our ability to build strong and genuine relationships with our client’s, we’ve been doing so for over 30 years and are continuing to grow.
The purpose of the role of Data engineer is to convert raw data into meaningful insights for organisational stakeholders. The ideal candidate will have a strong background in data engineering and analytics with excellent communication skills.
Key Responsibilities
Implement best practices in deployment, standards, security, business processes, and data.
Design data models using Kimball and Inmon methodologies.
Conduct data analysis to provide actionable insights.
Manage stakeholders effectively, with strong communication skills and the ability to convert technical language to functional language and vice versa
Skills and Experience
Data expertise: A solid grasp of data warehousing and data lake concepts, indicating familiarity with fundamental data storage and management principles.
Data Analysis and Modelling: Strong skills in data profiling, data analysis, and data modelling, indicative of a robust analytical foundation.
Database: Experience with SQL Server with the ability to improve the performance of SQL server inquiries and develop new inquiries. SSAS (tabular), and possibly DB.
Integration: Minimum of 5 years of experience with SSIS.
Reporting: Proficiency in Power BI (DAX), SSRS, and possibly Crystal Report
Qualifications
Bachelor’s degree in computer science, Information Systems, or a related field.
Proven experience as a Data Engineer or similar role.
Excellent problem-solving skills and attention to detail.
Strong communication and stakeholder management skills.
What you can expect
Attractive remuneration offering and benefits including our reward and recognition scheme with monthly prizes
Supportive team culture with ongoing professional development opportunities
Inclusive and friendly office environment, with regular activities to encourage engagement and interaction between different teams
WFH (after training) 2 days per week, work from our beautiful office at Barangaroo 3 days per week
Join us
Click APPLY to submit your application. Successful applicants will need to submit work rights, police and credit checks through our online credit provider as a condition of employment.
Please note:
Only candidates with valid work rights within Australia will be considered.
This role is being managed by Coverforce Talent & Development and we will not accept unsolicited resumes/applications from Recruitment Agencies.
Show more
Show less","Data Engineering, Data Analytics, Data Warehousing, Data Lake, Data Modelling, Kimball Methodology, Inmon Methodology, SQL Server, SSAS (Tabular), DB, SSIS, Power BI, DAX, SSRS, Crystal Report, Computer Science, Information Systems, ProblemSolving, Communication, Stakeholder Management","data engineering, data analytics, data warehousing, data lake, data modelling, kimball methodology, inmon methodology, sql server, ssas tabular, db, ssis, power bi, dax, ssrs, crystal report, computer science, information systems, problemsolving, communication, stakeholder management","communication, computer science, crystal report, data engineering, data lake, data modelling, dataanalytics, datawarehouse, dax, db, information systems, inmon methodology, kimball methodology, powerbi, problemsolving, sql server, ssas tabular, ssis, ssrs, stakeholder management"
Data Engineer,Pinpoint Talent,Greater Sydney Area,https://au.linkedin.com/jobs/view/data-engineer-at-pinpoint-talent-3771480264,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Our
NSW State Government client
are seeking multiple
Data Engineers
to join the team on an
initial 9 month contract with view to extend
.
Data Engineer | Sydney CBD / WFH Hybrid | $775 - $905 per day + super | NSW Gov
Purpose of the role:
Design and build a top-tier data & analytics system for the department.
Ensure accurate, data-driven insights for informed executive decisions.
Deliver high-quality service of care to NSW with a focus on data accessibility and accuracy.
Key Accountabilities:
Develop a pipeline in data lake from scratch-built on S3 buckets, Snowflake, Lambda functions, using Python, Glue.
Create data pipelines between various source systems such as SAS, Oracle, Salesforce Guidewire and SQL Server to an enterprise Data Lake / warehouse complying with a STAR schema with multiple SCD’s, CDC’s and FACT tables
Document technical specifications, data models, process flows, etc
Work with Product and Business Owners, collaborate with delivery and technical team members on development, tests, and support for a particular subject areas
Conduct deep dive triage in support of ETL development, issue analysis and resolution
Experience:
6+ years with industry-standard ETL tools, including 3+ years with SAS or Informatica or Matillion
6+ years with data warehouses / marts supporting business requirements, BI report development, and report analysis
6+ years of intermediate to advanced experience in data modelling and manipulation
Experience developing within an agile (i.e. Scrum or Kanban) framework
Experience using docker development environment
Experience working in a matrixed environment on a fast-paced project
Business experience with insurance industry
Exposure to Snowflake database
Experience as an SAS developer, for data management and reporting tools
Experience in information systems and development
Experience in business analysis and data analysis applications in complex information system environment.
Data modeling experience with ERWin, SPARX
Experience with Python
Experience with AWS tools set Lambda and Glue.
If this sounds like you,
APPLY TODAY!
For any inquiries, send an email to:
jai@pinpointtalent.com.au
Follow PINPOINT TALENT on LinkedIn for job updates.
Show more
Show less","Data Engineering, Data & Analytics System Design, DataDriven Insights, Data Accessibility, Data Accuracy, Data Pipeline Development, Data Lake, S3 Buckets, Snowflake, Lambda Functions, Python, Glue, Data Warehouses / Marts, Business Requirements, BI Report Development, Report Analysis, Data Modelling, Data Manipulation, Agile (Scrum / Kanban) Framework, Docker Development Environment, Matrixed Environment, Insurance Industry, SAS Developer, Data Management, Reporting Tools, Information Systems, Development, Business Analysis, Data Analysis, ERWin, SPARX, AWS Tools Set (Lambda Glue)","data engineering, data analytics system design, datadriven insights, data accessibility, data accuracy, data pipeline development, data lake, s3 buckets, snowflake, lambda functions, python, glue, data warehouses marts, business requirements, bi report development, report analysis, data modelling, data manipulation, agile scrum kanban framework, docker development environment, matrixed environment, insurance industry, sas developer, data management, reporting tools, information systems, development, business analysis, data analysis, erwin, sparx, aws tools set lambda glue","agile scrum kanban framework, aws tools set lambda glue, bi report development, business analysis, business requirements, data accessibility, data accuracy, data analytics system design, data engineering, data lake, data management, data manipulation, data modelling, data pipeline development, data warehouses marts, dataanalytics, datadriven insights, development, docker development environment, erwin, glue, information systems, insurance industry, lambda functions, matrixed environment, python, report analysis, reporting tools, s3 buckets, sas developer, snowflake, sparx"
Data Engineer,LMG,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-lmg-3771483519,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Who are we?
LMG is the largest aggregator across Australia and New Zealand supporting a community of over 6,000 brokers and advisers. Proudly family-owned and led, LMG supports businesses operating under their own brand or the Loan Market brand, and partners with over 65 banks and lenders. The business has grown rapidly, reaching a $350 billion loan book and helping more than 300,000 customers in 2022.
We're not a conventional company with a lot of rules and hierarchy, and we don’t intend to become one. We're a big company with a start-up attitude. Our success is based on hiring outstanding people and accepting nothing less than being the best at what we do.
About the role:
As a data engineer in the Data &amp; Analytics team, you will be responsible for building and managing pipelines to ingest data from our CRM, revenue systems and a large number of external data sources to power timely reporting and machine learning models as well as developing integration pipes to embed AI models into production systems.
You will have the opportunity to shape the foundations of data and ML frameworks at LMG to deliver data driven decisions and AI solutions at scale to our customers. You will be working alongside talented data engineers, data scientists and analysts in a collaborative team environment to deliver innovative solutions.
This is a hybrid role (2 days in the office per week) that can be done from Sydney, Melbourne, Brisbane or Perth.
Key responsibilities:
Play a key role in building the data and ML Ops foundations at LMG to
facilitate analytics, reporting and AI rollout at scale
Design, develop, and maintain scalable data pipelines for ingesting,
transforming and storing data
Implement monitoring and alerting solutions for data quality, pipeline health
and performance metrics
Develop and manage machine learning operations (MLOps) frameworks to
support streaming and batch use cases
Create and manage CICD processes for data pipelines and machine learning
models with monitoring and
Develop change data capture (CDC) strategies for real-time and batch data
updates synchronisation across systems and environments
Collaborate with data scientists and analysts to understand data
requirements and implement solutions
Troubleshoot pipeline related issues and provide technical support to
ensure process reliability and integrity
To succeed in this role you will bring with you:
Bachelor's degree or higher qualification in engineering, technology or quantitative discipline
5+ years experience in data engineering with in-depth knowledge of data warehousing, ETL processes and data integration techniques in cloud environments.
Strong coding skills in python, shell scripting and SQL
Hands on experience in designing and implementing dbt pipelines, setting up monitoring and troubleshooting issues
Experience in setting up bespoke workflows outside of a managed services space
Proficiency in GCP products (BigQuery, Cloud Storage and Cloud Scheduler) is an added advantage
Experience in CICD tools like Github or GitLab and sound understanding of devops best practices
Experience working with technical and non-technical stakeholders to gather requirements and implement solutions
Our benefits:
6 ‘Me’ days per year
16 weeks paid parental leave & up to 52 weeks paid superannuation
Paid Fertility Treatment leave
12 days personal leave
Paid Study leave
Hybrid work policy
Wellbeing and L&D programs
Religious and cultural swap leave
Option to purchase additional annual leave
LMGPerks! Thousands of dollars of savings and discounts with the biggest international and local brands
Paid volunteer day
Employee referral program
Public transport annual pass loans
At LMG, we believe that a diverse and inclusive workforce is essential to our success. We're committed to creating a workplace where everyone feels valued, respected, and supported, regardless of their race, gender identity, sexual orientation, religion, age, or any other aspect of their identity. We recognize that diversity brings a variety of perspectives and experiences that are essential to fostering innovation and driving business success. As an inclusive employer, we strive to create a culture where everyone can thrive and bring their best selves to work every day.
Show more
Show less","Data Engineering, Data Warehousing, ETL, Data Integration, Python, Shell Scripting, SQL, DBT, BigQuery, Cloud Storage, Cloud Scheduler, CICD, DevOps, Github, GitLab, GCP","data engineering, data warehousing, etl, data integration, python, shell scripting, sql, dbt, bigquery, cloud storage, cloud scheduler, cicd, devops, github, gitlab, gcp","bigquery, cicd, cloud scheduler, cloud storage, data engineering, data integration, datawarehouse, dbt, devops, etl, gcp, github, gitlab, python, shell scripting, sql"
Data Engineer - Immediate Start,Maltem Australia,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineer-immediate-start-at-maltem-australia-3780236585,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Join our Data and Analytics team as we continue our journey to build innovative cloud-based data solutions. Our engineers operate in a collaborative and autonomous environment where they are empowered and continuously learn, becoming masters in their craft.
This is a fantastic opportunity for someone who is passionate about big data technologies, enjoys solving problems and takes pride in delivering high quality solutions. You will work in a multi-talented, diverse team across multiple geographical locations and implement solutions in a fast-paced environment.
Main Responsibility
Be the change catalyst and champion for data management. Collaborate with the other Data practioners across the organization as the governing body for most data decisions and issue resolution
Provide guidance and set standards of functional excellence in methodologies, processes and SOPs to enable improvement of Global / Local data operations
Develop, construct, test and maintain data architectures
Align architecture with business requirements
Data acquisition
Use programming language and tools
Identify ways to improve data reliability, efficiency and quality
Conduct research for industry and business questions
Use large data sets to address business issues
Deploy sophisticated analytics programs, machine learning and statistical methods
Prepare data for predictive and prescriptive modelling
Use data to discover tasks that can be automated
Deliver updates to stakeholders based on analytics
Skills and Experience
Python Programming, Advanced SQL Knowledge, Database Migration (Impala to Presto would be advantageous), SharePoint and CSV Manipulation, Apache Iceberg, Cloud Services, Familiarity with cloud storage solutions, specifically Amazon S3, ETL Tools, Proficiency in Bitbucket, Knowledge of Control-M, Jira Proficiency.
Minimum 3-5 Years’ data engineering experience on AWS
Minimum 1-2 years working experience on AWS Glue and RedShift.
Strong working knowledge of SQL & Python.
Must have previous experience working on data lake on S3
AWS data related certification will be added advantage
Experience in Agile delivery including definition of User Stories
Previous experience supporting AWS data pipelines will be added advantage
Strong understanding of Data & Analytics concepts and technologies including data warehousing, data lakes, ETL and visualisation and reporting, and how business value is created from these technologies.
Strong understanding of Data Architecture, Data modelling, Data marts and Metadata management best practice
Strong communication and presentation skills
Degree in Computer Science, Data Science, IT, or similar field
Communication Skills: Excellent communication abilities for engaging with stakeholders, providing timely updates, and articulating complex technical details in a clear and concise manner.
So, are you ready to join us?
Show more
Show less","Python, SQL, Database Migration, Apache Iceberg, Cloud Services, Amazon S3, ETL Tools, Bitbucket, ControlM, Jira, AWS Glue, RedShift, Data Warehousing, Data Lakes, ETL, Visualization, Reporting, Data Architecture, Data Modelling, Data Marts, Metadata Management, Communication Skills, Presentation Skills","python, sql, database migration, apache iceberg, cloud services, amazon s3, etl tools, bitbucket, controlm, jira, aws glue, redshift, data warehousing, data lakes, etl, visualization, reporting, data architecture, data modelling, data marts, metadata management, communication skills, presentation skills","amazon s3, apache iceberg, aws glue, bitbucket, cloud services, communication skills, controlm, data architecture, data lakes, data marts, data modelling, database migration, datawarehouse, etl, etl tools, jira, metadata management, presentation skills, python, redshift, reporting, sql, visualization"
Senior Data Engineer - Sydney,GOSOL,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-sydney-at-gosol-3667428203,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"About GoSol
GOSOL, IT consulting and Staffing organization, can help you enhance Technology, Digital, Project and Business Transformation. Guided by integrity and trust, we believe in offering an unparalleled experience as your Tech of choice. For businesses, this means providing you with access to the quality solution.
Find us at - https://gosol.com.au/
Job Description
In this role, you will design, build, and maintain batch and real-time data pipelines on On-premises and cloud platforms to ingest data from the various source systems.
Post ingestion, you will be responsible for integrating the data asset in the data model and making the data ready for consumption for the end-user use cases and downstream applications.
Key Responsibilities* Enhance, optimize and maintain existing data ingestion, transformation and extraction pipelines and assets built for reporting and analytics on Cloud (Azure + Databricks) and Big Data Cloudera platform. Work with the Product Owner to understand the priorities and OKRs for the quarter and gather detailed requirements from the initiative owners or program sponsor as per the Epics planned to be delivered in the quarter. Build new and optimized data pipelines, assets to meet the end-user requirements. The Data pipelines must adhere to all the architecture, design and engineering principles. Design the data pipelines and assets to meet non-functional requirements (Security, reliability, performance, maintainability, scalability, and usability). Most importantly, they should be designed to be to keep the compute cost low on Cloud. Data wrangling, Data profiling and data analysis for new datasets ingested from source systems and derived/built from existing datasets with the on-premises and cloud-native tools Functionally understand the Data assets working with various SMEs and apply the transformation rules required to build the target data asset.
Experience and Qualifications* Bachelor's degree in maths, statistics, computer science, information management, finance or economics 7+ years' experience working in Data Engineering and Warehousing. Experience in building fully automated end to end data pipelines using on-premise or cloud based data platforms. Cloud experience Azure based analytics/reporting pipeline Hands on delivery of solutions for Reporting and Analytics use cases. Hands On with advanced SQL on Data Warehouse , Big Data and Data Bricks Experience in data profiling, source-target mappings, ETL development, SQL optimisation, testing and implementation Experience in working effectively on Cloud DWs Knowledgeable in Big Data tools like Spark (python/scala), Hive, Impala, Hue and storage (e.g. HDFS, HBase) Knowledgeable in CICD processes BitBucket/GitHub, Jenkins, Nexus etc. Knowledgeable managing structured and unstructured data types like json, xml, avro Track record of implementing databases and data access middleware and high-volume batch and (near) real-time processing
Only candidates with valid work rights of Australia will be considered.
Only candidates with valid work rights of Australia will be considered.
Show more
Show less","Data Engineering, Data Warehousing, Data Pipelines, Data Integration, Data Profiling, Data Analysis, Data Mining, Data Visualization, SQL, Azure, Databricks, Hadoop, Spark, Python, Scala, Hive, Impala, Hue, HDFS, HBase, JSON, XML, Avro, Jenkins, Nexus, Big Data, Cloud Computing, Data Modeling, Data Extraction, Data Transformation, Data Loading, Data Cleansing, Data Governance, Data Security, Data Privacy, Data Quality, Data Architecture, Data Design, Data Engineering Principles, Data Architecture Principles, Data Modeling Principles","data engineering, data warehousing, data pipelines, data integration, data profiling, data analysis, data mining, data visualization, sql, azure, databricks, hadoop, spark, python, scala, hive, impala, hue, hdfs, hbase, json, xml, avro, jenkins, nexus, big data, cloud computing, data modeling, data extraction, data transformation, data loading, data cleansing, data governance, data security, data privacy, data quality, data architecture, data design, data engineering principles, data architecture principles, data modeling principles","avro, azure, big data, cloud computing, data architecture, data architecture principles, data design, data engineering, data engineering principles, data extraction, data governance, data integration, data loading, data mining, data modeling principles, data privacy, data profiling, data quality, data security, data transformation, dataanalytics, databricks, datacleaning, datamodeling, datapipeline, datawarehouse, hadoop, hbase, hdfs, hive, hue, impala, jenkins, json, nexus, python, scala, spark, sql, visualization, xml"
Data Engineer (Databricks & DBT),CareCone Group,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineer-databricks-dbt-at-carecone-group-3783449182,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Position: Data Engineer (Databricks & DBT)
Location: Sydney/Melbourne/Brisbane
Job Description:
Current or prior experience as Data Engineer / Consultant
Experience of working with data projects in Banking, Finance, telecom domains
Data engineering experience with Databricks & DBT
Must have good collaboration and communication skills
Good to have Unix Shell Scripting Knowledge
Must have strong experience working hands on Bigdata technologies like Hive, Impala, Cloudera etc
Interested Candidates can share their updated resume to nidhi.goyal@carecone.com.au
Show more
Show less","Data Engineer, Data Engineering, Databricks, DBT, Data Projects, Banking, Finance, Telecom, Hive, Impala, Cloudera, Unix Shell Scripting, Bigdata","data engineer, data engineering, databricks, dbt, data projects, banking, finance, telecom, hive, impala, cloudera, unix shell scripting, bigdata","banking, bigdata, cloudera, data engineering, data projects, databricks, dataengineering, dbt, finance, hive, impala, telecom, unix shell scripting"
Senior Data Engineer,Quantexa,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-quantexa-3597329467,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Founded in 2016 with only a handful of individuals, Quantexa was built with a purpose that through a greater understanding of context, better decisions can be made. 7 years, 15 locations and 650+ employees later we still believe that today. We connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data.
Due to the continuous success and high demand from our customers, we are looking for a Senior Data Engineer with a proven track record to join the Quantexa family. 🚀
What does a Senior Data Engineer role at Quantexa look like?
In order to be a successful Senior Data Engineer at Quantexa, you’ll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.
Being Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. You’ll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service.
We want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala, Data Fusion and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but don’t worry If that’s not your strongest language or if you haven’t used it before, we make sure that every Quantexan goes through our training academy so they’re comfortable and confident with using our platform.
Requirements
What do I need to have?
We’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.
The desire to learn and code in Scala
Experience in working in an Agile environment
Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.
A strong coding background in either Java, Python or Scala
Experience of building data processing pipelines for use in production “hands off” batch systems, including either traditional ETL pipelines and/or analytics pipelines.
Passion and drive to grow within one of the UK’s fastest growing scale-ups.
Consulting or business facing skills and a desire to work with customers.
Benefits
Why join Quantexa?
We know that just having an excellent glass door rating isn’t enough, so we’ve put together a competitive package as a way of saying thank you for all your hard work and dedication.
We offer:
Competitive salary 💰
Company bonus
11% Superannuation
Private healthcare with Medibank
Dental, Orthodontics & Optics Cover
Free Calm App Subscription #1 app for meditation, relaxation and sleep 🧘‍♀️
20 days annual leave, 13 national holidays + birthday off! 🌴
Ongoing personal development
Great WeWork Office Space & Company wide socials
Our mission
We have one mission. To help businesses grow. To make data easier. And to make the world a better place. We’re not a start-up. Not anymore. But we’ve not been around that long either. What we are is a collection of bright, passionate minds harnessing complexities and helping our clients and their communities. One culture, made of many. Heading in one direction – the future.
It's All About You
Quantexa is proud to be an Equal Opportunity Employer. We’re dedicated to creating an inclusive and diverse work environment, where everyone feels welcome, valued, and respected. We want to hear from people who are passionate about their work and align with our values. Qualified applications will receive consideration for employment without regard to their race, colour, ancestry, religion, national origin, sex, sexual orientation, gender identity, age, citizenship, marital, disability, or veteran status. Whoever you are, if you’re a curious, caring, and authentic human being who wants to help push the boundaries of what’s possible, we want to hear from you.
Internal pay equity across departments is crucial to our global compensation philosophy. Grade level and salary ranges are determined through interviews and a review of experience, education, training, knowledge, skills, and abilities of the applicant, equity with other team members, and alignment with market data.
Quantexa is committed to providing reasonable accommodations in our talent acquisition processes. If you require support, please inform our Talent Acquisition Team.
Show more
Show less","Spark, Hadoop, Scala, Data Fusion, Elasticsearch, Google Cloud Platform, Java, Python, ETL pipelines, Analytic pipelines, Agile, Scrum, Consulting, Business facing skills, Data processing pipelines, Data Science","spark, hadoop, scala, data fusion, elasticsearch, google cloud platform, java, python, etl pipelines, analytic pipelines, agile, scrum, consulting, business facing skills, data processing pipelines, data science","agile, analytic pipelines, business facing skills, consulting, data fusion, data processing pipelines, data science, elasticsearch, etl pipelines, google cloud platform, hadoop, java, python, scala, scrum, spark"
Azure Data Engineer,Pinpoint Talent,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/azure-data-engineer-at-pinpoint-talent-3777884124,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Our client is looking for an experienced Azure Data Engineer to work on a key client project. They are setting up the client’s foundational Azure Cloud and Data platforms, including the installation and configuration of various Azure Data products/components, and building data pipelines to execute a number of their data use cases.
Role scope
includes:
Working with the Data Architect to help articulate and document the overall end-to-end Azure Data solution
Designing and implementing data storage solutions on Azure
Installing and configuring Azure data products
Developing ETL (Extract, Transform, Load) processes to ingest, transform, and load data into Azure data platforms
Building and maintaining data pipelines using relevant Azure services
Ensuring data security and compliance with data governance policies and regulations
Performance tuning and optimization of Azure data solutions to meet scalability and performance requirements
Your Skills & Experience:
Must-have skills, technical requirements, education, experience
3+ or 7+ (senior) years in a software of data engineering role with demonstrated experience in data integration projects
Practical experience with the following Azure products:
Purview
Azure Data Factory (ADF)
Synapse
Azure data storage solutions, including Blob Storage, Data Lake Storage and Delta Lake
Azure Data Migration Service
Azure DevOps
Azure Active Directory
Integrating to Power BI
Experience building data assets such as data lakes, data warehouses, data marts, or customer data platforms
Show more
Show less","Azure, Purview, Azure Data Factory (ADF), Synapse, Blob Storage, Data Lake Storage, Delta Lake, Azure Data Migration Service, Azure DevOps, Azure Active Directory, Power BI, Data lakes, Data warehouses, Data marts, Customer data platforms","azure, purview, azure data factory adf, synapse, blob storage, data lake storage, delta lake, azure data migration service, azure devops, azure active directory, power bi, data lakes, data warehouses, data marts, customer data platforms","azure, azure active directory, azure data factory adf, azure data migration service, azure devops, blob storage, customer data platforms, data lake storage, data lakes, data marts, data warehouses, delta lake, powerbi, purview, synapse"
PySpark Data Engineer,Jenny Barbour IT and Project Recruitment,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/pyspark-data-engineer-at-jenny-barbour-it-and-project-recruitment-3771479574,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Employer of choice
Media Broadcast industry
Superior technology
You will contribute to both developing and delivering our applications and supporting the use of our applications. This role is suited to a dedicated individual who is happy to work close to the business for mutual success.
Key Skills & Responsibilities
The successful applicant should possess:
Strong experience with PySpark in a software engineering environment
Experience deploying solutions in cloud environments (Azure / AWS).
Ability to manage applications on Linux and Windows.
A team first mentality, focusing on output as a team, rather than as an individual
Solid knowledge of SQL databases, specifically Microsoft SQL.
Excellent communication skills, both written and verbal
Experience working in an agile environment
Exposure to developing tests (unit and integration)
To submit your interest for this role, click on the Apply button quoting reference # 5319
Due to high volume, we will do our very best to contact all applicants and do apologise if there is a delay in this process. Please contact hina@jennybarbour.com.au with an application enquiry
Show more
Show less","PySpark, Azure, AWS, Linux, Windows, SQL, Microsoft SQL, Unit Testing, Integration Testing, Agile Development","pyspark, azure, aws, linux, windows, sql, microsoft sql, unit testing, integration testing, agile development","agile development, aws, azure, integration testing, linux, microsoft sql, spark, sql, unit testing, windows"
Senior Data Engineer - Azure/Databricks,Maltem Australia,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-azure-databricks-at-maltem-australia-3782470851,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"About Maltem:
Maltem is a dynamic and innovative data & analytics consultancy based in Sydney, Australia. As a leading player in the industry, Maltem is committed to driving excellence in data-driven solutions. We pride ourselves on fostering a collaborative and growth-oriented environment, pushing the boundaries of what's possible in the data realm.
Responsibilities:
Lead the end-to-end design, development, and optimization of data pipelines using Databricks, ensuring high performance and scalability.
Collaborate closely with cross-functional teams to gather requirements and translate them into technical solutions, utilizing Azure services and Azure Data Factory.
Mentor and guide junior data engineers, providing technical expertise and best practices in data engineering.
Evaluate and enhance existing data systems, implementing data governance standards to maintain data accuracy and reliability.
Stay updated with industry trends and emerging technologies in the data engineering landscape.
Requirements:
Qualifications:
In-depth expertise in Databricks, including configuration, optimization, and a strong understanding of Spark-based processing.
Proficiency in Azure services, specifically Azure Data Factory, and a solid grasp of the Azure ecosystem.
Previous exposure to Striim for data ingestion would be advantageous.
Bachelor's or Master's degree in Computer Science, Engineering, or related field.
Why Maltem:
Joining Maltem means being part of an exciting greenfield project that pushes the boundaries of data engineering. As a Senior Data Engineer, you'll have the opportunity to contribute to the growth of a cutting-edge consultancy and make a significant impact in the data space. Be a key player in a team that values innovation, collaboration, and continuous learning.
Apply now to be part of this exciting journey with Maltem!
Show more
Show less","Databricks, Spark, Azure services, Azure Data Factory, Azure ecosystem, Striim, Data pipelines, Data engineering, Data governance, Computer Science, Software engineering","databricks, spark, azure services, azure data factory, azure ecosystem, striim, data pipelines, data engineering, data governance, computer science, software engineering","azure data factory, azure ecosystem, azure services, computer science, data engineering, data governance, databricks, datapipeline, software engineering, spark, striim"
Data Engineer,Nine,"North Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-nine-3768158871,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Company Description
Nine. Australia’s Media Company.
Underpinned by our people, our strategic focus is on content, connections and growth.
Driven by our purpose - Australia belongs here - and guided by our values - walk the talk, turn over every stone, keep it human - we are the home of Australia’s most loved content and trusted brands across News, Sport, and Entertainment.
We lead the total television market in Australia through our key brands of Channel 9, Gem, Go, Rush and Life, and of course you can consume all our content, live or on-demand, on Australia’s leading free streaming platform, 9Now. We also own Stan, the market leading local subscription streaming service.
Our total publishing business covers The Sydney Morning Herald, The Age, The Australian Financial Review, to the breadth of nine.com.au and The Pedestrian Group.
In our total audio business we are building talk radio for the next generation across 2GB, 3AW, 4BC and 6PR.
Nine is also an active participant in Australian consumer marketplaces through our majority shareholding in Domain and our ownership of Drive.
Job Description
Join the Product and Technology data team to continue to make data a competitive advantage.Work with key stakeholders to build high-performing data products and reduce the complexity in our data systems and data models. You will enable the use of data to create innovative solutions for our business, focusing in the data commercialization space.
Work with a passionate team of data professionals that loves actively helping shape business strategy through data driven techniques
Responsibilities:
Design and implement data pipelines and data processing frameworks at petabyte scale in Google Cloud, Apache Airflow, BigQuery and other cutting edge tools.
Craft solutions that directly enable reporting specialists, analysts and super-users by exposing consumption focused dimensional data models and data products/ visualisations.
Work across the Data Engineering teams, Vendors and Business Stakeholders to contribute to the continuous improvement of data products/ processes/ systems
Provide data on industry trends, emerging technologies, and tool sets - we are not afraid to experiment!
Keep up ­to ­date with media and technology industry changes and stimulate discussion.
Ensure deliveries are up to date with the latest technologies.
Qualifications
What we would like to see:
Experience in a Data Engineering role with knowledge and application of SQL and Python used on Cloud Platform preferably Google Cloud Platform
Designing data processing frameworks in Airflow and setting up DAGs
Data analytics and reporting tools like Tableau ad Data Studio
Ability to work in agile development and delivery processes, along with solution design and implementation
Solid understanding of database structures, theories, principles & practices
Solid understanding of CI/CD, Version Control like Github or Bitbucket.
Don’t worry if you can’t tick every single one of these boxes, we would still like to hear from you. With a
willingness to learn, a positive attitude and a growth mindset anything is possible at Nine!
Additional Information
Life at Nine
Nine is a people business. Being part of the team means you’ll be in a culture that promotes creativity and innovation, diversity and inclusion, is open to feedback, rewards impact and ensures everyone is heard. Importantly, you'll receive the following:
Career development and quality training
Up to 16 weeks paid primary carer’s parental leave
Discounts on lifestyle, entertainment, and leisure memberships, including health insurance, dental and gym
Discounts on products and services with corporate partners
Our Commitment to Diversity and Inclusion:
At Nine, we are committed to fostering a workforce that embraces all aspects of diversity and inclusion and where practices are equitable to ensure our people experience a sense of belonging. From day one, you'll be encouraged to bring your whole self to work and will be supported to perform at your best. Should you require any adjustments to the recruitment process in order to equitably participate, we encourage you to advise us at the time of application.
We encourage applications from Aboriginal and Torres Strait Islander people, people with disabilities, and of all ages, nationalities, backgrounds and cultures.
Disclaimer:
We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.
Show more
Show less","Data Engineering, SQL, Python, Google Cloud Platform, Apache Airflow, BigQuery, Tableau, Data Studio, Agile Development, Solution Design, CI/CD, Version Control, Github, Bitbucket, Database Structures, Database Theories, Database Principles, Database Practices","data engineering, sql, python, google cloud platform, apache airflow, bigquery, tableau, data studio, agile development, solution design, cicd, version control, github, bitbucket, database structures, database theories, database principles, database practices","agile development, apache airflow, bigquery, bitbucket, cicd, data engineering, data studio, database practices, database principles, database structures, database theories, github, google cloud platform, python, solution design, sql, tableau, version control"
Data Engineer - Redshift,CareCone Group,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineer-redshift-at-carecone-group-3785223198,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Position: Data Engineer- Redshift
Location: Sydney, Melbourne , Brisbane
Role: Permanent (Full-time)
Skills Required:
Must to have:
Previous knowledge of SDS and SDS2 environments
Extensive SQL Skills
Good understanding of Postgres & DB2
Redshift - Must be super strong in Redshift
Must have banking experience
Good to have:
Some data analysis experience
Interested candidates can send their updated resume to anoop.redhu@carecone.com.au
Show more
Show less","SDS, SDS2, SQL, Postgres, DB2, Redshift, Data Analysis","sds, sds2, sql, postgres, db2, redshift, data analysis","dataanalytics, db2, postgres, redshift, sds, sds2, sql"
Principal / Senior Data Engineer - Data Platform Engineering,Simple Machines,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/principal-senior-data-engineer-data-platform-engineering-at-simple-machines-3776691898,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"There is nothing ordinary about Simple Machines. We have a team of inspiring technical leaders in Data Engineering, Machine Learning, and Artificial Intelligence. We focus on data-driven projects that drive impactful business results.
We create cutting-edge business-driven data platforms and software solutions to help our clients unleash the full potential of their data. We are seeking Data Engineers who are willing to challenge the status quo and push the envelope as we expand our team across Australia and New Zealand.
About You
A passion for solving complex problems with clients and leading them through the process from discovery to technical implementation is required. Your ability to understand client needs and develop market-leading solutions comes naturally to you.
About the Role
Located at our awesome Darlinghurst office, the Principal / Senior Data Engineer will lead and mentor teams from various organisations, collaborate with engineering teams, and work on world-class programs.
Sound good so far?
Responsibilities
Collaborate with business stakeholders to understand their domain and data needs
Architect, design, and implement scalable solutions to meet stakeholder requirements
Mentor teams across a variety of organisations
Requirements
Understanding of big data considerations, such as security, governance, reliability, and scalability
Ability to upgrade existing solutions to meet new requirements
Ability to model data to meet business requirements and make informed technology recommendations
Deep understanding of structured, semi-structured, and unstructured data
Hands-on experience with batch processing and streaming tools and technologies, such as Spark, Kafka, Flink, DBT, and Athena
Experience with cloud platforms (AWS, GCP, Azure)
Proficiency in programming languages such as Python, Java, Scala, GoLang, or Rust
Experience with data warehousing technologies (BigQuery, Snowflake, Databricks) and data storage formats (Parquet, Delta, ORC, Avro, JSON)
Nice-to-Haves
Experience with infrastructure as code tools (Terraform, Pulumi, CDK)
Knowledge of containerised solutions (Docker, Kubernetes)
Familiarity with CI/CD tools (GitHub Actions, Azure DevOps, JetBrains TeamCity, Octopus Deploy, ArgoCD)
Experience with testing tools and frameworks (DBT, Great Expectations, Soda)
About Simple Machines
Simple Machines isn’t your typical tech company. Rather, we’re a firm of passionate and talented expert consultants that are as much a community as a company.
When we set out to make a difference nearly 10 years ago, we did so because we weren’t happy with the status quo of technology consultancies. We wanted to build a company we would want to hire ourselves. And we’ve done just that.
At the core of our company is a team of talented individuals, many of whom are at the top of their field. We regularly present at local and international conferences, committing to many of the open-source projects we utilise in our day-to-day engagements and release many other projects back to the broader technology community.
Apply now and let’s see where the connection takes us.
Show more
Show less","Data Engineering, Machine Learning, Artificial Intelligence, Datadriven projects, Data platforms, Software solutions, Problem solving, Data understanding, Client needs, Marketleading solutions, Big data considerations, Security, Governance, Reliability, Scalability, Solutions upgrading, Data modeling, Technology recommendations, Structured data, Semistructured data, Unstructured data, Batch processing, Streaming tools, Spark, Kafka, Flink, DBT, Athena, Cloud platforms, AWS, GCP, Azure, Python, Java, Scala, GoLang, Rust, Data warehousing, BigQuery, Snowflake, Databricks, Data storage formats, Parquet, Delta, ORC, Avro, JSON, Infrastructure as code, Terraform, Pulumi, CDK, Containerised solutions, Docker, Kubernetes, CI/CD tools, GitHub Actions, Azure DevOps, JetBrains TeamCity, Octopus Deploy, ArgoCD, Testing tools, DBT, Great Expectations, Soda","data engineering, machine learning, artificial intelligence, datadriven projects, data platforms, software solutions, problem solving, data understanding, client needs, marketleading solutions, big data considerations, security, governance, reliability, scalability, solutions upgrading, data modeling, technology recommendations, structured data, semistructured data, unstructured data, batch processing, streaming tools, spark, kafka, flink, dbt, athena, cloud platforms, aws, gcp, azure, python, java, scala, golang, rust, data warehousing, bigquery, snowflake, databricks, data storage formats, parquet, delta, orc, avro, json, infrastructure as code, terraform, pulumi, cdk, containerised solutions, docker, kubernetes, cicd tools, github actions, azure devops, jetbrains teamcity, octopus deploy, argocd, testing tools, dbt, great expectations, soda","argocd, artificial intelligence, athena, avro, aws, azure, azure devops, batch processing, big data considerations, bigquery, cdk, cicd tools, client needs, cloud platforms, containerised solutions, data engineering, data platforms, data storage formats, data understanding, databricks, datadriven projects, datamodeling, datawarehouse, dbt, delta, docker, flink, gcp, github actions, golang, governance, great expectations, infrastructure as code, java, jetbrains teamcity, json, kafka, kubernetes, machine learning, marketleading solutions, octopus deploy, orc, parquet, problem solving, pulumi, python, reliability, rust, scala, scalability, security, semistructured data, snowflake, soda, software solutions, solutions upgrading, spark, streaming tools, structured data, technology recommendations, terraform, testing tools, unstructured data"
Senior Data Engineer,Hays,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-hays-3784294630,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Your new company
Our client is a higher education organisation.
Your new role
A minimum of 3 years proven experience as a Senior Data Engineer
Design, implement, test, deploy, stable, secure, and scalable data engineering solutions and pipelines
What you'll need to succeed
Expert in SQL and Modelling in a 3 tier Data Architecture
Demonstrated extensive use of Talend, Snowflake & Power BI
Demonstrated ability to work independently and be a self-starter.
Ability to clearly understand Business Requirements and the underlying drivers of the Business.
Understanding of the Business priorities and able to refine the Product Backlog
Expert understanding of Agile Scrum, cadence and ceremonies
Ability to work across numerous projects.
Clear and accurate documentation is a current focus for the team.
Be someone the Date Engineering team can look up to and rely on for guidance.
Experience of University & Research is desirable.
Hybrid role - 50% on campus: 50% WFH
What you'll get in return
6 months contract
Rate: $800-$1000 / day
What you need to do now
If you're interested in this role, click 'apply now' to forward an up-to-date copy of your CV, or call us now.
If this job isn't quite right for you, but you are looking for a new position, please contact us for a confidential discussion about your career.
Show more
Show less","SQL, Data Modeling, Talend, Snowflake, Power BI, Agile Scrum, Data Engineering, Data Pipelines","sql, data modeling, talend, snowflake, power bi, agile scrum, data engineering, data pipelines","agile scrum, data engineering, datamodeling, datapipeline, powerbi, snowflake, sql, talend"
AWS Data Engineer (5 Months) - Sydney,AWS Data Engineer (5 Months) - Sydney,Greater Sydney Area,https://au.linkedin.com/jobs/view/aws-data-engineer-5-months-sydney-at-aws-data-engineer-5-months-sydney-3768748963,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"AWS Data Engineer - Sydney;
This is a TERM Contract role, of 5 months, with possible extensions.
A salary of $ 175K annual is envisaged.
Requirements:
Experience as AWS Data Engineer
Experience on working with AWS data services such as Glue, Athena,
AWS main Storage Service: S3, RDS, Redshift, DynamoDB
Has strong experience in the Data Architecture, Solution Design, and Development
Excellent interpersonal, verbal, and written skills and the ability to interact with all levels of stakeholders, support personnel, and clients.
Must Have:
Good SQL language knowledge.
Preferred
:
Working experience with Agile Methodology and Kanban
AWS main Compute Service: EC2, Lambda, ECS, EKS
Experience working with source code management tools such as AWS CodeCommit or GitHub
Please send your detailed resume to peter@klareconsulting.com
Show more
Show less","AWS Data Engineer, AWS Glue, AWS Athena, AWS S3, AWS RDS, AWS Redshift, AWS DynamoDB, Data Architecture, Solution Design, Development, Interpersonal Skills, Verbal Skills, Written Skills, SQL, Agile Methodology, Kanban, AWS EC2, AWS Lambda, AWS ECS, AWS EKS, AWS CodeCommit, GitHub","aws data engineer, aws glue, aws athena, aws s3, aws rds, aws redshift, aws dynamodb, data architecture, solution design, development, interpersonal skills, verbal skills, written skills, sql, agile methodology, kanban, aws ec2, aws lambda, aws ecs, aws eks, aws codecommit, github","agile methodology, aws athena, aws codecommit, aws data engineer, aws dynamodb, aws ec2, aws ecs, aws eks, aws glue, aws lambda, aws rds, aws redshift, aws s3, data architecture, development, github, interpersonal skills, kanban, solution design, sql, verbal skills, written skills"
Senior Data Engineer | Ai,DeepAi,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-ai-at-deepai-3772699374,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"🚀
Exciting Career Opportunity for a Senior Data Engineer
🚀
Are you a seasoned Data Engineer seeking a dynamic role at the forefront of AI-driven solutions to combat financial fraud? Our client, a tech unicorn valued at over $1 billion, are currently expanding their team here in Sydney.
As a Senior Data Engineer, you'll play a crucial role in managing, transforming, and cleansing high-volume data, contributing to the success of our Tier 1 clients in fraud, compliance, and financial crime.
What To Expect
Engage with internal/external stakeholders.
Collaborate with cross-functional teams in Agile settings.
Utilize cutting-edge big-data technologies: Spark, Hadoop, Scala, Data Fusion, Elasticsearch.
Continuous learning and growth in a supportive environment.
Requirements
Proven big data experience.
Desire to learn and code in Scala.
Agile environment experience.
Expertise in Spark, Hadoop, or Elasticsearch.
Strong coding background in Java, Python, or Scala.
Building data processing pipelines for production.
Consulting/business-facing skills.
Why Join?
Cutting-Edge Technology: Work with the latest open-source big-data technologies, including Spark, Hadoop, Scala, Data Fusion, and Elasticsearch, hosted on Google Cloud.
Global Impact: Collaborate with top-tier clients worldwide, including major banks, governments, and enterprises, contributing to the fight against financial crime on a global scale.
Career Growth: Join a successful and stable company with a clear growth strategy. Our Partner invests in your development, providing training, certifications, and exposure to diverse projects.
Agile Environment: Be part of an Agile work culture, leading regular sprints and Scrum meetings with a multidisciplinary team, including Data Scientists, Business Analysts, and Project Managers.
Competitive Package: Enjoy a competitive salary, company bonus, superannuation, and additional perks such as private healthcare, dental, and optics cover.
Our partner’s commitment to innovation, global impact, and career growth makes this opportunity perfect for Data Engineers passionate about making a real difference in the world of AI and data analytics. Join us in shaping the future of financial security!
Show more
Show less","Data Engineering, Scala, Java, Python, Spark, Hadoop, Elasticsearch, Data Fusion, Agile, Scrum, Data Processing Pipelines, Big Data, Cloud Computing, Business Facing Skills, Financial Crime, Data Analytics, AI","data engineering, scala, java, python, spark, hadoop, elasticsearch, data fusion, agile, scrum, data processing pipelines, big data, cloud computing, business facing skills, financial crime, data analytics, ai","agile, ai, big data, business facing skills, cloud computing, data engineering, data fusion, data processing pipelines, dataanalytics, elasticsearch, financial crime, hadoop, java, python, scala, scrum, spark"
Senior Data Engineer,Workforce Australia for Individuals,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-workforce-australia-for-individuals-3784591070,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Your New Company
Our client is a higher education organisation.
Your new role
A minimum of 3 years proven experience as a Senior Data Engineer
Design, implement, test, deploy, stable, secure, and scalable data engineering solutions and pipelines
What You'll Need To Succeed
Expert in SQL and Modelling in a 3 tier Data Architecture
Demonstrated extensive use of Talend, Snowflake & Power BI
Demonstrated ability to work independently and be a self-starter.
Ability to clearly understand Business Requirements and the underlying drivers of the Business.
Understanding of the Business priorities and able to refine the Product Backlog
Expert understanding of Agile Scrum, cadence and ceremonies
Ability to work across numerous projects.
Clear and accurate documentation is a current focus for the team.
Be someone the Date Engineering team can look up to and rely on for guidance.
Experience of University & Research is desirable.
Hybrid role - 50% on campus: 50% WFH
What You'll Get In Return
6 months contract
Rate: $800-$1000 / day
What You Need To Do Now
If you're interested in this role, click 'apply now' to forward an up-to-date copy of your CV, or call us now.
If this job isn't quite right for you, but you are looking for a new position, please contact us for a confidential discussion about your career.
Please click here to apply.
Show more
Show less","SQL, Data Modeling, 3Tier Architecture, Talend, Snowflake, Power BI, Agile Scrum, Product Backlog, Documentation, University & Research","sql, data modeling, 3tier architecture, talend, snowflake, power bi, agile scrum, product backlog, documentation, university research","3tier architecture, agile scrum, datamodeling, documentation, powerbi, product backlog, snowflake, sql, talend, university research"
Senior Microsoft Data Engineer  & PowerBI Consultants,BI & DW Australia,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-microsoft-data-engineer-powerbi-consultants-at-bi-dw-australia-3729014348,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Senior Consultants - Microsoft Data & Analytics
A leading Data & Analytics Professional Services consultancy is looking for two Senior Consultants for their practice. One a Data Engineer and the other a Power BI developer
What's In It for You
A competitive permanent salary package with the opportunity to work with large enterprise clients in many verticals including financial services, utilities, government and more!
The role
This role is the epitome of the archetypal IT business consultant where you will apply your proven Microsoft Business Intelligence development expertise within top ASX 500 style organisations. You will be working exclusively on high level projects within the Sydney market and will utilise the Microsoft Azure Data Services technologies, Databricks, Data Factory and Power BI. You will become a key member of the team assisting with the growth of the Microsoft Practice.
The skills needed
The ideal candidate for this true technical consulting role will have some or all of the following experience:
Microsoft Platform knowledge of Azure Data Services, the Microsoft Data cloud stack and on prem Microsoft BI technologies
Databricks is advantageous
Power BI experience
Azure Data Factory is a huge advantage
Data modelling experience
Ideally previous Professional services experience with niche Data consultancies
Previous on prem SSIS, SSAS (tabular and modular) and SSRS, Power BI experience
High level client liaison skills
Documentation (business focused, technology focused and project focused)
As mentioned salary levels are excellent and remuneration will match your experience.
Show more
Show less","Microsoft Azure, Power BI, Microsoft Data cloud, SSIS, SSAS, SSRS, Databricks, Azure Data services, Azure Data Factory, Data modelling, Documentation, Client liaison","microsoft azure, power bi, microsoft data cloud, ssis, ssas, ssrs, databricks, azure data services, azure data factory, data modelling, documentation, client liaison","azure data factory, azure data services, client liaison, data modelling, databricks, documentation, microsoft azure, microsoft data cloud, powerbi, ssas, ssis, ssrs"
Senior Frontend Engineer | Data Visualization | Sydney Hybrid,Displayr,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-frontend-engineer-data-visualization-sydney-hybrid-at-displayr-3775685130,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"About Us
At Displayr, our mission is to make it easier for everyone to find and share the stories in their data. We do this by creating software that makes it easy to do all aspects of data analysis and reporting, from simple tables through to machine learning, PowerPoint automation to online dashboards.
Our Engineering team is at the heart of this mission, and we need YOU.
We're a bootstrapped company - no corporate debt, no external investors. In the last five years, we've grown from 11 to over 95 people, and we have 1500+ great clients, from small businesses through to Amazon & Meta.
We have recently won 1st place in the Great Places to Work, Best Tech Companies List 2023 ANZ for small employers and we are proud to be certified as a Great Place to Work for the second year in a row.
Displayr is pleased to partner with Work180, an organization dedicated to improving the workplace for Diversity and Inclusion, particularly working conditions for women.
About the role
Join our Data Visualization team developing the world's best data visualization tool which is fast, easy to use, and feature-rich.
Use your technical expertise to design and create TypeScript packages that drive the visualizations within Displayr.
Collaborate with our development team to elevate the capabilities of our visualization framework.
Code review the work of your peers and mentor your teammates, encouraging them to grow technically and professionally.
Convert ideas to products and features in a highly collaborative environment.
Participate in the evaluation of new candidates for the engineering team.
About You
Essential
5 years+ of solid frontend development experience.
3 years+ of TypeScript experience.
You have a Degree in Computer Science or Computer Software.
You deeply understand the platforms and libraries you use.
Your CV is full of achievements you are proud of.
You are concise, clear, and courteous in your communication.
You are curious and innovative.
You take calculated risks and are not afraid of failure.
Desirable
Background in Data Visualization is preferred however we are happy to train the right person.
Experience with d3 would be a big advantage.
A good eye for UI & UX design.
Technical Skills
Front-end: TypeScript/JavaScript/React
Back-end: C#, SQL
Supporting systems: C#, Python, R, Rust, Node.js
Platforms: Azure, AWS, Kubernetes
Why Displayr?
At Displayr you are not just a cog in a large engine. You'll be able to see the decisions you make have a tangible impact.
If you want to work on big thorny problems that will build upon, test, and show off your exceptional technical skills, then you'll thrive at Displayr.
Displayr is entirely bootstrapped and debt-free, so continues to grow safely
Displayr has a high-trust environment, where you'll be given a serious degree of autonomy and flexibility.
We prefer evidence over intuition, experience, and ""best practice"".
We work hard to make the best idea win, not the idea of the most experienced person.
International company - offices in Sydney, Chicago, and London.
Benefits
Competitive base salary.
Share options in the business (join a fast-growing company in its early stages).
14 weeks of paid parental leave.
Relaxed workplace and culture.
Flexible working hours.
Full-time or part-time roles available.
$2000 Learning allowance to help you grow your skills.
Employee Assistance Program.
Can be offered as a Hybrid role - get the best of both worlds and work from our office in Pyrmont (with access to pool and tennis court) and your home office.
Or offered as a fully remote position Australia wide
We believe that a workplace that welcomes a variety of perspectives, experiences, and backgrounds is important and will only push us to be better. We never discriminate on the basis of gender, age, ethnic background, disability status, religion, or sexual orientation.
Show more
Show less","TypeScript, JavaScript, React, C#, SQL, Python, R, Rust, Node.js, Azure, AWS, Kubernetes, d3","typescript, javascript, react, c, sql, python, r, rust, nodejs, azure, aws, kubernetes, d3","aws, azure, c, d3, javascript, kubernetes, nodejs, python, r, react, rust, sql, typescript"
Consultant - Data Engineer,Synpulse8,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/consultant-data-engineer-at-synpulse8-3577494403,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Are you excited about transforming the way we bank by using the latest technology? We are too!
About the job:
Work with clients to solve business problems in fraud, compliance and financial crime and present project results
Manage, transform, and cleanse high volume data
Automate data processing to enable on-going alerts on high-risk activity
Work very closely with data scientists to ensure efficient and effective delivery of solutions
Use leading open source big-data tools, such as Spark, Hadoop, Scala and Elasticsearch. You should be comfortable with working with high profile clients on their sites
Emerging and open-source technologies such as Spark, Hadoop, and Scala
Collaborate on scalability issues involving access to massive amounts of data and information
Guide junior consultants in the best practices of data engineering
About you:
2-6 years of experience working in the Financial Services sector on big data project implementations.
Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch .
Experience of building data processing pipelines for use in production “hands off” batch systems, including either (or preferably both) traditional ETL pipelines and/or analytics pipelines.
Strong coding experience in the likes of Scala or Java.
Coding experience using Python.
Client facing experience, good communication and presentation skills.
Bachelor’s Degree in Computer Science, Physics, Mathematics, or similar degree or equivalent.
Enthusiasm to learn and develop emerging technologies and techniques.
Strong technical communication skills with demonstrable experience of working in rapidly changing client environments.
Demonstrate strong analytical and problem-solving skills and the ability to debug and solve technical challenges with sometimes unfamiliar technologies.
Why join us:
Want to transform what banking means with you!
Are inclusive and diverse
Will guide you
Are committed to creating a flexible, supportive work environment that helps you effectively manage your work and family commitments
Embrace innovate-thinking and entrepreneurship in everything we do
Are award winning and known for our commitment to outcomes
Apply the latest tech and new ways of working
Support your personal growth
Your documents to start the process:
Your link to your git/code repo
A short summary of you
Resume or CV
We look forward to hearing from you soon.
Should you have any queries, feel free to contact our local HR representative.
Show more
Show less","Spark, Hadoop, Scala, Elasticsearch, Java, Python, Batch systems, ETL pipelines, Analytics pipelines, Big data, Data engineering, Data processing, Data cleansing, Data analytics, Emerging technologies, Opensource technologies, Scalability, Problemsolving, Analytical skills, Data science, Clientfacing, Communication skills, Presentation skills","spark, hadoop, scala, elasticsearch, java, python, batch systems, etl pipelines, analytics pipelines, big data, data engineering, data processing, data cleansing, data analytics, emerging technologies, opensource technologies, scalability, problemsolving, analytical skills, data science, clientfacing, communication skills, presentation skills","analytical skills, analytics pipelines, batch systems, big data, clientfacing, communication skills, data engineering, data processing, data science, dataanalytics, datacleaning, elasticsearch, emerging technologies, etl pipelines, hadoop, java, opensource technologies, presentation skills, problemsolving, python, scala, scalability, spark"
Senior Consultant - Databricks and Microsoft Fabric Data & Analytics Engineer,BI & DW Australia,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-consultant-databricks-and-microsoft-fabric-data-analytics-engineer-at-bi-dw-australia-3736841534,2023-12-17,New South Wales, Australia,Mid senior,Hybrid,"Senior Consultant - Databricks and Microsoft Fabric Data & Analytics Engineer
A leading Data & Analytics Professional Services consultancy is looking for two Microsoft Data Engineer Consultants for their practice.
What's In It for You
A competitive permanent salary package with the opportunity to work with large enterprise clients in many verticals including financial services, utilities, government and more!
The role
This role is the epitome of the archetypal IT business consultant where you will apply your proven Databricks and Microsoft Data & Analytics development expertise within top ASX 500 style organisations. You will be working exclusively on high level projects within the Sydney market and will utilise the Microsoft Azure Fabric Services technologies, Synapse, OneLake, Data Factory and Power BI. You will become a key member of the team assisting with the growth of the Microsoft Practice.
The skills needed
The ideal candidate for this true technical consulting role will have some or all of the following experience:
Databricks
Microsoft Platform knowledge of Azure Fabric Azure Data Services, the Microsoft Data cloud stack and on prem Microsoft BI technologies
Power BI experience
Azure Data Factory experience is a huge advantage
Synapse Analytics experience is a huge advantage
Data modelling experience
DBT, Snowflake, AWS, GCP experience all valuable
Ideally previous Professional services experience with niche Data consultancies
Previous on prem SSIS, SSAS (tabular and modular) and SSRS, Power BI experience
High level client liaison skills
Documentation (business focused, technology focused and project focused)
As mentioned salary levels are excellent and remuneration will match your experience.
Show more
Show less","Databricks, Microsoft Azure, Synapse, OneLake, Data Factory, Power BI, DB, Snowflake, AWS, GCP, SSIS, SSAS, SSRS","databricks, microsoft azure, synapse, onelake, data factory, power bi, db, snowflake, aws, gcp, ssis, ssas, ssrs","aws, data factory, databricks, db, gcp, microsoft azure, onelake, powerbi, snowflake, ssas, ssis, ssrs, synapse"
Data Engineer in NYC,Capital Fund Management (CFM),"New York, United States",https://www.linkedin.com/jobs/view/data-engineer-in-nyc-at-capital-fund-management-cfm-3760874412,2023-12-17,Hamilton,United States,Associate,Onsite,"ABOUT CFM
Founded in 1991, we are a global quantitative and systematic asset management firm applying a scientific approach to finance to develop alternative investment strategies that create value for our clients.
We value innovation, dedication, collaboration and the ability to make an impact and together we create an environment for talented and passionate experts in research, technology and business to explore new ideas and challenge assumptions.
ABOUT THE ROLE
As pioneers in scientific, quantitative trading, we explore more and more datasets in order to shape and consolidate our trading decisions. The Data Technology team works closely with Research teams to integrate new datasets and make sure data integration pipelines run smoothly and efficiently.
Data is the fuel that powers our investment strategies: intraday price dynamics are used to better our investment decisions; buy/sell intents of market participants help to trade at the best available price…
You will join our New York office.
Base salary 105-170 K-USD
+ Bonus
Your main responsibility will be to make sure that they have all the data and tools they need to improve our trading strategies, including:
Integration of ‘trial’ datasets
Development & monitoring of data pipelines
Technical support to quant researchers
Contribution to our platform tooling
We choose between python and C++ for our developments based on the technical requirements with occasional Linux bash scripting and SQL. The job will involve a diverse set of tasks, including collaboration with quant research, so there will be plenty opportunities to grow.
SKILLSET REQUIREMENTS/QUALIFICATIONS
You have a Master Degree from an engineering school or university equivalent
You have at least 3 years experience in data analysis or similar role
You have a working visa allowing you to work in the USA
You have experience in python
You have an ability to work on Linux environment
You are a team player and have good communication skills
You are interested in technology
You are fluent in written & spoken English
Your “plusses”
C++ not required but is a plus
Knowledge of French is a plus
EQUAL OPPORTUNITIES STATEMENT
We are continuously striving to be an equal opportunity employer and we prohibit any discrimination based on sex, disability, origin, sexual orientation, gender identity, age, race, or religion. We believe that our diversity, breadth of experience, and multiple points of view are among the leading factors in our success.
CFM is a signatory of the Women Empowerment Principles
FOLLOW US
Follow us on Twitter and LinkedIn or
visit our website to find out more about CFM.
Show more
Show less","Python, C++, SQL, Linux, Data Analysis, Data Integration, Data Pipelines, Quantitative Trading, Bash Scripting","python, c, sql, linux, data analysis, data integration, data pipelines, quantitative trading, bash scripting","bash scripting, c, data integration, dataanalytics, datapipeline, linux, python, quantitative trading, sql"
Distributed Systems Engineer - Analytical Database Platform,Cloudflare,"New York, United States",https://www.linkedin.com/jobs/view/distributed-systems-engineer-analytical-database-platform-at-cloudflare-3776255537,2023-12-17,Hamilton,United States,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
About Role
We are looking for an experienced and highly motivated engineer to join our team and contribute to our analytical database platform. The platform is a critical component of Cloudflare Analytics which provides real-time visibility into the health and performance of Cloudflare customers' online properties.
The team builds and maintains a high-performance, scalable database platform powered by ClickHouse, optimized for analytical workloads. We help our customers, both internal and external, to gain a deeper understanding of their online properties, identify trends and patterns, and make informed decisions about how to optimize their web performance, security, and other key metrics. Our mission is to empower customers to leverage their data to drive better outcomes for their business.
As a Distributed systems engineer - Analytical Database Platform, you will:
Develop and implement new platform components for the Cloudflare Analytical Database Platform to improve functionality and performance.
Add more database clusters to accommodate the growing volume of data generated by Cloudflare products and services.
Monitor and maintain the performance and reliability of existing database platform clusters, and identify and troubleshoot any issues that may arise.
Work to identify and remove bottlenecks within the analytics database platform, including optimizing query performance and streamlining data ingestion processes.
Collaborate with the ClickHouse open-source community to add new features and functionality to the database, as well as contribute to the development of the upstream codebase.
Collaborate with other teams across Cloudflare to understand their data needs and build solutions that empower them to make data-driven decisions.
Participate in the development of the next generation of the database platform engine, including researching and evaluating new technologies and approaches that can improve the database's performance and scalability.
Key qualifications:
3+ years of experience working in software development covering distributed systems, and databases.
Strong programming skills (C++ is preferable), as well as a deep understanding of software development best practices and principles.
Strong knowledge of SQL and database internals, including experience with database design, optimization, and performance tuning.
A solid foundation in computer science, including algorithms, data structures, distributed systems, and concurrency.
Ability to work collaboratively in a team environment, as well as communicate effectively with other teams across Cloudflare.
Strong analytical and problem-solving skills, as well as the ability to work independently and proactively identify and solve issues.
Experience with ClickHouse is a plus.
Experience with SALT or Terraform is a plus.
Experience with Linux container technologies, such as Docker and Kubernetes, is a plus.
If you're passionate about building scalable and performant databases using cutting-edge technologies, and want to work with a world-class team of engineers, then we want to hear from you! Join us in our mission to help build a better internet for everyone!
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Distributed systems, Databases, C++, Software development, SQL, Database internals, Algorithms, Data structures, Concurrency, ClickHouse, SALT, Terraform, Docker, Kubernetes, Linux container technologies","distributed systems, databases, c, software development, sql, database internals, algorithms, data structures, concurrency, clickhouse, salt, terraform, docker, kubernetes, linux container technologies","algorithms, c, clickhouse, concurrency, data structures, database internals, databases, distributed systems, docker, kubernetes, linux container technologies, salt, software development, sql, terraform"
Software Engineer - Data Infrastructure (Senior),Clear Street,"New York, United States",https://www.linkedin.com/jobs/view/software-engineer-data-infrastructure-senior-at-clear-street-3754730791,2023-12-17,Hamilton,United States,Mid senior,Onsite,"About Clear Street:
Clear Street is building modern infrastructure for capital markets. Founded in 2018 by top Wall Street and Silicon Valley veterans, Clear Street is an independent, non-bank prime broker designed to solve the industry’s most neglected problem: legacy technology.
We have built a proprietary, cloud-native clearing and custody system from the ground floor to replace the outdated infrastructure used across capital markets. Our platform is fully integrated with central clearing houses and exchanges to support billions in trading volume per day. We’ve agonized about our data model abstractions, created horizontal scalability, and crafted thoughtful APIs. All so we can provide a best-in-class experience for our clients.
By combining highly-skilled product and engineering talent with seasoned finance professionals, we’re building the essentials to compete in today’s fast-paced markets.
The Role:
As an experienced Data Engineer on our Data Infrastructure team, you will play an integral role in the design and execution of our shared data tooling and infrastructure. You will create reusable data platform elements and tools that improve the way engineering teams transform data. As a voice of experience in the team, you will help mentor teammates, evolve our technical standards and best practices, and further our culture of system designs and data pipeline architecture.
The Team:
You’ll work horizontally across the engineering organization to determine what data pipeline problems people have, how they are solving them, and how those solutions could generalize across the firm. You will partner with others across the organization to understand complex data-related issues and seek effective compromises.
Requirements:
You have 8+ years of experience designing and architecting systems that deliver solutions to complex data problems.
You are a data modeling pro; you understand how to create unified definitions of types from different source data representations.
You have experience with data science and statistical analysis methods, especially related to financial domains.
You communicate technical ideas with ease and always look to collaborate to deliver high quality products.
Your experience will help you mentor team members, define our engineering standards, and drive a system design approach to building new services.
Data Infrastructure Team Stack:
Python, Snowflake, Argo, Kafka, Docker, Kubernetes
We offer
:
The opportunity to join a small and growing team of good people, where you can make a difference.
A new, high-quality code base with little technical debt and room to build new services and features.
An environment that embraces the utility of a DevOps oriented culture and combines it with a focus on CI/CD methodology.
A meritocratic philosophy that champions collaboration.
Competitive compensation, benefits, and perks.
The Base Salary Range for this role is $170,000 - $240,000. This range is representative of the starting base salaries for this role at Clear Street. Where a candidate falls in this range will be based on job related factors such as relevant experience, skills, and location. This range represents Base Salary only, which is just one element of Clear Street's total compensation. The range stated does not include other factors of total compensation such as bonuses or equity.
At Clear Street, we offer competitive compensation packages, company equity, 401k matching, gender neutral parental leave, and full medical, dental and vision insurance. Our belief has always been that we are better as a business when we are all together in person. As such, beginning on January 2, 2023, we are requiring employees to be in the office 4 days per week. In-office benefits include lunch stipends, fully stocked kitchens, happy hours, a great location, and amazing views.
Our top priority is our people. We’re continuously investing in a culture that promotes collaboration. We help each other through challenges and celebrate each other's successes. We believe that modern workplaces succeed by virtue of having high-performance workforces that are diverse — in ideas, in cultures, and in experiences. We put in the effort to make such a workplace a daily reality and are proud to be an equal opportunity employer.
Show more
Show less","Python, Snowflake, Argo, Kafka, Docker, Kubernetes, DevOps, CI/CD, Data modeling, Data science, Statistical analysis, Financial domains","python, snowflake, argo, kafka, docker, kubernetes, devops, cicd, data modeling, data science, statistical analysis, financial domains","argo, cicd, data science, datamodeling, devops, docker, financial domains, kafka, kubernetes, python, snowflake, statistical analysis"
Sr. Data Analyst,257,"New York, United States",https://www.linkedin.com/jobs/view/sr-data-analyst-at-257-3785867852,2023-12-17,Hamilton,United States,Mid senior,Onsite,"We are seeking a Senior Data Analyst to join our growing startup, focusing on leveraging data insights to drive business decisions for 257 and our clients. This role involves advanced data analysis, visualization, and client-facing interactions. This role is ideal for someone who thrives in an innovative and fast-paced environment with ambitious goals and few rules.
Responsibilities
Data Analysis - conduct data analysis that drive actionable insights for 257 and its clients.
Data Visualization - Create compelling visualizations of these analyses
Story Telling - Write reports, write emails and conduct meetings that tell clear, concise and persuasive stories around the underlying data. Ensure digestibility by non-technical stakeholders
Client Engagement - Engage with clients to understand their business, and their spoken and latent requirements, and then answer these needs with insightful reports
Efficiency Seeking - Identify opportunities to make your work repeatable across many project and client engagements. Streamline reporting processes for efficiency
Collaboration - Work closely with data science, data engineering and business teams. Communicate clearly and thoughtfully across email, Slack and phone
Curiosity - Stay up-to-date with energy industry trends, new data initiatives, and emerging technologies in the data analysis field
Requirements
Bachelor’s or Master’s degree in a technical field (CS, math, statistics, economics, etc.)
Proven experience in data analysis. Strong problem-solving skills
Proficient in SQL is a must and knowledge in Python/R and cloud platforms in a plus.
Experience with BI / visualization tools such as Quicksight, Tableau, etc
Experience in client-facing roles, with excellent communication skills
Ability to distill complex concepts for non-technical stakeholders
Willingness to provide blinded/anonymized samples of prior work and do a case study
Strong reference from someone known to 257
Experience in energy, prop tech or building sciences is a big plus
Why Join Us?
Fascinating technical challenge - wrangling large data sets, identifying compelling themes
Interesting business challenge - teaching the energy industry to leverage data
Impactful - a chance to accelerate residential electrification (~40% of all US emissions)
Great team culture - high empowerment, low politics, straight-talking.
Career and personal growth - be on the ground floor of something BIG
Benefits
Health Care Plan (Medical, Dental & Vision)
Retirement Plan (401k, IRA)
Paid Time Off (Vacation, Sick & Public Holidays)
Stock Option Plan
Show more
Show less","Data Analysis, Data Visualization, Story Telling, Client Engagement, Efficiency Seeking, Collaboration, Curiosity, SQL, Python, R, Cloud Platforms, BI / Visualization Tools, Quicksight, Tableau, Communication Skills, Energy Industry, Prop Tech, Building Sciences","data analysis, data visualization, story telling, client engagement, efficiency seeking, collaboration, curiosity, sql, python, r, cloud platforms, bi visualization tools, quicksight, tableau, communication skills, energy industry, prop tech, building sciences","bi visualization tools, building sciences, client engagement, cloud platforms, collaboration, communication skills, curiosity, dataanalytics, efficiency seeking, energy industry, prop tech, python, quicksight, r, sql, story telling, tableau, visualization"
Data Centre network Engineer,Zealogics Inc,"New York, United States",https://www.linkedin.com/jobs/view/data-centre-network-engineer-at-zealogics-inc-3787740055,2023-12-17,Hamilton,United States,Mid senior,Onsite,"Skills Required
The ideal candidate will have 5-7 years of experience with a Financial Services Firm, Technology vendor or systems integrator designing, and implementing large scale networks
Expert Level Technical Skills Required
Ethernet technologies: SPT, 802.1Q, VPC, Multilayer Switching; Leaf-Spine IP Fabric
IP Routing: RIP, OSPF, BGP, MPBGP, TCP/IP
Multicast routing: PIM, IGMP, MSDP
VxLAN
Platform knowledge on Cisco Switching & Routing Platforms
Nexus 9000 Series Data Centre Switching PlatformoASR 1000x Series Routers
ASR 9000 Series Routers
Platform Knowledge on Arista Switching platforms
Working knowledge on various Network OS
Cisco IOS, IOS-XE, NXOS, IOS-XR
Arista EOS
Skills Desired:
Ability to use Microsoft applications such as Visio, Word, Excel and Outlook
Proficient in UNIX or Linux
Certifications: CCIE or equivalent
Experience with scripting / automation using Python / Ansible is a strong plus.
Tools: DNA Center, Wireshark, Infoblox, HPNA, Splunk, Sevone, Ansible, Netconf is a plus
WAN technologies: SONET/SDH, MPLS, Classic TDM
Working knowledge on SDN, SDWAN, DMVPN
Sound knowledge in public cloud, Kubernetes, storage over IP and VMWare
Strong sense of security concept include defense in depth, zero trust networking, least privilege principle, risks and controls.
Additionally, the ideal candidate should be
Flexible and adaptable to meet the team’s targets
Honest, hardworking, and reliable
Possess a strong sense of ownership/accountability
Rate Range -$49-$54
Powered by JazzHR
21thAbpEgL
Show more
Show less","Ethernet technologies, IP Routing, Multicast routing, VxLAN, Cisco Switching & Routing Platforms, Arista Switching platforms, Network OS, Microsoft applications, UNIX or Linux, CCIE, Scripting / automation, Python, Ansible, DNA Center, Wireshark, Infoblox, HPNA, Splunk, Sevone, Netconf, WAN technologies, SDN, SDWAN, DMVPN, Public cloud, Kubernetes, Storage over IP, VMWare, Security concept, Defense in depth, Zero trust networking, Least privilege principle","ethernet technologies, ip routing, multicast routing, vxlan, cisco switching routing platforms, arista switching platforms, network os, microsoft applications, unix or linux, ccie, scripting automation, python, ansible, dna center, wireshark, infoblox, hpna, splunk, sevone, netconf, wan technologies, sdn, sdwan, dmvpn, public cloud, kubernetes, storage over ip, vmware, security concept, defense in depth, zero trust networking, least privilege principle","ansible, arista switching platforms, ccie, cisco switching routing platforms, defense in depth, dmvpn, dna center, ethernet technologies, hpna, infoblox, ip routing, kubernetes, least privilege principle, microsoft applications, multicast routing, netconf, network os, public cloud, python, scripting automation, sdn, sdwan, security concept, sevone, splunk, storage over ip, unix or linux, vmware, vxlan, wan technologies, wireshark, zero trust networking"
"Backend Engineer, Data Privacy Technologies",Stripe,"New York, United States",https://www.linkedin.com/jobs/view/backend-engineer-data-privacy-technologies-at-stripe-3765544673,2023-12-17,Hamilton,United States,Mid senior,Onsite,"Who we are
About Stripe
Stripe is a financial infrastructure platform for businesses. Millions of companies—from the world’s largest enterprises to the most ambitious startups—use Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Our mission is to increase the GDP of the internet, and we have a staggering amount of work ahead. That means you have an unprecedented opportunity to put the global economy within everyone’s reach while doing the most important work of your career.
About The Team
Stripe will succeed at our mission of increasing the GDP of the internet only if we prove ourselves worthy of our users’ trust. Software engineers on the Data Privacy Technologies team do work that is a critical part of accomplishing this mission by abstracting away data security and privacy concerns for Stripe and its users.
What you’ll do
The Data Privacy Technologies team builds systems and tools to allow Stripe to deeply reason about and protect its data, at scale. As an engineering team, we leverage data classification, machine learning as well as pseudonymization techniques such as tokenization, redaction, filtering, and masking.
Responsibilities
Design, build, and operate core infrastructure (e.g., APIs, services and frameworks) and tooling used by all of Stripe’s engineering teams, for example to annotate and automatically de-identify sensitive data
Improve engineering standards and processes
Make impactful decisions about systems, productivity, security and privacy — their edge cases, failure modes and tradeoffs
Collaborate closely with other engineering teams and cross-functionally with legal, product, compliance and operations teams to embed best practices for data protection into how products and infrastructure are built
Who you are
We’re looking for someone who meets the minimum requirements to be considered for the role. If you meet these requirements, you are encouraged to apply. The preferred qualifications are a bonus, not a requirement.
Minimum Requirements
2+ years software engineering experience in a high-stakes production environment
Empathy, excellent communication skills and a deep respect for the power of collaboration
A learning mindset, regardless of level or experience
The ability to think creatively and holistically about reducing risk in a complex, fast-changing environment
The ability to drive next steps when encountering ambiguous problems without clear ownership
Preferred Qualifications
Data platform engineering experience
Machine learning experience
Full stack experience
Privacy and/or security experience
Pay and benefits
The annual US base salary range for this role is $146,500 - $198,100. For sales roles, the range provided is the role’s On Target Earnings (""OTE"") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. This salary range may be inclusive of several career levels at Stripe and will be narrowed during the interview process based on a number of factors, including the candidate’s experience, qualifications, and location. Applicants interested in this role and who are not located in the US may request the annual salary range for their location during the interview process.
Additional benefits for this role may include: equity, company bonus or sales commissions/bonuses; 401(k) plan; medical, dental, and vision benefits; and wellness stipends.
Show more
Show less","Software engineering, Data classification, Machine learning, Tokenization, Redaction, Filtering, Masking, API, Web services, Frameworks, Infrastructure tooling, Full stack, Data platform engineering, Privacy, Security","software engineering, data classification, machine learning, tokenization, redaction, filtering, masking, api, web services, frameworks, infrastructure tooling, full stack, data platform engineering, privacy, security","api, data classification, data platform engineering, filtering, frameworks, full stack, infrastructure tooling, machine learning, masking, privacy, redaction, security, software engineering, tokenization, web services"
"Backend / API Engineer, IAM Data Plane",Stripe,"New York, United States",https://www.linkedin.com/jobs/view/backend-api-engineer-iam-data-plane-at-stripe-3776520012,2023-12-17,Hamilton,United States,Mid senior,Onsite,"Who we are
About Stripe
Stripe is a financial infrastructure platform for businesses. Millions of companies—from the world’s largest enterprises to the most ambitious startups—use Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Our mission is to increase the GDP of the internet, and we have a staggering amount of work ahead. That means you have an unprecedented opportunity to put the global economy within everyone’s reach while doing the most important work of your career.
About The Team
Stripe’s Identity and Access Management (IAM) team builds innovative solutions for Stripe customers to confidently manage access to their data and resources. IAM Data Plane, part of Identity Access Management platform, is at the front-line for building trust with Stripe customers. We do this by ensuring correct access to merchant data and maintaining a highly available and performant infrastructure. The team is responsible for ensuring that our user’s data is safe and protected and allowing only appropriately authenticated requests.
This role is critical for Stripe and its customers. Our systems in IAM power all of Stripe and ensure strict standards for Authentication and Authorization throughout the product and the impact of our work cannot be understated. IAM engineers operate load bearing dependencies on every request to Stripe, and on a long chain of trust that our users rely on.
What you’ll do
As an engineer on the team you will have the opportunity to solve a broad range of challenging problems at scale. You will partner with many other teams at Stripe, building and designing solutions that are scalable to power Stripe authentication and authorization needs.
Responsibilities
Build new infrastructure, services, and features that operate at scale for serving Stripe request authentication and authorization needs
Ensure that our services are scalable, extensible, reliable, and performant which meet SLAs for our users.
Have a broad scope of influence, with frequent collaboration across teams and functions
Develop and execute against both short- and long-term roadmaps. Make effective tradeoffs that consider business priorities, user experience, and a sustainable technical foundation
Uphold our high engineering standards, and elevate quality and engineering efficiency within our codebase and processes
Who you are
We’re looking for someone who meets the minimum requirements to be considered for the role. If you meet these requirements, you are encouraged to apply. The preferred qualifications are a bonus, not a requirement.
Minimum Requirements
Have a strong technical background, including 2+ years of experience designing and engineering large-scale systems
Have experience with distributed computing systems
Prefer simple solutions and designs over complex ones, and have a good intuition for what will last and scale
Attention to technical detail and the nitty gritty
Experience writing high quality code in a major programming language (ideally Java)
Attention to detail, especially around performance, accessibility, reliability, and security
Preferred Requirements
Experience with building distributed computing systems
Experience with Identity Access Management systems
Experience optimizing the end to end performance of distributed systems
You prefer simple solutions and designs over complex ones, and have a good intuition for what is lasting and scalable
You enjoy working with a diverse group of people with different areas of expertise. Engineering works closely with a variety of teams from Security to mobile and product teams.
Pay and benefits
The annual US base salary range for this role is $157,100 - $212,500. For sales roles, the range provided is the role’s On Target Earnings (""OTE"") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. This salary range may be inclusive of several career levels at Stripe and will be narrowed during the interview process based on a number of factors, including the candidate’s experience, qualifications, and location. Applicants interested in this role and who are not located in the US may request the annual salary range for their location during the interview process.
Additional benefits for this role may include: equity, company bonus or sales commissions/bonuses; 401(k) plan; medical, dental, and vision benefits; and wellness stipends.
Show more
Show less","Software Design, Engineering, Distributed Computing Systems, Authentication, Authorization, Scalability, Performance, Java, Systems Engineering, Identity Access Management","software design, engineering, distributed computing systems, authentication, authorization, scalability, performance, java, systems engineering, identity access management","authentication, authorization, distributed computing systems, engineering, identity access management, java, performance, scalability, software design, systems engineering"
"Quant Engineer, Sophisticated Data Problems",Thurn Partners,"New York, United States",https://www.linkedin.com/jobs/view/quant-engineer-sophisticated-data-problems-at-thurn-partners-3759043986,2023-12-17,Hamilton,United States,Mid senior,Onsite,"The Firm and Project
This quant trading shop has a culture that emulates that of a tech/fintech business and offers complete visibility over every facet of their practice. This is to say: no one is siloed to one specific function/widget of the ecosystem but can see all IP across the business. Where in other buy-side businesses 'glass walls' would prevent interaction between those who sit on two different Equities or Futures pods, here you can chat to whoever you like.
Still streamlined with ~100 headcount, they are growing in a measured fashion, and hire smart technologists who are intrigued by the most sophisticated quantitative and computational problems in industry.
They are constantly seeking alpha and datasets untapped by competitors, and leverage both fundamental and alternative data to build their world-class models. To enhance this process, they are seeking an engineer to design and build a hybrid (cloud/on prem) quant research data platform from the ground in modern Python.
The Role
Design and build scalable and performant software from the ground up for data integration, validation and monitoring.
Build ETL Pipelines and work on complex caching problems.
Partner with Researchers and ML scientists to implement widely varied datasets: from tick-by-tick to alternative datasets to event-based data..
Required Skills/Experience
Degree in Computer Science, Electrical Engineering or similar - ideally from top institution.
4+ years experience working as a Software Engineer on data-intensive platforms.
Python expertise also ideal if not required.
Knowledge and expertise in the Cloud.
Pre-Application
Please do not apply if you're looking for a contract or remote work.
You must be eligible to live and work in the US, without requiring sponsorship.
Please ensure you meet the required experience section prior to applying.
Allow 1-5 working days for a response to any job enquiry.
Your application is subject to our privacy policy, found here: https://www.thurnpartners.com/privacy-policy
Show more
Show less","Python, Cloud computing, Software engineering, Data integration, Data validation, Data monitoring, ETL pipelines, Caching, Machine learning, Data science, Tickbytick data, Alternative data, Eventbased data","python, cloud computing, software engineering, data integration, data validation, data monitoring, etl pipelines, caching, machine learning, data science, tickbytick data, alternative data, eventbased data","alternative data, caching, cloud computing, data integration, data monitoring, data science, data validation, etl pipelines, eventbased data, machine learning, python, software engineering, tickbytick data"
Staff Software Engineer - Data Engineer,Character.AI,"New York, United States",https://www.linkedin.com/jobs/view/staff-software-engineer-data-engineer-at-character-ai-3742681633,2023-12-17,Hamilton,United States,Mid senior,Onsite,"About Us
Character’s mission is to give everyone on earth access to their own deeply personalized superintelligence that helps them live their best lives.
Achieving our mission will require solving ambitious technical challenges, including engineering, research, and design, and we are assembling a world-class team to do so. Our founding team includes AI pioneers from Google Brain and Meta Research whose research has led to major breakthroughs in natural language understanding and dialog applications such as Transformers and Google LaMDA.
Check out our beta to get a glimpse into the future.
About The Role
Responsibilities:
As a Staff Software Engineer - Data Engineering at Character, you'll have the opportunity to work on a diverse and exciting set of projects spanning both software engineering and data engineering domains. Your initial focus will be on three key areas:
Activate our ""data flywheel"": Develop data pipelines and collaborate with ML researchers to ensure continuous improvement of our AI models and alignment with human feedback.
Evolve our A/B testing framework: Conduct a thorough analysis of our current A/B testing framework and prepare it for the next generation of data-driven product growth.
Enable product team insights: Design, implement, and maintain a robust data warehousing solution for consistent and reliable reporting and ease of data exploration as we continue to better understand our users.
In addition, you may work on optimizing database and application performance, cleaning pre-training datasets, and contributing to service oriented architecture alongside the Staff Software Engineer - Application.
Requirements:
B.A.S. in Computer Science or equivalent experience.
4+ years minimum experience in data engineering within a consumer-facing technology company
Experience building data warehousing and data pipelines with Big Query, dbt, Ray, Beam and Spark.
Experience designing, implementing and supporting end-to-end A/B tests
Experiencing managing Elasticsearch, Spanner, Postgres
In-depth experience building and managing infrastructure in a cloud environment (esp. GCP)
Strong Python and SQL experience
Desired Experience:
Experience tuning databases and optimizing query performance
Experience setting up containerization with Docker and Kubernetes
Familiarity with PyTorch or JAX
Excellent problem-solving skills and the ability to work independently and as part of a team
Strong communication and collaboration skills
Experience writing and maintaining Golang and Terraform code
You will be a good fit if you are proactive and have a “get things done” mindset. Given our current pace of growth and load on our systems, most people have had a significant impact during their first week at the company.
Character is an equal opportunity employer and does not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, veteran status, disability or any other legally protected status. We value diversity and encourage applicants from a range of backgrounds to apply.
Show more
Show less","Python, SQL, Big Query, dbt, Ray, Beam, Spark, Elasticsearch, Spanner, Postgres, GCP, Docker, Kubernetes, PyTorch, JAX, Golang, Terraform, Data warehousing, Data pipelines, A/B testing, Infrastructure management, Cloud computing","python, sql, big query, dbt, ray, beam, spark, elasticsearch, spanner, postgres, gcp, docker, kubernetes, pytorch, jax, golang, terraform, data warehousing, data pipelines, ab testing, infrastructure management, cloud computing","ab testing, beam, big query, cloud computing, datapipeline, datawarehouse, dbt, docker, elasticsearch, gcp, golang, infrastructure management, jax, kubernetes, postgres, python, pytorch, ray, spanner, spark, sql, terraform"
Senior Software Engineer (Data Strategy),Skill Inventions Inc,"New York, United States",https://www.linkedin.com/jobs/view/senior-software-engineer-data-strategy-at-skill-inventions-inc-3782892029,2023-12-17,Hamilton,United States,Mid senior,Onsite,"Title: Senior Software Engineer (Data Strategy)
Location: NYC, NY(Onsite)
Full time
Responsibilities
Work with a product manager as technical lead of a team of ~5 engineers, data scientists, and analysts to design, scope, and oversee work in an Agile environment.
Manage junior data and web engineers, focusing on productivity, quality, and professional development.
Partner with the head of Data Strategy and other senior engineers to create and evangelize best-in-class engineering competency and tooling within the organization.
Enforce strong development standards across the team through code reviews, automated testing, and monitoring.
Establish strong relationships with internal clients as an engineering representative for data strategy.
Contribute to the overall Data Strategy vision and execution via quarterly planning and executive committee reporting.
Partner regularly improving engineering recruiting process for the required skillsets and resourcing demands.
Learn the complex business of reinsurance to coach data technologists and execute the team's initiatives more effectively.
Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting, and new data products.
Innovate new ways to leverage large and small datasets to drive revenue via the development of new products with the Data Strategy team, as well as the enhancement of existing products.
Architect engineering solutions using the latest cloud technologies in a process that spans hypothesis-validating prototypes to large-scale production data products, ensuring internal security and regulatory compliance.
Design solutions that account for unstructured data and document management system(s), including ingesting, tracking, parsing, analyzing, and summarizing documents at scale.
Perform exploratory and goal-oriented data analyses to understand and validate the requirements of data products and help create product roadmaps.
Develop, implement, and deploy front-ends and APIs, which may involve business intelligence dashboards, data pipelines, machine learning algorithms, and file ingestion mechanisms.
Qualifications
5-8+ years of relevant experience in data-focused software engineering
Master's Degree or Ph.D. in data science, computer science, or related quantitative field such as applied mathematics, statistics, engineering or operations research, or equivalent experience.
Experience working with Python-based server-side web frameworks like FastAPI or Django
Strong knowledge of SQL and familiarity with the high-level properties of modern data stores.
Strong understanding of the contemporary SDLC, including dev/QC/prod environments, unit/integration/UA testing, CI/CD, etc.
Experience building and maintaining CI/CD pipelines with tools such as Azure DevOps, GitLab, Travis, Jenkins, etc.
2+ years of data analysis, AI, or data science work.
Experience with data cleaning, enrichment, and reporting to business users.
Extensive experience with (py)Spark, Python, JSON, and SQL.
Experience integrating data from semi-structured and unstructured sources.
Knowledge of various industry-leading SQL and NoSQL database systems.
Experience with or strong interest in learning about LLMs in a productized context.
Additional Qualifications
Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks.
Experience with web scraping and crowdsourcing technologies.
Experience with Databricks and optimizing Spark clusters.
Experience architecting web ecosystems from the ground up, including monolith vs. microservice decisions, caching technologies, security integrations, etc.
Experience working with data visualization dashboarding tools (PowerBI, Tableau).
Insurance domain knowledge or strong interest in developing it.
Experience with the MS Azure cloud environment.
Show more
Show less","Python, FastAPI, Django, SQL, SDLC, CI/CD, Azure DevOps, GitLab, Travis, Jenkins, Spark, JSON, NoSQL, LLMs, Entity resolution, Streaming technologies, ELT/ETL frameworks, Web scraping, Crowdsourcing technologies, Databricks, Microservices, Caching technologies, Data visualization, PowerBI, Tableau, Azure","python, fastapi, django, sql, sdlc, cicd, azure devops, gitlab, travis, jenkins, spark, json, nosql, llms, entity resolution, streaming technologies, eltetl frameworks, web scraping, crowdsourcing technologies, databricks, microservices, caching technologies, data visualization, powerbi, tableau, azure","azure, azure devops, caching technologies, cicd, crowdsourcing technologies, databricks, django, eltetl frameworks, entity resolution, fastapi, gitlab, jenkins, json, llms, microservices, nosql, powerbi, python, sdlc, spark, sql, streaming technologies, tableau, travis, visualization, web scraping"
Lead Data Engineer,Alldus,"New York, United States",https://www.linkedin.com/jobs/view/lead-data-engineer-at-alldus-3772048522,2023-12-17,Hamilton,United States,Mid senior,Onsite,"Our client helps companies ensure the security of their personnel and operations with detailed, street-level threat intelligence. Utilizing AI to aggregate public and proprietary information, they have developed the world’s largest dataset of global threat patterns. Their visualization platform assists companies in conducting geolocation-based analysis for travel safety, threat intelligence, crisis management, site selection, expansion and more.
Responsibilities
Create, test, and sustain strong data pipelines, incorporating NLP models for extracting and enriching text-based data.
Formulate methods for data transformation, validation, and analysis to maximize data utility.
Develop a data warehousing strategy for an enriched data lifecycle.
Establish operational data quality, ensuring prompt issue resolution.
Contribute to GIS architecture strategies and lead resultant transformation initiatives.
Skillset
Minimum of 5 years of crafting operational data pipelines and creating associated platforms.
Vast experience in implementing and overseeing data warehouses, models, and executing schema changes for over 5 years.
Proficiency in initiating warehouses from inception and utilizing AWS tools like Redshift and S3 is beneficial.
Deep grasp of SQL, data models, and executing schema changes.
Exhibited leadership in operationalizing data quality.
Proven history of producing Python code ready for production, with knowledge of the Pandas framework.
Benefits
Full time remote working (US Based only)
Salary: TBC with client
If this sounds like the role for you, upload your resume via the 'Apply Now’ link below or send your resume directly to angelo@alldus.com for consideration.
40142
Show more
Show less","Data Pipelines, NLP, Data Transformation, Data Analysis, Data Warehousing, Data Quality, GIS Architecture, SQL, Schema Changes, Python, Pandas","data pipelines, nlp, data transformation, data analysis, data warehousing, data quality, gis architecture, sql, schema changes, python, pandas","data quality, data transformation, dataanalytics, datapipeline, datawarehouse, gis architecture, nlp, pandas, python, schema changes, sql"
Data Developer,Michael Page,"New York, United States",https://www.linkedin.com/jobs/view/data-developer-at-michael-page-3744355401,2023-12-17,Hamilton,United States,Mid senior,Onsite,"Competitive Compensation Package and Hybrid Position|Professional Growth and Leadership Opportunities
About Our Client
The client is a leading Investment Management firm in New York, NY. The role will be on site in a hybrid capacity.
Job Description
Lead in designing and developing enterprise solutions for ESG Investment Team, Central Research Team, and Investment Teams.
Collaborate closely with project team members, business stakeholders, and technology experts to understand business problems and existing processes.
Take charge of architecture and solution design, ensuring successful implementation of technology solutions.
Manage developer calls and backlog for application development, ensuring efficient coordination and prioritization of tasks.
Conduct ad-hoc analysis and present results in a clear and concise manner, providing valuable insights to support decision-making processes.
MPI does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity or expression, national origin, age, disability, veteran status, marital status, or based on an individual's status in any group or class protected by applicable federal, state or local law. MPI encourages applications from minorities, women, the disabled, protected veterans and all other qualified applicants.
The Successful Applicant
Must have:
Python and SQL skills
Familiarity with cloud based solutions - Snowflake preferred
Experience with financial data
7-12 YOE
What's on Offer
Base compensation is between $140,000 to $200,000. Bonus and benefits will be included in a total compensation package.
Apply today using the link, and your resume will be reviewed in 24 hours.
Contact: Caitlin Dye
Quote job ref: JN-092023-6189133
Show more
Show less","Python, SQL, Snowflake, Cloud computing, Financial data, Software development, Project management, Data analysis, Decisionmaking","python, sql, snowflake, cloud computing, financial data, software development, project management, data analysis, decisionmaking","cloud computing, dataanalytics, decisionmaking, financial data, project management, python, snowflake, software development, sql"
Senior Data Engineer,The Trade Desk,"New York, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-the-trade-desk-3781943039,2023-12-17,Hamilton,United States,Mid senior,Onsite,"The Trade Desk is a global technology company with a mission to create a better, more open internet for everyone through principled, intelligent advertising. Handling over 1 trillion queries per day, our platform operates at an unprecedented scale. We have also built something even stronger and more valuable: an award-winning culture based on trust, ownership, empathy, and collaboration. We value the unique experiences and perspectives that each person brings to The Trade Desk, and we are committed to fostering inclusive spaces where everyone can bring their authentic selves to work every day.
Do you have a passion for solving hard problems at scale? Are you eager to join a dynamic, globally- connected team where your contributions will make a meaningful difference in building a better media ecosystem? Come and see why Fortune magazine consistently ranks The Trade Desk among the best small- to medium-sized workplaces globally.
What we do:
Our data engineers are end-to-end owners who have the opportunity to participate in many aspects of designing, building, and delivering data-focused products for our stakeholders.
We believe in building end to end modeling pipelines that have a core focus on meeting core KPI’s and being able to validate these KPI’s. We work very closely with data scientists to evaluate modelling solutions up front from both a runtime, accuracy, and reproducibility perspective. Additionally, we prioritize frequent collaboration with data science, product, and other engineering teams to ensure that pipelines and tooling we build are meaningful, scalable, and pleasant to use.
Our system performs every day, 24/7, serving global traffic. We build a distributed system in a highly collaborative environment, utilizing a broad range of technologies. Our data engineers work on finding solutions to algorithmic, optimization, and scale challenges in everything we do.
What you’ll do:
You will work with data scientists, ML pipelines, data processing automation, data processing pipelines, model deployments and experimentation configuration, data quality, data warehousing, data privacy and governance – to name a few.
You will solve petabyte-scale data challenges across large-scale distributed systems coordinating thousands of servers in cloud and physical data centers around the world.
Senior Engineers contribute to more than our product – they build up our team. Through a combination of mentoring, technical leadership, and/or direct management of small teams, they make others better and raise the bar for those around them.
Our Senior Data Engineers are end-to-end owners. You will participate actively in all aspects of designing, building, and delivering data products for our clients.
Who you are:
You have a sustained track record of making significant, self-directed, and end-to-end contributions to large, impactful data engineering projects. You think beyond just the task at hand to deeply understand the ‘why’ behind what you are doing.
You understand engineering fundamentals. On our scale, many off-the-shelf techniques and existing technologies (open source and enterprise) do not work. You can work from first principles to evaluate solutions and adapt them to a unique environment.
You have experience in data engineering.
You have experience working with data scientists and understand and empathize with their needs and comfort levels.
You can think through model lifecycles end to end. This covers evaluating model feasibility - both runtime performance and from an accuracy POV; thinking of validation up front and challenging if the validation processes are sound; and model pipeline from training to deployments to setting up experiments.
You have worked with data modeling, configured AB test experimentation, and have experience working in technologies like SQL, ETL, and technologies similar to Spark, Airflow, SQL, data warehousing, etc...
You have experience with application deployments using tools analogous to Docker, Kubernetes, etc…
Bonus points for if you have experience working with deep learning models such as Tensorflow, Pytorch etc...
Experience with high level languages such as C#, Java, Python, Scala
You are a broadly skilled engineer, accustomed to developing web services. You have experience building always-on systems, working across a variety of technologies and service layers.
You have a product-focused mindset. You have the passion and ability to contribute to the process of discovering what will delight our clients and push forward one of the world’s largest and most influential industries toward a vision of openness, transparency, and evidence-based decision-making.
You work with confidence and without ego. Our engineers have deep knowledge and exercise a high degree of leadership in their daily work. You have strong opinions that are weakly held, defensible ideas, and advocate for what you believe is right. You are also adept at identifying and evaluating trade-offs, willing to be proven wrong, and quick to walk through fire to support your fellow teammates.
You value, seek out, and foster diversity. We are a global team from many diverse backgrounds, with different experiences and perspectives. To complement this team, you will welcome ideas that are different from your own and be skilled at finding and building from common ground.
You are a creative thinker, not bound by “the way things have always been done”.
What you know is less important than how well you learn and innovate. We don't need engineers who know all the answers; we need engineers who can invent the answers no one has thought of yet, to the questions yet to be asked. What and how you can contribute is what’s important to us. Our consideration is not limited by the kind of education you have or the specific technologies you have experience with. Variety of technical challenge is one of the best things about working at The Trade Desk as an engineer, but we do not expect you to know every technology we use when you start. What we care about is that you can learn quickly and solve complex problems using the best tools for the job. Our culture runs much deeper than just having fun together (though, we do that well too...) – the people we want on our team are trust-builders, generous givers, scrappy problem solvers, and gritty pursuers of excellence. Does this sound like you? If so, we welcome your application and the chance to meet you.
The Trade Desk does not accept unsolicited resumes from search firm recruiters. Fees will not be paid in the event a candidate submitted by a recruiter without an agreement in place is hired; such resumes will be deemed the sole property of The Trade Desk. The Trade Desk is an equal opportunity employer. All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law.
[LA JOBS ONLY]
The Trade Desk will consider qualified applicants with criminal histories for employment in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring, Ordinance No. 184652.
[SF JOBS ONLY]
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
NY, CO, CA, and WA residents only:
In accordance with NY, CO, CA, and WA law, the range provided is The Trade Desk's reasonable estimate of the base compensation for this role. The actual amount may differ based on non-discriminatory factors such as experience, knowledge, skills, abilities, and location. All employees may be eligible to become The Trade Desk shareholders through eligibility for stock-based compensation grants, which are awarded to employees based on company and individual performance. The Trade Desk also offers other compensation depending on the role such as sales-based incentives and commissions. Plus, expected benefits for this role include comprehensive healthcare (medical, dental, and vision) with premiums paid in full for employees and dependents, retirement benefits such as a 401k plan and company match, short and long-term disability coverage, basic life insurance, well-being benefits, reimbursement for certain tuition expenses, parental leave, sick time of 1 hour per 30 hours worked, vacation time for full-time employees up to 120 hours thru the first year and 160 hours thereafter, and around 13 paid holidays per year. Employees can also purchase The Trade Desk stock at a discount through The Trade Desk’s Employee Stock Purchase Plan.
The Trade Desk also offers a competitive benefits package. Click here to learn more.
Note:
Interns are not eligible for variable incentive awards such as stock-based compensation, retirement plan, vacation, tuition reimbursement or parental leave
At the Trade Desk, Base Salary is one part of our competitive total compensation and benefits package and is determined using a salary range. The base salary range for this role is
$121,500—$222,800 USD
Show more
Show less","Data engineering, ML pipelines, Data processing automation, Data processing pipelines, Model deployments, Experimentation configuration, Data quality, Data warehousing, Data privacy, Data governance, Data modeling, AB test experimentation, SQL, ETL, Spark, Airflow, Docker, Kubernetes, Deep learning models, Tensorflow, Pytorch, C#, Java, Python, Scala, Web services, Product mindset, Diversity, Creativity","data engineering, ml pipelines, data processing automation, data processing pipelines, model deployments, experimentation configuration, data quality, data warehousing, data privacy, data governance, data modeling, ab test experimentation, sql, etl, spark, airflow, docker, kubernetes, deep learning models, tensorflow, pytorch, c, java, python, scala, web services, product mindset, diversity, creativity","ab test experimentation, airflow, c, creativity, data engineering, data governance, data privacy, data processing automation, data processing pipelines, data quality, datamodeling, datawarehouse, deep learning models, diversity, docker, etl, experimentation configuration, java, kubernetes, ml pipelines, model deployments, product mindset, python, pytorch, scala, spark, sql, tensorflow, web services"
Senior Data Engineer,Advantis Global,"New York, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-advantis-global-3724450676,2023-12-17,Hamilton,United States,Mid senior,Onsite,"About This Featured Opportunity
At Advantis Global, we are dedicated to connecting YOU to highly sought-after opportunities. Our client, a well-known Fortune 500 company in the Renewable Energy industry, is seeking an experienced Data Engineer to join their team. This is a fantastic opportunity to build and own large parts of the Onshore Wind Data Infrastructure. As a Data Engineer, you will be tasked with finding ways to improve scalability, reliability, and performance of critical data pipelines. You'll also be responsible for building, deploying, and managing data solutions and tools that drive analytics and insights across the business. In this role, you will collaborate closely with Data Scientists, Data Engineers, Analytics Engineers, Reliability Engineers, and Field Engineers to design, build, test, deploy, and run data-driven solutions. Additionally, you will play a key role in diagnosing and troubleshooting system failures using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) methods. Join us in driving optimization, testing, and tool development to enhance the quality of our data products.
THE OPPORTUNITY FOR YOU
Day-to-Day Responsibilities
Build and own large parts of Onshore Wind Data Infrastructure, focusing on scalability, reliability, and performance improvements for critical data pipelines.
Deploy and manage data solutions and tools that support analytics and insights.
Maintain and enhance toolsets, scripts, frameworks, and support Data Scientists and Analytics Engineers.
Collaborate with cross-functional teams to design, build, test, deploy, and operate data-driven solutions.
Utilize Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) to diagnose and troubleshoot system failures.
Contribute to optimization, testing, and tool development to improve data product quality.
KEY SUCCESS FACTORS
Qualifications/Requirements
Strong teamwork and collaboration skills, with the ability to give and receive constructive feedback.
Bachelor's or Master's degree in computer science, science, economics, statistics, mathematics, or equivalent practical experience in Data Engineering.
Solid background in Data Modeling, Data Architecture, and Data Infrastructure Optimization.
Minimum of 4 years of industry experience building, managing, and optimizing distributed data processing systems (e.g., Hadoop, Spark, Hive, Trino, Flink, Kafka, etc.).
Proficiency in software engineering using languages like Python and Java to build scalable data solutions.
Experience with both Relational databases (Postgres, MySQL, Oracle) and non-relational/analytics databases (Cassandra, MongoDB, Druid).
Familiarity with open file formats (Parquet, JSON, ORC, etc.) and storage technologies (S3, Azure Blobstore, HDFS, etc.).
Experience with modern data science toolsets such as Jupyter Notebook, Zeppelin, Superset, and Tensor Flow.
Familiarity with software development practices like source control (Git), Test Driven Development, and Agile Analytics Development.
Proficiency in deploying and managing solutions in the cloud (AWS, Azure, Google Cloud Platform).
Experience with building, deploying, and managing solutions in a container environment (e.g., Docker, Kubernetes).
Clear and concise communication skills to explain complex concepts.
Passion for delivering superior customer experiences.
Preferred Qualifications
Familiarity with implementing and managing security within data systems.
Experience with system monitoring tools like Prometheus, New Relic, Elastic Stack, Grafana, etc.
Ability to consume and create Web APIs (e.g., ReST, gRPC).
Experience with deploying and managing search solutions like Solr/Elastic Search.
Basic project management knowledge and skills.
Benefits
Company sponsored Health, Dental, and Vision coverage.
Advantis Global is an equal opportunity employer and makes employment decisions based on merit, qualifications, and abilities. Our company policy prohibits unlawful discrimination based on various characteristics. Additionally, we are committed to promoting pay equity and maintaining a harassment-free workplace for all employees.
Show more
Show less","Data Engineering, Data Modeling, Data Architecture, Data Infrastructure Optimization, Distributed Data Processing Systems, Python, Java, Relational Databases, Nonrelational Databases, Analytics Databases, Open File Formats, Storage Technologies, Data Science Toolsets, Source Control, Test Driven Development, Agile Analytics Development, Cloud Computing, Container Management, System Monitoring, Web APIs, Search Solutions, Project Management","data engineering, data modeling, data architecture, data infrastructure optimization, distributed data processing systems, python, java, relational databases, nonrelational databases, analytics databases, open file formats, storage technologies, data science toolsets, source control, test driven development, agile analytics development, cloud computing, container management, system monitoring, web apis, search solutions, project management","agile analytics development, analytics databases, cloud computing, container management, data architecture, data engineering, data infrastructure optimization, data science toolsets, datamodeling, distributed data processing systems, java, nonrelational databases, open file formats, project management, python, relational databases, search solutions, source control, storage technologies, system monitoring, test driven development, web apis"
Lead Data Engineer,"Verticalmove, Inc","New York, United States",https://www.linkedin.com/jobs/view/lead-data-engineer-at-verticalmove-inc-3777097261,2023-12-17,Hamilton,United States,Mid senior,Onsite,"About Our Company
Verticalmove is a member of Inc Magazine 2023 fastest-growing private companies in America!
We build digital transformation, product, and software engineering teams! We help our clients achieve successful digital transformations, and talented professionals reach their optimal progression throughout their careers. Our portfolio of clients includes start-ups financed by the most exclusive venture capital firms, and established Fortune 500 companies such as Salesforce.com, American Express, CVS Health, United Healthcare, Sony Interactive, and many others.
Your Responsibilities:
Here's what your role will entail:
Engage in collaborative efforts with Agile teams to conceptualize, create, evaluate, implement, and maintain technical solutions within the realm of full-stack development tools and technologies.
Team up with a group of developers renowned for their expertise in machine learning, distributed microservices, and full-stack systems.
Harness the potential of programming languages like Java, Scala, Python, and dive into the realm of Open Source RDBMS and NoSQL databases like DynamoDB and RDS/Postgres. Embrace the world of cloud-based data warehousing services, including Redshift and Snowflake.
Share your enthusiasm for keeping pace with the latest technology trends, experimenting with and adopting new technologies, participating actively in both internal and external technology communities, and offering mentorship to fellow members of the engineering community.
Collaborate with digital product managers to deliver robust cloud-based solutions that enable impactful experiences, ultimately empowering millions of Americans in their journey toward financial empowerment.
Execute unit tests and engage in peer reviews with team members to ensure your code is impeccably designed, elegantly crafted, and finely tuned for optimal performance.
Basic Qualifications:
A Bachelor’s Degree.
A minimum of 6 years of experience in application development (Please note that internship experience does not apply).
A minimum of 2 years of experience in the realm of big data technologies.
At least 1 year of hands-on experience with cloud computing, whether it's AWS, Microsoft Azure, or Google Cloud.
Preferred Qualifications:
A remarkable 7+ years of experience in application development, featuring proficiency in Java, SQL, Scala, or Python.
Over 4 years of practical experience with a public cloud platform (AWS, Microsoft Azure, or Google Cloud).
Over 4 years of expertise with Distributed data/computing tools, encompassing MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, DataBricks, or MySQL.
A substantial 4+ years of involvement in real-time data and streaming applications.
A wealth of over 4 years of hands-on experience with NoSQL implementation, whether it's with databases like Mongo or Cassandra.
Over 4 years of experience in the realm of data warehousing, working with platforms such as Redshift or Snowflake.
Over 4 years of proficiency with UNIX/Linux systems, including mastery of basic commands and shell scripting.
At least 2 years of experience with Agile engineering practices.
Show more
Show less","Java, Scala, Python, RDBMS, NoSQL, DynamoDB, RDS/Postgres, Redshift, Snowflake, Big data technologies, Cloud computing, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, DataBricks, MySQL, Realtime data, Streaming applications, Mongo, Cassandra, Data warehousing, UNIX/Linux, Agile engineering practices","java, scala, python, rdbms, nosql, dynamodb, rdspostgres, redshift, snowflake, big data technologies, cloud computing, aws, microsoft azure, google cloud, mapreduce, hadoop, hive, emr, kafka, spark, gurobi, databricks, mysql, realtime data, streaming applications, mongo, cassandra, data warehousing, unixlinux, agile engineering practices","agile engineering practices, aws, big data technologies, cassandra, cloud computing, databricks, datawarehouse, dynamodb, emr, google cloud, gurobi, hadoop, hive, java, kafka, mapreduce, microsoft azure, mongo, mysql, nosql, python, rdbms, rdspostgres, realtime data, redshift, scala, snowflake, spark, streaming applications, unixlinux"
Data Engineer,Alldus,"New York, United States",https://www.linkedin.com/jobs/view/data-engineer-at-alldus-3772066176,2023-12-17,Hamilton,United States,Mid senior,Onsite,"Senior Data Engineer
We are currently recruiting a Senior Data Engineer for an exciting seed level startup in New York. Our client are using data and AI to work with customers and predict consumer trends.
You as the ideal candidate will be responsible for:
Building data pipelines
Working across various teams including Data Science and Machine Learning to translate requirements into data pipelines
Writing code in Python
Integrating data from various sources
You as the ideal candidate will require:
Data Engineering experience
Experience building out data pipelines
Python
GCP Experience
ML/MLOps exposure – beneficial
Please not this role would be hybrid*
This is a great opportunity for an ambitious Data Engineer to join an exciting startup with great growth opportunities.
If you are interested, please apply or send a resume to kieran@alldus.com
39640
Show more
Show less","Data Engineering, Data pipelines, Python, GCP, Machine Learning, MLOps","data engineering, data pipelines, python, gcp, machine learning, mlops","data engineering, datapipeline, gcp, machine learning, mlops, python"
Database Infrastructure Engineer,Selby Jennings,"New York, United States",https://www.linkedin.com/jobs/view/database-infrastructure-engineer-at-selby-jennings-3776959515,2023-12-17,Hamilton,United States,Mid senior,Onsite,"A leading High Frequency Trading Firm is seeking a skilled Database Infrastructure Engineer to join its Platform Engineering team.
What you’ll do:
Be responsible for designing, implementing, and integrating database platforms and automation on both Windows and Linux systems.
Work closely with a global team of database administrators to own and build out the existing database environment, workflows, and automation.
Optimize and scale applications and software related to database access.
What you’ll need:
Experience with at least one database platform and working knowledge of others (e.g., SQL Server, Postgres, Redis, MongoDB, or Cassandra).
Proficiency in programming languages such as Python, Java, and C#.
Working knowledge of networking protocols, information security concepts, and operating system internals.
This role will be on-site in New York
Show more
Show less","Database Design, Database Integration, Database Automation, Linux, Windows, SQL Server, Postgres, Redis, MongoDB, Cassandra, Python, Java, C#, Networking protocols, Information security concepts, Operating system internals","database design, database integration, database automation, linux, windows, sql server, postgres, redis, mongodb, cassandra, python, java, c, networking protocols, information security concepts, operating system internals","c, cassandra, database automation, database design, database integration, information security concepts, java, linux, mongodb, networking protocols, operating system internals, postgres, python, redis, sql server, windows"
Senior Python Data Engineer,Luxoft,"New York, United States",https://www.linkedin.com/jobs/view/senior-python-data-engineer-at-luxoft-3774716777,2023-12-17,Hamilton,United States,Mid senior,Onsite,"*** THIS IS A FULL TIME ROLE (C2C IS NOT POSSIBLE)***
Project Description:
We are looking for an experienced Python Data Engineer to expand our Systematic Data Platform team. The team develops data pipelines for a top-tier Hedge Fund.
Responsibilities:
Design, develop, and maintain data pipelines.
Develop data APIs.
Analyze and organize raw data.
Conduct complex data analysis and report on results.
Must have skills:
5+ years of Python.
Solid data engineering skills.
Strong SQL.
Nice-to-Have Skills:
Data Science
Statistics
Data Modelling
*Compensation for NY is 100000-180000 USD Gross per Year
Show more
Show less","Python, Data Engineering, SQL, Data Science, Statistics, Data Modeling","python, data engineering, sql, data science, statistics, data modeling","data engineering, data science, datamodeling, python, sql, statistics"
Senior Data Analyst,Caraway Home,"New York, United States",https://www.linkedin.com/jobs/view/senior-data-analyst-at-caraway-home-3787922537,2023-12-17,Hamilton,United States,Mid senior,Remote,"Who are we:
Caraway is a home goods brand on a mission to craft well-designed, non-toxic kitchenware that thoughtfully raises the standards of what you cook with. We’re passionate about creating elevated products, free of chemicals, that are better for you, your family, and the world. We prioritize a holistic approach to the creation of Caraway products. We partner with ethical manufacturers where their employees are safe, paid fairly, given benefits, and work regulated hours. Our eco-friendly manufacturing process releases less CO2 into the environment than traditional non-stick coatings and we engage in sustainable packaging practices, free of single-use plastics and ship in packaging made from recycled materials. We pride ourselves on continuously striving to do and be better - for you and the planet - every step of the process.
Since launching in November 2019, we’ve become one of the fastest growing digitally-native brands of the last decade and are now sold in retailers including Target, Bloomingdale’s, Crate & Barrel, Container Store, Costco, Zola, and many more.
We’ve amassed over 58,000+ five-star reviews from happy customers and hundreds of press hits from the likes of Vogue, Domino, Good Housekeeping, Food & Wine, Architectural Digest, PureWow, Fortune, CNN.
Caraway is a fully-remote company filled with motivated and hardworking innovators and doers. We’re continually developing programs to support our distributed team to ensure a successful remote experience. If you’re looking to join a brand that folks love, read on and apply!
The Opportunity:
We are looking for a Senior Data Analyst to be a founding member of our Data & Insights team; the Sr. Data Analyst will report into the VP of Growth & Digital Product. In this role you will partner with stakeholders across the company to leverage analytics to drive the business and better serve our customers, primarily with a focus on the DTC business and growth marketing. This role is also critical in building data products and dashboards to empower data-driven decision making across the organization. You will be responsible for both finding answers to important business questions as well as helping to surface key insights and data to drive business strategy. Caraway strives to create a culture of employees who are comfortable and can self-service many of their data needs — you will be a key player in enabling this and evangelizing our data-first culture.
Who You Are:
Ideal candidates are resourceful, curious and clear communicators. You’re able to navigate ambiguity and apply your experience and instinct to help craft thoughtful responses and insights on the business to key stakeholders. You get excited about helping to answer big questions without obvious answers and can piece together disparate data to tell your story. You’re able to work with people at various levels to help everyone better understand the business and metrics that drive it.
Responsibilities:
Collaborate with key stakeholders to understand, prioritize and answer business questions to achieve our company objectives
Contribute to our Analytics function for Marketing, Acquisition and Product data infrastructure needs, reporting and analytics needs with focus on quantifying the impact of marketing initiatives, improving conversion, retention, product adoption, and revenue generation
Conduct in-depth data analysis to uncover insights and trends, and generate actionable recommendations
Own the company's data stack, ensuring data accuracy, integrity, and accessibility
Identify opportunities to optimize marketing strategies and campaigns through data-driven insights
Collaborate with cross-functional teams to design, execute, and analyze experiments and A/B tests
Stay up-to-date with industry trends and best practices in marketing analytics and data management
Analyze customer data, helping to dissect behavioral & purchase data including repeat purchase rates, establishing an LTV metric, determining CAC thresholds, etc.
Consult with leaders across the business to develop analytic roadmaps with particular emphasis on insights that accelerate growth, primarily with a focus on the Caraway DTC business
Perform analyses that enable a wide range of stakeholders to deepen their quantitative understanding of their business, with a particular emphasis on insights that accelerate growth
Develop dashboards and reporting that empower self-service, data-driven decision making
Work with our technology partners and data engineers to ensure high-quality, usable data are captured to support reliable business analytics
Build scalable methodology and tools to accelerate development of our data and analytics practice
Partner with Finance to build and refine dashboards for key model inputs and support ad hoc analysis
Knowledge, skills, and abilities:
Proven experience in the Consumer, DTC, E-Commerce, or similar industry as a Analyst, Marketing, Growth, Performance space
Strong analytical skills with the ability to derive insights from complex data sets
Strong SQL skills and a deep understanding of relational databases and how to leverage a wide-variety of data structures to drive insights and power customer-facing communications
Experience in building and maintaining dashboards and reports in BI tools like Looker or Tableau
Experience using Shopify data to provide insights to the business
Experience working on incrementality tests or marketing mix models to measure marketing channel effectiveness
Knowledge of statistical modeling and experimental design
Familiarity with marketing analytics platforms and ETL process leveraging tools such as FiveTran, Snowflake, dbt
Excellent communication and presentation skills, with the ability to convey complex ideas to non-technical stakeholders through oral and written communication, including presentations
Bachelor's degree in Marketing, Business, Statistics, or a related field preferred
Caraway Life:
Caraway is an exciting, fully remote DTC retail startup company that is bringing kitchenware and home goods into the modern era, leading from a non-toxic and sustainable foundation.
Team: You will be joining a passionate and collaborative fully-remote team that prides itself on high-quality work, supporting one another, sharing our successes, and building a brand we truly love, together.
Culture: At Caraway, we value continuous learning and career growth. We strive to continually optimize our processes, ensuring that we’re thoughtful in everything we do, both internally and externally. We listen to our employees, celebrate our wins, and constantly look for ways to be better for our team, and the world.
Workplace: Being a fully-remote company, we have no headquarters and are truly distributed across the country. Our leadership team sits on both the East and West Coast and we have folks in a variety of states. We offer a monthly stipend towards co-working space should folks need to switch up their working environment every now and then.
Benefits: We value you and your family. With this in mind, full-time eligible employees are provided:
Health, dental, and vision offerings for you and your dependents
Competitive salary + equity options
Unlimited PTO
11+ paid holidays
3 months Parental leave
Fully Remote Company
Company-sponsored monthly wellness stipend
Company-sponsored monthly coworking stipend
Pre-tax Commuter and FSA/Dependent Care FSA, and HSA options
401(k) plan
High quality home and kitchen product + swag, F&F discounts
Caraway is an Equal Opportunity & Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status
Powered by JazzHR
wkuG7NlKv0
Show more
Show less","Data analysis, SQL, Looker, Tableau, Shopify, FiveTran, Snowflake, dbt, BI tools, MySQL, PostgreSQL, Data modeling, Experimental design, Statistical modeling, Marketing analytics, ETL processes, Communication, Presentation, Business intelligence, Data visualization, Analytics, Dashboards, Reporting","data analysis, sql, looker, tableau, shopify, fivetran, snowflake, dbt, bi tools, mysql, postgresql, data modeling, experimental design, statistical modeling, marketing analytics, etl processes, communication, presentation, business intelligence, data visualization, analytics, dashboards, reporting","analytics, bi tools, business intelligence, communication, dashboard, dataanalytics, datamodeling, dbt, etl, experimental design, fivetran, looker, marketing analytics, mysql, postgresql, presentation, reporting, shopify, snowflake, sql, statistical modeling, tableau, visualization"
Senior Software Engineer - Data Engineering,LiveRamp,"New York, United States",https://www.linkedin.com/jobs/view/senior-software-engineer-data-engineering-at-liveramp-3711767956,2023-12-17,Hamilton,United States,Mid senior,Remote,"LiveRamp is the data collaboration platform of choice for the world’s most innovative companies. A groundbreaking leader in consumer privacy, data ethics, and foundational identity, LiveRamp is setting the new standard for building a connected customer view with unmatched clarity and context while protecting precious brand and consumer trust. LiveRamp offers complete flexibility to collaborate wherever data lives to support the widest range of data collaboration use cases—within organizations, between brands, and across its premier global network of top-quality partners.
Hundreds of global innovators, from iconic consumer brands and tech giants to banks, retailers, and healthcare leaders turn to LiveRamp to build enduring brand and business value by deepening customer engagement and loyalty, activating new partnerships, and maximizing the value of their first-party data while staying on the forefront of rapidly evolving compliance and privacy requirements.
You Will:
Engineer cloud agnostic data infrastructure solutions that support running advanced data modeling techniques at scale.
Work closely with our data science team to adjust and optimize code, queries and infrastructure for scale.
Work closely with our data science team to help them iterate over new models and new data sets rapidly and make those models production ready
Build, deploy and maintain RESTful API services to petabyte-scale backends.
Write automation tools for all aspects of Python development: infrastructure orchestration and configuration, code build and release pipelines (CI/CD), data pipeline testing frameworks.
Work in multiple cloud environments such as AWS and GCP.
Treat infrastructure as code leveraging tools like Terraform, Docker and Kubernetes.
Help design new product offerings in the Measurement space.
Your Team Will:
Collaborate with a data science team that develops Reach, Frequency, Lift and Attribution batch and time-series ML backed models for in-depth ad campaign measurement business intelligence analysis.
Enable configuration-driven workflows that automate batch and streaming-based processing on top of Measurement services and assets.
Provide an orchestration platform for complex measurement model builds using the latest open source and cloud native technologies.
Explore setting up ML Ops stacks as applied to measurement analytics.
About You:
5+ years experience in data engineering and backend services development.
Expert SQL knowledge and proven experience working with relational databases.
Strong command of Python and SQLAlchemy.
Good understanding of distributed computing and distributed SQL tools such as Presto or Spark SQL.
Strong ability to break down complex problems to their essential components and design and implement elegant solutions for them.
Experience designing, building and deploying RESTful APIs
Strong understanding of various code testing frameworks and tools.
Curious to learn and comfortable evaluating and adapting to the latest tools and industry best practices.
Have a startup personality and enjoy working as part of a Scrum team: smart, ethical, friendly, hard-working, and productive.
Strong oral and written communication, project management and organizational skills.
Preferred Skills:
Experience with processing hundreds of terabytes or petabyte scale of data.
Strong mathematical or physics background.
Expertise in applying machine learning to data to maximize data value.
Experience working with distributed SQL engines such as Trino/Presto and/or SQL warehouses such as Redshift or BigQuery.
Experience building infrastructure to enable large scale Machine Learning development (ML Ops)
Benefits:
People: Work with talented, collaborative, and friendly people who love what they do.
Fun: We host in-person and virtual events such as game nights, happy hours, camping trips, and sports leagues.
Work/Life Harmony: Flexible paid time off, paid holidays, options for working from home, and paid parental leave.
Comprehensive Benefits Package: Medical, dental, vision, life, and disability. Plus, mental health support (via Talkspace), flexible time off, parental leave, family forming benefits, and a flexible lifestyle and wellbeing reimbursement program (up to $375 per quarter, U.S. LiveRampers)
Savings: Our 401K matching plan—1:1 match up to 6% of salary—helps you plan ahead. Also Employee Stock Purchase Plan - 15% discount off purchase price of LiveRamp stock (U.S. LiveRampers)
RampRemote: A comprehensive office equipment and ergonomics program—we provide you with equipment and tools to be your most productive self, no matter where you're located
Location: work in the heart of *Location* and/or from home (remote-eligible roles)
More About Us:
LiveRampers are empowered to live our values of committing to shared goals and operational excellence. Connecting LiveRampers to new ideas and to one another is one of our guiding principles—one that informs how we hire, train, and grow our global teams across nine countries and four continents. By continually building inclusive, high belonging teams, LiveRampers can deliver exceptional work, champion innovative ideas, and be their best selves. Click here to learn more about Diversity, Inclusion, & Belonging (DIB) at LiveRamp.
For All NYC POSTINGS & SF POSTINGS
The approximate annual base compensation range is $156,000 to $199,000 the actual offer, reflecting the total compensation package and benefits, will be determined by a number of factors including the applicant's experience, knowledge, skills, and abilities, geography, as well as internal equity among our team.
LiveRamp is an affirmative action and equal opportunity employer (AA/EOE/W/M/Vet/Disabled) and does not discriminate in recruiting, hiring, training, promotion or other employment of associates or the awarding of subcontracts because of a person's race, color, sex, age, religion, national origin, protected veteran, disability, sexual orientation, gender identity, genetics or other protected status. Qualified applicants with arrest and conviction records will be considered for the position in accordance with the San Francisco Fair Chance Ordinance.
California residents: Please see our California Personnel Privacy Policy for more information regarding how we collect, use, and disclose the personal information you provide during the job application process.
To all recruitment agencies: LiveRamp does not accept agency resumes. Please do not forward resumes to our jobs alias, LiveRamp employees or any other company location. LiveRamp is not responsible for any fees related to unsolicited resumes.
More About Us:
LiveRamp’s mission is to connect data in ways that matter, and doing so starts with our people. We know that inspired teams enlist people from a blend of backgrounds and experiences. And we know that individuals do their best when they not only bring their full selves to work but feel like they truly belong. Connecting LiveRampers to new ideas and one another is one of our guiding principles—one that informs how we hire, train, and grow our global team across nine countries and four continents. Click here to learn more about Diversity, Inclusion, & Belonging (DIB) at LiveRamp.
Show more
Show less","Data collaboration, Data infrastructure, Data modeling, Data science, Data pipelines, RESTful APIs, Python, SQLAlchemy, Presto, Spark SQL, Terraform, Docker, Kubernetes, Machine learning, Distributed computing, Distributed SQL, Trino, Redshift, BigQuery, ML Ops, SQL, Relational databases","data collaboration, data infrastructure, data modeling, data science, data pipelines, restful apis, python, sqlalchemy, presto, spark sql, terraform, docker, kubernetes, machine learning, distributed computing, distributed sql, trino, redshift, bigquery, ml ops, sql, relational databases","bigquery, data collaboration, data infrastructure, data science, datamodeling, datapipeline, distributed computing, distributed sql, docker, kubernetes, machine learning, ml ops, presto, python, redshift, relational databases, restful apis, spark sql, sql, sqlalchemy, terraform, trino"
Data Test Engineer II (New York Only),Measures for Justice,"New York, United States",https://www.linkedin.com/jobs/view/data-test-engineer-ii-new-york-only-at-measures-for-justice-3771149628,2023-12-17,Hamilton,United States,Mid senior,Remote,"Location
: Remote*
*Only candidates in NY will be considered.
Hours:
Full-Time
Salary:
$101,116
To be considered for the role, candidates must be authorized to work in the United States without sponsorship.
About Measures for Justice
Measures for Justice is a nonpartisan nonprofit organization dedicated to developing tools and services that help communities, including the institutions that serve them, reshape how the criminal justice system works. Since 2011, we have been collecting, standardizing, and publishing criminal justice data to increase transparency, accessibility, and accountability. We’re leading a movement to change the future of criminal justice.
Role Overview
As a Data Test Engineer II at Measures for Justice, you will partner closely with a cross-functional Scrum team to ensure the quality of our data pipelines, ETL processes, and databases. You will craft test strategies, execute cases, and communicate gaps to enhance software functionality and provide high-quality data for external partners using our Commons product. Through your expertise, Measures for Justice will continue to provide access to high-quality criminal justice data to help communities reshape the criminal justice system.
Key Responsibilities
Collaborate closely with a designated Scrum team to ensure data quality and process adherence
Perform day-to-day testing activities, maintain strong communication with development teams
Understand requirements and make recommendations for testing strategies
Identify and fill gaps for testing needs and requirements
Demonstrate a commitment to diversity, equity, inclusion, and belonging by modeling inclusive behaviors and proactively managing bias
Your Experience
Strong background in quality assurance and/or test engineering
Skilled in testing data pipelines, ETL processes, and databases
Proven experience designing and building automated test suites
Proficiency with 1 programming language (bonus points for Java, Javascript, SQL, Python)
Strong communication, listening, and collaboration skills
Proven experience with independent work, self-direction, and prioritization
Bonus Points
Experience testing code written in SPSS and/or R
Background working with data analysts, data scientists, and/or data engineers
If you do not have the skills/experience mentioned in the bonus points section, we still encourage you to apply!
Benefits
Meaningful work that contributes to positive social change
Collaborative and inclusive work environment fostering growth and work-life balance
Comprehensive medical, dental, and vision coverage
Generous paid time off, including 20 days vacation, 10 sick days, 15 paid holidays (including the week between Christmas and New Year’s Day!), and 2 days of community service
Reduced hours on summer Fridays
Paid parental leave
401k matching up to 3% of annual salary
Professional development opportunities via Bravely, Coursera, and more
Flexible work arrangements, including remote and hybrid work, for eligible positions
Full technology package including computer, monitors, bluetooth devices, and more
Commitment to Diversity, Equity, Inclusion, and Belonging
Measures for Justice is committed to fostering diversity, equity, inclusion, and belonging in our workplace. We strive to create a welcoming environment for people of all backgrounds by eradicating barriers to access and opportunity. We encourage applicants from all backgrounds and experiences to apply.
EEO Statement
Measures for Justice provides equal employment opportunity to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law.
Legal Requirements
Background checks will be conducted for candidates who receive an offer of employment and employment will be contingent on the evaluation of any information received consistent with applicable laws. Follow-up background checks will be conducted on employees after each five years of employment.
Show more
Show less","Java, JavaScript, SQL, Python, SPSS, R, ETL, Databases, Test suites, Test strategies, Test cases, Data analysis, Data pipelines, Quality assurance, Communication, Collaboration, Selfdirection, Prioritization","java, javascript, sql, python, spss, r, etl, databases, test suites, test strategies, test cases, data analysis, data pipelines, quality assurance, communication, collaboration, selfdirection, prioritization","collaboration, communication, dataanalytics, databases, datapipeline, etl, java, javascript, prioritization, python, quality assurance, r, selfdirection, spss, sql, test cases, test strategies, test suites"
Data Analyst,"LanceSoft, Inc.","New York, United States",https://www.linkedin.com/jobs/view/data-analyst-at-lancesoft-inc-3780038169,2023-12-17,Hamilton,United States,Mid senior,Hybrid,"Data Analyst III - NYC, NY – 3 days onsite per week
Only W2 candidates. We don't work on C2C for this role
Experience:
8+ years of experience in all phases of data modeling, data warehousing, data mining, data entity analysis, logical database design and relational database definition, or an equivalent combination of education and work experience required
Show more
Show less","Data Modeling, Data Warehousing, Data Mining, Data Entity Analysis, Logical Database Design, Relational Database Definition","data modeling, data warehousing, data mining, data entity analysis, logical database design, relational database definition","data entity analysis, data mining, datamodeling, datawarehouse, logical database design, relational database definition"
Principal Data Engineer(NY/NJ),Medidata Solutions,"New York, United States",https://www.linkedin.com/jobs/view/principal-data-engineer-ny-nj-at-medidata-solutions-3689934753,2023-12-17,Hamilton,United States,Mid senior,Hybrid,"Medidata: Powering Smarter Treatments and Healthier People
Medidata, a Dassault Systèmes company, is leading the digital transformation of life sciences, creating hope for millions of people. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 2,000+ customers and partners access the world's most trusted platform for clinical development, commercial, and real-world data. Known for its ground-breaking technological innovations, Medidata has supported more than 30,000 clinical trials and 9 million study participants. And Medidata’s ongoing commitment to infusing the patient voice into trial designs and solutions is helping to create a better and more inclusive experience for all participants in clinical studies. Medidata is involved in nearly 40% of company-initiated trial starts globally, with studies conducted in more than 140 countries. More than 70% of novel drugs approved by the Food and Drug Administration (FDA) in 2022 were developed with Medidata software. Medidata is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www.medidata.com and follow us @medidata.
Technical Services at Medidata is seeking a Principal Data Engineer to develop high-quality data engineering services and tools for our clients. The ideal candidate will have a deep understanding of data management in the context of clinical trials, deep working experience in software development, infrastructure, cloud, data engineering, and experience developing tools for data reconciliation across different data sources and modalities.
Our technology is in an industry that is improving the lives of millions. Collect our client’s data from different modalities into our platform and make them available wherever and whenever customers need it, safely and reliably. There is no room for error. If you are looking for a better place to use your passion and your desire to drive change, this is the place to be. As part of the Technical Services team, the successful candidate will play a significant part in being a data engineer/data management trusted advisor across all clients.
You will be supporting the technology data and analytics to improve stakeholder engagement, idea intake, roadmap management and optimal data related to product performance while ensuring execution and delivery transparency. In this role you will actively contribute and influence how ideas and strategy become a reality; directly impacting clinical and business practices. You will make a difference solving the impossible.
Position in this function is responsible for the full data lifecycle management, including design, prioritization, development cycle management for end-of-life, new, existing and acquisition products.
Leverage market insights to understand market/customer needs to identify new opportunities or make adjustments to current TS data offerings. The key to a successful offering is to ask why, to use frameworks and models building solid relationships across business and technology.
The ideal candidate will have the right balance between data strategy, data engineering and execution.
The Principal Data Engineer assists the VP of Technical Services to define and execute the data strategy within Technical Services and is responsible for the growth goals the data of a
related product portfolio achieves annually.
Data Engineering Expertise:
The candidate should have extensive experience in data engineering, particularly in the context of clinical trials. This includes proficiency in ETL (Extract, Transform, Load) processes, data reconciliation across different data sources, and managing the full data lifecycle.
Software Development and Infrastructure Knowledge:
The ideal candidate should possess a strong background in software development, infrastructure, and cloud technologies. This is crucial for developing data engineering tools and solutions and ensuring they are efficient and reliable.
Data Management and Regulatory Knowledge:
The candidate must have in-depth knowledge of data management practices, including regulatory requirements related to clinical trials, such as FDA guidance documents and CDISC standards. Understanding good clinical practices (GCP), protocol, and SDTM mapping is essential.
The day-to-day responsibilities for this person would include:
Developing and maintaining a data vision and roadmap.
Prioritizing backlog and ensuring stakeholders are informed.
Managing cross-functional stakeholders, including design, engineering, scrum teams, and business leaders.
Creating data as a product Business Cases to support planning.
Presenting data opportunities in leadership discussions with the VP of Technical Services.
Implementing data engineering strategies that align with the company's business goals.
Leading data reconciliation efforts across various data sources and modalities.
Ensuring data engineering processes are efficient, effective, and compliant with regulatory requirements.
Providing technical leadership in software development, infrastructure, cloud, and data engineering.
Identifying opportunities for process improvement and new technology adoption.
Developing and maintaining digital health data pipelines.
Collaborating with data scientists on healthcare data ingestion solutions.
Working on data analytics, machine learning, and data visualization.
Ensuring data privacy and anonymization.
Integrating various data collection modalities into clinical development programs.
Utilizing data handling strategies to address data quality issues and outliers.
Driving process optimization and pattern recognition.
Supporting the development of tools for data insights and validation.
Having a high-level understanding of artificial intelligence methods.
The role involves collaboration with various cross-functional teams, including design, engineering, and data science. This is a senior individual collaborator job
Requirements:
5+ years of experience in a leadership role within a sponsor or CRO organization
Expertise in software development, infrastructure, cloud, data engineering, and data reconciliation
Strong project management skills and experience managing cross-functional teams
Excellent communication skills and the ability to work effectively with both technical and non-technical stakeholders
Knowledge of regulatory requirements related to data management in clinical trials, including FDA guidance documents and industry standards such as CDISC
Pushing for new ways to use data engineering in data management, with experience in technological adoption
Guiding regarding software applications that manage data.
5+ years of demonstrable experience on defining data strategy, roadmap, requirements and translate into artifacts that drive data management. This includes market evaluation, persona development, capability and journey mapping
5+ years’ experience in defining the product vision, goals, benefits and then track KPI’s to measure success/outcomes
The candidate must understand good clinical practices (GCP), protocol, protocol deviations, metadata, basic SDTM mapping, programming
Having prior Sponsor/CRO experience you will be a proactive advisor to let clients know what is possible, based on the Medidata products and platform capabilities communicating effectively with the client organizations recommending solutions
The key role is that the candidate should be aware of how study data should be processed, how systems should be configured and validated to get true data that can fit the protocol endpoints.
Strong communication and planning skills
Work more independently and autonomously
Represent the culture of performance, collaboration and opportunity.
The candidate sholud have an MS Degree or equivalent work experience in field.
The salary range for positions that will be physically based in the NYC Metro Area is $184,500-246,000.
Base pay is one part of the Total Rewards that Medidata provides to compensate and recognize employees for their work. Most sales positions are eligible for a commission on the terms of applicable plan documents, and many of Medidata's non-sales positions are eligible for annual bonuses. Medidata believes that benefits should connect you to the support you need when it matters most and provides best-in-class benefits, including medical, dental, life and disability insurance; 401(k) matching; unlimited paid time off;and 10 paid holidays per year
Equal Employment Opportunity
In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Medidata are based on merit, qualifications and abilities. Medidata is committed to a policy of non-discrimination and equal opportunity for all employees and qualified applicants without regard to race, color, religion, gender, sex (including pregnancy, childbirth or medical or common conditions related to pregnancy or childbirth), sexual orientation, gender identity, gender expression, marital status, familial status, national origin, ancestry, age, disability, veteran status, military service, application for military service, genetic information, receipt of free medical care, or any other characteristic protected under applicable law. Medidata will make reasonable accommodations for qualified individuals with known disabilities, in accordance with applicable law.
Show more
Show less","Data Engineering, ETL (Extract Transform Load), Data Reconciliation, Software Development, Infrastructure, Cloud, Data Management, Regulatory Knowledge, FDA, CDISC SDTM, GCP (Good Clinical Practices), Protocol, Healthcare Data Ingestion, Data Analytics, Machine Learning, Data Visualization, Data Privacy, Anonymization, Data Collection Modalities, Data Quality, Outliers, Process Optimization, Pattern Recognition, Data Insights, Validation, Artificial Intelligence","data engineering, etl extract transform load, data reconciliation, software development, infrastructure, cloud, data management, regulatory knowledge, fda, cdisc sdtm, gcp good clinical practices, protocol, healthcare data ingestion, data analytics, machine learning, data visualization, data privacy, anonymization, data collection modalities, data quality, outliers, process optimization, pattern recognition, data insights, validation, artificial intelligence","anonymization, artificial intelligence, cdisc sdtm, cloud, data collection modalities, data engineering, data insights, data management, data privacy, data quality, data reconciliation, dataanalytics, etl extract transform load, fda, gcp good clinical practices, healthcare data ingestion, infrastructure, machine learning, outliers, pattern recognition, process optimization, protocol, regulatory knowledge, software development, validation, visualization"
"Data Engineer 2 (Menlo Park, CA hybrid) #3328",GRAIL,"Menlo Park, CA",https://www.linkedin.com/jobs/view/data-engineer-2-menlo-park-ca-hybrid-%233328-at-grail-3787026236,2023-12-17,Daly City,United States,Mid senior,Onsite,"GRAIL is a healthcare company whose mission is to detect cancer early, when it can be cured. GRAIL is focused on alleviating the global burden of cancer by developing pioneering technology to detect and identify multiple deadly cancer types early. The company is using the power of next-generation sequencing, population-scale clinical studies, and state-of-the-art computer science and data science to enhance the scientific understanding of cancer biology, and to develop its multi-cancer early detection blood test. GRAIL is headquartered in Menlo Park, CA with locations in Washington, D.C., North Carolina, and the United Kingdom. GRAIL, LLC is a wholly-owned subsidiary of Illumina, Inc. (NASDAQ:ILMN). For more information, please visit www.grail.com .
Operational Intelligence is seeking an experienced Data Engineer to help grow our ability to organize, transform, analyze and monitor operational data streams to derive significant insights while maintaining high standards of data integrity and reproducibility. In this role, you will play a critical role in amplifying the signal present in the noise of information generated by day-to-day operations. The varied nature of this role requires excellent interpersonal skills and the ability to easily translate information between functional groups within the organization. Working closely with different partners, you will collaborate with various teams, from Operations to Software Engineering to build and maintain access to data. The insights and processes you generate will help drive the informed decision-making process and ensure GRAIL’s robust products continue to detect cancer early!
You Will:
Build and maintain end-to-end data analytics pipelines that include: design, data gathering, processing, analysis, iteration with partners, and presentation of results
Define and develop data workflows from data pre-processing and automation through data dissemination
Perform critical data analyses to support operations such as real-time process monitoring or troubleshooting
Process, analyze, and present a diverse data set including customer service, sample processing, engineering, and many other sources
Assist in developing and handling interactive data visualization and analytics tools for reporting and trending
Work closely with the Lab Operations teams to generate useful metrics for monitoring efficacy and preventing failures before they occur
Assist in the preparation of presentation materials for Process Monitoring meetings where analyses and graphs are presented to various partners
Interact cross-functionally with other teams including software, operations, research, and product development
Your Background Will Include
BS/MS in a quantitative scientific field (computer science, engineering, mathematics, statistics, bioinformatics, etc.) with 3 - 5 years of experience
Experience with R or Python programming and at least one system-level programming language like Go or C++
Experience with SQL development and data warehousing concepts (e.g. ETL/ELT)
Effective written and verbal communication skills
Excellent teammate with a proven track record of success in a multi-functional team environment
Technical leadership, self-direction, and willingness to both teach others and learn new techniques is highly desirable
$115,000 - $130,000 a year
The expected, full-time, annual base pay scale for this position is $115,000 - $130,000. Actual base pay will consider skills, experience, and location.
The location for this role is Menlo Park and you will be expected to work a hybrid schedule which means two days in the office.
Based on the role, colleagues may be eligible to participate in an annual bonus plan tied to company and individual performance, or an incentive plan. We also offer a long-term incentive plan to align company and colleague success over time.
In addition, GRAIL offers a progressive benefit package, including flexible time-off, a 401k with a company match, and alongside our medical, dental, vision plans, carefully selected mindfulness offerings.
GRAIL is an Equal Employment Office and Affirmative Action Employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other legally protected status. We will reasonably accommodate all individuals with disabilities so that they can participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation. GRAIL maintains a drug-free workplace.
Show more
Show less","Data Engineering, Data Analytics, Data Pipelines, Data Warehousing, SQL, ETL/ELT, R, Python, Go, C++, Data Visualization, Data Analysis, Statistics, Data Dissemination, Data Preprocessing, Data Automation, Cloud Computing, Machine Learning, Artificial Intelligence, Business Intelligence","data engineering, data analytics, data pipelines, data warehousing, sql, etlelt, r, python, go, c, data visualization, data analysis, statistics, data dissemination, data preprocessing, data automation, cloud computing, machine learning, artificial intelligence, business intelligence","artificial intelligence, business intelligence, c, cloud computing, data automation, data dissemination, data engineering, data preprocessing, dataanalytics, datapipeline, datawarehouse, etlelt, go, machine learning, python, r, sql, statistics, visualization"
Senior Data Analyst,Nutrien,Pueblo-Cañon City Area,https://www.linkedin.com/jobs/view/senior-data-analyst-at-nutrien-3785891264,2023-12-17,Pueblo,United States,Mid senior,Onsite,"At Nutrien, our purpose is
Feeding the Future
and we do so with safety, inclusion, integrity and results as our core values. Nothing is more important than sending our people home safe, every day.
Nutrien Ag Solutions is the retail division of Nutrien™,
the largest crop inputs company in the world. As part of our collective mission of Feeding the Future, Nutrien Ag Solutions provides full-acre solutions through our trusted crop consultants at more than 2,000 locations in North America, South America, Europe and Australia. For more than 150 years, we have been helping growers achieve the highest yields with a wide selection of products, including our proprietary brands: Loveland Products, Inc.; Proven®Seed and Dyna-Gro®Seed; as well as financial, custom application and precision ag services.
We harvest the best. Diverse views and experience make us strong. We look for people who have a safety-first mindset, who are collaborative team players, who deliver on their commitments, act with integrity, and who support inclusion.
Working at Nutrien Ag Solutions will provide you an opportunity to help us raise the expectation of what an agriculture company can be, and grow your career.
The Senior Data Engineer will work with various Data Product teams to model their data, ensure good data management practices are being followed, validate data quality, improve efficiency, and support analytic needs. You will also work with the data owners from the business to understand what the data represents, how it relates to other data, and the rules for how it behaves.
What you will do:
Perform data analysis to extract knowledge or insight from structured or unstructured data.
Translate data results into reports, tables, graphs, and charts to convey information to administration.
Work with business partners to understand requirements for data use and analysis.
Identify new opportunities to leverage data and to integrate data to enhance business value - build data-driven models to answer business questions.
Perform ongoing monitoring and refinement of reports.
Provide technical support for existing reports, dashboards, or other resources.
Create ad hoc reports and views on a frequent basis to assist administration in understanding, researching, and analyzing issues.
Teach and advocate the use of business intelligence resources across levels of the organization.
Promote usability standards and design solutions with a high-quality, effortless UX.
Model our core value of safety by enabling a culture in which you are actively contributing to a safe work environment.
Other duties as required
What you will bring:
Bachelor's Degree required; experience may be considered in lieu of education.
Minimum of 6 years of data experience required.
Proficiency in SQL and DAX required.
Clear understanding of different data domains in operation and experience in building PowerBI workflows, Star Schemas, and Dax Studio.
Experience with visualization standards for reporting.
Analytical experience identifying key attributes.
Knowledge of data cataloging and documentation.
Experience with CI/CD standards.
Experience with Microsoft Power BI, Python, Uber H3 Hexagon Package, and Github preferred.
Experience with common data storage platforms (Azure, SQL Server, SnowFlake) preferred.
Demonstrated experience querying and transforming large datasets preferred.
Demonstrated experience using data to strengthen business decisions preferred.
Ability to travel 0-5%.
Compensation & Benefits:
The estimated salary that Indeed, Glassdoor and LinkedIn suggested does not represent Nutrien's compensation structure.
Salary Range: $106,800.00 - $146,850.00 This range is estimated for the Senior Data Analyst position in Loveland, Colorado. Actual salary and benefits may differ based upon location.
We provide an attractive benefits package that includes comprehensive medical, dental, vision coverage, and life insurance and well as disability coverage for positions working more than 30 hours per week. In addition, we have a retirement program that encourages our employees to save for the longer term, with generous matching employer contributions. Our benefit package also demonstrates our culture of care with paid vacation, sick days and holidays as well as paid personal and maternity/parental leaves and an Employee and Family Assistance Program. Details of the benefits package will be shared in the application process.
In addition to base pay, this role is also eligible to participate in our annual incentive plan and long-term incentive plan, consistent with the terms of our program(s) where discretionary pay out of awards is reflecting components such as performance of the company and the employee. Details will be discussed through the application process.
This information is provided in compliance with applicable state equal pay and pay equity legislation and is the company's good faith and reasonable estimate of the compensation range and benefits offered for this position. The compensation offered to the successful applicant may vary based on factors including experience, skills, education, location, and other job-related reasons. Nutrien also makes internal equity a consideration in all pay decisions.
Are you a good match? Apply today!
Nutrien Ag Solutions is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics.
This job will remain posted until filled. In accordance with Nutrien policies, you will be required to undergo a background check, and may be required to undergo a substance test. While we appreciate all applications we receive, only candidates under consideration will be contacted.
To stay connected to us and for the latest job postings and news,
f
ollow us on:
LinkedIn, Facebook and Twitter
Show more
Show less","Data Analysis, Data Visualization, Reporting, SQL, DAX, PowerBI, Star Schemas, Dax Studio, CI/CD, Microsoft Power BI, Python, Uber H3 Hexagon Package, Github, Azure, SQL Server, SnowFlake, Data Cataloging, Data Documentation","data analysis, data visualization, reporting, sql, dax, powerbi, star schemas, dax studio, cicd, microsoft power bi, python, uber h3 hexagon package, github, azure, sql server, snowflake, data cataloging, data documentation","azure, cicd, data cataloging, data documentation, dataanalytics, dax, dax studio, github, microsoft power bi, powerbi, python, reporting, snowflake, sql, sql server, star schemas, uber h3 hexagon package, visualization"
Contract Data Analyst - Remote (UK based) - Outside IR35 – 6 Months,Realtime Recruitment,"County Antrim, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/contract-data-analyst-remote-uk-based-outside-ir35-%E2%80%93-6-months-at-realtime-recruitment-3766938259,2023-12-17,Belfast, United Kingdom,Associate,Onsite,"Contract Data Analyst - Remote - £350 per day - Outside IR35 - 6 Months
Realtime is looking for an experienced Data Analyst for an exclusive client, to work with them on a data cleansing project that is of critical importance to the implementation of a new system. This is a greenfield project and very exciting work.
The ideal person should have a broad understanding of data analysis, strong SQL skills and experience and a passion for working with complex data sets.
What is in it for me?
Initially, a 6-month contract (likely to extend)
Daily rate of £350 pd
Start in January 2024
What do I need as a Data Analyst?
Expertise with SQL Server Analysis Services
Proven experience as a Data Analyst with expertise in Partner for Windows
Strong analytical skills with the ability to translate data into actionable insights
Proficiency in data visualization and reporting tools
Excellent communication and presentation skills
SQL Architecture Experience
Apply today or get in touch with Nathaniel to hear more!
Show more
Show less","Data Analyst, SQL, Data Cleansing, Data Visualization, Reporting Tools, Communication, Presentation, SQL Architecture","data analyst, sql, data cleansing, data visualization, reporting tools, communication, presentation, sql architecture","communication, dataanalytics, datacleaning, presentation, reporting tools, sql, sql architecture, visualization"
Data Engineer II,Mintel,"Belfast, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-ii-at-mintel-3775954703,2023-12-17,Belfast, United Kingdom,Associate,Hybrid,"We’re looking for an experienced Data or Analytics Engineer to join our team developing, building and maintaining our Data infrastructure and pipelines. The data structures we create are consumed by application teams (using Python and AWS), data scientists (using Jupyter notebooks and AWS Sagemaker), data analysts (using Looker) and operations teams throughout the business.
Our team works on a stack consisting primarily of DBT, Snowflake and Terraform, though we also work with Fivetran, Python and the AWS stack. We are looking for candidates with general data and software knowledge and are considering experience in our specific stack a bonus.
We have a flexible approach to hybrid working and are open to discussing part time working for this role.
The base office for this role will be in Belfast.
What You Will Do:
Develop and optimize data infrastructure and pipelines:
Work within a team to design and enhance data infrastructure and pipelines using DBT, Snowflake and Terraform, ensuring reliability and a high standard of quality.
Establish coding standards and best practices:
Drive consistency by setting and maintaining coding standards, documentation, and best practices for data engineering processes, fostering clarity and efficiency.
Collaborate closely with stakeholders:
Engage with architecture, application teams, data scientists, data analysts and operations teams to understand data needs and provide effective solutions.
Stay current and contribute to continuous improvement:
Keep up with industry trends, emerging technologies, and advancements in data engineering. Actively participate in knowledge-sharing sessions, contributing to the team's continuous learning and improvement.
What We Are Looking For:
2+ years of real world development experience gathered as part of a data focused development or analytics team
Experience with SQL - ideally within a cloud data environment (we use Snowflake)
Knowledge of data modelling, data quality and cloud development concepts
An understanding of git, CI/CD processes and infrastructure as code
A high level of empathy for end users
A willingness to learn and a focus on personal growth
An enthusiasm for contributing to both new feature development and system reliability and performance improvements
Experience with DBT (data build tool), Snowflake and Terraform would be a plus - this is what you will spend your time in day to day
What you'll get in return
A culture that supports true collaboration whilst embracing remote working with a company wide hybrid working week.
Approach to personal development where we encourage individuals to grow and share what they’ve learned.
Social events, both within the department and across the company
Generous holiday allowance with the opportunity to buy back additional holiday.
A day off to celebrate your birthday
Giving back is part of our culture with this in mind Mintel gives each employee 2 days per year to give to a worthwhile cause.
Why join Mintel? We are the world's leading market intelligence agency and technology underpins our success. We help our clients understand consumers and consumer markets, and we aspire to provide the market intelligence behind every great business decision. Our technology allows us to deliver data, insights, and opinions in a compelling way to our clients across the globe.
Mintel is committed to building and supporting a diverse workforce, providing opportunities for all. 26% of our Engineering team and 23% of our Engineering leaders are women. We are proud to be a signatory of Tech Talent Charter, highlighting our goal of improving diversity within our team and supporting industry-wide improvement. We offer flexible working options and provide support, development and growth opportunities.
Show more
Show less","Data engineering, Data infrastructure, Data pipelines, DBT, Snowflake, Terraform, Fivetran, Python, AWS, Jupyter notebooks, AWS Sagemaker, Looker, SQL, Data modelling, Data quality, Cloud development, Git, CI/CD processes, Infrastructure as code","data engineering, data infrastructure, data pipelines, dbt, snowflake, terraform, fivetran, python, aws, jupyter notebooks, aws sagemaker, looker, sql, data modelling, data quality, cloud development, git, cicd processes, infrastructure as code","aws, aws sagemaker, cicd processes, cloud development, data engineering, data infrastructure, data modelling, data quality, datapipeline, dbt, fivetran, git, infrastructure as code, jupyter notebooks, looker, python, snowflake, sql, terraform"
Data Scientist,Search 5.0,"Belfast, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/data-scientist-at-search-5-0-3768098758,2023-12-17,Belfast, United Kingdom,Associate,Hybrid,"Data Scientist
Belfast
The company
This is an opportunity for a Data Scientist to work with one of the biggest names in the industry and take their career to the next level. They have a global presence, instantly recognisable brand and have been solving complex data problems in the real word.
You will work along side some of the best Data talent who all working on the next generation of data solutions. There is no better time to join as they continue to scale their global team.
This would be perfect for someone looking to make their 2nd move in industry.
This role is hybrid and based in Belfast, Northern Ireland.
What you will get
Excellent training and development.
Access to continuous learning and development
Excellent holidays.
Fully funded certifications and tailored career plan.
Healthcare and insurances
Flexible hybrid working
Excellent bonus structure.
Up to £40,000 - £50,000 basic depending on experience.
What you will do
Work along-side a high-performing team of Data professionals.
Build bespoke Data Science and Data engineering solutions.
Work with a range of frameworks and tools.
Have an opportunity to learn from the best in the industry.
Collaborate with the rest of the team and knowledge share.
What you will need
Degree in relevant subject, PhD or a Masters
Experience in Data Science.
Python experience.
Exposure to AI and Machine Learning. Nice to have
Ability to work independently and with a team.
Excellent communication skills.
A right to work in the UK without sponsorship indefinitely.
Find out more
If you would like to have a confidential conversation and find out more about this opportunity, then get in touch with
Con Lam
at Search 5.0 - 07360 532134 or click on the apply button below.
Show more
Show less","Data Science, Python, AI, Machine Learning, Data Engineering, Data Solutions","data science, python, ai, machine learning, data engineering, data solutions","ai, data engineering, data science, data solutions, machine learning, python"
Data Engineer,Ocho,"Belfast, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-ocho-3781189177,2023-12-17,Belfast, United Kingdom,Mid senior,Onsite,"Data Engineer
5+ years experience
Comfortable with Python
Please only UK visa holders living in Belfast to apply
Company Overview:
Our client are a Belfast technology company that specializes in delivering innovative solutions to theirclients. Their talented team of experts is at the forefront of digital transformation, providing cutting-edge services and products.
Position Overview:
As a Data Engineer, you will play a pivotal role in designing, implementing, and maintaining robust data solutions. The ideal candidate will possess strong Python programming skills, a deep understanding of databases, and hands-on experience with cloud platforms. You will work in a collaborative environment, interfacing with cross-functional teams to deliver high-quality data engineering solutions.
Key Responsibilities:
Data Architecture Design:
Collaborate with stakeholders to understand data requirements and design scalable and efficient data architectures.
Python Programming:
Utilize your expertise in Python to develop, test, and maintain data processing pipelines and ETL workflows.
Database Management:
Manage and optimize databases, ensuring data quality, integrity, and security.
Implement performance tuning strategies for database operations.
Cloud Platform Experience:
Demonstrate proficiency in cloud platforms, particularly AWS or Azure, to deploy and manage data solutions.
ETL Development:
Design and implement ETL processes to extract, transform, and load data from diverse sources into our data ecosystem.
Collaboration:
Work closely with data scientists, analysts, and other stakeholders to understand their requirements and deliver tailored data solutions.
Documentation:
Maintain comprehensive documentation for data engineering processes, ensuring knowledge transfer and scalability.
Qualifications and Skills:
Bachelor’s degree in Computer Science, Engineering, or a related field.
Proven experience as a Data Engineer, with a focus on Python programming.
Strong understanding of database systems (SQL, NoSQL) and hands-on experience with data modeling.
Experience with cloud platforms such as AWS or Azure.
Familiarity with big data technologies (e.g., Hadoop, Spark) is a plus.
Excellent problem-solving and communication skills.
Ability to work independently and collaboratively in a team environment.
If you are passionate about data engineering, Python, and cutting-edge technologies, and you thrive in a collaborative and innovative environment, we invite you to apply for this exciting opportunity.
Please reach out to Brendan McCrory directly with any questions on the role.
Show more
Show less","Python, Database Management, Data Architecture, AWS, Azure, ETL, Big Data, Hadoop, Spark, SQL, NoSQL, Data Modeling, Collaborative Environment, Communication, Problem Solving","python, database management, data architecture, aws, azure, etl, big data, hadoop, spark, sql, nosql, data modeling, collaborative environment, communication, problem solving","aws, azure, big data, collaborative environment, communication, data architecture, database management, datamodeling, etl, hadoop, nosql, problem solving, python, spark, sql"
"Senior Software Engineer, Data Platform","DailyPay, Inc.","Belfast, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-software-engineer-data-platform-at-dailypay-inc-3765035701,2023-12-17,Belfast, United Kingdom,Mid senior,Onsite,"About Us:
DailyPay is the leader in the on-demand pay industry with an unrivaled technology platform, an unmatched list of blue-chip clients and an extensive list of industry awards. We are rewriting the invisible rules of finance by creating a new financial system. A financial system that is more equitable and inclusive, and benefits everyone. A financial system that enables workers to access their earned pay when they need it. We believe that money should move faster and smoother between employer and employee, between merchant and shopper, between financial institution and customer.
We are a mission-driven company hyper-focused on designing technology that can build a better financial system and future. It's no wonder that we are growing at an extraordinary pace. Now we are looking for people who are as passionate as we are about reimagining how money moves. If you're willing to define new rules, change systems and lives, come join us at DailyPay.
The Role:
Our applications make millions of dollars available to employees every day preventing late fees, use of predatory lending, and offering clarity into earned wages. We plan on being around for a long time and are investing in a platform to sustain that: tools to support the deployment, uptime, and reliability of other teams' services.
You'll work alongside team members to improve how other developers deploy and access their services, how we move data across our platform, and how we automate operations to enable future scale. You'll be responsible for:
Building and maintaining applications and infrastructure that support our data services in areas like data pipelines and analysis, and machine learning
Collaborating with and supporting other teams on how to build and mature their services to use shared resources provided by the infrastructure team and how to be monitorable, serviceable, and stable
Growing in your knowledge and capabilities to contribute even more to our platform
Essential Criteria:
Experience developing applications and their deployment mechanisms, including testing, scaling, and monitoring
Experience with one or more programming languages such as Go and Python
Experience in apps using Kafka or other event stream/message broker platforms
Experience with infrastructure-as-code, cloud formation, terraform, etc
Some familiarity with Infrastructure like Databases, API Layers, Load Balancers, etc
A critical eye, a curious nature, and a voracious appetite to learn
An interest in documented, measurable systems that fix themselves
What We Offer:
Competitive compensation
Private health insurance option
Employee Resource Groups
Fun company outings and events
Generous PTO Allowance
5% Pension contribution
Opportunity for equity ownership
Pay Transparency.
DailyPay takes a market-based approach to compensation and compensation may vary depending on your location. U.S. locations are categorized into two tiers based on a cost of labor index for that geographic area. The salary ranges are listed by geographic tier. Additionally, this role may be eligible for variable incentive compensation in addition to stock options. Where a candidate fits within the compensation range for a role is based on their demonstrated experience, qualifications, skills and internal equity.
Compensation Range (Belfast)
£55,100—£71,300 GBP
DailyPay does not accept and will not review unsolicited resumes from search firms.
DailyPay is committed to fostering an inclusive, equitable culture of belonging, grounded in empathy and respect, which values openness to opinions, awareness of lived experiences, fair treatment and access for all. We strive to build and develop diverse teams to create an organisation where innovation thrives, where the full potential of each person is engaged, and their views, beliefs and values are integrated into our ways of working.
We are an equal opportunities employer and welcome applications from all sections of the community.
Show more
Show less","Go, Python, Apache Kafka, Infrastructureascode, Terraform, Cloud formation, Databases, API Layers, Load Balancers","go, python, apache kafka, infrastructureascode, terraform, cloud formation, databases, api layers, load balancers","apache kafka, api layers, cloud formation, databases, go, infrastructureascode, load balancers, python, terraform"
Senior Data Analyst,Ocorian,"Belfast, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-ocorian-3714696669,2023-12-17,Belfast, United Kingdom,Mid senior,Remote,"Company Description
Ocorian delivers administration and compliance services for funds, corporate, capital markets and private clients.
No matter where in the world our clients' hold financial interests, or however they are structured or need to be supported, we will provide a tailored and personalised solution that is individual to their needs and fully compliant to the needs of their stakeholders, regulators and the investment itself.
We manage over 15,000 structures for 6000+ clients with a global footprint operating from 20+ locations. Our scale offers all our people great opportunities to develop their knowledge and skills and to progress their careers.
Job Description
Purpose of the Job
Ocorian are hiring a Senior Data and BI Associate as part of the company’s organic growth.
Reporting to the Global Head of Analytics, the post-holder will be part of the Finance Team.
This role involves providing expert database and BI stack knowledge.
To perform database and BI stack tasks in accordance with Group standards and industry best practice to support Ocorian data and BI requirements.
To utilize Microsoft database and business intelligence for delivery of database solutions, dataflow, Tabular data modelling, dashboards, and reporting.
To maintain and contribute to documentation and knowledge base for subject matter.
Where the role involves interactions across Ocorian, the ability to work logically and efficiently with excellent communication and reporting skills are necessary.
Main Responsibilities
To analyse requirements and systems to provided data and BI/reporting solutions to all users at Ocorian.
To utilize BI tools such as automated data processing, dataflows, and delivery of BI solutions.
Using preexisting codebase, support and enhance SQL objects, SSAS Tabular data models, SSRS paginated reports, and Power BI dashboards.
Be a team player and assist other members of the Company including the sharing of knowledge and skills.
Be proactive, manage expectations and keep stakeholders regularly updated on the progress.
Where this includes working with colleagues across the business, regularly reporting to the Global Head of Analytics on database and BI task.
To support the implementation of Group Ocorian information security policies and standards. Ability to follow programming standards, naming conventions, and design best practices.
To work with Group IT Team for corrective actions, resolution progress, and mitigation of vulnerabilities.
And any other such duties that might be reasonably required for this role.
Qualifications
Knowledge, Skills & Experience
Degree holder
5+ years of SQL programming and Microsoft BI stack experience, covering:
Advanced T-SQL and able to demonstrate knowledge of creating SQL programmability objects (functions and stored procedures), using cursors, loops, CTE, dynamic-SQL, SQL/JSON functionality an advantage.
Database design and data modelling experience.
Experience of SQL Server Integration Services creating data flow from various data sources.
Experience of SQL Server Analysis Services creating and editing Tabular data models with understanding of row level security, knowledge of DAX and advantage.
Experience of SQL Server Reporting Services, creation of paginated reports, administration of folders/user access, creation of data driven subscriptions.
Experience gained within a financial services or finance background, with an understanding of front office trust/fund systems and accounting systems.
Power BI Desktop and Power BI Service administration advantageous.
The postholder must be diligent and committed to continuous learning, with a positive outlook.
Excellent people skills with high level of integrity, strong written and oral communication skills, and the ability to work under pressure.
Ability and willingness to support the future growth and success of the Group.
Additional Information
All staff are expected to embody our core values that underpin everything that we do and that reflect the skills and behaviours we all need to be successful. These are:
We are AMBITIOUS – We aim high and are always seeking new ways to delight our people and clients
We are AGILE – We act on our initiative to get things done
We are COLLABORATIVE – We achieve more working together
We are ETHICAL – We behave with integrity at all times
Show more
Show less","SQL, TSQL, SSAS, SSRS, Power BI, Power BI Desktop, Power BI Service, DAX, CTE, JSON, Data modeling, Dataflow, SQL Server Integration Services, SQL Server Analysis Services, SQL Server Reporting Services, Front office trust/fund systems, Accounting systems","sql, tsql, ssas, ssrs, power bi, power bi desktop, power bi service, dax, cte, json, data modeling, dataflow, sql server integration services, sql server analysis services, sql server reporting services, front office trustfund systems, accounting systems","accounting systems, cte, dataflow, datamodeling, dax, front office trustfund systems, json, power bi desktop, power bi service, powerbi, sql, sql server analysis services, sql server integration services, sql server reporting services, ssas, ssrs, tsql"
Data Engineer,Anson McCade,"Belfast, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-anson-mccade-3778516561,2023-12-17,Belfast, United Kingdom,Mid senior,Hybrid,"Principal Data Engineer
£65,000 - £87,000 + 10% Bonus
Belfast based - Hybrid working
A consultancy at the heart of ground-breaking innovation and transformative solutions where expertise meets ambition. With a focus on collaborative brilliance shaping industries and propelling client towards their full potential whilst driving meaningful change is seeking to add a Principal Data Engineer to the team based in Belfast.
Requirements of a Principal Data Engineer;
Technical experience working with; Python, Pyspark, Java, SQL, AWS
Experience working on the strategy to build out a capability
Experience training of junior members of the team
Exposure to Data Science principals
Strong leadership experience
Principal Data Engineer
£65,000 - £87,000 + 10% Bonus
Show more
Show less","Python, Pyspark, Java, SQL, AWS, Data Science, Leadership, Data Engineering","python, pyspark, java, sql, aws, data science, leadership, data engineering","aws, data engineering, data science, java, leadership, python, spark, sql"
Data Engineer,Anson McCade,"Belfast, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-anson-mccade-3766803556,2023-12-17,Belfast, United Kingdom,Mid senior,Hybrid,"Data Engineer
Belfast based - Hybrid
Overview
Do you have what it takes to turn raw data into actionable insights and drive innovation?
One of UK's most well renowned innovation and transformation consultancies is searching for a Data Engineer to come join their Digital Engineering hub based in Belfast. The organisation is known for delivering Greenfield cutting-edge digital solutions using new emerging technologies and trends.
As part of their Digital team, you’ll work alongside colleagues from across the engineering hub – delivering transformative digital solutions to today’s most complex business challenges. Their teams are trusted to deliver and given the space to be awesome. They're an inclusive community for the curious, generous, pragmatic and committed digital practitioner.
This is a fantastic opportunity to join an organisation that values and invests in their culture, regularly running events, providing free certifications to learn from, allowing staff to work with flexibility to suit their needs and more.
The role will offer you the chance to get involved in solving a wide array of complex problems. These could be for start-ups wanting to scale at pace or some of the globes biggest household names.
Role requirements
2+ years experience with Python, Spark, or Py-Spark
Experience with SQL for databases
Experience developing Data/ETL pipelines
Knowledge of AWS cloud
Salary details
Base salary: £32,000 - £40,000
Pension: 8% non contributory
Bonus: 10%
If this sounds like something you would be interested in make sure to apply now and we'll get in touch!
Show more
Show less","Data Engineering, Python, Apache Spark, ETL Pipelines, SQL, AWS","data engineering, python, apache spark, etl pipelines, sql, aws","apache spark, aws, data engineering, etl pipelines, python, sql"
Data Engineer,PA Consulting,"Belfast, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-pa-consulting-3737693929,2023-12-17,Belfast, United Kingdom,Mid senior,Hybrid,"Company Description
We believe in the power of ingenuity to build a positive human future.
As strategies, technologies and innovation collide, we create opportunity from complexity.
Our diverse teams of experts combine innovative thinking and breakthrough use of technologies to progress further, faster. Our clients adapt and transform, and together we achieve enduring results.
An innovation and transformation consultancy, we are over 4000 specialists in consumer and manufacturing, defence and security, energy and utilities, financial services, government and public services, health and life sciences, and transport. Our people are strategists, innovators, designers, consultants, digital experts, scientists, engineers and technologists.
We operate globally from offices across the UK, US, Netherlands and Nordics. We are guided by our client work and needs; however, you have autonomy to manage your time and diary to suit your work/life balance.
Job Description
Do you have what it takes to turn raw data into actionable insights and drive innovation?
As a team, we transform our client’s businesses using a combination of strategic thinking, customer-centric service design, and agile engineering practices. We do this at internet scale – driving innovation, and enriching people’s lives. Our skills and talent enable PA’s purpose of creating positive a human future in a technology driven world.
As part of our Digital team, you’ll work alongside colleagues from across PA – delivering transformative digital solutions to today’s most complex business challenges. Our teams are trusted to deliver and given the space to be awesome. We’re an inclusive community for the curious, generous, pragmatic and committed digital practitioner.
You'll have:
Experience in the design and deployment of production data pipelines from ingestion to consumption within a big data architecture, using Java, Python, Scala, Spark, SQL. Perform tasks such as writing scripts, extracting data using APIs, writing SQL queries etc. Work closely with other engineering teams to integrate data engineering component into production systems. Knowledge of data cleaning, wrangling, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Ability to travel to client site, where required, will be a consideration.
Experience in processing large amounts of structured and unstructured data, including integrating data from multiple sources through ingestion and curation functions on AWS cloud using AWS native or custom programming. Knowledge of data mining, machine learning, natural language processing is an advantage. You enjoy working within cross-functional Agile teams and you are familiar with Scrum ceremonies.
You’ll be comfortable designing and building for the AWS cloud and will have designed and worked on architectures that include Platform-as-a-Service components and perhaps even server-less and container technologies.
Qualifications
AWS is a significant growth area for us with a diverse and growing capability and we are looking for a Data Engineer with experience in AWS cloud technologies for ETL pipeline, data warehouse and data lake design/building and data movement. There are a variety of different tools, cloud technologies and approaches and while we have a preference for AWS tooling experience, open-source equivalence will be suitable.
As an Data Engineer, you’ll have experience working in teams to design, build, and maintain large scale data solutions and applications using AWS data and analytics services (or open-source equivalent) such as EMR, Glue, RedShift, Kinesis, Lambda, DynamoDB. Your team members will look to you as a trusted expert and will expect you to define the end-to-end software development lifecycle in line with modern AWS best practices. Your AWS experience spans data engineering, data science and product development projects, plus you will have an understanding of stream and batch processing.
Additional Information
Life At PA encompasses our peoples' experience at PA. It's about how we enrich peoples’ working lives by giving them access to unique people and growth opportunities and purpose led meaningful work.
We believe diversity fuels ingenuity. Diversity of thought brings exciting perspectives; diversity of experience brings a wealth of knowledge, and diversity of skills brings the tools we need. When we bring people together with diverse backgrounds, identities, and minds, embracing that difference through an inclusive culture where our people thrive; we unleash the power of diversity – bringing ingenuity to life.
Find out more about Life at PA here.
We are dedicated to supporting the physical, emotional, social and financial well-being of our people. Check out some of our extensive benefits:
Health and lifestyle perks accompanying private healthcare for you and your family
25 days annual leave with the opportunity to buy 5 additional days
Generous company pension scheme
Opportunity to get involved with community and charity-based initiatives
Annual performance-based bonus
PA share ownership
Tax efficient benefits (cycle to work, give as you earn)
We recruit, retain, reward and develop our people based solely on their abilities and contributions and without reference to their age, background, disability, genetic information, parental or family status, religion or belief, race, ethnicity, nationality, sex, sexual orientation, gender identity (or expression), political belief veteran status, or other by any other range of human difference brought about by identity and experience. We are on a journey towards ensuring our workforce is diverse at all levels and that our firm is representative of the world around us. We welcome applications from underrepresented groups.
Adjustments or accommodations
- Should you need any adjustments or accommodations to the recruitment process, at either application or interview, please contact us.
Show more
Show less","Java, Python, Scala, Spark, SQL, AWS, EMR, Glue, RedShift, Kinesis, Lambda, DynamoDB, ETL, Data warehouse, Data lake, Data mining, Machine learning, Natural language processing, Scrum, PlatformasaService, Serverless, Container technologies, Data engineering, Data science, Product development, Stream processing, Batch processing","java, python, scala, spark, sql, aws, emr, glue, redshift, kinesis, lambda, dynamodb, etl, data warehouse, data lake, data mining, machine learning, natural language processing, scrum, platformasaservice, serverless, container technologies, data engineering, data science, product development, stream processing, batch processing","aws, batch processing, container technologies, data engineering, data lake, data mining, data science, datawarehouse, dynamodb, emr, etl, glue, java, kinesis, lambda, machine learning, natural language processing, platformasaservice, product development, python, redshift, scala, scrum, serverless, spark, sql, stream processing"
Lead Data Engineer,PA Consulting,"Belfast, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-pa-consulting-3737699213,2023-12-17,Belfast, United Kingdom,Mid senior,Hybrid,"Company Description
We believe in the power of ingenuity to build a positive human future.
As strategies, technologies and innovation collide, we create opportunity from complexity.
Our diverse teams of experts combine innovative thinking and breakthrough use of technologies to progress further, faster. Our clients adapt and transform, and together we achieve enduring results.
An innovation and transformation consultancy, we are over 4000 specialists in consumer and manufacturing, defence and security, energy and utilities, financial services, government and public services, health and life sciences, and transport. Our people are strategists, innovators, designers, consultants, digital experts, scientists, engineers and technologists.
We operate globally from offices across the UK, US, Netherlands and Nordics. We are guided by our client work and needs; however, you have autonomy to manage your time and diary to suit your work/life balance.
Job Description
Are you up for the challenge of creating seamless user experiences from every angle?
Our teams are trusted to deliver and given the space to be awesome. We’re an inclusive community for the curious, generous, pragmatic and committed digital practitioner. You'll be leading teams who are collectively responsible for delivering functioning solutions to our clients. Junior members of the team will look to you for support, guidance and help with decisions.
You'll have:
Experience in the design and deployment of production data pipelines from ingestion to consumption within a big data architecture, using Java, Python, Scala, Spark, SQL. Perform tasks such as writing scripts, extracting data using APIs, writing SQL queries etc. Work closely with other engineering teams to integrate data engineering component into production systems. Knowledge of data cleaning, wrangling, visualization and reporting, with an understanding of the best, most efficient use of associated tools and applications to complete these tasks. Ability to travel to client site, where required, will be a consideration.
Experience in processing large amounts of structured and unstructured data, including integrating data from multiple sources through ingestion and curation functions on AWS cloud using AWS native or custom programming. Knowledge of data mining, machine learning, natural language processing is an advantage. You enjoy working within cross-functional Agile teams and you are familiar with Scrum ceremonies.
You’ll be comfortable designing and building for the AWS cloud and will have designed and worked on architectures that include Platform-as-a-Service components and perhaps even server-less and container technologies.
Apply today by completing our online application.
Qualifications
AWS is a significant growth area for us with a diverse and growing capability and we are looking for a data engineer with experience in AWS cloud technologies for ETL pipeline, data warehouse and data lake design/building and data movement. There are a variety of different tools, cloud technologies and approaches and while we have a preference for AWS tooling experience, open-source equivalence will be suitable.
As an Lead Data Engineer, you’ll have experience leading teams to design, build, and maintain large scale data solutions and applications using AWS data and analytics services (or open-source equivalent) such as EMR, Glue, RedShift, Kinesis, Lambda, DynamoDB. Your team members will look to you as a trusted expert and will expect you to define the end-to-end software development lifecycle in line with modern AWS best practices. Your AWS experience spans data engineering, data science and product development projects, plus you will have an understanding of stream and batch processing.
Additional Information
Life At PA encompasses our peoples' experience at PA. It's about how we enrich peoples’ working lives by giving them access to unique people and growth opportunities and purpose led meaningful work.
We believe diversity fuels ingenuity. Diversity of thought brings exciting perspectives; diversity of experience brings a wealth of knowledge, and diversity of skills brings the tools we need. When we bring people together with diverse backgrounds, identities, and minds, embracing that difference through an inclusive culture where our people thrive; we unleash the power of diversity – bringing ingenuity to life.
Find out more about Life at PA here.
We are dedicated to supporting the physical, emotional, social and financial well-being of our people. Check out some of our extensive benefits:
Health and lifestyle perks accompanying private healthcare for you and your family
25 days annual leave with the opportunity to buy 5 additional days
Generous company pension scheme
Opportunity to get involved with community and charity-based initiatives
Annual performance-based bonus
PA share ownership
Tax efficient benefits (cycle to work, give as you earn)
We recruit, retain, reward and develop our people based solely on their abilities and contributions and without reference to their age, background, disability, genetic information, parental or family status, religion or belief, race, ethnicity, nationality, sex, sexual orientation, gender identity (or expression), political belief veteran status, or other by any other range of human difference brought about by identity and experience. We are on a journey towards ensuring our workforce is diverse at all levels and that our firm is representative of the world around us. We welcome applications from underrepresented groups.
Adjustments or accommodations
- Should you need any adjustments or accommodations to the recruitment process, at either application or interview, please contact us.
Show more
Show less","Java, Python, Scala, Spark, SQL, Data engineering, AWS, Data cleaning, Data wrangling, Data visualization, Data reporting, Data mining, Machine learning, Natural language processing, Agile, Scrum, ETL pipeline, Data warehouse, Data lake, EMR, Glue, RedShift, Kinesis, Lambda, DynamoDB","java, python, scala, spark, sql, data engineering, aws, data cleaning, data wrangling, data visualization, data reporting, data mining, machine learning, natural language processing, agile, scrum, etl pipeline, data warehouse, data lake, emr, glue, redshift, kinesis, lambda, dynamodb","agile, aws, data cleaning, data engineering, data lake, data mining, data reporting, data wrangling, datawarehouse, dynamodb, emr, etl pipeline, glue, java, kinesis, lambda, machine learning, natural language processing, python, redshift, scala, scrum, spark, sql, visualization"
Principal Data Engineer,Rapid7,"Belfast, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/principal-data-engineer-at-rapid7-3731725431,2023-12-17,Belfast, United Kingdom,Mid senior,Hybrid,"Rapid7 seeks an experienced, highly driven, and dynamic Principal Data Engineer to take our data engineering function to the next level. Come and join our efforts in unlocking the value of data through industry-leading innovation, cutting edge modern tooling, democratization at scale and building exceptional and trusted data products for the company!
About The Team
As we spearhead a cultural shift to a data-driven business, Data Engineering serves as the Hub for all teams at Rapid7 from ML Ops, to Sales and Operations to Platform and Engineering. Our team is a highly skilled yet egoless group of data magicians (and humorists) with a penchant for innovation and a knack for problem solving.
About The Role
As a Principal Data Engineer, you will be responsible for the technical advancement of our data engineering function tackling our most complex challenges, mentoring our highly motivated core of data engineers and senior data engineers, and building strong partnerships with data consumers across the organization. You’ll have the liberty to drive business value through thought leadership and innovation as we continue to work towards the high standard we’ve set for ourselves - a best-in-class data platform.
In This Role, You Will
Scope and stage larger scale initiatives into well-defined milestones to avoid a monolithic deliverables
Provide thought leadership and contribute to the vision of our data engineering function
Lead team processes such as on-call rotations, bug triage, technical direction, standards, and execution
Own delivery architecture/execution of major component(s) from conception to release
Looked up to for technical mentorship within the data engineering team. Make others better through code reviews, focus on documentation, and technical guidance
Act as a resource sought for technical advice and weigh in on technical decisions that impact other teams as well
Understand the tradeoffs between technical and business needs, interact and negotiate with key stakeholders, and deliver solutions that take all of these needs into account
Regularly take complex designs / codebases and simplify them without being asked
Regularly contribute improvements to team’s existing SDLC (ie: CICD) or other methods, programs, etc
The Skills You’ll Bring Include
8+ years of hands on data engineering experience and at least 4 years in a senior-level data engineering role
8+ years of experience in at least one programming language such as Python, Java, Scala is required (Python is our most commonly used language); Advanced SQL expertise is required
Experience working in a modern lakehouse is required (Snowflake is preferred); Modern warehousing best practices should be second nature
Cloud experience is required (AWS is strongly preferred); Terraform is highly preferred
Knowledge and ideally hands on experience working with container services is required (ECS, Kubernetes, etc)
Experience working in a mature SDLC environment (ie: CICD) is required
Modern tech stack experience is a plus (dbt, Fivetran, Snowflake. Airflow)
Experience as a leader within a data engineering team and ability to mentor teammates
Strong work ethic, resiliency, persistence, and urgency; Data Engineering holds itself to a high standard so you’ll need to keep up!
Sharp business and interpersonal skills; ability to influence at senior levels across business units to drive change and achieve common goals
BS or MS in Computer Science, Analytics, Statistics, Informatics, Information Systems or
Another Quantitative Field Or Equivalent Experience
We know that the best ideas and solutions come from multi-dimensional teams. That’s because these teams reflect a variety of backgrounds and professional experiences. If you are excited about this role and feel your experience can make an impact, please don’t be shy - apply today.
About Rapid7
At Rapid7, we are on a mission to create a secure digital world for our customers, our industry, and our communities. We do this by embracing tenacity, passion, and collaboration to challenge what’s possible and drive extraordinary impact.
Here, we’re building a dynamic workplace where everyone can have the career experience of a lifetime. We challenge ourselves to grow to our full potential. We learn from our missteps and celebrate our victories. We come to work every day to push boundaries in cybersecurity and keep our 10,000 global customers ahead of whatever’s next.
Join us and bring your unique experiences and perspectives to tackle some of the world’s biggest security challenges.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, protected veteran status or any other status protected by applicable national, federal, state or local law.
Show more
Show less","Data Engineering, Data Analysis, Data Warehousing, Python, Java, Scala, SQL, Snowflake, AWS, Terraform, ECS, Kubernetes, SDLC, CICD, dbt, Fivetran, Airflow, Leadership, Mentoring, Communication, Problem Solving, Innovation","data engineering, data analysis, data warehousing, python, java, scala, sql, snowflake, aws, terraform, ecs, kubernetes, sdlc, cicd, dbt, fivetran, airflow, leadership, mentoring, communication, problem solving, innovation","airflow, aws, cicd, communication, data engineering, dataanalytics, datawarehouse, dbt, ecs, fivetran, innovation, java, kubernetes, leadership, mentoring, problem solving, python, scala, sdlc, snowflake, sql, terraform"
Business Data Analyst,Thrive IT Systems Ltd,"Belfast, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/business-data-analyst-at-thrive-it-systems-ltd-3787095339,2023-12-17,Belfast, United Kingdom,Mid senior,Hybrid,"Must Have skill-
BA. Experienced in Balance Sheet and Reconciliations processes. Analyze/challenge investigations across balance sheet accounts. Requirements gathering, BRD preparation, flow creation for exception generation and remediation processes. Working with Technology, Finance, Operations etc. Manage UAT and post prod release validation from BA perspective.
Job description -
Role & Responsibilities:
Data Analysis/BA experience
A SME/Substantiation champion having in-depth experience in Financial accounting/Controller with a global bank.
Deep understanding/knowledge of principles, practises, and theories of Accounting (GL, BS & Account Control, Charts of Accounts etc).
Strong expertise in data analysis and reconciliation on large data sets, mappings and setups.
Should be able to identify remediate and perform a root cause analysis on the results breaks.
Conduct quality assurance reviews to assess reconciliation and substantiation practises.
Analyse complex business requirements and translate them into functional specifications and technical requirements and signoffs and
process flows and exception identification process.
Strong expertise in Excel functionalities and Business Intelligence tools like Tableau, PowerBI.
Work with Technology, Operations and Finance for release related activities and risk and issue mitigation
Testing:
Strong experience in testing lifecycle - UAT and Defect Resolution
Assist end users in testing.
Postproduction Support
Stakeholder Collaboration:
Demonstrates collaboration skills and techniques which appropriately define alternate solutions; resolve conflicts; and create lasting, productive partnerships with clients.
Able to work effectively and efficiently toward goals in a complex, diverse environment with multiple and changing demands.
Liaise with business users, technology teams, compliance officers, and operations teams to gather requirements and ensure alignment with business objectives.
Collaborate with stakeholders to define key performance indicators (KPIs) and metrics for measuring and monitoring investment banking performance.
Communicate effectively with stakeholders at all levels, including presenting findings, recommendations, and project updates.
Requirements:
Bachelor/master’s degree in finance or MBA. Accounting qualification (CA, CPA etc)
Experience of leading redesign, centralisation, standardisation and automate of Balance Sheet Substantiation activities.
Experience in designing and managing control functions in Financial services/Investment banks.
Strong Accounting knowledge and full understanding of account classification in General Ledger
Hands on years of experience as a Data Analyst, Business Analyst, or related role in the investment banking domain.
Hands-on experience with data analysis and interpretation, including data mapping, data transformation, and statistical analysis.
Proficient in using data analysis tools, data manipulation languages, and reporting tools (e.g., SQL, Excel, Tableau, Power BI).
Strong analytical and problem-solving skills, with the ability to derive insights from complex datasets.
Excellent communication and presentation skills, with the ability to effectively convey technical information to non-technical stakeholders.
Detail-oriented with strong organizational and multitasking abilities.
Show more
Show less","Balance Sheet, Reconciliation, Data Analysis, Financial Accounting, Controller, Chart of Accounts, Excel, Tableau, PowerBI, UAT, Defect Resolution, Stakeholder Collaboration, KPIs, CA, CPA, General Ledger, Data Analyst, Data Mapping, Data Transformation, SQL","balance sheet, reconciliation, data analysis, financial accounting, controller, chart of accounts, excel, tableau, powerbi, uat, defect resolution, stakeholder collaboration, kpis, ca, cpa, general ledger, data analyst, data mapping, data transformation, sql","balance sheet, ca, chart of accounts, controller, cpa, data mapping, data transformation, dataanalytics, defect resolution, excel, financial accounting, general ledger, kpis, powerbi, reconciliation, sql, stakeholder collaboration, tableau, uat"
Database Reliability Engineer (Hybrid Working),Stone Tide,"Belfast, Northern Ireland, United Kingdom",https://uk.linkedin.com/jobs/view/database-reliability-engineer-hybrid-working-at-stone-tide-3772736505,2023-12-17,Belfast, United Kingdom,Mid senior,Hybrid,"A wonderful opportunity for a Database Reliability Engineer who is interested in working in a profitable, well established, market leader who are shaping the progression of cutting edge digital healthcare technologies to benefit the UK population with leading clinical software.
The right type of candidate will have :-
5+ years of experience in RDBMS database administration or database reliability engineering, in large mission-critical production environments.
3+ years of experience in database performance monitoring and tuning.
2+ years experience in data lake administration.
Tech Skills to note:-
Prometheus OR Grafana OR Cloudwatch database monitoring skills
PostgresQL OR Oracle SQL OR Data Lake administration and design skills
Comfortable working in Windows Powershell OR Linux/Unix Shell
This role would suit a candidate who is interested in working for a business who are making a difference to healthcare in the UK, working with an established cohesive team with good work ethic and real camaraderie. My client are very focussed on creating an excellent working environment to ensure their staff and happy and content. This is reflected in that they offer:-
Hybrid working
Unlimited holiday time
12 weeks of paid sick leave
5% Pension Contribution
TravelFlex - Every employee can work for 30 days per year in another country.
Healthcare and Medical Benefits for employees
Dental cover for employees
4 times Life Assurance cover
Work with high-end hardware and systems
Flexible, social and professional company culture; work from home flexibility
Support to learn and grow by providing unlimited access to LinkedIn Learning.
For more information, please apply for a full JD
Show more
Show less","Database Administration, Database Reliability Engineering, RDBMS, Performance Monitoring, Data Lake Administration, Prometheus, Grafana, Cloudwatch, PostgreSQL, Oracle SQL, Windows PowerShell, Linux/Unix Shell","database administration, database reliability engineering, rdbms, performance monitoring, data lake administration, prometheus, grafana, cloudwatch, postgresql, oracle sql, windows powershell, linuxunix shell","cloudwatch, data lake administration, database administration, database reliability engineering, grafana, linuxunix shell, oracle sql, performance monitoring, postgresql, prometheus, rdbms, windows powershell"
AI/ML Engineer-Data Scientist,Saransh Inc,"Los Angeles, CA",https://www.linkedin.com/jobs/view/ai-ml-engineer-data-scientist-at-saransh-inc-3760316009,2023-12-17,Malibu,United States,Mid senior,Onsite,"We are Looking for AI/ML Engineer-Data Scientist for one of our Healthcare Client
Experience in LLM and Generative AI
Experience in Document extraction/chat
Strong experience in Python, NLP
Experience working in a cloud-native environment such as AWS
Should have hands on experience with AWS Neptune or Neo4J graph database
Experience in building and maintaining open-domain or health care domain-specific ontologies
Understanding of knowledge graphs
Have experience in building graph-based ontology from scratch and working with structured and unstructured data
Experience supporting ML models development on big data infrastructure (on knowledge graph would be a bonus)
Hands on python to build knowledge Graph/ontologies.
Experience with AWS Textract, Comprehend Medical (nice to have
Show more
Show less","Artificial Intelligence, Machine Learning, Generative AI, Natural Language Processing, Python, Cloudnative Environment, AWS, AWS Neptune, Neo4J, Knowledge Graphs, Ontologies, Knowledge Graph Construction, Big Data Infrastructure, AWS Textract, AWS Comprehend Medical","artificial intelligence, machine learning, generative ai, natural language processing, python, cloudnative environment, aws, aws neptune, neo4j, knowledge graphs, ontologies, knowledge graph construction, big data infrastructure, aws textract, aws comprehend medical","artificial intelligence, aws, aws comprehend medical, aws neptune, aws textract, big data infrastructure, cloudnative environment, generative ai, knowledge graph construction, knowledge graphs, machine learning, natural language processing, neo4j, ontologies, python"
Senior Cloud Data Engineer,"Ursus, Inc.","Westlake Village, CA",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-ursus-inc-3762446387,2023-12-17,Malibu,United States,Mid senior,Onsite,"JOB TITLE: Senior Cloud Data Engineer
TOP 3 SKILLS:**
AWS Redshift
Python
SQL
LOCATION: Hybrid
-
2 Days/Week Onsite
-
Westlake Village, CA
DURATION: 3-6 Month Contract to Hire Opportunity
RATE RANGE: $55-$60/Hour
Summary
This role is a highly visible and value driven individual contributor role, providing data infrastructural help for fellow analysts and data engineers. Chosen candidate will be hands on developing ETL frameworks, Pipelines and managing current data Systems and implementing new pipelines and integrations. Candidate should be SQL savvy, self-sufficient in handling large volumes of data as well as excellent data visualization capabilities using Tableau or like solutions
Responsibilities
Design and build reliable, scalable data infrastructure with leading privacy and security techniques to safeguard data using AWS technologies
Design and Build scalable, secure, low latency, resilient and cost-effective solutions for enabling predictive and prescriptive analytics across the organization
Take over and scale our data models (Tableau, Dynamo DB, Kibana)
Experience in shipping low-latency massive scale systems to production
Communicate data-backed findings to a diverse constituency of internal and external stakeholders
Build frameworks for data ingestion pipeline both real time and batch using best practices in data modeling, ETL/ELT processes and hand off to data engineers
Participate in technical decisions and collaborate with talented peers.
Review code, implementations and give meaningful feedback that helps others build better solutions.
Helps drive technology direction and choices of technologies by making recommendations based on experience and research.
Qualifications
7 or more years of experience working directly with enterprise data solutions
Hands on experience working in a public cloud environment and on-prem infrastructure.
Specialty on Columnar Databases like Redshift Spectrum, Time Series data stores like Apache Pinot and the AWS cloud infrastructure
Experience with in-memory, serverless, streaming technologies and orchestration tools such as Docker, Spark, Kafka, Airflow, Kubernetes is needed
Excellent SQL skills and Python coding is a must
Current hands-on implementation experience required, possessing 5or more years of IT platform implementation experience.
AWS Certified Big Data - Specialty desirable
Experience designing and implementing AWS big data and analytics solutions in large digital and retail environments is desirable
Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases, data lakes, and schemas.
Experience with AWS Cloud Data Lake Technologies and operational experience of Kinesis/Kafka, S3, Glue and Athena.
Experience with any of the message / file formats: Parquet, Avro, ORC
Design and development experience on subscribing to a Streaming Service, EMS, MQ, Java, XSD, File Adapter, and ESB based applications
Experience in distributed architectures such as Microservices, SOA, RESTful APIs and data integration architectures is a plus
Hands on experience migrating On-Prem Data solutions to cloud
Prior experience managing On-prem Enterprise Data Warehouse solutions like Netezza, Microsoft SQL Server Enterprise edition, MY SQL is a plus
Experience with a wide variety of data processing technologies, including
Big Data Stack (Spark, spectrum, Flume, Kafka, Kinesis etc.)
Data streaming (Kafka, SQS/SNS queuing, etc)
Expert in Columnar databases primarily, Redshift or like technologies lile Snowflake, Firebolt
Expert in Commonly used AWS services (S3, Lambda, Redshift, Glue, EC2, etc)
Expertise in Python, pySpark or similar programming languages is a must have
BI tools (Tableau, Power BI, SSIS, SSRS, MicroStrategy) is a plus
Skilled in AWS Compute such as EC2, Lambda, Beanstalk, or ECS
Skilled in AWS Management and Governance suite of products such as CloudTrail, CloudWatch, or Systems Manager
Skilled in Amazon Web Services (AWS) offerings, development, and networking platforms
Proficiency in Netezza, Oracle, MYSQL and Microsoft SQL Server Databases is a plus
Understanding Continuous Integration / Continuous Delivery with experience in Jenkins
IND123
Show more
Show less","AWS Redshift, Python, SQL, Tableau, AWS technologies, Predictive analytics, Prescriptive analytics, Dynamo DB, Kibana, Real time, Batch, ETL/ELT processes, Apache Pinot, Docker, Spark, Kafka, Airflow, Kubernetes, OLTP, OLAP, Kinesis/Kafka, S3, Glue, Athena, Parquet, Avro, ORC, Microservices, SOA, RESTful APIs, Data integration architectures, Data warehousing, Netezza, MySQL, Microsoft SQL Server, Snowflake, Firebolt, pySpark, Power BI, SSIS, SSRS, MicroStrategy, EC2, Lambda, Beanstalk, ECS, CloudTrail, CloudWatch, Systems Manager, Jenkins","aws redshift, python, sql, tableau, aws technologies, predictive analytics, prescriptive analytics, dynamo db, kibana, real time, batch, etlelt processes, apache pinot, docker, spark, kafka, airflow, kubernetes, oltp, olap, kinesiskafka, s3, glue, athena, parquet, avro, orc, microservices, soa, restful apis, data integration architectures, data warehousing, netezza, mysql, microsoft sql server, snowflake, firebolt, pyspark, power bi, ssis, ssrs, microstrategy, ec2, lambda, beanstalk, ecs, cloudtrail, cloudwatch, systems manager, jenkins","airflow, apache pinot, athena, avro, aws redshift, aws technologies, batch, beanstalk, cloudtrail, cloudwatch, data integration architectures, datawarehouse, docker, dynamo db, ec2, ecs, etlelt processes, firebolt, glue, jenkins, kafka, kibana, kinesiskafka, kubernetes, lambda, microservices, microsoft sql server, microstrategy, mysql, netezza, olap, oltp, orc, parquet, powerbi, predictive analytics, prescriptive analytics, python, real time, restful apis, s3, snowflake, soa, spark, sql, ssis, ssrs, systems manager, tableau"
Lead Data Engineer,KORE1,"Westlake Village, CA",https://www.linkedin.com/jobs/view/lead-data-engineer-at-kore1-3624638200,2023-12-17,Malibu,United States,Mid senior,Onsite,"KORE1, a nationwide provider of staffing and recruiting solutions, has an immediate opening for ""Lead Data Engineer"".
PURPOSE OF THE JOB
This role will provide strong leadership in developing the Enterprise data architecture of Digital and Retail businesses across different guitar Center brands, providing technical guidance, architecture and enforcing technical standards. Provides assessments and prototyping of new concepts and technologies and often develop capabilities to be handed off to operational development teams. Provides analysis on complex problems as well as be an active member of enterprise technology design decisions.
Responsibilities/Duties
Accountable for modernization, migration/transformation to a cloud data platform
Design and build reliable, scalable data infrastructure with leading privacy and security techniques to safeguard data using AWS technologies.
Architect scalable, secure, low latency, resilient and cost-effective solutions for enabling predictive and prescriptive analytics across the organization.
Design/ Architect frameworks to Operationalize ML models through serverless architecture and support unsupervised continuous training models.
Take over and scale our data models (Tableau, Dynamo DB, Kibana)
Communicate data-backed findings to a diverse constituency of internal and external stakeholders.
Build frameworks for data ingestion pipeline both real time and batch using best practices in data modeling, ETL/ELT processes and hand off to data engineers.
Participate in technical decisions and collaborate with talented peers.
Review code, implementations and give meaningful feedback that helps others build better solutions.
Helps drive technology direction and choices of technologies by making recommendations based on experience and research.
Minimum Requirements & Special Attributes
10 or more years of experience working directly with enterprise data solutions.
Hands on experience working in a public cloud environment and on-prem infrastructure.
Hands on experience with Sage Maker, Jupiter Notebook, Tensor flow, Kubeflow, Airflow like platforms is required
Experience with NLP models, Churn, Retention and Anomaly detection models is required.
Specialty on Columnar Databases like Redshift Spectrum, Time Series data stores like Apache Pinot and the AWS cloud infrastructure
Experience with in-memory, serverless, streaming technologies and orchestration tools such as Spark, Kafka, Airflow, Kubernetes
Current hands-on implementation experience required, possessing 7or more years of IT platform implementation experience.
AWS Certified Big Data - Specialty desirable.
Experience designing and implementing AWS big data and analytics solutions in large digital and retail environments is desirable.
Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases, data lakes, and schemas.
Experience with AWS Cloud Data Lake Technologies and operational experience of Kinesis/Kafka, S3, Glue and Athena.
Experience with any of the message / file formats: Parquet, Avro, ORC
Design and development experience on a Streaming Service, EMS, MQ, Java, XSD, File Adapter, and ESB based applications.
Experience in distributed architectures such as Microservices, SOA, RESTful APIs and data integration architectures.
Hands on experience migrating On-Prem Data solutions to cloud.
Prior experience managing On-prem Enterprise Data Warehouse solutions like Netezza is a plus.
Experience with a wide variety of modern data processing technologies, including.
Big Data Stack (Spark, spectrum, Flume, Kafka, Kinesis etc.)
Data streaming (Kafka, SQS/SNS queuing, etc.)
Expert in Columnar databases primarily, Redshift or like technologies lile Snowflake, Firebolt
Expert in Commonly used AWS services (S3, Lambda, Redshift, Glue, EC2, etc)
Expertise in Python, pySpark or similar programming languages is a must have.
BI tools (Tableau, Domo, MicroStrategy) are a plus.
Skilled in AWS Compute such as EC2, Lambda, Beanstalk, or ECS
Skilled in AWS Management and Governance suite of products such as CloudTrail, CloudWatch, or Systems Manager
Skilled in Amazon Web Services (AWS) offerings, development, and networking platforms
Skilled in AWS Analytics such as Athena, EMR, or Glue
Proficiency in Oracle, MYSQL and Microsoft SQL Server Databases is a plus.
Understanding Continuous Integration / Continuous Delivery with experience in Jenkins
About Kore1
Specializing in professional and technical recruiting, KORE1 is committed to supporting top IT, Engineering, Creative, Scientific, Accounting and Finance professionals in their career paths. We build deep relationships with leading companies, connecting them to exceptional talent every day. With extensive industry expertise and unmatched opportunities, our goal is to provide a unique experience for our contractors and consultants as they prepare for their next role. We are passionate about matching the right people with the right companies.
Kore1 provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Kore1 complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Kore1 expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of Kore1's employees to perform their job duties may result in discipline up to and including discharge.
Show more
Show less","AWS, Data Architecture, Cloud Computing, Data Engineering, Data Modeling, Data Pipeline, Data Warehousing, Machine Learning, ETL/ELT, Spark, Kafka, Airflow, Kubernetes, Redshift, Snowflake, Firebolt, Python, Java, SQL, NoSQL, Tableau, Domo, MicroStrategy, Jenkins, EC2, Lambda, Beanstalk, ECS, CloudTrail, CloudWatch, Systems Manager, Athena, EMR, Glue, Oracle, MySQL, Microsoft SQL Server","aws, data architecture, cloud computing, data engineering, data modeling, data pipeline, data warehousing, machine learning, etlelt, spark, kafka, airflow, kubernetes, redshift, snowflake, firebolt, python, java, sql, nosql, tableau, domo, microstrategy, jenkins, ec2, lambda, beanstalk, ecs, cloudtrail, cloudwatch, systems manager, athena, emr, glue, oracle, mysql, microsoft sql server","airflow, athena, aws, beanstalk, cloud computing, cloudtrail, cloudwatch, data architecture, data engineering, data pipeline, datamodeling, datawarehouse, domo, ec2, ecs, emr, etlelt, firebolt, glue, java, jenkins, kafka, kubernetes, lambda, machine learning, microsoft sql server, microstrategy, mysql, nosql, oracle, python, redshift, snowflake, spark, sql, systems manager, tableau"
"Sr. Site Reliability Engineer (Application Software, Data)",SpaceX,"Hawthorne, CA",https://www.linkedin.com/jobs/view/sr-site-reliability-engineer-application-software-data-at-spacex-3641773386,2023-12-17,Malibu,United States,Mid senior,Onsite,"SpaceX was founded under the belief that a future where humanity is out exploring the stars is fundamentally more exciting than one where we are not. Today SpaceX is actively developing the technologies to make this possible, with the ultimate goal of enabling human life on Mars.
SR. SITE RELIABILITY ENGINEER (APPLICATION SOFTWARE), DATA
The application software team is the central nervous system of SpaceX – we create mission critical applications that are used throughout SpaceX to accelerate launch vehicle production and flight as well as systems that allow Starlink to grow into a worldwide fast, reliable Internet service. Our missions support scientific research, classified national security space, and commercial opportunities. Software engineering and innovation is at the core of these programs.
Our team is currently creating and evolving systems to enable rapid build and reuse of Starship as well as scaling the Starlink network. We have built systems to support concurrent streams of data from many always-on assets to manage the world's largest satellite constellation and the world's largest rocket. We work directly with engineers across all programs to enable and accelerate the success of Starlink, Starlink, and Starshield.
Aerospace experience is not required to be successful here - rather we look for smart, motivated, collaborative site reliability engineers who love solving problems and want to make an impact on a super inspiring mission. You will have full ownership of challenging problems, working with a team of enthusiastic engineers to design and produce solutions that enable SpaceX to move towards our goals at a rapid pace. The success of the missions at SpaceX depends on the software that you and your team produce.
RESPONSIBILITIES:
Upgrade existing distributed systems to become sharded and geo-redundant in multiple data centers
Advance existing deployment, monitoring, and alerting infrastructure to support a multi-region environment
Manage petabyte scale bare metal compute clusters
Closely collaborate with engineers across all programs to create highly operable, scalable, and maintainable products
Engage throughout the whole software development lifecycle of services -- from inception to design, deployment, operation, and iterative refinement
Focus on performance bottlenecks and performance improvement techniques
BASIC QUALIFICATIONS:
Bachelor's degree in computer science, engineering, math, or scientific discipline and 5 years of software development experience OR 7+ years of professional experience building software with site reliability or DevOps in lieu of a degree
Experience with Linux operating systems
PREFERRED SKILLS AND EXPERIENCE:
5+ years of rigorous experience with site reliability or DevOps for web applications
Programming experience in Python, C#, Java, Scala, Go or similar languages
Experience with in-stream, big data processing and analytics using Apache Kafka, Spark, Flink, SQL or similar
Experience with relational and non-relational databases, data lakes e.g. HBase, Hive, Delta Lake, PostgreSQL, CockroachDB or similar
Experience troubleshooting network-layer issues
Experience with configuring Istio
Good understanding of version control, testing, continuous integration, build, deployment and monitoring
Active Top Secret or TS/SCI. Note that an active clearance may provide the opportunity for you to work on sensitive SpaceX missions. If so, you will be subject to pre-employment drug and random drug and alcohol testing
ADDITIONAL REQUIREMENTS:
Willing to work extended hours and weekends when needed
COMPENSATION AND BENEFITS:
Pay Range:
Software Engineer/Senior: $160,000.00 - $220,000.00/per year
Your actual level and base salary will be determined on a case-by-case basis and may vary based on the following considerations: job-related knowledge and skills, education, and experience.
Base salary is just one part of your total rewards package at SpaceX. You may also be eligible for long-term incentives, in the form of company stock, stock options, or long-term cash awards, as well as potential discretionary bonuses and the ability to purchase additional stock at a discount through an Employee Stock Purchase Plan. You will also receive access to comprehensive medical, vision, and dental coverage, access to a 401(k) retirement plan, short & long-term disability insurance, life insurance, paid parental leave, and various other discounts and perks. You may also accrue 3 weeks of paid vacation & will be eligible for 10 or more paid holidays per year. Exempt employees are eligible for 5 days of sick leave per year.
ITAR REQUIREMENTS:
To conform to U.S. Government export regulations, applicant must be a (i) U.S. citizen or national, (ii) U.S. lawful, permanent resident (aka green card holder), (iii) Refugee under 8 U.S.C.
1157, or (iv) Asylee under 8 U.S.C.
1158, or be eligible to obtain the required authorizations from the U.S. Department of State. Learn more about the ITAR here.
SpaceX is an Equal Opportunity Employer; employment with SpaceX is governed on the basis of merit, competence and qualifications and will not be influenced in any manner by race, color, religion, gender, national origin/ethnicity, veteran status, disability status, age, sexual orientation, gender identity, marital status, mental or physical disability or any other legally protected status.
Applicants wishing to view a copy of SpaceX's Affirmative Action Plan for veterans and individuals with disabilities, or applicants requiring reasonable accommodation to the application/interview process should notify the Human Resources Department at (310) 363-6000.
Show more
Show less","Python, C#, Java, Scala, Go, Apache Kafka, Spark, Flink, SQL, Linux, HBase, Hive, Delta Lake, PostgreSQL, CockroachDB, Istio, Version control, CI/CD, DevOps, Data lakes, Distributed systems, Bare metal compute clusters, Networklayer issues","python, c, java, scala, go, apache kafka, spark, flink, sql, linux, hbase, hive, delta lake, postgresql, cockroachdb, istio, version control, cicd, devops, data lakes, distributed systems, bare metal compute clusters, networklayer issues","apache kafka, bare metal compute clusters, c, cicd, cockroachdb, data lakes, delta lake, devops, distributed systems, flink, go, hbase, hive, istio, java, linux, networklayer issues, postgresql, python, scala, spark, sql, version control"
Sr. Software Google Data Migration Eng,Eliassen Group,"Burbank, CA",https://www.linkedin.com/jobs/view/sr-software-google-data-migration-eng-at-eliassen-group-3778894591,2023-12-17,Malibu,United States,Mid senior,Onsite,"Our client, a global media and entertainment industry leader, needs a skilled Sr. Software Google Data Migration Eng.
Due to client requirement, applicants must be willing and able to work on a w2 basis. For our w2 consultants, we offer a great benefits package that includes Medical, Dental, and Vision benefits, 401k with company matching, and life insurance.
Rate: $75 - $90 / hr. w2
Potential Candidates for the Sr. Software Google Data Migration Eng:
Must have a security-first mindset, including experience with designing, maintaining, and optimizing permission models, on-premises data gateways, custom connectors for both structured and unstructured services and working knowledge of several scripting language (M, Power FX, Powershell, etc)
Requirements of the Sr. Software Google Data Migration Eng:
5+ years of progressive experience in an IT related field
5+ years of demonstrated experience designing, implementing, and integrating enterprise-level Messaging and Collaboration technologies
3+ years of experience in project management of large IT initiatives
3+ years of experience working with and directing suppliers in an outsourced environment
Experience migrating, consolidating, and optimizing Google Workspace/G-suite, Box, and other collaboration platforms, including complex APIs an into Microsoft-based toolsets
Skills, experience, and other compensable factors will be considered when determining pay rate. The pay range provided in this posting reflects a W2 hourly rate; other employment options may be available that may result in pay outside of the provided range.
Please be advised- If anyone reaches out to you about an open position connected with Eliassen Group, please confirm that they have an Eliassen.com email address and never provide personal or financial information to anyone who is not clearly associated with Eliassen Group. If you have any indication of fraudulent activity, please contact InfoSec@eliassen.com.
Job ID: 381074
Show more
Show less","Google Data Migration, Security, Permission models, Onpremises data gateways, Custom connectors, Scripting languages (M Power FX Powershell), Enterpriselevel Messaging and Collaboration technologies, Project management, Supplier management, Google Workspace/Gsuite, Box, Collaboration platforms, APIs, Microsoftbased toolsets","google data migration, security, permission models, onpremises data gateways, custom connectors, scripting languages m power fx powershell, enterpriselevel messaging and collaboration technologies, project management, supplier management, google workspacegsuite, box, collaboration platforms, apis, microsoftbased toolsets","apis, box, collaboration platforms, custom connectors, enterpriselevel messaging and collaboration technologies, google data migration, google workspacegsuite, microsoftbased toolsets, onpremises data gateways, permission models, project management, scripting languages m power fx powershell, security, supplier management"
AWS Cloud Data Engineer,LHH,"Westlake Village, CA",https://www.linkedin.com/jobs/view/aws-cloud-data-engineer-at-lhh-3784805847,2023-12-17,Malibu,United States,Mid senior,Onsite,"One of our high profile clients is looking to fill their
AWS Cloud Data Engineer
role that is
hybrid in Westlake Village, California
. Our client offers an attractive benefits package and is committed to the values of accountability, curiosity, empathy, equity, nimbleness, partnership, and transparency.
It's a fun place to work that values team collaboration, autonomy, and work/life balance.
Does this sound like something you'd be interested in? If so, apply today.
Responsibilities
:
Accountable for modernization, migration/transformation to a cloud data platform
Design and build reliable, scalable data infrastructure with leading privacy and security techniques to safeguard data using AWS technologies
Architect scalable, secure, low latency, resilient and cost-effective solutions for enabling predictive and prescriptive analytics across the organization
Design/ Architect frameworks to Operationalize ML models through serverless architecture and support unsupervised continuous training models
Take over and scale our data models (Tableau, Dynamo DB, Kibana)
Communicate data-backed findings to a diverse constituency of internal and external stakeholders
Build frameworks for data ingestion pipeline both real time and batch using best practices in data modeling, ETL/ELT processes and hand off to data engineers
Participate in technical decisions and collaborate with talented peers.
Review code, implementations and give meaningful feedback that helps others build better solutions.
Helps drive technology direction and choices of technologies by making recommendations based on experience and research.
Requirements:
5+ years of experience working directly with enterprise data solutions
2+ years of experience working in a public cloud environment and on-prem infrastructure.
2+ years of experience with Columnar Databases like Redshift Spectrum, Time Series data stores like Apache Pinot and the AWS cloud infrastructure
2+ years of experience with in-memory, serverless, streaming technologies and orchestration tools such as Spark, Kafka, Airflow, Kubernetes
Current hands-on implementation experience required, possessing 2 or more years of IT platform implementation experience.
AWS Certified Big Data - Specialty (very nice to have)
2+ years of experience with designing and implementing AWS big data and analytics solutions in large digital and retail environments is desirable
2+ years of experience with online transactional (OLTP) processing and analytical processing (OLAP) databases, data lakes, and schemas.
Experience with AWS Cloud Data Lake Technologies and operational experience of Kinesis/Kafka, S3, Glue and Athena.
Experience with any of the message / file formats: Parquet, Avro, ORC
Design and development experience on a Streaming Service, EMS, MQ, Java, XSD, File Adapter, and ESB based applications
Experience in distributed architectures such as Microservices, SOA, RESTful APIs and data integration architectures.
Hands on experience migrating On-Prem Data solutions to cloud
Prior experience managing On-prem Enterprise Data Warehouse solutions like Netezza is a plus
Job Type:
Direct Hire
Location:
Hybrid
Salary:
$110k to $145k
Benefits:
Medical, Dental, Vision, 401(k), PTO, Family Planning & more
Show more
Show less","AWS, Cloud Data Engineer, Data Migration, Data Infrastructure, Data Security, Data Analytics, Machine Learning, Tableau, Dynamo DB, Kibana, Data Modeling, ETL/ELT, Spark, Kafka, Airflow, Kubernetes, AWS Certified Big Data  Specialty, Redshift Spectrum, Apache Pinot, OLTP, OLAP, Data Lakes, Kinesis, S3, Glue, Athena, Parquet, Avro, ORC, Streaming Service, EMS, MQ, Java, XSD, File Adapter, ESB, Microservices, SOA, RESTful APIs, Data Integration, Netezza","aws, cloud data engineer, data migration, data infrastructure, data security, data analytics, machine learning, tableau, dynamo db, kibana, data modeling, etlelt, spark, kafka, airflow, kubernetes, aws certified big data specialty, redshift spectrum, apache pinot, oltp, olap, data lakes, kinesis, s3, glue, athena, parquet, avro, orc, streaming service, ems, mq, java, xsd, file adapter, esb, microservices, soa, restful apis, data integration, netezza","airflow, apache pinot, athena, avro, aws, aws certified big data specialty, cloud data engineer, data infrastructure, data integration, data lakes, data migration, data security, dataanalytics, datamodeling, dynamo db, ems, esb, etlelt, file adapter, glue, java, kafka, kibana, kinesis, kubernetes, machine learning, microservices, mq, netezza, olap, oltp, orc, parquet, redshift spectrum, restful apis, s3, soa, spark, streaming service, tableau, xsd"
"Data Analytics Engineer, Lead",Harbor Freight Tools,"Calabasas, CA",https://www.linkedin.com/jobs/view/data-analytics-engineer-lead-at-harbor-freight-tools-3774995920,2023-12-17,Malibu,United States,Mid senior,Onsite,"Lead in the design, development, and support of systems and applications to collect, analyze, and transform large volumes of business-data into enterprise reporting and analytic solutions. Initiatives that lead to greater customer insight, optimized performance, enhanced engagement, improved retention, increased revenue and decreased cost. Requires strong technical experience and proficiency within Data Warehousing, Reporting, and Business Intelligence. Responsible for individual-contributor tasks, as well as, leading projects involving database analysts and report developers charged with creating data warehouse and reporting/analytics solutions, and ensuring the delivery of high-quality, consistent and accurate data, analytics and reports.
Work with business partners and management teams to the ensure collection and analysis of appropriate data and metrics to facilitate improvements in processes and profit
Work with executives and leadership teams to formulate hypotheses and design research aimed at improving key performance indicators
Design and develop architecture and data integrity constraints with data infrastructure engineers to ensure the data collection pipeline and data analysis infrastructure meet the needs of the business
Identify requirements, design, create, deploy, and support self-service BI reports and cubes to empower discovery and information-sharing for business partners
Design, develop, implement, manage and monitor Business Intelligence dashboards
Utilize expertise in data modeling, ETL architecture, and report design for department initiatives, and produce detailed documentation including data flow diagrams, logical diagrams, and physical diagrams as needed
Lead development of processes and procedures for collecting, formatting and preparing data from a variety of sources into a central location for reporting, statistical analysis, and research data by a variety of business departments
Analyze, define, and document system requirements for data/work flow and interfaces with other application systems and reporting needs
Manage tasks/projects requiring days/weeks/months to produce, possibly spanning multiple projects and multiple functional areas or entire enterprise
Define project scope, goals and deliverables that support business goals in collaboration with management and stakeholders.
Plan, schedule, track, and report status on project timelines and deliverables using appropriate tools for efforts associated with data analytics and report automation.
Assume lead role in planning complex projects involving multiple sub-projects; simultaneously balancing longer term large scale projects and smaller incremental enhancements and bug fixes.Provide leadership in the development of standardized templates and processes for validating data, analytics and reports prior to distribution.
Will lead team/department initiatives involving other staff members
May provide mentoring or tutelage to junior staff members
Scope:
Staff supervision and development — No
Decision making – Yes
Provide data for decision support
Provide consultation or expert advice
Participate in planning business objectives
Travel – No (0%)
Location — Remote
Flex Designation – Remote, Anywhere
RequirementsEducation And/or Experience
Bachelor of Science degree in Computer Science or Information Systems preferred
Masters of Science/Business or Advanced Certifications preferred
4+ years relevant job experience in technology, analytics, or Business Intelligence
Significant experience with modern data analytics infrastructure, such as MS SQL Server (SSIS/SSRS), IBM Cognos, SPSS, TM1, Netezza, Oracle, OBIEE, Financials, etc.
Experience designing, developing, deploying, and enhancing hi-visibility computer programs, analytics, dashboards and reports
Skills Required:
Adept at rapid prototyping and deployment of stable and maintainable BI solutions
Hands-on experience with data-related programming and other high-level languages preferred
Strong knowledge of distributed computing, data warehouse, data mining, business analytics and software development
Significant experience with modern data analytics infrastructure, such as MS SQL Server (SSIS/SSRS), IBM Cognos, SPSS, TM1, Netezza, Oracle, OBIEE, Financials, etc.
Strong knowledge of data structures, analysis, replication and distributed/ relational data & database mapping
Hands on experience with SQL query language, data warehouse, RDBMS and common BI tools and techniques
Strong computer science, statistics and mathematics background
An analytical thinker that excels at analyzing and understanding data to answer questions
Equipped to handle responsibility, collaborate and excel in a fast-paced, small team environment
Advanced knowledge/experience with databases, data modeling and data architecture practices as well as typical RDBMS within business (Sales, Finance, CRM, Marketing, HR, etc…)
Relies on experience and business acumen to plan and accomplish goals within time budget
Physical Requirements:
General office environment, ability to sit for long periods of time. Ability to move about an office
The anticipated salary range for this position is $122,200 - $183,400 depending on location, knowledge, skills, education and experience. This position is also eligible for an annual discretionary bonus. In addition, we offer comprehensive and competitive benefits to Associates (and their families) such as medical, dental, vision, life insurance, short-term and long-term disability. Eligible Associates are able to enroll in our company’s 401k plan. Associates will accrue paid time off up to 236 hours per year (inclusive of PTO, floating holidays, and paid holidays). Paid sick time up to 80 hours per year unless otherwise required by law.
Show more
Show less","Data Warehousing, Reporting, Business Intelligence, Data Modeling, ETL Architecture, Report Design, SQL, Data Analysis, Data Mining, Distributed Computing, Database Mapping, RDBMS, Data Structures, Data Analytics, Software Development, Tableau, Power BI, Qlik","data warehousing, reporting, business intelligence, data modeling, etl architecture, report design, sql, data analysis, data mining, distributed computing, database mapping, rdbms, data structures, data analytics, software development, tableau, power bi, qlik","business intelligence, data mining, data structures, dataanalytics, database mapping, datamodeling, datawarehouse, distributed computing, etl architecture, powerbi, qlik, rdbms, report design, reporting, software development, sql, tableau"
Data Analyst I,Tarzana Treatment Centers,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-analyst-i-at-tarzana-treatment-centers-3645045417,2023-12-17,Malibu,United States,Mid senior,Onsite,"Position Title:
Data Analyst I
Department:
PDD
Reports To:
Director
Position Summary
The Data Analyst II position conducts advanced research and data analysis for the purpose of developing, submitting federal and state reports. In addition, the position requires trend data analysis for key internal programs and management decision-making. This person will be working with TTC College to ensure all the data needs for the college and its program as being fulfilled.
Salary Range
$24.75 - $28.50 per hour DOE
We Offer a Competitive Benefits Package
Medical Insurance
Dental Insurance
Vision Care Plan
Life Insurance
Paid Holidays (12)
Paid Vacation Time
Sick Time
401(k) Retirement Plan
Competitive wages
Stability and career advancement
Continuing Education Opportunities
HOURS
40 per week, Monday-Friday.
CATEGORIES OF DUTIES
Designs data collection instruments, including forms and fields in currently utilized data gathering software
Utilizes SPSS and other statistical software to analyze and interpret data
Prepares SPSS syntax files
Develops reports for agency’s performance improvement and management decision-making
Responds to data requests for grant/proposal preparation and other needs of the agency
Performs other related duties as assigned by Administrator or Director of Program Development
EDUCATION/EXPERIENCE
Bachelor’s degree and/or equivalent work experience in quantitative and qualitative data analysis using SPSS.
SETTING
Tarzana Treatment Centers, Inc. is a non-profit corporation, which provides behavioral healthcare services, including chemical dependency.
EQUAL OPPORTUNITY EMPLOYER
Tarzana Treatment Centers, Inc. does not discriminate as to age, race, nationality, religion, gender, sexual orientation and disability in its hiring practices.
ADA REQUIREMENT
Tarzana Treatment Centers, Inc. supports the American with Disabilities Act. Discrimination is prohibited in all aspects of employment against disabled persons, who, with reasonable accommodations, can perform the essential functions of a job.
Show more
Show less","Data Analysis, Research, SPSS, Statistical Software, Report Writing, Data Collection, Data Interpretation, Grant/Proposal Preparation, Quantitative Analysis, Qualitative Analysis, Behavioral Healthcare, Chemical Dependency","data analysis, research, spss, statistical software, report writing, data collection, data interpretation, grantproposal preparation, quantitative analysis, qualitative analysis, behavioral healthcare, chemical dependency","behavioral healthcare, chemical dependency, data collection, data interpretation, dataanalytics, grantproposal preparation, qualitative analysis, quantitative analysis, report writing, research, spss, statistical software"
Data Scientist,Wellth,"Marina del Rey, CA",https://www.linkedin.com/jobs/view/data-scientist-at-wellth-3772677686,2023-12-17,Malibu,United States,Mid senior,Remote,"Wellth Is a Pioneering Digital Health Company.
We seek to motivate and incentivize healthy habits for high-risk patients through individualized care plans. Our approach draws on a unique combination of behavioral economics, habit science, and cutting-edge technology to drive positive behavior and habit change among some of the US's most underserved and high-needs patient populations. Our work has produced demonstrable results, with several large health insurers reporting significant reductions in hospitalization rates by up to 44% in some cases. Our ambition is to be more than just another digital health startup; we aspire to be a category leader in the $88B health outcomes market, singularly focused on improving the lives and medical outcomes of historically hard-to-engage populations. Our work immediately impacts people's lives daily; we have served over 30,000 patients, prevented an estimated 9,000 days in the hospital, and saved 320 lives.
You will have the opportunity to solve interesting healthcare data problems.
Some of the problems you'll work on include:
Architect and implement scalable analytics to demonstrate customer outcomes.
Identify high-risk patients that are likely to gain significant benefit from Wellth programs.
Develop propensity score models for generating synthetic control groups.
Build and evolve predictive models: churn, health improvement, disease progression, behavioral improvement, and more.
Investigate potential expansion / impact areas not yet addressed by Wellth's solutions.
Tell a powerful story about the impact of Wellth's positive behavior change through data and data visualizations.
Specifically, you will:
Develop and maintain a codebase for Wellth's second-generation data analysis and visualization package, focusing on healthcare outcomes.
Triage operational issues, address data bugs, create and maintain documentation.
Build durable self-service data tools operating on health data that support product insights for our customers.
Optimize and experiment with new and existing business-critical data models.
Seek, make available, and consume new data sets, including but not limited to healthcare data standard sets (e.g. claims data, clinical information, EHR data)
Serve as a thought partner to our Product Managers and Product leadership team. Educate our teams to help us ask better questions, interpret findings more effectively, and drive informed decision-making.
Provide leadership on industry best practices, trending topics, and emergent technologies to continuously improve and refine our processes and systems.
You will be a member of a diverse and supportive team.
You will work with:
Product leaders, on building out a scalable healthcare outcomes analytics product.
Customer success leaders, to deliver evidence of Wellth's impact on outcomes.
Data engineers and analysts, to establish and maintain best practices in engineering and data operations.
Manager of Data Engineering, focusing on your professional growth in data science and leadership with the healthcare outcomes domain.
You might be the candidate we're looking for.
We're looking for someone who has:
5+ years of professional experience as a data scientist in the healthcare sector, with hands-on experience with claims data, patient data, and other healthcare data sets.
5+ years of professional experience as a healthcare data analyst or analytics engineer.
A strong background in statistics, particularly as applied to healthcare data.
Advanced proficiency in SQL, Python and data visualization tools.
Knowledge of agile and scrum methodologies
Proficient working with Git and collaborating via pull/merge requests
Experience working within a notebook environment like Jupyter or SageMaker.
Familiarity with healthcare data standards and experience working with various healthcare data sets.
Comfort with autonomy in the face of ambiguous problems, delivering independently and in a team environment.
Experience with operating production data systems: responding to and triaging issues, optimizing performance, and increasing stability, among other things.
Experience with design and architecture of data-intensive production systems
Excellent communication skills and the ability to present complex data in an accessible way.
We expect our candidates can:
Be proactive and independent in a startup environment, comfortable with imperfect data and infrastructure, partial documentation and stale playbooks.
Take ownership of large customer data concerns from start to finish and collaborate well with cross-functional roles.
Be a part of the solution that is helping to fix all aspects of healthcare from provider to payer to, most importantly, the person seeking care.
In addition to the above, our ideal candidate has:
5+ years of experience working with HEDIS and STARS measures
3+ years familiarity with data orchestration technologies like dbt, Dagster, and Fivetran
Published at least 2 papers to industry publications or conferences
Experience with at least one data quality/observability tool such as dbt tests, Great Expectations, Metaplane, etc.
3+ years of experience working with sensitive health data
You will enjoy working with us at Wellth.
The benefits of working with us include:
Flexible working arrangements, supporting strong remote-first practices and tools with the option to work in our excellent gathering space in Marina Del Rey.
Robust paid time off and generous parental leave program.
Competitive salary & commission structure.
Health, dental, vision insurance, and FSA/HSA plans.
Ability to have a positive impact on people who need it most.
Support of a highly dedicated team focused on building the future of healthcare.
Base Compensation: $130,000 - $180,000
Wellth is an equal-opportunity employer. We value diversity at our company. We do not discriminate on the basis of race, religion, ethnicity, national origin, citizenship, gender, gender identity, sexual orientation, age, veteran status, disability, genetic information, or any other protected characteristic.
Show more
Show less","Data Analytics, Predictive Modeling, Data Visualization, Machine Learning, Healthcare Data, Patient Data, Claims Data, SQL, Python, Git, Jupyter, SageMaker, Scrum, Agile, Data Orchestration, Data Quality, Data Observability, HEDIS, STARS, Data Engineering, Data Science, Data Analysis, Data Visualization, Statistical Analysis, Healthcare, Data Standards, Autonomy, Communication, Presentation Skills, Teamwork, Proactivity, Independence, ProblemSolving, SolutionOrientation, Critical Thinking, Data Architecture, Data Systems, Software Development, Healthcare Outcomes, Behavioral Economics","data analytics, predictive modeling, data visualization, machine learning, healthcare data, patient data, claims data, sql, python, git, jupyter, sagemaker, scrum, agile, data orchestration, data quality, data observability, hedis, stars, data engineering, data science, data analysis, data visualization, statistical analysis, healthcare, data standards, autonomy, communication, presentation skills, teamwork, proactivity, independence, problemsolving, solutionorientation, critical thinking, data architecture, data systems, software development, healthcare outcomes, behavioral economics","agile, autonomy, behavioral economics, claims data, communication, critical thinking, data architecture, data engineering, data observability, data orchestration, data quality, data science, data standards, data systems, dataanalytics, git, healthcare, healthcare data, healthcare outcomes, hedis, independence, jupyter, machine learning, patient data, predictive modeling, presentation skills, proactivity, problemsolving, python, sagemaker, scrum, software development, solutionorientation, sql, stars, statistical analysis, teamwork, visualization"
"Data Analyst Associate, SharePoint/Nintex - Remote (JP9999)","3 Key Consulting, Inc.","Thousand Oaks, CA",https://www.linkedin.com/jobs/view/data-analyst-associate-sharepoint-nintex-remote-jp9999-at-3-key-consulting-inc-3647590043,2023-12-17,Malibu,United States,Mid senior,Remote,"Job Title:
Data Analyst Associate, SharePoint/Nintex - Remote (JP9999)
Location:
Thousand Oaks, Ca. (Remote)
Employment Type:
Contract
Business Unit:
Unified Communications
Duration:
12 months with likely extensions and/or conversion to permanent
Posting Date:
3/28/2022
Notes:
100% Remote but must work west coast time zone
3 Key Consulting is hiring a
Data Analyst Associate
for a consulting engagement with our direct client, a leading global biopharmaceutical company.
Job Description
The ideal candidate enjoys tackling challenges and excels at organizing information from numerous sources to provide well-constructed deliverables.
This may include, but is not limited to, the following:
Extensive working knowledge of SharePoint Online, MS Teams, AvePoint, Nintex Workflows, Nintex Forms and Power Platform
Working with SharePoint Permissions, SharePoint Lists, SharePoint External Sharing, MS Teams permissions.
Developing, enhancing, automating, and managing workflow models
Creating dashboard for reporting
Why is the Position Open?
Supplement additional workload on team.
Top Must Have Skills
Bachelors degree and 3-5 years working experience
SharePoint Online
MS Teams
Power Platform
Day To Day Responsibilities
Working with customers closely to handle any SharePoint Permission needs, update site details, perform attestation.
Work closely with business owners to handle their custom needs.
Participate in Scrum process and ensure end to end delivery of needs on SharePoint Online, MS Teams and Power Platform.
Work as part of Build team to handle any SharePoint site modifications.
Preferred Qualifications And Traits
Passion for proactively identifying opportunities through creative data analysis and modeling
Intellectual curiosity with ability to learn new concepts/frameworks, algorithms and technology rapidly as needs arise
Ability to manage multiple, competing priorities simultaneously
Ability to deliver work and provide positive leadership in a fast-paced, multi-project team-oriented environment.
Ability to work in highly collaborative, cross-functional environments
Education
Bachelor’s degree (mathematics, computer science, engineering, or other quantitative discipline)
Preferred Skills
Extensive knowledge and Experience of SharePoint Online and MS Teams
Experience with Power Apps and Power Automate
Working knowledge of O365 platform
Understanding and working in C# and CSOM
Understanding business needs and developing novel yet practical solutions to meet those needs
Employee Value Proposition
The person in this role will gain experience in SharePoint online, MS Teams, Guest users, Powershell scripting
Red Flags
No SharePoint online understanding.
No MS Teams experience.
No Power Platform experience
Interview Process
Phone and WebEx only.
Two rounds of interviews - one with Team and HM and another with HM and Director.
We invite qualified candidates to send your resume to
recruiting@3keyconsulting.com
. If you decide that you’re not interested in pursuing this position, please feel free to look at the other positions on our website www.3keyconsulting.com/careers. You are welcome to also share this opportunity with anyone you think might be interested in applying for this role.
Regards,
3KC Talent Acquisition Team
Show more
Show less","SharePoint Online, MS Teams, Power Platform, Power Apps, Power Automate, O365 platform, C#, CSOM, PowerShell scripting, Scrum process, Guest users","sharepoint online, ms teams, power platform, power apps, power automate, o365 platform, c, csom, powershell scripting, scrum process, guest users","c, csom, guest users, ms teams, o365 platform, power apps, power automate, power platform, powershell scripting, scrum process, sharepoint online"
Senior Cloud Data Engineer,"Ursus, Inc.","Thousand Oaks, CA",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-ursus-inc-3729088419,2023-12-17,Malibu,United States,Mid senior,Hybrid,"JOB TITLE: Senior Cloud Data Engineer
**TOP 3 SKILLS:**
AWS Redshift
Python
SQL
LOCATION: Hybrid
-
2 Days/Week Onsite
-
Westlake Village, CA
DURATION: 3-6 Month Contract to Hire Opportunity
RATE RANGE: $55-$60/Hour
SUMMARY:
This role is a highly visible and value driven individual contributor role, providing data infrastructural help for fellow analysts and data engineers. Chosen candidate will be hands on developing ETL frameworks, Pipelines and managing current data Systems and implementing new pipelines and integrations. Candidate should be SQL savvy, self-sufficient in handling large volumes of data as well as excellent data visualization capabilities using Tableau or like solutions
RESPONSIBILITIES:
Design and build reliable, scalable data infrastructure with leading privacy and security techniques to safeguard data using AWS technologies
Design and Build scalable, secure, low latency, resilient and cost-effective solutions for enabling predictive and prescriptive analytics across the organization
Take over and scale our data models (Tableau, Dynamo DB, Kibana)
Experience in shipping low-latency massive scale systems to production
Communicate data-backed findings to a diverse constituency of internal and external stakeholders
Build frameworks for data ingestion pipeline both real time and batch using best practices in data modeling, ETL/ELT processes and hand off to data engineers
Participate in technical decisions and collaborate with talented peers.
Review code, implementations and give meaningful feedback that helps others build better solutions.
Helps drive technology direction and choices of technologies by making recommendations based on experience and research.
QUALIFICATIONS:
7 or more years of experience working directly with enterprise data solutions
Hands on experience working in a public cloud environment and on-prem infrastructure.
Specialty on Columnar Databases like Redshift Spectrum, Time Series data stores like Apache Pinot and the AWS cloud infrastructure
Experience with in-memory, serverless, streaming technologies and orchestration tools such as Docker, Spark, Kafka, Airflow, Kubernetes is needed
Excellent SQL skills and Python coding is a must
Current hands-on implementation experience required, possessing 5or more years of IT platform implementation experience.
AWS Certified Big Data - Specialty desirable
Experience designing and implementing AWS big data and analytics solutions in large digital and retail environments is desirable
Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases, data lakes, and schemas.
Experience with AWS Cloud Data Lake Technologies and operational experience of Kinesis/Kafka, S3, Glue and Athena.
Experience with any of the message / file formats: Parquet, Avro, ORC
Design and development experience on subscribing to a Streaming Service, EMS, MQ, Java, XSD, File Adapter, and ESB based applications
Experience in distributed architectures such as Microservices, SOA, RESTful APIs and data integration architectures is a plus
Hands on experience migrating On-Prem Data solutions to cloud
Prior experience managing On-prem Enterprise Data Warehouse solutions like Netezza, Microsoft SQL Server Enterprise edition, MY SQL is a plus
Experience with a wide variety of data processing technologies, including
Big Data Stack (Spark, spectrum, Flume, Kafka, Kinesis etc.)
Data streaming (Kafka, SQS/SNS queuing, etc)
Expert in Columnar databases primarily, Redshift or like technologies lile Snowflake, Firebolt
Expert in Commonly used AWS services (S3, Lambda, Redshift, Glue, EC2, etc)
Expertise in Python, pySpark or similar programming languages is a must have
BI tools (Tableau, Power BI, SSIS, SSRS, MicroStrategy) is a plus
Skilled in AWS Compute such as EC2, Lambda, Beanstalk, or ECS
Skilled in AWS Management and Governance suite of products such as CloudTrail, CloudWatch, or Systems Manager
Skilled in Amazon Web Services (AWS) offerings, development, and networking platforms
Proficiency in Netezza, Oracle, MYSQL and Microsoft SQL Server Databases is a plus
Understanding Continuous Integration / Continuous Delivery with experience in Jenkins
IND123
Show more
Show less","AWS Redshift, Python, SQL, AWS technologies, Tableau, Dynamo DB, Kibana, Docker, Spark, Kafka, Airflow, Kubernetes, Parquet, Avro, ORC, Netezza, Microsoft SQL Server, MySQL, Big Data Stack, Data streaming, Columnar databases, Snowlake, Firebolt, pySpark, BI tools, AWS Compute, AWS Management, AWS Networking, Jenkins","aws redshift, python, sql, aws technologies, tableau, dynamo db, kibana, docker, spark, kafka, airflow, kubernetes, parquet, avro, orc, netezza, microsoft sql server, mysql, big data stack, data streaming, columnar databases, snowlake, firebolt, pyspark, bi tools, aws compute, aws management, aws networking, jenkins","airflow, avro, aws compute, aws management, aws networking, aws redshift, aws technologies, bi tools, big data stack, columnar databases, data streaming, docker, dynamo db, firebolt, jenkins, kafka, kibana, kubernetes, microsoft sql server, mysql, netezza, orc, parquet, python, snowlake, spark, sql, tableau"
Data Engineer / Adtech / Hybrid in Los Angeles area,Motion Recruitment,"Thousand Oaks, CA",https://www.linkedin.com/jobs/view/data-engineer-adtech-hybrid-in-los-angeles-area-at-motion-recruitment-3785896195,2023-12-17,Malibu,United States,Mid senior,Hybrid,"Our client is in the Advertising space and looking for a Data Engineer to come in and migrate Data to the cloud using Big Data tools. They are looking for someone with Adtech experience and someone who can come in and make an immediate impact. Lastly this role will be a direct hire position on a hybrid model in the Los Angeles area.
Basic Qualifications (Required Skills & Experience)
4-6 years of experience
Data Engineering experience
Python, SQL, AWS, Spark, PySpark, Glue
Other Qualifications & Desired Competencies
Hybrid in the Los Angeles area
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
401(k) matching
Posted By:
Casey Ryan
Show more
Show less","Data Engineering, Data Migration, Big Data tools, Cloud Computing, Adtech experience, Python, SQL, AWS, Apache Spark, PySpark, Amazon Glue","data engineering, data migration, big data tools, cloud computing, adtech experience, python, sql, aws, apache spark, pyspark, amazon glue","adtech experience, amazon glue, apache spark, aws, big data tools, cloud computing, data engineering, data migration, python, spark, sql"
"Senior Engineer, Powertrain Field Data Analysis",Fisker,"Manhattan Beach, CA",https://www.linkedin.com/jobs/view/senior-engineer-powertrain-field-data-analysis-at-fisker-3732604363,2023-12-17,Malibu,United States,Mid senior,Hybrid,"About Fisker Inc.
California-based Fisker Inc. is revolutionizing the automotive industry by developing the most emotionally desirable and eco-friendly electric vehicles on Earth. Passionately driven by a vision of a clean future for all, the company is on a mission to become the No. 1 e-mobility service provider with the world’s most sustainable vehicles. To learn more, visit www.FiskerInc.com – and enjoy exclusive content across Fisker’s social media channels: Facebook, Instagram, Twitter, YouTube and LinkedIn. Download the revolutionary new Fisker mobile app from the App Store or Google Play store.
Job Description
At the heart of Fisker’s visionary product lies a state of the art powertrain – and you can play a central role in its delivery. We are looking for a data analyst used to logging and interpreting vehicle system data and able to translate this experience into the analysis of cloud data gathered from our growing vehicle fleet to develop failure analysis, fleet data interpretations and support the creation of digital twins of the evolving physics of Fisker powertrains in the market.
Fisker is a fast-moving technologically-focused startup, and in this context you will need considerable flexibility and adaptability to deal with the rapidly-developing requirements at the leading edge of EV powertrain implementation.
Skills And Experience
Strong experience of on-vehicle logging with diagnostic tools such as Vector CANape and CANalyzer.
Strong experience of fault analysis and diagnosis in a production vehicle environment during development and preferably in the field.
Good data processing skills to transfer this experience to the analysis of cloud-sourced data.
Strong experience with vehicle and powertrain level test systems and methods
Familiarity with tools and test methods for in vehicle and component level testing and development.
Experience with requirements management tracking and ALM tools (for example using DOORS, Polarion or JAMA)
Experience of effective supplier interaction
Good analytical and problem solving skills
Qualifications
5+ years relevant professional experience
BS or MS in electrical, mechanical engineering or related engineering discipline
Expected Salary Range:
$67,480 to $145,000 per year.
Additional compensation:
Salary is one part of total compensation which includes bonuses, equity awards as applicable, and benefits. An employee is eligible to participate in Fisker’s equity program, subject to the rules governing such programs.
Benefits:
Fisker provides comprehensive medical, prescription, dental, vision, and disability insurance packages for full-time employees, their spouse or domestic partner, and children up to age 26. Coverage is effective on the first day of employment, and Fisker covers most of the premiums for employees.
The salary offered may vary depending on multiple individualized factors including market location, job-related knowledge, education/training, certifications, key skills, experience, internal peer equity as well as business considerations. Fisker reasonably and in good faith expects to pay for the position within the salary range listed, taking into account the wide variety of factors listed above.
Fisker Group Inc. (“Fisker”) is an Equal Opportunity Employer. Employment at Fisker is governed by merit, qualifications, and professional competence. Fisker does not discriminate against any employee or applicant because of race, creed, national origin/ethnicity, color, religion, gender identity/expression, sexual orientation, marital status, age, veteran status, disability status, genetic information, pregnancy or related conditions, or any other basis protected by law.
Show more
Show less","Data analysis, Logging, Vehicle system data, Cloud data, Failure analysis, Fleet data interpretation, Digital twins, Powertrains, Vehicle logging, Diagnostic tools, Vector CANape, CANalyzer, Fault analysis, Vehicle environment, Data processing, Vehicle test systems, Vehicle testing, Component testing, Requirements management tracking, ALM tools, DOORS, Polarion, JAMA, Supplier interaction, Analytical skills, Problemsolving skills, MATLAB, Python, SQL, Hadoop, Spark, AWS, Azure, Microsoft, Google Cloud, Machine learning, Artificial intelligence, Big data, Data science, Calculus, Probability, Statistics, Linear algebra","data analysis, logging, vehicle system data, cloud data, failure analysis, fleet data interpretation, digital twins, powertrains, vehicle logging, diagnostic tools, vector canape, canalyzer, fault analysis, vehicle environment, data processing, vehicle test systems, vehicle testing, component testing, requirements management tracking, alm tools, doors, polarion, jama, supplier interaction, analytical skills, problemsolving skills, matlab, python, sql, hadoop, spark, aws, azure, microsoft, google cloud, machine learning, artificial intelligence, big data, data science, calculus, probability, statistics, linear algebra","alm tools, analytical skills, artificial intelligence, aws, azure, big data, calculus, canalyzer, cloud data, component testing, data processing, data science, dataanalytics, diagnostic tools, digital twins, doors, failure analysis, fault analysis, fleet data interpretation, google cloud, hadoop, jama, linear algebra, logging, machine learning, matlab, microsoft, polarion, powertrains, probability, problemsolving skills, python, requirements management tracking, spark, sql, statistics, supplier interaction, vector canape, vehicle environment, vehicle logging, vehicle system data, vehicle test systems, vehicle testing"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773091267,2023-12-17,Malibu,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Science, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, NoSQL, Dynamodb, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Docker images, Legal compliance, Data management tools, Data classification, Retention","data engineering, machine learning, data science, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, docker images, legal compliance, data management tools, data classification, retention","airflow, applied machine learning, aws, azure, bash, data classification, data engineering, data management tools, data science, docker, docker images, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, legal compliance, machine learning, nosql, python, retention, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Santa Monica, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707749,2023-12-17,Malibu,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML Data OPs, Data Pre/Post Processing, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Data Platforms, Data Frameworks, Big Data, Data Governance, Data Compliance, Data Infrastructure, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, NoSQL, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Classification, Data Retention, Data Management Tools","data engineering, ml data ops, data prepost processing, data mining, data cleaning, data normalization, data modeling, data platforms, data frameworks, big data, data governance, data compliance, data infrastructure, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, data retention, data management tools","airflow, applied machine learning, aws, azure, bash, big data, data classification, data cleaning, data compliance, data engineering, data frameworks, data governance, data infrastructure, data management tools, data mining, data normalization, data platforms, data prepost processing, data retention, datamodeling, docker, etl, gcp, git, helm, java, kafka, kubernetes, ml data ops, nosql, python, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Hawthorne, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709550,2023-12-17,Malibu,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML Data Ops, Data Pipelines, ML Datasets, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Airflow, KubeFlow, NLP, Automated Testing, Bash, Spark, Docker, Kubernetes, Microservices, Kafka, Storm, SparkStreaming, SQL, NoSQL","data engineering, ml data ops, data pipelines, ml datasets, data mining, data cleaning, data normalization, data modeling, airflow, kubeflow, nlp, automated testing, bash, spark, docker, kubernetes, microservices, kafka, storm, sparkstreaming, sql, nosql","airflow, automated testing, bash, data cleaning, data engineering, data mining, data normalization, datamodeling, datapipeline, docker, kafka, kubeflow, kubernetes, microservices, ml data ops, ml datasets, nlp, nosql, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712028,2023-12-17,Malibu,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Machine learning, Data mining, Data cleaning, Data normalization, Data modeling, Statistical analysis, Data visualization, Pandas, R, Airflow, KubeFlow, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied machine learning, Legal compliance, Data management tools, Data classification, Data retention","data engineering, machine learning, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, airflow, kubeflow, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, legal compliance, data management tools, data classification, data retention","airflow, applied machine learning, aws, azure, bash, data classification, data cleaning, data engineering, data management tools, data mining, data normalization, data retention, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, legal compliance, machine learning, pandas, python, r, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Culver City, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712019,2023-12-17,Malibu,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, NoSQL, Kafka, Storm, SparkStreaming, Machine Learning, Data Classification, Data Retention, Data Management","python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, nosql, kafka, storm, sparkstreaming, machine learning, data classification, data retention, data management","airflow, aws, azure, bash, data classification, data management, data retention, docker, dynamodb, gcp, git, helm, java, kafka, kubernetes, machine learning, nosql, python, snowflake, spark, sparkstreaming, sql, storm"
Database Engineer,Robert Half,"Portland, Oregon Metropolitan Area",https://www.linkedin.com/jobs/view/database-engineer-at-robert-half-3763302980,2023-12-17,Hillsboro,United States,Mid senior,Onsite,"Robert Half is seeking an ambitious Database Engineer to join our clients dynamic IT team. The Database Engineer will play a crucial role in maintaining and securing their databases, implementing backup and recovery procedures, and contributing to data warehousing initiatives. Strong communication skills are essential for collaborating with cross-functional teams and stakeholders. This is a full time, permanent opportunity with competitive benefits. They require all employees to be 100% onsite at their office in downtown Portland, Oregon. Free parking is provided!
Responsibilities:
Maintaining and enhancing the performance of existing database programs.
Assisting in designing new database programs that meet the organization's data storage needs using technologies such as SQL, Oracle, Snowflake, and Azure SQL.
Monitoring databases and related systems to ensure optimized performance.
Test, troubleshoot, optimize, and secure enterprise databases.
Write and support SQL queries, stored procedures, triggers, and functions.
Create and maintain ETL jobs and multiple event data pipelines.
Writing new support programs and scripts to increase data storage capacity.
Reviewing database and user reports, as well as system information.
Performing debugging procedures on database scripts and programs, as well as resolving conflicts.
Adhering to best practices in securely storing, backing up, and archiving data.
Documenting processes related to database design, configuration, and performance.
Develop and implement backup and recovery procedures to prevent data loss.
Qualifications:
Bachelor’s degree from an accredited institution or equivalent professional experience.
3+ years of experience working within database architecture and/or data modeling.
5+ years using SQL and SQL Server
Sound knowledge of best practices in database engineering and data security.
Experience with presenting actionable insights to business stakeholders
Capable of investigating, familiarizing and mastering new data sets quickly
Understanding of advanced data warehousing concepts, data modeling and extract, transform and load development
Preferred Qualifications:
Visualization experience using Tableau or Power BI
Able to create dashboards
Prior experience using Snowflake
Have worked with data in support of a CRM or ERP system
Experience with automation
Benefits:
Comprehensive Health, Dental, RX and benefits
Company paid Life Insurance and Disability
2 weeks of PTO + sick time & paid holidays
Flexible Spending Plans
Employee Assistance Program
401K Retirement Company match
Free onsite parking/ commuter benefits
Fun, collaborative culture
Beautiful office views
Company cannot provide sponsorship. No C2C engagements.
Show more
Show less","SQL, Oracle, Snowflake, Azure SQL, ETL, Tableau, Power BI, CRM, ERP, Data warehousing, Data modeling, Data security, Data storage, Database architecture, Database engineering, Data analysis, Data visualization, Data pipelines, Data sets, Data transformation, Data Integration, Data management, Backup and recovery, Data migration, Data mining, Data governance, Data quality, Big data, Cloud computing, Data science, Machine learning, Artificial intelligence, Business intelligence, Data analytics, Data engineering","sql, oracle, snowflake, azure sql, etl, tableau, power bi, crm, erp, data warehousing, data modeling, data security, data storage, database architecture, database engineering, data analysis, data visualization, data pipelines, data sets, data transformation, data integration, data management, backup and recovery, data migration, data mining, data governance, data quality, big data, cloud computing, data science, machine learning, artificial intelligence, business intelligence, data analytics, data engineering","artificial intelligence, azure sql, backup and recovery, big data, business intelligence, cloud computing, crm, data engineering, data governance, data integration, data management, data migration, data mining, data quality, data science, data security, data sets, data storage, data transformation, dataanalytics, database architecture, database engineering, datamodeling, datapipeline, datawarehouse, erp, etl, machine learning, oracle, powerbi, snowflake, sql, tableau, visualization"
"Business Data Analyst II, Basketball",Nike,"Beaverton, OR",https://www.linkedin.com/jobs/view/business-data-analyst-ii-basketball-at-nike-3775857032,2023-12-17,Hillsboro,United States,Mid senior,Onsite,"Become a Part of the NIKE, Inc. Team
NIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it's about each person bringing skills and passion to a challenging and constantly evolving game.
WHO ARE WE LOOKING FOR?
We're looking for a Business Data Analyst II, Basketball who will drive analytical workstreams that help identify, evaluate, and catalyze action against critical priorities for our Basketball and Athlete Teams. You will develop and communicate business insights that help influence key partners in a complex, cross-functional environment. You will play a critical role on the team that drives Nike's long-term vision of serving elite athletes and creates long-term value for Nike and our partners!
WHAT WILL YOU WORK ON?
As a Business Analyst, you will be responsible for driving Nike's long-term Basketball strategy forward by providing on-going research and analytical support. You will support the identification and evaluation of critical priorities and strategic problems ultimately leading to crucial action on these priorities. In addition, you will establish new processes and tools that creates value across the cross-functional Basketball teams as well as external partnerships across multiple industries.
The focus across both serving elite athletes as well as everyday athletes/consumers is also unique to this role.
WHO WILL YOU WORK WITH?
This Business Data Analyst will be part of the larger Global Basketball team, that will consist of key stakeholders across Business, Marketing, Sports Marketing, Product and Planning. Additionally, this role will work across key data and analytics driven functions. This role will report directly to our Director, Global Athlete Business along with support from the Global Basketball Leadership Team.
WHAT YOU WILL BRING?
Bachelor's degree or combination of relevant education, experience and training
2+ years of corporate development, business development, corporate strategy, investment banking, strategy consulting, finance, or related experience
Ability to complete quantitative and qualitative analysis and financial models
Experience conducting and/or analyzing research related to consumers, industries, and competitors
Comfort analyzing research related to consumers, industries, and competitors
Proven ability to operate with a high degree of ambiguity and to work with a flexible schedule
Logical problem structuring and strategic problem-solving skills
Superior interpersonal skills and proven track record to work cross-functionally
Excellent written, oral, and communication skills
Ability to work on cross-functional teams and provide project management support
Proficiency in standard business software applications (e.g., Keynote, PowerPoint, Excel)
NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.
NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.
Benefits
Whether it's transportation or financial health, we continually invest in our employees to help them achieve greatness - inside and outside of work. All who work here should be able to realize their full potential.
Show more
Show less","Data Analysis, Business Strategy, Financial Modeling, Consumer Research, Problem Solving, Communication, Project Management, Microsoft Office Suite, PowerPoint, Excel","data analysis, business strategy, financial modeling, consumer research, problem solving, communication, project management, microsoft office suite, powerpoint, excel","business strategy, communication, consumer research, dataanalytics, excel, financial modeling, microsoft office suite, powerpoint, problem solving, project management"
Data Analyst Part Time,Voxmediallc,"Greater Sudbury, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-voxmediallc-3758276918,2023-12-17,Sudbury, Canada,Mid senior,Onsite,"Summary:
The Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, SQL, R, Python, DataDriven DecisionMaking, Performance Metrics, Reports, Optimization, A/B Testing, Data Quality, Data Integrity, Accuracy, Completeness, Data Collection, Data Cleansing, Data Manipulation, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL Processes","data analysis, statistical techniques, sql, r, python, datadriven decisionmaking, performance metrics, reports, optimization, ab testing, data quality, data integrity, accuracy, completeness, data collection, data cleansing, data manipulation, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, accuracy, completeness, data collection, data integrity, data manipulation, data quality, dataanalytics, datacleaning, datadriven decisionmaking, etl, hypothesis testing, optimization, performance metrics, powerbi, python, r, reports, sql, statistical modeling, statistical techniques, tableau"
Associate / Principal Mechanical Engineer - Data Centres,Hydrock,"Plympton, England, United Kingdom",https://uk.linkedin.com/jobs/view/associate-principal-mechanical-engineer-data-centres-at-hydrock-3752877121,2023-12-17,Plymouth, United Kingdom,Mid senior,Onsite,"Job Advert
We have a fantastic opportunity for a Principal or Associate Mechanical Engineer to join our established Data Centre specialist team within our successful national MEP division.
You will have the opportunity to develop your consulting engineering expertise with a clear plan for progression, honing your skills within commercial management, client service and people leadership skills.
The MEP Team
We excel in delivering MEP engineering design services from project inception to construction completion, emphasizing early involvement, collaboration with the design team and tailored solutions. We serve a wide range of sectors including commercial, industrial, leisure, retail, manufacturing, logistics, heritage and residential, for end-user clients, investors, developers, architects, project managers or engineer-led teams. You can check out MEP’s project portfolio here.
Responsibilities Of The Role
Undertake the design of data centre specific mechanical systems from RIBA 1 through to RIBA 4c. Be fully aware of the current technical trends in the sector and be able to apply these to new projects.
Lead projects (or elements of large projects) and take responsibility for the technical delivery and financial profitability of the project.
As the lead discipline for data centre projects, you will also manage sub-disciplines and external contractors, assisted by the team’s design managers.
Be an expert in the sector and support with the training/upskilling of the Hydrock MEP team.
Assist in the marketing of the team externally and be able to generate fee income for new and existing clients.
You’ll be set up for success if you have:
Proven experience as a Principal or Associate Mechanical Engineer with a strong track record in the UK leading mechanical (ideally M&E) delivery on a variety of Data Centre building service projects.
Relevant mechanical engineering qualification at HNC, HND, Bachelors or Masters level.
Ideally a chartered engineer or working towards membership with a relevant institution.
Sound knowledge of BIM processes, Revit, IES and other design software.
Working knowledge of current building regulations with a strong understanding and passion to apply sustainable design principles.
A passion for sharing knowledge and developing junior members of the team.
Experience of winning work, managing teams, and undertaking business operations such as marketing, commercial and financial responsibilities (Associate level only).
What's Great About Hydrock 'in a Nutshell'
We are a British-owned integrated multi-disciplinary engineering consultancy of over 900 staff in 21 offices across the UK. Our driving motivation is to be a ‘Force for Good’, as it is our aim to improve the quality of people’s lives from our employees to our clients, through to the communities we work in and our planet as a whole through the work we do.
From the buildings that surround you, the roads and bridges you cross, all related to the infrastructure that we create, we aim to offer the most sustainable possibilities to shape the places, communities and society that we live in through meeting our client’s needs. Through the path we are on to delivering a green future, the result is something that everyone can be proud of.
Our welcoming and friendly culture is something we are proud of and has gained us recognition with 9 years in the Top 100 Best Companies to Work For list. Check out some of our incredible projects which have been awarded: 2022 Net Zero Award for Bay Technology Centre, Integration and Collaborative Working Award for YGG Tan-y-Lan primary school, The Deaf Academy awarded for its Universal Design at the 2022 Civic Trust Awards and Bristol’s iconic waterfront Wapping Wharf Living making a double win at the Bristol Property Awards!
To top off, here’s our 2022 wrap up video!
What We Can Offer You
Inspiring and supportive colleagues
Reward for progression and hard work
An opportunity to develop your soft skills, as well your technical skills
Competitive starting salary
Excellent health benefits 25 days of holiday (buy/sell up to five days), accrue 1 day extra every 2 years, with bonus holidays too!
An earlier finish on Friday (4pm!)
An opportunity to give back: “Day off for good cause” (on a workday)
A huge range of flexible benefits, including climate perks and an EV car leasing scheme
Our biggest event: Challenge Day!
A place to feel included
We champion diversity, equity and inclusion. As an Equal Opportunities Employer, we commit to supporting our employees and ensure we create a safe environment that nurtures you to perform at your best. Offering our people flexibility is an important factor in achieving this aim.
We consider all application individually with the required qualifications and knowledge without regard to any of the protected characteristics. We would like to provide everyone with a fair selection, assessment and employment experience so we ask with are made aware of any physical or neurodiverse condition within your application for which appropriate reasonable adjustments can be made by us.
Looking for the next steps?
Once you have completed your application through our careers site, we aim to review and respond to you as soon as your application’s been reviewed.
If shortlisted, a member of our Recruitment Team will call you for an initial pre-screen by phone (typically 30 minutes) to help us assess your motivations and interest in the position and Hydrock.
If you progress following this telephone pre-screen, you will be invited to attend a formal interview by video conference (Microsoft Teams) or in our offices.
For the latest updates and news, connect with us on our LinkedIn page!
Department
MEP
Contract type
Permanent
Hours
37.5 Hours
Salary
Competitive
Show more
Show less","MEP Design, Revit, IES, BIM, M&E, Sustainable Design, Building Regulations, Commercial Management, Client Service, People Leadership, Project Management, SubDisciplines Management, External Contractors Management, Team Leadership, Fee Generation, Marketing, Business Operations","mep design, revit, ies, bim, me, sustainable design, building regulations, commercial management, client service, people leadership, project management, subdisciplines management, external contractors management, team leadership, fee generation, marketing, business operations","bim, building regulations, business operations, client service, commercial management, external contractors management, fee generation, ies, marketing, me, mep design, people leadership, project management, revit, subdisciplines management, sustainable design, team leadership"
Data Systems Engineer,CyberCoders,"Houston, TX",https://www.linkedin.com/jobs/view/data-systems-engineer-at-cybercoders-3779627997,2023-12-17,Union,United States,Mid senior,Hybrid,"Job Title
: Data Systems Engineer
Job Location
: Baton Rouge, LA OR Houston, TX (mainly remote; however, candidates MUST be local to Baton Rouge or Houston)
Salary
: $70-100K DOE (benefits, time off, etc.)
Requirements
Bachelor's degree in Computer Science, Engineering, or related field (or equivalent work experience).
3+ years of recent professional experience developing SQL queries and ETL processes.
Experience with Microsoft T-SQL, .NET, Visual Studio, and/or Oracle is preferred, but not required.
For over 60 years, we have provided solutions for industrial valve/instrumentation applications in the oil/gas industry. Due to recent growth, we are seeking a talented Data Systems Engineer to join our team. This position requires a Bachelor's degree in Computer Science, Engineering, or related field (or equivalent work experience) and 3+ years of recent professional experience developing SQL queries and ETL processes. Experience with Microsoft T-SQL, .NET, Visual Studio, and/or Oracle is preferred, but not required.
What You Will Be Doing
Developing solutions to support business analytics/data processing needs.
Designing, developing, and maintaining data lake/warehouse.
Implementing reports/data extracts and data sources for visualization tools (ex. Tableau).
What You Need for this Position
Bachelor's degree in Computer Science, Engineering, or related field (or equivalent work experience).
3+ years of recent professional experience developing SQL queries and ETL processes.
Preferred Qualifications
Azure
T-SQL
.NET, SSRS, SSIS, Excel
Visual Studio, SQL Managment Studio
Oracle PL/SQL
What's In It for You
Competitive Base Salary ($70-100K DOE)
Medical, Dental, Vision Insurance
Time Off
So, if you are a Data Systems Engineer with SQL and ETL experience, please apply today!
Email Your Resume In Word To
Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:
Brittany.Owen@CyberCoders.com
Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : BO4-1777444L440 -- in the email subject line for your application to be considered.***
Brittany Owen - Executive Recruiter - CyberCoders
Applicants must be authorized to work in the U.S.
CyberCoders is proud to be an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work
– In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.
Show more
Show less","SQL, ETL, Data Lake, Data Warehouse, Tableau, Data Visualization, Azure, .NET, Visual Studio, SQL Management Studio, Oracle PL/SQL, SSRS, SSIS, Excel","sql, etl, data lake, data warehouse, tableau, data visualization, azure, net, visual studio, sql management studio, oracle plsql, ssrs, ssis, excel","azure, data lake, datawarehouse, etl, excel, net, oracle plsql, sql, sql management studio, ssis, ssrs, tableau, visual studio, visualization"
"Data Engineer with Netsuite at Houston, Texas, Hybrid",Steneral Consulting,"Houston, TX",https://www.linkedin.com/jobs/view/data-engineer-with-netsuite-at-houston-texas-hybrid-at-steneral-consulting-3682449785,2023-12-17,Union,United States,Mid senior,Hybrid,"Must be local DL
Share only 2 profiles only
Urgently seeking Houston based Data Migration Engineers for a 4 - 5 month consulting project. Selected individual will be migrating financial data and reports from a legacy system over to Netsuite.
Required: Netsuite, SQL, Excel and ability to sit onsite 2-3 days a week at a minimum.
Location is Houston, Texas they must be local
They must have Netsuite -if they don't, we can't submit. They must be local to Houston or we can't submit.
Data Migration Engineer with Netsuite (required)
Urgently seeking Houston based Data Migration Engineers for a 4 - 5 month consulting project. Selected individual will be migrating financial data and reports from a legacy system over to Netsuite.
Required: Netsuite, SQL, Excel and ability to sit onsite 3+ days a week at a minimum.
Show more
Show less","Data Migration, NetSuite, SQL, Excel, Consulting, Financial Data, Reports, Legacy System","data migration, netsuite, sql, excel, consulting, financial data, reports, legacy system","consulting, data migration, excel, financial data, legacy system, netsuite, reports, sql"
"Senior Data Engineer, Infrastructure-Dallas, Austin, or San Antonio, TX",H-E-B,"San Antonio, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-infrastructure-dallas-austin-or-san-antonio-tx-at-h-e-b-3740525124,2023-12-17,Union,United States,Mid senior,Hybrid,"Since H-E-B Digital Technology's inception, we've been investing heavily in our customers' digital experience, reinventing how they find inspiration from food, how they make food decisions, and how they ultimately get food into their homes. This is an exciting time to join H-E-B Digital--we're using the best available technologies to deliver modern, engaging, reliable, and scalable experiences to meet the needs of our growing audience. If you enjoy taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, we want you as part of our team.
Our Partners thrive The H-E-B Way. In the
Senior Data Engineer, Infrastructure
job, that means you have a...
HEART FOR PEOPLE... you can organize multiple engineers, negotiate solutions, and provide upward communication
HEAD FOR BUSINESS... you consistently demonstrate and uphold the standards of coding, infrastructure, and process
PASSION FOR RESULTS... you're capable of high-velocity contributions in multiple technical domains
What you will do:
Design and deploy batch and streaming data pipeline infrastructure using IaC and CI/CD
Implement features to ensure data platform performance, reliability, and security
Develop solutions to improve monitoring and observability for data pipelines and platform infrastructure
Build data platform components using hybrid cloud services (AWS, GCP, and Azure)
Use configuration management tools to provision system images and install and configure Linux application servers
Help contain costs by delivering solutions to monitor data platform utilization and expenditure
Project you will impact:
Build a world class data platform that can handle petabytes of data
Improve the data quality and consumer experience for 100K+ enterprise data consumers
Who you are:
Hands-on experience in DevOps for cloud infrastructure and data pipelines
Solid background in Linux, networking, SSL/TLS cert management, secrets management, IAM and security best practices
Experienced programmer in one or more languages such as Bash/Shell, Python, Java, Go, Ruby
Understanding of Big Data and Hybrid Cloud infrastructure. Experienced in technologies such as Kafka, Kubernetes, Spark, Databricks, AWS EMR, S3, data warehouses (Snowflake, Teradata)
Significant experience with one or more cloud infrastructure providers (AWS, GCP, Azure)
Experienced in cloud administration and Infrastructure as Code (Terraform, Cloud Formation, AWS CDK, Pulumi)
Comfortable with configuration management tools (Ansible, Puppet, Chef, Salt)
Have worked with enterprise monitoring, APM, and log analysis tools like Datadog, Splunk, ELK Stack, New Relic
Experienced with CI/CD tools such as GitLab CI/CD and Jenkins
Up to date on the latest technology developments. Should be able to evaluate and propose new tooling/solutions for data platform
Excellent written, oral communication and presentation skills
Bonus:
DevOps certifications
Cloud certifications
DATA3232
Show more
Show less","IaC, CI/CD, Linux, Networking, SSL/TLS cert management, Secrets management, IAM, Bash/Shell, Python, Java, Go, Ruby, Kafka, Kubernetes, Spark, Databricks, AWS EMR, S3, Snowflake, Teradata, AWS, GCP, Azure, Terraform, Cloud Formation, AWS CDK, Pulumi, Ansible, Puppet, Chef, Salt, Datadog, Splunk, ELK Stack, New Relic, GitLab CI/CD, Jenkins","iac, cicd, linux, networking, ssltls cert management, secrets management, iam, bashshell, python, java, go, ruby, kafka, kubernetes, spark, databricks, aws emr, s3, snowflake, teradata, aws, gcp, azure, terraform, cloud formation, aws cdk, pulumi, ansible, puppet, chef, salt, datadog, splunk, elk stack, new relic, gitlab cicd, jenkins","ansible, aws, aws cdk, aws emr, azure, bashshell, chef, cicd, cloud formation, databricks, datadog, elk stack, gcp, gitlab cicd, go, iac, iam, java, jenkins, kafka, kubernetes, linux, networking, new relic, pulumi, puppet, python, ruby, s3, salt, secrets management, snowflake, spark, splunk, ssltls cert management, teradata, terraform"
Data Engineer – up to £150-200k total compensation,Saragossa,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-%E2%80%93-up-to-%C2%A3150-200k-total-compensation-at-saragossa-3766637121,2023-12-17,Slough, United Kingdom,Associate,Onsite,"Big tech is fragile and career outlook right now is very uncertain. What about working at a hedge fund where their growth is almost certain?
This data engineering team hired multiple people last year – most from big-tech – and plan to do the same this year.
A large portion of your work will technically be non-finance, with the main finance focused work completed by other teams. The real-world, complex data engineering problems are all yours.
You’ll be joining a team that builds and has built thousands of ingestion pipelines, with data coming from every single potential avenue it possibly can. To get the best understanding of an investment, all data points are considered. You’ll bring the data into the business, build the data infrastructure around it and deliver the data to the right person in the business to build their investment models.
Tech is also completely flexible. Most of the work is done within Python, C# and Scala with a range of databases. Snowflake is widely used, as are Docker and Kubernetes for containerisation. ETL and ELT tech are also used every day, primarily Airflow, Spark, Hive and a lot more.
You’ll need to come from a strong academic background with some commercial experience in a data heavy software or data engineering role. Financial experience of some kind would be a huge plus, but absolutely not a requirement. Experience with as much of the tech stack above is preferred and most likely, but you don’t need experience with all. Picking up new tech will be part of your job.
This is a $60 billion plus fund, where being at the cutting edge of technology in general, let alone finance, is key for them. With this amount of ambition, it’s why their bonuses can be some of the biggest in the industry. First year total compensation could be as high as £200,000 and this has the potential to get better year on year, based on your performance.
When in the office you have a fully catered canteen with a Michelin standard chef, barista coffee, a gym and more.
Want to explore (or to continue exploring) high-tech in the Hedge Fund world?
No up-to-date CV required initially, we can work on updating it together if need be.
Show more
Show less","Python, C#, Scala, Snowflake, Apache Airflow, Spark, Hive, Docker, Kubernetes, ETL, ELT","python, c, scala, snowflake, apache airflow, spark, hive, docker, kubernetes, etl, elt","apache airflow, c, docker, elt, etl, hive, kubernetes, python, scala, snowflake, spark"
Associate Data Analyst,Tripadvisor,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/associate-data-analyst-at-tripadvisor-3783818513,2023-12-17,Slough, United Kingdom,Associate,Onsite,"Do you have a passion for data and analytics?
Who We Are
Bokun is an innovative travel tech company with headquarters in Reykjavik and Oxford. As we continue to experience a period of exciting growth, we are seeking talented individuals to join our cross-functional team. As part of the Tripadvisor family, we are passionate about helping businesses in the travel industry succeed, and we're looking for a motivated individual who shares our mission.
Who We Are Looking For
We're seeking a self-motivated individual who is enthusiastic about using data to create excellent software and drive business decisions. Our ideal candidate is a fast learner with a curious mindset, who can work independently and collaboratively to analyze data and uncover insights that will help us create more value for our company and our customers. If you're passionate about data analysis and believe in its ability to inform intelligent decision-making you will fit right in at Bokun.
You should currently be present in Reykjavik, Iceland, where our office is at.
What You Will Do
You will join a dynamic and agile team in a quickly changing environment. You will work in a diverse environment with
Product and Engineering on creating outstanding software
Sales and Marketing on optimize and improve business performance
Management team on making better-informed decisions and develop strategies for the company's growth
Some of the tasks that we know you will be doing:
Collaborate with Product and other internal stakeholders to understand and define what data is essential to measure the impact of the work we are doing
Partner with Engineering on implementing the tracking needed to measure impact
Become an expert in translating what Product and Engineering teams are working onto into specific analysis, metrics or reports that yield actionable insights
Investigate data visualization needs of different teams and translate obtained insight into actions and present these to stakeholders
Enable stakeholders to access data and monitor key business metrics by being an expert in building and maintaining dashboards and reports using Mixpanel and Amplitude
Create and present reports and ad-hoc analysis to management
Partner with Product and Engineering teams on A/B test design and analyzing test results to measure the impact of changes, and make recommendations to stakeholders
Education & Experience
An education focused on Statistics, Math, Data Science or similar technical disciplines
Excellent problem-solver & strong analytical skills
Technical aptitude to learn new tools, technologies, and methodologies
Ability to translate complex analysis findings into a clear narrative and actionable insights
Excellent communication skills, with the ability to listen, and collaborate
Product and/or Marketing Analytics experience is a plus
Knowledge of Mixpanel and Amplitude is a plus
Knowledge in Python, R or other analytical programming language is a plus
Knowledge of the SQL language is a plus
Experience with an AB testing tool such as Optimizely, Google Optimize, etc. is a plus
Knowledge of the travel industry is a plus
Show more
Show less","Data Analytics, Data Science, Statistics, Math, Mixpanel, Amplitude, SQL, Python, R, A/B testing, Optimizely, Google Optimize","data analytics, data science, statistics, math, mixpanel, amplitude, sql, python, r, ab testing, optimizely, google optimize","ab testing, amplitude, data science, dataanalytics, google optimize, math, mixpanel, optimizely, python, r, sql, statistics"
Data Center Operations Engineer,Cloudflare,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-center-operations-engineer-at-cloudflare-3766550732,2023-12-17,Slough, United Kingdom,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
About The Department
In this role, you will be focused on maintaining the Cloudflare global network. You'll work closely with Cloudflare’s SRE (Site Reliability Engineering) team, Network Engineering team, Network Deployment Engineering team and with various vendors and partners (including hardware vendors, datacenter and network providers, and ISPs) to maintain and improve our global infrastructure. You will further be responsible for the development and implementation of consistent processes and visibility measurements for consistent and effective management of our infrastructure. This is a highly visible position that requires deep technical understanding of datacenter infrastructure, networking (physical), and basic experience with data analysis and project management.
To be successful in this position, you should have excellent technical skills, communication skills, and be able to navigate a range of challenges and constraints (e.g. schedule adherence, time zones, and cultures). You will have the opportunity to (literally) build a faster, safer Internet for our millions of users and the billions of web surfers that visit their sites each month.
Who You Are
You will thrive in a hypergrowth engineering environment and be self driven with a keen attention to detail. You will come with a deep technical understanding of Data Center colocation environments, network architecture and server technologies. You will be used to working through partners to support infrastructure delivery to a number of remote locations. You will have had experience managing operational environments, and used to developing new approaches to improve delivery efficiency or operational stability.
What You'll Do
Collaborating with internal teams (Infrastructure, Network Engineering and SRE). Create documentation and manage remote contractors to complete datacenter tasks, working with hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 300+ datacenter locations
Maintain Data Center environment operational availability
Creating and maintaining documentation, plans, SOP’s, MOP’s etc.
Support and configure network infrastructure where required
Providing feedback to internal teams to support internal tools and external vendor partnerships
Required Experience
Minimum of 5 yrs of Linux systems administration
Experience with Juniper, Cisco and DWDM network equipment
Experience managing and instructing remote contractors
Familiarity with work required to stand up infrastructure in remote colocation facilities
Experience running and improving operational processes, including automation tooling, in a rapidly changing environment
Familiarity with day-to-day tasks and projects common to Data Center Operations (deployment, migration, decommissioning etc.)
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Incident management
Other Responsibilities May Include
Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure
Assist with the definition, documentation and implementation of consistent processes across all region
Limited travel
Examples Of Desirable Skills, Knowledge And Experience
Bachelor’s degree; technical background in engineering, computer science, or MIS
Direct experience executing on complex data center/infrastructure projects
Previous experience installing / maintaining data center (and other IT) infrastructure and DCIM tools
Experience running and improving operational processes in a rapidly changing environment
Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills
Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously
Ability to manage MS excel and Google spreadsheets
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Must be a team player
Bonus Points
Multi-lingual; experience working with infrastructure in multiple countries
Comfortable with remote “lights-out” and out-of-band access to data center resources
Linux certifications (RHCSA etc.)
Network certifications (CCNA, JNCIA or higher)
Configuration management systems such as Saltstack, Chef, Puppet or Ansible
Scripting or software development experience in Bash, Python or Go-lang
Familiarity with load balancing and reverse proxies such as Nginx, Varnish, HAProxy, Apache
Experience in working within a large scale SaaS vendor
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Data Center Operations, Linux Systems Administration, Juniper, Cisco, DWDM, Remote Contractor Management, Data Center Infrastructure, Network Architecture, Server Technologies, Automation Tooling, JIRA, Incident Management, MS Excel, Google Spreadsheets, Bash, Python, Golang, Nginx, Varnish, HAProxy, Apache, RHCSA, CCNA, JNCIA, Saltstack, Chef, Puppet, Ansible","data center operations, linux systems administration, juniper, cisco, dwdm, remote contractor management, data center infrastructure, network architecture, server technologies, automation tooling, jira, incident management, ms excel, google spreadsheets, bash, python, golang, nginx, varnish, haproxy, apache, rhcsa, ccna, jncia, saltstack, chef, puppet, ansible","ansible, apache, automation tooling, bash, ccna, chef, cisco, data center infrastructure, data center operations, dwdm, golang, google spreadsheets, haproxy, incident management, jira, jncia, juniper, linux systems administration, ms excel, network architecture, nginx, puppet, python, remote contractor management, rhcsa, saltstack, server technologies, varnish"
Lead Data Engineer,Catch Resource Management,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-catch-resource-management-3778508420,2023-12-17,Slough, United Kingdom,Associate,Hybrid,"Lead Data Engineer
– Data Engineer, ETL, Azure Data Factory, SSIS, Azure, Data Warehouse, Data Warehousing, Power BI, BI, Business Intelligence, D365, Dynamics 365, Dynamics AX, Dataverse, SQL, T-SQL, Azure Base Data, ERP, Data Synapse – Central London – Hybrid – FTC - £80-£90K
Our client is undertaking a migration of their global Finance platform to Microsoft Dynamics 365. They are seeking an experienced Reporting and BI Data Engineer to join them on a 12-18 month fixed term basis.
Key skills & Experience:
Technical experience with data related Microsoft Azure features such as Data Synapse and Data Lakes
Technical experience with PowerBI and Data Warehouses
Experience with SQL and ETL pipelines
Ability to lead work being delivered across multiple technical teams and resources
Hands-on experience in designing and developing PowerBI reports
Experience in managing business and technical stakeholders, having strong communications skills and ability to explain technical content to non-technical stakeholders
Reporting and MI experience in the Finance/ERP space (not essential)
Main Responsibilities:
Supporting enhancements to our Data Warehouse such as migration to Azure, working with colleagues across our IT team
Leading on changes to our Data Warehouse to ensure compatibility with D365 data
Working with the Reporting and BI Functional Lead to support the definition of reporting requirements
Designing and developing PowerBI reports to meet the defined requirements
Supporting the review & adaptation of business processes to ensure consistency, accuracy, and simplicity of reporting tools and outputs
Location:
Central London (Hybrid 1-2 days a week)
Candidates must be eligible to work in this country.
Catch Resource Management is a leading provider of Dynamics 365, JD Edwards, NetSuite and other ERP resources to both end users and to product suppliers/authors.
Our consultants deliver a completely professional resourcing service, always backed up by our team of ERP specialists who are all experienced in full project life cycle implementation and support, thus ensuring that we fully understand our clients’ requirements and our candidates’ skills.
If you have the relevant skills and experience for this position we would welcome your application, however please note that we receive high levels of responses to our advertisements so can only immediately respond to those that are a close match. However, if you are interested in hearing about similar positions then please register on our website: www.catchgroup.com.
Show more
Show less","Data Engineer, Data Warehousing, BI, ETL, Azure, PowerBI, Dynamics 365, SQL, TSQL, ERP, Data Lakes, Data Synapse, Power BI, D365","data engineer, data warehousing, bi, etl, azure, powerbi, dynamics 365, sql, tsql, erp, data lakes, data synapse, power bi, d365","azure, bi, d365, data lakes, data synapse, dataengineering, datawarehouse, dynamics 365, erp, etl, powerbi, sql, tsql"
Scala Data Engineer,EXL,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/scala-data-engineer-at-exl-3768264951,2023-12-17,Slough, United Kingdom,Associate,Hybrid,"EXL (NASDAQ: EXLS)
is a global analytics and digital solutions company that partners with clients to improve business outcomes and unlock growth. Bringing together domain expertise with robust data, powerful analytics, cloud and AI to create agile, scalable solutions and execute complex operations for the world’s leading corporations.
EXL
was founded on the core values of innovation, collaboration, excellence, integrity and respect creating value from data to ensure faster decision-making and transforming operating models. Key industries including Insurance, Healthcare, Banking and Financial Services, Media, and Retail among others.
Headquartered in New York, our team is over 40,000 strong, with more than 50 offices spanning six continents. For information, visit www.exlservice.com.
Role Title: Scala Data Engineer
BU/Segment: Analytics
Location:
London or Northampton, United Kingdom (Flexible hybrid working)
Employment Type:
Fixed Term Contract
Summary of the role:
EXL
is a dynamic and innovative company at the forefront of cutting-edge technology. We are seeking a talented and experienced
Scala Data Engineer
to join our growing team. If you are passionate about data engineering, have a strong background in Scala, and thrive in a collaborative and fast-paced environment, we want to hear from you.
As a Scala Data Engineer at
EXL
you will play a key role in designing, implementing, and optimizing data pipelines within the Hadoop ecosystem. You will leverage your expertise in Scala, Hive/Impala, and Unix scripting to ensure the efficient processing of large datasets. The ideal candidate will have a solid background in data engineering, with a focus on performance tuning in Hadoop environments.
As part of your duties, you will be responsible for:
Design and implement scalable and robust data pipelines using Scala within the Hadoop ecosystem.
Collaborate with cross-functional teams to understand data requirements and deliver solutions that meet business needs.
Perform performance tuning in Hadoop with a focus on optimizing processes for large datasets.
Utilize scripting languages like Unix to automate data workflows and enhance efficiency.
Demonstrate strong SQL programming skills for data manipulation and analysis.
Communicate effectively with stakeholders and actively participate in project meetings.
Bring a deep understanding of banking and financial services, with a bonus if you have experience in data science projects within retail banking.
Qualifications and experience we consider to be essential for the role:
6-8 years of experience in data engineering within the Hadoop ecosystem.
Proficient in Scala, Hive/Impala, and Unix scripting.
Strong SQL programming skills.
Experience in performance tuning in Hadoop with large datasets.
Excellent communication and stakeholder management skills.
· Experience in banking and financial services, with a focus on data science projects in retail banking preferred
As part of a leading global analytics and digital solutions company, you can look forward to:
A competitive salary with private healthcare, critical illness life assurance at 4 x your annual salary, income protection insurance, and a rewarding pension.
At
EXL
, we are committed to providing our employees with the tools and resources they need to succeed and excel in their careers. We offer a wide range of professional and personal development opportunities. We also support a range of learning initiatives that allow our employees to build on their existing skills and knowledge. From online courses to seminars and workshops, our employees have the opportunity to enhance their skills and stay up to date with the latest trends and technologies.
As an Equal Opportunity Employer ,
EXL
is committed to diversity. Our company does not discriminate based on race, religion, colour, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, age, or disability status.
At
EXL
, we offer a flexible hybrid working model that allows employees to live a balanced, healthy lifestyle while strengthening our culture of collaboration.
To be considered for this role, you must already be eligible to work in the United Kingdom.
Show more
Show less","Scala, Hadoop, Hive, Impala, Unix, SQL, Banking, Financial Services, Retail Banking, Data Science","scala, hadoop, hive, impala, unix, sql, banking, financial services, retail banking, data science","banking, data science, financial services, hadoop, hive, impala, retail banking, scala, sql, unix"
Senior/Lead Data Engineer - UK / Remote,Taptap Send,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-lead-data-engineer-uk-remote-at-taptap-send-3788076768,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Our Mission
Reduce inequity by helping immigrants move money home by becoming the leading cross-border fintech for immigrants.
The Team
> Founded by Harvard grads / 3rd-time founders --- also founded Segovia (successful exit) and GiveDirectly (>300M revenue, fastest-growing NGO).
> Top performers from top and varied organizations (e.g., Bain, Bolt, Google, McKinsey, SafeBoda, Uber, Zipline) --- we like moving fast, and we hire people that do as well.
> Driven by mission, guided by values (see below) – in deed, not just word.
> Team as global as the mission, >30 languages from 25 countries (i.e., if you’re interested in learning to make Chicken Moambe, we’re the place).
About Us
Taptap Send is backed by top VCs (Spark, Canaan, Reid Hoffman, Breyer Capital, etc.), rapidly growing and a great place for those looking for both impact and a fast-paced tech startup environment. Read more about the journey in Reid Hoffman’s words or TechCrunch article
Launched in 2019, Taptap is tapping into a transformative shift in global finance: from banks to phones. While this has affected wealthier countries as well, it has been nothing short of a revolution for the emerging markets where the overwhelming majority of population had been unable to access digital finance previously. Its first product, Taptap Send, allows immigrants to send money home instantly and with no fee*, saving money in the process. With a formal market size of >700B alone, we’ve got a lot more saving to do for our users. And that’s just the beginning….
Our Investors
Spark Capital, Canaan, Reid Hoffman, Breyer Capital (Jim Breyer), Unbound (Shravin Mittal), Wamda (Fadi Ghandour), Firstminute Capital, Slow Ventures (Sam Lessin), Helios Partners (Souleymane Ba), Crossbeam Ventures (Ali Hamed), Nikesh Arora, Samih Toukan, and many other context-specific angels
We charge fees for transfers on fixed exchange rate corridors (e.g., XOF, XAF).
The Role
As a Data Engineer, you will create components of a modern data platform that will be the foundation of Taptap Send’s decision-making ability. The systems you help create and the problems you help solve,, will be pivotal to the success of TTS.
This role is key to the success of TTS. You will play a critical part in the functioning of every team, with stakeholders ranging from customer success,, growth, markets,, product, and finance.
The Data Engineer will be implementing critical data pipelines, and innovating and improving infrastructure to support these pipelines while advancing best practices in privacy and data and information security.
Responsibilities
Apply automation mindset and DevSecOps principles to data systems and architectures
Integrate with APIs of 3rd party data sets to enable data pipelines and reporting use cases
Build data systems and pipelines using SaaS systems like Amazon DMS and DBT Cloud
Build solutions to various business objectives using your technical skills and your extensive understanding of the TTS data landscape
Collaborate with the product engineering team and the data analytics team, serving as a bridge between the two.
Data work:
Analyse and organise raw data
Prepare data for prescriptive and predictive modelling
Explore ways to enhance data quality and reliability
Collaborate with data scientists and analysts on several projects
Requirements
Knowledge of programming languages (e.g. Java and Python)
Hands-on experience with relational databases (like PostgreSQL) and data warehouses (like AWS Redshift)
Familiar with data modeling and data governance concepts, and agile methodologies
Familiarity with industry toolsets: Stitch, DMS, etc…
Statistics knowledge, analytical skills, and an understanding of big data technologies
Previous experience as a data engineer or in a similar role
Technical expertise with data models
Great numerical and analytical skills
Degree in Computer Science, IT, or similar field; a Master’s is a plus or seven years' equivalent experience
Taptap Values
Impact first.
Team next.
Accept reality.
Propose solutions.
Win with grit.
Be proactively candid, with yourself and others.
Love the particular.
Own it.
Create positive energy.
Maybe, even have fun.
Equal Opportunity Employer
Taptap Send is an equal opportunity employer dedicated to building an inclusive and diverse workforce. All employment decisions are decided on the basis of qualifications, experience and business need. We strongly encourage applications who are members of underrepresented communities to apply.
If you require reasonable adjustments to be made during the recruitment process, please do not hesitate to let us know. Any information you provide us in this respect will only be used to accommodate your requirements.
Note: we recognize imposter syndrome is real - any candidate that does not perfectly fit every characteristic of this role is still strongly encouraged to apply.
Show more
Show less","Java, Python, PostgreSQL, AWS Redshift, Stitch, DMS, Data modeling, Data governance, Statistics, Analytical skills, Big data technologies, Data engineering, Data pipelines, DevSecOps, Agile methodologies, Data quality, Data reliability, Prescriptive modeling, Predictive modeling, Data analysis, SQL","java, python, postgresql, aws redshift, stitch, dms, data modeling, data governance, statistics, analytical skills, big data technologies, data engineering, data pipelines, devsecops, agile methodologies, data quality, data reliability, prescriptive modeling, predictive modeling, data analysis, sql","agile methodologies, analytical skills, aws redshift, big data technologies, data engineering, data governance, data quality, data reliability, dataanalytics, datamodeling, datapipeline, devsecops, dms, java, postgresql, predictive modeling, prescriptive modeling, python, sql, statistics, stitch"
"Data Engineer (Redshift) -- Fulltime / 12 Months Contract -- Los Angeles, CA - Onsite",Lorven Technologies Inc.,United States,https://www.linkedin.com/jobs/view/data-engineer-redshift-fulltime-12-months-contract-los-angeles-ca-onsite-at-lorven-technologies-inc-3681083408,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Job Title: Data Engineer(Redshift)
Job Location: Los Angeles, CA - Onsite
Duration : Fulltime / 12 Months Contract
Job Description
Need Strong experience in below skill sets.
Informatica
Python
SQL
Redshift
Show more
Show less","Informatica, Python, SQL, Redshift","informatica, python, sql, redshift","informatica, python, redshift, sql"
Senior Data Engineer,Wise Skulls,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-wise-skulls-3648236040,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Title: Senior Data Engineer
Location: Fortmill, SC; Austin, TX (Remote for Now)
Duration: 6+ Months (Possibility of Extension)
Implementation Partner: Infosys
End Client: To be disclosed
Jd
8+ Years of Experience
Expert with Hands on Scripting/Coding - ETL with SQL & No SQL, Big Data, AWS (Lambda, Glue, Step Functions, ECR, ECS, Cloud Watch), Container (Docker), PySpark, CI/CD DevOps.
Intermediate - API/Kafka, BFS domain experience, Testing/QA concepts.
Show more
Show less","SQL, NoSQL, Big Data, AWS, Lambda, Glue, Step Functions, ECR, ECS, Cloud Watch, Docker, PySpark, CI/CD DevOps, Kafka, Testing, QA","sql, nosql, big data, aws, lambda, glue, step functions, ecr, ecs, cloud watch, docker, pyspark, cicd devops, kafka, testing, qa","aws, big data, cicd devops, cloud watch, docker, ecr, ecs, glue, kafka, lambda, nosql, qa, spark, sql, step functions, testing"
REMOTE: AWS Data Engineer,Stellar Professionals,United States,https://www.linkedin.com/jobs/view/remote-aws-data-engineer-at-stellar-professionals-3661713356,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Applicant must have 5 years of relevant experience with the following:
Bachelor's or master's degree in computer science, Data Engineering, or a related field.
Professional experience as a Data Engineer, with a focus on AWS data services and technologies.
Strong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.
Proficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing a
Hands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.
In-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.
Familiarity with big data technologies such as Hadoop, Hive, or Presto is a plus.
Solid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.
Excellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.
Strong communication and collaboration skills, with the ability to work effectively in a team-oriented environment
Show more
Show less","AWS data services, ETL processes, AWS Glue, AWS Lambda, Apache Spark, Python, Scala, Java, AWS data storage services, Amazon S3, Amazon Redshift, Amazon DynamoDB, Data modeling, Data warehousing, Data integration, Hadoop, Hive, Presto, SQL, PostgreSQL, MySQL, Oracle, Problemsolving, Communication, Collaboration","aws data services, etl processes, aws glue, aws lambda, apache spark, python, scala, java, aws data storage services, amazon s3, amazon redshift, amazon dynamodb, data modeling, data warehousing, data integration, hadoop, hive, presto, sql, postgresql, mysql, oracle, problemsolving, communication, collaboration","amazon dynamodb, amazon redshift, amazon s3, apache spark, aws data services, aws data storage services, aws glue, aws lambda, collaboration, communication, data integration, datamodeling, datawarehouse, etl, hadoop, hive, java, mysql, oracle, postgresql, presto, problemsolving, python, scala, sql"
Junior Data Engineer,Monkey Tilt,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-at-monkey-tilt-3777032155,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"About US
Currently in stealth mode, Monkey Tilt is an exciting new venture in the online gaming world. Backed by prominent venture capitalists, we are poised to disrupt the traditional online gaming landscape with our innovative approach, cutting-edge technology and breadth of product offerings. Join a young but experienced executive team with deep roots in online gaming. We are VC-backed, well funded, and have inked partnerships with tier-one celebrities who are deeply invested in Monkey Tilt’s success.
Job Description
Join our team as a Junior Data Engineer, where you'll play a crucial role in building and optimizing data pipelines and solutions using leading technologies and methodologies. In this position, you'll collaborate with experienced professionals to develop robust and scalable data systems. This role offers a great opportunity to enhance your data engineering skills in a dynamic and nurturing environment.
Responsibilities:
Develop and maintain ETL/ELT pipelines using Python and Databricks, focusing on efficient data processing and transformation.
Utilize SQL and DBT for data modeling and querying, ensuring data accuracy and accessibility.
Work with distributed data systems to to handle large-scale data processing.
Manage real-time streaming data workloads using technologies like Kafka and Kinesis.
Design and oversee production database systems, understanding the nuances of various data stores and their operational trade-offs.
Continuously explore and integrate business intelligence tools, data security and privacy standards (like GDPR, CCPA), and data governance frameworks into our data solutions.
Qualifications:
Basic experience or familiarity with Python and data pipeline development, particularly using Databricks.
Knowledge of SQL for data modeling and querying.
Understanding of data workflow orchestration with tools such as Apache Airflow.
Exposure to distributed data processing systems (e.g., Redshift, Spark) and real-time data streaming platforms (e.g., Kafka).
Interest in database design, data storage solutions, and AI-driven data analytics.
Keen to learn about various aspects of data engineering, including business intelligence tools, data governance, and machine learning applications.
A proactive approach to learning and applying new technologies in the field of data engineering and AI.
We are looking for someone who:
Is a hustler/grinder who thinks outside the box
Thrives in a fast paced evolving environment
Background or keen interest in gambling/casino and crypto industries
Not afraid to put in the work for big rewards
Join the Monkey Tilt product team at an early stage and directly contribute to shaping its trajectory. This opportunity allows you to be a key player in the organization's foundational growth.
Show more
Show less","Python, Databricks, ETL/ELT, SQL, DBT, Kafka, Kinesis, Redshift, Spark, Apache Airflow, Data governance, Data security, GDPR, CCPA, Business intelligence, AI, Machine learning","python, databricks, etlelt, sql, dbt, kafka, kinesis, redshift, spark, apache airflow, data governance, data security, gdpr, ccpa, business intelligence, ai, machine learning","ai, apache airflow, business intelligence, ccpa, data governance, data security, databricks, dbt, etlelt, gdpr, kafka, kinesis, machine learning, python, redshift, spark, sql"
Azure Data Engineer - £85k + 10% bonus - 100% remote,Hawksworth,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/azure-data-engineer-%C2%A385k-%2B-10%25-bonus-100%25-remote-at-hawksworth-3777860153,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Position: Azure Data Engineer / £85k + 10% bonus
Industry: London insurance market
Workplace: Remote 🏡 - occasional meetups in London
Our client are a leading insurance broker in the city of London.
If you're a Data Engineer (Azure) with experience in the Financial Services or Consulting, please apply. You can 📧 your cv to matt@hawksworthuk.com or message me on LinkedIn.
What do we need? 🤔
Our client are moving over to Azure Synapse, so if you have been part of the team who's been deploying this, happy days!
One of the 🔑 bits to this role is that you have experience of modern methods of working, ideally playing a key role in Azure deployments and managing the back-log.
Tech environment:
Azure / DevOps / Automation
Policy Admin
Power BI
SQL Server
ETL - Modelling
Data Factory - Integration tool
Azure SQL DB
You'll be joining a company who are one of the largest 💪🏼 in the Lloyd's insurance market and a Data Practice that is growing accordingly to keep up with the rest of the business, so having the 🔥Azure Synapse🔥 experience will be invaluable, especially as this is the current focus for the team.
If this is good timing and you're keen to see the full position description, please do get in touch ☎️
I look forward to hearing from you.
#dataengineer #azuresynapse #londonmarket #insurance #data #london
Show more
Show less","Azure, DevOps, Automation, Policy Admin, Power BI, SQL Server, ETL, Modelling, Data Factory, Azure SQL DB, Azure Synapse","azure, devops, automation, policy admin, power bi, sql server, etl, modelling, data factory, azure sql db, azure synapse","automation, azure, azure sql db, azure synapse, data factory, devops, etl, modelling, policy admin, powerbi, sql server"
Data Engineer,Harvey Nash,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-harvey-nash-3776607964,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Data Engineer – Central London, FTC asap for 12 months, up to £452/day!
REMOTE
My Client,
one of the Big 4 tech companies
, is looking for a
Data Engineer!
Required Skills/Qualifications:
· Independent and communicative
· SQL, writing pipelines
· Data Modelling and best practices
· How to model data warehouse
· Have a strong product sense – understand
Top Daily Responsibilities:
· Each person in the team gets an area of responsibility, owning a product area within the risk team
· To actually work on projects, that have mainly two purposes – one: brining more data into the decision layer to ensure that it is better (can this transaction move or not)
· Improving our analytics capabilities (what success looks like from risk)
· Working with XFNs (assets and software engineers) to build data sets, writing pipeline, do data modelling and creating dashboards for leadership and tracking.
If you have above experience and expertise - apply ASAP as this opportunity might be perfect for you!
Show more
Show less","SQL, Data modeling, Data warehousing, Data pipelines, Data analytics, XFNs, Dashboards","sql, data modeling, data warehousing, data pipelines, data analytics, xfns, dashboards","dashboard, dataanalytics, datamodeling, datapipeline, datawarehouse, sql, xfns"
Data Engineer,Transparity,"England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-transparity-3779821742,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"We’re looking for a Data Engineer!
As a Data Engineer you will be working as part of an agile team in the Data & AI practice to deliver solutions to our clients ensuring they get maximum value from their data. We play to people’s strengths, and you can expect to be hands-on, providing the opportunity for you to utilise and continually develop your existing technical and project skills to deliver high quality solutions.
We work with a Winning from Anywhere® approach but there is an element of travel to client sites when required, along with travel to our company conference and events.
What you’ll be delivering
Designing and implementing robust ETL/ELT solutions in the Microsoft/Azure stack.
Designing and implementing data lakes with a medallion design approach.
Scalable solutions to satisfy our clients current and future needs.
Migrating on-premises data solutions to the cloud. So, knowledge of the SQL Server stack (SQL, SSIS, SSAS and SSRS) is beneficial.
The continued support and development of a data solution following go live.
What is required to be successful in this role
Ability to build relationships within a team and with our clients, creating an environment where people feel they belong and can fully contribute.
Extensive experience in a Data Engineering or Data Warehouse Development role involving hands-on experience with the Azure and\or SQL Server technology stack.
Experience in designing and developing ETL\ELT processes using the Azure platform (Azure Synapse Analytics, Data Factory, Data Bricks) supported with SQL and Python.
Knowledge of implementing data lakes and medallion lake house design.
Experience of working with large data volumes of data from multiple sources.
Experience in writing complex Python \ SQL queries (DDL, DML) and Stored Procedures.
Exposure to Analysis Services Development Multi-Dimensional\Tabular (MDX, DAX).
Understanding of business intelligence and data warehousing concepts.
Strong database design and development skills in any relational database technology.
Good knowledge of optimisation techniques and design patterns for large data solutions.
What you’ll get in return
In addition to a competitive salary, there are a number of benefits and perks we offer to say thank you for being a part of our journey:
To work at a 2* Outstanding company to work for
Flexibility over where you work with our Winning from Anywhere® approach
25 days holiday
Home working allowance paid monthly
Winning from Anywhere® allowance to help your set up at home
24/7 GP at hand
Immediate 24/7 access to an Employee Assistance Programme
We’ll support you when you’re not well with our Company sick pay scheme
Private health insurance after a successful year’s service
Enhanced parental pay and leave, supporting you with those precious life moments
Access to the Perkbox platform to help make your money go further
Cyclescheme
Electric car scheme
And many more!
About us
We do Microsoft Cloud. That’s it.
But we aren’t like any other Microsoft partner. Because no other Microsoft partner has the people who make us who we are.
Our culture is a cornerstone of our success, as we attract and retain enterprise-level talent by looking after our people, who deliver outstanding results for our customers. From our newest starter right up to our board, everyone plays a vital role in our journey and gets a say in how we achieve our goals.
We are focused on creating an inclusive environment where our team members thrive based upon integrity, strong work ethic, a one-team ethos, a desire to learn and growth, and a commitment to deliver exceptional customer service. Therefore, first and foremost we recruit with our cultural fit in mind.
Our outstanding people along with our relationship with Microsoft set us apart and has helped us obtain all 6 Microsoft Partner Designations, 12 Advanced Specialisations and Azure Expert MSP status.
We're on a mission to be the most respected Microsoft Partner globally. To get there, we'll ensure Transparity is an outstanding place to work with an industry-leading team committed to delivering the very best for our customers.
Do you want to join our journey? Are you our next member of #TeamTransparity?
Show more
Show less","Data Engineering, Data Warehouse Development, ETL/ELT, Azure, SQL Server, Azure Synapse Analytics, Data Factory, Data Bricks, SQL, Python, Data lakes, Business intelligence, Data warehousing, Database design, Optimisation techniques, Design patterns","data engineering, data warehouse development, etlelt, azure, sql server, azure synapse analytics, data factory, data bricks, sql, python, data lakes, business intelligence, data warehousing, database design, optimisation techniques, design patterns","azure, azure synapse analytics, business intelligence, data bricks, data engineering, data factory, data lakes, data warehouse development, database design, datawarehouse, design patterns, etlelt, optimisation techniques, python, sql, sql server"
Data Engineer,Plexus Resource Solutions,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-plexus-resource-solutions-3782032643,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Senior Data Engineer Role @ Web3 Platform
Plexus is working with a well-funded very early-stage start-up within the Blockchain space. They are a small team of 13 who are exploring the depths of the execution layers on various Blockchains, ensuring that they understand their individual qualities, construct models that reflect current affairs and develop pipelines to deliver to the broader community.
We are looking for a Senior Data Engineer to join the team.
Requirements:
Strong Data Indexing experience is a Must!
Python and SQL experience
Someone who want to work in an innovative sector (Crypto)
Someone open to a start-up too and wants to work in a fast paced environment
The role is remote based, but there is a preference for London Based!
GMT +/- 2 hours
Salary is very competitive base + Equity
This role is permanent position!
Show more
Show less","Data Indexing, Python, SQL","data indexing, python, sql","data indexing, python, sql"
Data Engineer,Revoco,"England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-revoco-3766936323,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"This is working for a well known high street bank, through a respected consultancy.
It is a 12 month Fixed term contract for12 months (imagine a bit like an inside IR35 contract except with the benefits of a permanent employee) .
The salary is 80K
There is no sponsorship for this role, sorry.
2 days a week in central London.
You will play a pivotal role implementing and fine-tuning data pipelines within the Hadoop ecosystem. You will be experienced in Scala, Hive/Impala, and Unix scripting. The ideal candidate should possess a solid background in data engineering, with a specialised focus on optimising performance in Hadoop environments.
As part of your responsibilities, you will:
Design and deploy scalable and resilient data pipelines using Scala within Hadoop.
Conduct performance tuning in Hadoop, concentrating on enhancing processes for handling large datasets.
Scripting languages such as Unix to automate data workflows and improve overall efficiency.
SQL programming for proficient data manipulation and analysis.
Effectively communicate with stakeholders and actively engage in project meetings.
Comprehensive understanding of banking and financial services, a bonus if you have hands-on experience in data science projects within retail banking.
Qualifications and essential experience for the role include:
Data engineering within the Hadoop ecosystem.
Scala, Hive/Impala, and Unix scripting.
Strong SQL programming.
Previous experience in performance tuning in Hadoop, particularly with large datasets.
Excellent communication skills.
Show more
Show less","Data Engineering, Hadoop, Scala, Hive, Impala, Unix Scripting, SQL, Performance Tuning, Data Pipelines, Banking, Financial Services, Data Science, Retail Banking","data engineering, hadoop, scala, hive, impala, unix scripting, sql, performance tuning, data pipelines, banking, financial services, data science, retail banking","banking, data engineering, data science, datapipeline, financial services, hadoop, hive, impala, performance tuning, retail banking, scala, sql, unix scripting"
Informatica Data Engineer,BrightBox Group Ltd,"England, United Kingdom",https://uk.linkedin.com/jobs/view/informatica-data-engineer-at-brightbox-group-ltd-3785308748,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Job Title: Informatica Data Engineer
UK Based
Remote
6-month contract – IR35 yet to be determined
I am looking for an Informatica Data Engineer to join a team of engineers, architects, data scientists, and experience designers, who help customers drive value from digital solutions. The mission is to build intelligent and modern platforms for applications and data, decreasing risk and time to value. The business works with a diverse range of clients, from global insurance, pharma, healthcare, and retail to smart home devices, space exploration, and more.
Responsibilities:
- Designing and creating pipelines to transform data in a scalable and repeatable manner.
- Developing efficient code to automate the process of ingesting and cleaning data.
- Applying appropriate mechanisms to ensure data flows smoothly towards target data sources.
- Collaborating with cross-functional teams on customer projects to support digital transformation initiatives.
- Understanding and analysing data problems and developing innovative solutions to address them.
- Keeping up to date with the latest industry trends and best practices in data engineering.
Qualifications:
- Bachelor's degree in Computer Science, Engineering, or a related field.
- Strong experience with Informatica PowerCenter and other Informatica tools.
- Solid understanding of data engineering principles and best practises.
- Proficiency in programming languages such as SQL, Python, or Scala.
- Knowledge of cloud platforms, particularly Azure, and experience with cloud data engineering technologies.
- Familiarity with data modelling and data warehousing concepts.
- Strong problem-solving and analytical skills.
- Excellent communication and teamwork abilities.
If you are a highly driven individual with a passion for data engineering and a desire to contribute to the success of our clients digital transformation journeys, we would love to hear from you.
Show more
Show less","Informatica PowerCenter, Informatica, Data engineering, SQL, Python, Scala, Azure, Cloud data engineering, Data modelling, Data warehousing, Problemsolving, Analytical skills, Communication, Teamwork","informatica powercenter, informatica, data engineering, sql, python, scala, azure, cloud data engineering, data modelling, data warehousing, problemsolving, analytical skills, communication, teamwork","analytical skills, azure, cloud data engineering, communication, data engineering, data modelling, datawarehouse, informatica, informatica powercenter, problemsolving, python, scala, sql, teamwork"
SAP Data Analyst/Coordinator,Johnson Controls,"Wichita, KS",https://www.linkedin.com/jobs/view/sap-data-analyst-coordinator-at-johnson-controls-3772999375,2023-12-17,Wichita,United States,Mid senior,Onsite,"Build your best future with the Johnson Controls Team
in Wichita, KS
Johnson Controls (JCI) builds some of the most popular and recognizable residential HVAC brands in the industry including York, Coleman, Luxaire, Champion, and Guardian. The JCI facility in
Wichita, Kansas
produces furnaces, coils, split and packaged systems for the residential and light commercial sectors. It sits on 63 acres and has about 1.3 million square feet under roof. With a workforce of approximately 1600 employees, JCI is the largest non-aircraft employer in Wichita. Our employees design, manufacture, and distribute around one million pieces of quality HVAC equipment each year.
What We Offer
Competitive Pay.
Paid vacation, holidays, and sick time – 15 days of vacation first year.
Comprehensive benefits package including 401K, medical, dental, and vision care – Available day one.
Extensive product and on-the-job/cross-training opportunities, supported by our outstanding internal resources.
Encouraging and collaborative team environment
Dedication to safety through our Zero Harm policy
Check us out: A Day in a life of Johnson Controls.
This position will not offer visa sponsorship now or in the future.
What You Will Do
As SAP Data Analyst/Coordinator you will be supporting new residential HVAC product introductions, cost savings, and maintenance changes at the Wichita, KS facility. You will be working with Engineering teams and other functional groups to ensure that all required Material Master Data and Bill of Material changes for an Engineering change have been completed and implemented accurately in SAP for release to other functional groups, and finally to production.
How You Will Do
Monitor, track, and coordinate Engineering Changes in SAP.
Assist with implementation of new products, cost reductions, and maintenance changes in SAP.
Attend new product introduction meetings and keep accurate notes.
Capture project information to prepare progress reports and to identify critical path items.
Creation/Review/Inactivation of SAP Material Master Data.
Creation/Review/Inactivation of SAP multi-level Bills of Materials.
Coordinate Master Data requirements across departments to ensure consistency and accuracy.
Monitor, track, review creation or changes of Master Data in MDG/SAP.
Learn the MDG system to become the lead analyst and user support in that area.
Learn the SAP GRC Access Control system to support end-users on the Wichita, KS campus.
Learn company processes to problem solve successfully. Function as SAP Super-User.
Develop training and other end-user documentation.
Participate in SAP process improvement initiatives to meet business goals.
Create SAP test scripts and execute testing for new process development in SAP.
Work with SAP Center of Excellence (CoE) on technical updates or opportunities.
What We Look For
Must have 4 years minimum of hands-on SAP experience, specifically with Material Master Data, Bills of Material creation or changes, and Engineering Changes in SAP.
SAP proficiency with SAP modules MM, PP, LO-ECH, SD is required.
Must have 4 years of experience with accessing SAP Master Data and query writing.
Advanced level of proficiency with MS Excel, Word, SharePoint, and Outlook is required.
Highly organized, pragmatic, and personable with a very clear and efficient communication style.
Strong analytical and problem-solving ability is required.
Proven record of working in a fast paced, cross-functional team environment.
Experience running/organizing meetings and accurate note keeping of action items.
Bachelor's Degree preferred. Information Technology, Business, Project, or related field.
Johnson Controls International plc. is an equal employment opportunity and affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, protected veteran status, genetic information, sexual orientation, gender identity, status as a qualified individual with a disability or any other characteristic protected by law. To view more information about your equal opportunity and non-discrimination rights as a candidate, visit EEO is the Law. If you are an individual with a disability and you require an accommodation during the application process, please visit here.
Show more
Show less","SAP, SAP MM, SAP PP, SAP LOECH, SAP SD, SAP MDG, SAP GRC, MS Excel, MS Word, SharePoint, Outlook, Material Master Data, Bills of Material, Engineering Changes, Master Data Management, Product Introductions, Cost Reductions, Maintenance Changes, Project Management, Data Analysis, Reporting, Communication, Problem Solving, Teamwork, Meeting Organization, Note Keeping","sap, sap mm, sap pp, sap loech, sap sd, sap mdg, sap grc, ms excel, ms word, sharepoint, outlook, material master data, bills of material, engineering changes, master data management, product introductions, cost reductions, maintenance changes, project management, data analysis, reporting, communication, problem solving, teamwork, meeting organization, note keeping","bills of material, communication, cost reductions, dataanalytics, engineering changes, maintenance changes, master data management, material master data, meeting organization, ms excel, ms word, note keeping, outlook, problem solving, product introductions, project management, reporting, sap, sap grc, sap loech, sap mdg, sap mm, sap pp, sap sd, sharepoint, teamwork"
Senior Marketing Data Analyst,First International Bank & Trust,"Bismarck, ND",https://www.linkedin.com/jobs/view/senior-marketing-data-analyst-at-first-international-bank-trust-3760589511,2023-12-17,North Dakota,United States,Mid senior,Onsite,"We are seeking a highly analytical and detail-oriented Senior Marketing Data Analyst to join our marketing team. In this role, you will champion Salesforce Marketing Cloud, serve as a SME (Subject Matter Expert) on our customer databases, lead generation strategies, measure ROI on campaigns, research, and supervise our current Marketing Data Analyst. The ideal candidate will have a strong background in data analysis and marketing analytics with an emphasis on Salesforce Marketing Cloud and Google Analytics.
This position is In-Person and will have the employee primarily located in one of our Branches.
Key Responsibilities
Champion Salesforce Marketing Cloud including but not limited to:
Recommend and implement marketing automation to streamline/align sales efforts and increase effectiveness.
Collaborate with SMEs to architect prospect engagement activities.
Share outcomes of efforts through reporting and suggest enhancements for better results.
Serve as SME of FIBT’s customer database and share insights with business line leaders and operations teams to help identify opportunities and drive effective strategies for engagement and growth.
Lead effective strategies for customer engagement and cross-selling; includes targeted digital marketing campaigns and SEO/SEM practices; includes collaboration with third parties.
Measure ROI of online and offline advertising campaigns including, but not limited to, reporting of KPIs such as leads, conversion rates, website traffic and social media engagement.
Conduct competitive research and analyze benchmarking data, including consumer behavioral insights.
Supervise marketing data team member(s)
Qualifications
Bachelor degree in Marketing or Business Administration; a minimum of 7 years of marketing analyst experience, or equivalent combination of education and experience is required.
Salesforce Marketing Cloud experience highly preferred.
Google Analytics experience is highly preferred.
In-depth understanding of SEM campaign strategies and SEO practices and experience with PPC campaigns.
First International Bank and Trust is a family-owned full-service Community Bank with locations in ND, MN, SD, and AZ. We show our employees we care by providing competitive benefits and training and growth opportunities. Here are the things we offer within our full-time benefits package:
Health, Dental & Vision
401K Retirement Plan with Employer Match
Paid Parental Leave
Life and Disability Insurance
Generous PTO and Holiday pay
If you are interested in learning more, you can apply or if you have questions before applying you can reach out directly at TalentAcquisition@FIBT.com.
Equal Opportunity Employer
Show more
Show less","Salesforce Marketing Cloud, Google Analytics, Data analysis, Marketing analytics, Customer databases, Lead generation, ROI measurement, Campaign reporting, SEO/SEM, Digital marketing, KPI reporting, Competitive research, SEM campaign strategies, SEO practices, PPC campaigns","salesforce marketing cloud, google analytics, data analysis, marketing analytics, customer databases, lead generation, roi measurement, campaign reporting, seosem, digital marketing, kpi reporting, competitive research, sem campaign strategies, seo practices, ppc campaigns","campaign reporting, competitive research, customer databases, dataanalytics, digital marketing, google analytics, kpi reporting, lead generation, marketing analytics, ppc campaigns, roi measurement, salesforce marketing cloud, sem campaign strategies, seo practices, seosem"
Senior Marketing Data Analyst,First International Bank & Trust,"Grand Forks, ND",https://www.linkedin.com/jobs/view/senior-marketing-data-analyst-at-first-international-bank-trust-3760589508,2023-12-17,North Dakota,United States,Mid senior,Onsite,"We are seeking a highly analytical and detail-oriented Senior Marketing Data Analyst to join our marketing team. In this role, you will champion Salesforce Marketing Cloud, serve as a SME (Subject Matter Expert) on our customer databases, lead generation strategies, measure ROI on campaigns, research, and supervise our current Marketing Data Analyst. The ideal candidate will have a strong background in data analysis and marketing analytics with an emphasis on Salesforce Marketing Cloud and Google Analytics.
This position is In-Person and will have the employee primarily located in one of our Branches.
Key Responsibilities
Champion Salesforce Marketing Cloud including but not limited to:
Recommend and implement marketing automation to streamline/align sales efforts and increase effectiveness.
Collaborate with SMEs to architect prospect engagement activities.
Share outcomes of efforts through reporting and suggest enhancements for better results.
Serve as SME of FIBT’s customer database and share insights with business line leaders and operations teams to help identify opportunities and drive effective strategies for engagement and growth.
Lead effective strategies for customer engagement and cross-selling; includes targeted digital marketing campaigns and SEO/SEM practices; includes collaboration with third parties.
Measure ROI of online and offline advertising campaigns including, but not limited to, reporting of KPIs such as leads, conversion rates, website traffic and social media engagement.
Conduct competitive research and analyze benchmarking data, including consumer behavioral insights.
Supervise marketing data team member(s)
Qualifications
Bachelor degree in Marketing or Business Administration; a minimum of 7 years of marketing analyst experience, or equivalent combination of education and experience is required.
Salesforce Marketing Cloud experience highly preferred.
Google Analytics experience is highly preferred.
In-depth understanding of SEM campaign strategies and SEO practices and experience with PPC campaigns.
First International Bank and Trust is a family-owned full-service Community Bank with locations in ND, MN, SD, and AZ. We show our employees we care by providing competitive benefits and training and growth opportunities. Here are the things we offer within our full-time benefits package:
Health, Dental & Vision
401K Retirement Plan with Employer Match
Paid Parental Leave
Life and Disability Insurance
Generous PTO and Holiday pay
If you are interested in learning more, you can apply or if you have questions before applying you can reach out directly at TalentAcquisition@FIBT.com.
Equal Opportunity Employer
Show more
Show less","Salesforce Marketing Cloud, Google Analytics, Data analysis, Marketing analytics, Customer relationship management (CRM), Customer engagement, Crossselling, Digital marketing, Search engine optimization (SEO), Search engine marketing (SEM), Payperclick (PPC) advertising, Competitive research, Benchmarking, Reporting, Team supervision","salesforce marketing cloud, google analytics, data analysis, marketing analytics, customer relationship management crm, customer engagement, crossselling, digital marketing, search engine optimization seo, search engine marketing sem, payperclick ppc advertising, competitive research, benchmarking, reporting, team supervision","benchmarking, competitive research, crossselling, customer engagement, customer relationship management crm, dataanalytics, digital marketing, google analytics, marketing analytics, payperclick ppc advertising, reporting, salesforce marketing cloud, search engine marketing sem, search engine optimization seo, team supervision"
Senior Data Analytics Developer,BC Public Service,"British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-data-analytics-developer-at-bc-public-service-3776692320,2023-12-17,British Columbia, Canada,Associate,Remote,"Ministry of Finance
This is a temporary opportunity until July 12, 2024.
A permanent appointment may result from this temporary appointment.
Remote work is allowed, this position can work up to full time from their home in British Columbia subject to an approved telework agreement. The locations listed above are to assist applicants in searching for this opportunity and are not a complete list of locations.
Senior Data Analytics Developer
$88,636.83 - $101,099.95 annually which includes a 9.9% Temporary Market Adjustment*
Bring your in-depth knowledge and experience with data to this opportunity
The Senior Data Analytics Developer provides expert data analysis, data design, data modeling and data quality services. Participates in the design, development, maintenance and production support of data pipeline and data products (e.g., data warehouse, data mart, etc.) for the Finance Reporting & Enhanced Data Analysis (FREDA) branch.
Qualifications for this role include:
Certificate or higher in the computer science field plus three years of *related experience;
OR,
An equivalent combination of relevant education and experience may be considered.
Minimum three years’ experience working with Structured Query Language (SQL) and/or data engineering programming and/or scripting languages (such as Python, Scala, etc.).
*Related experience includes a combination of the following:
o Relational and/or object data modeling;
o Data analysis and data cleansing;
o Working with data pipelines;
o Corporate data design, warehousing and/or business area analysis techniques.
Preference may be given to applicants with experience leading project teams.
For more information and to apply online by December 27, 2023, please go to:
https://bcpublicservice.hua.hrsmart.com/hr/ats/Posting/view/107645
Show more
Show less","Data Analytics, Data Design, Data Modeling, Data Quality, Data Pipeline, Data Products, Data Warehousing, Data Mart, Structured Query Language (SQL), Data Engineering, Programming, Scripting, Python, Scala, Relational Data Modeling, Object Data Modeling, Data Analysis, Data Cleansing, Data Warehousing, Business Area Analysis","data analytics, data design, data modeling, data quality, data pipeline, data products, data warehousing, data mart, structured query language sql, data engineering, programming, scripting, python, scala, relational data modeling, object data modeling, data analysis, data cleansing, data warehousing, business area analysis","business area analysis, data design, data engineering, data mart, data pipeline, data products, data quality, dataanalytics, datacleaning, datamodeling, datawarehouse, object data modeling, programming, python, relational data modeling, scala, scripting, structured query language sql"
Data Application Developer,Bosa Properties,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/data-application-developer-at-bosa-properties-3776654165,2023-12-17,British Columbia, Canada,Associate,Hybrid,"About The Company
Bosa Properties Inc. (“BPI”) is an end-to-end real estate company offering human-centered solutions for property development and management. Based out of Vancouver, Canada, our team of in-house experts work across residential, commercial and master-planned projects, with a growing residential portfolio that includes more than 20,000 homes that are built or under development, and an income portfolio that includes 5.5 million square feet of retail, industrial, office and rental properties under management.
We are an organization of high achievers and believe what is created in community is always better than what is created in isolation. We set high expectations and support you in achieving excellence. We are building a trusted community to collaborate, learn from each other and deliver results.
About The Role
Bosa Properties is seeking a Data Application Developer to join our innovative real estate team. Reporting to the Senior Manager, Data & Analytics, this pivotal role involves gathering and analyzing data application requirements and leading the development of intuitive data applications. Your expertise in Power Platform and readiness to adapt to AWS Front-End tools & services will be crucial in driving data-driven decision-making and supporting organizational growth. Collaborate with diverse teams to deliver impactful data solutions that propel business success.
What You'll Do
Partner with Architect and Data Engineers to define the architecture, and integration roadmap on data application development & usage within Bosa properties;
Design and develop the data applications for the organization;
Build and manage Power App & Power Automate solutions within Power Platform Environment;
Define objects and their relationships/ data models within our back-end systems to support the data applications;
Partner with stakeholders across various departments to understand their data needs and create meaningful, intentional data solutions;
Translate complex business requirements into technical specifications for data application development;
Identify and implement opportunities for optimization and improvement in data application development processes;
Support the evaluation and migration of data application tools when needed;
Act as a thought leader by staying current with industry analytics best practices and trends, driving innovation within the team, and integrating external data sources to enhance our insights on customers, products, and markets.
What You Bring
Five (5) years of experience in data analysis, business intelligence, or a related field;
Post-secondary education in Computer Science, Data Science, Statistics, or a related field;
Advanced skills in Power Platform Management (specifically Power App & Power Automate solutions) and Power BI;
Demonstrated understanding of the AWS environment. Bonus if you have experience with Aurora, Amplify and/or Quicksight.
Demonstrated ability to listen intently and work collaboratively, building strong relationships across the organization and with external partners;
Proven track record of effectively communicating insights through data visualization and storytelling with the ability to adjust project plans to accommodate new priorities or opportunities;
Demonstrated expertise in data manipulation language (DAX and/or SQL), data warehousing concepts and ETL processes;
Deep understanding of RDBMS concepts.
Salary
The expected base salary range for this role is $96,700 - $133,000. Our salary ranges and bonus percentages are determined by job family and level. Base salary is determined by a combination of factors including, but not limited to, education and training, years of relevant experience, and internal equity.
Who You Are
Trustworthy: You lead with trust when interacting with your team and other departments. You proactively build trust by demonstrating credibility, empathy, and sincerity.
Humble: Unpretentious and self-aware, you take responsibility for your mistakes. You know that egos are barriers to doing our best work and always learning.
Community focused: You believe what is created in community is always better than what is created in isolation and excellence is created through collaboration.
How to Apply
We value your interest in the Bosa Properties. While we can only respond to shortlisted applicants, we will keep your information on file and consider you for future opportunities as they come available.
You belong here! If your experience and interests match with some of the above, we want you to apply. We are dedicated to building a diverse community, where everyone belongs.
Accommodations will be provided as requested by candidates taking part in all aspects of the selection process.
#LI-Hybrid
Show more
Show less","Power Platform, Power App, Power Automate, Power BI, AWS, Aurora, Amplify, Quicksight, RDBMS, DAX, SQL, ETL, Data analysis, Business intelligence, Data warehousing, Data manipulation language","power platform, power app, power automate, power bi, aws, aurora, amplify, quicksight, rdbms, dax, sql, etl, data analysis, business intelligence, data warehousing, data manipulation language","amplify, aurora, aws, business intelligence, data manipulation language, dataanalytics, datawarehouse, dax, etl, power app, power automate, power platform, powerbi, quicksight, rdbms, sql"
Senior Data Engineer-Canada,Zortech Solutions,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-canada-at-zortech-solutions-3744624746,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Role: Senior Data Engineer
Location: Vancouver BC-Onsite
Duration: 6+ Months
Job Description
Must have 9+ years of experience
Must have strong experience with SQL, PostgreSQL, Python and AWS
Show more
Show less","Data Engineering, SQL, PostgreSQL, Python, AWS","data engineering, sql, postgresql, python, aws","aws, data engineering, postgresql, python, sql"
Senior Data Engineer- Finance & Analytics,lululemon,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-finance-analytics-at-lululemon-3761305583,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Requirements
Description & Requirements
Who we are
lululemon is an innovative performance apparel company for yoga, running, training, and other athletic pursuits. Setting the bar in technical fabrics and functional design, we create transformational products and experiences that support people in moving, growing, connecting, and being well. We owe our success to our innovative product, emphasis on stores, commitment to our people, and the incredible connections we make in every community we're in. As a company, we focus on creating positive change to build a healthier, thriving future. In particular, that includes creating an equitable, inclusive and growth-focused environment for our people.
About This Team
The Senior Data Engineer will be part of our Finance Data Hub team, building enterprise data streams, ingesting data from multiple data sources, and provisioning data as raw data streams and canonical business events. We are working towards integrating our Finance data sources into our federated enterprise data. The engineer would also be responsible for continuously identifying opportunities for automation and reduction of technical debt & manual data manipulations to create efficiencies & optimize data delivery. They should also work with cross-functional business teams, identify data hydration solutions, assess the feasibility, and provide recommendations to other developers on automation opportunities ensuring that delivered frameworks are scalable.
A day in the life
Co-ordinate with multiple data source teams in getting data published to our Finance Data Warehouse
Working with stakeholders including data, design, product, and executive teams, and assisting them with data related and technical issues
Building Scalable Data Pipelines for generating training datasets for machine learning deliverables
Mentoring Junior resources and drive end to end design, implementation and delivery of engineering components
Communicating with and educate both senior and junior colleagues to further embed data science and analytics across the organization
Strive for continuous improvement of code quality and development practices
Willingness to adapt to and self-learn new technologies and deliver on them
Translating business issues to technical terms
Understanding, leveraging, and applying best practices effectively. Also leads by example and comes up with coding standards and best practices for technology
Collaborating with cross-functional teams – business stakeholders, engineers, program management, project management, etc. - to produce the best solutions possible
Anticipating system/application challenges and proposes solutions for the same
Contributing to story sizing and work estimates for implementation, validation, delivery and documentation
Reviewing user stories to ensure a quality user experience, well-defined acceptance criteria and thorough test coverage
Participating in design and code review to ensure quality and testability of feature code
Qualifications
BS in Computer Science or Related
5+ years of data engineering experience
Good understanding of modern data platforms including data lakes and data warehouse, with good knowledge of the underlying architecture, preferably in Snowflake
Proven experience in assembling large, complex sets of data that meets non-functional and functional business requirements
Experience in identifying, designing, and implementing integration, modelling, and orchestration of complex Finance data and at the same time look for process improvements, optimize data delivery and automate manual processes.
Working experience of scripting, data science and analytics (SQL, Python, PowerShell, JavaScript)
Working experience of performance tuning and optimization, bottleneck problems analysis, and technical troubleshooting in a, sometimes, ambiguous environment.
Working experience of working with cloud based systems (Azure experience preferred)
Must haves
Acknowledge the presence of choice in every moment and take personal responsibility for your life.
Possess an entrepreneurial spirit and continuously innovate to achieve great results.
Communicate with honesty and kindness and create the space for others to do the same.
Lead with courage, knowing the possibility of greatness is bigger than the fear of failure.
Foster connection by putting people first and building trusting relationships.
Integrate fun and joy as a way of being and working, aka doesn’t take yourself too seriously.
Compensation And Benefits Package
lululemon’s compensation offerings are grounded in a pay-for-performance philosophy that recognizes exceptional individual and team performance. The typical hiring range for this position is from $117,600 - $154,400 annually ; the base pay offered is based on market location and may vary depending on job-related knowledge, skills, experience, and internal equity. As part of our total rewards offering, permanent employees in this position may be eligible for our competitive annual bonus progra m, subject to program eligibility requirements.
At Lululemon, Investing In Our People Is a Top Priority. We Believe That When Life Works, Work Works. We Strive To Be The Place Where Inclusive Leaders Come To Develop And Enable All To Be Well. Recognizing Our Teams For Their Performance And Dedication, Other Components Of Our Total Rewards Offerings Include Support Of Career Development, Wellbeing, And Personal Growth
Extended health and dental benefits, and mental health plans
Paid time off
Savings and retirement plan matching
Generous employee discount
Fitness & yoga classes
Parenthood top-up
Extensive catalog of development course offerings
People networks, mentorship programs, and leadership series (to name a few)
Note: The incentive programs, benefits, and perks have certain eligibility requirements. The Company reserves the right to alter these incentive programs, benefits, and perks in whole or in part at any time without advance notice.
Workplace arrangement
This role is classified as Hybrid under our SSC Workplace Policy:
Hybrid
In-person collaboration is important, and much of the role can be performed remotely. Work is performed onsite at least 3 days per week.
Show more
Show less","Data Engineering, Snowflake, Data Lakes, Data Warehouses, SQL, Python, PowerShell, JavaScript, Azure, Cloud Computing, Data Science, Analytics, Machine Learning, Data Pipelines, Data Integration, Data Modeling, Data Orchestration, Data Optimization, Data Automation, Technical Troubleshooting, Performance Tuning","data engineering, snowflake, data lakes, data warehouses, sql, python, powershell, javascript, azure, cloud computing, data science, analytics, machine learning, data pipelines, data integration, data modeling, data orchestration, data optimization, data automation, technical troubleshooting, performance tuning","analytics, azure, cloud computing, data automation, data engineering, data integration, data lakes, data optimization, data orchestration, data science, data warehouses, datamodeling, datapipeline, javascript, machine learning, performance tuning, powershell, python, snowflake, sql, technical troubleshooting"
Senior Data Engineer,Tally,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-tally-3754706694,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Money is the #1 source of stress for Americans, and Tally is on a mission to make people less stressed and better off financially. Tally supports consumers in their financial journey and enables them to manage their credit card debt, save money on interest, and pay off their debt efficiently.
We are building a team that strives for excellence in all things and is committed to living Tally’s values of
Don’t Say Nothing, Own the Outcome,
and
Change Shoes Often
.
Learn more about our values and who we are here
.
Tally is backed by some of the world’s top investors, including Andreessen Horowitz, and Kleiner Perkins, and has raised $172MM in total funding.
Our Data Platform team is responsible for managing our data infrastructure, architecting and implementing business-critical data pipelines, enabling data driven decision making by building data products, maintaining high code quality standards to improve the availability, reliability and accuracy of our data, and supporting all internal teams. Our Data platform consists of AWS Redshift and AWS S3 and Glue catalog-based data lake solutions and other big data distributed sql engines. We use Apache airflow as our primary ETL orchestration and workflow monitoring platform.
As a Senior Data Engineer, you will develop solutions to turn millions of data points into actionable insights. You will constantly strive to evolve the data architecture and scalability. You will work on designing data architecture to support all business teams.
Core Technologies
Apache Spark, Scala, Apache Kafka, Kinesis, Python, Airflow, EMR, ECS
Role Impact
Develop solutions to turn millions of data points into actionable insights
Design, implement, and manage robust, scalable, and high-performance data pipelines for collecting, processing, and storing large volumes of structured and unstructured data
Optimize and fine-tune existing data pipelines for improved performance, reliability, and maintainability.
Collaborate with cross-functional teams to define data governance, security, and privacy policies, ensuring compliance with industry standards and regulations
In This Role We Value
Experience with big data technologies
Experience designing data pipelines, data infrastructure, data architecture and/or data platform products.
Passion for the latest distributed analytics technologies, ETL, and Big Data systems (such as data manipulation, data engineering, data analysis)
Interest in mentoring, coaching and leading junior engineers in the team
Highly skilled in data architecture and ETL design.
Collaborate well with other data/software engineers and business partners
Passion for solving complex and ambiguous problems and are skilled to make technical tradeoffs
Highly motivated to enable data-driven decision making by building supportive data products
Healthy appreciation of security principles and work hard to protect the consumer’s data
Ability to understand product requirements beyond the written word and take pride in building resilient and maintainable code to support a rapidly evolving product
A Message From Our Leadership
""One of our core values at Tally is 'Change shoes often.' We do this with our peers and our customers every day, and we want candidates to share in our commitment to empathy. Empathy really is the foundation of inclusion, and Tally prides itself in being a company that prioritizes inclusion and diversity. We’re dedicated to creating a workplace culture where people are included and treated equitably.""
Jason Brown, Co-founder and CEO
This organization participates in
E-Verify
. See more information
here
.
Tally Core Value Don’t Say Nothing
At Tally, one of our core values is Don’t Say Nothing (DSN), which speaks to our culture of transparency, and sharing all that we can. In the spirit of DSN, the annual salary range for this role is $146,000 - $231,000. We offer a comprehensive total rewards package that includes: 12 paid holidays; company subsidized medical, dental, vision and prescription insurance; company paid short and long term disability insurance and life and AD&D insurance; wellness programs; voluntary life and disability insurance; pre-tax savings accounts for commuter benefits, health savings accounts and flexible savings accounts; paid parental and other eligible leaves of absence; 401(k) retirement savings plan; and an employee assistance program. Additional perks include: phone/internet reimbursement: co-working stipend; wellness reimbursement; home office set-up allowance; and access to discount programs. For more information about Tally visit: https://www.meettally.com/.
Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. The Company reserves the right to modify this information at any time, subject to applicable law.
Show more
Show less","Apache Spark, Apache Kafka, Kinesis, Python, Apache Airflow, AWS Redshift, AWS S3, AWS Glue, EMR, ECS, Scala","apache spark, apache kafka, kinesis, python, apache airflow, aws redshift, aws s3, aws glue, emr, ecs, scala","apache airflow, apache kafka, apache spark, aws glue, aws redshift, aws s3, ecs, emr, kinesis, python, scala"
Senior Data Engineer,ActiveState,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-activestate-3315929262,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"ActiveState is securing open source software supply chains and enhancing dependency management with an on-demand SaaS Platform and CLI tool that lets DevSecOps automatically build any runtime environment using any open source language ecosystem on any platform. We’re building an ambitious language distribution platform so that no engineer ever has to suffer dependency hell again, and we need your help!
This position is open to full-time telecommuters anywhere in Canada or continental US. You can also choose to work from our headquarters in beautiful Vancouver, BC.
This position is open to experienced candidates with a track record in this area. We’re building up our systems to scale out to more languages and ecosystems, and we’re looking for someone who knows how to find, ingest, analyze, interpret, and improve open source ecosystem data to help us secure the Internet’s source code!
What You’ll be Doing
Our audacious goal is to build all of the open source software released on the internet completely from source, in an automated and repeatable way. As a Senior Data Ingestion Engineer, your challenge is to build repeatable and maintainable data ingestion pipelines where we can make improvements over time to get more and better data.
This position is a mixture of development and operations; good coding skills are a must. We ingest information about open source languages and libraries from all corners of the Internet, including upstream code repositories, public source control hosting systems, and vulnerability tracking systems.
This position is more than ETL; in many circumstances, the data we need is not available in an easily-accessible format and we need to invent ways to get it and get it right, at scale. If this sounds like an intriguing opportunity, we want to talk to you!
Your Primary Responsibilities Include
Understanding and gathering data from the Internet for open source languages and libraries.
Improving the quality of our data and pipelines by building automated monitoring and improvements systems that allow our data to get better over time.
Designing, building, deploying, and managing tools and processes to help colleagues understand data, especially in the context of open source languages, libraries, and build systems.
Designing, building, and managing ETL and warehousing pipelines, including recommending tools, technologies, and practices for us to adopt.
Testing and documenting your work.
What’s In It For You
Working for a stable and growing company that offers the environment and personal growth potential of a start-up as well as the stability of a successful business with established revenue.
The chance to grow and grow with a team, as we expand our data portfolio and team.
The chance to collaborate with a smart, considerate, enthusiastic team of people.
The chance to work on a project that will change the work lives of developers around the world, including your own!
Competitive salary and bonus plan.
Comprehensive benefits package and health/wellness credit program.
Requirements
3+ years of experience with ETL and data management technologies (for example, Airflow, Snowflake, and Matillion), especially for use with unstructured or semi-structured data
Strong development experience with at least one of the following languages:
Python
Go
Perl
Ruby
Java
Demonstrated ability to develop, customize, deploy, and maintain and develop business intelligence software.
Experience creating and optimizing data models.
Demonstrated ability to perform, validate and document both ad hoc and automated data analysis and reporting. Ability to apply statistical methods to data sets is a plus.
Excellent written and spoken communication skills, both technical and non-technical, including the ability to make data and analysis understandable and relevant to a diverse set of audiences.
ASSETS
If you have experience with any of the following please make sure to highlight it in your cover letter:
Build engineering, especially for languages such as Java, Go, and Rust
Open Source projects and culture, especially dependency management and library maintenance
Data processing, messaging, and workflow technologies such as Kafka, Map/Reduce, Hadoop, Hive, PrestoDB, Luigi, Airflow, Storm, Argo etc.
PostgreSQL
Data engineering in cloud environments, especially AWS and GCP
Kubernetes
Agile processes, including breaking large projects up into smaller stories, estimation, working in branches (GitHub Flow), code review, and CI
Practical development in Go, Perl, Python, Tcl, Elm, Java, and Ruby
Data science and/or machine learning
Designing and implementing APIs and microservices
What To Expect
We’re a polyglot company and embrace using the best language or tool for the given task at hand. We gladly use Python, Elm, JavaScript, Golang, Bazel, Docker, Kubernetes, Haskell, Airflow, and other modern tools. Quality is as important as speed. We’re building for the long run, so you’ll need to enjoy writing tests and documentation too.
We use open source software whenever possible, and we also like to contribute back to the open source ecosystem. We embrace open sourcing both libraries and tools developed in-house where that makes sense.
Our day to day work practices are centered around GitHub, pull requests, code review, continuous testing, integration, and deployment, and agile development. We coordinate with each other and the rest of the company using Slack for chat, Zoom for video calls and screen sharing, Jira, and Google Drive. We’re always looking to improve our practices and we expect you to help us to do so.
WORKING AT ACTIVESTATE
ActiveState has a collaborative, respectful, and professional culture. We’re all about working together to find the best solutions, and making sure that the experience of doing so is positive for everyone involved. There is a commitment from the CEO on down to making work at ActiveState a great experience for all.
Our company is a team of 65+ and growing, with half of the positions in technical roles. We maintain a set of core, overlapping hours, but we’re flexible with specific start and end times and are understanding about appointments and life events.
Our vision is to have an ActiveState solution on every device on every planet, so we certainly don’t lack ambition! But even though we’re ambitious we don’t expect work to become your life. We know you will do your best work in a positive environment free from death marches. For more about working at ActiveState and our Glassdoor rating go to www.activestate.com/careers.
How To Apply
Please submit your contact info, resume, and a cover letter below. Submissions without a cover letter will not be considered. We look forward to hearing from you!
We are committed to creating a welcoming environment for everyone at ActiveState and we welcome applicants from all walks of life.
Even if you don’t feel you meet every exact requirement, we still would love to hear from you and why you think you would be an awesome addition to our team and we encourage you to apply.
Show more
Show less","ETL, Data management technologies, Unstructured data, Semistructured data, Python, Go, Perl, Ruby, Java, Business intelligence software, Data models, Data analysis, Data reporting, Statistical methods, Communication skills, Data science, Machine learning, APIs, Microservices, Git, GitHub, Pull requests, Code review, Continuous testing, Continuous integration, Continuous deployment, Agile development, Slack, Jira, Google Drive, Kubernetes, SQL, PostgreSQL, Hadoop, Hive, PrestoDB, Kafka, Map/Reduce, Storm, Argo, Luigi, Airflow, Bazel, Docker, Haskell, Elm, Tcl","etl, data management technologies, unstructured data, semistructured data, python, go, perl, ruby, java, business intelligence software, data models, data analysis, data reporting, statistical methods, communication skills, data science, machine learning, apis, microservices, git, github, pull requests, code review, continuous testing, continuous integration, continuous deployment, agile development, slack, jira, google drive, kubernetes, sql, postgresql, hadoop, hive, prestodb, kafka, mapreduce, storm, argo, luigi, airflow, bazel, docker, haskell, elm, tcl","agile development, airflow, apis, argo, bazel, business intelligence software, code review, communication skills, continuous deployment, continuous integration, continuous testing, data management technologies, data models, data reporting, data science, dataanalytics, docker, elm, etl, git, github, go, google drive, hadoop, haskell, hive, java, jira, kafka, kubernetes, luigi, machine learning, mapreduce, microservices, perl, postgresql, prestodb, pull requests, python, ruby, semistructured data, slack, sql, statistical methods, storm, tcl, unstructured data"
Senior BigData Engineer,"LanceSoft, Inc.","Burnaby, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-bigdata-engineer-at-lancesoft-inc-3782861675,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Job Description...
Job Description:
PURPOSE OF THE JOB:
Performs research, design, implementation and support tasks as a member of Altran team. Works in accordance with project guidelines, quality standards and code conventions.
Responsible for area/areas within the team area of responsibility (AOR). One of the current team AOR is improving BigData Platform used by of the world's largest social media platform which deals with few petabytes of data coming to the system daily.
Investigate, create, and implement the solutions for existing technical challenges, including building/enhancing the frameworks and tools used by other development teams.
MAIN TASKS AND RESPONSIBILITIES
Obtains tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all stakeholders.
Ensures that assigned area/areas are delivered within set deadlines and required quality objectives.
Provides estimations, agrees task duration with the manager and contributes to project plan of assigned area.
Analyzes scope of alternative solutions and makes decision about area implementation based on his/her experience and technical expertise.
Leads functional and architectural design of assigned areas. Makes sure design decisions on the project meet architectural and design requirements.
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raises red flags in crisis situations which are beyond his/her AOR.
Responsible for resolving crisis situations within his/her AOR.
Initiates and conducts code reviews, creates code standards, conventions and guidelines.
Suggests technical and functional improvements to add value to the product;
Constantly improves his/her professional level.
Collaborates with other teams.
If required, make yourselves available for the visits to the client location.
Show more
Show less","Big Data, Data Analysis, Frameworks, Software Development, Code Reviews, Code Standards, Functional Design, Architectural Design, Risk Management, Crisis Management, Collaboration","big data, data analysis, frameworks, software development, code reviews, code standards, functional design, architectural design, risk management, crisis management, collaboration","architectural design, big data, code reviews, code standards, collaboration, crisis management, dataanalytics, frameworks, functional design, risk management, software development"
Senior Data Engineer,Fraser Health Authority,"Surrey, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-fraser-health-authority-3757321338,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Salary range
The salary range for this position is CAD $44.14 - $63.45 / hour
Why Fraser Health?
Fraser Health is responsible for the delivery of hospital and community-based health services to over 1.9 million people in 20 diverse communities from Burnaby to Fraser Canyon on the traditional territories of the Coast Salish and Nlaka’pamux Nations. Our team of 45,000 staff, medical staff and volunteers is dedicated to serving our patients, families and communities to deliver on our vision: Better health, best in health care.
At System Optimization department of Fraser Health, we are looking for a Senior Data Engineer to join our agile and fast paced team. Reporting to the Data Engineering & Infrastructure Manager, this position is part of the team responsible for developing and maintaining ETL pipelines, developing data models and supporting the creation of a hybrid cloud model. A Senior Data Engineer leads the team across our data strategy, analysis, data integration, and data management.
The work performed has a significant impact on the delivery and use of our data products to the organization. The Senior Data Engineer will rely on extensive background and experience in data modeling and building traditional and distributed data pipelines to help guide and explore new grounds in adopting new tools to improve System Optimization’s Data Lake and relevant infrastructure.
We currently have
4 Full Time vacancies
for this role. We offer the option of hybrid work with in office days based at our Central City offices in Surrey, BC, with access to rapid transit, shopping, restaurants, on-site gym and other amenities.
Job duties:
Lead the evolution and ensure the performance of our System Optimization Data Lake
Prepare technical documents and oversee technical issues
Identify areas for improvement and provide solutions
Clarify program intent and requirements with various personnel, and suggest changes
Guide cloud migration from on premise SQL servers to MS Azure
Liaise with various data source teams and stakeholders, assisting with data and technical issues
Develop and align ETL/ELT processes to current and developing architecture
Perform complex programming and data structure alterations
Strive for code quality improvement and self-learn new technologies
Translate business issues into technical terms and implement best practices
Collaborate with cross-functional teams to produce optimal solutions
Anticipate system challenges and propose solutions
Contribute to work estimates and participate in design and code reviews
Provide mentorship for junior Data Engineering team members.
Qualifications:
Bachelor's degree in Health Information Science, Computer Science, Software Engineering or a related study
Seven (7) years of related experience in a large complex organization
A good understanding of modern data platforms with proven experience in assembling large, complex sets of data that meets non-functional and functional business requirements using a variety of languages.
An equivalent combination of education, training and experience may be considered.
Preference will be given to candidates with experience in:
Five years of data engineering experience or equivalent combination of education, training, and experience.
Hands-on experience with scripting, analytics, and Microsoft BI tools (SQL, Python, PowerShell, JavaScript, SSIS, SSAS, SSRS, PowerBI)
Understanding of data platforms including data lakes and data warehouses
Experience assembling large, complex data sets that meet business requirements
Proficiency in integration, modelling, and orchestration of complex data; optimization and automation skills required
Expertise in designing and optimizing data table structures, reports, and queries
Knowledge of performance tuning and troubleshooting in ambiguous environments
Experience with Azure DevOps, Azure Data Factory, and Azure Synapse is a plus
Ability to design, implement, and monitor best practices for Dev framework
Experience with large volume data; Healthcare data experience is a plus
Effective October 26th 2021, all new hires to Fraser Health will need to have full COVID 19 vaccination (have received a full series of a World Health Organization “WHO” approved vaccine against infection by SARS-COV-2, or a combination of approved WHO vaccines). Please note this applies to all postings, and individual medical exemptions must be approved by the Provincial Health Officer.
Curious to learn what it’s like to work here? Like us on
Facebook
(@fraserhealthcareers), follow us on
Twitter
&
Instagram
(@FHCareer), or connect with us on
LinkedIn
(fraserhealthcareers) for first-hand employee insights.
Detailed Overview
Supporting the Vision, Values, Purpose and Commitments of Fraser Health including service delivery that is centered around patients/clients/residents and families:
The Senior Data Engineer provides senior consulting to Fraser Health executive within an assigned portfolio in the areas of data and data management, decision support and in corporate business areas which may include the development and implementation of application architecture. Leads enterprise-wide information systems projects; manages timelines and resources; provides leadership to team members and may facilitate change management strategies as part of the project. Coordinates the procurement and management of external services. Manages assigned staff. A Senior Data Engineer leads the team across our data strategy, analysis, data integration, and data management.
Responsibilities
Provides senior data engineering services to all disciplines and partners within an assigned portfolio by keeping abreast of the initiatives and issues within the business areas assigned and leading the business area through developing business cases, process reengineering initiatives, etc. Formulates project plans with the business areas and/or provides input into operational or strategic plans.
Ensures that projects and initiatives are aligned with the Fraser Health's Strategic direction.
Provides project leadership to a variety of large, complex projects including a number of diverse disciplines and/or affecting multiple partners; develops and manages project timelines; facilitates change management strategies; defines tasks; identifies, sources and manages resources such as contractors, staff or dollars; identifies and mitigates project related risks; provides guidance to team members.
Ensures project closure by providing post-implementation evaluation; measuring key outcomes, assessing project related staff, ensuring customer satisfaction, obtaining project sign off, and documenting lessons learned.
Provides leadership to staff assigned to lead small to medium scale projects by providing guidance on project requirements, budget management, or technical issues; receives updates from assigned team leaders on project status.
Provides input into project budget development and, once approved, manages the budget dollars by approving required contractors, purchasing software and hardware. Monitors and reports on budget variances if necessary and discusses with project sponsor as required.
Liaises and communicates with project sponsors and partners by monitoring and reporting on all phases of projects to ensure success and to increase overall quality of products implemented. Provides updates on project status and all aspects of the project as required.
Manages assigned projects using standard documentation methodologies.
Manages assigned staff by selecting, supervising, and evaluating staff; assigns, evaluates and monitors work assignments; disciplines staff as necessary and initiates terminations; assesses and recommends staffing requirements; orients staff; authorize leaves of absences and approves overtime as necessary.
Champions data management methodologies and best practices through mentoring, coaching, and communicating with team members, business units and staff.
Researches and analyzes market, technology, and industry trends and standards related to technologies for the purposes of collaboration in problem solving merging business needs with a focus on leveraging data. Provides recommendations on the use and evolution of applications and tools.
Provides expert advice and consultation by performing or managing the research, analysis, and recommendations for proposed technology changes. Provides input into development and into the use of new technology for inclusion in the technology architecture. Provides consultation and advice to senior management on technology alternatives and solutions.
Develops annual goals and objectives for the area of responsibility, in collaboration with the Manager, ensuring consistency with department plans and authority objectives and strategies.
Responsible for collaborating with teams to advise, design, and implement the data infrastructure and systems to support the organization's data-driven initiatives.
Responsible for developing and maintaining ETL pipelines, developing data models and supporting the creation of a hybrid cloud model.
Applying performance tuning and optimization, perform bottleneck problems analysis, and technical troubleshooting.
Participates in assigned Fraser Health, Provincial, and Federal committees; chairs and/or leads discussion as required.
Performs other related duties as required.
Qualifications
Bachelor's degree in Health Information Science, Computer Science, Software Engineering or a related study or equivalent combination of education, training and experience. Seven (7) years of related experience in a large complex organization. A good understanding of modern data platforms with proven experience in assembling large, complex sets of data that meets non-functional and functional business requirements using a variety of languages.
COMPETENCIES:
Demonstrates the leadership practices of the Fraser Health Leadership Framework of Clear, Caring and Courageous and creates the conditions for people to succeed.
Professional/Technical Capabilities
Ability to provide leadership to a variety of large project teams in an environment that constantly changes and that has fluctuating priorities.
Advanced knowledge of information management, business processes, technologies and applications.
Knowledge of all components of a technical architecture.
Ability to translate business needs into application architecture requirements.
Ability to quickly comprehend the functions and capabilities of new technologies.
Demonstrated knowledge of the project management process and the systems development life cycle.
Ability to be organized, goal-oriented, proactive, solution-oriented, pragmatic, and the ability to understand the long-term and short-term perspectives.
Diplomatic negotiation skills and the ability to influence.
Understanding of and the ability to manage the political climate of the organization.
Ability to provide input into project budget development, manage assigned dollars and estimate financial impact of application architecture alternatives.
Demonstrated decision making ability within complex and diverse issues.
Physical ability to perform the duties of the position.
Show more
Show less","Data engineering, ETL/ELT, SQL, Python, JavaScript, SSIS, SSAS, SSRS, PowerBI, Microsoft BI tools, Azure DevOps, Azure Data Factory, Azure Synapse, Data lakes, Data warehouses, Data modeling, Data integration, Data orchestration, Data optimization, Data automation, Data table structures, Reports, Queries, Performance tuning, Troubleshooting, Healthcare data, Cloud migration, Agile","data engineering, etlelt, sql, python, javascript, ssis, ssas, ssrs, powerbi, microsoft bi tools, azure devops, azure data factory, azure synapse, data lakes, data warehouses, data modeling, data integration, data orchestration, data optimization, data automation, data table structures, reports, queries, performance tuning, troubleshooting, healthcare data, cloud migration, agile","agile, azure data factory, azure devops, azure synapse, cloud migration, data automation, data engineering, data integration, data lakes, data optimization, data orchestration, data table structures, data warehouses, datamodeling, etlelt, healthcare data, javascript, microsoft bi tools, performance tuning, powerbi, python, queries, reports, sql, ssas, ssis, ssrs, troubleshooting"
Expert Data Engineer,Demonware,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/expert-data-engineer-at-demonware-3761303565,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Job Title:
Expert Data Engineer
Requisition ID:
R022055
Job Description:
Senior Data Engineer
Who we are:
Demonware is a member of the Activision family of studios. We run every part of the players online experience, from logging in to matchmaking to climbing the leader boards, for popular video game franchises such as Call of Duty, Tony Hawk Pro Skater, Crash Bandicoot and Skylanders.
What We Do
The work we do together every day impacts almost half a billion players around the world. We solve big company challenges with a small company feel. We are technology-driven with a culture of openness. This means collaborating to solve interesting challenges, enjoying the freedom to talk to anyone, and always feeling respected. We take the work seriously; ourselves, not so much.
Our mission:
Demonware's data team is responsible for creating the services and infrastructure that ingest the billions of events our games emit each day. At this scale, the challenges we face empower our engineers to flex their creative engineering muscles. When we get things right the analysts, studios and developers that interact with our systems can rapidly go from question to insight without ever having to talk to us.
The Demonware data team also works with studios to model and instrument the things that matter. We work with our customers to ship data forward applications, ensure compliance of data produced and oversee the quality throughout.
We are looking for a data engineer to join our dedicated data engineering team thats responsible for managing our large-scale data footprint.
Responsibilities:
Design, develop, and support scalable and efficient data applications, tools and frameworks.
Collaborate with internal data customers to provide optimized solutions for their data needs.
Assist in developing solutions around our big data platforms using technologies such as Kafka, Spark, Airflow, Spark-Streaming, Flink, Beam, and others.
Build efficient data models that allow faster data processing and transformations at scale.
Apply data engineering expertise in solving problems around our large-scale distributed big data systems.
Build API integrations with our internal systems and third-party data sources.
Collaborate closely with other engineers on the data team who are responsible for managing various aspects of our data footprint including data instrumentation, ingestion, streaming, processing, transformation, and visualization.
About you:
Comfortable with one or more computer programming languages (Python, Scala, Java, Kotlin, etc.)
Experience with concepts of distributed systems.
Experience with cloud computing concepts (Familiarity with AWS, GCP, or Azure cloud).
Familiarity with some of the big data technologies (Kafka, PubSub, Spark, Presto, Flink, etc.).
Familiarity with version control tools (Git commands) and basic understanding of containerization, build, and deployment processes.
Good problem-solving and analytical skills.
Ability to navigate ambiguity and provide optimal solutions.
Good understanding of software development practices (preferably agile practices).
Good communication and ability to work with partners independently.
Bonus Points:
Experience with CI/CD, unit and integration testing, orchestration and containerization tools e.g. GitHub, Jenkins, Airflow, Docker, Kubernetes, etc.
Experience in solving complex data engineering problems at a really large scale.
What Demonware Offers:
Relocation Support, generous paid vacation, RRSP matching program and top-tier medical, dental and life insurance coverage for you and your dependents and perks.
Our ability to build immersive and innovate worlds is only enhanced by diverse teams working in an inclusive environment. We aspire to have a culture where everyone can thrive in order to connect and engage the world through epic entertainment. We provide a suite of benefits that promote physical, emotional and financial well-being for Every World - weve got our employees covered!
The videogame industry and therefore our business is fast-paced and will continue to evolve. As such, the duties and responsibilities of this role may be changed as directed by the Company at any time to promote and support our business and relationships with industry partners.
Activision is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, protected veteran status, or any other basis protected by applicable law and will not be discriminated against on the basis of disability.
The standard base pay range for this role is143,593 303,715 CADCAD annually. These values reflect the expected annualized base pay range of new hires in British Columbia, Canada. Ultimately, your offer will be based on several factors, including relevant experience, performance, and work location. Your Talent Professional can share additional details for this role during the hiring process.
Show more
Show less","Python, Scala, Java, Kotlin, Kafka, Spark, Airflow, SparkStreaming, Flink, Beam, AWS, GCP, Azure, Git, Docker, Kubernetes, GitHub, Jenkins, CI/CD, Unit testing, Integration testing, Orchestration, Containerization","python, scala, java, kotlin, kafka, spark, airflow, sparkstreaming, flink, beam, aws, gcp, azure, git, docker, kubernetes, github, jenkins, cicd, unit testing, integration testing, orchestration, containerization","airflow, aws, azure, beam, cicd, containerization, docker, flink, gcp, git, github, integration testing, java, jenkins, kafka, kotlin, kubernetes, orchestration, python, scala, spark, sparkstreaming, unit testing"
Data Analyst-Part Time (AmazonWork),BCJobs,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-amazonwork-at-bcjobs-3781369277,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Job Title: Data Analyst-Part Time
Overview:
Seeking a detail-oriented Data Analyst to analyze large datasets, extract insights, and drive data-driven decision-making.
Responsibilities
Collect, analyze, and interpret data for actionable insights.
Develop and maintain dashboards, reports, and visualizations.
Collaborate with cross-functional teams to meet data requirements.
Ensure data accuracy and completeness.
Qualifications
Proven experience in data analysis.
Proficiency in SQL, Python, or similar tools.
Strong analytical and communication skills.
Bachelor's degree in a relevant field.
Value
Show more
Show less","Data Analysis, Data Visualization, SQL, Python, Analytical Skills, Communication Skills, Bachelor's Degree","data analysis, data visualization, sql, python, analytical skills, communication skills, bachelors degree","analytical skills, bachelors degree, communication skills, dataanalytics, python, sql, visualization"
Software Engineer - Senior Big Data Engineer,Capgemini Engineering,"British Columbia, Canada",https://ca.linkedin.com/jobs/view/software-engineer-senior-big-data-engineer-at-capgemini-engineering-3787954238,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Life at Capgemini
Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer:
Flexible work
Healthcare including dental, vision, mental health, and well-being programs
Financial well-being programs such as 401(k) and Employee Share Ownership Plan
Paid time off and paid holidays
Paid parental leave
Family building benefits like adoption assistance, surrogacy, and cryopreservation
Social well-being benefits like subsidized back-up child/elder care and tutoring
Mentoring, coaching and learning programs
Employee Resource Groups
Disaster Relief
About Capgemini Engineering
World leader in engineering and R&D services, Capgemini Engineering combines its broad industry knowledge and cutting-edge technologies in digital and software to support the convergence of the physical and digital worlds. Coupled with the capabilities of the rest of the Group, it helps clients to accelerate their journey towards Intelligent Industry. Capgemini Engineering has more than 55,000 engineer and scientist team members in over 30 countries across sectors including Aeronautics, Space, Defense, Naval, Automotive, Rail, Infrastructure & Transportation, Energy, Utilities & Chemicals, Life Sciences, Communications, Semiconductor & Electronics, Industrial & Consumer, Software & Internet.
Capgemini Engineering is an integral part of the Capgemini Group, a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided every day by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get the Future You Want | www.capgemini.com
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Please be aware that Capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process.
Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.
Please be aware that Capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process.
Name of the position: Sr. Big Data Engineer (Python)
PURPOSE OF THE JOB
Responsible for area/areas within the team area of responsibility (AOR). One of the current team AOR is implement high-performance graph storage solution, integrate listeners into systems like Hadoop, Spark, and internal tools to produce graph nodes and vertices and help build visualizations and traversals of the graphs.
Investigate, build, and implement the solutions for existing technical challenges, including building/improving the frameworks and tools used by other development teams.
Key responsibilities:
Acquires' tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all collaborators.
Ensures that assigned area/areas are delivered within required quality objectives.
Provides estimations, agrees task duration with the manager and chips in to project plan of assigned area.
Analyzes scope of alternative solutions and makes decision about area implementation based on their experience and technical expertise.
Leads functional and architectural design of assigned areas. Makes sure design decisions on the project meet architectural and design requirements.
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raises red flags in crisis situations which are beyond their AOR.
Responsible for resolving crisis situations within their AOR.
Initiates and conducts code reviews, creates code standards, conventions and guidelines.
Suggests technical and functional improvements to make valuable contributions to the product.
Constantly improves their professional level.
Collaborates with other teams.
If required, make yourselves available for the visits to the client location.
Required Skills
5+ years of code profiling experience in Python
Hive, Presto, Spark (pySpark, SparkSQL), Apache Airflow, Apache Iceberg
Experience in handling Data file formats: Parquet
Experience in Docker, K8s, Git
Proficiency in both SQL databases and NoSQL databases
Knowledge of data warehousing concepts and tools
Familiarity with frameworks like Apache Spark
Proficiency in distributed computing and parallel processing
Ability to organize and document solutions.
Organizing systems and processes to make the difficult trivial.
Would be a plus:
Experience with Scala
Experience with AWS
Show more
Show less","Python, Hive, Presto, Spark (pySpark SparkSQL), Apache Airflow, Apache Iceberg, Parquet, Docker, Kubernetes, Git, SQL databases, NoSQL databases, Data warehousing, Apache Spark, Distributed computing, Parallel processing, Scala, AWS","python, hive, presto, spark pyspark sparksql, apache airflow, apache iceberg, parquet, docker, kubernetes, git, sql databases, nosql databases, data warehousing, apache spark, distributed computing, parallel processing, scala, aws","apache airflow, apache iceberg, apache spark, aws, datawarehouse, distributed computing, docker, git, hive, kubernetes, nosql databases, parallel processing, parquet, presto, python, scala, spark pyspark sparksql, sql databases"
Expert Data Engineer (Equest),BCJobs,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/expert-data-engineer-equest-at-bcjobs-3781822267,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Job Title: Expert Data Engineer
Requisition ID: R022055
Job Description
Senior Data Engineer
Who We Are
Demonware is a member of the Activision family of studios. We run every part of the players' online experience, from logging in to matchmaking to climbing the leader boards, for popular video game franchises such as Call of Duty, Tony Hawk Pro Skater, Crash Bandicoot and Skylanders.
What We Do
The work we do together every day impacts almost half a billion players around the world. We solve big company challenges with a small company feel. We are technology-driven with a culture of openness. This means collaborating to solve interesting challenges, enjoying the freedom to talk to anyone, and always feeling respected. We take the work seriously; ourselves, not so much.
Our Mission
Demonware's data team is responsible for creating the services and infrastructure that ingest the billions of events our games emit each day. At this scale, the challenges we face empower our engineers to flex their creative engineering muscles. When we get things right the analysts, studios and developers that interact with our systems can rapidly go from question to insight without ever having to talk to us.
The Demonware data team also works with studios to model and instrument the things that matter. We work with our customers to ship data forward applications, ensure compliance of data produced and oversee the quality throughout.
We are looking for a data engineer to join our dedicated data engineering team that's responsible for managing our large-scale data footprint.
Responsibilities
Design, develop, and support scalable and efficient data applications, tools and frameworks.
Collaborate with internal data customers to provide optimized solutions for their data needs.
Assist in developing solutions around our big data platforms using technologies such as Kafka, Spark, Airflow, Spark-Streaming, Flink, Beam, and others.
Build efficient data models that allow faster data processing and transformations at scale.
Apply data engineering expertise in solving problems around our large-scale distributed big data systems.
Build API integrations with our internal systems and third-party data sources.
Collaborate closely with other engineers on the data team who are responsible for managing various aspects of our data footprint including data instrumentation, ingestion, streaming, processing, transformation, and visualization.
About you:
Comfortable with one or more computer programming languages (Python, Scala, Java, Kotlin, etc.)
Experience with concepts of distributed systems.
Experience with cloud computing concepts (Familiarity with AWS, GCP, or Azure cloud).
Familiarity with some of the big data technologies (Kafka, PubSub, Spark, Presto, Flink, etc.).
Familiarity with version control tools (Git commands) and basic understanding of containerization, build, and deployment processes.
Good problem-solving and analytical skills.
Ability to navigate ambiguity and provide optimal solutions.
Good understanding of software development practices (preferably agile practices).
Good communication and ability to work with partners independently.
Bonus Points:
Experience with CI/CD, unit and integration testing, orchestration and containerization tools e.g. GitHub, Jenkins, Airflow, Docker, Kubernetes, etc.
Experience in solving complex data engineering problems at a really large scale.
What Demonware Offers
Relocation Support, generous paid vacation, RRSP matching program and top-tier medical, dental and life insurance coverage for you and your dependents and perks.
Our ability to build immersive and innovate worlds is only enhanced by diverse teams working in an inclusive environment. We aspire to have a culture where everyone can thrive in order to connect and engage the world through epic entertainment. We provide a suite of benefits that promote physical, emotional and financial well-being for 'Every World' - we've got our employees covered!
The videogame industry and therefore our business is fast-paced and will continue to evolve. As such, the duties and responsibilities of this role may be changed as directed by the Company at any time to promote and support our business and relationships with industry partners.
Activision is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, protected veteran status, or any other basis protected by applicable law and will not be discriminated against on the basis of disability.
The standard base pay range for this role is 143,593 - 303,715 CAD CAD annually. These values reflect the expected annualized base pay range of new hires in British Columbia, Canada. Ultimately, your offer will be based on several factors, including relevant experience, performance, and work location. Your Talent Professional can share additional details for this role during the hiring process.
Value
Show more
Show less","Python, Scala, Java, Kotlin, Kafka, Spark, Airflow, SparkStreaming, Flink, Beam, AWS, GCP, Azure, Git, Docker, Kubernetes, CI/CD, Unit testing, Integration testing, Orchestration tools, Containerization tools, GitHub, Jenkins","python, scala, java, kotlin, kafka, spark, airflow, sparkstreaming, flink, beam, aws, gcp, azure, git, docker, kubernetes, cicd, unit testing, integration testing, orchestration tools, containerization tools, github, jenkins","airflow, aws, azure, beam, cicd, containerization tools, docker, flink, gcp, git, github, integration testing, java, jenkins, kafka, kotlin, kubernetes, orchestration tools, python, scala, spark, sparkstreaming, unit testing"
Software Engineer - Senior Big Data Engineer,Capgemini Engineering,"British Columbia, Canada",https://ca.linkedin.com/jobs/view/software-engineer-senior-big-data-engineer-at-capgemini-engineering-3787511751,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Life at Capgemini
Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer:
Flexible work
Healthcare including dental, vision, mental health, and well-being programs
Financial well-being programs such as 401(k) and Employee Share Ownership Plan
Paid time off and paid holidays
Paid parental leave
Family building benefits like adoption assistance, surrogacy, and cryopreservation
Social well-being benefits like subsidized back-up child/elder care and tutoring
Mentoring, coaching and learning programs
Employee Resource Groups
Disaster Relief
About Capgemini Engineering
World leader in engineering and R&D services, Capgemini Engineering combines its broad industry knowledge and cutting-edge technologies in digital and software to support the convergence of the physical and digital worlds. Coupled with the capabilities of the rest of the Group, it helps clients to accelerate their journey towards Intelligent Industry. Capgemini Engineering has more than 55,000 engineer and scientist team members in over 30 countries across sectors including Aeronautics, Space, Defense, Naval, Automotive, Rail, Infrastructure & Transportation, Energy, Utilities & Chemicals, Life Sciences, Communications, Semiconductor & Electronics, Industrial & Consumer, Software & Internet.
Capgemini Engineering is an integral part of the Capgemini Group, a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided every day by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get the Future You Want | www.capgemini.com
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Please be aware that Capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process.
Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.
Please be aware that Capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process.
Name of the position: Sr. Big Data Engineer (Python)
Purpose of the Job
Performs research, design, implementation and support tasks as a member of Capgemini team. Works in accordance with project guidelines, quality standards and code conventions.
Responsible for area/areas within the team area of responsibility (AOR). One of the current team AOR is implement high-performance graph storage solution, integrate listeners into systems like Hadoop, Spark, and internal tools to produce graph nodes and vertices and help build visualizations and traversals of the graphs.
Investigate, build, and implement the solutions for existing technical challenges, including building/improving the frameworks and tools used by other development teams.
Main Tasks and Responsibilities
Obtains tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all collaborators.
Ensures that assigned area/areas are delivered within set timelines and required quality objectives.
Provides estimations, agrees task duration with the manager and chips in to project plan of assigned area.
Analyzes scope of alternative solutions and makes decision about area implementation based on their experience and technical expertise.
Leads functional and architectural design of assigned areas. Makes sure design decisions on the project meet architectural and design requirements.
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raises red flags in crisis situations which are beyond their AOR.
Responsible for resolving crisis situations within their AOR.
Initiates and conducts code reviews, creates code standards, conventions and guidelines.
Suggests technical and functional improvements to make valuable contributions to the product;
Constantly improves their professional level.
Collaborates with other teams.
If required, make yourselves available for the visits to the client location.
Required Education And Experience.
Must have:
University degree or equivalent experience in Computer Related Sciences or similar
5+ years of code profiling experience in at least one of the following: Python,
Big Data tools: Hive, Presto, Spark(pySpark, SparkSQL), Apache Airflow, Apache Iceberg
Data file formats: Parquet
Docker, K8s, Git
Proficiency in both SQL databases and NoSQL databases
Knowledge of data warehousing concepts and tools
Familiarity with big data processing frameworks like Apache Spark
Proficiency in distributed computing and parallel processing
Ability to organize and document solutions.
Organising systems and processes to make the difficult trivial.
Would be a plus:
Experience with Scala
Experience with AWS
Show more
Show less","Python, Big Data tools: Hive Presto Spark(PySpark SparkSQL) Apache Airflow Apache Iceberg, Data file formats: Parquet, Docker, K8s, Git, SQL databases, NoSQL databases, Data warehousing concepts and tools, Apache Spark, Distributed computing, Parallel processing, Scala, Amazon Web Services","python, big data tools hive presto sparkpyspark sparksql apache airflow apache iceberg, data file formats parquet, docker, k8s, git, sql databases, nosql databases, data warehousing concepts and tools, apache spark, distributed computing, parallel processing, scala, amazon web services","amazon web services, apache spark, big data tools hive presto sparkpyspark sparksql apache airflow apache iceberg, data file formats parquet, data warehousing concepts and tools, distributed computing, docker, git, k8s, nosql databases, parallel processing, python, scala, sql databases"
Senior Data Analyst / Full-time (Remote),Staffinggroup,"Langley, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-data-analyst-full-time-remote-at-staffinggroup-3688879663,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Company Profile
Note: After Applying , Check your email Inbox or Spam Folder For Next Steps.
Important : After Applying , Check Inbox or Spam Folder For Next Steps.
The Lead Data Analyst will work with our Data Analytics & Insights team at our Corporate office located in Langley . They will be responsible for ensuring master data integrity in key business systems as well as maintaining the processes that support the data quality.
Key Responsibilities & Duties
Work with our internal data architecture team optimizing data structure and flow through reporting
Integrate disparate data sources for use in reporting
Build and structure datasets for use in analytical reports
Build analytical displays in partnership with analytics team and internal departments
Integrate analysis framework of users (or self where applicable) into reporting to improve decision making continuity and provide insights
Provide data insights and solutions to company leadership to help drive decision-making
Qualifications
Proficient in SQL and Python
Confident in business intelligence platforms such as Microsoft Power BI, Tableau, Alteryx, etc.
Capacity to multi-task in a fast-paced startup environment
Undergrad degree in comp sci/programming with 3y experience in related field
Masters degree in a Programming/Analytics related field with 1y experience or 5+y experience in a similar function preferred
Powered by Webbtree
Show more
Show less","SQL, Python, Microsoft Power BI, Tableau, Alteryx, Data Quality, Data Integration, Data Analysis, Data Visualization, Reporting","sql, python, microsoft power bi, tableau, alteryx, data quality, data integration, data analysis, data visualization, reporting","alteryx, data integration, data quality, dataanalytics, microsoft power bi, python, reporting, sql, tableau, visualization"
Senior Data Analyst [BHJOB13022_12997],Ignite Technical Resources,"Richmond, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-data-analyst-bhjob13022-12997-at-ignite-technical-resources-3768295050,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"On behalf of our Healthcare client,
Ignite Technical Resources
is looking for a
Senior Data Analyst
with our client for a
long-term contract opportunity
.
Responsibilities
Develop and publish dashboard to measure performance and quality indicator by combining data extracted from the OceanMD Analytics platform with other process and quality measures.
Document standard operating procedures for data collection and analysis processes.
Maintain a data dictionary.
Work with DRO and CHS implementation team members to identify data analytics needs and document requests.
Develops complex data extraction and transformation scripts, including data linkages, to prepare data for analysis. Performs analysis and prepares reports and other analytical products (e.g. dashboards, scorecards). Implements indicators for monitoring and regular reporting.
Skilled with and uses tools such as Excel, Power BI, SAS, SPSS, Python
Translates complex data analysis into charts, tables, infographics and other easily consumed visuals to educate and inform team members, health care providers and the public.
Assists the DRO team in the development of dashboards, tables, charts and trending graphs to inform presentations and reports.
Takes measures to protect all data and information sources to comply with protection of confidentiality and privacy policies.
Produces regular and ad hoc reports as requested.
Identify and implement prospective and methods to avoid, reduce and rectify data quality issues.
Collaborates with the CHS Knowledge Translation and Evaluation team to support the development of key performance indicators and reporting requirements to monitor benefits of the DRO program.
Identifies learning needs that contribute to skill enhancement fosters a working environment that stimulates learning and inquiry for the team.
Skills And Experience
Advanced or Expert skills in one or more of the following: SAS, R, SQL, Python
Experience working with health administrative data and data warehouses
Manipulating and compiling large databases
Significant experience writing programs and queries to manipulate and compile large health datasets and analyze the date accordingly
Master’s degree in Statistics, Computer Science, or related discipline
Experience within BC Health Authorities/Sector is a plus
Due to the volume of resumes, only those candidates being considered will be contacted.
Ignite Technical Resources is a Vancouver-based Information Technology resourcing firm built around three principles: speed, efficiency and quality. We provide highly trained and qualified technical consultants to your organization as a flexible work force option.
Ignite. Wired for people.
Follow Ignite on Twitter
@ignitetechnical
and become a fan on
Facebook
for up-to-date information about our company including job postings!
About Ignite Technical Resources.
Founded in 2005 and headquartered in Vancouver BC, Ignite Technical Resources has been the preferred IT Staffing provider for many of Canada’s largest and well-known organizations for over 15 years.By specializing in Information Technology, we ensure that our processes are focused exclusively on providing qualified IT Professionals and our commitment to speed ensures your teams and projects are staffed quickly and efficiently. Our years of experience provide us with one of the most well-developed networks of candidates in the Canadian market. Come see why Ignite Technical is one of Canada’s premier providers of IT Professionals On-Demand.Ignite Technical Resources – Your Source for Top Talent
Show more
Show less","Dashboard, Data Analytics, Data Extraction, Data Transformation, Data Warehousing, Data Dictionary, Data Linkage, Reports, Data Visualization, Data Quality, Performance Indicators, Data Confidentiality, Data Privacy, Health Administrative Data, SAS, R, SQL, Python, Statistics, Computer Science, Excel, Power BI, SPSS","dashboard, data analytics, data extraction, data transformation, data warehousing, data dictionary, data linkage, reports, data visualization, data quality, performance indicators, data confidentiality, data privacy, health administrative data, sas, r, sql, python, statistics, computer science, excel, power bi, spss","computer science, dashboard, data confidentiality, data dictionary, data extraction, data linkage, data privacy, data quality, data transformation, dataanalytics, datawarehouse, excel, health administrative data, performance indicators, powerbi, python, r, reports, sas, spss, sql, statistics, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Kelowna, British Columbia, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3752009903,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Advanced Statistical Techniques, SQL, R, Python, Performance Metrics, A/B Testing, Data Visualization, Tableau, Power BI, Statistical Modeling, Hypothesis Testing","data analysis, advanced statistical techniques, sql, r, python, performance metrics, ab testing, data visualization, tableau, power bi, statistical modeling, hypothesis testing","ab testing, advanced statistical techniques, dataanalytics, hypothesis testing, performance metrics, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Data Engineer,Affinity,"North Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-affinity-3772187234,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"Job Description:
On behalf of our client, Affinity is looking for Data Engineers for approved corporate projects to work with data architects and other team members to ensure understanding of requirements and mapping documents and develop extract(s)/report(s) per requirements.
Deliverables:
Ensuring clear understanding of requirements.
Code build as per requirements and mapping documents.
Functional and regression testing.
Data analysis and validation as per mapping documents.
Production deployment and verification.
Qualifications:
Applied knowledge in Big Data platforms, ideally with exposure to Hadoop ecosystem (HDFS, Hive, SPARK, NoSQL, YARN)
Experience developing complex SQL queries
Good programming skill in Scala
Knowledge of data validations and DataOps (Jenkins, SVN/Git)
Knowledge in enterprise systems
Hourly Rate: $80-$100 per hour.
Affinity Earn:
Know someone who’s great for this, or any of our open roles? Earn up to $4,000/year for each successful referral through Affinity Earn.   You can also earn up to $50,000 for helping us find new clients.   Learn about our referral program at   https://affinity-group.ca/earn/   or browse our jobs & follow us at   https://www.linkedin.com/company/affinity-staffing/jobs/
About Affinity:
Affinity Group is a full-service Information Technology services and staffing company. We believe recruiting is about creating long term relationships that foster a mutually beneficial partnership - an affinity. Bringing a new style of recruiting founded on five core principles: Transparency – Flexibility – Efficiency – Agility – Inclusivity.
We teamed up with ClimatePartner on 2022 to offset our emissions and move toward being a more environmentally friendly company and we are proud to now be officially Carbon Neutral Certified.
For more information on Affinity, please visit   www.affinity-group.ca
Job Number: 9464
Show more
Show less","Big Data, Hadoop, Hive, Apache Spark, NoSQL, YARN, SQL, Scala, Data Validation, DataOps, Jenkins, SVN, Git, Enterprise Systems","big data, hadoop, hive, apache spark, nosql, yarn, sql, scala, data validation, dataops, jenkins, svn, git, enterprise systems","apache spark, big data, data validation, dataops, enterprise systems, git, hadoop, hive, jenkins, nosql, scala, sql, svn, yarn"
"Software Engineer, Data Platform",Rivian,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/software-engineer-data-platform-at-rivian-3726545814,2023-12-17,British Columbia, Canada,Mid senior,Onsite,"About Rivian
Rivian is on a mission to keep the world adventurous forever. This goes for the emissions-free Electric Adventure Vehicles we build, and the curious, courageous souls we seek to attract.
As a company, we constantly challenge what’s possible, never simply accepting what has always been done. We reframe old problems, seek new solutions and operate comfortably in areas that are unknown. Our backgrounds are diverse, but our team shares a love of the outdoors and a desire to protect it for future generations.
Role Summary
The Vehicle Software Data Platform team is seeking a skilled individual to join our team and work on the infrastructure that hosts a cutting-edge Vehicle Cloud interface built on AWS. The main objective is to create a highly advanced distributed messaging system that caters to IoT use cases related to Electric Vehicles/Chargers and Cloud Use. We are utilizing various technologies like AWS S3, Kafka, and streaming technologies like Apache Flink on Kubernetes to build notification systems, stream analytics, and anomaly detection systems. This person will play a crucial role in developing a platform to store petabytes of vehicle data from multiple in-vehicle ECUs. Additionally, they will help design and construct the next-generation architecture for streaming video, audio, and telematics data between vehicles and mobiles.
Responsibilities
Design and develop Rivian bidirectional communication interface between Cloud and Vehicle.
Primary responsibility includes designing, coding, and troubleshooting technical issues as a Data Engineer.
Willing to be on-call rotation as needed to support Staging and Production systems.
Work with DevOps engineers to design systems or resolve issues.
Contribute to planning, prioritizing, and leading as needed in different capacities.
Must be willing to take up new challenges and flexible in adapting to changing requirements, design needs, and deployment challenges.
Must be flexible to work with multiple teams and collaborate within and across teams in dynamic work environments.
Qualifications
Bachelor’s degree in CS or any STEM discipline.
Knowledge of Cloud fundamentals (preferably in AWS) and distributed systems.
Robust system and component-level design and troubleshooting skills, including memory CPU profiling on cloud-based services.
Strong coding skills in one or more programming languages – Golang, Python, Rust, Java. Must include knowledge of how-to unit tests and system tests.
Good foundation on Unix and bash. Some basic knowledge of Cloud networking is preferred.
Preferred good foundations on Containerization – Docker, Kubernetes, or similar technologies
Familiarity with CI/CD, Git, and Release Management.
Preferred understanding of Cloud resource provisioning using CDK, Cloud Formation, or Terraform – not a requisite.
Pay Disclosure
Salary Range/Hourly Rate for British Columbia Based Applicants: 104,000 CAD -123,000 CAD (actual compensation will be determined based on experience, location, and other factors permitted by law).
Benefits Summary: Rivian provides robust supplementary medical/Rx, dental and vision insurance for employees, their spouse or domestic partner, and children. Coverage is effective on the first day of employment, and Rivian covers all of the premiums.
Equal Opportunity
Rivian is an equal opportunity employer and complies with all applicable federal, state, and local fair employment practices laws. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, ancestry, sex, sexual orientation, gender, gender expression, gender identity, genetic information or characteristics, physical or mental disability, marital/domestic partner status, age, military/veteran status, medical condition, or any other characteristic protected by law.
Rivian is committed to ensuring that our hiring process is accessible for persons with disabilities. If you have a disability or limitation, such as those covered by the Americans with Disabilities Act, that requires accommodations to assist you in the search and application process, please email us at candidateaccommodations@rivian.com .
Candidate Data Privacy
Rivian may collect, use and disclose your personal information or personal data (within the meaning of the applicable data protection laws) when you apply for employment and/or participate in our recruitment processes (“Candidate Personal Data”). This data includes contact, demographic, communications, educational, professional, employment, social media/website, network/device, recruiting system usage/interaction, security and preference information. Rivian may use your Candidate Personal Data for the purposes of (i) tracking interactions with our recruiting system; (ii) carrying out, analyzing and improving our application and recruitment process, including assessing you and your application and conducting employment, background and reference checks; (iii) establishing an employment relationship or entering into an employment contract with you; (iv) complying with our legal, regulatory and corporate governance obligations; (v) recordkeeping; (vi) ensuring network and information security and preventing fraud; and (vii) as otherwise required or permitted by applicable law.
Rivian may share your Candidate Personal Data with (i) internal personnel who have a need to know such information in order to perform their duties, including individuals on our People Team, Finance, Legal, and the team(s) with the position(s) for which you are applying; (ii) Rivian affiliates; and (iii) Rivian’s service providers, including providers of background checks, staffing services, and cloud services.
Rivian may transfer or store internationally your Candidate Personal Data, including to or in the United States, Canada, the United Kingdom, and the European Union and in the cloud, and this data may be subject to the laws and accessible to the courts, law enforcement and national security authorities of such jurisdictions.
Please note that we are currently not accepting applications from third party application services.
Show more
Show less","AWS, Kafka, Apache Flink, Kubernetes, Unix, Golang, Python, Rust, Java, Docker, Kubernetes, Git, Cloud Formation, Terraform","aws, kafka, apache flink, kubernetes, unix, golang, python, rust, java, docker, kubernetes, git, cloud formation, terraform","apache flink, aws, cloud formation, docker, git, golang, java, kafka, kubernetes, python, rust, terraform, unix"
Data Analyst,Affinity,"Burnaby, British Columbia, Canada",https://ca.linkedin.com/jobs/view/data-analyst-at-affinity-3771774483,2023-12-17,British Columbia, Canada,Mid senior,Remote,"Job Description:
On behalf of our healthcare client, Affinity is looking for a Data Analyst to develop and publish a performance dashboard, to analyze referral patterns between healthcare provider senders and receivers to inform implementation strategy and roadmaps and to support the development of reports and briefing notes by providing data analysis and visuals to support findings.
Responsibilities:
• Develop and publish the performance dashboard to measure performance and quality indicator by combining data extracted from the Analytics platform with other process and quality measures.
• Document standard operating procedures for data collection and analysis processes.
• Maintain a data dictionary.
• Work with implementation team members to identify data analytics needs and document requests.
• Develops complex data extraction and transformation scripts, including data linkages, to prepare data for analysis. Performs analysis and prepares reports and other analytical products (e.g. dashboards, scorecards). Implements indicators for monitoring and regular reporting.
• Skilled with and uses tools such as Excel, Power BI, SAS, SPSS, Python
• Translates complex data analysis into charts, tables, infographics and other easily consumed visuals to educate and inform team members, health care providers and the public.
• Assists the team in the development of dashboards, tables, charts and trending graphs to inform presentations and reports.
• Takes measures to protect all data and information sources to comply with protection of confidentiality and privacy policies.
• Produces regular and ad hoc reports as requested.
• Identify and implement prospective and methods to avoid, reduce and rectify data quality issues.
• Collaborates with appropriate personnel to obtain data and understand its meaning in order to properly analyze it.
• Collaborates with the team to support the development of key performance indicators and reporting requirements to monitor benefits of the program.
• Identifies learning needs that contribute to skill enhancement fosters a working environment that stimulates learning and inquiry for the team.
• Collaborates with all team members and communicates information in a timely manner.
• Performing other related duties as assigned.
Qualifications:
• 5+ years of recent related experience or an equivalent combination of education, training, and experience.
• Advanced or Expert skills in one or more of the following: SAS, R, SQL, Python
• Experience working with health administrative data and data warehouses
• Manipulating and compiling large databases
• Significant experience writing programs and queries to manipulate and compile large health datasets and analyze the date accordingly
• Master’s degree in Statistics, Computer Science, or related discipline
• Experience within healthcare is a plus
Hourly Rate: $90-$100 per hour.
Affinity Earn:
Know someone who’s great for this, or any of our open roles? Earn up to $4,000/year for each successful referral through Affinity Earn.   You can also earn up to $50,000 for helping us find new clients.   Learn about our referral program at   https://affinity-group.ca/earn/   or browse our jobs & follow us at   https://www.linkedin.com/company/affinity-staffing/jobs/
About Affinity:
Affinity Group is a full-service Information Technology services and staffing company. We believe recruiting is about creating long term relationships that foster a mutually beneficial partnership - an affinity. Bringing a new style of recruiting founded on five core principles: Transparency – Flexibility – Efficiency – Agility – Inclusivity.
We teamed up with ClimatePartner on 2022 to offset our emissions and move toward being a more environmentally friendly company and we are proud to now be officially Carbon Neutral Certified.
For more information on Affinity, please visit   www.affinity-group.ca
Job Number: 9452
Show more
Show less","Data Analysis, Dashboard Development, Data Extraction, Data Transformation, Data Warehousing, Health Administrative Data, Large Database Management, Program and Query Writing, Statistics, Computer Science, SAS, R, SQL, Python, Excel, Power BI, SPSS, Visualization, Data Dictionary, Data Quality Management, Collaboration, Communication","data analysis, dashboard development, data extraction, data transformation, data warehousing, health administrative data, large database management, program and query writing, statistics, computer science, sas, r, sql, python, excel, power bi, spss, visualization, data dictionary, data quality management, collaboration, communication","collaboration, communication, computer science, dashboard development, data dictionary, data extraction, data quality management, data transformation, dataanalytics, datawarehouse, excel, health administrative data, large database management, powerbi, program and query writing, python, r, sas, spss, sql, statistics, visualization"
Senior Data Engineer [Remote],ScalePad,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-remote-at-scalepad-3760557690,2023-12-17,British Columbia, Canada,Mid senior,Remote,"We're Hiring!
We're looking for brilliant thinkers to join our #Rocketeers. If you've ever wondered what it's like to work in a place where people enjoy their work and where talent is more important than the title, then keep reading.
What is ScalePad?
ScalePad is a market-leading software-as-a-service (SaaS) company with headquarters in 3 major cities of Canada; Vancouver, Toronto and Montreal. However, we are proud to say our employee reach is now global so we can best serve our partners all over the world.
Our success is no accident: Our award-winning platform strengthens connections between Managed Service Providers (MSPs) and their clients. Trusted by more than 10,000 partners worldwide, ScalePad’s applications are used by MSPs to improve their clients' overall IT environments and enhance the client experience. The Best Apps in Your Stack™️ deliver actionable insights that lead to enhanced service delivery through automated data collection and analysis.
You can contribute to our innovation and appreciate how your work is helping take this company to a higher level of operational maturity. More on that here.
Your mission should you choose to accept it.
As a Senior Data Engineer, you will be responsible for the continuation of our business intelligence, data management and operation solutions. This means you will actively work to develop newer and better ways to ensure we are always scalable and aligning directly with our vision of growth. You will work with Data Scientists, Business Analysts and Software Architects to develop amazing solutions. We believe in a collaborative workspace, therefore your input will be crucial to the continuous innovation of our application. This is a unique opportunity to participate in every aspect of the data engineering cycle. You will have the guidance to learn, optimize, advance, and grow every quarter.
Responsibilities.
Familiarize yourself with our current initiatives, teams, and operational workflows to seamlessly integrate into ongoing projects.
Master our technology architecture, gaining a deep understanding of our systems and infrastructure.
Gain insight into our business operations structure, needs, and cycles to align your work with organizational goals.
Join a new development team and kick off major projects.
Collaborate closely with data scientists to enable their projects with robust data engineering solutions.
Work on data lake and data warehouse solutions, optimizing data storage and retrieval processes.
Develop solutions that seamlessly connect different departments, fostering efficient data flow and information sharing.
You have autonomy in your ability to manage and prioritize your tasks with an innate ability to shift gears when required.
You are a sponge when it comes to learning and excel even when outside your comfort zone.
Maintain an unwavering commitment to the quality of your work and that of your teammates, upholding high standards.
Embrace a learning mindset, excelling even outside your comfort zone and staying updated on industry trends.
Possessing experience in OLAP databases, artificial intelligence, machine learning, and event sourcing is considered an asset.
Qualifications.
A minimum of 5 years of hands-on Data Engineer experience, with a proven track record of delivering data solutions.
Strong familiarity with Agile development methodologies and practices.
Proficiency in programming languages such as Python, R, C#, or equivalents, along with experience in crafting data solutions.
Sound knowledge of SQL and NoSQL solutions, including data modeling and optimization.
Experience in building web application back-ends, consuming and creating REST APIs, and working with various databases, including ORM/ODMs.
A level of familiarity with Databricks, PySpark and DBT
A deep understanding of data pipelines, from data extraction to transformation and loading (ETL).
A passion for Big Data, data mining, artificial intelligence, and machine learning.
A curious mindset, a love for learning, and a knack for problem-solving and idea-sharing.
A drive to build practical and valuable solutions for end-users.
You have a natural inclination for shipping and iterating at high velocity.
You can step back and appreciate the value of recognizing a mistake.
Perks, Perks, Perks.
Mentorships by industry veterans with a 100% track record of industry success in funding and scaling software companies creates endless career opportunities
Training and professional development budget provided to help advance your career and skills (available every year to every employee)
World-class offices and facilities in downtown Vancouver, Toronto and Montreal, for locals seeking a hybrid experience
Brand new, top-of-the-line hardware, and equipment providing an ideal working environment either at home or in the office
A top of the line benefits package
Monthly stipend to assist with creating the best hybrid work experience.
Employee Stock Options provided to all eligible employees!
Work-life balance.
We love our #Rocketeers! We value their work and recognize that to be a great #Rocketeer you have to have a work-life balance. Our teams work on a hybrid model. This may mean working in one of our hub offices a few set days a week with your team or working remotely, whichever is deemed most beneficial for the success of the employee and specified role. Our goal is to provide the best work-life balance, whether that is in the office or at home.
Even while working remotely our teams are dedicated to staying connected throughout the day, so there is never a dull moment and you still feel part of the community, no matter where you are working from.
Dream jobs do not knock on your door every day.
ScalePad is not your typical software company. When we hire you, we aren’t just offering you a job, but rather we are committing to investing in both you and your long-term career. You'll help shape how this modern SaaS company operates and make a genuine impact on the future of our people, product, and partners.
At ScalePad, we believe in the power of Diversity, Equity, Inclusion, and Belonging (DEIB) to drive innovation, collaboration, and success. We are committed to fostering a workplace where every individual's unique experiences and perspectives are valued, and where employees from all backgrounds can thrive. Our dedication to DEIB is woven into the fabric of our culture, guiding our actions and decisions as we build a stronger and more inclusive future together. Join us and be part of a team that celebrates differences, embraces fairness, and ensures that everyone has an equal opportunity to contribute and grow. Together, we're creating an environment where diverse voices are not only heard but also amplified, where everyone feels valued, and where we can all achieve our full potential.
Please no recruiters or phone calls.
Show more
Show less","Data Engineering, Business Intelligence, Data Management, Data Analytics, Machine Learning, Artificial Intelligence, Python, R, C#, SQL, NoSQL, Databricks, PySpark, DBT, REST APIs, ORM/ODM, ETL, Big Data, Data Mining","data engineering, business intelligence, data management, data analytics, machine learning, artificial intelligence, python, r, c, sql, nosql, databricks, pyspark, dbt, rest apis, ormodm, etl, big data, data mining","artificial intelligence, big data, business intelligence, c, data engineering, data management, data mining, dataanalytics, databricks, dbt, etl, machine learning, nosql, ormodm, python, r, rest apis, spark, sql"
Senior Bigdata Engineer,Capgemini Engineering,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-bigdata-engineer-at-capgemini-engineering-3784215355,2023-12-17,British Columbia, Canada,Mid senior,Hybrid,"Life at Capgemini
Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer:
Flexible work
Healthcare including dental, vision, mental health, and well-being programs
Financial well-being programs such as 401(k) and Employee Share Ownership Plan
Paid time off and paid holidays
Paid parental leave
Family building benefits like adoption assistance, surrogacy, and cryopreservation
Social well-being benefits like subsidized back-up child/elder care and tutoring
Mentoring, coaching and learning programs
Employee Resource Groups
Disaster Relief
About Capgemini Engineering
World leader in engineering and R&D services, Capgemini Engineering combines its broad industry knowledge and cutting-edge technologies in digital and software to support the convergence of the physical and digital worlds. Coupled with the capabilities of the rest of the Group, it helps clients to accelerate their journey towards Intelligent Industry. Capgemini Engineering has more than 55,000 engineer and scientist team members in over 30 countries across sectors including Aeronautics, Space, Defense, Naval, Automotive, Rail, Infrastructure & Transportation, Energy, Utilities & Chemicals, Life Sciences, Communications, Semiconductor & Electronics, Industrial & Consumer, Software & Internet.
Capgemini Engineering is an integral part of the Capgemini Group, a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided every day by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get the Future You Want | www.capgemini.com
Job Title: Senior Big Data Engineer
Location : Vancouver, BC or Toronto, ON (Hybrid)
Purpose of the job:
As a Bigdata Engineer, you will work on one of the world's largest social media platform which deals with a few petabytes of data coming to the system daily. You will contribute as part of R&D self-organized team working in an aggressive, innovative environment for our client.
Investigate, build, and implement the solutions for many technical challenges using innovative technologies, including building/improving Bigdata processing platform enabling work of software used by hundreds of millions of users.
Key responsibilities:
Acquires tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all collaborators.
Ensures that assigned area/areas are delivered within set timeline and required quality objectives.
Provides estimations, agrees task duration with the manager and chips in to project plan of assigned area.
Evaluating existing data systems.
Developing standard methodologies for data coding to ensure consistency within the system.
Updating and optimizing local and metadata models.
Evaluating implemented data systems for variances, discrepancies, and efficiency.
Solving and optimizing data systems.
Understand the business drivers and analytical use-cases and translate these to data products
Design, implement and maintain pipelines that produce business critical data reliably and efficiently using cloud technology
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raise red flags in crisis situations which are beyond responsibilities.
Responsible for resolving crisis situations within responsibilities.
Initiates and conducts code reviews, builds code standards, conventions, and guidelines.
Suggests technical and functional improvements to contribute to the product;
Constantly improves their professional level.
Collaborates with other teams.
Required Skills
Proven experience as Bigdata Engineer.
Proven Python and Java coding skill with 5+Years of experience.
Established big-data systems experience in Hadoop, Spark/SparkSQL, HiveSQL
Experience with large-scale graph data storage
Strong OOP skills
Effective communication (oral & written), collaboration, and communication skills
Result oriented approach
Would be a plus:
Experience with Airflow, Apache Atlas
Experience with AWS.
Experience with Kafka.
Experience with JavaScript and Go.
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.
Show more
Show less","Python, Java, Hadoop, Spark/SparkSQL, HiveSQL, OOP, Airflow, Apache Atlas, AWS, Kafka, Javascript, Go","python, java, hadoop, sparksparksql, hivesql, oop, airflow, apache atlas, aws, kafka, javascript, go","airflow, apache atlas, aws, go, hadoop, hivesql, java, javascript, kafka, oop, python, sparksparksql"
Software Engineer - Big Data Engineer,Capgemini,"British Columbia, Canada",https://ca.linkedin.com/jobs/view/software-engineer-big-data-engineer-at-capgemini-3768867574,2023-12-17,British Columbia, Canada,Mid senior,Hybrid,"Life at Capgemini
Capgemini Supports All Aspects Of Your Well-being Throughout The Changing Stages Of Your Life And Career. For Eligible Employees, We Offer
Collaborating with teams of creative, fun, and driven colleagues
Flexible work options enabling time and location-based flexibility
Company-provided home office equipment
Virtual collaboration and productivity tools to enable hybrid teams
Comprehensive benefits program (Health, Welfare, Retirement and Paid time off)
Other perks and wellness benefits like discount programs, and gym/studio access.
Paid Parental Leave and coaching, baby welcome gift, and family care/illness days
Back-up childcare/elder care, childcare discounts, and subsidized virtual tutoring
Tuition assistance and weekly hot skill development opportunities
Experiential, high-impact learning series events
Access to mental health resources and mindfulness programs
Access to join Capgemini Employee Resource Groups around communities of interest
About Capgemini
Capgemini is a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided everyday by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get The Future You Want | www.capgemini.com
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Please be aware that Capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process
Applicants for employment in Canada must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in Canada by Capgemini.
Name of the position:
Big Data Engineer
Reports to
: Team Lead/Delivery Manager
Department/Project
: Engineering
Job Description
As a Bigdata Engineer, you will work on one of the world's largest social media platform which deals with a few petabytes of data coming to the system daily. You will contribute as part of R&D self-organized team working in an aggressive, innovative environment for our client.
Investigate, build, and implement the solutions for many technical challenges using innovative technologies, including building/improving Bigdata processing platform enabling work of software used by hundreds of millions of users.
Key Responsibilities
Acquires tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all collaborators.
Ensures that assigned area/areas are delivered within set timeline and required quality objectives.
Provides estimations, agrees task duration with the manager and chips in to project plan of assigned area.
Evaluating existing data systems.
Developing standard methodologies for data coding to ensure consistency within the system.
Updating and optimizing local and metadata models.
Evaluating implemented data systems for variances, discrepancies, and efficiency.
Solving and optimizing data systems.
Understand the business drivers and analytical use-cases and translate these to data products
Design, implement and maintain pipelines that produce business critical data reliably and efficiently using cloud technology
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raise red flags in crisis situations which are beyond responsibilities.
Responsible for resolving crisis situations within responsibilities.
Initiates and conducts code reviews, builds code standards, conventions, and guidelines.
Suggests technical and functional improvements to contribute to the product;
Constantly improves their professional level.
Collaborates with other teams.
Required Skills
Proven experience as Bigdata Engineer.
Proven Python and Java coding skill with 5+Years of experience.
Established big-data systems experience in Hadoop, Spark/SparkSQL, HiveSQL
Experience with large-scale graph data storage
Strong OOP skills
Effective communication (oral & written), collaboration, and communication skills
Result oriented approach
Would Be a Plus
Experience with Airflow, Apache Atlas
Experience with AWS.
Experience with Kafka.
Experience with JavaScript and Go.
Show more
Show less","Bigdata, Hadoop, Spark, SparkSQL, HiveSQL, Python, Java, OOP, Airflow, Apache Atlas, AWS, Kafka, JavaScript, Go","bigdata, hadoop, spark, sparksql, hivesql, python, java, oop, airflow, apache atlas, aws, kafka, javascript, go","airflow, apache atlas, aws, bigdata, go, hadoop, hivesql, java, javascript, kafka, oop, python, spark, sparksql"
Software Engineer - Senior Big Data Engineer,Capgemini,"British Columbia, Canada",https://ca.linkedin.com/jobs/view/software-engineer-senior-big-data-engineer-at-capgemini-3770382167,2023-12-17,British Columbia, Canada,Mid senior,Hybrid,"Life at Capgemini
Capgemini Supports All Aspects Of Your Well-being Throughout The Changing Stages Of Your Life And Career. For Eligible Employees, We Offer
Collaborating with teams of creative, fun, and driven colleagues
Flexible work options enabling time and location-based flexibility
Company-provided home office equipment
Virtual collaboration and productivity tools to enable hybrid teams
Comprehensive benefits program (Health, Welfare, Retirement and Paid time off)
Other perks and wellness benefits like discount programs, and gym/studio access.
Paid Parental Leave and coaching, baby welcome gift, and family care/illness days
Back-up childcare/elder care, childcare discounts, and subsidized virtual tutoring
Tuition assistance and weekly hot skill development opportunities
Experiential, high-impact learning series events
Access to mental health resources and mindfulness programs
Access to join Capgemini Employee Resource Groups around communities of interest
About Capgemini
Capgemini is a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided everyday by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get The Future You Want | www.capgemini.com
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Applicants for employment in Canada must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in Canada by Capgemini.
Please be aware that Capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process.
Name of the position: Sr. Big Data Engineer (Python/Scala)
PURPOSE OF THE JOB
Performs research, design, implementation and support tasks as a member of Capgemini team. Works in accordance with project guidelines, quality standards and code conventions.
Responsible for area/areas within the team area of responsibility (AOR). One of the current team AOR is implement high-performance graph storage solution, integrate listeners into systems like Hadoop, Spark, and internal tools to produce graph nodes and vertices and help build visualizations and traversals of the graphs.
Investigate, create, and implement the solutions for existing technical challenges, including building/enhancing the frameworks and tools used by other development teams.
Main Tasks And Responsibilities
Obtains tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all stakeholders.
Ensures that assigned area/areas are delivered within set deadlines and required quality objectives.
Provides estimations, agrees task duration with the manager and contributes to project plan of assigned area.
Analyzes scope of alternative solutions and makes decision about area implementation based on his/her experience and technical expertise.
Leads functional and architectural design of assigned areas. Makes sure design decisions on the project meet architectural and design requirements.
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raises red flags in crisis situations which are beyond his/her AOR.
Responsible for resolving crisis situations within his/her AOR.
Initiates and conducts code reviews, creates code standards, conventions and guidelines.
Suggests technical and functional improvements to add value to the product;
Constantly improves his/her professional level.
Collaborates with other teams.
If required, make yourselves available for the visits to the client location.
Must Have
REQUIRED EDUCATION AND EXPERIENCE
University degree in Computer Related Sciences or similar
5+ years of solid code profiling experience in at least one of the following: Python, Scala
Big Data tools: Hive, Presto, Spark(pySpark, SparkSQL), Apache Airflow, Apache Iceberg
Data file formats: Parquet
Docker, K8s, Git
Proficiency in both SQL databases and NoSQL databases
Knowledge of data warehousing concepts and tools
Familiarity with big data processing frameworks like Apache Spark
Proficiency in distributed computing and parallel processing
Ability to organize and document solutions.
A passion for streamlining systems and processes to make the difficult trivial.
Would Be a Plus
Experience with AWS
Show more
Show less","Python, Scala, Hadoop, Spark, Presto, Apache Airflow, Apache Iceberg, Apache Spark, SQL, NoSQL, Parquet, Docker, Kubernetes, Git, Data warehousing, Distributed computing, Parallel processing, AWS, Graph storage solution, Visualization, Framework, Tool, Code review, Code standards, Conventions, Guidelines","python, scala, hadoop, spark, presto, apache airflow, apache iceberg, apache spark, sql, nosql, parquet, docker, kubernetes, git, data warehousing, distributed computing, parallel processing, aws, graph storage solution, visualization, framework, tool, code review, code standards, conventions, guidelines","apache airflow, apache iceberg, apache spark, aws, code review, code standards, conventions, datawarehouse, distributed computing, docker, framework, git, graph storage solution, guidelines, hadoop, kubernetes, nosql, parallel processing, parquet, presto, python, scala, spark, sql, tool, visualization"
Software Engineer - Senior Data Engineer,Capgemini,"British Columbia, Canada",https://ca.linkedin.com/jobs/view/software-engineer-senior-data-engineer-at-capgemini-3749364925,2023-12-17,British Columbia, Canada,Mid senior,Hybrid,"Life at Capgemini
Capgemini Supports All Aspects Of Your Well-being Throughout The Changing Stages Of Your Life And Career. For Eligible Employees, We Offer
Flexible work options enabling time and location-based flexibility
Collaborating with teams of creative, fun, and driven colleagues
Company-provided home office equipment
Comprehensive benefits program (Health, Welfare, Retirement and Paid time off)
Other perks and wellness benefits like discount programs, and gym/studio access.
Paid Parental Leave and coaching, baby welcome gift, and family care/illness days
Back-up childcare/elder care, childcare discounts, and subsidized virtual tutoring
Tuition assistance and weekly hot skill development opportunities
Experiential, high-impact learning series events
Access to mental health resources and mindfulness programs
Access to join Capgemini Employee Resource Groups around communities of interest
About Capgemini
Capgemini is a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided everyday by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get The Future You Want | www.capgemini.com
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Applicants for employment in Canada must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in Canada by Capgemini.
Name of the position:
Senior Data Engineer
Department/Project:
Engineering
PURPOSE OF THE JOB
As Senior Engineer, you will work on one of the world's largest social media platform which deals with a few petabytes of data coming to the system daily. You will contribute as part of R&D self-organized team working in a challenging, innovative environment for our client.
Investigate, create, and implement the solutions for many technical challenges using cutting edge technologies, including building/enhancing BigData processing platform enabling work of software used by hundreds of millions of users.
Main Tasks And Responsibilities
Obtains tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all stakeholders.
Ensures that assigned area/areas are delivered within set deadlines and required quality objectives.
Provides estimations, agrees task duration with the manager and contributes to project plan of assigned area.
Analyzes scope of alternative solutions and makes decisions about area implementation based on his/her experience and technical expertise.
Leads functional and architectural design of assigned areas. Makes sure design decisions on the project meet architectural and design requirements.
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raise red flags in crisis situations which are beyond his/her AOR.
Responsible for resolving crisis situations within his/her AOR.
Initiates and conducts code reviews, creates code standards, conventions, and guidelines.
Suggests technical and functional improvements to add value to the product;
Constantly improves his/her professional level.
Collaborates with other teams.
Must Have
REQUIRED EDUCATION AND EXPERIENCE
University degree in Computer Related Sciences or similar.
5+ years' experience as Data Engineer.
Strong proficiency in Scala programming language.
Proven experience with Apache Spark for big data processing.
Solid knowledge of distributed computing, data structures, and algorithms.
Proficiency in SQL and data modelling.
Rigor in high code quality, automated testing, and other engineering best practices
Strong OOP skills
Effective communication, collaboration, and interpersonal skills
Result oriented approach
Good English (oral & written) and communication skills in general.
Would Be a Plus
Experience in AWS.
Airflow experience would be plus.
Show more
Show less","Scala, Apache Spark, SQL, Data modeling, Distributed computing, Data structures, Algorithms, OOP, Automated testing, Engineering best practices, AWS, Airflow","scala, apache spark, sql, data modeling, distributed computing, data structures, algorithms, oop, automated testing, engineering best practices, aws, airflow","airflow, algorithms, apache spark, automated testing, aws, data structures, datamodeling, distributed computing, engineering best practices, oop, scala, sql"
Data Developer III,Seaspan ULC,"North Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/data-developer-iii-at-seaspan-ulc-3775815121,2023-12-17,British Columbia, Canada,Mid senior,Hybrid,"Job Description
The Data Developer is accountable for developing and expanding data warehouse and analytics solutions, primarily focusing on Azure environment. This role involves a high level of technical proficiency of data development, expansion of data warehousing and analytical solutions, including supporting the construction of Data Lake processes, implementing ETL routines, collaborating with Power BI report creators, and implementing coding standards.
Note: This position qualifies for an optional hybrid work schedule after 90 days of onsite work and onboarding. Employee will be subject to the terms of the Flexible Work Policy Agreement.
What You’ll Do
Develops and Maintains Data Lake ingestion and staging of corporate data sources for Analytics purposes using Azure Data Factory.
Executes Data Lake lifecycle processes that support effective data provisioning capabilities to feed analytical requirements, ingest IOT data sources and support Machine Learning requirements.
Develops ETL routines using Azure Data Factory to load transformed data into the Data Warehouse.
Performs continually Data Warehouse solutions to form the foundation of Seaspan’s analytical capability.
Develops and executes Azure Analysis services data models with a focus on standardized data sets, calculations, and definitions to support Power BI report development by Analytics end users.
Works closely with Power BI report creators empowering them to map reports to standardized data models that are under pinned by the Data Warehouse solution.
Maintains and supports code and component repositories to track and sustain critical elements of the Analytics stack that drives quality and versioning during the development lifecycle.
Coordinates all Analytics change activities using a Devops approach, making use of Azure Devops tools.
Develops and Executes IOT Stream Analytics data feeds that can be consumed within the Data Warehouse for analytics modelling purposes.
What You’ll Bring
BS/BA degree in Computer Science, Mathematics, or related field.
4 – 6 years of experience in business data warehouse and analytics/BI architectures, including the design and development across multiple technologies and analytical platforms.
At least two years of experience in analytical development leveraging Azure analytic components. Must have experience delivering projects leveraging all the Azure analytic components end-to-end.
Experience in delivering machine learning capabilities within a cloud data warehouse environment.
Experience implementing predictive analytic models with proven business impacts.
Preferred experience in implementations Azure Stream Analytics for IOT data analytic purposes. Including data stream modelling for data warehouse consumption.
Experience in delivering analytics development within a Devops environment with excellence shown in backlog planning, building, testing and delivery.
Must be excellent at dimensional modelling, leveraging industry standards such as Kimball design methodology.
Ability to u understand the value of a well-designed and documented Analytic environment with attention to detail regarding architecture design, requirement documentation, version control and business continuity.
Solid knowledge of Azure analytic components such as Data Lake/Blob Storage, Azure Data Factory, Data Bricks, Azure SQL, Azure SQL Data Warehouse, Azure Analysis Services and Power BI Services.
Knowledge of Machine Learning, IoT, Artificial Intelligence and Predictive Analytic related technologies.
Familiar with SQL Master Data Services (MDS) and Data Quality Services (DQS).
Knowledge in data science is preferred.
Knowledge in SAP technologies will be considered an asset.
Why you’ll love working here
This is a full-time, permanent position. With multiple operations and long-term projects, Seaspan employees enjoy job stability as we look ahead to build ships for decades to come.
In addition to an annual bonus and pension plan matching, this role provides you a very competitive salary in line with the successful candidate’s experience.
We also provide a best-in-class health and wellness benefits package for this position, including such things as full health care (e.g. unlimited physiotherapy), dental, vision, life insurance, medical leave coverage, parental leave coverage, childcare benefit partnership, wellness/fitness reimbursement for memberships or registration fees, and an Employee Family Assistance Program (EFAP).
This is a full-time permanent position because we have long term projects that need long term strategic support.
Parking is included and to assist those that are using transit, we operate a complimentary shuttle bus Monday through Friday with various pick up and drop off points around North Vancouver.
The estimated salary for this position is $115,000-$125,000. In determining final salary, Seaspan considers many factors including the successful Candidate’s skillset and experience as well as position location and internal equity. The final base salary offer will be at the Company’s sole discretion and presented as part of a competitive total compensation package.
About Us
Seaspan Shipyards is one of North America’s most modern shipyards and unique in its ability to provide a complete range of services including new construction, conversion, refit, repair and life-cycle maintenance work for Canada’s Coast Guard and Navy as well as commercial vessels including cruise-ships and deep-sea vessels. Seaspan Shipyards operations include Vancouver Shipyards, Vancouver Drydock and Victoria Shipyards.
To learn more, visit our website:
www.seaspan.com
Seaspan is committed to Employment Equity, supports diversity in the workplace, and encourages applications from all qualified individuals including, women, members of visible minorities, Indigenous persons, and persons with disabilities. Furthermore, we are committed to providing an inclusive and accessible environment. If you require reasonable accommodation during the recruitment process, please let us know, and we will work with you to support your request.
Show more
Show less","Azure, Data Factory, Data Warehouse, ETL, Power BI, Azure Analysis Services, Data Lake, Azure SQL, Azure SQL Data Warehouse, Azure Devops, Azure Stream Analytics, Kimball design methodology, SQL Master Data Services (MDS), Data Quality Services (DQS), Machine Learning, IoT, Artificial Intelligence, Predictive Analytic, SAP","azure, data factory, data warehouse, etl, power bi, azure analysis services, data lake, azure sql, azure sql data warehouse, azure devops, azure stream analytics, kimball design methodology, sql master data services mds, data quality services dqs, machine learning, iot, artificial intelligence, predictive analytic, sap","artificial intelligence, azure, azure analysis services, azure devops, azure sql, azure sql data warehouse, azure stream analytics, data factory, data lake, data quality services dqs, datawarehouse, etl, iot, kimball design methodology, machine learning, powerbi, predictive analytic, sap, sql master data services mds"
Software Engineer - Senior Data Engineer,Capgemini Engineering,"British Columbia, Canada",https://ca.linkedin.com/jobs/view/software-engineer-senior-data-engineer-at-capgemini-engineering-3757461959,2023-12-17,British Columbia, Canada,Mid senior,Hybrid,"Life at Capgemini
Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer:
Flexible work options enabling time and location-based flexibility
Collaborating with teams of creative, fun, and driven colleagues
Company-provided home office equipment
Comprehensive benefits program (Health, Welfare, Retirement and Paid time off)
Other perks and wellness benefits like discount programs, and gym/studio access.
Paid Parental Leave and coaching, baby welcome gift, and family care/illness days
Back-up childcare/elder care, childcare discounts, and subsidized virtual tutoring
Tuition assistance and weekly hot skill development opportunities
Experiential, high-impact learning series events
Access to mental health resources and mindfulness programs
Access to join Capgemini Employee Resource Groups around communities of interest
About Capgemini
Capgemini is a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided everyday by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get The Future You Want | www.capgemini.com
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Applicants for employment in Canada must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in Canada by Capgemini.
Name of the position:
Senior Data Engineer
Department/Project:
Engineering
PURPOSE OF THE JOB
As Senior Engineer, you will work on one of the world's largest social media platform which deals with a few petabytes of data coming to the system daily. You will contribute as part of R&D self-organized team working in a challenging, innovative environment for our client.
Investigate, create, and implement the solutions for many technical challenges using cutting edge technologies, including building/enhancing BigData processing platform enabling work of software used by hundreds of millions of users.
Main Tasks And Responsibilities
Obtains tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all stakeholders.
Ensures that assigned area/areas are delivered within set deadlines and required quality objectives.
Provides estimations, agrees task duration with the manager and contributes to project plan of assigned area.
Analyzes scope of alternative solutions and makes decisions about area implementation based on his/her experience and technical expertise.
Leads functional and architectural design of assigned areas. Makes sure design decisions on the project meet architectural and design requirements.
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raise red flags in crisis situations which are beyond his/her AOR.
Responsible for resolving crisis situations within his/her AOR.
Initiates and conducts code reviews, creates code standards, conventions, and guidelines.
Suggests technical and functional improvements to add value to the product;
Constantly improves his/her professional level.
Collaborates with other teams.
Required Education And Experience
Must have:
University degree in Computer Related Sciences or similar.
5+ years' experience as Data Engineer.
Strong proficiency in Scala programming language.
Proven experience with Apache Spark for big data processing.
Solid knowledge of distributed computing, data structures, and algorithms.
Proficiency in SQL and data modelling.
Rigor in high code quality, automated testing, and other engineering best practices
Strong OOP skills
Effective communication, collaboration, and interpersonal skills
Result oriented approach
Good English (oral & written) and communication skills in general.
Would be a plus:
Experience in AWS.
Airflow experience would be plus.
Show more
Show less","Apache Spark, Scala, SQL, AWS, OOP, Data Modeling, Distributed Computing, Data Structures, Algorithms, Cloud Computing, Airflow, Code Reviews, Agile Development, Technical Writing, Data Engineering","apache spark, scala, sql, aws, oop, data modeling, distributed computing, data structures, algorithms, cloud computing, airflow, code reviews, agile development, technical writing, data engineering","agile development, airflow, algorithms, apache spark, aws, cloud computing, code reviews, data engineering, data structures, datamodeling, distributed computing, oop, scala, sql, technical writing"
"Senior Software Engineer, Data Engineering",Tanium,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-software-engineer-data-engineering-at-tanium-3668131415,2023-12-17,British Columbia, Canada,Mid senior,Hybrid,"The Basics
The Tanium Senior Software Engineer, Data Engineering team member will be focused on data storage and access technologies. You will build and maintain best-of-breed products and infrastructure as part of a nimble development team. Tanium focuses on a customer engagement model and feedback process to ensure our products are designed the right way from the beginning. When new product ideas are identified, our software engineers design, develop, test, and deploy the products and supporting services from the ground up, while iterating with product management and customers for feedback and input.
What You Will Do
Architect and help build high-performance, secure, highly scalable, observable, supportable, and maintainable data platforms for use by Tanium’s products and services
Create automated unit tests, API tests, integration tests and ensure quality delivery
Work with existing persistent data stores, caches, messages buses, and other server-side technologies
Author technical documents and artifacts for developed software
We Are Looking For Someone With
Education
Bachelor's degree or equivalent experience
CS Degree or related technical field preferred
Experience
Must be able to obtain Reliability status (RS) for Protected A, B, C at a minimum
Experience with Kubernetes/Helm Charts
5+ years industry experience, 7+ preferred
Experience building large cloud-based data services
Knowledge of at least one of Golang (preferred), C++, or Rust
Experience with PostgreSQL
Experience with API design and development
Experience with Azure or AWS Database Services
Experience with Infrastructure as Code (Terraform, Pulumi)
Experience with modern software engineering development and automation tools like git and CI/CD pipelines
Other
Demonstrates excellent judgment for balancing between rapid development, long-term code maintainability and supportability
Believes in the power of and the need for writing automated tests as part of development
Experienced debugger who can put out fires under pressure when things go wrong in production environments and address the root causes of those fires for the future
Has knowledge of a variety of modern backend software frameworks and the versatility to learn new tools and languages
About Tanium
Tanium, the industry’s only provider of converged endpoint management (XEM), leads the paradigm shift in legacy approaches to managing complex security and technology environments. Only Tanium protects every team, endpoint, and workflow from cyber threats by integrating IT, Operations, Security, and Risk into a single platform that delivers comprehensive visibility across devices, a unified set of controls, and a common taxonomy for a single shared purpose: to protect critical information and infrastructure at scale. Tanium has been named to the Forbes Cloud 100 list for six consecutive years and ranks on Fortune’s list of the Best Large Workplaces in Technology. In fact, more than half of the Fortune 100 and the U.S. armed forces trust Tanium to protect people; defend data; secure systems; and see and control every endpoint, team, and workflow everywhere. That’s the power of certainty. Visit www.tanium.com and follow us on LinkedIn.
On a mission. Together.
At Tanium, we are stewards of a culture that emphasizes the importance of collaboration, respect, and diversity. In our pursuit of revolutionizing the way some of the largest enterprises and governments in the world solve their most difficult IT challenges, we are strengthened by our unique perspectives and by our collective actions.
We are an organization with stakeholders around the world and it’s imperative that the diversity of our customers and communities is reflected internally in our team members. We strive to create a diverse and inclusive environment where everyone feels they have opportunities to succeed and grow because we know that only together can we do great things.
Taking care of our team members
Each of our team members has 5 days set aside as volunteer time off (VTO) to contribute to the communities they live in and give back to the causes they care about most.
For more information on how Tanium processes your personal data, please see our Privacy Policy.
Show more
Show less","Kubernetes, Helm Charts, Golang, C++, Rust, PostgreSQL, API design, Azure, AWS, Terraform, Pulumi, Git, CI/CD, AWS Database Services","kubernetes, helm charts, golang, c, rust, postgresql, api design, azure, aws, terraform, pulumi, git, cicd, aws database services","api design, aws, aws database services, azure, c, cicd, git, golang, helm charts, kubernetes, postgresql, pulumi, rust, terraform"
"Senior Software Engineer, Core Data Services",Qumulo,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-software-engineer-core-data-services-at-qumulo-3509668698,2023-12-17,British Columbia, Canada,Mid senior,Hybrid,"About The Company
Qumulo is the leader in Hybrid Cloud File Storage, providing real-time visibility, scale and control of your data across on-prem and cloud. Qumulo’s real-time analytics enable customers to understand their storage at a granular level, detect bottlenecks and accelerate performance. Built to scale across data center and cloud, Qumulo enables programmatic configuration and management of usage, capacity and performance. Qumulo’s innovative approach continuously delights customers with new capabilities, 100% usable capacity and direct access to experts. See storage made simple at www.qumulo.com
About The Position
Engineers working on core services build the modern distributed storage engine that powers our on-premises and cloud products along with the various services that help customers store, access, and protect their data. The software stack includes a distributed and fault-tolerant block storage layer, a transactional file system, and multiple access protocols and replication/backup features. Access and permissions are provided with IAM and by interfacing with identity systems such as Active Directory and OpenLDAP. If you're excited about working on industry-leading data storage software that scales to multiple petabytes with sub-millisecond access latencies, and allows customers to run all their unstructured data workflows on one platform, we want to talk to you!
Senior Software Engineers at Qumulo work alongside people who crave collaborative coding environments. It's a community where individual engineers have a voice, and teams have the autonomy to drive their own progress.
Responsibilities
Collaborate with and provide technical leadership to team members on architecture and design of new/updated features
Contribute to feature development and testing using languages like C and Rust
Participate in team SDLC processes like stand-ups/reviews/pair programming/etc. and improve them as appropriate
Own, define and implement customer-facing features and internal improvements
Help debug and fix test failures and product issues
Qualifications
Experience programming in a compiled language like C/C++, Rust, C#, Java or Go
Firm grasp of computer science fundamentals like algorithms, data structures, concurrent programming, and operating systems concepts.
Work collaboratively with a team on shared work using strong written and oral communication skills
Previous experience with block or file storage systems, identity systems, Linux tooling, Linux networking, kernel interfaces, and/or GDB debugging would be a plus
5+ years of proven ability providing technical mentoring in a software development role
BS in Computer Science or equivalent experience
Key Benefits
The annual pay range for the role is USD $175,000 - $250,000. Individual pay depends on various factors, such as role level, relevant experience and skills, and location. Pay ranges are reviewed and typically updated each year. Offers are made within the pay range applicable at the time. U.S. based employees have access to healthcare benefits, short-term and long-term disability coverage, basic life insurance, wellbeing benefits, flexible time off, and paid holidays, among others.
Excellent healthcare coverage
Parental leave
401K investment plan
Unlimited paid time off, strongly encouraged to take at least 3 weeks per year
Other Details
Qumulo is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment without regard to race, color, gender, religion, sex, sexual orientation, age, disability, military status, or national origin or any other characteristic protected under federal, state, or applicable local law.
Please note that employment at Qumulo is contingent upon completion of a satisfactory background check.
For more information on our Applicant and Employee Privacy Notice please click on the link below:
https://qumulo.com/applicant-employee-privacy-notice
Show more
Show less","C, Rust, Linux, Block storage, File storage, Identity systems, GDB debugging, Algorithms, Data structures, Concurrent programming, Operating systems, IAM, Active Directory, OpenLDAP","c, rust, linux, block storage, file storage, identity systems, gdb debugging, algorithms, data structures, concurrent programming, operating systems, iam, active directory, openldap","active directory, algorithms, block storage, c, concurrent programming, data structures, file storage, gdb debugging, iam, identity systems, linux, openldap, operating systems, rust"
Data Quality Engineer Lead,Swim Recruiting,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/data-quality-engineer-lead-at-swim-recruiting-3764328903,2023-12-17,British Columbia, Canada,Mid senior,Hybrid,"6-month contract Data Quality Engineer Lead role working on a growing team delivering data and analytics solutions for a global enterprise organization
Data Quality Engineer Lead contract role
High growth team developing data and analytics solutions
Hybrid work environment in Vancouver, 3 days a week in office
What & Why:
As a result of growth our client is looking to add a Lead Data Quality Engineer to a growing team. You will be a part of the data engineering team building test automation frameworks. You should have data QA Leadership experience and be able to research, communicate and roll out both QA manual and automation strategy. You should also have intermediate to Advanced SQL skills as you need to be capable of creating complex queries and leveraging SQL for testing batch and real-time data. This is a really cool opportunity to join a growing team as they continue to build out their data and analytics capabilities.
Who:
This client is one of the most recognizable Canadian brands, a true Vancouver success story who has expanded globally. They are well known for their corporate culture and has won major awards for their accomplishments. They have invested majorly in technology and have had an incredibly successful past few years, including a major strategic acquisition. They have an all-star leadership team who keeps you accountable and provides support while still affording you tremendous autonomy. They are located close to rapid transit in newly renovated offices in downtown Vancouver and have a hybrid work environment with 3 days a week in office.
You:
You will bring the following education, skills and experience to the role:
5+ years of experience as SDET in BI space is needed
3+ years experience with data-centric testing in Business Intelligence/Data Engineering projects. This individual needs to have hands-on experience with ETL testing & BI reports testing (ideally with PowerBI).
2+ years experience with test automation in a DevOps environment. This individual will be involved in creating and executing scripts, and both integrating and running these through the CI/CD pipelines.
Experience in ETL/DWH and BI reports testing.
Proficient in complex SQL scripting.
Expertise in creating test automation scripts.
Able to create test strategy, test cases, test execution & reporting.
Expertise in testing both batch & real-time data using SQL queries.
Nice to Haves: Kafka, running pipelines in ADF (Azure Data Factory), Snowflake
Next Steps:
If the sound of this opportunity excites you, and you’re confident that it’s a good fit for your experience and career goals, then we’d love to hear from you! Please send your updated resume to us by applying to this posting and one of our awesome team of recruiters will be in touch.
Show more
Show less","Data Quality Engineer, Data Engineering, Test Automation Frameworks, Data QA Leadership, SQL, ETL Testing, BI Reports Testing, PowerBI, Test Automation, DevOps, CI/CD Pipelines, Complex SQL Scripting, Test Strategy, Test Cases, Test Execution, Test Reporting, Batch & RealTime Data Testing, Kafka, ADF (Azure Data Factory), Snowflake","data quality engineer, data engineering, test automation frameworks, data qa leadership, sql, etl testing, bi reports testing, powerbi, test automation, devops, cicd pipelines, complex sql scripting, test strategy, test cases, test execution, test reporting, batch realtime data testing, kafka, adf azure data factory, snowflake","adf azure data factory, batch realtime data testing, bi reports testing, cicd pipelines, complex sql scripting, data engineering, data qa leadership, data quality engineer, devops, etl testing, kafka, powerbi, snowflake, sql, test automation, test automation frameworks, test cases, test execution, test reporting, test strategy"
"Data Engineer, Data Platform",Grammarly,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3689966129,2023-12-17,British Columbia, Canada,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Python, Java, Scala, Relational databases, SQL, APIs, Microservices, AWS, Azure, GCE, Opensource software, Data lakes, Data engineering, Data processing, System design, Internal tools, Admin sites, Machine Learning, Natural Language Processing","python, java, scala, relational databases, sql, apis, microservices, aws, azure, gce, opensource software, data lakes, data engineering, data processing, system design, internal tools, admin sites, machine learning, natural language processing","admin sites, apis, aws, azure, data engineering, data lakes, data processing, gce, internal tools, java, machine learning, microservices, natural language processing, opensource software, python, relational databases, scala, sql, system design"
Data Engineer,Vancouver Fraser Port Authority,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-vancouver-fraser-port-authority-3754982829,2023-12-17,British Columbia, Canada,Mid senior,Hybrid,"Detailed Job Description
JOIN US IN CHARTING A COURSE TO A MORE SUSTAINABLE FUTURE
Join an organization that plays a critical role in Canada’s economy. At the Vancouver Fraser Port Authority, we focus on doing the incredible work it takes to become the world’s most sustainable port.
The Vancouver Fraser Port Authority is responsible for the stewardship of the Port of Vancouver, Canada’s largest and most diversified port. Our mandate as a Canada Port Authority, as set out by the Canada Marine Act, is to enable Canada’s trade objectives, ensuring goods are moved safely and responsibly through the Port of Vancouver. That means providing infrastructure and ensuring the safe and efficient movement of marine traffic and cargo within the port. It also means we carry out reviews of proposed activities and developments on federal port lands and waters to determine whether a project can go ahead without impacts to the environment and community that cannot be mitigated.
We are committed to diversity, inclusion, and sustainability. We believe in creating a workplace that reflects the richness of our community and empowers all employees to reach their full potential.
The
data engineer
is instrumental in providing technical support and high availability for the enterprise databases, data warehouses and related technology. Using a deep knowledge or experience in implementing scalable analytics, BI/ reporting solutions, and common services such as Data Governance and Data Quality in hybrid data architectures (cloud and on-prem), this position will be a strong advocate of data modeling methodologies, data engineering and analytics best practices.
This position provides exceptional client-centered service on a consistent basis to all stakeholder groups. This involves working on multiple projects involving the implementation of new or enhancement of existing systems, solutions, and processes.
What You Do
Participate in analysis, planning and developing detailed roadmaps for the modernization of the existing port authority data platform. Work closely with the senior systems analyst –databases on the implementation of roadmaps.
Be the technical owner of organization's data lake; including both on-premises (SQL server enterprise) and cloud-based environments (Azure/AWS).
Work closely with the manager, data infrastructure to architect, create, design, implement, test data models and database management systems.
Support the full analytics cycle of data design, ETL development, validating and reporting (either Agile or Waterfall). Help improve overall design and delivery standards.
Collaborate with external resources, ensuring the data architectures and standard are being adhered to through proper knowledge transfer, code versioning, governance, and quality assurance.
Ensure all upgrades, additions, and changes to VFPA’s data infrastructure, tools and reporting portals are consistent in supporting the IS department’s service level commitments to the organization.
Research new technologies, implement proofs of concept/MVPs and make recommendations to build scalable and supportable infrastructure, as appropriate.
Perform root-cause analysis for service interruption and develop solutions to complex technical problems and take appropriate urgent actions.
Configure, perform and monitor system level backups and restore via enterprise level backup solutions.
Proactively maintains data infrastructure: database servers, performance monitoring, security configuration, database software (SQL) - installation, patching, upgrades, decommissioning etc.
Develop feasible data backup and recovery procedures to prevent data loss - high availability and disaster recovery procedures.
Implement and maintain database security (user access, roles and privileges) and audit functions.
Plan for storage and capacity for raw data sources ingestion into the landing zone of VFPA data lake.
Monitor, tune and optimize the performance of databases.
Create and update database objects.
Participate in the collection, extraction, analysis, and documentation of analytics solutions from design through to code level.
Create/enhance and document logical and physical database models.
Create VFPA semantic layer views for reporting/dashboard and analytics.
Participate in forming strategies for VFPA data availability, security, archival, recovery and migration.
Build and deploy visualizations/reports using PowerBI or Tableau to either on-premises and/or cloud environments.
What You Bring
Must have a degree in Information Technology from a recognized post-secondary educational institution.
Minimum of 5 years of senior data development experience in a mid to large corporate enterprise environment.
Must Have Demonstrated Experience With The Following
Physical and logical design of database architecture for relational databases, data warehouses and data lakes.
Database optimization, data replication, database recovery and performance tuning.
Cloud data platforms (Azure and AWS) along with on-premises data integration.
Solid skills with the Microsoft BI technology stack (SSIS, SSAS, SSRS, SQL Server, TSQL, MDX, DAX, Power BI, Power Pivot, Power Query).
Experience with Azure DevOps or TFS as code version controlling tool.
Understanding of on-premises and on cloud dashboard deployments.
Orchestration of the modern data pipelines for continuous delivery.
Data Modeling/ Dimension modeling – start schemas, CDC etc.
Functional programing languages (R, Python etc.), APIs and web services.
Data security principles, efficiency, quality and governance.
Complex technical and architectural documentation skills.
What We Offer
We offer a robust total rewards package including:
Market-competitive base salaries and variable incentive plan.
Generous and inclusive benefits offering including retirement benefits plan.
Flexible remote work program.
Earned time off program.
Paid vacation and personal care leave.
Learning, development, and advancement opportunities.
Onsite gym with towel service in a national landmark in the heart of Vancouver’s waterfront.
Friendly, unique, and respectful workplace.
Location:
Vancouver, Canada
Job type:
Bargaining Unit, Full-Time, Permanent, Hybrid (Limited)
Working environment:
Normal working office conditions and participation in 7x24 on-call support.
This is Canadian-based employment, and it is expected that all employees maintain legal entitlement to work in Canada.
At the Vancouver Fraser Port Authority, we are dedicated to employment equity and a workplace reflective of the diversity of our port community. We welcome applications from all qualified candidates including women, persons with disabilities, Indigenous peoples, and members of visible minorities. Reasonable accommodation can be provided in the selection process and while working with the port authority. If you require support applying to our openings, please contact us at
careers@portvancouver.com
.
We recognize and acknowledge that we are located on the traditional territories of the xʷməθkʷəy̓əm (Musqueam), Sḵwx̱wú7mesh (Squamish), and səlilwətaɬ (Tsleil-Waututh) Nations. To learn more about our indigenous relations and commitment to reconciliation, please visit our
website
.
Show more
Show less","Data engineering, Analytics, Business intelligence (BI), Data governance, Data quality, SQL, Azure, AWS, SSIS, SSAS, SSRS, TSQL, MDX, DAX, Power BI, Power Pivot, Power Query, Azure DevOps, TFS, Cloud data platforms, Data pipelines, Data modeling, Dimension modeling, CDC, R, Python, APIs, Web services, Data security, Data efficiency, Data governance, Technical documentation, Architectural documentation","data engineering, analytics, business intelligence bi, data governance, data quality, sql, azure, aws, ssis, ssas, ssrs, tsql, mdx, dax, power bi, power pivot, power query, azure devops, tfs, cloud data platforms, data pipelines, data modeling, dimension modeling, cdc, r, python, apis, web services, data security, data efficiency, data governance, technical documentation, architectural documentation","analytics, apis, architectural documentation, aws, azure, azure devops, business intelligence bi, cdc, cloud data platforms, data efficiency, data engineering, data governance, data quality, data security, datamodeling, datapipeline, dax, dimension modeling, mdx, power pivot, power query, powerbi, python, r, sql, ssas, ssis, ssrs, technical documentation, tfs, tsql, web services"
"Learning Content Developer (Program Manager, Training & Development Level I), Data Sciences and Support",NYC Health + Hospitals,"New York, NY",https://www.linkedin.com/jobs/view/learning-content-developer-program-manager-training-development-level-i-data-sciences-and-support-at-nyc-health-%2B-hospitals-3724484772,2023-12-17,Irvington,United States,Associate,Onsite,"About NYC Health + Hospitals:
NYC Health + Hospitals is the largest public health care system in the United States. We provide essential outpatient, inpatient and home-based services to more than one million New Yorkers every year across the city’s five boroughs. Our large health system consists of ambulatory centers, acute care centers, post-acute care/long-term care, rehabilitation programs, Home Care, and Correctional Health Services. Our diverse workforce is uniquely focused on empowering New Yorkers, without exception, to live the healthiest life possible.
At NYC Health + Hospitals, our mission is to deliver high quality care health services, without exception. Every employee takes a person-centered approach that exemplifies the ICARE values (Integrity, Compassion, Accountability, Respect, and Excellence) through empathic communication and partnerships between all persons.
Job Description:
General Tasks and responsibilities will include:
Works with the data literacy team to develop high quality customized training content that aligns with business objectives
Define learning strategies and design development programs that include assessments, curricula, experiential learning, and evaluative metrics
Create design documents outlining course learning objectives, content, assessments, and evaluation criteria
Assist in creating and implementing education learning & development programs that support effective delivery of training programs for data and analytics
Understand and develop evaluation tools that measure impact and effectiveness of learning and development programs
Facilitates learning sessions in in-class, virtual and online formats.
Leads and partakes in course/training development projects as assigned by the Director of Data Literacy.
Other duties as directed
Minimum Qualifications:
1. Baccalaureate Degree from an accredited college or university in Education, Public Health, Psychology, Personnel Administration, Management or Public Administration, or a related approved discipline; and,
2. Five years of progressively responsible experience in planning and manpower development and training programs, medical or public health administration or other fields related to training, development and manpower analysis, of which at least two years must have been in a managerial, supervisory or administrative capacity related to the organization, formulation, development and implementation of such plans and programs: or,
3. A Master's Degree from an accredited college or university in Education, Management, Social Work, Medical Administration, Psychology, Personnel Administration, Public Administration, or a related approved field with four years of experience as described in (2), and two years in a managerial, supervisory or administrative capacity; and,
4. Extensive knowledge of the principals of training and manpower development programs and ability to analyze and determine the needs of the Corporation; or,
5. A satisfactory combination of training, education and experience indicating the ability to perform satisfactorily.
Special Note: In addition to meeting the qualification requirements stated above, assignment to Level II requires evidence of advance study in relevant disciplines.
Department Preferences
Excellent time management skills and attention to detail; with strong written and verbal communication skills
Proficiency in workplace learning and development principles and authoring tools with knowledge of Instructional design architecture, preferably in Camtasia
Ability to work in an Agile development process
Experience in identifying and managing complex group dynamics and reacting in real time to changes in content and/or process
Ability to effectively gather requirements from subject matter experts.
Knowledgeable In:
Curriculum Development
Bloom’s Taxonomy
Virtual/In class facilitation
NYC Health and Hospitals offers a competitive benefits package that includes:
Comprehensive Health Benefits for employees hired to work 20+ hrs. per week
Retirement Savings and Pension Plans
Loan Forgiveness Programs for eligible employees
Paid Holidays and Vacation in accordance with employees' Collectively bargained contracts
College tuition discounts and professional development opportunities
Multiple employee discounts programs
Show more
Show less","Curriculum Development, Bloom's Taxonomy, Virtual/In class facilitation, Instruction Design, Camtasia, Agile development, Subject Matter Experts","curriculum development, blooms taxonomy, virtualin class facilitation, instruction design, camtasia, agile development, subject matter experts","agile development, blooms taxonomy, camtasia, curriculum development, instruction design, subject matter experts, virtualin class facilitation"
Data Scientist,ADP,"New York, NY",https://www.linkedin.com/jobs/view/data-scientist-at-adp-3785758671,2023-12-17,Irvington,United States,Associate,Onsite,"ADP is looking for the next generation of Data Scientists to join our:
Data Scientist Development Program
(Full-time)
What's our secret to success?
Well, that's easy. People like you. We're looking for enthusiastic new graduates to help us build the next generation of ADP Data Products. As part of the
Data Science Development Program
, your goal is simple: learn as much as possible in
8 weeks.
We'll get you started with the tools, technology, and technical knowledge you need to succeed. Then, you'll be placed amongst our brightest associates as an
Associate Data Scientist.
Does the thought of working with one of the most comprehensive employment data sets excite you?
Are you inspired by transformation and making an impact on the lives of millions of people every day?
Are you a technologist first and foremost who approaches every problem wearing that hat while going out of your way to champion creativity and build diverse, productive, and engaged teams?
Well, this may be the role for you. Ready to design what's next?
In this role, you'll acquire, clean, and analyze large data sets and do exploratory analysis to understand our data and check if it meets project requirements. Moreover, you'll be developing data products to make predictions and recommendations within ADP products. It's more than digits and dials - you're making huge decisions that impact millions of customers.
To thrive in this career, you should have the knowledge of the whole data science project's life cycle. You'll need to have some formal education in quantitative analytics and know design patterns and data structures. You have the skills to create new models using state-of-the-art technologies. You will collaborate with data scientists across ADP to solve various problems with different methodologies and analyses. Given the scale of our data sets and the complex problems we give you to solve, you will stay challenged and engaged with years of exciting projects ahead-all while growing your skills and progressing in a rewarding career.
Like what you see?
Apply now!
Learn more about ADP at tech.adp.com/careers
ADP
is a global leader in HR technology, offering the latest AI and machine learning-enhanced payroll, tax, HR, benefits, and much more. We believe our people make all the difference in cultivating an inclusive, down-to-earth culture that welcomes ideas, encourages innovation, and values belonging. ADP has a deep commitment to diversity, equity, and inclusion as a global Best Places to Work, DiversityInc® Top 50 Company, Best CEO and company for women, LGBTQ+, multicultural talent, and more. Learn more about ADP's commitment on our YouTube channel: http://adp.careers/DEI_Videos
ADP Chief Data Office
allows ADP to stay competitive as well as compliant with ever-changing privacy and security regulations. It will ensure reinforcements are in place to mitigate risk and enable value creation from ADP data and analytics.
ADP Labs
located in Porto Alegre, Brazil, provides software development and infrastructure for world-wide ADP products, working with large-scale initiatives across multiple geographies, including Brazil, the United States, and India. As an internationally distributed software development team, we are hiring a
Data Scientist
for
the ADP Chief Data Office.
What You'll Do
Here's what you can expect on a typical day:
Advanced Analytics & Predictive Modeling. You formulate analytical problems, extract & analyze data from multiple systems, summarize critical findings/insights, and develop a compelling visual representation of analyses with actionable recommendations.
Team Collaboration. You will be part of a collaborative team, listening to needs, then finding and sharing solutions. You're comfortable and skilled at presenting your projects and findings to both peers and leadership. You collaborate with product, design, and engineering teams to plan, design, develop and deploy high-impact data products supporting multiple products across ADP.
Research & Development. You design and implement start-of-the-art data products, machine learning models, and systems to optimize the HCM system user experience. You continuously looking for new ways to leverage ADP data to create new products and services and find more insights. You are passionate about learning and applying new techniques within data science that leads to innovations.
Deliver at an epic scale. You write pristine code and documentation, champion and define best practices to produce highly adaptive, high-performing, continuous learning AI models/systems.
To Succeed In This Role
May 2024 Graduates only
Education. Accepting three levels of education: Bachelor's Degree, Master's Degree and/or PH.D degree in Computer Science, Statistics, Engineering, or a related field.
Communication. You are an advanced English language speaker.
Technical skills. You have fluency in Python-including popular data science packages (pandas, sklearn, etc.), proficiency in SQL, notably writing and optimizing queries, and strong knowledge of data structures, algorithms, and methods used in the data science field.
Nice To Have (optional)
Knowledge of natural language processing
Experience with integrating data products (recommender systems, classifiers, chatbots, etc.) into applications in production environments
Exposure to Jupyter notebooks, MLflow, and/or Databricks platform
AWS cloud solutions (S3, Glue, Lambda, SageMaker)
Experience with distributed computing environments (Spark/Hive)
You'll Love Working Here Because You Can
Have courageous team collaboration. Courage comes from how associates are willing to have difficult conversations, speak up, be an owner, and challenge one another's ideas to find the best solution.
Be surrounded by curious learners. We align ourselves with other smart people in an environment where we grow and elevate one another to the next level. We encourage our associates to listen, stay agile, and learn from mistakes.
Base salary offers for this position may vary based on factors such as location, skills, and relevant experience. Some positions may include additional compensation in the form of bonus, equity, or commissions. If you are a full-time salaried or hourly worker, we offer the following benefits: Medical, Dental, Vision, Life Insurance, Matched 401(k), Student Loan Repayment Program, Wellness Program, Short-and Long-Term Disability, Charitable Contribution Match, Holidays, Personal Days & Vacation, Paid Volunteer Time Off, and more. The compensation for this role is $56,900.00 - $125,300.00 / Year
Base salary offers for this position may vary based on factors such as location, skills, and relevant experience. Some positions may include additional compensation in the form of bonus, equity, or commissions. If you are a full-time salaried or hourly worker, we offer the following benefits: Medical, Dental, Vision, Life Insurance, Matched 401(k), Student Loan Repayment Program, Wellness Program, Short-and Long-Term Disability, Charitable Contribution Match, Holidays, Personal Days & Vacation, Paid Volunteer Time Off, and more. The compensation for this role is $48,400.00 - $129,780.00 / Year
Diversity, Equity, Inclusion & Equal Employment Opportunity at ADP:
ADP affirms that inequality is detrimental to our associates, our clients, and the communities we serve. Our goal is to impact lasting change through our actions. Together, we unite for equality and equity. ADP is committed to equal employment opportunities regardless of any protected characteristic, including race, color, genetic information, creed, national origin, religion, sex, affectional or sexual orientation, gender identity or expression, lawful alien status, ancestry, age, marital status, or protected veteran status and will not discriminate against anyone on the basis of a disability. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.
Ethics at ADP:
ADP has a long, proud history of conducting business with the highest ethical standards and full compliance with all applicable laws. We also expect our people to uphold our values with the highest level of integrity and behave in a manner that fosters an honest and respectful workplace. Click https://jobs.adp.com/life-at-adp/ to learn more about ADP's culture and our full set of values.
Show more
Show less","Data Science, Predictive Modeling, Data Analysis, Machine Learning, Python, SQL, Data Structures, Algorithms, Jupyter Notebooks, MLflow, Databricks, AWS Cloud Solutions, Spark, Hive, Natural Language Processing, Recommender Systems, Classifiers, Chatbots","data science, predictive modeling, data analysis, machine learning, python, sql, data structures, algorithms, jupyter notebooks, mlflow, databricks, aws cloud solutions, spark, hive, natural language processing, recommender systems, classifiers, chatbots","algorithms, aws cloud solutions, chatbots, classifiers, data science, data structures, dataanalytics, databricks, hive, jupyter notebooks, machine learning, mlflow, natural language processing, predictive modeling, python, recommender systems, spark, sql"
"Summer Intern - Aviation, Data Analysis & Forecasting",The Port Authority of New York & New Jersey,"New York, NY",https://www.linkedin.com/jobs/view/summer-intern-aviation-data-analysis-forecasting-at-the-port-authority-of-new-york-new-jersey-3785863366,2023-12-17,Irvington,United States,Associate,Onsite,"Description
The Port Authority of New York & New Jersey offers paid internships, providing students with the opportunity to work with experienced professionals on assignments critical to building, operating, and maintaining transportation and infrastructure assets in New York and New Jersey. Assignments include hands-on projects where interns quickly become part of the team and help the agency with its mission of keeping the region moving. The diversity of our businesses creates a variety of opportunities for students to gain exposure to the work of a first-class public agency.
The
Aviation Department
is looking for a
Data Analysis & Forecasting
intern to join their team!
About The Aviation Department:
The Aviation Department manages five airports at The Port Authority of New York and New Jersey, one of the busiest airport systems in the country. Within the Aviation Department, the Strategic Analysis & Forecasting Unit’s responsibilities fall under three core focus areas: Data Collection & Reporting, Forecasting, and Industry Analysis. The team develops forecasting models, industry intelligence and strategic analysis to support the Aviation Department’s business and strategic plans, terminal and capacity improvements, and revenue estimates. To aid in this effort, the team is also responsible for collecting and reporting of passenger, cargo, and operations data via the Airline Data Entry Portal (ADEP).
Responsibilities:
Use data analysis and visualization tools to analyze and derive insights from aviation datasets
Assist with developing innovative methodologies for forecasting passenger volumes
Help conduct aviation industry research, data analyses, and prepare activity forecasts
Summarize analyses and forecasts in presentation and written formats
Create data models and dashboards in Power BI to support various data analysis tasks
Develop and implement technical solutions to enhance data collection processes
Analyze and validate monthly activity data from the region’s major airports
Lead routine meetings with internal and external stakeholders
Communicate with airlines and assist with special compliance projects
Qualifications:
Enrollment in an undergraduate or graduate program through the Fall 2024 semester (August 2024 or beyond)
Experience with Microsoft apps such as Word, Excel, PowerPoint, Outlook, Teams, SharePoint, OneDrive, etc.
Excellent written and oral communication skills
Interest in contributing to the success of the New York and New Jersey region
Interest in public service and/or transportation
Willingness to learn
Desired:
Major in Transportation Planning, Engineering, Statistics, Information Systems, Data Analytics, or a related field
Strong technical background and familiarity with Microsoft Power BI and Power Apps (or similar data analysis and visualization tools)
1-2+ years of experience working with both qualitative and quantitative data
Self-starter comfortable managing complex projects with competing deadlines
An interest in data collection, management, and visualization
Willingness to create and apply innovative ideas to real problems for continuous improvement
Advanced writing and communication skills
Internship Details:
The start date will be Thursday, May 30 with an orientation that will take place at 4 World Trade Center in Lower Manhattan
The internship will last 12 weeks, ending on Friday, August 23
This is a full-time internship (generally 5 days per week at 7.25 hours of work per day plus a 45-minute break)
The format is expected to be hybrid (2 days virtual, 3 days in-person at 4 World Trade Center in Lower Manhattan)
The hourly salary range is $17.20 to $28.85 (the hourly salary is determined by internship type and school year)
These details are subject to change
How To Apply:
Click “Apply Now”
Follow the prompts
Please submit the following with your application
A resume that clearly indicates your expected graduation date
A cover letter detailing why you want to intern at The Port Authority of New York & New Jersey
How To Apply:
Interested candidates should apply to this job by clicking on the ""Apply Now"" button and submitting a combined cover letter and resume. The Port Authority of NY & NJ welcomes veteran and military spouse applications.
Only applicants under consideration will be contacted.
About The Port Authority:
Founded in 1921, the Port Authority of New York and New Jersey builds, operates, and maintains many of the most important transportation and trade infrastructure assets in the country. The agency's network of aviation, ground, rail, and seaport facilities is among the busiest in the country, supports more than 550,000 regional jobs, and generates more than twenty three billion in annual wages and eighty billion in annual economic activity. The Port Authority also owns and manages the 16-acre World Trade Center site, where the 1,776-foot-tall One World Trade Center is now the tallest skyscraper in the Western Hemisphere.
Equal Opportunity Employer
The Port Authority of New York & New Jersey/Port Authority Trans-Hudson (PATH) is an Equal Opportunity Employer.
Show more
Show less","Data analysis, Visualization, Microsoft Power BI, Power Apps, SQL, SAS, R, Python, Tableau, Forecasting, Industry analysis, Data collection, Reporting, Statistics, Information systems, Data analytics, Communication skills, Writing skills, Presentation skills, Teamwork, Problemsolving, Analytical skills","data analysis, visualization, microsoft power bi, power apps, sql, sas, r, python, tableau, forecasting, industry analysis, data collection, reporting, statistics, information systems, data analytics, communication skills, writing skills, presentation skills, teamwork, problemsolving, analytical skills","analytical skills, communication skills, data collection, dataanalytics, forecasting, industry analysis, information systems, microsoft power bi, power apps, presentation skills, problemsolving, python, r, reporting, sas, sql, statistics, tableau, teamwork, visualization, writing skills"
A&MPLIFY Data Science Analyst,Alvarez & Marsal,"New York, NY",https://www.linkedin.com/jobs/view/a-mplify-data-science-analyst-at-alvarez-marsal-3787197563,2023-12-17,Irvington,United States,Associate,Onsite,"Description
Data Science – Analyst Job Description
Summary
A&MPLIFY is a professional service consultancy combining the power of design, digital technology, and artificial intelligence to help clients supercharge value creation, agility and customer loyalty. The A&MPLIFY team is hand-picked from the top global consulting firms and Fortune 500 customer-focused companies. Individually, we are experience designers, commerce strategists, technologists, data scientists, and operations leaders with deep industry experience. Together we are the unique team clients need to drive growth and build relevance with their customers. With a commitment to positive social impact, we foster a vibrant, inclusive workplace of diverse voices and talents who enjoy what they do, and with whom they do it.
The Intelligence Analyst sits within our Data Science & Analytics community of practice. This is a hybrid role spanning remote, client, and in-office work locations and requires occasional travel.
Job Description
Work in a collaborative environment with teams across A&M and within Digital to drive clients’ Digital Transformation in a broad range of industries via scalable Analytics and AI / ML solutions which solve business problems and create value by leveraging client and external data.
Participate in client discussions, interact with client organization to articulate the value of data science approaches, different AI service offerings and guide them on implementation of the same.
Quickly understand client needs, scope of work effort, assemble teams, manage delivery, and articulate findings to client executives
Review and analyze developed models for complete, robustness and alignment to client needs
Qualifications
0-3 years professional work experience as a data scientist or on advanced analytics / statistics projects.
Strong mathematics / econometric / statistical skills, bachelor’s degree in related field
Ability to design databases and run mathematical models in the database to calculate relationship strengths
Excellent team-oriented and interpersonal skills, with a strong interest in consulting
Outstanding communication skills with the ability to clearly articulate findings and present solutions to business partners and customers
Ability to analyze information and identify second-order implications to policy, processes, communications and training
Experience with common data science tools such as Python, R, Pytorch, TensorFlow, Keras
Experience with BI tools (Tableau, Power BI, Looker)
Preferred Qualifications
Experience working in a consultative capacity with internal or external clients.
Experience with building and deploying predictive and prescriptive analytics model
Experience designing and implementing organizational or programmatic performance measurement strategies
Experience in building intelligent apps and custom analytics platforms
Experience working with big data distributed programming languages, and ecosystems: Spark, Hadoop, Snowflake, Kafka
Graph Theory work, potentially Neo4J, TigerGraph, Neptune
Good understanding of modeling platforms (Azure AutoML, SageMaker, DataBricks, DataRobot and H2O.ai)
Familiarity with Cloud-based environments such as AWS (S3/EC2), Azure, Google Cloud
Experience with Cognitive Analytics including NLP / NLG, Computer Vision, Sentiment analysis
Knowledge of web frameworks to build web applications (R Shiny, Dash)
Knowledge of other coding languages such as Java, Matlab, SAS, C++
Diversity & Inclusion
A&M’s entrepreneurial culture celebrates independent thinkers and doers who can positively impact our clients and shape our industry. The collaborative environment and engaging work—guided by A&M’s core values of Integrity, Quality, Objectivity, Fun, Personal Reward, and Inclusive Diversity—are the main reasons our people love working at A&M. Inclusive Diversity means we embrace diversity, and we foster inclusiveness, encouraging everyone to bring their whole self to work each day. It runs through how we recruit, develop employees, conduct business, support clients, and partner with vendors. It is the A&M way.
Equal Opportunity Employer
It is Alvarez & Marsal’s practice to provide and promote equal opportunity in employment, compensation, and other terms and conditions of employment without discrimination because of race, color, creed, religion, national origin, ancestry, citizenship status, sex or gender, gender identity or gender expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, family medical history, genetic information or other protected medical condition, political affiliation, or any other characteristic protected by and in accordance with applicable laws. Employees and Applicants can find A&M policy statements and additional information by region here.
Unsolicited Resumes from Third-Party Recruiters
Please note that as per A&M policy, we do not accept unsolicited resumes from third-party recruiters unless such recruiters are engaged to provide candidates for a specified opening and in alignment with our Inclusive Diversity values. Any employment agency, person or entity that submits an unsolicited resume does so with the understanding that A&M will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity.
Show more
Show less","Data Science, Analytics, AI, ML, Python, R, Pytorch, TensorFlow, Keras, Tableau, Power BI, Looker, SQL, Spark, Hadoop, Snowflake, Kafka, Graph Theory, Neo4J, TigerGraph, Neptune, Azure AutoML, SageMaker, DataBricks, DataRobot, H2O.ai, AWS, S3, EC2, Azure, Google Cloud, NLP, NLG, Computer Vision, Sentiment Analysis, R Shiny, Dash, Java, Matlab, SAS, C++","data science, analytics, ai, ml, python, r, pytorch, tensorflow, keras, tableau, power bi, looker, sql, spark, hadoop, snowflake, kafka, graph theory, neo4j, tigergraph, neptune, azure automl, sagemaker, databricks, datarobot, h2oai, aws, s3, ec2, azure, google cloud, nlp, nlg, computer vision, sentiment analysis, r shiny, dash, java, matlab, sas, c","ai, analytics, aws, azure, azure automl, c, computer vision, dash, data science, databricks, datarobot, ec2, google cloud, graph theory, h2oai, hadoop, java, kafka, keras, looker, matlab, ml, neo4j, neptune, nlg, nlp, powerbi, python, pytorch, r, r shiny, s3, sagemaker, sas, sentiment analysis, snowflake, spark, sql, tableau, tensorflow, tigergraph"
Data Governance Analyst - 3 days on-site - Temp position to start January 1.,"Career Developers, Inc.","New York, NY",https://www.linkedin.com/jobs/view/data-governance-analyst-3-days-on-site-temp-position-to-start-january-1-at-career-developers-inc-3767591185,2023-12-17,Irvington,United States,Associate,Onsite,"Refer a friend: Referral fee program.
Career Developers Inc.
, a well-established staffing agency/consulting firm, is celebrating 29 years in operation. Headquartered in West Palm Beach, FL, we offer comprehensive staffing services nationwide. With a portfolio of carefully chosen clients to represent, we ensure a productive partnership that exceeds most others. For our candidates, our commitment and goal lie in efficiently managing your expectations through business intelligence, spending time for interview preparations, providing open communication, and delivering exceptional feedback throughout the process. We look forward to helping advance your career!
Data Governance Analyst
(1+ year Assignment with the Right to Hire)
Location: New York, NY - - 3 days a week on site.
Rate: 50-55hr (Green Card or USC is required for this role)
PLEASE NOTE
: This would be for a January 1, 2024 start
3-5 years of experience within Data Governance or any type of Data Analyst role, with the desire to learn Data Governance.
Bachelor's degree or equivalent years of experience. Professional experience will be considered in lieu of education.
People Skills
Excellent interpersonal skills and the ability to successfully interact with a variety of stakeholders, from senior management to hands-on specialists.
A patient & adaptable approach to training colleagues of varying experience levels.
Confidence in raising opportunities for improvement to management for action.
Takes critical feedback in stride and adjusts approaches accordingly.
Acts as a point of help and excellent customer service.
Demonstrates willingness to learn and adapt quickly.
Technical Skills
Expertise in applying critical thinking skills to distill clarity from incomplete or conflicting information (e.g. root cause analysis, process auditing).
Proficiency with writing SQL queries for data analysis purposes in different databases (e.g. ANSI SQL, Microsoft SQL Server (MSSQL, T-SQL), Oracle (PL SQL), Postgres (PostgreSQL)).
Strong understanding of database design and data structures (e.g., normalization, Metamodel, Conceptual Data Model (CDM), Logical Data Model (LDM), and Physical Data Model (PDM)).
Experience in using Microsoft Visio or similar tools for creating process flow documentation.
Experience with Microsoft PowerPoint or other storytelling tools to create succinct and compelling presentations.
Experience in using Microsoft Excel for analysis and reporting, including formulas, Pivot tables, and charts.
Demonstrated experience with visualization and/or reporting tools (e.g., Qlik Sense, Qlikview, Tableau, Crystal Reports, SAP Web Intelligence (WebI))
Familiarity with Data Governance Tools (e.g., Alation, Atlan, Collibra, Informatica)
Familiarity with data modeling tools (e.g. Idera ER/Studio, SAP PowerDesigner, erwin)
Familiarity with Agile (e.g., scrum, Kanban) approach
Non-technical Skills
Demonstrated strength in translating technical complexity to a business audience and vice versa
High attention to detail and self-driven pursuit of accuracy
Ability to proactively solve problems and seek to own the resolution of issues throughout their lifecycle
Proven ability to manage assigned tasks and deliverables to a successful conclusion in a timely manner
Familiarity with Privacy laws and regulations (e.g. CCPA, GDPR)
SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst SQL Data Governance Analyst
Show more
Show less","SQL, Database Design, Data structures, Microsoft Visio, Microsoft PowerPoint, Microsoft Excel, Qlik Sense, Qlikview, Tableau, Crystal Reports, SAP Web Intelligence (WebI), Alation, Atlan, Collibra, Informatica, Idera ER/Studio, SAP PowerDesigner, erwin, Agile, scrum, Kanban, CCPA, GDPR","sql, database design, data structures, microsoft visio, microsoft powerpoint, microsoft excel, qlik sense, qlikview, tableau, crystal reports, sap web intelligence webi, alation, atlan, collibra, informatica, idera erstudio, sap powerdesigner, erwin, agile, scrum, kanban, ccpa, gdpr","agile, alation, atlan, ccpa, collibra, crystal reports, data structures, database design, erwin, gdpr, idera erstudio, informatica, kanban, microsoft excel, microsoft powerpoint, microsoft visio, qlik sense, qlikview, sap powerdesigner, sap web intelligence webi, scrum, sql, tableau"
"Director, Data Scientist - Biopharma",Pfizer,"Lake Forest, IL",https://www.linkedin.com/jobs/view/director-data-scientist-biopharma-at-pfizer-3729747089,2023-12-17,Libertyville,United States,Associate,Hybrid,"ROLE SUMMARY:
The Commercial Analytics team at Pfizer is looking for a Director, Data Science who is passionate about crafting and implementing predictive modeling and statistical analysis to build end-to-end solutions and insights that have a direct impact on patient's lives and the future of Pfizer as a data-driven organization. You will be a thought partner to the business, understand strategic goals, and then use your skills and subject matter expertise to surface impactful insights that drive business decisions and patient benefits. With colleagues across the globe, our rigorous analytical expertise is relied upon as the compass and decision support for the enterprise. Our dynamic, exciting team of subject-matter experts comes from diverse backgrounds and experiences, including market research, data science, digital analytics, finance, and consulting. Our culture is about getting things done iteratively and rapidly, with open feedback and debate along the way. We believe Data Science is a team sport, but we strive for independent decision-making and taking smart risks.
ROLE RESPONSIBILITIES:
This role is accountable for delivering data science driven support for an assigned brand and will partner with US Commercial, business, and digital teams, to develop and implement models, insights, and data products that drive brands’ strategic priorities. This is an individual contributor role.
Take deep dives in large-scale data to identify key insights that will shape future product/brand strategy for a specific therapeutic area.
Collaborate with cross-functional teams to identify new growth opportunities, develop data requirements, establish critical metrics, and evangelize data products.
Design, deploy, and evaluate experiments that help define opportunities for higher adoption, improved business performance, and better patient experience.
Conduct hypothesis-driven exploratory analyses, select appropriate ML algorithms, and build complex optimization engines to deliver impactful data solutions!
Research new technologies and methods across data science and data engineering to improve the technical capabilities of the team.
Communicate insights to senior management by distilling complex analysis and concepts into concise business-focused takeaways.
QUALIFICATIONS:
Bachelor’s degree with 10+ years of experience OR Masters Degree with 9+ years of experience OR PhD with 7+ years of experience
Degree preferably in engineering, economics, statistics, computer science, or related quantitative field.
Preferred experience in Applied Econometrics, Statistics, Data Mining, Machine Learning, Analytics, Mathematics, Operations Research, Industrial Engineering, or related field preferred.
Working knowledge of relational databases, including SQL, and large-scale distributed systems such as Hadoop and or working in Snowflake/Databricks
Ability to implement data science pipelines and applications in a general programming language such as Python, Scala, Java, or R.
Practical experience with and theoretical understanding of ML algorithms for classification, regression, clustering, and anomaly detection
Well versed with and have experience applying various statistical methodologies including Bayesian and non-parametric techniques, hypothesis testing, ANOVA, Regression, fixed and random effects etc. to measure the impact of experiments
Hands on experience working in big data environments such as Hadoop, Spark, and using Python / SQL or comparable languages for manipulating and analyzing complex clickstream and or unstructured data
Ability to extract significant business insights from data and identify the roots behind the patterns
Experience working with a data visualization tool/package, including Dash, Tableau, and Angular etc.
Communication skills for communicating complex quantitative analyses to senior business executives
Candidate demonstrates a breadth of diverse leadership experiences and capabilities including: the ability to influence and collaborate with peers, develop and coach others, oversee and guide the work of other colleagues to achieve meaningful outcomes and create business impact.
Other Job Details:
Last Date to Apply for Job: October 31, 2023
The annual base salary for this position ranges from $144,900.00 to $241,500.00.* In addition, this position is eligible for participation in Pfizer’s Global Performance Plan with a bonus target of 20.0% of the base salary and eligibility to participate in our share based long term incentive program. We offer comprehensive and generous benefits and programs to help our colleagues lead healthy lives and to support each of life’s moments. Benefits offered include a 401(k) plan with Pfizer Matching Contributions and an additional Pfizer Retirement Savings Contribution, paid vacation, holiday and personal days, paid caregiver/parental and medical leave, and health benefits to include medical, prescription drug, dental and vision coverage. Learn more at Pfizer Candidate Site – U.S. Benefits | (uscandidates.mypfizerbenefits.com). Pfizer compensation structures and benefit packages are aligned based on the location of hire. The United States salary range provided does not apply to Tampa, FL or any location outside of the United States.
The annual base salary for this position in Tampa, FL ranges from $130,400.00 to $217,300.00.
Relocation assistance may be available based on business needs and/or eligibility.
Sunshine Act
Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative.
EEO & Employment Eligibility
Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer.
Marketing and Market Research
Show more
Show less","Predictive modeling, Statistical analysis, Datadriven organization, Data science, Data mining, Machine learning, Analytics, Operations research, Industrial engineering, Relational databases, SQL, Hadoop, Snowflake, Databricks, Python, Scala, Java, R, ML algorithms, Bayesian, Nonparametric techniques, Hypothesis testing, ANOVA, Regression, Hadoop, Spark, Dash, Tableau, Angular","predictive modeling, statistical analysis, datadriven organization, data science, data mining, machine learning, analytics, operations research, industrial engineering, relational databases, sql, hadoop, snowflake, databricks, python, scala, java, r, ml algorithms, bayesian, nonparametric techniques, hypothesis testing, anova, regression, hadoop, spark, dash, tableau, angular","analytics, angular, anova, bayesian, dash, data mining, data science, databricks, datadriven organization, hadoop, hypothesis testing, industrial engineering, java, machine learning, ml algorithms, nonparametric techniques, operations research, predictive modeling, python, r, regression, relational databases, scala, snowflake, spark, sql, statistical analysis, tableau"
Staff Data Engineer,Recruiting from Scratch,"Evanston, IL",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744388815,2023-12-17,Libertyville,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Agile Engineering Practices, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Management Tools, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering practices, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention","agile engineering practices, airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Northfield, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742679043,2023-12-17,Libertyville,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","ETL, MLOps, Python, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Apache Beam, Kafka, SQL, NoSQL, Git, Kubernetes, Docker, Linux, Cloud Computing, Machine Learning, Data Analytics, Artificial Intelligence, NLP, Computer Vision","etl, mlops, python, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, apache beam, kafka, sql, nosql, git, kubernetes, docker, linux, cloud computing, machine learning, data analytics, artificial intelligence, nlp, computer vision","apache beam, artificial intelligence, aws, azure, cicd, cloud computing, computer vision, dataanalytics, devops, docker, etl, gcp, git, iac, kafka, kubernetes, linux, machine learning, mlops, nlp, nosql, python, scala, spark, sql"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Glenview, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742677176,2023-12-17,Libertyville,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","ETL, ELT, Python, Machine Learning, MLOps, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Apache Beam, Kafka","etl, elt, python, machine learning, mlops, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, apache beam, kafka","apache beam, aws, azure, cicd, devops, elt, etl, gcp, iac, kafka, machine learning, mlops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Winnetka, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742673588,2023-12-17,Libertyville,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","ETL, ML Ops, Python, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Apache Beam, Kafka, Machine Learning, AI/ML","etl, ml ops, python, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, apache beam, kafka, machine learning, aiml","aiml, apache beam, aws, azure, cicd, devops, etl, gcp, iac, kafka, machine learning, ml ops, python, scala, spark"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Evanston, IL",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709524,2023-12-17,Libertyville,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Machine learning, Data mining, Data cleaning, Data modeling, Data visualization, Pandas, R, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, Spark Streaming, Applied machine learning, Legal compliance, Data management tools, Data classification, Data retention","data engineering, machine learning, data mining, data cleaning, data modeling, data visualization, pandas, r, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, spark streaming, applied machine learning, legal compliance, data management tools, data classification, data retention","airflow, applied machine learning, aws, azure, bash, data classification, data cleaning, data engineering, data management tools, data mining, data retention, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, legal compliance, machine learning, nlp, pandas, python, r, snowflake, spark, spark streaming, sql, storm, visualization"
Analyst - Advanced data analytics,BBA Consultants,"Mont-St-Hilaire, Quebec, Canada",https://ca.linkedin.com/jobs/view/analyst-advanced-data-analytics-at-bba-consultants-3717085461,2023-12-17,Beloeil, Canada,Associate,Hybrid,"Type of position:
Regular
Looking to build your career with us?
Join BBA and become part of a team of talented people who have a passion for success and a willingness to push themselves to offer their clients the best. You’ll also have the chance to work on stimulating and instructive projects. You’ll be coached by experienced people who are committed to developing their colleagues and happy to share their knowledge.
BBA is a stimulating, friendly, fun and inclusive place to work. We focus on building complementary teams and provide employees with a work environment that fosters personal and professional development.
With us, you’ll have the opportunity to...
Design and develop advanced data analysis and artificial intelligence (AI) projects for our industrial and manufacturing clients .
Support our clients in leveraging their data by proposing innovative solutions based on concepts such as machine learning and predictive maintenance.
Have an impact on tomorrow’s companies by participating in their digital transformation.
Work on a stimulating team that pushes you to excel and learn more.
Lead project teams as a technical leader using your experience and skills.
Develop a network of contacts and practical experience in the field while serving as a client representative.
Participate in managing and implementing utility systems and software as well as other industrial software packages.
Ensure the quality of professional services and maintain good client relationships.
Do your qualities and values match our corporate culture?
Action-oriented
Strong communication and soft skills
Resourceful and ingenious
Creative
Rigorous
Team player
Profile And Essential Skills
Bachelor's degree in applied mathematics, computer science or another field relevant to advanced data analysis
Bilingual (English and French)
Good knowledge of data management (warehousing, virtualization, modelling, extraction and manipulation of real-time data flows)
Good knowledge of the various data analysis tools (machine learning, Python and R languages)
Strong ability to analyze and synthesize
Ability to absorb large amounts of information and prioritize actions
Ability to influence and political acumen
Good knowledge of the main AI players and up-to-date knowledge of current market trends
Experience in AI, machine learning, natural language processing or other related fields
Experience in coaching and developing machine learning models for real-time processing in production environments
Experience with a variety of learning algorithms, e.g., supervised, unsupervised and reinforcement learning and different variants of neural network models
Soft skills in managing projects and technical teams
Assets
Master's degree or PhD in statistics, applied mathematics, computer science or another field relevant to advanced data analysis
Knowledge of SCADA systems and industrial communication protocols
Knowledge of industrial data historian systems (PI or other systems)
Basic knowledge of software programming and relational databases
With Us, You’ll Have Many Benefits
A dynamic engagement committee with something for everyone
Access to many experts and mentoring
A collaborative team that shares its ideas and knowledge
Custom training and development plans
A flexible schedule that allows you to work at the office and from home
A friendly environment where everyone’s ideas are listened to and where there is no sense of hierarchy
Friendly, eco-mindful and high-tech workspaces
About BBA
For over 40 years, we’ve been pushing the boundaries of engineering to serve our clients in the Energy and Natural Resources sector. At BBA, we develop innovative, flexible and sustainable solutions, from strategy to execution.
BBA is one of Canada’s leading private consulting engineering firms, focused on practice (PCB), with a network of 15 offices. Our teams work closely to deliver projects that shape tomorrow’s industry at the local, national and international levels.
Our people’s passion and excellence make BBA a Canada’s Best Employer and one of Canada’s Best Managed Companies .
We have unique talents, and we’re committed to providing a workplace where you can be yourself . Job postings are written in gender-neutral language.
Learn more about us by visiting our LinkedIn page and join the networks of our talent attraction specialists to speak with them directly.
Show more
Show less","Data analysis, Artificial intelligence, Machine learning, Predictive maintenance, Data management, Warehousing, Virtualization, Modelling, Data extraction, Data manipulation, Python, R, Supervised learning, Unsupervised learning, Reinforcement learning, Neural network models, SCADA systems, Industrial communication protocols, Industrial data historian systems, Software programming, Relational databases","data analysis, artificial intelligence, machine learning, predictive maintenance, data management, warehousing, virtualization, modelling, data extraction, data manipulation, python, r, supervised learning, unsupervised learning, reinforcement learning, neural network models, scada systems, industrial communication protocols, industrial data historian systems, software programming, relational databases","artificial intelligence, data extraction, data management, data manipulation, dataanalytics, datawarehouse, industrial communication protocols, industrial data historian systems, machine learning, modelling, neural network models, predictive maintenance, python, r, reinforcement learning, relational databases, scada systems, software programming, supervised learning, unsupervised learning, virtualization"
Analyst - Advanced data analytics,BBA Consultants,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/analyst-advanced-data-analytics-at-bba-consultants-3717081904,2023-12-17,Beloeil, Canada,Associate,Hybrid,"Type of position:
Regular
Looking to build your career with us?
Join BBA and become part of a team of talented people who have a passion for success and a willingness to push themselves to offer their clients the best. You’ll also have the chance to work on stimulating and instructive projects. You’ll be coached by experienced people who are committed to developing their colleagues and happy to share their knowledge.
BBA is a stimulating, friendly, fun and inclusive place to work. We focus on building complementary teams and provide employees with a work environment that fosters personal and professional development.
With us, you’ll have the opportunity to...
Design and develop advanced data analysis and artificial intelligence (AI) projects for our industrial and manufacturing clients .
Support our clients in leveraging their data by proposing innovative solutions based on concepts such as machine learning and predictive maintenance.
Have an impact on tomorrow’s companies by participating in their digital transformation.
Work on a stimulating team that pushes you to excel and learn more.
Lead project teams as a technical leader using your experience and skills.
Develop a network of contacts and practical experience in the field while serving as a client representative.
Participate in managing and implementing utility systems and software as well as other industrial software packages.
Ensure the quality of professional services and maintain good client relationships.
Do your qualities and values match our corporate culture?
Action-oriented
Strong communication and soft skills
Resourceful and ingenious
Creative
Rigorous
Team player
Profile And Essential Skills
Bachelor's degree in applied mathematics, computer science or another field relevant to advanced data analysis
Bilingual (English and French)
Good knowledge of data management (warehousing, virtualization, modelling, extraction and manipulation of real-time data flows)
Good knowledge of the various data analysis tools (machine learning, Python and R languages)
Strong ability to analyze and synthesize
Ability to absorb large amounts of information and prioritize actions
Ability to influence and political acumen
Good knowledge of the main AI players and up-to-date knowledge of current market trends
Experience in AI, machine learning, natural language processing or other related fields
Experience in coaching and developing machine learning models for real-time processing in production environments
Experience with a variety of learning algorithms, e.g., supervised, unsupervised and reinforcement learning and different variants of neural network models
Soft skills in managing projects and technical teams
Assets
Master's degree or PhD in statistics, applied mathematics, computer science or another field relevant to advanced data analysis
Knowledge of SCADA systems and industrial communication protocols
Knowledge of industrial data historian systems (PI or other systems)
Basic knowledge of software programming and relational databases
With Us, You’ll Have Many Benefits
A dynamic engagement committee with something for everyone
Access to many experts and mentoring
A collaborative team that shares its ideas and knowledge
Custom training and development plans
A flexible schedule that allows you to work at the office and from home
A friendly environment where everyone’s ideas are listened to and where there is no sense of hierarchy
Friendly, eco-mindful and high-tech workspaces
About BBA
For over 40 years, we’ve been pushing the boundaries of engineering to serve our clients in the Energy and Natural Resources sector. At BBA, we develop innovative, flexible and sustainable solutions, from strategy to execution.
BBA is one of Canada’s leading private consulting engineering firms, focused on practice (PCB), with a network of 15 offices. Our teams work closely to deliver projects that shape tomorrow’s industry at the local, national and international levels.
Our people’s passion and excellence make BBA a Canada’s Best Employer and one of Canada’s Best Managed Companies .
We have unique talents, and we’re committed to providing a workplace where you can be yourself . Job postings are written in gender-neutral language.
Learn more about us by visiting our LinkedIn page and join the networks of our talent attraction specialists to speak with them directly.
Show more
Show less","Data analysis, Artificial intelligence, Machine learning, Predictive maintenance, Data management, Warehousing, Virtualization, Modeling, Extraction, Manipulation, Data analysis tools, Python, R languages, SCADA systems, Industrial communication protocols, Industrial data historian systems, Software programming, Relational databases","data analysis, artificial intelligence, machine learning, predictive maintenance, data management, warehousing, virtualization, modeling, extraction, manipulation, data analysis tools, python, r languages, scada systems, industrial communication protocols, industrial data historian systems, software programming, relational databases","artificial intelligence, data analysis tools, data management, dataanalytics, datawarehouse, extraction, industrial communication protocols, industrial data historian systems, machine learning, manipulation, modeling, predictive maintenance, python, r languages, relational databases, scada systems, software programming, virtualization"
Engineering Project Data Analyst,CMC Electronics,"Montréal-Ouest, Quebec, Canada",https://ca.linkedin.com/jobs/view/engineering-project-data-analyst-at-cmc-electronics-3782805082,2023-12-17,Beloeil, Canada,Associate,Hybrid,"CMC is a company recognized worldwide for its innovative spirit and excellence in the design and manufacture of advanced avionics solutions for the civil and military aviation markets.
This is a unique opportunity to be part of the CMC Electronics family. Use your knowledge and expertise to contribute to an experienced and motivated team mastering the latest design technologies.
Why join our team?
Reporting to the Team Lead, Analytics & Project Governance, the Project Data Analyst will collaborate with Engineering to make data-driven decisions by providing them with the analytical tools and resources required.
The successful candidate will collaborate with cross-functional teams to ensure effective project governance, gather, analyze, and interpret project data, providing valuable insights that will optimize engineering project management.
CMC supports a hybrid work model, working in the office 3 days a week. If you are passionate about data analysis, project management, enjoy working in a collaborative environment, and are eager to contribute to the success of an evolving company, we would love to hear from you.
As an Engineering Project Data Analyst your day-to-day responsibilities will include:
Gather, compile and analyze data from various sources to identify trends, and opportunities to steer project decisions
Implement KPIs on project performance
Build and adapt visualization tools to provide insights on program status
Support demand management across Engineering
Establish engineering project best practices
Support continuous improvement and change management initiatives as it relates to Engineering project execution
To Join Our Team, You Should Have
Bachelor degree in a related field (e.g., Data Science, Engineering, Business Administration) or a combination of relevant education and experience
Minimum three (3) years in a similar role
Hands-on with experience working with various data analytics tools (SQL, Power BI, Tableau, or other). Strong Excel (Power Query)
Ability to communicate at different levels of the organization (engineering and executives)
Expertise in modernizing and optimizing data analytics
Well versed in Agile and JIRA capabilities
Aerospace industry (asset)
Ability to work with a multi stakeholder team
Primavera or other project management tools
Carrier At CMC Electronics Offers
An opportunity to work in the aeronautics industry with avionics enthusiasts;
Organization of international caliber on a human scale;
Stimulating multi-disciplined environment;
Career advancement opportunities;
Management team engaged in your success;
Competitive global compensation (group insurance program paid 100% by CMC Electronics and a group pension plan with employer match);
Financial education assistance program;
Free parking and electric car charging stations available;
On-site cafeteria at low cost;
Closed between Christmas and New Year's Day;
Social activities
CMC is committed to employment equity and to the equal access program. CMC encourages members of designated groups to apply for jobs. Please note that the use of the masculine gender is only intended to lighten the text.
Show more
Show less","Data Analysis, Project Management, Data Visualization, Business Administration, Engineering, Data Science, SQL, Power BI, Tableau, Excel, Power Query, Agile, JIRA, Primavera, Stakeholder Management, Avionics","data analysis, project management, data visualization, business administration, engineering, data science, sql, power bi, tableau, excel, power query, agile, jira, primavera, stakeholder management, avionics","agile, avionics, business administration, data science, dataanalytics, engineering, excel, jira, power query, powerbi, primavera, project management, sql, stakeholder management, tableau, visualization"
Senior Data Engineer,Lyft,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-lyft-3674205251,2023-12-17,Beloeil, Canada,Mid senior,Onsite,"TBS - Data Engineering
At Lyft, our mission is to improve people’s lives with the world’s best transportation. To do this, we start with our own community by creating an open, inclusive, and diverse organization.
Here at Lyft, Data is the only way we make decisions. It is the core of our business, helping us create a transportation experience for our customers and providing insights into the effectiveness of our product launch & features.
This team focuses on supporting our business by building the data transport, collection, and storage that powers our Transit, Bikes, and Scooters business. We are looking for a Data Engineer to build scalable solutions, leveraging their data expertise and our technology stack to provide timely, accurate data for our internal and external customers. You will have the opportunity to be a member of a new, growing part of our micro mobility platform. This role will involve collaborating with product managers, external stakeholders, GMs, engineers, and data scientists to gather and translate requirements into solutions, ensuring that data-driven decisions are at the core of our business.
This role reports to the Engineering Manager
Responsibilities:
Owner of core company data pipelines, responsible for scaling up data processing flow to meet the rapid data growth at Lyft
Evolve data model and data schema based on business and engineering needs
Implement systems tracking data quality and consistency
Develop tools supporting self-service data pipeline management (ETL)
SQL and MapReduce job tuning to improve data processing performance
Write well-crafted, well-tested, readable, maintainable code
Participate in code reviews to ensure code quality and distribute knowledge
Unblock, support and communicate with internal & external partners to achieve results
Experience:
5+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
Good understanding of SQL Engine and able to conduct advanced performance tuning
Strong skills in scripting language (Python, Ruby, Bash)
2+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)
Comfortable working directly with data analytics to bridge Lyft's business goals with data engineering
Benefits:
Extended health and dental coverage options, along with life insurance and disability benefits
Mental health benefits
In addition to provincial observed holidays, team members get 15 days paid time off, with an additional day for each year of service
4 Floating Holidays each calendar year prorated based off of date of hire
10 paid sick days per year regardless of province
Family building benefits
Access to a Health Care Savings Account
18 weeks of paid parental leave. Biological, adoptive, and foster parents are all eligible
Pre-tax commuter benefits
Lyft proudly pursues and hires a diverse workforce. Lyft believes that every person has a right to equal employment opportunities without discrimination because of race, ancestry, place of origin, colour, ethnic origin, citizenship, creed, sex, sexual orientation, gender identity, gender expression, age, marital status, family status, disability, pardoned record of offences, or any other basis protected by applicable law or by Company policy. Lyft also strives for a healthy and safe workplace and strictly prohibits harassment of any kind. Accommodation for persons with disabilities will be provided upon request in accordance with applicable law during the application and hiring process. Please contact your recruiter now if you wish to make such a request.
Starting in September 2023, this role will be in-office on a hybrid schedule — Team Members will be expected to work in the office 3 days per week on Mondays, Thursdays and a team-specific third day. Additionally, hybrid roles have the flexibility to work from anywhere for up to 4 weeks per year.
Show more
Show less","Data Engineering, Hadoop Ecosystem, MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet, SQL, MySQL, PostgreSQL, SQL Server, Oracle, Python, Ruby, Bash, Airflow, Oozie, Azkaban, UC4","data engineering, hadoop ecosystem, mapreduce, yarn, hdfs, hive, spark, presto, pig, hbase, parquet, sql, mysql, postgresql, sql server, oracle, python, ruby, bash, airflow, oozie, azkaban, uc4","airflow, azkaban, bash, data engineering, hadoop ecosystem, hbase, hdfs, hive, mapreduce, mysql, oozie, oracle, parquet, pig, postgresql, presto, python, ruby, spark, sql, sql server, uc4, yarn"
Data Engineer,DRW,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-drw-3667387067,2023-12-17,Beloeil, Canada,Mid senior,Onsite,"DRW
is a diversified trading firm with over 3 decades of experience bringing sophisticated technology and exceptional people together to operate in markets around the world. We value autonomy and the ability to quickly pivot to capture opportunities, so we operate using our own capital and trading at our own risk.
Headquartered in Chicago with offices throughout the U.S., Canada, Europe, and Asia, we trade a variety of asset classes including Fixed Income, ETFs, Equities, FX, Commodities and Energy across all major global markets. We have also leveraged our expertise and technology to expand into three non-traditional strategies: real estate, venture capital and cryptoassets.
We operate with respect, curiosity and open minds. The people who thrive here share our belief that it’s not just what we do that matters–it's how we do it. DRW is a place of high expectations, integrity, innovation and a willingness to challenge consensus.
Our team works on a variety of high-impact machine learning projects across DRW's businesses, and we are looking for a
Data Engineer
to join us. We work with very large amounts of data and in this role, you will build critical systems to manage and optimize our data workflows. The output of your work will be directly used in all our machine learning experiments and production models and you will have the chance to make great impact to our business. As a member of our team, you will be surrounded by cutting-edge technology and experienced machine learning, data and software professionals providing you with the best possible environment to succeed. You will always have interesting new projects to apply your skillset while also becoming an expert on certain areas.
Responsibilities
Manage large scale data projects, from design to implementation
Create and manage ETL and data pipelines
Continuously manage, clean and validate historical and live data for downstream ML tasks
Combine large datasets from a variety of sources
Design, build and maintain dashboards to visualize historical and live data
Build scalable and performant software using software engineering best practices
Drive data projects and lead new initiatives
Collaborate with stakeholders from different departments
Qualifications
A bachelor’s or a master’s degree in computer science, software engineering or equivalent
3+ years of experience building software systems
Proven track record of working with large datasets (ingesting, cleaning, preprocessing, analyzing and validating data)
Excellent programming and scripting skills (Python, SQL, Go or equivalent)
Experience working with timeseries structured data or streaming data
Experience with relational databases, time series databases
Strong understanding of software engineering principles and best practices
Strong problem-solving and analytical skills
Excellent communication and collaboration skills
Experience with any of these technologies is an asset: Redis, RabbitMQ, GraphQL, Kafka, ELK, Docker, AMQP
What DRW Montreal Has To Offer You
Recognized as one of Canada’s Best Employers for the past 8 years
Committed to continuous learning & development
Industry leading benefits package and perks
Focused on employee well-being and work-life balance
Community initiatives, volunteer program and opportunities for giving back
Find out more about all of our perks & benefits at: https://drw.com/work-at-drw/montreal-benefits
If you are an experienced software developer who is passionate about data and enjoys working with cutting-edge technologies and solving complex problems, we encourage you to apply for this exciting position. You will have the opportunity to work with a talented and dedicated team to create innovative solutions that can make a significant impact to our business.
For more information about DRW's processing activities and our use of job applicants' data, please view our Privacy Notice at https://drw.com/privacy-notice.
California residents, please review the California Privacy Notice for information about certain legal rights at https://drw.com/california-privacy-notice.
Show more
Show less","Python, SQL, Go, Redis, RabbitMQ, GraphQL, Kafka, ELK, Docker, AMQP, ETL, Software Engineering, Data Pipelines, Data Visualization, Performance Tuning, ProblemSolving, Analytical Skills, Communication, Collaboration, Relational Databases, Time Series Databases, Data Preprocessing, Data Validation","python, sql, go, redis, rabbitmq, graphql, kafka, elk, docker, amqp, etl, software engineering, data pipelines, data visualization, performance tuning, problemsolving, analytical skills, communication, collaboration, relational databases, time series databases, data preprocessing, data validation","amqp, analytical skills, collaboration, communication, data preprocessing, data validation, datapipeline, docker, elk, etl, go, graphql, kafka, performance tuning, problemsolving, python, rabbitmq, redis, relational databases, software engineering, sql, time series databases, visualization"
Senior Data Engineer - SF/NY/Montreal,CoinList,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-sf-ny-montreal-at-coinlist-3736041909,2023-12-17,Beloeil, Canada,Mid senior,Onsite,"CoinList is where early adopters invest in and trade the best digital assets, and is now one of the fastest growing crypto financial platforms. We support the full lifecycle of crypto investment, from token sales through token distribution, trading, lending, and crypto-specific services such as staking and access to decentralized-finance opportunities. CoinList users trade and store Bitcoin, Ether, Filecoin, and many other popular crypto assets through our full-service exchange, CoinList Pro and CoinList.co, while also getting access to the best new tokens before they list on other exchanges. We are the blue chip platform for blockchain companies, helping Filecoin, Blockstack, Solana, Celo, Algorand, Dapper, and others raise over $1.1 Billion. Our customer base includes validators, miners, founders, CEOs, crypto funds, bitcoin OGs, as well as a broad array of crypto enthusiasts. We are backed by top-tier investors, have offices in San Francisco, New York, Montreal and are just getting started.
As an engineer at CoinList you'll find that your days are filled with all sorts of fascinating challenges -- architecting a high performance exchange, to simplifying the complex to help bring crypto to a mainstream audience, to developing smart contracts. Our work is difficult and ambitious, which is what makes it fun.
Specifically, we are looking for a Senior engineer with 5+ years of experience to work with our Data team and the system that powers our analytics and some user-facing products.
Our tech stack (!= requirements)
Prefect
DBT
AWS
Terraform
Codebases in Python, Rust, SQL
User-facing product in Rust, PostgreSQL
If you are an entrepreneurial person with exceptional engineering talent, we'd love to hear from you!
Who you are:
You like to ship high quality code. You make deadlines for yourself and you move fast. You focus on the things that matter and push back on things that don't. People know they can count on you to get things done.
You love to code. You build stuff for fun on nights and weekends. You contribute to open source. You have deep expertise in certain areas but you consider yourself a generalist and you enjoy learning new platforms.
You're really smart. You were a top student in your computer science program or have been teaching yourself programming from an early age. You like being around people who challenge you.
You're curious. You want to understand how things work. You value interesting things, especially outside your discipline. You like teaching others and constantly learning. You read and question things.
You're scrappy and entrepreneurial. You've built apps for fun and worked on side projects before. You're interested in all aspects of startups - from user testing, to marketing, to raising capital. If you haven't already started your own company, you think you might like to in the future.
You care about the product. You might not be a product expert, but you value good design. You enjoy thinking about why certain products work and discussing new apps. You want to build delightful products that users love and that make people's lives more meaningful.
You have good values. You enjoy hanging out with your teammates, make the workplace more fun for everyone, and care about doing the right thing.
You are a 10x thinker, far more excited by huge disruptions than by small improvements. If something is hard, that makes it more fun, and you know that if anyone can figure it out, you can.
What you will do:
You'll contribute in all areas. You might choose to go deep in web development or building open source dev tools to automate our operations, but like your teammates you'll make a habit of working across the stack, frequently picking up new languages and platforms as needed.
You'll ship product. From hack days, to experimental prototypes, to developing a new type of auction model, you'll constantly be building and shipping product. You'll write clean, well-documented code, do unit tests, follow best practices, and have the benefit of working in a place where people move fast.
You'll rapidly grow and improve. At CoinList you'll be working with some of the smartest engineers you've ever met, who love to learn and teach each other. You'll find yourself stretching to new areas, immersed in feedback, and engrossed in deep conversations on a daily basis.
You'll help shape the product. Our whole team cares deeply about the product and has significant input into its development. You'll beta test new prototypes and talk directly with users, and your ideas will always be heard.
You'll learn about startups. From fundraising, to recruiting, to negotiating an office lease, you'll learn everything there is to know about growing and running a startup of your own, and we'll do everything we can to connect you with mentors and advisors to help you with your career for years to come.
You'll be an owner. We believe in hiring smart people and giving them as much responsibility as they can handle. Whether it's running a critical token sale or leading our work on a new platform, we'll make sure you are always pushing yourself to new levels.
You'll create the future. Crypto is a far bigger deal than most people realize, and at CoinList you will be be at the forefront of it. There are all sorts of technical challenges you'll be working though and new questions you'll have to answer, in partnership with founders for the leading crypto projects across the globe. If you're successful, you'll build something the world has never before seen.
Location & Hybrid Office Setup:
This role is targeted in one of our three area hubs where teammates have a hybrid schedule of 2-3 days in the office and the other days at home.
San Francisco, New York, Montreal
As an early employee at CoinList you will be a critical part of our core team and have a huge influence over the direction of the company. We will compensate you well, invest deeply in your development, and do everything we can to make sure this is the single best work experience of your life.
At CoinList, we are proud to be an Equal Opportunity Employer. We celebrate diversity, value our differences, and are committed to creating an inclusive environment for all employees.
Base salary range: 175 - 215k + equity + bonus. We are open to a range of background and experience for this senior role.
Show more
Show less","Python, SQL, Rust, DevOps, DBT, PostgreSQL, Terraform, AWS, Prefect","python, sql, rust, devops, dbt, postgresql, terraform, aws, prefect","aws, dbt, devops, postgresql, prefect, python, rust, sql, terraform"
Data Engineer #: 23-05889,HireTalent - Diversity Staffing & Recruiting Firm,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-engineer-%23-23-05889-at-hiretalent-diversity-staffing-recruiting-firm-3764247374,2023-12-17,Beloeil, Canada,Mid senior,Onsite,"Job Description
Roles and responsibilities:
The Data science engineer role is critical to focus on below areas:
Work on the enhancement requests on application features from business users.
Work on voice surveillance side of the project to monitor various voice channels.
Perform transformation of incoming data for surveillance.
Work on ways to reduce false-positive alerts in the application.
Help troubleshoot production incidents in timely manner.
Develop and enhance Data science models to reduce noise from alerts.
Attend regular meetings and interact with internal and external teams.
Work on multiple tasks and respect aggressive schedule.
Gather and document technical requirements and specifications.
Work in a fast paced environment. Support may include extra hours, nights and weekends.
Required Skills include:
Experience with Big Data technologies including large data stores like Hadoop, Elastic, Apache Solr, Lucene and comparable stacks.
Advanced knowledge of Python and JSON.
Knowledge of Docker containerized application.
Advanced knowledge of SQL systems, with experience developing in one of these databases, Oracle, Microsoft SQL Server, PostgreSQL, and MySQL.
Strong UNIX skills.
Advanced knowledge or experience with any of the NoSQL systems such as ElasticSearch, CouchDB, MongoDB, HBase, MarkLogic, Neo4J, Redis.
Expertise in CICD, Bit bucket, Artifactory, ansible, Jenkins and sonar cube.
Experience with JAVA, J2EE, JavaScript, Perl.
Experience with Machine Learning.
Experience with all project lifecycle stages – Requirements Gathering to Post Implementation.
Experienced as a team player working in a global team.
Ability to manage conflicting requests on time in a continually fast moving environment.
Must be a self-starter with attention to detail.
Strong communications (written and oral) skills in a front-office setting.
Preferred Qualifications
Bachelor's degree in computer science, math, statistics, or other related field.
Experience with Front Office traders, IT support and Operation teams in a Capital Markets domain.
Experience with voice data transcription, silence removal, AI is a plus.
Knowledge of Docker orchestration tools like Kubernetes etc.
Knowledge of Cloud Computing (AWS, Azure, Google).
Show more
Show less","Data Engineering, Data Analysis, Data Transformation, Machine Learning, Python, JSON, Docker, SQL, NoSQL, Java, J2EE, JavaScript, Perl, CICD, Bitbucket, Artifactory, Ansible, Jenkins, SonarQube","data engineering, data analysis, data transformation, machine learning, python, json, docker, sql, nosql, java, j2ee, javascript, perl, cicd, bitbucket, artifactory, ansible, jenkins, sonarqube","ansible, artifactory, bitbucket, cicd, data engineering, data transformation, dataanalytics, docker, j2ee, java, javascript, jenkins, json, machine learning, nosql, perl, python, sonarqube, sql"
"Network Data Engineer - L3 at Montreal, Qubec (Onsite) -- Fulltime Employment",Lorven Technologies Inc.,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/network-data-engineer-l3-at-montreal-qubec-onsite-fulltime-employment-at-lorven-technologies-inc-3607486685,2023-12-17,Beloeil, Canada,Mid senior,Onsite,"Job Title: Network Data Engineer - L3
Location: Montreal, Qubec (Onsite)
Duration: Fulltime Employment
Cisco ACI, DNAC, Routing & Switching, Nexus, Cisco Wireless, NetBrain
Job Description
Monitor day-to-day operations and management for customer Network infrastructure.
Coordinate with operations team and local site person on day-to-day activity
Configuring, Troubleshooting & Maintaining DC LAN and DC Interconnect WAN Networks.
Manage and Maintain IP addressing schemes.
Network protocol configuration, maintenance, and troubleshooting
Provide inputs for Capacity Planning/ Migration/ Optimization / Future upgrade strategy based on the performance trend
Regular IOS Updating and hotfixes
Responsible for implementing best practice/hardening
Managing and monitoring of log files of network devices
Preparation of Daily/Weekly/Monthly reports
Regular audit of network devices and take corrective action accordingly
Performance Management & Optimization
Root Cause Analysis
Good knowledge in vendor management
Coordination with multiple vendors
Handling escalated incidents or high critical tickets.
Plan and schedule for change management.
Develop and maintain architecture diagrams and documents
Good Communication skills.
Required Skills
Hands-on experience required on the following technologies:
Cisco ACI, DNAC, VXLAN
Routing protocols - EIGRP, OSPF, BGP, PBR
LAN Switching - VLAN, VTP, STP, HSRP, GLBP,
Nexus Switches - VDC, VPC, FEX (2K, 5K, 7K,9K)
TCP/IP protocol suite, Quality of Service
Cisco Wireless (WLC) and WAP
NetBrain
Show more
Show less","Cisco ACI, DNAC, VXLAN, Routing protocols, LAN Switching, Nexus Switches, TCP/IP protocol suite, Quality of Service, Cisco Wireless (WLC) and WAP, NetBrain","cisco aci, dnac, vxlan, routing protocols, lan switching, nexus switches, tcpip protocol suite, quality of service, cisco wireless wlc and wap, netbrain","cisco aci, cisco wireless wlc and wap, dnac, lan switching, netbrain, nexus switches, quality of service, routing protocols, tcpip protocol suite, vxlan"
Data Analyst Part Time,Voxmediallc,"Brossard, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-voxmediallc-3757206498,2023-12-17,Beloeil, Canada,Mid senior,Onsite,"Summary:
Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Data Visualization, SQL, R, Python, Analytical Thinking, Problem Solving, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL Processes","data analysis, statistical techniques, data visualization, sql, r, python, analytical thinking, problem solving, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl processes","ab testing, analytical thinking, data management, dataanalytics, etl, hypothesis testing, powerbi, problem solving, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Data Analyst - Payroll & HR,American Iron & Metal (AIM),"Montréal-Est, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-analyst-payroll-hr-at-american-iron-metal-aim-3759318688,2023-12-17,Beloeil, Canada,Mid senior,Onsite,"Company Description
American Iron & Metal (AIM) is a family-owned company and recognized global leader in the metal recycling industry with more than 125 sites and 4000 employees worldwide. We have continued to prosper for the last eight decades thanks to the dedication of our employees and the ongoing trust and support of our customers.
Become part of team AIM, a growing team with an entrepreneurial spirit who has over the years evolved into a successful and multifaceted company with business divisions that include metal recycling, decommissioning and demolition, auto-parts sales and recycling, manufacturing of solder assemblies, construction waste recycling, and production of customized industrial and mining products.
We take pride in doing good things for the environment to help create a greener, more sustainable future for all.
It’s simple; we do it right. We AIM for excellence.
Job Description
Reporting to the Payroll Supervisor, the Data Analyst will be strategically positioned within the larger HR function. In a context of system evolution and integration, this addition to the team will act as a resource person for all AIM entities in Canada. Your main responsibilities will be :
Collaborate with HR teams to identify and understand their reporting needs and develop solutions to meet those needs.
Create and analyze various reports and monitoring tools to strategically support various internal customers, such as payroll and human resources.
Create interface tools (APIs) to integrate data.
Create and work on complex files as an Excel expert - macros/VBA.
Participate in the implementation and integration of new payroll and HR systems.
Create procedures and reports to ensure the accuracy and integrity of HR data.
Manage HR & Payroll SharePoints and act as a trainer for users.
Create and maintain database.
Participate in various projects and initiatives related to HR systems and processes.
Qualifications
Bachelor's degree in Human Resources, Business Administration or any other relevant field.
Minimum 3 Years Experience In a Similar Role.
Experience in data analysis and reporting tools, such as Excel and Power BI.
Experience with SQL and VBA (an asset).
Working with colleagues in Ontario and the US, bilingualism in spoken and written English and French is required.
Good knowledge of HRIS and databases.
Excellent analytical and problem-solving skills.
Excellent communication and collaboration skills.
Ability to manage multiple projects simultaneously.
Proven track record in continuous improvement and solution-oriented thinking.
Additional Information
Work schedule: Monday to Friday, 35 hours per week;
Group insurance after 3 months;
Group RRSP after 6 months;
Free parking;
On-site gym;
Social events (BBQ, Tire sur neige, raffles, etc.).
The American Iron & Steel Company and its subsidiaries offer equal employment opportunities to all. The masculine gender is used only to lighten the text. Only those selected for an interview will be contacted.
Show more
Show less","SQL, VBA, Macros, Data analysis, Reporting tools, Excel, Power BI, HRIS, Databases, Analytical skills, Problemsolving skills, Communication skills, Collaboration skills, Project management, Continuous improvement, Solutionoriented thinking, HR policies and procedures","sql, vba, macros, data analysis, reporting tools, excel, power bi, hris, databases, analytical skills, problemsolving skills, communication skills, collaboration skills, project management, continuous improvement, solutionoriented thinking, hr policies and procedures","analytical skills, collaboration skills, communication skills, continuous improvement, dataanalytics, databases, excel, hr policies and procedures, hris, macros, powerbi, problemsolving skills, project management, reporting tools, solutionoriented thinking, sql, vba"
Data Analyst Part Time,Voxmediallc,"Longueuil, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-voxmediallc-3757203915,2023-12-17,Beloeil, Canada,Mid senior,Onsite,"Summary:
Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistics, Data Mining, Data Visualization, Tableau, Power BI, SQL, R, Python, A/B Testing, ETL, Data Management, Hypothesis Testing, Machine Learning, Algorithms, Optimization","data analysis, statistics, data mining, data visualization, tableau, power bi, sql, r, python, ab testing, etl, data management, hypothesis testing, machine learning, algorithms, optimization","ab testing, algorithms, data management, data mining, dataanalytics, etl, hypothesis testing, machine learning, optimization, powerbi, python, r, sql, statistics, tableau, visualization"
Data Scientist / Scientifique des données,"Premier Pallets, Inc.","Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-scientist-scientifique-des-donn%C3%A9es-at-premier-pallets-inc-3787791021,2023-12-17,Beloeil, Canada,Mid senior,Onsite,"CWP Energy Solutions, a McGill St Laurent Company-
CWP Energy Solutions delivers value to renewable power generators by de-risking their business and stabilizing their returns. Using cutting-edge technology, proprietary data and deep power markets expertise, our data-driven platform gives our partners the tools and intelligence to optimize the sale and purchase of their electricity while managing risks from growing grid intermittency. We partner with renewable energy generators across North America operating in deregulated power markets who seek to maximize the returns on their solar, wind and battery storage investments.
We are proud of our entrepreneurial approach that encourages initiative, experimentation and risk taking. This role is a great opportunity for an individual who wants to create value in an organization that has limited red tape and join a team that operates in a startup-like environment.
Key Responsibilities
Design and implement data models, algorithms, analytics, and dashboards to identify and communicate Power Markets insights.
Comprehend, develop, and deploy time series analysis within the context of power markets.
Research objective functions, regularization techniques, and constraints that will let you confidently deploy complex models under real world conditions/uncertainty.
Apply statistics, machine learning, and a variety of other techniques to analyze the data, generate models, and develop data monetization strategies.
Analyze drivers of day ahead and real time congestion market outcomes based on a variety of factors, including demand, supply, renewables, specific transmission, generation outage impacts, bids, facility limits, etc.
Perform various ad hoc fundamental analysis for asset managers and develop reports for our business partners.
Develop automated tools to simplify the production of reports and analysis using SQL and Python.
Qualifications
Master's degree in actuary, finance, engineering, or related field.
5+ years of experience in a data scientist role, with strong analytical, statistical, and quantitative skills.
Strong programming skills in Python and SQL.
Ability to write technical and commercial reports to convey information in a clear and concise manner.
Data Visualization skills (Power BI, Tableau) is an asset.
Knowledge of North American power markets (RTO/ISO) is an asset.
Excellent written and spoken English skills. French is an asset.
What we offer:
A results-driven corporate culture.
Full group insurance and the opportunity to contribute to an RRSP program.
A vacation policy upon arrival.
A public transit incentive program.
Flexible hours and hybrid work.
A beautiful office with a gym, a lounge, locker rooms and showers.
Version française
CWP Energy Solutions, une compagnie du groupe McGill St Laurent-
CWP Energy Solutions se positionne auprès des générateurs d'énergies renouvelables en réduisant les risques de leurs activités et en stabilisant leurs revenus. Grâce à des technologies de pointe, à un large répertoire de données exclusives et à une expertise approfondie des marchés de l'énergie, notre plateforme fournit à nos partenaires les outils et l'intelligence de marché nécessaires pour optimiser la vente et l'achat de leur électricité tout en gérant les risques liés à l'intermittence croissante du réseau. Nous travaillons en partenariat avec des producteurs d'énergie renouvelable à travers l'Amérique du Nord qui opèrent sur des marchés déréglementés et qui cherchent à maximiser le rendement de leurs investissements en énergie solaire et éolienne ainsi qu’en stockage de batterie.
Nous sommes fiers de notre approche entrepreneuriale qui encourage l'initiative, l'expérimentation et la prise de risque. Ce poste est une excellente occasion de créer de la valeur dans une organisation où l'aspect administratif est limité et est idéal pour une personne qui aspire à opérer dans un environnement de type startup.
Responsabilités principales :
Concevoir et implanter des modèles prédictifs, des algorithmes, des analyses et des tableaux de bord afin d'identifier et de communiquer des informations sur les marchés de l'énergie.
Analyser les facteurs qui influencent le prix de l'électricité sur les marchés en temps réel et sur le « day ahead » en fonction de divers facteurs, notamment la demande, l'offre, les énergies renouvelables, la transmission, les pannes de production, les enchères et les capacités des générateurs, etc.
Comprendre, développer et déployer des analyses de séries temporelles sur les marchés d'électricité.
Rechercher des fonctions objectives, des techniques de régularisation et des contraintes qui permettront de déployer avec assurance des modèles complexes dans des conditions réelles.
Faire appel aux statistiques, à l'apprentissage automatique et à une variété d'autres techniques pour analyser les données, générer des modèles et développer des stratégies de monétisation des données.
Effectuer diverses analyses fondamentales ad hoc pour les gestionnaires d'actifs et élaborer des rapports pour nos partenaires d'affaires.
Participer activement au développement et à l'amélioration des processus opérationnels automatisés (SQL et Python).
Qualifications recherchées :
Maîtrise en actuariat, finance, ingénierie ou dans un domaine connexe.
5+ ans d'expérience dans un rôle de scientifique des données, avec de fortes compétences analytiques, statistiques et quantitatives.
Solides compétences en programmation en Python et SQL.
Connaissance d’outils de visualisation de données (Power BI, Tableau) est un atout.
Connaissance des marchés nord-américains de l’électricité (RTO/ISO) est un atout.
Capacité à rédiger des rapports techniques et commerciaux afin de transmettre des informations de manière claire et concise.
Excellentes compétences en anglais écrit et parlé. Le français est un atout.
Ce que nous offrons:
Une culture d'entreprise axée sur les résultats.
Une assurance collective complète et la possibilité de contribuer à un programme de REER.
Une politique de vacances dès ton arrivée.
Un programme d’incitatif pour le transport en commun.
Un mode de travail hybride.
Un magnifique bureau avec un gym, un lounge, des vestiaires et des douches.
Powered by JazzHR
NGBasUWZtJ
Show more
Show less","Data Science, Machine Learning, Statistics, Python, SQL, Data Visualization, Power BI, Tableau, Finance, Engineering, Actuary, Data Monetization, Data Mining, Time Series Analysis, Analytics, Dashboards, AI, Algorithms, RTO/ISO, Power Markets","data science, machine learning, statistics, python, sql, data visualization, power bi, tableau, finance, engineering, actuary, data monetization, data mining, time series analysis, analytics, dashboards, ai, algorithms, rtoiso, power markets","actuary, ai, algorithms, analytics, dashboard, data mining, data monetization, data science, engineering, finance, machine learning, power markets, powerbi, python, rtoiso, sql, statistics, tableau, time series analysis, visualization"
Expert Data Engineering- EN,CN,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/expert-data-engineering-en-at-cn-3768861799,2023-12-17,Beloeil, Canada,Mid senior,Onsite,"At CN, we work together to move our company—and North America—forward. Be part of our Information & Technology (I&T) team, a critical piece of the engine that keeps us in motion. From enterprise architecture to operational technology, our teams use the agile methodology to automate and digitize our railroad ensuring our operations run optimally and safely and our employees can focus on value-added tasks. You will be able to develop your skills and career in our close-knit, safety-focused culture working together as ONE TEAM. The careers we offer are meaningful because the work we do matters. Join us!
Job Summary
The Expert, Data Engineering is responsible to lead/mentor a team of specialists in the process of building, managing, and optimizing data pipelines, moving them effectively into production for key data and analytics consumers, shaping the enterprise Data as a Service (DaaS) model and delivering on IT business models. The Expert Data Engineering develops best practices and optimizes data pipelines to deliver products and services aligned with business expectations. The incumbent plays a pivotal role in operationalizing data and analytics initiatives, defining and building CN’s data integration and delivery roadmap.
Main Responsibilities
Data Engineering
Ensure optimal data delivery architecture and processes are consistent throughout ongoing projects
Optimize CN’s data architecture to support the next generation of products and data initiatives
Build, manage, and optimize data pipelines, moving them effectively into production for key data and analytics consumers such as business or data analysts and data scientists
Build data and domain event models, implement business rules, and engineer scalable data pipelines
Ensure compliance with data governance and security requirements while creating, improving and operationalizing integrated and reusable data pipelines
Enable faster data access, integrate data reuse, and improve time-to-solution for data and analytics initiatives
Integrate analytics and data science results with business processes
Promote effective data management practices
Data Design
Collaborate with stakeholders and architects to model data landscape and define secure data exchange approaches
Meet with stakeholders to identify fit-for-purpose within CN’s existing data ecosystem and deliver options and agile solutions
Design and develop processing pipelines that ingest data into Data Hubs
Expose the data into meaningful formats, like GraphQL
Provide day-to-day support and technical expertise to both technical and non-technical teams
Participate in building data engineering expertise and framework
Translate business needs into technical requirements
Data Development
Acquire, ingest and process data from multiple sources and systems
Design and develop Extract, Transform, and Load (ETL) pipelines using multiple sources of data in various formats
Conduct metadata management, data cleansing and preparation
Produce well documented quality codes
Data Operations
Use Agile methodologies to streamline project delivery aligned with goals, timelines, and budget
Build monitoring and debugging tools to analyze data pipelines
Help unify software development and operations seamlessly, efficiently, and cost effectively
Improve software quality, automate processes, and accelerate software releases
Data Quality Assurance
Use Agile development practises for code reviews and testing to develop and deliver data pipelines
Develop and implement test plans and scripts for various data quality processes
Maintain manual and automated test scripts
Working Conditions
The role has standard working conditions in an office environment with a regular workweek from Monday to Friday and offers remote work.
Requirements
Experience
Data Engineering
Minimum 6 years of experience in a data engineering role, working in different data management disciplines including data integration, modelling, optimization and quality
Experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental or multi-departmental data management and analytics initiative
Experience with at least 2 of the following technologies: Python, Scala, SQL, Java
Experience with Spark Streaming with Java or Scala is a MUST
Experience with Databricks is a MUST
Experience with Cloud and non-Cloud based Hadoop ecosystem
Experience with Cloud platforms such as Azure, Google Platform
Experience with Linux and shell scripting
Experience/Knowledge on GraphQL
Experience deploying applications into production environments such as code packaging, integration testing, monitoring and release management
Experience with multiple database technologies such as Distributed Processing (Spark, Hadoop, EMR), traditional RDBMS (MS SQL Server, Oracle, DB2, MySQL, PostgreSQL), NoSQL (MongoDB, DynamoDB, Cassandra)
Experience working in an Agile team environment leading a team with parallel deliverables for multiple clients
Education/Certification/Designation
Bachelor's Degree in Computer Science, Electrical Engineering or Software Engineering
Master’s Degree or PhD in Data Management, Data Analytics, Information Systems, or a related quantitative field*
Professional Engineer (P.Eng.) Degree*
Google or Azure Data Engineering or Databricks certification*
Any designation for these above would be considered as an asset
Competencies
Ability to communicate complex solutions
Applies analytical thinking
Innovates through problem solving
Knows the business and stays current with industry trends to elevate expertise and work
Demonstrates organizational abilities and detail oriented
Collaborates with key internal stakeholders to enable higher productivity
Works independently with little supervision
Applies agile mindset
Ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets
Demonstrates commitment to high standards of ethics, regulatory compliance, customer service, and business integrity
About CN
CN is a world-class transportation leader and trade-enabler. Essential to the economy, to the customers, and to the communities it serves, CN safely transports more than 300 million tons of natural resources, manufactured products, and finished goods throughout North America every year. As the only railroad connecting Canada’s Eastern and Western coasts with the Southern tip of the U.S. through a 19,500 mile rail network, CN and its affiliates have been contributing to community prosperity and sustainable trade since 1919. CN is committed to programs supporting social responsibility and environmental stewardship. At CN, we work as ONE TEAM, focused on safety, sustainability and our customers, providing operational and supply chain excellence to deliver results.
For internal candidates, note that the grade level of the position will depend on the employee's experience.
At CN, we are dedicated to building North America’s safest, most inclusive and sustainable railroad, which includes reflecting the communities in which we operate. Research shows that candidates from underrepresented groups often don’t apply unless they feel they fit the job posting at 100%. Even if you don’t see yourself in every job requirement listed in a posting, we still encourage you to apply. If you require an accommodation for the recruitment process (including alternate formats of materials, accessible meeting rooms or other accommodations), please reach out to our team at cnrecruitment@cn.ca.
As an equal employment opportunity employer, all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, and other protected status as required by applicable law. We thank all applicants for their interest, however, only candidates under consideration will be contacted. Please monitor your email on a regular basis, as communication is primarily made through email.
Show more
Show less","Data Engineering, Data Architecture, Data Pipelines, Data Governance, Data Security, Agile Development, Spark Streaming, Databricks, Hadoop Ecosystem, Cloud Platforms, Linux, Shell Scripting, GraphQL, Code Packaging, Integration Testing, Monitoring, Release Management, Database Technologies, Distributed Processing, RDBMS, NoSQL, Agile Team Environment, Computer Science, Electrical Engineering, Software Engineering, Data Management, Data Analytics, Information Systems, Professional Engineer, Google Data Engineering, Azure Data Engineering, Analytical Thinking, Problem Solving, Business Knowledge, Industry Trends, Organizational Abilities, Detail Oriented, Collaboration, Independent Work, Agile Mindset, Structured Data, SemiStructured Data, Unstructured Data, Ethics, Regulatory Compliance, Customer Service, Business Integrity","data engineering, data architecture, data pipelines, data governance, data security, agile development, spark streaming, databricks, hadoop ecosystem, cloud platforms, linux, shell scripting, graphql, code packaging, integration testing, monitoring, release management, database technologies, distributed processing, rdbms, nosql, agile team environment, computer science, electrical engineering, software engineering, data management, data analytics, information systems, professional engineer, google data engineering, azure data engineering, analytical thinking, problem solving, business knowledge, industry trends, organizational abilities, detail oriented, collaboration, independent work, agile mindset, structured data, semistructured data, unstructured data, ethics, regulatory compliance, customer service, business integrity","agile development, agile mindset, agile team environment, analytical thinking, azure data engineering, business integrity, business knowledge, cloud platforms, code packaging, collaboration, computer science, customer service, data architecture, data engineering, data governance, data management, data security, dataanalytics, database technologies, databricks, datapipeline, detail oriented, distributed processing, electrical engineering, ethics, google data engineering, graphql, hadoop ecosystem, independent work, industry trends, information systems, integration testing, linux, monitoring, nosql, organizational abilities, problem solving, professional engineer, rdbms, regulatory compliance, release management, semistructured data, shell scripting, software engineering, spark streaming, structured data, unstructured data"
Data Scientist,Clarifai,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-scientist-at-clarifai-3717903353,2023-12-17,Beloeil, Canada,Mid senior,Remote,"About The Company
Clarifai is a leading, full-lifecycle deep learning AI platform for computer vision, natural language processing, LLM's and audio recognition. We help organizations transform unstructured images, video, text, and audio data into structured data at a significantly faster and more accurate rate than humans would be able to do on their own. Founded in 2013 by Matt Zeiler, Ph.D. Clarifai has been a market leader in AI since winning the top five places in image classification at the 2013 ImageNet Challenge. Clarifai continues to grow with employees remotely based throughout the United States, Canada, Argentina, India and Estonia.
We have raised $100M in funding to date, with $60M coming from our most recent Series C, and are backed by industry leaders like Menlo Ventures, Union Square Ventures, Lux Capital, New Enterprise Associates, LDV Capital, Corazon Capital, Google Ventures, NVIDIA, Qualcomm and Osage.
Clarifai is proud to be an equal opportunity workplace dedicated to pursuing, hiring, and retaining a diverse workforce.
Your Impact
As a Data Scientist you will do integral work that allows the expansion of model creation and validation activities in support of the Platform. This activity will raise the quality and number of models available in the Clarifai Platform.
The Opportunity
You will manage the development of labeled data sets
Develop machine learning models
Be responsible for the machine learning model performance
Document the development of models
Support client engagements for creating custom models
Requirements
Machine learning development experience
Data Exploration expertise
Python scripts and Jupyter notebook experience
SQL programming experience
Technical writing skills
Data Science analysis experience
Cloud computing skills (AWS, GCP)
College degree BS (computer science, math, physics)
Ability to work in North America time zones
Great to Have
Master of Science degree or higher
Experience with JupyterLab
Open source development experience
Github
Show more
Show less","Computer vision, Natural language processing, Large language models, Audio recognition, Machine learning, Python, Jupyter notebook, SQL, Technical writing, Data Science analysis, Cloud computing (AWS GCP), College degree in computer science math or physics, JupyterLab, Open source development, Github","computer vision, natural language processing, large language models, audio recognition, machine learning, python, jupyter notebook, sql, technical writing, data science analysis, cloud computing aws gcp, college degree in computer science math or physics, jupyterlab, open source development, github","audio recognition, cloud computing aws gcp, college degree in computer science math or physics, computer vision, data science analysis, github, jupyter notebook, jupyterlab, large language models, machine learning, natural language processing, open source development, python, sql, technical writing"
Data Engineer  Customer Data Platforms,iShare Inc.,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-engineer-customer-data-platforms-at-ishare-inc-3771991645,2023-12-17,Beloeil, Canada,Mid senior,Remote,"Hiring on behalf of a client for
Data Engineer Customer data platform
Full time/ Remote based in Cananda
We are seeking an experienced and passionate Data Engineer who can designing, developing, maintain and govern data assets and data related products in customer platforms by liaising with multiple stakeholders.
Role and Responsibilities:
Customer Data Asset Stewardship (CDA, including configuration modification/change to source(s), stitch, etc.)
Analysis and remediation within the Customer Data Assets
Coordination and tracking with Source Data Stewards to ensure source remediation is performed
Administrating access management for Customer Data Asset to Direct and Indirect Users
Ensuring the CDA is used only for approved uses
Create the data integration and data diagram documentation
Lead the data validation, UAT and regression test for new data asset creation
Create and maintain data models, including schema design and optimization
Work with stakeholders to understand the data requirements to design, develop, and maintain complex ETL processes
Create and manage data pipelines that automate the flow of data, ensuring data quality and consistency
Requirement/Competencies:
Bachelor’s or Master’s (preferred) degree in in a quantitative or technical field such as Computer Science, Information Technology, Computer Engineering or equivalent
4+ years of experience in Data Engineering, Data Governance and working with Customer Data Platforms (CDP)
Knowledge of all popular CDPs and expert working knowledge of at least one CDP
Knowledge of market trends including cloud technology
Hands on data stewardship experience
Strong working knowledge of SQL, Hadoop, Hive, Python and PySpark
Demonstrated interest in learning about emerging data platforms and Big Data technologies
Strong communication, problem solving, and organizational skills
Must be detail oriented, quality driven, and able to shift quickly to new tasks
Good to have:
Experience working with Amperity CDP
This is a remote position.
Show more
Show less","Data Engineering, Cloud Technology, Data Stewardship, SQL, Hadoop, Hive, Python, PySpark, Data Integration, Data Validation, Regression Testing, Data Modeling, ETL, Data Pipelines, Amperity CDP","data engineering, cloud technology, data stewardship, sql, hadoop, hive, python, pyspark, data integration, data validation, regression testing, data modeling, etl, data pipelines, amperity cdp","amperity cdp, cloud technology, data engineering, data integration, data stewardship, data validation, datamodeling, datapipeline, etl, hadoop, hive, python, regression testing, spark, sql"
Principal Data Scientist - Machine Learning/Scientifique Principal des Données - Apprentissage Automatique,Zinnia,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/principal-data-scientist-machine-learning-scientifique-principal-des-donn%C3%A9es-apprentissage-automatique-at-zinnia-3684313985,2023-12-17,Beloeil, Canada,Mid senior,Remote,"WHO WE ARE:
Building on its rich heritage, Zinnia is transforming the way life and annuity products are created, sold, and administered – from lead to origination to claims – through Zinnia Open Insurance. We're addressing the new realities of the marketplace, and we understand what consumers want and deserve: a great experience from the moment they consider these products until their policies pay out.
Zinnia Open Insurance is advanced data-driven technology that accelerates innovation, enables unrivaled speed-to-market, deepens relationships with policyholders, and empowers better risk-based outcomes. Open transforms the end-to-end experience to make everything faster, more transparent, and more collaborative for consumers, carriers, and advisors. Zinnia is a new tech company, but we're not new to insurance. We're L&A experts and technology innovators building Open Insurance to power exceptional experiences that evolve ahead of the marketplace. We're Zinnia. And we're Open. Join us, and experience Life, More Open.
WHO YOU ARE:
As a key member of our data team at Zinnia you will be on a team that is advancing our tech capabilities and learning to leverage our data to develop innovative data applications. We are relentless in our drive to reliably deliver outstanding products at scale. We are growing fast, but we can go further faster with experienced, collaborative, challenge-seeking analytics leaders like yourself.
Our data team serves Zinnia through data engineering, data analysis, and data science. Our goal is to help uncover opportunities and make decisions with data. We partner with all department stakeholders across the company to develop deeper predictors of behavior, develop insights that drive business strategy and build solutions to optimize our internal and external experiences.
WHAT YOU'LL DO:
Work with stakeholders to intake complicated business problems and translate them into solvable data science projects.
Utilize algorithms and data mining many disparate structured and unstructured data sources to solve data science problems
Contributes to the data science strategy and works to quantify impact against use cases ensuring we are focused on the most valuable opportunities
Develop machine learning based applications using supervised and unsupervised models using R or Python that optimize and personalize customer experiences or reduce manual effort on our internal teams through automated decision making
Develop and support models to enable things such as prescriptive insights, automated decisioning, and insights derived from large audio and text corpus.
Partner with data engineering to take your model from development to deployed infrastructure
Develop experiments to understand model impact, monitor live model analytics, and manage training and retraining pipelines
Brainstorm future use cases and contribute to the learning culture of the data science team
Partner with and mentor other data scientists and data analysts
Partner with multiple marketing teams, manage multiple projects and help conceptualize applications that directly drive company growth and strategy.
You will communicate with a team and articulate ideas to both team members and non-technical stakeholders
You will connect machine learning applications to business needs and help facilitate process changes based on algorithmic solution implementation
WHAT YOU'LL NEED:
You have Data Science experience building and validating machine learning and forecasting models in R or Python
Bachelor's degree or relevant experience can substitute for formal education
You have experience in supervised and unsupervised model techniques such as random forest, gradient boosting, support vector machines, k-means and hierarchical clustering, causal models, mixture models and experience in advanced modeling techniques such as reinforcement learning, neural networks, and natural language modeling
You have experience in delivering natural language projects utilizing techniques such as text summarization, topic modeling, entity extraction, semantic encoding, and valence analysis
You have experience working in an agile business setting
You have experience with relational cloud databases like BigQuery and Snowflake and are comfortable working with unstructured datasets such as unstructured text and audio
Python or R for machine learning algorithms and analysis.
Google Cloud Platform: Kubernetes, BigQuery, MongoDB, Vertex AI, and more: we keep adopting new tools as we grow!
Airflow and dbt for data pipelining.
Tableau for data visualization and consumer facing dashboards.
WHAT'S IN IT FOR YOU?
The opportunity to work with a fantastic team with a great culture;
The opportunity to be part of an innovative and exciting company that is completely changing an outdated industry;
The opportunity to grow professionally in a dynamic and fun environment;
A competitive compensation plan;
Hybrid work from home and office model;
A flexible time off policy which includes unlimited wellness days, personal days and vacation time;
Two volunteer days;
A day off on your Birthday;
The company paid top-tier health benefits plan and a telemedicine service;
A stipend for your home office setup.
À propos
S'appuyant sur son riche héritage, Zinnia transforme la façon dont les produits d'assurance vie et de rente sont créés, vendus et administrés - du début à la fin - grâce à l'Assurance Ouverte Zinnia. Nous nous attaquons aux nouvelles réalités du marché et nous comprenons ce que les consommateurs veulent et méritent : une expérience exceptionnelle depuis le moment où ils envisagent ces produits jusqu'au paiement de leurs réclamations.
L'Assurance Ouverte Zinnia est une technologie avancée basée sur les données qui accélère l'innovation, permet une vitesse de mise sur le marché incomparable, approfondit les relations avec les propriétaires de polices et permet de meilleurs résultats basés sur les risques. L'Assurance Ouverte transforme l'expérience de bout en bout afin de rendre tout plus rapide, plus transparent et plus collaboratif pour les consommateurs, les assureurs et les conseillers. Zinnia est une nouvelle entreprise technologique, mais nous ne sommes pas nouveaux dans le domaine de l'assurance. Nous sommes des experts en assurance vie et rente et des innovateurs technologiques qui construisent l'Assurance Ouverte pour offrir des expériences exceptionnelles qui évoluent avant le marché. Nous sommes Zinnia. Et nous sommes ouverts. Rejoignez-nous, et faites l'expérience de la vie, Plus Ouverte.
Qui vous êtes
En tant que membre essentiel de notre équipe de données chez Zinnia, vous serez à la tête d'une équipe qui fait avancer nos capacités techniques et apprend à utiliser nos données pour développer des applications de données innovantes. Nous sommes déterminés à livrer de manière efficace et fiable des produits exceptionnels. Notre croissance est rapide, mais nous pouvons aller encore plus vite avec des leaders analytiques expérimentés, collaboratifs et prêts à relever des défis comme vous.
Notre équipe de données sert Zinnia à travers l'ingénierie des données, l'analyse des données et la science des données. Notre objectif est de contribuer à la découverte d'opportunités et à la prise de décisions à partir de données. Nous travaillons en partenariat avec toutes les parties prenantes de l'entreprise pour développer des indicateurs de comportement plus profonds, des idées qui orientent la stratégie de l'entreprise et des solutions qui optimisent nos expériences internes et externes.
Dans ce rôle, vous serez responsable de:
Travailler avec les parties prenantes pour comprendre des problèmes commerciaux complexes et les traduire en projets de science des données qui peuvent être résolus.
Utiliser des algorithmes et l'exploration de données dans de nombreuses sources de données disparates, structurées et non structurées, pour résoudre les problèmes de science des données.
Contribuer à la stratégie de science des données et travailler à quantifier l'impact par rapport aux cas d'utilisation en s'assurant que nous nous concentrons sur les opportunités les plus valables.
Développer des applications basées sur machine learning en utilisant des modèles supervisés et non supervisés à l'aide de R ou Python qui optimisent et personnalisent les expériences des clients ou réduisent l'effort manuel de nos équipes internes
Développer et soutenir des modèles pour permettre des éléments tels que les perspectives prescriptives, la prise de décision automatisée et les perspectives dérivées de grands corpus audio et textuels.
Travailler en partenariat avec l'ingénierie des données pour faire passer votre modèle du développement à l'infrastructure déployée.
Développer des expérimentations pour comprendre l'impact des modèles, surveiller les analyses des modèles en direct et gérer les pipelines de formation et de recyclage.
Réfléchir à de futurs cas d'utilisation et contribuer à la culture d'apprentissage de l'équipe de science des données.
Travailler en partenariat avec d'autres scientifiques et analystes de données et leur offrir un mentorat.
Travailler en partenariat avec plusieurs équipes de marketing, gérer plusieurs projets et aider à conceptualiser des applications qui stimulent directement la croissance et la stratégie de l'entreprise.
Vous communiquez avec une équipe et articulez des idées à la fois aux membres de l'équipe et aux parties prenantes non techniques.
Vous ferez le lien entre les applications d'apprentissage automatique et les besoins de l'entreprise et contribuerez à faciliter les changements de processus basés sur la mise en œuvre de solutions algorithmiques.
Ce que nous cherchons:
Vous avez de l'expérience en science des données pour construire et valider des modèles d'apprentissage automatique et de prévision en R ou Python.
Un baccalauréat ou une autre expérience pertinente peut remplacer l'éducation formelle.
Vous avez de l'expérience dans les techniques de modèles supervisés et non supervisés tels que la forêt aléatoire, le gradient boosting, les machines à vecteurs de support, les k-means et le clustering hiérarchique, les modèles de causalité, les modèles de mélange et de l'expérience dans les techniques de modélisation avancées telles que l'apprentissage par renforcement, les réseaux neuronaux et la modélisation du langage naturel.
Vous avez de l'expérience dans la réalisation de projets en langage naturel utilisant des techniques telles que le résumé de texte, la modélisation de sujets, l'extraction d'entités, l'encodage sémantique et l'analyse de valence.
Vous avez de l'expérience dans un environnement de travail agile.
Vous avez de l'expérience avec les bases de données relationnelles en cloud comme BigQuery et Snowflake et vous êtes à l'aise avec les ensembles de données non structurées comme le texte non structuré et l'audio.
Python ou R pour les algorithmes d'apprentissage automatique et l'analyse.
Google Cloud Platform : Kubernetes, BigQuery, MongoDB, Vertex AI, et plus encore : nous continuons à adopter de nouveaux outils au fur et à mesure de notre croissance !
Airflow et dbt pour le pipelining de données.
Tableau pour la visualisation des données et les tableaux de bord destinés aux consommateurs.
Ce que nous offrons:
L'opportunité de travailler avec une équipe fantastique dotée d'une grande culture;
L'opportunité de faire partie d'une entreprise innovante et passionnante qui change complètement une industrie dépassée;
L'opportunité de grandir professionnellement dans un environnement dynamique et amusant;
Un plan de rémunération compétitif ;
Modèle hybride de travail à domicile et au bureau ;
Une politique de vacances flexible qui comprend des jours de bien-être, des jours personnels et des vacances illimitées ;
Deux jours de bénévolat ;
Un jour de congé pour votre anniversaire;
Un plan d'assurance maladie de premier ordre et un service de télémédecine;
Une allocation pour l'aménagement de votre bureau à domicile.
Show more
Show less","Data Science, Machine Learning, Data Mining, Predictive Analytics, Natural Language Processing, Python, R, BigQuery, Snowflake, Kubernetes, MongoDB, Vertex AI, Airflow, dbt, Tableau","data science, machine learning, data mining, predictive analytics, natural language processing, python, r, bigquery, snowflake, kubernetes, mongodb, vertex ai, airflow, dbt, tableau","airflow, bigquery, data mining, data science, dbt, kubernetes, machine learning, mongodb, natural language processing, predictive analytics, python, r, snowflake, tableau, vertex ai"
Data Engineering Lead/Senior Data Engineer – Hybrid,Quantum Management Services Ltd.,"Urban agglomeration of Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-engineering-lead-senior-data-engineer-%E2%80%93-hybrid-at-quantum-management-services-ltd-3777306785,2023-12-17,Beloeil, Canada,Mid senior,Hybrid,"Position:
Data Engineering Lead/Senior Data Engineer (Retail industry)
Location:
Montreal – Hybrid
Whether you are a Senior Data Engineer who is ready to transition into a leadership role or a Lead Data Engineer ready to take on a new challenge, this could be the opportunity for you!
This role is focused on leading the development within their current data stack, including Snowflake, DBT, and AWS serverless architecture as well as mentoring and providing technical guidance to data engineers, developers, and analysts.
Key Responsibilities
Lead development within current data stack, including Snowflake, DBT, and AWS serverless architecture
Mentor and provide technical guidance to data engineers, developers, and analysts
Oversee the design and development of data pipelines to ingest data from various sources and transform raw data into clean and scalable models to support BI and analytics
Lead the evaluation and implementation of a data orchestration tool (e.g., Airflow, Dagster) to help coordinate and monitor data pipelines and downstream dependencies
Implement and monitor data quality processes to ensure accurate, reliable, and timely data delivery to users
Implement processes to monitor and continually re-evaluate the cost and performance of the infrastructure and tools within our data platform
Who We’re Looking For
Bachelor’s degree or higher in Computer Science, Software Engineering, or a related field
At least five years of experience in data engineering or a related field, at least one year as a technical lead
Expertise in using DBT on top of modern data warehousing technology
Strong programming skills in Python (or similar languages) with respect to the manipulation and analysis of large data sets
Strong technical skills in SQL and data modelling
Strong understanding of how data and technology fit within strategic business objectives
Ability and willingness to learn new technologies and take on challenges
A Bonus, But Not Required
Experience with AWS technologies such as Lambda, API Gateway, and Event Bridge
Experience with data streaming technologies like Kafka
Experience working in retail, e-commerce, marketing, and related data sets
If you are ready to take the lead in data engineering and be part of a dynamic, innovative team, please send your resume in Word format to Stephanie Scappaticcio at stephanie.scappa@quantum.ca .
REFER A PERM HIRE AND EARN $1,000!
For more details, click here . Conditions apply.
OUR VIRTUAL DOORS ARE OPEN!
We’re also bringing the interviews to you by various web applications, virtually! Contact us today for your next opportunity.
CNESST permit number: AP-2000414
Show more
Show less","Data Engineering, Snowflake, DBT, AWS, Data Pipelines, Data Orchestration, Airflow, Dagster, Data Quality, Data Platform, Python, SQL, Data Modeling, Business Objectives, AWS Lambda, API Gateway, Event Bridge, Kafka, Data Streaming","data engineering, snowflake, dbt, aws, data pipelines, data orchestration, airflow, dagster, data quality, data platform, python, sql, data modeling, business objectives, aws lambda, api gateway, event bridge, kafka, data streaming","airflow, api gateway, aws, aws lambda, business objectives, dagster, data engineering, data orchestration, data platform, data quality, data streaming, datamodeling, datapipeline, dbt, event bridge, kafka, python, snowflake, sql"
Bilingual Data Analyst,Salt,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/bilingual-data-analyst-at-salt-3784389968,2023-12-17,Beloeil, Canada,Mid senior,Hybrid,"Our client is looking for a Bilingual Data Analyst to join their team; helping them understand their clients business.
The ideal candidate will be someone who is passionate about data and the power, it has to tell stories to our clients about their business.
The person they are searching for is going to help the clients identify opportunities to leverage data in their business decision. Help clients identify their KPIs and create compelling dashboards that simplify access to that data.
Requirements:
● Fluent written and spoken English and French
● Advanced knowledge of tabular data.
● Strong emphasis on data quality. Understand the different techniques of data processing, cleansing, and verifying.
● Understand how to access different databases and create advanced queries for data analysis.
● Curiosity to dive into available data and enjoy searching for patterns that could indicate new insights
● At least 2 years of professional experience
● Strong data visualization skills Tableau, Looker, PowerBI, Quicksight or any visualization tool in the same vein.
● Mash up and analyze different data set to support the business and management with clear and insightful analyses
● Understand how to implement and analyze Google Analytics 4 data. Especially eCommerce data.
● Experience with marketing attribution a plus.
● Bachelor’s Degree ranging from mathematics, statistics, and computer science to business administration, economics or finance.
Note: This position is HYBRID - at least two times a week in the office, mandatory.
Show more
Show less","Data Analysis, Data Visualization, Tableau, Looker, PowerBI, Quicksight, Data Quality, Data Processing, Data Cleansing, Data Verification, Database Access, Advanced Queries, Google Analytics 4, eCommerce Data, Marketing Attribution, Tabular Data","data analysis, data visualization, tableau, looker, powerbi, quicksight, data quality, data processing, data cleansing, data verification, database access, advanced queries, google analytics 4, ecommerce data, marketing attribution, tabular data","advanced queries, data processing, data quality, data verification, dataanalytics, database access, datacleaning, ecommerce data, google analytics 4, looker, marketing attribution, powerbi, quicksight, tableau, tabular data, visualization"
Senior Data Developper | Développeur(se) de données senior,MaintainX,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/senior-data-developper-d%C3%A9veloppeur-se-de-donn%C3%A9es-senior-at-maintainx-3729913641,2023-12-17,Beloeil, Canada,Mid senior,Hybrid,"MaintainX offre une gamme de fonctions évoluées conçues pour libérer les travailleurs industriels et de première ligne des presse-papiers grâce à des outils numériques qui simplifient les processus de travail et stimulent la productivité. Le logiciel aide les travailleurs à faire le suivi de la maintenance réactive et préventive ainsi qu'à surveiller les activités quotidiennes, comme les inspections de sécurité, les inspections de la qualité et les listes de contrôle, le tout à l'aide d'une piste de vérification numérique. Nous contribuons à l'amélioration de l'efficacité opérationnelle pour plus de 7 000 entreprises, dont Univar, Quikrete, ABInBev, Duracell, Marriott et McDonald's.
Nous avons levé 104 millions de dollars en capital-risque à la suite d'un récent tour de table de série C mené par Bain Capital Ventures, Bessemer Ventures, August Capital, Amity Ventures, Ridge Ventures ainsi que des PDG de GE, Twilio, Toast et PagerDuty.
Nous voulons que MaintainX soit la plateforme d'exécution du travail par excellence pour les fabricants, dans les domaines de la maintenance, des opérations et de la sécurité.
Nous sommes à la recherche d'un ingénieur en chef des données réfléchi et bien équilibré pour rejoindre notre équipe en pleine expansion ! Vous serez responsable de la construction des fondations de notre architecture de données pour alimenter les décisions à l'échelle de l'entreprise chez MaintainX, en plus d'aider à construire des outils que nos clients commerciaux utilisent tous les jours. Vous serez le premier professionnel de l'ingénierie des données à rejoindre une équipe saine et collaborative, améliorant continuellement nos pratiques d'ingénierie et fournissant une technologie innovante, tout en gardant le plaisir.
Votre objectif sera de développer et de maintenir notre plateforme de données pour créer et soutenir les produits de données de MaintainX. Les ingénieurs de données sont les garants de l'intégrité, de la qualité et de l'accessibilité de nos données. Ils s'assurent que notre plateforme recueille des données de haute qualité à partir de toutes les applications en appliquant les meilleures pratiques d'ingénierie logicielle à nos pipelines de données. Les ingénieurs de données permettent aux utilisateurs de données de MaintainX d'accéder à des données exploitables où et quand ils en ont besoin.
Ce que vous ferez:
Gestion de l'entrepôt de données et du lac de données : Concevoir, développer et maintenir notre entrepôt de données et notre infrastructure de lac de données, en veillant à ce que les données soient accessibles, sécurisées et bien organisées.
Intégration des données : Collaborer avec des data scientists, des analystes et d'autres parties prenantes pour concevoir et mettre en œuvre des pipelines de données pour les processus ETL (Extract, Transform, Load).
Qualité des données et gouvernance : Mettre en œuvre des contrôles de qualité des données, des règles de validation et des politiques de gouvernance des données pour maintenir l'exactitude et la conformité des données.
Optimisation des performances : Contrôler et optimiser en permanence les pipelines de données et le stockage en termes de performances, d'évolutivité et de rentabilité.
Sécurité des données : Assurer la sécurité des données et la conformité aux normes et réglementations du secteur, en mettant en œuvre des mécanismes de cryptage, de contrôle d'accès et d'audit.
Documentation : Créer et maintenir une documentation complète pour les processus d'ingénierie des données, les modèles de données et les meilleures pratiques.
Collaboration : Travailler en étroite collaboration avec des équipes interfonctionnelles pour comprendre leurs besoins en matière de données et les aider à concevoir et à développer des solutions de données qui s'alignent sur les objectifs de l'entreprise.
Intégration complète (facultatif) : Si vous avez de l'expérience en développement full-stack, vous serez également impliqué dans l'intégration de solutions d'ingénierie des données dans nos applications à l'aide de Node.js, React et TypeScript.
À propos de vous:
5+ ans d'expérience en tant qu'ingénieur de données travaillant sur la modélisation des données, l'entreposage des données, les technologies de lac de données (par exemple AWS Redshift, AWS S3, Snowflake, Hadoop) et le développement ETL (pipelines de données / d'événements).
Familiarité avec la gouvernance des données et les meilleures pratiques en matière de sécurité
Vous êtes enthousiaste à l'idée de travailler avec une équipe à distance ; vous appréciez de collaborer sur des problèmes, de poser des questions, de fournir un retour d'information et de soutenir les autres dans leurs objectifs.
Vous avez de l'expérience dans le mentorat technique, le coaching et l'amélioration des résultats techniques des personnes qui vous entourent.
A apprécier :
Expérience avec Python, SQL, Snowflake, Airflow
Expérience avec les internes d'un moteur de calcul distribué (Spark, Presto, DBT, ou Flink/Beam)
Expérience dans le déploiement et la mise à l'échelle de solutions ML en utilisant des frameworks open-source (MLFlow, TFX, H2O, etc.)
Expérience dans la construction d'applications full-stack (Node.js, React, TypeScript)Qui sommes-nous:
Ce que vous y gagnerez :
Un salaire compétitif et des opportunités d'équité significatives.
Couverture des soins de santé, des soins dentaires et des soins de la vue.
Prenez les congés dont vous avez besoin.
Une culture de travail où :
Vous travaillerez aux côtés de personnes du monde entier qui reflètent les valeurs de MaintainX : Intelligents, Humbles, Optimistes
Nous croyons en la méritocratie, où les idées et les efforts sont publiquement célébrés.
A propos de nous :
Notre mission est de faciliter la vie des ouvriers dans le monde entier en créant des logiciels qui répondent à leurs besoins et à leurs réalités. Notre produit change véritablement la vie de 80% de la population active qui ne travaille pas derrière un bureau et qui a besoin d'un logiciel d'entreprise à portée de main.
MaintainX s'engage à créer un environnement diversifié. Tous les candidats qualifiés seront pris en considération pour un emploi sans égard à la race, la couleur, la religion, le sexe, l'identité ou l'expression sexuelle, l'orientation sexuelle, l'origine nationale, la génétique, le handicap, l'âge ou le statut d'ancien combattant.
MaintainX is the world leading mobile-first workflow management platform for industrial and frontline workers. We are a modern IoT enabled cloud based tool for maintenance, safety, and operations on equipment and facilities. MaintainX powers operational excellence for 7,000+ businesses including Univar, Quikrete, ABInBev, Duracell, Marriott and McDonald's.
We've raised $104 million in venture capital following a recent Series C funding round led by Bain Capital Ventures, Bessemer Ventures, August Capital, Amity Ventures, Ridge Ventures as well as CEO's from GE, Twilio, Toast and PagerDuty.
We want MaintainX to be the go-to work execution platform for manufacturers, across Maintenance, Operations and Safety.
We are looking for a thoughtful, well-rounded Lead Data Engineer to join our expanding team! You will be responsible for building the foundations of our data architecture to power company-wide decisions at MaintainX, in addition to helping build tools that our business customers use every day. You will be the first Data Engineering professional to come onboard, joining a healthy and collaborative team, continuously improving our engineering practices and delivering innovative technology, all while keeping it fun.
Your goal will be to develop and maintain our data platform to create and support MaintainX's data products. Data engineers are the owners of our data integrity, quality and accessibility. They make sure that our platform gathers high quality data from all applications by applying software engineering best-practices to our data pipelines. Data engineers enable MaintainX's data users to access actionable data where and when they need it.
What you'll do:
Data Warehouse and Data Lake Management: Design, develop, and maintain our data warehouse and data lake infrastructure, ensuring data is accessible, secure, and well-organized.
Data Integration: Collaborate with data scientists, analysts, and other stakeholders to design and implement data pipelines for ETL (Extract, Transform, Load) processes.
Data Quality and Governance: Implement data quality checks, validation rules, and data governance policies to maintain data accuracy and compliance.
Performance Optimization: Continuously monitor and optimize data pipelines and storage for performance, scalability, and cost efficiency.
Data Security: Ensure data security and compliance with industry standards and regulations, implementing encryption, access controls, and auditing mechanisms.
Documentation: Create and maintain comprehensive documentation for data engineering processes, data models, and best practices.
Collaboration: Work closely with cross-functional teams to understand their data needs and provide support in designing and developing data solutions that align with business objectives.
Full-Stack Integration (Optional): If you have full-stack development experience, you will also be involved in integrating data engineering solutions into our applications using Node.js, React, and TypeScript.
About you:
5+ years of experience as a Data Engineer working on data modeling, data warehousing, data lake technologies (e.g. AWS Redshift, AWS S3, Snowflake, Hadoop) and developing ETL (Data / Event pipelines)
Familiarity with data governance and security best practices
Excitement for working with a remote team; you value collaborating on problems, asking questions, delivering feedback, and supporting others in their goals
Experience with technical mentoring, coaching, and improving the technical output of the people around you.
Nice to have:
Experience with Python, SQL, Snowflake, Airflow
Experience with the internals of a distributed compute engine (Spark, Presto, DBT, or Flink/Beam)
Experience deploying and scaling ML solutions using open-source frameworks (MLFlow, TFX, H2O, etc.)
Experience building full-stack applications (Node.js, React, TypeScript)
What's in it for you:
Competitive salary and meaningful equity opportunities.
Healthcare, dental, and vision coverage.
Take what you need PTO.
A Work Culture where:
You'll work alongside folks across the globe that reflect the MaintainX values: Smart Humble Optimists
We believe in meritocracy, where ideas and effort are publicly celebrated
About us:
Our mission is to make the life of blue-collar workers easier worldwide by creating software that meets their needs and realities. Our product is truly life-changing for 80% of the workforce that doesn't work behind a desk and needs enterprise-grade software at their fingertips.
MaintainX is committed to creating a diverse environment. All qualified applicants will receive consideration for employment without regard to race, colour, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Show more
Show less","Data architecture, Data warehouse, Data lake, ETL pipelines, Data quality, Data governance, Data security, Data modeling, Data integration, SQL, Python, Snowflake, Airflow, Spark, Presto, DBT, Flink, Beam, MLFlow, TFX, H2O, Node.js, React, TypeScript","data architecture, data warehouse, data lake, etl pipelines, data quality, data governance, data security, data modeling, data integration, sql, python, snowflake, airflow, spark, presto, dbt, flink, beam, mlflow, tfx, h2o, nodejs, react, typescript","airflow, beam, data architecture, data governance, data integration, data lake, data quality, data security, datamodeling, datawarehouse, dbt, etl pipelines, flink, h2o, mlflow, nodejs, presto, python, react, snowflake, spark, sql, tfx, typescript"
Team Lead – Data Developer,Cirque du Soleil Entertainment Group,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/team-lead-%E2%80%93-data-developer-at-cirque-du-soleil-entertainment-group-3738209353,2023-12-17,Beloeil, Canada,Mid senior,Hybrid,"Team Lead - Data Engineer
Permanent position
We were born from an eclectic troop of performers. Come live the experience of being yourself every day to create the extraordinary.
OUR MISSION
Since 1984, Cirque du Soleil Entertainment Group has been focusing on ever more creative and magical work in order to constantly push the limits of the imagination and surprise spectators! Innovation and creativity are at the heart of our shows and will continue to transcend our upcoming productions around the world.
Today, we have a place for those who have a marked taste for thinking up exceptional products, for ensuring the smooth running of operations, for putting their ingenuity at the service of their team, in addition to contributing to enhancing their expertise and develop practices unique to Cirque du Soleil!
So don't wait any longer and join the Cirque adventure! You will meet ambitious people and people passionate about the world of entertainment!
YOUR MISSION
We are looking for a
Team Lead – Data Engineer
to join our Customer Data and Analytics team. The ideal candidate should have advanced experience in team management, data development, data pipeline management, and cloud data warehouse architecture.
As a Team Lead, you will manage a group of Data Engineers, which mission is to develop and maintain the customer data ecosystem, built on Google Cloud Platform. Your role will be to manage the delivery of the tasks, assign the proper resources, and help your team grow by giving them help on their professional development, as well as making sure the different projects are aligned with the team’s roadmap and objectives.
Main Responsibilities
Ensure that data engineering initiatives align with the organization's goals.
Define project scope, objectives, timelines, and allocate resources effectively.
Monitor project progress and address issues as they arise.
Manage workload distribution and task assignments.
Ensure that projects are completed on time and within budget.
Identify and mitigate potential risks associated with data engineering projects.
Work with data architects to design scalable and efficient data pipelines and systems.
Collaborate with other teams to understand their data needs and requirements.
Plan for scalability to accommodate growing data needs.
Develop contingency plans for data pipeline failures or other issues.
Monitor system performance and identify areas for improvement.
Provide technical guidance and mentorship to team members.
Ensure that the team follows best practices and coding standards.
Communicate effectively with stakeholders, including data analysts, data scientists, and business teams.
Ensure data quality and data governance standards are met.
Ensure compliance with relevant regulations (e.g., GDPR, CCPA…).
Encourage knowledge sharing within the team.
Encourage professional development for team members.
Manage relationships with third-party vendors and service providers.
Stay up-to-date with the latest technologies and trends in data engineering.
YOUR UNIQUE EXPERTISE
Education
Bachelor's or Master's degree in a relevant field such as computer science, business intelligence, information systems, or engineering.
Experience
At least 6-10 years of significant experience in database development, data pipeline management, or a similar role.
Proven leadership experience within a team of data experts (analysts, developers, scientists).
Experience with cloud computing systems and big data.
Expertise with Agile methodology.
Experience with Consumer Data Platform and Audience Management (CDP/DMP) implementations is a plus.
Experience with a data governance framework is a plus.
Technical Skills
Experience with cloud-based data services (e.g., AWS, Azure, GCP).
Experience with Google Cloud Platform is a plus
Proficiency in data extraction, transformation, and loading (ETL) processes.
Strong knowledge of data modeling, database design, and SQL.
Experience with data integration and data warehousing.
Familiarity with big data technologies such as Hadoop, Spark, and NoSQL databases.
Understanding of data security and compliance.
Experience with version control systems like Git.
Knowledge of data engineering tools and frameworks, such as Apache Airflow.
Knowledge of data quality standards and practices.
Ability to implement data governance policies and procedures.
Ability to set goals, prioritize tasks, and delegate responsibilities effectively.
Ability to troubleshoot data pipeline issues and optimize performance.
Other Skills
Strong leadership and team management skills.
Excellent communication and collaboration skills.
Experience in conducting performance reviews and providing constructive feedback.
Ability to manage and prioritize multiple tasks and initiatives.
Strong analytical and problem-solving skills.
Risk assessment and mitigation skills.
Ability to collaborate with cross-functional teams, including data analysts, data scientists, and business stakeholders.
Ability to work effectively with various stakeholders, vendors, and partners.
Effective presentation skills for conveying technical concepts to non-technical audiences.
Proficiency in project management methodologies and tools.
Experience in defining project scope, creating timelines, and managing resources.
Bilingualism (French/English) is preferred.
To keep pace with the creation, production and distribution of our products, our employer proposal is unique.
Here Is An Overview
An inclusive hybrid work mode based on team experience and flexible working hours
Access to an individual office on the days you come to the Montreal Studio, a stimulating environment that will allow you to put all your creativity to good use
A culture focused on the development of fair practices and unifying events
Professional growth focused on high quality projects to propel your career
A group insurance program, RRSPs and other benefits that we will be happy to present to you
Personalized support adapted to your professional ambitions: training program, regular check-ins, inter-team contribution.
Cirque du Soleil Entertainment Group is committed to creating and maintaining an accessible and inclusive workplace. Proud ambassador of equality in employment, Cirque du Soleil Entertainment Group is committed to eliminating discrimination based on age, skin color, origins, religion, sex, sexual orientation and any other characteristic. Neutral and inclusive writing is now preferred for all new job postings.
Show more
Show less","Team Lead, Data Engineer, Cloudbased data services, AWS, Azure, GCP, Google Cloud Platform, Data extraction, Data transformation, Data loading (ETL), Data modeling, Database design, SQL, Data integration, Data warehousing, Big data technologies, Hadoop, Spark, NoSQL databases, Data security, Data compliance, Version control systems, Git, Data engineering tools, Apache Airflow, Data quality standards, Data governance policies, Data governance procedures, Project management, Agile methodology, Consumer Data Platform, Audience Management (CDP/DMP) implementations, Data governance framework, Leadership, Team management, Communication, Collaboration, Performance reviews, Constructive feedback, Analytical skills, Problemsolving skills, Risk assessment, Risk mitigation, Crossfunctional teams, Data analysts, Data scientists, Business stakeholders, Vendors, Partners, Project scope, Timelines, Resources, Bilingualism","team lead, data engineer, cloudbased data services, aws, azure, gcp, google cloud platform, data extraction, data transformation, data loading etl, data modeling, database design, sql, data integration, data warehousing, big data technologies, hadoop, spark, nosql databases, data security, data compliance, version control systems, git, data engineering tools, apache airflow, data quality standards, data governance policies, data governance procedures, project management, agile methodology, consumer data platform, audience management cdpdmp implementations, data governance framework, leadership, team management, communication, collaboration, performance reviews, constructive feedback, analytical skills, problemsolving skills, risk assessment, risk mitigation, crossfunctional teams, data analysts, data scientists, business stakeholders, vendors, partners, project scope, timelines, resources, bilingualism","agile methodology, analytical skills, apache airflow, audience management cdpdmp implementations, aws, azure, big data technologies, bilingualism, business stakeholders, cloudbased data services, collaboration, communication, constructive feedback, consumer data platform, crossfunctional teams, data analysts, data compliance, data engineering tools, data extraction, data governance framework, data governance policies, data governance procedures, data integration, data loading etl, data quality standards, data scientists, data security, data transformation, database design, dataengineering, datamodeling, datawarehouse, gcp, git, google cloud platform, hadoop, leadership, nosql databases, partners, performance reviews, problemsolving skills, project management, project scope, resources, risk assessment, risk mitigation, spark, sql, team lead, team management, timelines, vendors, version control systems"
Expert Data Architect- EN,CN,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/expert-data-architect-en-at-cn-3780029592,2023-12-17,Beloeil, Canada,Mid senior,Hybrid,"At CN, we work together to move our company—and North America—forward. Be part of our Information & Technology (I&T) team, a critical piece of the engine that keeps us in motion. From enterprise architecture to operational technology, our teams use the agile methodology to automate and digitize our railroad ensuring our operations run optimally and safely and our employees can focus on value-added tasks. You will be able to develop your skills and career in our close-knit, safety-focused culture working together as ONE TEAM. The careers we offer are meaningful because the work we do matters. Join us!
Role Description
As part of CN Data Services (CNDS), the Expert, Data Architect will be responsible for ensuring - via models, standards, technologies, and processes – that data is managed as a strategic business asset in the same manner as our tangible rail equipment. It identifies, organizes, structures, and defines the business entities that are relevant to the enterprise in an effective and sustainable way. The Expert Data Architect is responsible for designing, building, managing, and optimizing domain-agnostic batch and stream data pipelines, moving them effectively from the source/system of record, shaping the enterprise Source Data Product, and delivering it for consumption. This includes defining solution score and plan architecture, collaborating with Enterprise Architecture and other solution architects, and providing the direction in the delivery of the solution to designers & integrators. More precisely, the Expert builds the solution, drives alignment of the proposed solution with the Enterprise & Domain strategies, master plans & roadmaps, and manages risk mitigation, and as required drives issue escalation & resolution related to the architecture. The experts sign off on architecture where he/she does have the delegation authority and governs the implementation of the architecture during the design phase.
Main Responsibilities
Data Engineering
Ensure optimal data delivery architecture and processes are consistent throughout ongoing projects.
Optimize CN’s data architecture to support the next generation of products and data initiatives.
Build, manage, and optimize data pipelines, moving them effectively into production for key data and analytics consumers such as business or data analysts and data scientists.
Build data and domain event models, implement business rules, and engineer scalable data pipelines.
Ensure compliance with data governance and security requirements while creating, improving, and operationalizing integrated and reusable data pipelines.
Enable faster data access, integrate data reuse, and improve time-to-solution for data and analytics initiatives.
Integrate analytics and data science results with business processes.
Promote effective data management practices.
Modernization
Work with peers to improve architecture processes and deliverables with the trending technology.
Participate in yearly product health assessment.
Maintain solution architecture roadmap based on Enterprise Architecture & Business strategic direction.
Technical Leadership and Governance oversight
Accountable to lead communication and to drive escalation & resolution with Stakeholders.
Ensuring Standards are being published and evangelized.
Ensuring the Enterprise Governance principles are being applied.
Requirements
Education & Experience
Minimum 10 years of overall work experience (including in OT, IT, Development engagements, and Data Integration).
Minimum 2 years demonstrating technical leadership qualities and/or overseeing deliverables
Minimum 2 years in a similar role.
Bachelor's degree in Computer science, Information Systems, Engineering or equivalent degree or relevant work experience.
Skills/Knowledge
Strong architecture knowledge within numerous fields: business, application, information, and technology architecture facets.
Hands-on experience in at least one of the Data Integration development technologies (Mulesoft, Kafka, Qlik Replicate, Azure Data Factory).
Strong Informatica Intelligent Cloud Services (IICS) or IBM DataStage experience will also be considered.
Good understanding of application design patterns (Dependency Injection, Microservices, and others) and modern platforms (API/Microservices, Mobile, MBaaS, Mulesoft, Container).
Knowledge and working experience in Google Cloud Platform – Big Query, Cloud SQL, Dataproc, Cloud Storage.
Comparable working experience in Azure Ecosystem will also be considered.
Strong experience with Agile/Scrum/Safe delivery methodology.
Strong ability to bring a complex concept to actionable architecture deliverables.
Strong capacity to learn new concepts and technology quickly with the capacity to coach, mentor, and develop others by teaching them.
Strong ability to taking hard decisions by understanding and explaining trade-offs between business enablement and economic impacts.
The role requires people skills to influence and orient business and technical audiences toward a common goal that is satisfying all stakeholders.
Very good ability to build collaborative cross-functional relationships, pulling together and maintaining a cohesive and high-functioning team.
Strong ability to lead the team on a mandate and to lead decisions.
Ability to deliver mandate with great autonomy – self-driven.
Excellent written and oral communication skills.
About CN
CN is a world-class transportation leader and trade-enabler. Essential to the economy, to the customers, and to the communities it serves, CN safely transports more than 300 million tons of natural resources, manufactured products, and finished goods throughout North America every year. As the only railroad connecting Canada’s Eastern and Western coasts with the Southern tip of the U.S. through a 19,500 mile rail network, CN and its affiliates have been contributing to community prosperity and sustainable trade since 1919. CN is committed to programs supporting social responsibility and environmental stewardship. At CN, we work as ONE TEAM, focused on safety, sustainability and our customers, providing operational and supply chain excellence to deliver results.
For internal candidates, note that the grade level of the position will depend on the employee's experience.
At CN, we are dedicated to building North America’s safest, most inclusive and sustainable railroad, which includes reflecting the communities in which we operate. Research shows that candidates from underrepresented groups often don’t apply unless they feel they fit the job posting at 100%. Even if you don’t see yourself in every job requirement listed in a posting, we still encourage you to apply. If you require an accommodation for the recruitment process (including alternate formats of materials, accessible meeting rooms or other accommodations), please reach out to our team at cnrecruitment@cn.ca.
As an equal employment opportunity employer, all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, and other protected status as required by applicable law. We thank all applicants for their interest, however, only candidates under consideration will be contacted. Please monitor your email on a regular basis, as communication is primarily made through email.
Show more
Show less","Data Integration, Data Architecture, Data Engineering, Data Pipelines, Data Governance, Data Security, Data Modeling, Business Rules, Analytics, Data Science, Cloud Computing, Big Data, Machine Learning, Artificial Intelligence, Agile Methodology, Scrum, Safe, Microservices, API, Mobile, MBaaS, Mulesoft, Container, Google Cloud Platform, Big Query, Cloud SQL, Dataproc, Cloud Storage, Azure Ecosystem, Informatica Intelligent Cloud Services (IICS), IBM DataStage, Azure Data Factory, Kafka, Qlik Replicate","data integration, data architecture, data engineering, data pipelines, data governance, data security, data modeling, business rules, analytics, data science, cloud computing, big data, machine learning, artificial intelligence, agile methodology, scrum, safe, microservices, api, mobile, mbaas, mulesoft, container, google cloud platform, big query, cloud sql, dataproc, cloud storage, azure ecosystem, informatica intelligent cloud services iics, ibm datastage, azure data factory, kafka, qlik replicate","agile methodology, analytics, api, artificial intelligence, azure data factory, azure ecosystem, big data, big query, business rules, cloud computing, cloud sql, cloud storage, container, data architecture, data engineering, data governance, data integration, data science, data security, datamodeling, datapipeline, dataproc, google cloud platform, ibm datastage, informatica intelligent cloud services iics, kafka, machine learning, mbaas, microservices, mobile, mulesoft, qlik replicate, safe, scrum"
"Big Data Engineer || Malvern,PA ||",Extend Information Systems Inc.,"Malvern, PA",https://www.linkedin.com/jobs/view/big-data-engineer-malvern-pa-at-extend-information-systems-inc-3730102495,2023-12-17,Swarthmore,United States,Mid senior,Hybrid,"Job Profile : Big Data Engineer
Job Location : Malvern,PA
Job Description
Technology requirements
Min 6 yrs hands on experience in Spark, Python, Scala, Hadoop, Java
Must have hands on experience in AWS ETL Glue, Lambda, DynamoDB
Must have hands on experience in PySpark on Jupyter notebook
Must have experience in CI/CD, AWS S3, AWS EC2, AWS IAM, AWS Data Lake, SQL, Hive, Shell Scripting
Building and managing public and private cloud infrastructure with AWS, EC2 and S3 resources
Participating in requirement analysis and planning the development
Support production environment
Designing automation processes as per the operational needs of an organization
Reviewing the code, design and providing expertise in the development and integration of systems
Work in a team of Data Engineer for Data ingestion process from Legacy System to AWS
Work in a Support team to analyze, triage and debug the incidents and failures occurred
Monitoring the cluster jobs in a daily basis
Building and managing public and private cloud infrastructure with AWS, EC2 and S3 resources
Participating in requirement analysis and planning the development
Support production environment
Designing automation processes as per the operational needs of an organization
Thanks & Regards,
Shubham
IT Recruiter
Shubham@extendinfosys.com
(571)-547-2870
Extend Information System INC
Show more
Show less","Spark, Python, Scala, Hadoop, Java, AWS ETL Glue, Lambda, DynamoDB, PySpark, Jupyter notebook, CI/CD, AWS S3, AWS EC2, AWS IAM, AWS Data Lake, SQL, Hive, Shell Scripting, Cloud Infrastructure, Requirement Analysis, Production Support, Automation Processes, Code Review, Systems Integration, Data Ingestion, Incident Analysis, Triage, Debugging, Cluster Jobs Monitoring","spark, python, scala, hadoop, java, aws etl glue, lambda, dynamodb, pyspark, jupyter notebook, cicd, aws s3, aws ec2, aws iam, aws data lake, sql, hive, shell scripting, cloud infrastructure, requirement analysis, production support, automation processes, code review, systems integration, data ingestion, incident analysis, triage, debugging, cluster jobs monitoring","automation processes, aws data lake, aws ec2, aws etl glue, aws iam, aws s3, cicd, cloud infrastructure, cluster jobs monitoring, code review, data ingestion, debugging, dynamodb, hadoop, hive, incident analysis, java, jupyter notebook, lambda, production support, python, requirement analysis, scala, shell scripting, spark, sql, systems integration, triage"
Senior Marketing Data Analyst,Security Finance,"Spartanburg, SC",https://www.linkedin.com/jobs/view/senior-marketing-data-analyst-at-security-finance-3784654038,2023-12-17,Duncan,United States,Mid senior,Hybrid,"Job Overview
As
Senior Marketing Data Analyst
you will have the opportunity to pull marketing data from multiple sources then do a deep dive into patterns and statistics within that data and transform those insights into future recommendations.
As a Senior Marketing Data Analyst, You Will
Work with other teams to export and import marketing data from disparate systems, perform data cleansing and merging functions and help find new ways to provide data consumers’ easy access to the information they need
Proactively identify trends, opportunities, patterns, anomalies, spikes, and other occurrences in data sets that will help increase sales and profitability from existing products and campaigns and potentially lead to ideas for new revenue streams
Aid in the design, improvement, and generation of reports and presentations for the Executive Team and senior leadership
Focus on web analytics and analysis of traffic and consumer activity to help build new campaigns to drive subscriptions to our premium digital products
Analyze the success of external activities and campaigns including search marketing and display ads
Translate data into clear, concise, actionable objectives
Work with marketing departments to assess, define, and develop report deliverables to meet their requirements
Support the design of appropriate data repository and reporting tools that will be used to organize, analyze, and report data
Backup for regular functions of the Budget and Analysis team and TM1 administration
Prompt and regular attendance is required
You could be a great addition as a Senior Marketing Data Analyst, if you have:
4 year degree in Business Management, Information Technology, Marketing, Finance, Statistics or Data Analysis
4+ years of experience in reporting and analytics involving both quick ad-hoc requests and long-term analysis based on complex queries
Must be based out of SC, GA, TN, or AL
Experience with Business Intelligence, data analysis tools and visualization a plus
Awareness of testing concepts and practices
Comprehensive understanding of customer relationship management and associates practices, strategies, and technologies
Advanced Excel skills
Reliable internet; DSL or faster
Willing to travel to home office for quarterly meetings and/or special projects
We Offer
TOP-of-the-line training - We are committed to helping you build a solid foundation and do your job to the best of your abilities.
An EXCELLENT benefits bundle that includes medical insurance (minimal cost to the employee), dental, life insurance, short-and long-term disability, profit sharing, 401k with company matching, and paid sick, holiday, and vacation time.
Community Service - As a company, we are pro-active members of the community. We make ongoing contributions to charities, local food banks, educational institutions, and more.
Growth Potential – We believe in fostering our employees’ talents and providing a pathway for their individual career story.
We are a leader in the financial services industry since 1955, and proudly provide easy and safe installment loans and income tax preparation to the communities where we operate. Thanks to our conveniently located branches, we are honored to provide personal service from people our customers know they can trust. At Security Finance, it is about being good stewards of our community, helping neighbors in times of need, and treating customers with the respect they deserve.
Come Begin Your Story! Apply today!
This position is subject to a background check for any convictions directly related to its duties and responsibilities. Only job-related convictions will be evaluated and will not automatically disqualify the candidate.
Show more
Show less","Data Analysis, Data Mining, Data Visualization, Business Intelligence, Data Cleansing, Data Merging, Data Integration, Data Warehousing, Data Reporting, Advanced Excel, Customer Relationship Management, Statistical Analysis, Trend Analysis, Forecasting, Predictive Analytics, DataDriven Decision Making, SQL, Python, R, Tableau, Power BI, Looker, Google Analytics","data analysis, data mining, data visualization, business intelligence, data cleansing, data merging, data integration, data warehousing, data reporting, advanced excel, customer relationship management, statistical analysis, trend analysis, forecasting, predictive analytics, datadriven decision making, sql, python, r, tableau, power bi, looker, google analytics","advanced excel, business intelligence, customer relationship management, data integration, data merging, data mining, data reporting, dataanalytics, datacleaning, datadriven decision making, datawarehouse, forecasting, google analytics, looker, powerbi, predictive analytics, python, r, sql, statistical analysis, tableau, trend analysis, visualization"
Lead Data Engineer,InfoVision Inc.,"Dallas, TX",https://www.linkedin.com/jobs/view/lead-data-engineer-at-infovision-inc-3773565680,2023-12-17,Mansfield,United States,Mid senior,Onsite,"Lead Data Developer
Dallas, TX
Long term contract
Qualifications:
1. 15+ years of experience in ETL & Data Warehousing
2. Should have excellent leadership & communication skills
3. Should have in depth knowledge on SSIS ETL Tool and good working knowledge on Power BI
4. Should have worked on data sources such as SAP and Salesforce
5. Should have very good knowledge of SSIS (ETL Tool), StreamSets (ETL Tool), Azure Cloud, ADF, Azure Synapse Analytics & Azure Hub Events
6. Should have executed atleast 2 Azure Cloud Data Warehousing projects
7. Should have worked atleast 4 projects using Agile/SAFe methodology
8. Should have demonstrated working knowledge on ITIL V4 concepts such as Incident Management, Problem Management, Change Management & Knowledge Management
9. Should have working experience on any DevOps tools like GitHub, Jenkins, etc & on semi-structured data formats like JSON, Parquet and/or XML files & written complex SQL queries for data analysis and extraction
10. Should have in depth understanding on Data Warehousing, Data Analysis, Data Profiling, Data Quality & Data Mapping
11. Should have cross global location experience and led a team of atleast 15+ members in a global delivery model
12. Should have experience in working with product managers, project managers, business users, applications development team members, DBA teams and Data Governance team on a daily basis to analyze requirements, design, development and deployment technical solutions
Responsibilities:
1. Lead nearshore and offshore team to do production support of existing EDW, design, build & deploy enhancements and bug fixes to existing EDW
2. Support the existing EDW system built using Azure SQL, Microsoft SSIS, StreamSets & Power BI
3. Be on call to support job incidents on rotational basis
4. Work with business and technology stakeholders to communicate EDW incidents/problems and manage their expectations
5. Leverage ITIL concepts to circumvent incidents, manage problems and document knowledge
6. Analyze the different source systems, profile data, understand, document & fix Data Quality issues
7. Gather requirements and business process knowledge in order to transform the data in a way that is geared towards the needs of end users
8. Write complex SQLs to extract & format source data for ETL/data pipeline
9. Create design documents, Source to Target Mapping documents and any supporting documents needed for deployment/migration
10. Design, Develop and Test ETL/Data pipelines
11. Write Unit Test cases, execute Unit Testing and document Unit Test results
12. Deploy ETL/Data pipelines
13. Use DevOps tools to version, push/pull code and deploy across environments
14. Support team during troubleshooting & debugging defects & bug fixes, business requests, environment migrations & other adhoc requests
15. Maintain and improve already existing processes
Ensure that the data pipelines are stable, secure, maintainable and highly available
Lokesh Kumar
lokesh.kumar@infovision.com
Show more
Show less","ETL, Data Warehousing, SSIS ETL tool, Power BI, SAP, Salesforce, StreamSets ETL tool, Azure Cloud, ADF, Azure Synapse Analytics, Azure Hub Events, Agile, SAFe methodology, ITIL V4, Incident Management, Problem Management, Change Management, Knowledge Management, DevOps tools, GitHub, Jenkins, JSON, Parquet, XML, Data Analysis, Data Profiling, Data Quality, Data Mapping, SQL, Unit Testing, Data pipelines","etl, data warehousing, ssis etl tool, power bi, sap, salesforce, streamsets etl tool, azure cloud, adf, azure synapse analytics, azure hub events, agile, safe methodology, itil v4, incident management, problem management, change management, knowledge management, devops tools, github, jenkins, json, parquet, xml, data analysis, data profiling, data quality, data mapping, sql, unit testing, data pipelines","adf, agile, azure cloud, azure hub events, azure synapse analytics, change management, data mapping, data profiling, data quality, dataanalytics, datapipeline, datawarehouse, devops tools, etl, github, incident management, itil v4, jenkins, json, knowledge management, parquet, powerbi, problem management, safe methodology, salesforce, sap, sql, ssis etl tool, streamsets etl tool, unit testing, xml"
Palantir Data Engineer,Trident Consulting,"Dallas, TX",https://www.linkedin.com/jobs/view/palantir-data-engineer-at-trident-consulting-3784650592,2023-12-17,Mansfield,United States,Mid senior,Onsite,"Trident Consulting is seeking a
""Palantir Data Engineer""
for one of our industry leading clients
.
Title: Palantir Data Engineer
Location: Dallas, TX
Job Type: Contract
Job Description:
8-10+years of experience in implementing analytical solutions by leveraging Networking data using Palantir foundry.
Integrate advanced analytics capabilities within the Palantir platform to enhance data-driven decision-making.
Expertise in Palantir technologies, including hands-on experience in analytics components.
About Trident:
Trident Consulting is an award-winning IT/engineering staffing company founded in 2005 and headquartered in San Ramon, CA. We specialize in placing high-quality vetted technology and engineering professionals in contract and full-time roles. Trident's commitment is to deliver the best and brightest individuals in the industry for our clients' toughest requirements.
Some of our recent awards include:
• 2022, 2021, 2020 Inc. 5000 fastest-growing private companies in America
• 2022, 2021 SF Business Times 100 fastest-growing private companies in Bay Area
Show more
Show less","Palantir Foundry, Data Analytics, Data Engineering, Networking, Advanced Analytics, Handson experience","palantir foundry, data analytics, data engineering, networking, advanced analytics, handson experience","advanced analytics, data engineering, dataanalytics, handson experience, networking, palantir foundry"
Functional Data Analyst,Numentica,"Dallas, TX",https://www.linkedin.com/jobs/view/functional-data-analyst-at-numentica-3777181552,2023-12-17,Mansfield,United States,Mid senior,Remote,"This is a remote position.
We are looking for a data analyst who can help drive business intelligence efforts and support the growth of the Business Process and Finance teams. This role provides an opportunity to solve the company’s most interesting problems and develop new business strategies.
Regularly mine data from large SQL databases
Build models that lead to better insights and business understanding.
Design and maintain reports/dashboards and track and communicate key performance indicators.
Assure quality and data integrity for revenue and business development.
Build logical and physical data models with Facts and dimensions, aggregates in Snowflake.
Interact with data source owners and participate in developing project plans.
Provide original insights beyond the scope of supporting reactive requests.
Support ongoing analysis of business performance trends and insights required to inform decisions.
Design and build new dashboards to monitor performance metrics; maintain existing dashboards for accuracy and incorporate changes in the business.
Build a strong relationship with data engineering to understand data availability, accuracy, and consistency; comfortable providing specifications for data tracking and data warehouse needs.
Proactively perform data exploration to discover actionable insights.
Present your research and insights to all levels of the company clearly and concisely.
Collaborate with IT team and other cross functional systems teams during design and support implementation of business solutions.
Responsible for designing, monitoring, measuring, and improving the quality of master data.
Qualifications
Responsible for designing, monitoring, measuring, and improving the quality of master data.
Experience in data architecture, data modelling, designing and data analysis with conceptual, logical, and physical modelling for OLAP and Snowflake data warehousing.
Experience in Normalization of OLTP an denormalization of entities for Snowflake Data warehousing,
Experience ins Data governance and master data management
Experience in SQL, PL/SQL packages, functions, stored procedures, triggers
Experience in BI/DW solutions (ETL, Data mart, OLAP) and BI reporting tools like Tableau.
Exposure to Oracle Fusion Cloud and SFDC functional areas
Exposure to Statistics, Econometrics, Data Science, and Machine Learning concepts and tools.
Engineering/technical background analyzing large data sets.
5+ years of analytical experience and comfortable working with large data sets Excellent programming logic with comprehension of at least one scripting language (i.e., Python, javascript, C++, R, etc.
Experience as an analyst in the following areas: finance, accounting, QTC and non-QTC
Extensive experience with SQL is critical; additional experience with BI & visualization tools is a strong plus (i.e., Qlik, Tableau, Looker)
Comfortable working with MS Excel and MS PowerPoint
Must be intellectually curious, eager to learn, self-motivated, detail-oriented, and able to multi-task
Able to operate with openness and efficiency with one or more SDLC methodologies (e.g., Scrum)
Strong communication skills and ability to work collaboratively with cross-functional teams
Show more
Show less","SQL, Snowflake, Tableau, Data modelling, Data architecture, Data governance, Data management, Statistics, Econometrics, Data Science, Machine Learning, Python, Javascript, C++, R, Qlik, Looker, MS Excel, MS PowerPoint, Scrum, SDLC methodologies","sql, snowflake, tableau, data modelling, data architecture, data governance, data management, statistics, econometrics, data science, machine learning, python, javascript, c, r, qlik, looker, ms excel, ms powerpoint, scrum, sdlc methodologies","c, data architecture, data governance, data management, data modelling, data science, econometrics, javascript, looker, machine learning, ms excel, ms powerpoint, python, qlik, r, scrum, sdlc methodologies, snowflake, sql, statistics, tableau"
Senior Specialist | Data Intelligence and Analytics - Global Biologics,Bristol Myers Squibb,"Devens, MA",https://www.linkedin.com/jobs/view/senior-specialist-data-intelligence-and-analytics-global-biologics-at-bristol-myers-squibb-3733887008,2023-12-17,Leominster,United States,Associate,Hybrid,"Working with Us
Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams rich in diversity. Take your career farther than you thought possible.
Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.
Position Summary
The Data Intelligence and Analytics (DI&A) team for Global Biologics provides data engineering and analytics solutions for biologics manufacturing sites at Bristol Myers Squibb. DI&A manages a wide variety of data platforms and works with stakeholders throughout manufacturing to develop analytics assets that aim to deliver more medicines to more patients faster. The Senior Specialist | DI&A will report to the Devens Senior Manager | DI&A and support business critical analytics platforms. The position requires an individual who is passionate about the intersection of data intelligence and manufacturing with a desire to push the boundaries of possibility.
Key Responsibilities
24x7 administrative support for Devens AVEVA PI system, Sartorius SIMCA, InfoBatch reporting and other validated analytics platforms essential to manufacturing.
Hands on development and implementation of analytics tools based on complex user requirements across multiple platforms and functional areas with limited guidance.
Perform troubleshooting utilizing critical thinking skills and techniques such as root cause analysis to resolve incidents in a timely fashion with minimal guidance.
Responsible for the execution and creation, review, and approval of documentation associated with procedure-driven compliance activities such as periodic reviews and on-site application recovery (OAR).
Develop an understanding of others' views and build rapport with stakeholders. Demonstrate awareness of functional business context and impact of DI&A efforts. Use these to positive effect.
Develop and maintain system training curricula and related materials. Execute on training plans for end users and administrators leading sessions on platform use.
Demonstrate openness to learning and developing, taking responsibility for personal growth that also supports the advancement of DI&A, Global Biologics and BMS objectives.
Qualifications & Experience
Bachelor’s degree in computer science or engineering with 2-4 years of experience.
At least two years of system administration experience, preferably with AVEVA PI.
At least one year of experience within a manufacturing environment, preferably within the pharmaceutical industry.
Working experience with Microsoft Windows Server in a domain setting, technical knowledge of network configuration, and competency with navigating complex virtualized computing environments.
Working experience administering relational databases with RDBMS tools such as Microsoft SQL Server Management Studio. Basic to intermediate competency with SQL.
Track record of delivering on small, well-scoped technical projects and tasks. Ability to manage small projects with minimal guidance. Explains own work clearly and confidently.
Familiarity with common manufacturing systems such as DCS, MES, and CMMS a plus.
Experience with data flows and interfaces associated with bench-top instrumentation, IIoT architectures, and system integration protocols a plus.
A desire to transform patient’s lives through science.
If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.
Uniquely Interesting Work, Life-changing Careers
With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in an inclusive culture, promoting diversity in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.
On-site Protocol
Physical presence at the BMS worksite or physical presence in the field is a necessary job function of this role, which the Company deems critical to collaboration, innovation, productivity, employee well-being and engagement, and it enhances the Company culture.
BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.
BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.
BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.
Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.
Show more
Show less","Data engineering, Data analytics, Manufacturing, AVEVA PI, Sartorius SIMCA, InfoBatch, SQL, Microsoft SQL Server Management Studio, DCS, MES, CMMS, Data flows, IIoT, System integration, System administration, Project management, Troubleshooting, Root cause analysis, Documentation, Training, Microsoft Windows Server, Relational databases, RDBMS tools, Technical writing, Verbal communication","data engineering, data analytics, manufacturing, aveva pi, sartorius simca, infobatch, sql, microsoft sql server management studio, dcs, mes, cmms, data flows, iiot, system integration, system administration, project management, troubleshooting, root cause analysis, documentation, training, microsoft windows server, relational databases, rdbms tools, technical writing, verbal communication","aveva pi, cmms, data engineering, data flows, dataanalytics, dcs, documentation, iiot, infobatch, manufacturing, mes, microsoft sql server management studio, microsoft windows server, project management, rdbms tools, relational databases, root cause analysis, sartorius simca, sql, system administration, system integration, technical writing, training, troubleshooting, verbal communication"
IT Data Analyst,Barrday Inc.,"Millbury, MA",https://www.linkedin.com/jobs/view/it-data-analyst-at-barrday-inc-3787770062,2023-12-17,Leominster,United States,Mid senior,Onsite,"Barrday, a manufacturer of composite materials, is in search of an IT Data Analyst.
ABOUT US:
Barrday is a leading advanced materials solutions company with an international manufacturing and sales presence. Our product lines encompass applications for the composite and protective markets. We are headquartered outside of Toronto, Canada in Cambridge, ON, with manufacturing facilities in Charlotte, NC and Millbury, MA. Our success is based on developing technologically advanced fiber reinforcement, prepreg and other material solutions for our customers in the aerospace, defense, industrial, energy and protective markets.
Summary
The IT Data Analyst will be responsible for collecting, analyzing, and interpreting large and complex data sets to identify valuable insights and patterns. Use advanced statistical and machine learning techniques to develop predictive models and algorithms that can help our organization make informed decisions. This is a full-time position with a typical workweek of 40 hours.
KEY RESPONSIBILITIES:
Collecting and cleaning data: Data analysts must be able to collect data from a variety of sources, such as ERP Databases, Machine Databases, and Barrday data warehouse. They must then clean the data to remove errors and inconsistencies.
Analyzing data: Data analysts use statistical software to analyze data and identify trends. They may also use machine learning algorithms to build models that can predict future behavior.
Develop and visualizing data using tools like SQL, PL/SQL, SSRS, and PowerBi.
Communicating findings: Data analysts must be able to communicate their findings to stakeholders in a clear and concise way. They may create reports, presentations, or dashboards to share their findings.
Developing solutions: Data analysts must be able to develop solutions to business problems using data. For example, they may promote improving customer service, or identify discrepancies in how users enter data.
Collaborating with cross-functional teams to develop data-driven solutions that address business challenges.
Continuously researching and exploring new data science techniques and tools to improve the efficiency and effectiveness of data analysis.
Develop and maintain project documentation, and other documentation as required during and after executing implementation tasks.
ADDITIONAL RESPONSIBILITIES AND ACCOUNTABILITIES:
Contribute to Barrday business initiatives as needed.
Provide 2nd tier help desk support as required.
KNOWLEDGE & EXPERIENCE REQUIREMENT:
A Bachelor's or Master's degree in computer science, statistics, mathematics, or a related field.
2-4 years of experience working with Oracle or SQL Server Database
Strong interpersonal, communication and organizational skills
Excellent computer literacy and previous experience with ERP systems. IFS experience preferred, but not mandatory.
Experience in governing IT processes
Experience in managing multiple stakeholders, facilitating workshops or dialogue on process design, and exploring solutions.
Understanding of or strong interest in manufacturing operations.
VALUES:
Barrday is an enterprise built upon teamwork and trust. Excellence, Integrity, Respect, and Innovation are our core values and guide our communications and our actions. We respect diversity in ideas and wish to give all employees the opportunity to flourish and grow to the best of their abilities. Our expectation is that each person will, by virtue of their behavior, assist in making everyone successful.
WORKING CONDITIONS:
Standard office hours from 8:00 to 5:00 pm.
On-site.
This is an office environment & manufacturing plant.
Occasional travel will be required.
After hours IT on call support may be asked.
Occasional weeknight/weekend work may be required.
EEO/AA/M/F/Vets/Disabled
“This position requires the individual to be a U.S. person as defined in the Arms Export Control Act (i.e., U.S. citizen, U.S. permanent lawful resident, or a ‘protected individual’ as defined by 8 U.S.C.
1324b(a)(3)).”
Powered by JazzHR
x3Df5IfQQ1
Show more
Show less","Data Analysis, Data Science, Machine Learning, Statistical Analysis, Data Visualization, SQL, PL/SQL, SSRS, PowerBi, Oracle Database, SQL Server Database, ERP Systems, IFS, Manufacturing Operations","data analysis, data science, machine learning, statistical analysis, data visualization, sql, plsql, ssrs, powerbi, oracle database, sql server database, erp systems, ifs, manufacturing operations","data science, dataanalytics, erp systems, ifs, machine learning, manufacturing operations, oracle database, plsql, powerbi, sql, sql server database, ssrs, statistical analysis, visualization"
Future Opportunity- Data Engineering Consultant,Avanade,"Syracuse, NY",https://www.linkedin.com/jobs/view/future-opportunity-data-engineering-consultant-at-avanade-3781783100,2023-12-17,Syracuse,United States,Associate,Onsite,"Job Description
RECRUITING FOR THE FUTURE
This posting is for a future job opportunity.
To support the needs of our clients, Avanade is actively recruiting and interviewing for the role outlined below, as we continue to build on another year of growth in our business. Now, with more than 60,000 employees around the globe, we are positioning our organization to effectively prepare for the future. What does this mean for you? We encourage you to apply and interview for this role without the need to decide now. This allows you to connect with leaders and hiring managers at a pace that works for you and when roles become available, you will be the first know.
Data Engineering Consultant
Our talented Data and AI Practice is made up of globally recognized experts- and there's room for more analytical and ambitious data professionals. If you're passionate about helping clients make better data-driven decisions to tackle their most complex business issues. let's talk. Take your skills to a new level and launch a career where you can can truly do what matters.
Come join us
As a Data Engineer within our Data and AI practice, you’ll design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.
Together we do what matters.
What You'll Do
Use your knowledge to plan and deliver data warehouses and storage
Take part in crafting and running bespoke data services for individual projects
Stay up to date with business best practice in using and retrieving data
Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining
Customize storage and extraction, metadata, and information repositories
Build and use effective metrics and monitoring processes
Help to develop business intelligence tools
Craft and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
Skills And Experiences
Strong knowledge of Python, Spark, and T-SQL
Microsoft Fabric/Synapse, Purview and Azure Databricks, PowerBI
Database, storage, collection and aggregation models, techniques, and technologies – and how to apply them in business
Ability to use technology to aim business problems using one or more Microsoft Analytics services for building data pipelines, data streams, and system integration
About You
Characteristics that can spell success for this role:
Advanced knowledge in entity and relationship extraction from unstructured data
Understanding of database table indexing, with emphasis on clustered column store tables
Proficient in data handling, understanding, analysis and interpretation of results
Experience with SQL technologies: Databricks (Spark), OR Azure Synapse
Data security: Must be able to demonstrate an understanding of data security at rest and in transit
Scale: Deliver performance at scale, and accurately anticipate system enhancement needs
Data integrity: Data manipulation, error identification and handling, and modeling
Enjoy your career
Some of the best things about working at Avanade
People-first culture that supports innovation and encourages people to move forward
Turn your ideas to human impact by cultivating a team that will help clients unlock what’s next
Opportunities to innovate in the Microsoft platform while charting your own career path
On-the-job growth through hackathons, innovation contests, and other expressions of our passion for innovating with purpose
Real-time access to technical and skilled resources globally
Find out more about some of our benefits here .
A great place to work
As you bring your skills and abilities to Avanade, you’ll get distinctive experiences, limitless learning, and ambitious growth in return. As we continue to build our diverse and inclusive culture, we become even more innovative and creative, helping us better serve our clients and our communities. You’ll join a community of smart, supportive collaborators to lift, mentor, and guide you, but to also lean on your expertise . You get a company purpose-built for business-critical, leading-edge technology solutions, committed to improving the way humans work, interact, and live. It’s all here, so take a closer look!
We work hard to provide an inclusive, diverse culture with a deep sense of belonging for all our employees. Visit our Inclusion & Diversity page.
Create a future for our people that focuses on
Expanding your thinking
Experimenting courageously
Learning and pivoting
Inspire greatness in our people by
Empowering every voice
Encouraging boldness
Celebrating progress
Accelerate the impact of our people by
Amazing the client
Prioritizing what matters
Acting as one
Learn more
To learn more about the types of projects our Data & AI team works on check out these case studies:
thyssenkrupp Materials Services uses data to help strike a delicate operational balance
What matters to SSE Renewables is innovating for a sustainable future
Hachette UK enables employees with machine learning-driven contract searches
Marston Holdings identifies significant cloud savings
Interested in knowing what’s going on inside Avanade? Check out our blogs:
Avanade Insights – exchange ideas that drive tomorrow’s innovation
Inside Avanade – explore what life is like working at Avanade
Compensation for roles at Avanade varies depending on a wide array of factors including but not limited to the specific office location, role, skill set and level of experience. As required by local law, Avanade provides a reasonable range of compensation for roles that may be hired in New York as set forth below and information on benefits offered is here.
Range of Starting Pay for role:
New York: $121,500 – $ 143,000
About Avanade
Avanade is the leading provider of innovative digital, cloud and advisory services, industry solutions and design-led experiences across the Microsoft ecosystem. Every day, our 59,000 professionals in 26 countries make a genuine human impact for our clients, their employees and their customers.
We have been recognized as Microsoft’s Global SI Partner of the Year more than any other company. With the most Microsoft certifications (60,000+) and 18 (out of 18) Gold-level Microsoft competencies, we are uniquely positioned to help businesses grow and solve their toughest challenges.
We are a people first company, committed to providing an inclusive workplace where employees feel comfortable being their authentic selves. As a responsible business, we are building a sustainable world and helping people from underrepresented communities fulfil their potential.
Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com .
Show more
Show less","Python, Spark, TSQL, Microsoft Fabric/Synapse, Purview, Azure Databricks, PowerBI, Database, Storage, Collection, Aggregation, Data pipelines, Data streams, System integration, Entity extraction, Relationship extraction, Data security, Data warehousing, Data mining, Business intelligence, Report generation, Data visualization, Analytics, SQL, Databricks, Azure Synapse, Machine learning, Cloud computing, Data integrity, Data manipulation, Error identification, Data modeling, Data analysis, Data interpretation","python, spark, tsql, microsoft fabricsynapse, purview, azure databricks, powerbi, database, storage, collection, aggregation, data pipelines, data streams, system integration, entity extraction, relationship extraction, data security, data warehousing, data mining, business intelligence, report generation, data visualization, analytics, sql, databricks, azure synapse, machine learning, cloud computing, data integrity, data manipulation, error identification, data modeling, data analysis, data interpretation","aggregation, analytics, azure databricks, azure synapse, business intelligence, cloud computing, collection, data integrity, data interpretation, data manipulation, data mining, data security, data streams, dataanalytics, database, databricks, datamodeling, datapipeline, datawarehouse, entity extraction, error identification, machine learning, microsoft fabricsynapse, powerbi, purview, python, relationship extraction, report generation, spark, sql, storage, system integration, tsql, visualization"
Data analytics with ML Developer,Intellectt Inc,"Syracuse, NY",https://www.linkedin.com/jobs/view/data-analytics-with-ml-developer-at-intellectt-inc-3656717896,2023-12-17,Syracuse,United States,Mid senior,Onsite,"Minimum 10+ years of experience is required, kindly do not submit any irrelevant resumes.
Please note along with the Experience in Data science Candidate must have either Mechanical Engineering Degree or he should have the experience in executing projects in the mechanical Engineering Domain.
NO RESUMES ACCEPTED WITHOUT MECHANICAL ENGINEERING DEGREE OR PROJECT EXPERIENCE
Job Description & Skill Requirement
Designing predictive models for self-running artificial intelligence (AI) software.
Transforming data science prototypes and applying appropriate ML algorithms and tools.
Ensuring that algorithms generate accurate user recommendations.
Turning unstructured data into useful information
Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.
Developing ML algorithms to analyze huge volumes of historical data to make predictions.
Running tests, performing statistical analysis, and interpreting test results.
Documenting machine learning processes.
Tools and frameworks.
Advanced proficiency with Python, Java, and R code writing.
Extensive knowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.
In-depth knowledge of mathematics, statistics, and algorithms.
Qualification
BE / MCA
Regards
Pavan Kalyan Byri
Technical Recruiter
Email: pavan.byri@intellectt.com
Direct number: 732-355-7378
Text Now: 424-248-1416
Show more
Show less","Java, R, Python, Machine Learning, Artificial Intelligence, Data Structures, Data Modeling, Software Architecture, Mathematics, Statistics, Algorithms","java, r, python, machine learning, artificial intelligence, data structures, data modeling, software architecture, mathematics, statistics, algorithms","algorithms, artificial intelligence, data structures, datamodeling, java, machine learning, mathematics, python, r, software architecture, statistics"
Data Analytics and ML Developer,Katalyst HealthCares & Life Sciences,"Syracuse, NY",https://www.linkedin.com/jobs/view/data-analytics-and-ml-developer-at-katalyst-healthcares-life-sciences-3768027683,2023-12-17,Syracuse,United States,Mid senior,Onsite,"Responsibilities
Designing predictive models for self-running artificial intelligence (AI) software.
Transforming data science prototypes and applying appropriate Client algorithms and tools.
Ensuring that algorithms generate accurate user recommendations.
Turning unstructured data into useful information
Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.
Developing Client algorithms to analyze huge volumes of historical data to make predictions.
Running tests, performing statistical analysis, and interpreting test results.
Documenting machine learning processes.
Reqquirements
Advanced proficiency with Python, Java, and R code writing.
Extensive knowledge of Client frameworks, libraries, data structures, data modeling, and software architecture.
In-depth knowledge of mathematics, statistics, and algorithms.
Show more
Show less","Machine Learning, Artificial Intelligence, Python, Java, R, Client frameworks, Data structures, Data modeling, Software architecture, Mathematics, Statistics, Algorithms","machine learning, artificial intelligence, python, java, r, client frameworks, data structures, data modeling, software architecture, mathematics, statistics, algorithms","algorithms, artificial intelligence, client frameworks, data structures, datamodeling, java, machine learning, mathematics, python, r, software architecture, statistics"
Data Analytics and ML Developer,Katalyst HealthCares & Life Sciences,"Syracuse, NY",https://www.linkedin.com/jobs/view/data-analytics-and-ml-developer-at-katalyst-healthcares-life-sciences-3768033725,2023-12-17,Syracuse,United States,Mid senior,Onsite,"Responsibilities
Designing predictive models for self-running artificial intelligence (AI) software.
Transforming data science prototypes and applying appropriate Client algorithms and tools.
Ensuring that algorithms generate accurate user recommendations.
Turning unstructured data into useful information
Solving complex problems with multi-layered data sets, as well as optimizing existing machine learning libraries and frameworks.
Developing Client algorithms to analyze huge volumes of historical data to make predictions.
Running tests, performing statistical analysis, and interpreting test results.
Documenting machine learning processes.
Advanced proficiency with Python, Java, and R code writing.
Extensive knowledge of Client frameworks, libraries, data structures, data modeling, and software architecture.
In-depth knowledge of mathematics, statistics, and algorithms.
BE / MCA
Show more
Show less","Artificial intelligence, Machine learning, Data science, Natural language processing, Predictive modeling, Data analysis, Data mining, Statistics, Algorithms, Python, Java, R, Data structures, Data modeling, Software architecture, Mathematics, Statistics","artificial intelligence, machine learning, data science, natural language processing, predictive modeling, data analysis, data mining, statistics, algorithms, python, java, r, data structures, data modeling, software architecture, mathematics, statistics","algorithms, artificial intelligence, data mining, data science, data structures, dataanalytics, datamodeling, java, machine learning, mathematics, natural language processing, predictive modeling, python, r, software architecture, statistics"
Data Warehouse Analyst,CXtec,"Syracuse, NY",https://www.linkedin.com/jobs/view/data-warehouse-analyst-at-cxtec-3771578799,2023-12-17,Syracuse,United States,Mid senior,Onsite,"For more than 40 years, CXtec has been helping organizations maximize value in their IT investments. We are the largest secondary market network hardware provider in North America. Today, we remain focused on providing creative life cycle management solutions for organization of all sizes, all over the world.
The annual base salary range for this position, based out of Syracuse, NY, is $78,735 – $96,383. The actual salary offered will vary based on applicant’s relevant skills, education, experience, certifications and geographic location as well as other business and organizational needs.
This position is located in Syracuse, NY and requires onsite work.
Summary:
We are looking for a data-driven problem solver to join our IT team as we strengthen our reporting and analytics capabilities. This position supports high visibility projects that transforms raw datasets into consumable tools that stakeholders can use to drive meaningful impact across the organization. The successful candidate will have a fine eye for detail, and should be skilled in designing data collection methods, which could involve creating data pipelines, ETL (Extract, Transform, Load) processes, and other data acquisition techniques using Snowflake and other similar tools.
What you'll be doing:
Responsible for designing, building, and supporting the components of data warehouse, such as ETL processes, databases, reports, and reporting environments utilizing Azure Data Lake, Snowflake and Microsoft Power BI.
Create and maintain stored procedures, functions, and triggers to provide solutions to business problems.
Maintain deep, complex knowledge of data structure and the interaction of data across multiple systems of record, including the primary ERP database and ancillary applications and platforms.
Accountable for understanding the business goals and the business use cases, translating them to data processing and data modeling and championing the business stakeholders’ objectives.
Assume additional responsibilities as required.
What we're looking for:
Bachelor's degree in Computer Science, Data Science, MIS, Finance or related field.
3 + years of prior Snowflake SQL with Oracle and T-SQL with Microsoft SQL Server Database experience.
Experience working in an Enterprise Data Warehouse environment.
Experience with API based data acquisition and management.
Understanding of Master Data Management/Data Governance.
Superior analytical, critical thinking and problem-solving skills with demonstrated ability to identify problems, use logic to address and recommend alternative solutions.
Ability to organize and manage multiple priorities, while demonstrating strong attention to detail and appreciation of deadlines.
Strong interpersonal, written, and verbal communication skills.
What we offer:
Health insurance and prescription drug coverage
Dental and Vision insurance
Company-paid life insurance and voluntary life options
Company paid short and long-term disability
Critical illness, accident and hospital indemnity insurance
Flexible spending accounts
Pet insurance
Traditional and Roth 401(k) retirement plan with company match
Tuition reimbursement and Employee Assistance Program
Paid vacation, sick/personal days and holidays
As a condition of employment, upon offer, candidates will be required to participate in a background check and complete a pre-employment drug screen.
We are proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to age, race, creed, color, national origin, religion, pregnancy, reproductive health decisions, sexual orientation, protected veteran or military status, sex, disability, marital or familial status, domestic violence victim status, gender identity or expression, predisposing genetic characteristics, criminal history or any other legally protected class. Individuals will not be discriminated against on the basis of disability.
Show more
Show less","Snowflake, Azure Data Lake, Power BI, SQL, Oracle, TSQL, Microsoft SQL Server, ETL, Data Warehouse, Master Data Management, Data Governance, API, Communication","snowflake, azure data lake, power bi, sql, oracle, tsql, microsoft sql server, etl, data warehouse, master data management, data governance, api, communication","api, azure data lake, communication, data governance, datawarehouse, etl, master data management, microsoft sql server, oracle, powerbi, snowflake, sql, tsql"
Senior Data Analyst,Robert Half,"Syracuse, NY",https://www.linkedin.com/jobs/view/senior-data-analyst-at-robert-half-3785769351,2023-12-17,Syracuse,United States,Mid senior,Remote,"Description
Remote role! As long as you are a NY State resident, you can be considered.
Chris Preble is working with a client he has placed a lot of people at before. This is another newly created position for this rapidly growing organization. Solid benefits and of course, you work from home.
SQL experience is a must have.
Responsibilities
Lead the effort to standardize and optimize internal reporting processes
Collaborate with cross-functional teams to identify key metrics and KPIs for different business units
Design and implement robust data architectures to support the company's analytics needs.
Develop and maintain data models to ensure accurate and consistent reporting
Utilize strong SQL skills to extract, transform, and analyze data from various sources
Translate business requirements into actionable analytics strategies
Collaborate effectively with stakeholders
Understand the unique analytics needs of departments and provide tailored solutions
Understand product requirements from external client's perspective and provide the right framing solution for analytics services
Other adhoc projects
Requirements
Must have current data analysis experience
Must have SQL experience
Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.
Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.
All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit
© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to
Show more
Show less","SQL, Data Analysis, Data Architectures, Data Models, Analytics Strategies, Stakeholder Collaboration, Business Requirements, Product Requirements, Adhoc Projects","sql, data analysis, data architectures, data models, analytics strategies, stakeholder collaboration, business requirements, product requirements, adhoc projects","adhoc projects, analytics strategies, business requirements, data architectures, data models, dataanalytics, product requirements, sql, stakeholder collaboration"
Senior Data Analyst (SQL),National Grid,"Syracuse, NY",https://www.linkedin.com/jobs/view/senior-data-analyst-sql-at-national-grid-3785551134,2023-12-17,Syracuse,United States,Mid senior,Hybrid,"About Us
National Grid is hiring a Senior Data Analyst (SQL) for our Billing Department in Syracuse office.
This is a Hybrid Role - Must live within a commutable distance to Syracuse, NY.
Every day we deliver safe and secure energy to homes, communities, and businesses. We are there when people need us the most. We connect people to the energy they need for the lives they live. The pace of change in society and our industry is accelerating and our expertise and track record puts us in an unparalleled position to shape the sustainable future of our industry.
To be successful we must anticipate the needs of our customers, reducing the cost of energy delivery today and pioneering the flexible energy systems of tomorrow. This requires us to deliver on our promises and always look for new opportunities to grow, both ourselves and our business.
Job Purpose
Analyze and interpret data, procedures, activities, and pertinent documents to provide relevant and up to date information to support decision-making and the effective working of National Grid.
Design and implement procedures and policies within their area of work to improve the working of National Grid.
Key Accountabilities
Independently collate, analyze and interpret work unit operational and/or financial data, procedures, activities, and pertinent business documents within one or multiple areas using pre-determined tools, methods and formats in order to support the department's decision making process.
Prepare and write relevant documentation in order that all stakeholders are fully informed and all evidence or statistics are documented.
Execute instructions and requests, plan and organize resources and make appropriate arrangements to ensure that work is carried out efficiently and in line with relevant policies and processes.
Maintain and provide information such as regulatory or business essential information in order to ensure regulators and other stakeholders are kept up to date with relevant National Grid policies, processes and programs.
Build upon technical / professional skills within own area of specialism or of other processes in order to continually grow your personal contribution to the business.
Manage the day to day relationship with relevant internal parties or employees in order to understand their requirements, deliver appropriate, customized solutions and advice, and build cross-functional working in line with National Grid policies and processes.
Develop and maintain effective relationships with key stakeholders, internal and external, in order to share best practice provide technical advice and build bases of influence.
Develop and maintain analytical systems, models and procedures in order to support design solutions and enhance time / consistency / safety / efficiency and operability.
Challenge existing ways of working and continuously seek ways to do things better in order to drive greater efficiencies within assigned area.
Research and analyze key themes from a wide range of data sources in order to identify how different scenarios may impact upon the business.
Design and implement new processes or policies in order to ensure all working practice (and where relevant, health and safety standards) are compliant with National Grid policies and regulatory requirements.
Translate business objectives into clearly defined business cases, annual plans and objectives in order to support achievement of departmental/operational area. .
Qualifications
Knowledge & Experience Requirements:
A Bachelor's degree in a business, systems, or financial area and up to 5 years of related experience, or equivalent work experience.
Proficient in relational database querying techniques (SQL) required.
Extensive and significant knowledge in one or more of customer account management, invoicing, financial structures, energy management
Able to define and manage work of others through project assignments or clerical operations
Proficient in Microsoft Office products (Excel, Word, PowerPoint, Access).
Experience and proficiency in relevant company related IT systems
Capability Requirements:
Analytical Thinking: Analyzes and interprets operational procedures, activities, and pertinent business documents toward logical conclusions and expected outcomes (identifying several likely causes or consequences of a situation)
Impact and Influence: Uses direct persuasion to influence others, using basic data, logic or a solid business case
Customer Orientation: Takes personal responsibility for correcting problems promptly and undefensively and communicates customer expectations to monitor delivery and satisfaction
Attention to Detail : Reviews accuracy of own work and checks that all details are completed
Conceptual Thinking: Sees patterns or trends in data or situations, notices when something is similar to a past situation
Information Seeking: Establishes the facts by digging deeper, asking probing questions and challenging initial responses from different sources
More Information
Salary
$76,000 - $89,000 a year
Salary commensurate with location and experience
This position has a career path which provides for advancement opportunities within and across bands as you develop and evolve in the position; gaining experience, expertise and acquiring and applying technical skills. Candidates will be assessed and provided offers against the minimum qualifications of this role and their individual experience.
National Grid is an equal opportunity employer that values a broad diversity of talent, knowledge, experience and expertise.  We foster a culture of inclusion that drives employee engagement to deliver superior performance to the communities we serve.  National Grid is proud to be an affirmative action employer. We encourage minorities, women, individuals with disabilities and protected veterans to join the National Grid team.
Show more
Show less","SQL, Data analysis, Data interpretation, Microsoft Office Suite (Excel Word PowerPoint Access), Customer account management, Invoicing, Financial structures, Energy management, Project management, Clerical operations, Analytical thinking, Impact and influence, Customer orientation, Attention to detail, Conceptual thinking, Information seeking","sql, data analysis, data interpretation, microsoft office suite excel word powerpoint access, customer account management, invoicing, financial structures, energy management, project management, clerical operations, analytical thinking, impact and influence, customer orientation, attention to detail, conceptual thinking, information seeking","analytical thinking, attention to detail, clerical operations, conceptual thinking, customer account management, customer orientation, data interpretation, dataanalytics, energy management, financial structures, impact and influence, information seeking, invoicing, microsoft office suite excel word powerpoint access, project management, sql"
Business Data Analyst,"KPH HEALTHCARE SERVICES, INC","East Syracuse, NY",https://www.linkedin.com/jobs/view/business-data-analyst-at-kph-healthcare-services-inc-3774991482,2023-12-17,Syracuse,United States,Mid senior,Hybrid,"Scope of Responsibilities
The Business Data Analyst
will partner cross functionally to understand underlying business issues and will provide data and reporting that will be used to improve process and profitability across the organization. As a Business Data Analyst, you will be responsible to review and interpret business data to provide summary reports/conclusions to various departments as well as working with management to improve efficiencies using all available technological resources. Business Data Analyst will also present the findings and recommendations of analysis to ProAct’s senior leadership team.
Job Summary
Analyze and review claims data at both a client and book of business level and provide information that will drive business decisions. Partner with department leaders to analyze data elements to identify opportunities to optimize performance across these departments and provide summary findings to senior leadership team. Create automated solutions to improve efficiencies across departments.
Partner with Director of Finance and Analytics to help develop and maintain internal reporting portal
Build out automated solutions using SQL to solve complex reporting and calculating needs
Utilize Machine Learning to solve for complex problems
Provide data/reporting from SQL upon request from various departments
Utilize Python or similar coding language to automate manual tasks
Collaborate with various department managers to identify, develop and maintain reporting needs
Creation of key metrics dashboards that will be included in internal reporting portal
Recommend systems process improvements to maximize data gathering efficiency
Assist with annual budget preparation and weekly and monthly forecasting
Collaborates with Finance to automate tasks and maximize time/efficiencies
Create reporting/query to track retail, mail and specialty network pricing performance
Create reporting/query on individual client performance to ensure contractual optimization
Maintains appropriate documentation of applications and business processes
Provide and analyze dashboard reporting for ProAct Pharmacy Services regarding overall business performance and medication margins
Use predictive modeling to identify possible fraudulent claim activity
Shares knowledge with peers and others and supports knowledge management efforts
Participates in developing individual skill sets to meet today and tomorrow’s business needs
Responsible for completing all mandatory and regulatory training programs
Perform other duties as assigned
Education
Required Bachelor’s Degree or higher in Business, Accounting or Finance
Experience
Preferred 2-3 or more years previous experience in related position
Preferred Experience in coding SQL and Python – knowledge of Web Design a plus
Must possess knowledge of security concepts applicable to this role and function.
Special Conditions of Employment
Criminal background check and/or drug test
Initial and continuous exclusion and sanction/disciplinary monitoring
Must sign and agree to the Elevated Privileges Acceptable User Agreement
Compensation
$63,000-66,592.00 per year
The final offer will be determined after careful consideration of multiple factors such as relevant skills, years of experience and education.
KPH Healthcare Services, Inc. is a multistate organization and abides by all local, state and federal regulations as it pertains to minimum wage requirements.
Show more
Show less","SQL, Python, Tableau, Data analysis, Reporting, Machine learning, Predictive modeling, Business intelligence, Data visualization, Web design, Security","sql, python, tableau, data analysis, reporting, machine learning, predictive modeling, business intelligence, data visualization, web design, security","business intelligence, dataanalytics, machine learning, predictive modeling, python, reporting, security, sql, tableau, visualization, web design"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Saint-Lambert-de-Lévis, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3752014050,2023-12-17,Quebec, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Data Interpretation, Reporting, Data Visualization, Programming (SQL R Python), Data Management, ETL Processes, Data Quality, Data Cleansing, Data Manipulation, Data Collection, A/B Testing, DataDriven Decision Making, Statistical Modeling, Hypothesis Testing, Communication, Collaboration","data analysis, statistical techniques, data interpretation, reporting, data visualization, programming sql r python, data management, etl processes, data quality, data cleansing, data manipulation, data collection, ab testing, datadriven decision making, statistical modeling, hypothesis testing, communication, collaboration","ab testing, collaboration, communication, data collection, data interpretation, data management, data manipulation, data quality, dataanalytics, datacleaning, datadriven decision making, etl, hypothesis testing, programming sql r python, reporting, statistical modeling, statistical techniques, visualization"
0644 - Développeur Big Data,agileDSS,"Québec, Quebec, Canada",https://ca.linkedin.com/jobs/view/0644-d%C3%A9veloppeur-big-data-at-agiledss-3478542471,2023-12-17,Quebec, Canada,Mid senior,Onsite,"agileDSS est une société d'origine canadienne spécialisée dans la Data. Ayant son siège social à Montréal avec des antennes à Québec et la France, nous accompagnons depuis près de 20 ans les grandes entreprises françaises et canadiennes dans la mise en place d'écosystème analytique unique et adapté.
Rejoindre agileDSS, c'est rejoindre une équipe entrepreneuriale qui place l'humain et le développement au cœur de ses priorités, à travers des projets où notre expertise sur des sujets liés notamment à la BI, au Cloud, au Data Engineering et au DataOps est reconnue, tant pour notre excellence que pour notre approche humaine et innovante.
L'opportunité
Que ce soit pour accompagner l'un de nos clients ou bâtir l'agileDSS de demain, nous recherchons continuellement des personnes ambitieuses, ayant à la fois la fibre entrepreneuriale et la passion de la data pour nous rejoindre.
Aujourd'hui, nous recherchons un
Développeur Big Data
pour pour combler une opportunité à titre
de Conseiller Permanent
pour notre bureau de
Montréal ou Québec.
Ce que vous ferez :
Participer, en collaboration avec les concepteurs, à l’élaboration et à la réalisation, des solutions ;
Mettre en œuvre des règles d’affaires de traitement et de valorisation des données corporatives et sectorielles (entrepôts, comptoirs, données maitresses et de référence, etc.) ;
Participer à des activités de nettoyage, de transformation, de conversion et de correction des données ;
Réaliser des tests durant la modification des applications, pour garantir l’intégrité opérationnelle des solutions de données ;
Participer à la mise en production itérative des solutions, travailler avec des équipes des différentes unités d’affaires pour établir et exécuter les tâches nécessaires, selon un ordre bien établi ;
Travailler en collaboration avec les autres professionnels TI et communiquer avec les différents partenaires afin de clarifier et bien comprendre leurs besoins ;
Participer à la définition des fonctions, services ou éléments techniques requis ;
Réaliser, documenter et valider les analyses demandées ;
Respecter les règles fonctionnelles et techniques ainsi que les normes de conception et de livraison de solutions ;
Participer ou réaliser des essais à différents niveaux de détails, qu’ils soient fonctionnels ou techniques ;
En contexte de livraison et de mise en production, soutenir les partenaires lors des essais d’acceptation ;
Participer à l’implantation des solutions retenues ;
Participer au plan d’évolution des applications et systèmes technologiques.
Compétences et profil recherchés :
5 années d’expérience pertinente en Big Data ;
Une solide expérience de travail de conception et d’optimisation de pipelines de données sur de grands volumes de données structurées et semi structurées ;
Expérience pertinente avec Azure Synapse ou Snowflake, ADF, etc. ;
Bonne connaissance de la gestion des données, de l'intégration des données et des techniques de développement des bases de données;
Expérience des technologies de base de données (par exemple, SQL, NoSQL, Oracle, Hadoop, Snowflake, Teradata, Databricks Delta Lake) ;
Expérience de travail avec les systèmes de base de données dans le cloud comme Google Big Query, Snowflake, Redshift ou équivalent ;
Connaissance dans l'utilisation d'outils de CI/CD (Azure DevOps, Jenkins, Travis CI, GitLab CI) et de versioning Git (GitHub, BitBucket) ;
Bilingue français et anglais (un atout : certains de nos clients offrent un milieu de travail bilingue)
Expérience sur des plateformes Cloud telles que Microsoft Azure, AWS ou GCP - Atout
Certification Databricks, Azure Synapse, Snowflake - Atout
Mais n’oublions pas le savoir-être !
Avant toute chose on cherche un nouveau collègue qui va s’épanouir dans notre environnement! Si tu es une personne qui aime apprendre et partager ses connaissances, qui comprend le service-conseil, qui possède une certaine autonomie, et qui est drôle et bienveillant tu as de grandes chances de te plaire chez nous !
Ce qu’on t’offre
Nous sommes avant tout une équipe de collaborateurs à taille humaine qui a à cœur le bien-être de ses employés,
c’est pourquoi nous t’offrons:
Une rémunération annuelle fixe et une bonification annuelle selon l’atteinte de ta performance
Des objectifs individuels accessibles et réalistes pour garantir un environnement sans pression
Des assurances collectives (télé-médecine, dentaire, vision, soins...)
Des Jours Personnelles - Maladies progressif selon l'ancienneté
Semaines de vacances progressif selon l'ancienneté + les jours de Noel
Remboursement des abonnements de sport, art et culture
Remboursement du transport à 100% (métro, bus, voiture,...)
Horaires flexibles et télétravail
Plan de développement pour chaque employé et coaching avec un mentor
Environnement non hiérarchique favorisant l’intrapreneuriat
Activités mensuelles de team building
Et bien d’autres encore !
agileDSS : Votre prochaine bonne décision.
Show more
Show less","Azure Synapse, Snowflake, Databricks Delta Lake, Google Big Query, Redshift, Azure DevOps, Jenkins, Travis CI, GitLab CI, GitHub, BitBucket, SQL, NoSQL, Oracle, Hadoop, Teradata, Microsoft Azure, AWS, GCP, Certification Databricks, Azure Synapse, Big Data, Data Engineering, DataOps, Business Intelligence, Cloud Computing","azure synapse, snowflake, databricks delta lake, google big query, redshift, azure devops, jenkins, travis ci, gitlab ci, github, bitbucket, sql, nosql, oracle, hadoop, teradata, microsoft azure, aws, gcp, certification databricks, azure synapse, big data, data engineering, dataops, business intelligence, cloud computing","aws, azure devops, azure synapse, big data, bitbucket, business intelligence, certification databricks, cloud computing, data engineering, databricks delta lake, dataops, gcp, github, gitlab ci, google big query, hadoop, jenkins, microsoft azure, nosql, oracle, redshift, snowflake, sql, teradata, travis ci"
Sr Software Developer (Data Harvesting),TalentNeuron,"Québec, Quebec, Canada",https://ca.linkedin.com/jobs/view/sr-software-developer-data-harvesting-at-talentneuron-3744400701,2023-12-17,Quebec, Canada,Mid senior,Hybrid,"TalentNeuron is a top-of-the-line HR analytics enterprise software-as-a-service product that utilizes a vast array of public data sets, including profiles, jobs, companies, universities, demographics, and skills. The product leverages cutting-edge cloud, big data, and open source technologies, frameworks, and toolsets. TalentNeuron stands as the world's foremost provider of global talent demand and supply data, offering real-time insights and predictive analytics on the job market, location, and competitive intelligence to assist clients in making informed human capital decisions, streamlining workforce access, and advancing corporate diversity, equity, and inclusion initiatives. Our clients include market leaders like Southwest, Nestle, Verizon, Facebook as well as hundreds of large and midsized enterprise companies, and that list is growing quickly.
TalentNeuron is an enterprise SaaS-based HR Analytics product built on a plethora of public datasets like Profiles, Jobs, Companies, Universities, demographic, and skills. The product is built on cutting-edge cloud, big data, and open-source technologies, frameworks, and toolsets. We are seeking talented & motivated Senior Software Developers to join the Harvesting Engineering (HE) team.
As a Senior Software Developer on the HE team, you will be responsible for designing, developing, and maintaining the Harvesting Engine of our software products. You will work closely with other software developers, product managers, and stakeholders to ensure that our applications and services meets the needs of our customers & platform and is scalable, reliable, and efficient.
What you will do
:
Participate in design and implementation of high-performing, scalable and optimized data solutions
Design, build and automate the deployment of our Harvesting Engineering applications
Scale mature microservice platforms using Java and strong command of AWS cloud services (e.g., ECS, Lamba)
Work with team on managing AWS resources (ECS/EKS clusters, etc.) and continuously improve deployment process of our applications
Work with administrative resources and support provisioning, monitoring, configuration and maintenance of AWS tools, S3, & Aurora RDBMS
Promote the integration of new cloud technologies and continuously evaluate new tools that will improve the organization’s capabilities while leading to lower total cost of operation
Support automation efforts across the team utilizing Infrastructure as Code (IaC) using Terraform, Configuration Management, and Continuous Integration (CI) / Continuous Delivery (CD) tools such as Jenkins
Work with the team to implement data governance, access control and identify and reduce security risks
What you will need:
Bachelor's or master’s degree in computer science, Information Systems, Engineering or related technical fields.
5-8 years’ experience in software development, including significant experience in Big Data and Cloud Services
Strong Experience in Java Spring Boot
Hands on experience in working with RDBMS DB like Postgres, MySQL
Expertise in developing back-end applications and microservices
Experience with AWS services such as S3, ECS, EKS, Lambda, Step Functions
Ability to support DevOps with service deployment & administration
Experience building monitoring capabilities for large scale backend applications
Good understanding and exposure to Terraform
A plus if you have experience with:
○ HTML parsing frameworks, web-scraping, proxies and Headless browsing.
○ data archival and working with unstructured data.
○ Exposure to languages like Python, C#
Good combination of technical and interpersonal skills with strong written and verbal communication; detail-oriented with the ability to work independently
Takes initiative on improvements and testing results
Owner mindset – identify, communicate, and act on issues and initiatives
Ability to handle multiple tasks and projects simultaneously in an organized and timely manner
Detail oriented, with the ability to plan, prioritize, and meet deadlines in a fast-paced environment
Ability to work independently, as well as part of a team
Experience working with fast-paced operations/dev teams and DevOps
What you will get:
A collaborative, team-oriented culture that embraces diversity & inclusion
Professional development and unlimited growth opportunities
Competitive salary, paid time off policy, and more!
This is an exciting opportunity to join a dynamic and growing software company as a Senior Software Developer. If you have a passion for leading the development of scalable, reliable, and efficient middleware components and services, and have 5+ years of experience in Backend development, we encourage you to apply for this role.
Show more
Show less","Cloud, Big Data, Open Source, Software Development, Java, Spring Boot, RDBMS, SQL, Postgres, MySQL, Microservices, DevOps, AWS, S3, ECS, EKS, Lambda, Terraform, Data Governance, Security","cloud, big data, open source, software development, java, spring boot, rdbms, sql, postgres, mysql, microservices, devops, aws, s3, ecs, eks, lambda, terraform, data governance, security","aws, big data, cloud, data governance, devops, ecs, eks, java, lambda, microservices, mysql, open source, postgres, rdbms, s3, security, software development, spring boot, sql, terraform"
Junior Data Analyst (Part-time),City of Corona,"Corona, CA",https://www.linkedin.com/jobs/view/junior-data-analyst-part-time-at-city-of-corona-3774489128,2023-12-17,Buena Park,United States,Mid senior,Onsite,"Description
The City of Corona has an exciting opportunity to join our Information Technology Department as a
Junior Data Analyst.
The Junior Data Analyst performs a variety of professional-level duties and assists in data analysis, programming, developing, and implementing solutions using various enterprise application data sources, and databases to generate data analysis reports, dashboards, files, and other informational requirements of City departments, other agencies, and the public; and performs related duties, as assigned.
Skills
This position requires you to work a maximum of 19 hours per week. The successful candidate will possess the following knowledge, skills, and abilities:
Knowledge
Knowledge of data analysis fundamentals including concepts, classifications, and terminology.
SQL for querying and manipulating data from relational databases.
Datamining techniques and tools, including machine learning, algorithms, and data modeling.
Data analysis tools such as Python, R, or similar programming languages.
Knowledge of data and statistical techniques and tools, including data cleaning, extraction, transformation, loading and visualization.
Skills & Abilities
Ability to understand, evaluate, and translate the needs of Data users into systems requirements.
Use programming skills and data analysis tools or equivalent software.
Exercise independent judgement within general policy guidelines.
Ability to approach data-related problems logically and systematically.
Strong attention to detail when working with data to ensure accuracy and reliability.
Education & Experience:
Possession of a valid Class C California Driver’s License and the ability to maintain insurability under the City’s Vehicle Insurance Policy.
Equivalent to a bachelor’s degree from an accredited college or university with major coursework in geographic information systems, computer science, information systems, business administration, or a related field.
OR
Four (4) years of increasingly responsible experience in data analysis or a related field.
Characteristics
Innovator- someone that can introduce new methods and ideas to our department.
Tech Savvy- passion for technology.
A self-starter with a can-do attitude.
A collaborative contributor who possesses intrinsic motivation, unwavering commitment, ambitious drive, an insatiable curiosity, adaptability, and reliability.
This position is FLSA Non-Exempt, eligible for overtime compensation.
To View The Positions Complete Job Description, Click Here.
Application Process
Applications will be accepted on a continuous basis until a sufficient number of qualified applications have been received. The deadline for first review of application is
Sunday
, December 17, 2023, at 5:00PM.
Applicants are encouraged to apply early as the recruitment may close at any time without notice after the first review period.
Candidates that successfully pass the screening process will be invited to a virtual oral interview tentatively scheduled for January 4, 2024. The top scoring candidates from the oral interview will be placed on an eligibility list from which the department may hire. The eligibility list established from this recruitment may be used to fill the current and/or additional vacancies throughout the City.
The successful candidate will be required to undergo a reference / background check including livescan and pass a pre-employment medical examination (which will include a drug/alcohol screening).
Community
A City whose heritage spans more than a century, the City of Corona, located in Riverside County, California, encompasses around 40 square miles and has a vibrant, culturally diverse population of approximately 168,100 residents. Residents of Corona enjoy a variety of cultural, recreational, and educational opportunities that provide a first-rate quality of life for its residents and visitors. Within an hour's travel time, outdoor enthusiasts can enjoy such winter activities as snowboarding and skiing and such summer activities as surfing, boating, or simply relaxing on a beach. Corona's economy is strong and getting stronger; more than 84,000 people work here. Our retail, commercial, and housing areas are expanding, and office development continues to be strong.
The Organization
The City of Corona is a General Law city operating under a Council-Manager form of government. Policymaking and legislative authority is vested in the City Council, consisting of a Mayor and four Council Members. Members of the City Council are elected by District to a four-year term. The Mayor is selected by the Council and serves as the presiding officer for one year.
The City is organized into the following departments/organizational units: Community Services, City Manager’s Office, Economic Development, Finance, Fire, Human Resources, Information Technology, Legal and Risk Management, Police, Planning & Development, Public Works and Utilities. Oversight of these departments is divided between the City Manager and two Assistant City Managers.
The City's total operating budget for Fiscal Year 2023 is $340.9 million with a General Fund operating budget of $177.6 million. Corona enjoys a workforce of 850 dedicated employees who provide high quality services to its residents and visitors each day.
Essential Duties And Responsibilities
To view all essential duties and responsibilities, click here.
Qualifications Guidelines
To view all qualification guidelines, click here.
PHYSICAL/MENTAL DEMANDS AND WORK ENVIRONMENT
To view the Physical/Mental Demands and Work Environment, click here .
Show more
Show less","Data analysis fundamentals, SQL, Python, R, Machine learning, Algorithms, Data modeling, Data cleaning, Data extraction, Data transformation, Data loading, Data visualization, Data mining, Data manipulation techniques and tools, Statistical techniques and tools","data analysis fundamentals, sql, python, r, machine learning, algorithms, data modeling, data cleaning, data extraction, data transformation, data loading, data visualization, data mining, data manipulation techniques and tools, statistical techniques and tools","algorithms, data analysis fundamentals, data cleaning, data extraction, data loading, data manipulation techniques and tools, data mining, data transformation, datamodeling, machine learning, python, r, sql, statistical techniques and tools, visualization"
"Data Engineer -Los Angeles, CA",Tata Consultancy Services,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-engineer-los-angeles-ca-at-tata-consultancy-services-3774068459,2023-12-17,Buena Park,United States,Mid senior,Onsite,"Job Title:
Data engineer (on-site in Los Angeles, CA)
Relevant Experience
(in Yrs)
6-8 Yrs
Must Have Technical/Functional Skills
Candidate should be strong in Snowflake and Redshift
Should have AWS experience
Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Interpret trends and patterns
Conduct complex data analysis and report on results
Prepare data for prescriptive and predictive modeling
Build algorithms and prototypes
Combine raw information from different sources
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition
Develop analytical tools and programs
Collaborate with data scientists and architects on several project
Experience Required
6-8 Yrs
Roles & Responsibilities
Candidate should be strong in Snowflake and Redshift
Analyze and organize raw data
Build data systems and pipelines
Evaluate business needs and objectives
Interpret trends and patterns
Conduct complex data analysis and report on results
Prepare data for prescriptive and predictive modeling
Build algorithms and prototypes
Combine raw information from different sources
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition
Develop analytical tools and programs
Collaborate with data scientists and architects on several project
Generic Managerial Skills
NA
Education
5+ years of leading technical team, BE, B-tech.
Show more
Show less","Snowflake, Redshift, AWS, Data Analysis, Data Systems, Data Pipelines, Business Needs, Business Objectives, Trends, Patterns, Predictive Modeling, Data Acquisition, ETL, Data Quality, Analytical Tools, Programming, Data Science, Data Architecture","snowflake, redshift, aws, data analysis, data systems, data pipelines, business needs, business objectives, trends, patterns, predictive modeling, data acquisition, etl, data quality, analytical tools, programming, data science, data architecture","analytical tools, aws, business needs, business objectives, data acquisition, data architecture, data quality, data science, data systems, dataanalytics, datapipeline, etl, patterns, predictive modeling, programming, redshift, snowflake, trends"
Sr. Data Engineer,Fanatics,"Los Angeles, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-fanatics-3743194557,2023-12-17,Buena Park,United States,Mid senior,Onsite,"Company Overview
Fanatics is building a leading global digital sports platform. The company ignites the passions of global sports fans and maximizes the presence and reach for hundreds of sports partners globally by offering innovative products and services across Fanatics Commerce, Fanatics Collectibles, and Fanatics Betting & Gaming, allowing sports fans to Buy, Collect and Bet. Through the Fanatics platform, sports fans can buy licensed fan gear, jerseys, lifestyle and streetwear products, headwear, and hardgoods; collect physical and digital trading cards, sports memorabilia, and other digital assets; and bet as the company builds its Sportsbook and iGaming platform. Fanatics has an established database of over 100 million global sports fans, a global partner network with over 900 sports properties, including major national and international professional sports leagues, teams, players associations, athletes, celebrities, colleges, and college conferences, and over 2,000 retail locations, including its Lids retail business stores.
As a market leader with more than 18,000 employees, and hundreds of partners, suppliers, and vendors worldwide, we take responsibility for driving toward more ethical and sustainable practices. We are committed to building an inclusive Fanatics community, reflecting and representing society at every level of the business, including our employees, vendors, partners and fans. Fanatics is also dedicated to making a positive impact in the communities where we all live, work, and play through strategic philanthropic initiatives.
We are a startup building products to transform the trading card industry. We aren’t a startup in the traditional sense, though. While we
are
a small team building new products from the ground up, we also have the backing of Fanatics – the world’s largest sports merchandiser with over 900 sports relationships and more than 81 million reachable fans. We have exclusive licensing deals with the MLB, NFL and NBA and the products we build will be used by millions of trading cards fans from day 1. We are out to reinvent the trading card industry, which is a bold and ambitious mission.
Fanatics Collectible’s Data Engineering, Science, and Analytics Team is hiring experienced data engineers. You will be among the founding members of our team. You will help build a world-class data engineering organization with a strong data and software engineering culture. You will also establish, advocate, and execute data strategy and technical capabilities that support the business and enable data-driven decisions. Ideally, you are passionate about the trading card industry, sports, and importantly, data and technology. Our data engineers are required to partner very closely with other engineering and business teams and understand our business needs. You will own high impact technical decisions and lead hiring efforts to help build out a world class data engineering team.
What does this mean as a member of the engineering team?
We are fans-and-collectors-obsessed and product-focused. We work backward from the focus of fans and collectors to drive our technical work and the development of products (physical and digital) that will reach tens of million.
We are agile and move at warp speed. We work hard and are not afraid to try, experiment, and pivot (as needed). We have a lot to do and are looking for folks who will drive projects to completion with a sense of urgency, but not at the expense of quality, scalability, and performance.
We think about scale. The trading card market is massive, and we will be the de facto entry point into the hobby.
We’re pragmatic. We embrace new technologies if they add business value.
We are passionate about trading cards, sports, and data. We take data seriously and work tirelessly to ensure their highest possible quality. We build data infrastructure to enable the seamless flow data from source to impactful actions.
Qualifications
A computer science or equivalent experience required.
5+ years of experience as a data engineer or software engineer focusing on building data infrastructure, solving complex technical challenges at a large scale, and delivering data products that drive business impact
Experience in architect, develop, deploy, and monitor a new, end-to-end data infrastructure leveraging cloud technologies that ingests data from a wide range of internal and external sources and enables user-friendly consumption of data for data-driven decisions
Experience in designing data architecture for multiple database models such as RDBMS, document, columnar, graph, and key-value data stores
Expertise in both SQL and Python
Proficiency in Java and JavaScript
Proficiency and both SQL and NoSQL database technologies
Well-verse in modern cloud native data tech stacks
Familiar with container tech such as Docker and Kubernetes
Familiar with data and ML orchestration tools
Experience in designing and implementing modern CI/CD pipelines with cloud native tools
Strong computer science fundamentals and data/software engineering hygiene
Excellent communication skills to be an effective bridge among our team, other engineering teams, and business partners/stakeholders
Key Responsibilities Include, But Not Limited To,
Research, architect, develop, deploy, monitor, and maintain an efficient, reliable, and secure cloud data infrastructure to enable data-driven decisions
Collaborate with internal and external tech teams and stakeholders/business partners
Research and integrate 3rd-party vendor data solutions
Help develop, implement, and evolve our data strategy, which will include data governance, data security, and tech roadmap
Establish and participate in on-call technical support
Contribute to data engineering and science team recruitment process
Actively mentor and coach team members on advanced technical methods, advocate best practices, and help others to grow their skill set
Set a high standard for data/software engineering excellence through example
Develop a strong understanding of Fanatics Collectibles and the trading cards industry
$160,000 - $200,000 a year
The salary range for this position is
$160,000-$200,000
, which represents base pay only and does not include short-term or long-term incentive compensation. When determining base pay, as part of a final compensation package, we consider several factors such as location, experience, qualifications, and training.
Ensure your Fanatics job offer is legitimate and don’t fall victim to fraud. Fanatics never seeks payment from job applicants. Feel free to ask your recruiter for a phone call or other type of communication for interview, and ensure your communication is coming from a Fanatics or Fanatics Brand email address. For added security, where possible, apply through our company website at www.fanaticsinc.com/careers
Tryouts are open at Fanatics! Our team is passionate, talented, unified, and charged with creating the fan experience of tomorrow. The ball is in your court now.
Fanatics is committed to responsible planning and purchasing (RPP) practices, working with its business partners across its global and multi-layered supply chain, to ensure that planning, sourcing, and purchasing decisions, along with other supporting processes, do not impede or conflict with the fulfillment of Fanatics’ fair labor practices.
NOTICE TO CALIFORNIA RESIDENTS/APPLICANTS
: In connection with your application, we collect information that identifies, reasonably relates to or describes you (“Personal Information”). The categories of Personal Information that we collect include your name, government issued identification number(s), email address, mailing address, other contact information, emergency contact information, employment history, educational history, criminal record, and demographic information. We collect and use those categories of Personal Information about you for human resources and other business management purposes, including identifying and evaluating you as a candidate for potential or future employment or other types of positions, recordkeeping in relation to recruiting and hiring, conducting criminal background checks as permitted by law, conducting analytics, and ensuring compliance with applicable legal requirements and Company policies. For additional information on how we collect and use personal information in connection with your job application, review our Candidate Privacy Policy-CA
Show more
Show less","Data engineering, Software engineering, Cloud technologies, SQL, Python, Java, JavaScript, NoSQL, Cloud native data tech stacks, Docker, Kubernetes, Data and ML orchestration tools, CI/CD pipelines, Data governance, Data security, Technical support, Mentoring, Coaching, Data strategy, DevOps, Agile, Scalability, Performance","data engineering, software engineering, cloud technologies, sql, python, java, javascript, nosql, cloud native data tech stacks, docker, kubernetes, data and ml orchestration tools, cicd pipelines, data governance, data security, technical support, mentoring, coaching, data strategy, devops, agile, scalability, performance","agile, cicd pipelines, cloud native data tech stacks, cloud technologies, coaching, data and ml orchestration tools, data engineering, data governance, data security, data strategy, devops, docker, java, javascript, kubernetes, mentoring, nosql, performance, python, scalability, software engineering, sql, technical support"
Senior Data Analytics Engineer,Fastly,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-analytics-engineer-at-fastly-3775563049,2023-12-17,Buena Park,United States,Mid senior,Onsite,"Fastly helps people stay better connected with the things they love. Fastly’s edge cloud platform enables customers to create great digital experiences quickly, securely, and reliably by processing, serving, and securing our customers’ applications as close to their end-users as possible — at the edge of the Internet. The platform is designed to take advantage of the modern internet, to be programmable, and to support agile software development. Fastly’s customers include many of the world’s most prominent companies, including Vimeo, Pinterest, The New York Times, and GitHub.
We're building a more trustworthy Internet. Come join us.
Senior Data Analytics Engineer
The Analytics team is building a modern data stack to cultivate data-driven decision-making across our high growth business. We are looking for a Senior Analytics Engineer to join our Analytics team, who is inquisitive, curious, and passionate about solving complex business problems with analysis. This role will provide the opportunity to work closely with business partners and build KPIs, dashboards, pipelines, and exploratory analysis to empower a data-driven culture across the company.
What You'll Do
You’ll collaborate and partner with executives, stakeholders, and decision-makers to prioritize, design, and fulfill their data analytics needs
You’ll identify KPIs driving business growth and performance and work closely with the business teams to define them
You’ll work across the data stack to design and build various data products (data models, dashboards, analysis) that enable the business to make data backed decisions
You’ll help garner trust in data from the business by documenting analytics best practices, assisting with data discoverability, governance, and accuracy
You’ll promote and drive a self-service data culture by building easy to use analytics products and teaching the business users how to use them
You’ll communicate progress, risks, and completion of projects to stakeholders
What We're Looking For
You have 5+ years of hands-on experience in BI or Analytics teams.
You have hands-on experience in building and managing data transformations (dbt experience is a plus)
You are able to craft functional and user-friendly data products (data models, dashboards, etc..) for both technical and non-technical stakeholders
You have experience with cloud data warehouse technologies (BigQuery is a plus)
You have experience with a data visualization tools (ie. Looker, Tableau, Mode)
You are a SQL expert, capable of complex, efficient, and logical SQL to accomplish data analysis goals
You are driven by logical and analytical thinking to solve problems. And have an eye for detail-oriented analysis and design
You have functional knowledge of financial, sales, or other core business metrics
You are able to manage professional relationships across a variety of stakeholders, including executives
You are able to identify and prioritize multiple requirements and manage expectations accordingly
You are collaborative and willing to traverse and contribute to the entire data stack, from front-end to back-end as needed
Work Hours:
This position will require you to be available during core business hours.
Work Location(s) & Travel Requirements:
This position is open to the following preferred office locations:
San Francisco, CA USA
Denver, CO USA
New York, NY USA
Culver City, CA USA
Fastly currently embraces a largely hybrid model for most roles which allows employees flexibility to split their time between the office and home.
This position may require travel as required by your role or requested by your manager.
Salary
The estimated salary range for this position is $143,860 to $179,820.
Starting salary may vary based on permissible, non-discriminatory factors such as experience, skills, qualifications, and location.
This role may be eligible to participate in Fastly’s equity and discretionary bonus programs.
Benefits
We care about you. Fastly works hard to create a positive environment for our employees, and we think your life outside of work is important too. We support our teams with great benefits that start on the first day of your employment with Fastly. Curious about our offerings?
We offer a comprehensive benefits package including medical, dental, and vision insurance. Family planning, mental health support along with Employee Assistance Program, Insurance (Life, Disability, and Accident), an open vacation policy and up to 18 days of accrued paid sick leave are there to help support our employees. We also offer 401(k) (including company match) and an Employee Stock Purchase Program. For 2023, we offer 10 paid local holidays, 11 paid company wellness days.
Why Fastly?
We have a huge impact. Fastly is a small company with a big reach. Not only do our customers have a tremendous user base, but we also support a growing number of open source projects and initiatives. Outside of code, employees are encouraged to share causes close to their heart with others so we can help lend a supportive hand.
We love distributed teams. Fastly’s home-base is in San Francisco, but we have multiple offices and employees sprinkled around the globe. As a new hire, you will be able to attend our IN-PERSON new hire orientation in our San Francisco office! It is an exciting week-long experience that we offer to new employees to build connections with colleagues across Fastly, participate in hands-on learning opportunities, and immerse yourself in our culture firsthand.
We value diversity. Growing and maintaining our inclusive and diverse team matters to us. We are committed to being a company where our employees feel comfortable bringing their authentic selves to work and have the ability to be successful -- every day.
We are passionate. Fastly is chock full of passionate people and we’re not ‘one size fits all’. Fastly employs authors, pilots, skiers, parents (of humans and animals), makeup geeks, coffee connoisseurs, and more. We love employees for who they are and what they are passionate about.
We’re always looking for humble, sharp, and creative folks to join the Fastly team. If you think you might be a fit please apply!
A fully completed application and resume or CV are required when applying.
Fastly is committed to ensuring equal employment opportunity and to providing employees with a safe and welcoming work environment free of discrimination and harassment. Our employment decisions are based on business needs, job requirements and individual qualifications.
All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, family or parental status, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
Consistent with the Americans with Disabilities Act (ADA) and federal or state disability laws, Fastly will provide reasonable accommodations for applicants and employees with disabilities. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact your Recruiter, or the Fastly Employee Relations team at
candidateaccommodations@fastly.com
or 501-287-4901.
Fastly collects and processes personal data submitted by job applicants in accordance with our Privacy Policy. Please see our privacy notice for job applicants.
Show more
Show less","Data Analytics, Business Intelligence, Data Visualization, SQL, Data Modeling, Dashboards, Data Warehousing, BigQuery, Looker, Tableau, Mode, dbt, Cloud Data Warehouse, Business Metrics, Financial Analysis, Sales Analysis, Stakeholder Management, Requirements Gathering, Project Management, Communication, Collaboration, Problem Solving, DetailOriented Analysis, Logical Thinking, Analytical Thinking","data analytics, business intelligence, data visualization, sql, data modeling, dashboards, data warehousing, bigquery, looker, tableau, mode, dbt, cloud data warehouse, business metrics, financial analysis, sales analysis, stakeholder management, requirements gathering, project management, communication, collaboration, problem solving, detailoriented analysis, logical thinking, analytical thinking","analytical thinking, bigquery, business intelligence, business metrics, cloud data warehouse, collaboration, communication, dashboard, dataanalytics, datamodeling, datawarehouse, dbt, detailoriented analysis, financial analysis, logical thinking, looker, mode, problem solving, project management, requirements gathering, sales analysis, sql, stakeholder management, tableau, visualization"
DevOps Engineer Senior Specialist [NTT DATA],CareerBeacon,"Victoria, IL",https://www.linkedin.com/jobs/view/devops-engineer-senior-specialist-ntt-data-at-careerbeacon-3670773535,2023-12-17,Galesburg,United States,Mid senior,Onsite,"Req ID:
246859
NTT DATA Services strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a DevOps Engineer Senior Specialist to join our team in Victoria, British Columbia (CA-BC), Canada (CA).
Candidate should have skills with, and a minimum of three years of experience in, software development, operations, and full stack development, including iterative code and script development.
This should include skills and experience with all of the following:
Data warehousing and administration of storage solutions;
Infrastructure code;
Container deployment, management, scaling and networking;
GitHub actions and workflows
CI/CD pipelines
Cloud native web frameworks
Application metrics, monitoring, and scaling
Candidate Should Have Experience In Designing And Building Client-side And Server-side Web Applications Using Modern Methods Of Software Development And Have a Detailed Understanding Of Front-end Web Development, Including
Developing, testing and maintaining build system, CI/CD pipelines and tools
Experience working on projects with an iterative (rapid product development, Agile Scrum Process) and user experience focus
Experience with application deployment and containerization
Automating and streamlining deployment processes
Experience with continuous deployment environments
Experience in software testing and test automation In addition to technical skills, the Full Stack Developer #1 (Dev Ops Specialist) must have experience with:
Mentoring and supporting less experienced developers
Fostering the team-wide adoption of best development practices
Developing team processes to guarantee code quality (including, but not limited to maintainability, testability, reusability, readability, performance)
Required
The
Full Stack Developer
#1 (
Dev Ops Specialist
) is required to have experience in at least one of the following, and the Province will give preference to a Full Stack Developer #1 (Dev Ops Specialist) who has experience with more than one of the following:
CI/CD pipeline frameworks
SQL- based ETL processes
12factor.net adherent application development
GitHub actions
GitHub container registry
Red Hat OpenShift
Best practices of versioning management tools, specifically Git
Build optimisation
SonarCloud
CodeQL
OWASP ZAP
ArgoCD
Vault
Mapping technologies (e.g. ArcGIS, Leaflet)
Cloud storage, including Google Cloud Storage
Kubernetes
Docker
Directed Acyclical Graphs (DAGs)
Specialist knowledge of Helm, Terraform; and
Senior-level knowledge of Python, Javascript, Typescript, SQ
#Launchjobs
About NTT DATA Services
NTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients' long-term success. Visit nttdata.com or LinkedIn to learn more.
NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
Show more
Show less","Agile Scrum, DevOps, Infrastructure code, Container management, Git, GitHub, CI/CD, Cloud native web frameworks, Data warehousing, Storage solutions, Kubernetes, Docker, Helm, Terraform, Python, Javascript, Typescript, SQL, 12factor.net, Google Cloud Storage, ArgoCD, Vault, SonarCloud, CodeQL, OWASP ZAP, Mapping technologies, Directed Acyclical Graphs (DAGs)","agile scrum, devops, infrastructure code, container management, git, github, cicd, cloud native web frameworks, data warehousing, storage solutions, kubernetes, docker, helm, terraform, python, javascript, typescript, sql, 12factornet, google cloud storage, argocd, vault, sonarcloud, codeql, owasp zap, mapping technologies, directed acyclical graphs dags","12factornet, agile scrum, argocd, cicd, cloud native web frameworks, codeql, container management, datawarehouse, devops, directed acyclical graphs dags, docker, git, github, google cloud storage, helm, infrastructure code, javascript, kubernetes, mapping technologies, owasp zap, python, sonarcloud, sql, storage solutions, terraform, typescript, vault"
Data Analyst III,Kern Health Systems,"Bakersfield, CA",https://www.linkedin.com/jobs/view/data-analyst-iii-at-kern-health-systems-3758264303,2023-12-17,Bakersfield,United States,Mid senior,Onsite,"We appreciate your interest in our organization and assure you that we are sincerely interested in your qualifications. A clear understanding of your background and work history will help us potentially place you in a position that meets your objectives and those of the organization. Qualified applicants are considered for positions without regard to race, color, religion, sex (including pregnancy, childbirth and breastfeeding, or any related medical conditions), national origin, ancestry, age, marital or veteran status, sexual orientation, gender identity, genetic information, gender expression, military status, or the presence of a non-job related medical condition or disability (mental or physical).
**Telecommute Position**
KHS reasonably expects to pay starting compensation for the Data Analyst III position in the range of $41.08 - 53.27 / Hour
General Purpose:
This position is responsible for ongoing analysis of health care information, including patient care, financial, and demographic data. The Data Analyst III position is one of the primary interfaces for complex reporting, analysis, data integrity, and report design and delivery for Kern Health Systems (KHS). A Data Analyst III should be proficient in problem solving. This position requires advanced experience in the development of database schemas and table definition of an RDBMS. The incumbent will provide professional skills necessary for complex report writing and presentation. The Data Analyst III must have a comprehensive understanding and ability of the structured query language (SQL) and end user reporting and data presentation.
It requires use of analytical tools (SQL, Business Objects, Excel, SAS), proficient and comprehensive in a data analysis methodology, use of presentation software, strong communication skills and a strong commitment to and affinity for delivering the highest level of customer service.
Perform complex data analysis in support of ad-hoc and planned customer requests. Works with customers to develop and understand analysis and report specifications. The Data Analyst III delivers extensive and in depth data products in report, presentation format, or verbally, to customer specifications and timelines with thoughtful and accurate analysis of data.
Essential Duties and Responsibilities:
Ability to address proposed research questions that are used to provide the business with accurate information in order to make operational business decisions.
Develops programs, methodologies, and files for analyzing and presenting data.
Highly organized individual that is willing to accept responsibility, shows good judgment, initiative, and resourcefulness, and is able to work independently, under supervision, or in a mentoring capacity.
Perform complex data analysis in support of ad-hoc and planned customer requests to deliver sophisticated analysis products, accurate and appropriate interpretation of data.
Ability to analyze health care data from conceptualization through presentation of the data with analytical tools (SAS, Excel, etc.) using a data analysis methodology.
Develop useful and insightful information from a variety of data sources.
Develop and prepare information products for associated staff and members.
Responsible for ongoing analysis of health care information, obtained from a variety of sources, including patient care, financial, and demographic data.
Works with the Governed Reporting System (GRS) team to create information rich reports utilizing the KHS operation data store or data warehouse and providing statistical verification of results.
The position works directly with internal business partners and management for data collection, statistical analysis, and documentation.
Expertise in creating formulas, calculations, and graphs in Microsoft Excel.
RDBMS - Thorough understanding of the techniques used in relational Database Management Systems.
SQL – Demonstrated ability in creating complex SQL queries language statements for analytical data mining. Use of SQL to create basic statistical analysis.
Data Mining – Competent with data mining activities and data analysis techniques in healthcare data, and the ability to identify problems, develop logical conclusions, and provide effective solutions.
Report Generation - Ability to work with end users to identify reporting requirements.
Statistical Analysis - Understanding of providing statistical analysis, verification of sample sizes required to develop a statistically valid model. Correct uses of sampling tests, statistical analysis tests and metrics.
Provide compliance reporting for the State of California.
Interacts with business leads in review of new concepts and ideas, project implementation, and problem resolution.
Understand extract, load, and translate (ETL) processes and database population.
Create and maintain documentation for the Business Development team, reporting, methodologies, and other areas as needed.
Work and communicate effectively with the various professionals throughout the organization.
Resolves elevated help desk incidents, change management requests, or problem tickets as assigned.
Employment Standards:
Bachelors degree in Computer Science, Mathematics, or related field from an accredited institution or equivalent and four (4) years progressive experience in report writing or reporting analytics preferably for a medium to large health care organization
OR
Eight (8) years of experience as an Analyst with KHS or similar organization with an emphasis on business analytics. Education may be substituted with report writing or analytical work experience on a year for year basis, and will be reviewed individually.
The Data Analyst III position is suited for a mid-career analytical person.
A mathematical background with experience in developing statistical models is helpful.
An understanding of managed care and the benefits of a centralized standard reporting team for an organization is highly recommended. Candidate must be a highly organized.
Other:
Possession of valid California Driver’s License and proof of valid State required auto liability insurance.
Required Travel Up to 10%.
We are an equal opportunity employer, dedicated to a policy of non-discrimination in employment on any basis.
Apply Now
Show more
Show less","SQL, Business Objects, Excel, SAS, Database Management Systems, Statistical Analysis, Mathematical Modeling, Managed Care, Data Mining, Data Analysis, ETL, Presentation Software, Communication Skills, Problem Solving, Report Writing, Report Design, Report Delivery","sql, business objects, excel, sas, database management systems, statistical analysis, mathematical modeling, managed care, data mining, data analysis, etl, presentation software, communication skills, problem solving, report writing, report design, report delivery","business objects, communication skills, data mining, dataanalytics, database management systems, etl, excel, managed care, mathematical modeling, presentation software, problem solving, report delivery, report design, report writing, sas, sql, statistical analysis"
Big Data Engineer,ICONMA,"O'Fallon, MO",https://www.linkedin.com/jobs/view/big-data-engineer-at-iconma-3762690948,2023-12-17,Missouri,United States,Mid senior,Onsite,"Big Data Engineer
Location: OFallon, MO
Duration: 12 months with Possible Extension/ with Possible Contract to hire
Description:
As a Senior Software Engineer in Data Platform & Engineering Services team, youll hold a valued role within a rapidly growing team inside one of the worlds most successful organizations, working closely with experienced and passionate engineers to solve problems customer problem.
You will be partnering with the data engineering teams, so the ability to influence and provide operational guidance is key.
Initially, the Developer focus will be contributing to the development of operational tools and practices that help maintain service availability across hosted and cloud-based infrastructure.
You must understand the full stack and how systems are built as well as a grasp of operational best practices.
Role:
As a member of the Unified Data Acquisition and Processing (UDAP) platform team, you will be responsible for building tools and systems that deploy and scale our applications and data in hybrid cloud and physical environments.
We enable the platform that helps teams across multiple programs to build, test, deploy and host hundreds of data pipelines across several global datacenters along with enterprise logging, monitoring and vulnerability detection.
All about You/Experience:
Experience in Data Warehouse related projects in product or service based organization
Experience solving for Scalability, Performance and Stability
Experience in a programming language in Java, Scala or Python
Experience working in SQL and relational databases
Operational experience in Big Data Stacks (Spark and Hadoop ecosystem)
Expert knowledge of Linux operating systems and environment and Scripting
A deep expertise in your field of Software Engineering
Expert at troubleshooting complex system and application stacks
Operational experience in Elastic Search (ELK stack) would be a plus
Operational experience troubleshooting network/server communication is a plus
Motivation, creativity, selfdirection, and desire to thrive on small project teams
Strong written and verbal English communication skills
What is the name of your group? How does that fit into the overall Client organization :
Data Enablement team, UDAPP Unified Data Acquisition and Processing Platform
What program will this person be supporting? Will this person be a part of a Guild If so, which one and how will they be contributing:
Yes, SDE
What is your teams main responsibility:
Our team is responsible for building out data pipelines and processes for data movement from sources to their new warehouse.
How would you describe the culture of your team:
What we look for is the person should be able to self research and can be autonomous in this position.
Cannot depend on others to perform their work.
Need the ability to research by oneself.
They would work with product owners (data scientist) and enterprise architecture team, and data engineers.
Would you please describe your management style:
Hands on because was in a technical position previously, has thorough understanding.
It's a small team but we all have great knowledge and experience.
I'm open for all suggestions and changes to streamline/reorganize our approach to grow the business.
We are building this platform and scaling it, but now we have lots of demand to use our platform.
We are trying to take it to the next phase, and produce an additional platform to process cloud processing.
What will a typical work day look like for this contractor:
We do an alternating standup schedule, and their daily work will depend on their story.
What are your top 3 required technical skills :
Java
Hadoop
Spark
What are a couple of desired/nice to have skills:
Cloud Applications such as Azure and/or AWS
What soft skills would you like to see in a candidate:
Ability to research
Work autonomous
Willing to learn
What level of competency is required :
Intermediate to Advanced
Show more
Show less","Java, Hadoop, Spark, SQL, Linux, Scripting, Software Engineering, Troubleshooting, Elastic Search (ELK stack), Cloud Applications, Azure, AWS","java, hadoop, spark, sql, linux, scripting, software engineering, troubleshooting, elastic search elk stack, cloud applications, azure, aws","aws, azure, cloud applications, elastic search elk stack, hadoop, java, linux, scripting, software engineering, spark, sql, troubleshooting"
Azure Data Engineer,CoxHealth,"Springfield, MO",https://www.linkedin.com/jobs/view/azure-data-engineer-at-coxhealth-3729763601,2023-12-17,Missouri,United States,Mid senior,Onsite,"Job Summary
The Azure Data Engineer works directly with the Microsoft EDW Development team, supporting the CoxHealth System. To be successful in this position, the Engineer must have strong communication skills and the ability to collaborate with different analysts/developers across the team. This position is focused on the development of data management solutions within the Microsoft Azure Synapse environment.
Job Requirements
Education
Required: Bachelor’s Degree in Information Technology, Computer Science, or other related discipline OR 8 years' experience in IT or Computer Science
Experience
Required: 1 year experience in one of the following areas:
Experience with cloud-based technologies, preferably Azure, Data Factory, Databricks, Synapse Analytics, Azure Key Vault
Experience with Spark, preferably in a Databricks notebook-based development environment
Experience designing Data Lakes and Data Warehouses
Experience leveraging, designing, and building database schemas for analytics
Experience designing, building, and maintaining ETL/ELT data pipelines, preferably SSIS & Data Factory using multiple data source types like custom API sources, file based, RDBMs systems (SQL Server, Oracle, Snowflake, etc.)
Skills
Develop, test and configure automated deployments of data management solutions (data ingestion, transformation, modeling, interfaces, etc.)
Understand SQL query optimization, profiling, and performance monitoring tools & techniques
Excellent communication skills required with ability to effectively collaborate with members of the technical team and with business leaders in a fully-remote or hybrid capacity.
Must be very detailed oriented, with excellent organizational skills with the ability to manage/prioritize multiple tasks concurrently.
Licensure/Certification/Registration
None Required
Show more
Show less","Azure, Data Factory, Databricks, Synapse Analytics, Azure Key Vault, Spark, Data Lakes, Data Warehouses, ETL/ELT, SSIS, SQL Server, Oracle, Snowflake, SQL query optimization, Data profiling, Performance monitoring, Communication skills, Collaboration, Organizational skills, Prioritization","azure, data factory, databricks, synapse analytics, azure key vault, spark, data lakes, data warehouses, etlelt, ssis, sql server, oracle, snowflake, sql query optimization, data profiling, performance monitoring, communication skills, collaboration, organizational skills, prioritization","azure, azure key vault, collaboration, communication skills, data factory, data lakes, data profiling, data warehouses, databricks, etlelt, oracle, organizational skills, performance monitoring, prioritization, snowflake, spark, sql query optimization, sql server, ssis, synapse analytics"
Lead Data Engineer,Professional Diversity Network,"Missouri, United States",https://www.linkedin.com/jobs/view/lead-data-engineer-at-professional-diversity-network-3785205041,2023-12-17,Missouri,United States,Mid senior,Onsite,"Job Title:
Data Engineering Lead
Overview:
As the Lead Data Engineer, you will lead design and development of enhanced data services and cloud enablement for Data and Services. You will push the limits of the state of the art with best-in-class data product enablement in the cloud and will design the roadmap develop data & analytics solutions that sit atop vast datasets. Your will enable a frictionless customer and developer experience, exposing data components for direct to customer use or as components for new product constructs. You will have the opportunity to create high performance analytic and ML solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data.
Overview
As the Lead Data Engineer, you will:
Lead the design of new data capabilities and infrastructure related to accessing and using MA, third party, and partner data to power Mastercard data products and solutions.
Design required new data pipes, data transfers, and compliance related infrastructure to enable that use of the data in the cloud.
Identify data-related capabilities and infrastructure requirements resulting from new and evolving product constructs and how those requirements can be developed cloud natively.
Understand the Data and Services business strategy, information infrastructure and data needs for both new and existing products and services across various product initiatives.
Collaborate with relevant data management, data governance and technical teams to enable long term viability of information assets and enterprise requirements.
Develop a horizontal technology roadmap in conjunction with Data Strategy & Management team input to enable technology solutions for managing data that meet the needs of multiple business stakeholder groups.
Identify existing data capability and infrastructure gaps or opportunities within and across initiatives and provide subject matter expertise in support of remediation.
All About You
Hands-on experience with technologies such as Python, Databricks, AWS, Java, Pyspark, Spark, Kafka, and SQL.
You are curious about and motivated by the future trends in data, AI/ML, analytics, and digital experience
Experience in data product development, analytical models, and model governance
Experience in anonymizing data and managing the use of data
Experience in data hygiene procedures, identity resolution capabilities or data management a plus
Exceptional interpersonal skills with proven experience in relationship building and partnering. Must work well in both team and individual settings and must be able to work with a geographically dispersed team.
Strong analytic capabilities and written and oral communication skills. Attention to detail is a must.
Strong project management skills and a demonstrated ability to understand complex information product constructs
Familiarity with industry best practices for collection and use of data
Motivated self-starter with ability to excel at multi-tasking in a fast-paced environment, outstanding English written and verbal communication skills
9a4ed076-6aa1-4317-ba96-40fcfb11142e
Show more
Show less","Data Engineering, Python, Databricks, AWS, Java, Pyspark, Spark, Kafka, SQL, Data Product Development, Analytical Models, Model Governance, Data Hygiene, Identity Resolution, Data Management, Interpersonal Skills, Relationship Building, Project Management, Complex Information Product Constructs, Data Collection, Data Use, English Communication","data engineering, python, databricks, aws, java, pyspark, spark, kafka, sql, data product development, analytical models, model governance, data hygiene, identity resolution, data management, interpersonal skills, relationship building, project management, complex information product constructs, data collection, data use, english communication","analytical models, aws, complex information product constructs, data collection, data engineering, data hygiene, data management, data product development, data use, databricks, english communication, identity resolution, interpersonal skills, java, kafka, model governance, project management, python, relationship building, spark, sql"
Business Data Analyst II,Simmons Bank,"Columbia, MO",https://www.linkedin.com/jobs/view/business-data-analyst-ii-at-simmons-bank-3777030290,2023-12-17,Missouri,United States,Mid senior,Onsite,"It's fun to work in a company where people truly BELIEVE in what they're doing!
We're committed to bringing passion and customer focus to the business.
The Business Data Analyst II position is a member of the Customer Service Support Team that supports reporting needs for the Customer Contact Center and ATM Channel. The role is responsible for processing and analyzing data, as needed, data from API, reports, documents, and other processes. The analyst will follow best practices in using source control and ensure data integrity throughout the process.
The Business Data Analyst will need the ability to understand various data sources and their contextual meaning as needed to support analysis and determine which data is needed for various requests.
Essential Duties And Responsibilities
Performing analysis using various tools and techniques as needed
Extracting data from various systems and sources into usable data
Serve as a resource in determining the approach to be utilized in implementing new analysis and technology
Estimate project complexity and timeframes
Identify and escalate issues as they are identified
Ensures all data management analysis and reporting activities are documented and maintained.
Responsible for providing back up to other Customer Service Support Analysts
Ensures all departmental documents and activities are performed in compliance with applicable laws, regulations, policies, and procedures as applicable to this position, including completion of required compliance training.
Performs other duties and responsibilities as assigned.
Qualifications
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required.
Skills
Ability to ingest data from various formats and perform complex analysis using different tools appropriate for the task
Ability to read and comprehend simple instructions, short correspondence, and memos
Ability to write complex reports, simple-to-business correspondence, and procedures
Ability to effectively speak, respond to questions, and present information to the following: one-on-one/small groups, employees, managers, and top management
Ability to apply critical thinking, using logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems.
Ability to work independently as well as collaboratively with a customer-service orientation.
Ability to read and interpret documents such as procedures manuals, general business correspondence and/or journals or government regulations.
Ability to maintain effective interpersonal relationships with management and team members.
Ability to read and interpret documents such as procedure manuals, general business correspondence and/or journals or government regulations
Education and/or Experience
High school diploma or its equivalent is required, and
4 Years of experience in a professional setting, preferably at an institution of higher education, and
Experience in data analytics and reporting
Computer Skills
MS Office programs
Microsoft Excel
Microsoft Power BI
Python is a plus
Certificates, Licenses, Registrations
Other Qualifications (including physical requirements)
Strong oral and written communication skills
Strong organizational, problem solving, and planning skills with the ability to set priorities and meet deadlines
Ability and desire to learn and use new software applications
Other
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Activities, duties and responsibilities may change at any time with or without notice.
Equal Employment Opportunity Information: Simmons First National Corporation and its subsidiaries are committed to a policy of equal employment with respect to a person's race, color, religion, sex, ancestry, sexual orientation, gender identity, national origin, covered veterans, military status, physical or mental disability or any other legally protected classifications. Simmons First National Corporation and its subsidiaries are committed to Affirmative Action Programs consisting of results-oriented procedures to ensure equal employment opportunities. These programs require positive action in lieu of neutral non-discrimination and merit hiring/performance policies.
Show more
Show less","Data Analysis, Data Extraction, Data Interpretation, Data Management, Data Reporting, Microsoft Office, Microsoft Excel, Microsoft Power BI, Python, Source Control, Project Management, Communication, Problem Solving, Planning, Time Management, Critical Thinking, Teamwork, Customer Service, SQL","data analysis, data extraction, data interpretation, data management, data reporting, microsoft office, microsoft excel, microsoft power bi, python, source control, project management, communication, problem solving, planning, time management, critical thinking, teamwork, customer service, sql","communication, critical thinking, customer service, data extraction, data interpretation, data management, data reporting, dataanalytics, microsoft excel, microsoft office, microsoft power bi, planning, problem solving, project management, python, source control, sql, teamwork, time management"
AWS Lead Data Engineer - Remote,Perficient,"Missouri, United States",https://www.linkedin.com/jobs/view/aws-lead-data-engineer-remote-at-perficient-3782267513,2023-12-17,Missouri,United States,Mid senior,Remote,"Job Description
We currently have a career opportunity for an AWS Data Engineer to join our Data Solutions Team. This is a
REMOTE
position and candidates can live anywhere in the US.
Why Work for Data Solutions
Wide range of clients, technologies, and applications: We work in a number of different industries, such as financial services, life sciences / pharma, healthcare, utilities, retail, and insurance. In addition to learning about each industry’s business processes, you are exposed to new technologies and different ways to apply them.
Broad Opportunities: The Data Solutions team works with data technologies at all touchpoints, from collection to usage. Our practice groups include Artificial Intelligence, Content, Data Governance, Data Management, Data Strategy and Search
Project Support: Your whole team is at your disposal for assistance .
Fast Growing: We’ve had explosive growth in the last three years to meet the demands of a massive transition to digital , and we continue to grow.
As an AWS Data Engineer, you will participate in all aspects of the software development lifecycle which includes estimating, technical design, implementation, documentation, testing, deployment and support of application developed for our clients. As a member working in a team environment, you will take direction from solution architects and leads on development activities.
Responsibilities
Participate in technical planning & requirements gathering phases including Design, code, test, troubleshoot, and document engineering software applications.
Ensuring that technical software development process is followed on the project, familiar with industry best practices for software development.
Develop applications in languages including Lamba, Glue, API Gateway, Kenesis, Postgres SQL, Kenesis, OpenForce, Python, Terraform
Demonstrate the ability to adapt and work with team members of various experience level.
Qualifications
5+ years as a Data Engineer working in a fast-paced team environment.
Must live or be able to travel to Charlotte, NC once per quarter. Travel expenses are paid by Perficient.
Hands on experience to production with AWS to include technologies like Lambda, Glue, API Gateway, Postgres SQL, Kenesis, OpenForce, Python, Terraform etc.
Strong debugging, problem solving and investigative skills. Ability to assimilate disparate information (log files, error messages etc.) and pursue leads to find root cause problems.
Experience with Agile/Scrum methodology.
Self-starter who can work independently.
Bachelor’s Degree in MIS, Computer Science, Math, Engineering or comparable major.
Strong consulting and communication skills. Ability to work effectively with various organizations in pursuit of problem solutions.
The salary range for this position takes into consideration a variety of factors, including but not limited to skill sets, level of experience, applicable office location, training, licensure and certifications, and other business and organizational needs. The new hire salary range displays the minimum and maximum salary targets for this position across all US locations, and the range has not been adjusted for any specific state differentials. It is not typical for a candidate to be hired at or near the top of the range for their role, and compensation decisions are dependent on the unique facts and circumstances regarding each candidate. A reasonable estimate of the current salary range for this position is $64,974 to $149,880. Please note that the salary range posted reflects the base salary only and does not include benefits or any potential equity or variable bonus programs. Information regarding the benefits available for this position are in our benefits overview .
Who We Are
Perficient is a leading global digital consultancy. We imagine, create, engineer, and run digital transformation solutions that help our clients exceed customers’ expectations, outpace competition, and grow their business. With unparalleled strategy, creative, and technology capabilities, our colleagues bring big thinking and innovative ideas, along with a practical approach to help our clients – the world’s largest enterprises and biggest brands succeed.
What We Believe
At Perficient, we promise to challenge, champion, and celebrate our people. You will experience a unique and collaborative culture that values every voice. Join our team, and you’ll become part of something truly special.
We believe in developing a workforce that is as diverse and inclusive as the clients we work with. We’re committed to actively listening, learning, and acting to further advance our organization, our communities, and our future leaders… and we’re not done yet.
Perficient, Inc. proudly provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, genetic information, marital status, amnesty, or status as a protected veteran in accordance with applicable federal, state and local laws. Perficient, Inc. complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. Perficient, Inc. expressly prohibits any form of unlawful employee harassment based on race, color, religion, gender, sexual orientation, national origin, age, genetic information, disability, or covered veterans. Improper interference with the ability of Perficient, Inc. employees to perform their expected job duties is absolutely not tolerated.
Disability Accommodations:
Perficient is committed to providing a barrier-free employment process with reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or accommodation due to a disability, please contact us.
Disclaimer:
The above statements are not intended to be a complete statement of job content, rather to act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time.
About Us
Perficient is always looking for the best and brightest talent and we need you! We’re a quickly growing, global digital consulting leader, and we’re transforming the world’s largest enterprises and biggest brands. You’ll work with the latest technologies, expand your skills, experience work-life balance, and become a part of our global community of talented, diverse, and knowledgeable colleagues.
Select work authorization questions to ask when applicants apply
1. Are you legally authorized to work in the United States?
2. Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)?
Show more
Show less","AWS Data Engineer, Agile/Scrum, Lambda, Glue, API Gateway, Postgres SQL, Kenesis, OpenForce, Python, Terraform, Software Development Lifecycle, Technical Planning, Requirements Gathering, Design, Code, Test, Troubleshooting, Documentation, Software Applications, Industry Best Practices, Debugging, Problem Solving, Investigative Skills, Log Files, Error Messages, Bachelor's Degree, Consulting Skills, Communication Skills","aws data engineer, agilescrum, lambda, glue, api gateway, postgres sql, kenesis, openforce, python, terraform, software development lifecycle, technical planning, requirements gathering, design, code, test, troubleshooting, documentation, software applications, industry best practices, debugging, problem solving, investigative skills, log files, error messages, bachelors degree, consulting skills, communication skills","agilescrum, api gateway, aws data engineer, bachelors degree, code, communication skills, consulting skills, debugging, design, documentation, error messages, glue, industry best practices, investigative skills, kenesis, lambda, log files, openforce, postgres sql, problem solving, python, requirements gathering, software applications, software development lifecycle, technical planning, terraform, test, troubleshooting"
Senior Database Engineer,Mastercard,"Missouri, United States",https://www.linkedin.com/jobs/view/senior-database-engineer-at-mastercard-3785147023,2023-12-17,Missouri,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Senior Database Engineer
The database engineering team is looking for a Senior Database Engineer to help set direction for MasterCard’s use and best practices for our application database solutions. You will work with application teams to implement sound RDMS data solutions that follow industry best practices and methods.
You will also be responsible for migrating existing databases to AWS. You will design, automate, and manage databases as a service leveraging deep experience in Open-Source database technologies like MySQL, Aurora, and other Cloud Native database technologies.
Role
Optimize and maintain legacy systems in MySQL & MongoDB
Implement differing approaches to data management & administration in AWS
Partner with dev teams to create DevSecOps constructed databases in AWS
Develop plans to migrate legacy on-premises databases into AWS data stores
Engineer various database services across multiple AWS services and regions
Be THE escalation point for solving database usage issues and malfunction
All About You
Deep experience with database engineering
Experience sharding, clustering, HA, scaling, monitoring, deploying, tuning, etc.
Experience being the escalation point for any and all DBE/DBA issues
Sr. experience in cloud databases in production, development, and QA environments
Work Conditions
Rotational pager for Production Support
Tech Stack
AWS, Aurora MySQL, MongoDB
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Show more
Show less","MySQL, Aurora MySQL, MongoDB, AWS, Relational Database Management System (RDMS), Data Administration, DevSecOps, Cloud databases, Data sharding, Clustering, High availability (HA), Scaling, Monitoring, Deployment, Tuning, Database engineering","mysql, aurora mysql, mongodb, aws, relational database management system rdms, data administration, devsecops, cloud databases, data sharding, clustering, high availability ha, scaling, monitoring, deployment, tuning, database engineering","aurora mysql, aws, cloud databases, clustering, data administration, data sharding, database engineering, deployment, devsecops, high availability ha, mongodb, monitoring, mysql, relational database management system rdms, scaling, tuning"
Lead Data Engineer,Mastercard,"Missouri, United States",https://www.linkedin.com/jobs/view/lead-data-engineer-at-mastercard-3765721300,2023-12-17,Missouri,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Data Engineering Lead
As the Lead Data Engineer, you will lead design and development of enhanced data services and cloud enablement for Data and Services. You will push the limits of the state of the art with best-in-class data product enablement in the cloud and will design the roadmap develop data & analytics solutions that sit atop vast datasets. Your will enable a frictionless customer and developer experience, exposing data components for direct to customer use or as components for new product constructs. You will have the opportunity to create high performance analytic and ML solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data.
Overview
As the Lead Data Engineer, you will:
Lead the design of new data capabilities and infrastructure related to accessing and using MA, third party, and partner data to power Mastercard data products and solutions.
Design required new data pipes, data transfers, and compliance related infrastructure to enable that use of the data in the cloud.
Identify data-related capabilities and infrastructure requirements resulting from new and evolving product constructs and how those requirements can be developed cloud natively.
Understand the Data and Services business strategy, information infrastructure and data needs for both new and existing products and services across various product initiatives.
Collaborate with relevant data management, data governance and technical teams to enable long term viability of information assets and enterprise requirements.
Develop a horizontal technology roadmap in conjunction with Data Strategy & Management team input to enable technology solutions for managing data that meet the needs of multiple business stakeholder groups.
Identify existing data capability and infrastructure gaps or opportunities within and across initiatives and provide subject matter expertise in support of remediation.
All About You
Hands-on experience with technologies such as Python, Databricks, AWS, Java, Pyspark, Spark, Kafka, and SQL.
You are curious about and motivated by the future trends in data, AI/ML, analytics, and digital experience
Experience in data product development, analytical models, and model governance
Experience in anonymizing data and managing the use of data
Experience in data hygiene procedures, identity resolution capabilities or data management a plus
Exceptional interpersonal skills with proven experience in relationship building and partnering. Must work well in both team and individual settings and must be able to work with a geographically dispersed team.
Strong analytic capabilities and written and oral communication skills. Attention to detail is a must.
Strong project management skills and a demonstrated ability to understand complex information product constructs
Familiarity with industry best practices for collection and use of data
Motivated self-starter with ability to excel at multi-tasking in a fast-paced environment, outstanding English written and verbal communication skills
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Pay Ranges
O'Fallon, Missouri: $138,000 - $221,000 USD
Show more
Show less","Python, Databricks, AWS, Java, Apache Spark, Kafka, SQL, AI, ML, Data hygiene, Identity resolution, Data management, Data governance, Data product development, Analytical models, Model governance, Data anonymization, Interpersonal skills, Relationship building, Partnering","python, databricks, aws, java, apache spark, kafka, sql, ai, ml, data hygiene, identity resolution, data management, data governance, data product development, analytical models, model governance, data anonymization, interpersonal skills, relationship building, partnering","ai, analytical models, apache spark, aws, data anonymization, data governance, data hygiene, data management, data product development, databricks, identity resolution, interpersonal skills, java, kafka, ml, model governance, partnering, python, relationship building, sql"
Senior Software Engineer / Data Engineer,Mastercard,"Missouri, United States",https://www.linkedin.com/jobs/view/senior-software-engineer-data-engineer-at-mastercard-3776146666,2023-12-17,Missouri,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Senior Software Engineer / Data Engineer
Overview
Job Description Summary
Mastercard is a global technology company behind the world’s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless®. We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.
As a Sr. Data Engineer in Data Platform and Engineering Services, you will have the opportunity to build high performance data pipelines to load into Mastercard Data Warehouse. Our Data Warehouse provides analytical capabilities to a number of business users who help different customers provide answers to their business problems through data. You will play a vital role within a rapidly growing organization, while working closely with experienced and driven engineers to solve challenging problems.
Role
Develop high quality, secure and scalable data pipelines using spark, Java/Scala on object storage and Hadoop
Follow MasterCard Quality Assurance and Quality Control processes
Leverage new technologies and approaches to innovating with increasingly large data sets
Work with project team to meet scheduled due dates, while identifying emerging issues and recommending solutions for problems
Perform assigned tasks and production incident independently
Contribute ideas to help ensure that required standards and processes are in place and actively look for opportunities to enhance standards and improve process efficiency
All About You
Experience in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
Experience of building data pipelines through Spark with Java/Scala on Hadoop or Object storage
Experience of working with Databases like Oracle, Netezza and have strong SQL knowledge
Experience of working on Nifi will be an added advantage
Experience of working with APIs will be an added advantage
Experience and working in Unix environment is added advantage
Experience of working in Agile teams
Strong analytical skills required for debugging production issues, providing root cause and implementing mitigation plan
Strong communication skills - both verbal and written – and strong relationship, collaboration skills and organizational skills
Ability to multi-task across multiple projects, interface with external / internal resources and provide technical leadership to junior team members
Ability to be high-energy, detail-oriented, proactive and able to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results
Ability to quickly learn and implement new technologies, and perform POC to explore best solution for the problem statement
Flexibility to work as a member of a matrix based diverse and geographically distributed project teams
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Pay Ranges
O'Fallon, Missouri: $115,000 - $184,000 USD
Show more
Show less","Data Engineering, Spark, Java, Scala, Hadoop, Object storage, Mastercard Quality Assurance, Quality Control, Oracle, Netezza, SQL, Nifi, Unix, Agile, Debugging, Communication, Multitasking, POC, Initiative, Selfmotivation, Matrix, Distributed, Data pipelines, Databases, Analytics, Project management, Incident management, Data Warehouse","data engineering, spark, java, scala, hadoop, object storage, mastercard quality assurance, quality control, oracle, netezza, sql, nifi, unix, agile, debugging, communication, multitasking, poc, initiative, selfmotivation, matrix, distributed, data pipelines, databases, analytics, project management, incident management, data warehouse","agile, analytics, communication, data engineering, databases, datapipeline, datawarehouse, debugging, distributed, hadoop, incident management, initiative, java, mastercard quality assurance, matrix, multitasking, netezza, nifi, object storage, oracle, poc, project management, quality control, scala, selfmotivation, spark, sql, unix"
Lead Software Engineer/Data Engineer,Mastercard,"Missouri, United States",https://www.linkedin.com/jobs/view/lead-software-engineer-data-engineer-at-mastercard-3732692175,2023-12-17,Missouri,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Lead Software Engineer/Data Engineer
Overview
Mastercard is the global technology company behind the world’s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless®. We ensure every employee can be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.
Data Engineering and platform under Data Platform and Engineering service program is focused on enabling insights into Mastercard network and help build data-driven products by curating and preparing data in a secure and reliable manner. Moving to a “Unified and Fault-Tolerant Architecture for Data Ingestion and Processing” is critical to achieving this mission.
As a Lead Site Reliability / DevOps Engineer in Data Platform and Engineering service program, you will have the opportunity to build high performance data pipelines to load into Mastercard Data Warehouse. Our Data Warehouse provides analytical capabilities to number of business users who help different customers provide answer to their business problems through data. You will play a vital role within a rapidly growing organization, while working closely with experienced and driven engineers to solve challenging problems.
Your Role
Partner on the design the next implementation of Mastercard secure, global data and insight architecture, building the data integration and processing capabilities and operationalizing “Unified Data Acquisition and Processing (UDAP) platform”
Identify and resolve performance bottlenecks either proactively
Work with the customer support group as needed to resolve performance issues in the field
Explore automation opportunity and develop tools to automate some of the day to day operations tasks
Provide performance metrics and maintain dashboards to reflect production systems health
Conceptualize and implement proactive monitoring where possible to catch issues early
Experiment with new tools to streamline the development, testing, deployment, and running of our data pipelines.
Work with cross functional agile teams to drive projects through full development cycle.
Help the team improve with the usage of data engineering best practices.
Collaborate with other data engineering teams to improve the data engineering ecosystem and talent within Mastercard.
Creatively solve problems when facing constraints, whether it is the number of developers, quality or quantity of data, compute power, storage capacity or just time.
Maintain awareness of relevant technical and product trends through self-learning/study, training classes and job shadowing.
All About You
At least Bachelor's degree in Computer Science, Computer Engineering or Technology related field or equivalent work experience
Experience in Data Warehouse related projects in product or service-based organization
Experience as a Site Reliability Engineering or DevOps Engineer
Experience overall with experience as a software engineer or software architect
Experience solving for scalability, Performance and stability
Expert knowledge of Linux operating systems and environment and Scripting (Shell and Python preferred)
A deep expertise in your field of Software Engineering
Expert at troubleshooting complex system and application stacks
Operational Experience in Big Data Stacks ( Hadoop ecosystem, Spark is a plus)
Operational Experience in real-time ,streaming and data pipelines relevant frameworks ( Kafka and NiFi is a plus)
Operational experience troubleshooting network/server communication
Experience with performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts
Expertise in enterprise metrics/monitoring with frameworks such as Splunk, Druid, Grafana
Experience with cloud computing services, particularly deploying and running services in Azure or AWS
A belief in data driven analysis and problem solving and a proven track record in applying these principles
An organized approach the planning and execution of major projects
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Pay Ranges
O'Fallon, Missouri: $138,000 - $221,000 USD
Show more
Show less","Linux, Shell, Python, Hadoop, Spark, Kafka, NiFi, Splunk, Druid, Grafana, Azure, AWS, SQL, ETL","linux, shell, python, hadoop, spark, kafka, nifi, splunk, druid, grafana, azure, aws, sql, etl","aws, azure, druid, etl, grafana, hadoop, kafka, linux, nifi, python, shell, spark, splunk, sql"
Lead Cloud Data Engineer,Mastercard,"Missouri, United States",https://www.linkedin.com/jobs/view/lead-cloud-data-engineer-at-mastercard-3705253987,2023-12-17,Missouri,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Lead Cloud Data Engineer
Overview
We are looking for a self-motivated and enthusiastic Lead Cloud Data Engineer to join the Data Platforms and Engineering Services team. This individual will be responsible to work with our Cloud Architects, Cloud infrastructure, Information Security, Technical Product Managers and Senior Stakeholders to ensure we are delivering the right solutions for our customers.
Data Platform provides real time streaming, batch data processing, pipeline orchestration, data lake management, data cataloging capabilities and deliver data on time with quality, supported by metrics. As a key player of our data platform team, you will have opportunity to use your expertise to work in solving big data problems, design, coding and analytical skills to build core capabilities, frameworks and data pipelines.
Role
Design, develop and implement large scale, high-volume, high-performance, highly available, scalable data infrastructure and pipelines for the Lake house data platforms in AWS cloud.
Work closely with senior data engineers and data architects using Agile methodology, take ownership of the delegated tasks and deliver the results in timely manner.
Assist in troubleshooting and resolving issues with data pipelines, ensuring they are running smoothly and efficiently.
Stay up to date with emerging technologies and trends in the data engineering and cloud space and willingness to learn and use new tools and platforms that can improve efficiency.
All About You
Experience with Software engineering or DevOps, Cloud computing - AWS and Distributed database systems (Hadoop, Oracle, Netezza etc.)
Experience in data processing platforms on cloud such as Data bricks, Apache Spark, and Snowflake to handle large-scale data processing and distributed computing across massive datasets.
Experience in programming with Python, Scala, R, Java.
Experience and knowledge of Bit Bucket, Rally, and Jenkins a plus
Experience in delivering end-to-end automation of deployment, monitoring and infrastructure management in a cloud environment.
Building and configuring environments supporting CI/CD tools using an Agile delivery methodology
Strong experience using infrastructure as Code (IaC) with tools such as Cloud Formation, Terraform to buildout AWS environments.
Supporting all tiers of applications deployed in the cloud, configuring network, cloud native services, infrastructure, and privileges for development teams (AWS Accounts, IAM Users and Roles, VPCs, S3 buckets, cloud native services) and Knowledge of cloud networking concepts, subnets, routing, load balancing, firewalls, and cloud security.
Experience in ETL orchestration and workflow management tools using open-source tools like Apache NIFI.
Knowledge with Hadoop (CDP), relational databases and SQL, ETL development, data validation and testing, Data Analysis, and experience with Hadoop technologies such as Hive, Impala, Sqoop, Hue, Spark, Python of Hadoop Platform is a plus.
BS or higher degree in Computer Science, Information Technology or relevant fields.
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Pay Ranges
O'Fallon, Missouri: $137,000 - $213,000 USD
Show more
Show less","AWS, Agile, Apache Spark, Apache Nifi, Hadoop, Python, Scala, Datawarehouses, Cloud computing, Data engineering, Data pipelines, Data lakes, CloudFormation, Terraform, Relational databases, ETL development, Data validation and testing, Data Analysis, SQL, Hive, Impala, Sqoop, Hue, Java, R, Bit Bucket, Rally, Jenkins, CI/CD, IaC, IAM Users and Roles, VPCs, S3 buckets, Subnets, Routing, Load balancing, Firewalls, Cloud security","aws, agile, apache spark, apache nifi, hadoop, python, scala, datawarehouses, cloud computing, data engineering, data pipelines, data lakes, cloudformation, terraform, relational databases, etl development, data validation and testing, data analysis, sql, hive, impala, sqoop, hue, java, r, bit bucket, rally, jenkins, cicd, iac, iam users and roles, vpcs, s3 buckets, subnets, routing, load balancing, firewalls, cloud security","agile, apache nifi, apache spark, aws, bit bucket, cicd, cloud computing, cloud security, cloudformation, data engineering, data lakes, data validation and testing, dataanalytics, datapipeline, datawarehouses, etl development, firewalls, hadoop, hive, hue, iac, iam users and roles, impala, java, jenkins, load balancing, python, r, rally, relational databases, routing, s3 buckets, scala, sql, sqoop, subnets, terraform, vpcs"
Lead Database Engineer,Mastercard,"Missouri, United States",https://www.linkedin.com/jobs/view/lead-database-engineer-at-mastercard-3766387146,2023-12-17,Missouri,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Lead Database Engineer
Job Posting Title
Lead Database Engineer
Job Description Summary
Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.
Overview
Database Engineering manages our databases with a goal to deliver secure, scalable and resilient infrastructure and software deployment capability for global solutions.
Provide administration of production, pre-production, UAT and development database systems and assist in designing of the database tier for our global systems
Provide a robust, scalable and high-performance production database environment working with system engineers, database developers and other database administrators
Role
The Lead Database Engineer role has the ability to perform 40% architecture and 60% engineering functions where the individual contributes and/or influences the strategic direction for the Database Engineering Program. The individual will build a strong partnership with management and may manage operational and/or strategic initiatives or support for the team. The Lead Database Engineer champions the Mastercard Way.
About You
Advanced hands-on experience in configuring, patching, upgrading, and optimizing database software and related products.
Advanced hands-on experience in collaborating with development teams in helping them achieve Reliability, Operability, Scalable, Performant and Secure systems with the Database Tier.
Advanced experience in security, backup/recovery, architecture, networking, and administration for assigned database discipline.
Expertise and deep understanding in Database Replication setup, configuration, troubleshooting both Physical as well as Logical Replication
Proven track record in working with technology teams for the development, maintenance, testing, implementation, and support of assigned database technologies and associated software products.
Proactively monitor with advanced troubleshooting skills for database issues escalated by the business team and internal monitoring in a timely manner with strong skills to drive to root cause and remediation of issues.
Ability to develop a strong partnership with technology teams and customer to understand business needs and with advanced skills to communicate solutions in business terms.
Strong to advanced performance and tuning skills with the ability to identify, respond and resolve database performance issues; communicates database performance resolution to technical teams and business partners.
Successfully follow change management, problem management and incident management processes and procedures.
Experience with IAC Automation tools for building automation for DB builds, Patching, maintenances
Ability to handle large to enterprise project/initiatives with specialized knowledge within assigned discipline and the ability to architect solutions.
Advanced written and verbal communication skills to promote Database Engineering.
Proven track record to proactively plan and coordinate larger technical and architectural efforts within the database team and with external technology teams.
Ability to influence Mastercard best practices in planning, coordinating, implementing, and maintaining database solutions.
Proven track record in training and mentoring members of the team by contributing to supporting documentation.
Communicate, collaborate, and work effectively in a global environment.
Creates a positive culture in Database Engineering following the Mastercard Way.
Corporate Security Responsibility
Every Person Working For, Or On Behalf Of, Mastercard Is Responsible For Information Security. All Activities Involving Access To Mastercard Assets, Information, And Networks Comes With An Inherent Risk To The Organization And Therefore, It Is Expected That The Successful Candidate For This Position Must:
Abide by Mastercard’s security policies and practices
Ensure the confidentiality and integrity of the information being accessed
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
**Desirable Knowledge/Experience**
Experience integrating vendor and open-source products into an overall system
Experience working on cloud-hosted services like AWS, Azure
Database Technologies Include (not limited to): Oracle, Redis, Oracle Exadata Administration, PostgreSQL, Cassandra. The ideal candidate should have experience in both RDBMS and NoSQL database technologies
Experience in development working with new trends
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All Activities Involving Access To Mastercard Assets, Information, And Networks Comes With An Inherent Risk To The Organization And, Therefore, It Is Expected That Every Person Working For, Or On Behalf Of, Mastercard Is Responsible For Information Security And Must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Show more
Show less","AWS, Azure, Cassandra, Oracle, Oracle Exadata Administration, PostgreSQL, Redis, RDBMS, NoSQL, IAC Automation tools, Python, Database Automation, Cloud Hosted Services, Physical Replication, Logical Replication, Database Replication, Backup/Recovery, Incident Management, Problem Management, Change Management, Performance Tuning, Database Solutions, Database Software, Database Products","aws, azure, cassandra, oracle, oracle exadata administration, postgresql, redis, rdbms, nosql, iac automation tools, python, database automation, cloud hosted services, physical replication, logical replication, database replication, backuprecovery, incident management, problem management, change management, performance tuning, database solutions, database software, database products","aws, azure, backuprecovery, cassandra, change management, cloud hosted services, database automation, database products, database replication, database software, database solutions, iac automation tools, incident management, logical replication, nosql, oracle, oracle exadata administration, performance tuning, physical replication, postgresql, problem management, python, rdbms, redis"
Lead Database Engineer,Professional Diversity Network,"Missouri, United States",https://www.linkedin.com/jobs/view/lead-database-engineer-at-professional-diversity-network-3785205016,2023-12-17,Missouri,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team - one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Lead Database Engineer
Job Posting Title
Lead Database Engineer
Job Description Summary
Who is Mastercard?
Mastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.
Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.
Overview
Database Engineering manages our databases with a goal to deliver secure, scalable and resilient infrastructure and software deployment capability for global solutions.
Provide administration of production, pre-production, UAT and development database systems and assist in designing of the database tier for our global systems
Provide a robust, scalable and high-performance production database environment working with system engineers, database developers and other database administrators
Role
The Lead Database Engineer role has the ability to perform 40% architecture and 60% engineering functions where the individual contributes and/or influences the strategic direction for the Database Engineering Program. The individual will build a strong partnership with management and may manage operational and/or strategic initiatives or support for the team. The Lead Database Engineer champions the Mastercard Way.
About You
Advanced hands-on experience in configuring, patching, upgrading, and optimizing database software and related products.
Advanced hands-on experience in collaborating with development teams in helping them achieve Reliability, Operability, Scalable, Performant and Secure systems with the Database Tier.
Advanced experience in security, backup/recovery, architecture, networking, and administration for assigned database discipline.
Expertise and deep understanding in Database Replication setup, configuration, troubleshooting both Physical as well as Logical Replication
Proven track record in working with technology teams for the development, maintenance, testing, implementation, and support of assigned database technologies and associated software products.
Proactively monitor with advanced troubleshooting skills for database issues escalated by the business team and internal monitoring in a timely manner with strong skills to drive to root cause and remediation of issues.
Ability to develop a strong partnership with technology teams and customer to understand business needs and with advanced skills to communicate solutions in business terms.
Strong to advanced performance and tuning skills with the ability to identify, respond and resolve database performance issues; communicates database performance resolution to technical teams and business partners.
Successfully follow change management, problem management and incident management processes and procedures.
Experience with IAC Automation tools for building automation for DB builds, Patching, maintenances
Ability to handle large to enterprise project/initiatives with specialized knowledge within assigned discipline and the ability to architect solutions.
Advanced written and verbal communication skills to promote Database Engineering.
Proven track record to proactively plan and coordinate larger technical and architectural efforts within the database team and with external technology teams.
Ability to influence Mastercard best practices in planning, coordinating, implementing, and maintaining database solutions.
Proven track record in training and mentoring members of the team by contributing to supporting documentation.
Communicate, collaborate, and work effectively in a global environment.
Creates a positive culture in Database Engineering following the Mastercard Way.
Corporate Security Responsibility
Every Person Working For, Or On Behalf Of, Mastercard Is Responsible For Information Security. All Activities Involving Access To Mastercard Assets, Information, And Networks Comes With An Inherent Risk To The Organization And Therefore, It Is Expected That The Successful Candidate For This Position Must:
Abide by Mastercard's security policies and practices
Ensure the confidentiality and integrity of the information being accessed
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard's guidelines.
**Desirable Knowledge/Experience**
Experience integrating vendor and open-source products into an overall system
Experience working on cloud-hosted services like AWS, Azure
Database Technologies Include (not limited to): Oracle, Redis, Oracle Exadata Administration, PostgreSQL, Cassandra. The ideal candidate should have experience in both RDBMS and NoSQL database technologies
Experience in development working with new trends
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All Activities Involving Access To Mastercard Assets, Information, And Networks Comes With An Inherent Risk To The Organization And, Therefore, It Is Expected That Every Person Working For, Or On Behalf Of, Mastercard Is Responsible For Information Security And Must:
Abide by Mastercard's security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard's guidelines.
In line with Mastercard's total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
PDN-9aad5c2c-6384-489f-a474-a37429189cfb
Show more
Show less","Database Engineering, Oracle, Redis, Oracle Exadata Administration, PostgreSQL, Cassandra, RDBMS, NoSQL, AWS, Azure, IAC Automation tools, Change management, Problem management, Incident management","database engineering, oracle, redis, oracle exadata administration, postgresql, cassandra, rdbms, nosql, aws, azure, iac automation tools, change management, problem management, incident management","aws, azure, cassandra, change management, database engineering, iac automation tools, incident management, nosql, oracle, oracle exadata administration, postgresql, problem management, rdbms, redis"
Data Engineer,"TDK Technologies, LLC","O'Fallon, MO",https://www.linkedin.com/jobs/view/data-engineer-at-tdk-technologies-llc-3775478936,2023-12-17,Missouri,United States,Mid senior,Hybrid,"Job Description:
Experience dealing with large volumes of data, from various sources, both structured and unstructured.
Ability to triage and talk through performance / scaling issues of dealing with data at scale.
Good understanding of how data will be read (file formats, partitioning, bucketing).
Extensive experience writing testable jobs using Spark (or equivalent) framework.
Programming & Scripting Languages: Java EE, Scala, Spark, SQL, Bash.
Web services & API standards: REST, OAuth, JSON.
Software Architectures (micro-services, event driven, peer-to-peer).
Application Security.
Asynchronous Pub-Sub and Point to Point Messaging Systems.
Advantage, if you have experience working in ETL and Hadoop Ecosystem: HBase, Solr, Spark Streaming, Kudu, Spring Boot, Spring Context, Spring Data Rest, General Cloudera experience.
Streaming within the Hadoop ecosystem is a plus.
Show more
Show less","Data Analysis, Data Processing, Data Warehousing, ETL, Hadoop, HBase, Kudu, Apache Spark, Solr, Spark Streaming, Java EE, Scala, SQL, Bash, REST, OAuth, JSON, Microservices, Eventdriven Architecture, PeertoPeer, Application Security, PubSub, PointtoPoint Messaging, Spring Boot, Spring Context, Spring Data Rest, Cloudera","data analysis, data processing, data warehousing, etl, hadoop, hbase, kudu, apache spark, solr, spark streaming, java ee, scala, sql, bash, rest, oauth, json, microservices, eventdriven architecture, peertopeer, application security, pubsub, pointtopoint messaging, spring boot, spring context, spring data rest, cloudera","apache spark, application security, bash, cloudera, data processing, dataanalytics, datawarehouse, etl, eventdriven architecture, hadoop, hbase, java ee, json, kudu, microservices, oauth, peertopeer, pointtopoint messaging, pubsub, rest, scala, solr, spark streaming, spring boot, spring context, spring data rest, sql"
Data Center Engineer,PRI Global,"O'Fallon, MO",https://www.linkedin.com/jobs/view/data-center-engineer-at-pri-global-3780003357,2023-12-17,Missouri,United States,Mid senior,Hybrid,"As per the client guidelines we are looking for only locals.
The requirement as follows
Job title: Data Center Engineer
Duration: 12+ months
Location: O'Fallon, MO
Major Accountabilities:
•Manage Data Center projects.
•Develop detailed requirements for upgrading existing corporate and MasterCard Data Centers, as well as re-architecting Data Centers to facilitate new requirements, technologies, and growth.
•Resolve Data Center operations problems impacting equipment performance globally.
•Participate in Data Center-related initiatives.
•Research and develop innovative solutions around Data Center technologies.
•Support assigned regional projects, which include responsibility for planning; time and cost control; resource utilization and implementation.
•Support Vendor management.
•Provide strategic technical direction on highly complex Data Center projects and activities involving the selection and recommendation of design approach and systems software technology.
•Serves as a point of escalation, resolving problems of diverse scopes where analysis of the situation requires in-depth technical evaluation and judgement
•Supports process improvement efforts to identify and test opportunities for automation and/or reduction in time to deployment
•Performs security posture, which includes proactively identifying security risks and implementing both risk mitigation plans and control functions
•Mentors junior staff by providing training to develop technical skills and capabilities across the team
Show more
Show less","Data Center management, Requirements analysis, Data Center architecture, Data Center operations, Problem resolution, Research and development, Vendor management, Technical direction, System software, Security posture, Risk mitigation, Process improvement, Automation, Deployment, Mentoring, Training","data center management, requirements analysis, data center architecture, data center operations, problem resolution, research and development, vendor management, technical direction, system software, security posture, risk mitigation, process improvement, automation, deployment, mentoring, training","automation, data center architecture, data center management, data center operations, deployment, mentoring, problem resolution, process improvement, requirements analysis, research and development, risk mitigation, security posture, system software, technical direction, training, vendor management"
Senior Data Engineer,Mastercard,"Missouri, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-mastercard-3776395298,2023-12-17,Missouri,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Senior Data Engineer
Overview
We are seeking a talented and motivated Senior Data Engineer to join our data engineering team. In this role, you will play a critical part in designing, developing, and optimizing data pipelines and solutions that enable efficient data processing and analysis. As a Senior Data Engineer, you will collaborate with cross-functional teams to drive data-driven decision-making and contribute to the continuous improvement of our data infrastructure.
Role
Design, develop, and maintain new data capabilities and infrastructure for utilizing Mastercard, third-party, and partner data to enhance Mastercard's data products and solutions
Create new data pipelines, data transfers, and compliance-oriented infrastructure to facilitate seamless data utilization within cloud environments
Identify existing data capability and infrastructure gaps or opportunities within and across initiatives and provide subject matter expertise in support of remediation
Collaborate with technical team and business stakeholders to understand data requirements and translate them into technical solutions
Work with large datasets, ensuring data quality, accuracy, and performance
Implement data transformation, integration, and validation processes to support analytics and reporting needs
Optimize and fine-tune data pipelines for improved speed, reliability, and efficiency
Implement best practices for data storage, retrieval, and archival to ensure data accessibility and security
Troubleshoot and resolve data-related issues, collaborating with the team to identify root causes
Document data processes, data lineage, and technical specifications for future reference
Participate in code reviews, ensuring adherence to coding standards and best practices
Collaborate with DevOps teams to automate deployment and monitoring of data pipelines
Additional tasks as required
All About You
Bachelor's or Master's degree in Computer Science, Engineering, or a related field
Proven experience in data engineering, with a strong track record of designing and implementing data solutions
Proficiency in programming languages such as Python, Java, or Scala, and experience with data processing frameworks (Spark, Hadoop, etc.)
In-depth understanding of data warehousing concepts, cloud platforms (AWS, Azure, GCP), and data modeling techniques
Strong knowledge of SQL and NoSQL databases, as well as data integration and transformation tools
Passion for and engagement with emerging trends in data, AI/ML, analytics, and digital experiences
Experience in data product development, analytical models, and model governance
Experience in anonymizing data and managing the use of data
Experience in data hygiene procedures, identity resolution capabilities or data management a plus
Ability to create strategies and plans that define how information can be utilized to support an organization's overall business strategy, and how that information and data is organized, and governed inside an organization
Strong project management skills and a demonstrated ability to understand complex information product constructs
Familiarity with industry best practices for collection and use of data
Outstanding problem-solving skills and the ability to navigate complex data challenges
Effective communication and collaboration skills to work with both technical and non-technical stakeholders
Experience with agile methodologies and DevOps practices
What is Data & Services?
The Data & Services Team (D&S) is a key differentiator for Mastercard, providing the cutting-edge services that help our customers grow. Focused on thinking big and scaling fast around the globe, this team is responsible for end-to-end solutions for a diverse global customer base. We combine traditional management consulting with our rich data assets and in-house technology to provide our clients with powerful insights and tools to drive fact-based decision making. Centered on data-driven technologies and innovation, our services include consulting, loyalty and marketing programs, test-and-learn business experimentation, and data-driven information and risk management. While specializing in the payments industry, Mastercard Data & Services also works closely with major retailers, airlines, and other enterprises, leveraging data and insights garnered from within and beyond its network.
D&S is continuously looking for passionate and talented technologists, who share our vision for empowering our customers to make better fact-based decisions, to join us and shape the growth of our team.
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Pay Ranges
O'Fallon, Missouri: $115,000 - $184,000 USD
Show more
Show less","Python, Java, Scala, Spark, Hadoop, SQL, NoSQL, Cloud platforms (AWS Azure GCP), Data modeling, Data warehousing, Data processing, Data integration, Data transformation, Data validation, Data accuracy, Data performance, Data storage, Data retrieval, Data archival, Data security, Data accessibility, DevOps, Coding standards, Agile methodologies, Data governance, Data hygiene, Data anonymization, Data management, Data stewardship, Data lineage","python, java, scala, spark, hadoop, sql, nosql, cloud platforms aws azure gcp, data modeling, data warehousing, data processing, data integration, data transformation, data validation, data accuracy, data performance, data storage, data retrieval, data archival, data security, data accessibility, devops, coding standards, agile methodologies, data governance, data hygiene, data anonymization, data management, data stewardship, data lineage","agile methodologies, cloud platforms aws azure gcp, coding standards, data accessibility, data accuracy, data anonymization, data archival, data governance, data hygiene, data integration, data lineage, data management, data performance, data processing, data retrieval, data security, data stewardship, data storage, data transformation, data validation, datamodeling, datawarehouse, devops, hadoop, java, nosql, python, scala, spark, sql"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Louisvuitton,"Kelowna, British Columbia, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-louisvuitton-3751474076,2023-12-17,Kelowna, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Data Mining, Statistical Analysis, Reporting, Machine Learning, A/B Testing, Data Quality Management, Data Collection, Data Cleansing, Data Visualization, Tableau, Power BI, SQL, R, Python, HTML, CSS, JavaScript, MySQL, NoSQL, Apache Spark, Hadoop, Linux, Windows","data analysis, data mining, statistical analysis, reporting, machine learning, ab testing, data quality management, data collection, data cleansing, data visualization, tableau, power bi, sql, r, python, html, css, javascript, mysql, nosql, apache spark, hadoop, linux, windows","ab testing, apache spark, css, data collection, data mining, data quality management, dataanalytics, datacleaning, hadoop, html, javascript, linux, machine learning, mysql, nosql, powerbi, python, r, reporting, sql, statistical analysis, tableau, visualization, windows"
Data Analyst Part Time,Voxmediallc,"Peterborough, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-voxmediallc-3757245503,2023-12-17,Kawartha Lakes, Canada,Mid senior,Onsite,"Summary:
The Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, SQL, R, Python, Data Visualization, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL","data analysis, sql, r, python, data visualization, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Staff Data Engineer,Recruiting from Scratch,"Hoboken, NY",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744395347,2023-12-17,Cooperstown,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Deployment, Kafka, Storm, Spark Streaming, ETL, Data Warehouses, Data classification, Data retention, Data Governance","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, spark streaming, etl, data warehouses, data classification, data retention, data governance","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data governance, data retention, data science, data warehouses, deployment, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, spark streaming, sql, storm, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Hoboken, NY",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773086835,2023-12-17,Cooperstown,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Pipeline, Python, Java, Bash, SQL, Git, Pandas, R, Airflow, Kubernetes, Docker, Helm, Spark, Snowflake, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Classification, Data Retention","data engineering, machine learning, data pipeline, python, java, bash, sql, git, pandas, r, airflow, kubernetes, docker, helm, spark, snowflake, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, data retention","airflow, applied machine learning, bash, data classification, data engineering, data pipeline, data retention, docker, dynamodb, etl, git, helm, java, kafka, kubernetes, machine learning, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Hoboken, NY",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709540,2023-12-17,Cooperstown,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML Data Ops, Data Pipelines, Data Mining, Data Cleaning, Data Normalization, Data Modeling, NLP Data Pipelines, Automated Test Suites, Technical Documentation, Git Version Control, Python, Java, Bash, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Management Tools, Data Classification, Data Retention","data engineering, ml data ops, data pipelines, data mining, data cleaning, data normalization, data modeling, nlp data pipelines, automated test suites, technical documentation, git version control, python, java, bash, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, data retention","airflow, applied machine learning, automated test suites, aws, azure, bash, data classification, data cleaning, data engineering, data management tools, data mining, data normalization, data retention, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git version control, helm, java, kafka, kubernetes, ml data ops, nlp data pipelines, python, snowflake, spark, sparkstreaming, sql, storm, technical documentation"
SAP Data Analyst,Innova Solutions International,"Macclesfield, England, United Kingdom",https://uk.linkedin.com/jobs/view/sap-data-analyst-at-innova-solutions-international-3778551608,2023-12-17,Greater Manchester, United Kingdom,Associate,Hybrid,"Contract Senior SAP Data Analyst – 6 month initial contract - Can be based in Macclesfield (will need to be onsite 3 days per week) - £450-475 per day (Outside IR35)
I have a fantastic opportunity for a SAP Data Analyst to join one of the biggest global SAP HANA programmes.
Successful candidates will need to have a proven track record working with SAP Data Objects, and a proven track record with SAP Data requirement analysis, data quality analysis, and good knowledge of data structures.
You will need to have expertise in one of the below areas to be considered for this role:
Manufacturing Execution
Order To Cash
Procure To Pay
Intelligent Clinical Supply Management
Experience working in the pharmaceutical industry would also be highly desirable for this position.
If this sound like you please apply for my immediate attention!
Show more
Show less","SAP HANA, SAP Data Objects, SAP Data requirement analysis, Data quality analysis, Data structures, Manufacturing Execution, Order To Cash, Procure To Pay, Intelligent Clinical Supply Management, Pharmaceutical industry","sap hana, sap data objects, sap data requirement analysis, data quality analysis, data structures, manufacturing execution, order to cash, procure to pay, intelligent clinical supply management, pharmaceutical industry","data quality analysis, data structures, intelligent clinical supply management, manufacturing execution, order to cash, pharmaceutical industry, procure to pay, sap data objects, sap data requirement analysis, sap hana"
Junior Microsoft Data Engineer,BES Group,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-microsoft-data-engineer-at-bes-group-3761168312,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"The BES Group are the leading end to end solution provider in the Testing, Inspection, Certification and Compliance sector. Our team of experts (hand-picked by our Careers Team) cover an extensive range of engineering services.
We’ve gone from strength to strength over recent years, welcoming the very best risk management businesses to our Group. As a result, we’ve grown both our team of experts and our suite of solutions in line with our customer’s needs. We’re private equity backed, which means we have the flexibility and support to move quickly and grow faster than any other company in our industry. We’re innovative and forward thinking and have the awards to prove it, but most of all we’re focused on helping make sure our customers leave nothing to chance.
Our head office is based in central Manchester and is the home to many of our central functions. To support our continued ambitious growth plans, we are delighted to be recruiting for a
Junior Data Engineer
to join our fantastic IS team.
What are we looking for?
For over 160 years, our highly skilled surveyors and engineering consultants have inspected, tested and reported on all types of industrial machinery. As a leader in our field in the UK and Ireland our services cover anything from large plant and pipelines to escalators and retail coffee machines to ensure industry, businesses and the public are safe and that will never change.
This is a fantastic opportunity to join our growing Data Engineering and BI team, in a role that will allow you to both apply and grow your skills through the delivery of data and insights to help drive change in all areas of our business. You will form a key part of the team to continually improve our Microsoft Azure based strategic data platform, from wrangling data from Dynamics 365 and other key sources to producing dashboards for key business decisions and supporting predictive analysis projects.
We are looking for a candidate with some commercial experience in a BI, Data Analysis or experience in an existing Junior Data Engineering role, or a graduate with a computer science degree with time served, who are vested in Microsoft technology, with proactive improvement and customer orientated mindset looking to grow their business and technical experience.
What will I be doing?
Reporting into the Head of Data Engineering you will own the development and implementation of data and reporting solutions using the latest Microsoft technologies: SQL Server Integration Services Azure Synapse Analytics, Azure Analysis Services, Azure Data Lake, Azure SQL Database, Azure Data Factory and Power BI (both dashboards and paginated reports)
Take responsibility for delivery of our BAU processes to agreed SLA, including suggesting and undertaking approved high-quality improvements to those processes for monitoring and quality purposes. Where required, working with key stakeholders to undertake root cause analysis and take ownership of issues through to resolution, managing communication regarding status.
Maintain documentation relating to production BI processes, ensuring core documentation is relevant for current processes and collateral regarding identification and resolution of frequent issues is matured
Work with other members of the team or directly with our business users to understand and document business requirements, evaluate options, research and propose suitable solutions. Using your stakeholder management skills to translate business requirements into design specifications
Ensure that all work is carried through the environments, source controlled with regularity and deployments packages are robust and well organised.
Take an active role in ensuring the highest quality of our processes and the data we provide, working with system and business owners to ensure a proactive approach.
Success In This Role Will Require
Increased business confidence in and usage of management information as the “single version of the truth” for supporting our business operations
A continued focus on resolution of issue tickets to agreed timescales and implementation of solutions which resolve first time and avoid repeat issues
The timely achievement of objectives and success criteria set at the beginning of each project/development activity, bringing innovative solutions whilst maintaining principles
Reduction in the size of the backlog of MI and data quality requests
Contribution of ideas and thinking to our innovation pipeline
What makes BES Group a great place to work?
We genuinely care. It’s basic, human instinct that runs through every person that works at BES Group. Knowing we always do the best job we can with absolutely no compromise means everything.
If you work for us, you will get the below and, so much more:
A competitive salary of £35,000 (negotiable, depending on experience)
A flexible hybrid working approach, the role will be in our Manchester office with home based working available (you must have suitable Wi-Fi, and provide your own office set up).
Amazing head offices in the heart of Manchester city centre
Development opportunities across the business
Discretionary annual company bonus and yearly salary review
25 days leave per year plus bank holidays (and an extra day off on your birthday to celebrate you!)
Shiny new Surface Pro
Access to lots of discounts and benefits via our company benefits portal including; retail, fitness, holiday and cinema discounts
The support of a superb employee assistance programme
Electric vehicle salary sacrifice scheme
Opportunities to attend volunteer days
Company contribution to charity fundraising you participate in; we want to support charities you are passionate about!
Employee Suggestion programme, because we value input from everyone in the BES Group
Up to 10% pension
What experience do I need?
To Work For Us You Must Have The Right Attitude, Aptitude And Appetite For What We Do.  As a Minimum, You Will Have
Demonstrable experience of meeting requirements using key aspects of the Microsoft stack.
Strong knowledge of T-SQL including stored procedures, ideally with performance tuning understanding.
Ideally experience of development using SQL Server Integration Services (SSIS), Azure Data Factory, and Azure Analysis Services.
Proficient in developing reports using SQL Server Reporting Services and/or dataset and reporting delivery in Microsoft Power BI
Understanding of modern data warehouse concepts
Strong Customer focus, both internal and external with experience in gathering low level requirements
Proficient in root cause analysis and working with users to understand issues through to resolution.
Experience of monitoring, tuning and troubleshooting BI solutions.
Experience of interpreting and analysing data
Knowledge and a proven track record of identifying and supporting improvements in data / data quality.
Degree (or equivalent Level 6 qualification) in Computer Science, Maths, Physics or other technical subject (desirable but not essential)
Ideal But Not Essential
Ideally experience of Microsoft Dynamics 365 (CE or F&O)
Experience of using Data Bricks or Azure Synapse or an object orientated language
Ability to work in a matrix environment
Your Background
Ideally 1-2 years of development experience
Successful delivery of Business Intelligence solutions
Strong communication skills; both written and oral
An analytical mindset and an automation first philosophy
Highly effective and able to take ownership for executing broad range of tasks
A self-starter and a completer finisher
To apply for this role, simply click “Apply”. We ask for some basic contact information and a CV or your LinkedIn profile, it’s that easy! Please keep in mind, successful candidates will be required to complete the relevant background checks as part of the recruitment process.
We want to inspire everyone to see how important safety is and we expect the same from our people. And if you’re on the same page as us about that and you’ve got the right skills, experience and attitude, you’ll fit right in. Simple. We’re all about diversity and inclusion and that means we want our people to be themselves. We’re delighted to be an equal opportunities employer and that will never change!
BES Group, Your Safety, Our Focus
Show more
Show less","Data Engineering, BI, Data Analysis, SQL Server Integration Services, Azure Synapse Analytics, Azure Analysis Services, Azure Data Lake, Azure SQL Database, Azure Data Factory, Power BI, TSQL, SSIS, SQL Server Reporting Services, Modern data warehouse concepts, Data Bricks, Azure Synapse, Microsoft Dynamics 365, CE, F&O, Data quality, Computer Science, Maths, Physics, Communication skills, Automation, Problem solving, Selfstarter","data engineering, bi, data analysis, sql server integration services, azure synapse analytics, azure analysis services, azure data lake, azure sql database, azure data factory, power bi, tsql, ssis, sql server reporting services, modern data warehouse concepts, data bricks, azure synapse, microsoft dynamics 365, ce, fo, data quality, computer science, maths, physics, communication skills, automation, problem solving, selfstarter","automation, azure analysis services, azure data factory, azure data lake, azure sql database, azure synapse, azure synapse analytics, bi, ce, communication skills, computer science, data bricks, data engineering, data quality, dataanalytics, fo, maths, microsoft dynamics 365, modern data warehouse concepts, physics, powerbi, problem solving, selfstarter, sql server integration services, sql server reporting services, ssis, tsql"
Junior Microsoft Data Engineer,Energy Jobline,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-microsoft-data-engineer-at-energy-jobline-3772980181,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"The BES Group are the leading end to end solution provider in the Testing, Inspection, Certification and Compliance sector. Our team of experts (hand-picked by our Careers Team) cover an extensive range of engineering services.
We’ve gone from strength to strength over recent years, welcoming the very best risk management businesses to our Group. As a result, we’ve grown both our team of experts and our suite of solutions in line with our customer’s needs. We’re private equity backed, which means we have the flexibility and support to move quickly and grow faster than any other company in our industry. We’re innovative and forward thinking and have the awards to prove it, but most of all we’re focused on helping make sure our customers leave nothing to chance.
Our head office is based in central Manchester and is the home to many of our central functions. To support our continued ambitious growth plans, we are delighted to be recruiting for a Junior Data Engineer to join our fantastic IS team.
What are we looking for?
For over 160 years, our highly skilled surveyors and engineering consultants have inspected, tested and reported on all types of industrial machinery. As a leader in our field in the UK and Ireland our services cover anything from large plant and pipelines to escalators and retail coffee machines to ensure industry, businesses and the public are safe and that will never change.
This is a fantastic opportunity to join our growing Data Engineering and BI team, in a role that will allow you to both apply and grow your skills through the delivery of data and insights to help drive change in all areas of our business. You will form a key part of the team to continually improve our Microsoft Azure based strategic data platform, from wrangling data from Dynamics 365 and other key sources to producing dashboards for key business decisions and supporting predictive analysis projects.
We are looking for a candidate with some commercial experience in a BI, Data Analysis or experience in an existing Junior Data Engineering role, or a graduate with a computer science degree with time served, who are vested in Microsoft technology, with proactive improvement and customer orientated mindset looking to grow their business and technical experience.
What will I be doing?
Reporting into the Head of Data Engineering you will own the development and implementation of data and reporting solutions using the latest Microsoft technologies: SQL Server Integration Services Azure Synapse Analytics, Azure Analysis Services, Azure Data Lake, Azure SQL Database, Azure Data Factory and Power BI (both dashboards and paginated reports)
Take responsibility for delivery of our BAU processes to agreed SLA, including suggesting and undertaking approved high-quality improvements to those processes for monitoring and quality purposes. Where required, working with key stakeholders to undertake root cause analysis and take ownership of issues through to resolution, managing communication regarding status.
Maintain documentation relating to production BI processes, ensuring core documentation is relevant for current processes and collateral regarding identification and resolution of frequent issues is matured
Work with other members of the team or directly with our business users to understand and document business requirements, evaluate options, research and propose suitable solutions. Using your stakeholder management skills to translate business requirements into design specifications
Ensure that all work is carried through the environments, source controlled with regularity and deployments packages are robust and well organised.
Take an active role in ensuring the highest quality of our processes and the data we provide, working with system and business owners to ensure a proactive approach.
Success In This Role Will Require
Increased business confidence in and usage of management information as the “single version of the truth” for supporting our business operations
A continued focus on resolution of issue tickets to agreed timescales and implementation of solutions which resolve first time and avoid repeat issues
The timely achievement of objectives and success criteria set at the beginning of each project/development activity, bringing innovative solutions whilst maintaining principles
Reduction in the size of the backlog of MI and data quality requests
Contribution of ideas and thinking to our innovation pipeline
What makes BES Group a great place to work?
We genuinely care. It’s basic, human instinct that runs through every person that works at BES Group. Knowing we always do the best job we can with absolutely no compromise means everything.
If you work for us, you will get the below and, so much more:
A competitive salary of £35,000 (negotiable, depending on experience)
A flexible hybrid working approach, the role will be in our Manchester office with home based working available (you must have suitable Wi-Fi, and provide your own office set up).
Amazing head offices in the heart of Manchester city centre
Development opportunities across the business
Discretionary annual company bonus and yearly salary review
25 days leave per year plus bank holidays (and an extra day off on your birthday to celebrate you!)
Shiny new Surface Pro
Access to lots of discounts and benefits via our company benefits portal including; retail, fitness, holiday and cinema discounts
The support of a superb employee assistance programme
Up to 10% pension
What experience do I need?
To Work For Us You Must Have The Right Attitude, Aptitude And Appetite For What We Do.  As a Minimum, You Will Have
Demonstrable experience of meeting requirements using key aspects of the Microsoft stack.
Strong knowledge of T-SQL including stored procedures, ideally with performance tuning understanding.
Ideally experience of development using SQL Server Integration Services (SSIS), Azure Data Factory, and Azure Analysis Services.
Proficient in developing reports using SQL Server Reporting Services and/or dataset and reporting delivery in Microsoft Power BI
Understanding of modern data warehouse concepts
Strong Customer focus, both internal and external with experience in gathering low level requirements
Proficient in root cause analysis and working with users to understand issues through to resolution.
Experience of monitoring, tuning and troubleshooting BI solutions.
Experience of interpreting and analysing data
Knowledge and a proven track record of identifying and supporting improvements in data / data quality.
Degree (or equivalent Level 6 qualification) in Computer Science, Maths, Physics or other technical subject (desirable but not essential)
Ideal But Not Essential
Ideally experience of Microsoft Dynamics 365 (CE or F&O)
Experience of using Data Bricks or Azure Synapse or an object orientated language
Ability to work in a matrix environment
Your Background
Ideally 1-2 years of development experience
Successful delivery of Business Intelligence solutions
Strong communication skills; both written and oral
An analytical mindset and an automation first philosophy
Highly effective and able to take ownership for executing broad range of tasks
A self-starter and a completer finisher
To apply for this role, simply click “Apply”. We ask for some basic contact information and a CV or your LinkedIn profile, it’s that easy! Please keep in mind, successful candidates will be required to complete the relevant background checks as part of the recruitment process.
We want to inspire everyone to see how important safety is and we expect the same from our people. And if you’re on the same page as us about that and you’ve got the right skills, experience and attitude, you’ll fit right in. Simple. We’re all about diversity and inclusion and that means we want our people to be themselves. We’re delighted to be an equal opportunities employer and that will never change!
BES Group, Your Safety, Our Focus
Show more
Show less","Data Engineering, Data Analysis, Business Intelligence, Microsoft Azure, SQL Server Integration Services, Azure Synapse Analytics, Azure Analysis Services, Azure Data Lake, Azure SQL Database, Azure Data Factory, Power BI, Dynamics 365, TSQL, Stored procedures, SQL Server Reporting Services, Data warehouse concepts, Root cause analysis, Data quality, Computer Science, Mathematics, Physics, Microsoft Dynamics 365, Data Bricks, Matrix environment, Communication skills, Analytical mindset, Automation first philosophy, Selfstarter, Completer finisher","data engineering, data analysis, business intelligence, microsoft azure, sql server integration services, azure synapse analytics, azure analysis services, azure data lake, azure sql database, azure data factory, power bi, dynamics 365, tsql, stored procedures, sql server reporting services, data warehouse concepts, root cause analysis, data quality, computer science, mathematics, physics, microsoft dynamics 365, data bricks, matrix environment, communication skills, analytical mindset, automation first philosophy, selfstarter, completer finisher","analytical mindset, automation first philosophy, azure analysis services, azure data factory, azure data lake, azure sql database, azure synapse analytics, business intelligence, communication skills, completer finisher, computer science, data bricks, data engineering, data quality, data warehouse concepts, dataanalytics, dynamics 365, mathematics, matrix environment, microsoft azure, microsoft dynamics 365, physics, powerbi, root cause analysis, selfstarter, sql server integration services, sql server reporting services, stored procedures, tsql"
Data Engineer,Protein Works,"Runcorn, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-protein-works-3730322344,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"Initial Description
This is an incredible opportunity to join a growing and highly talented Business Intelligence team. You will be responsible for delivering and maintaining business critical reporting systems across the Protein Works including marketing, trading, operations and finance. You will have a strong understanding and appreciation of data engineering concepts and how to apply them using the Google Cloud Platform. You will possess an unwavering passion for data and how it can improve business performance.
The role: Data Engineer
Immediate Responsibilities
Managing and delivering end to end BI reporting projects from initial brief/requirement gathering to data pipeline development (APIs/ETL/ELT/SQL) to visualization
Undertaking ad hoc BI tasks based on business requirements and priorities
As part of the BI team you will be responsible for the day to day maintenance and enhancement of the data quality, security and performance of the Data Lake/Warehouse
Assisting your Data Analyst colleagues to generate impactful business insights and analysis
Support the Senior Data Engineer in managing the BI ticket system to ensure tasks are resolved in a timely and effective manner
You’ll Naturally Be Like This…
Possess a “can-do” attitude
Highly enthusiastic with a drive to improve and develop new ways of working
Ability to clearly communicate complex topics in simple terms, to a variety of audiences
Avid self-learner & share knowledge across the business
Highly motivated self-starter
Place a high value on delivering business impact
Positively impatient
Great at building positive, productive collaborations and teams
Excellent interpersonal skills
Non-Negotiables
Demonstrable data engineering experience with an enterprise data warehouse platform
Extremely strong SQL skills
Significant API experience
Experience with visualization tools e.g Data Studio, Tableau, Power BI
Ability to gather requirements from stakeholders across the business and turn into actual deliverables
Solid spreadsheet experience: Excel & Google Sheets
Highly numerate, logical and analytical
Highly organised and excellent verbal and written skills
Ability to multi-task and deliver to deadlines
Excellent relationship building skills and ability to influence at various levels
Well-organised, with an eye for detail in everything you do
Desirables
Bachelor’s degree in computer science or related field
Experience with the Google Cloud Platform/Data Stack including:
BigQuery
Cloud Functions
Cloud Storage
Pub/Sub
IAM Management
Looker Data Studio
Coding experience with Python/Javascript/C#
Familiarity with statistical/machine learning/AI concepts and techniques
Understanding of data pipeline/orchestration tools e.g. dbt, dataform
Appreciation of GCP’s serverless technologies e.g. Cloud Run/Workflows
Understanding of Google’s marketing stack, Google Analytics, Google Tag Manager, Google Ads, Google Search Console
Experience with low-code/no-code data platforms e.g. Retool, Hex
Show more
Show less","Data Engineering, Google Cloud Platform, BigQuery, Cloud Functions, Cloud Storage, Pub/Sub, IAM Management, Looker Data Studio, Python, Javascript, C#, SQL, Data Visualization, Data Quality, Data Security, Data Performance, Business Intelligence, Data Analytics, Data Analysis, Data Pipeline, Data Warehouse, ETL, ELT, Data Lake, Google Analytics, Google Tag Manager, Google Ads, Google Search Console, GCP Serverless Technologies, Cloud Run, Workflows","data engineering, google cloud platform, bigquery, cloud functions, cloud storage, pubsub, iam management, looker data studio, python, javascript, c, sql, data visualization, data quality, data security, data performance, business intelligence, data analytics, data analysis, data pipeline, data warehouse, etl, elt, data lake, google analytics, google tag manager, google ads, google search console, gcp serverless technologies, cloud run, workflows","bigquery, business intelligence, c, cloud functions, cloud run, cloud storage, data engineering, data lake, data performance, data pipeline, data quality, data security, dataanalytics, datawarehouse, elt, etl, gcp serverless technologies, google ads, google analytics, google cloud platform, google search console, google tag manager, iam management, javascript, looker data studio, pubsub, python, sql, visualization, workflows"
Data Engineer - Insurance,Jobs for Humanity,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-insurance-at-jobs-for-humanity-3773362037,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with Booking to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Booking
Job Description
Data Engineer II - Insurance Data
Location: Manchester, UK
At Booking.com, we believe in the power of data to drive our decisions. We are a technology-driven company that values innovation. But we are more than just data and code. We are about the excitement of exploring new places, making friends, and creating unforgettable experiences. Through our products, partners, and people, we want to empower everyone to experience the world.
About The Data Team
Our Data team is responsible for managing and analyzing data to improve our business performance and enhance our customers' experience. As a Data Engineer, you will play a crucial role in delivering data excellence and innovation, shaping Booking.com's data journey.
Insurance Data
The Insurance Data team focuses on creating and maintaining insurance data systems using a Data Vault. We aim to empower our analysts and product teams to make data-driven decisions, streamline processes, and gain a deeper understanding of our systems.
Role Overview
As a Data Engineer, you will work with a team of passionate individuals. Your role is to deliver a fully integrated, scalable, high-performance, and compliant data platform and pipelines. You will be responsible for replacing existing ad-hoc data solutions with modern, efficient technologies and practices. We expect you to actively contribute to team discussions, share your opinions, and align with team decisions.
Key Job Responsibilities and Duties:
Develop scalable and reliable data pipelines and platforms.
Collaborate with business users to implement performant, scalable, secure, and well-governed data pipelines.
Ensure end-to-end data quality in core datasets and pipelines.
Deliver products in an agile environment with a focus on quality assurance and ongoing optimization.
Contribute to your team's ways of working through regular reviews and improvement discussions.
Engage with other teams to resolve data quality issues.
Maintain data quality, security, integrity, and governance according to company standards and best practices.
Stay up-to-date with the latest technology standards and guidelines.
Connect business and product goals to technical tasks.
Prioritize customer impact and perform root cause analysis to prevent recurrence of issues.
Provide out-of-hours support as required.
Role Qualifications and Requirements:
We are looking for driven Data Engineers who enjoy problem-solving, are collaborative, and goal-oriented. If you have a strong foundation in Big Data Engineering, we want to hear from you!
Requirements:
Appropriate degree or suitable background and experience in technology.
Proven experience in Data Modelling using frameworks like Data Vault.
Proven experience of designing, building, and optimizing Data Warehouses using tools such as Snowflake.
Proven experience in Python or a similar scripting language.
Experience building data engineering pipelines using tools such as Apache Airflow and DBT.
Familiarity with a cloud platform, ideally AWS.
Self-motivated to explore new technologies.
Excellent communication skills for effective collaboration with technical and non-technical stakeholders.
Excellent attention to detail.
Self-starter, highly motivated, and a team player.
Strong problem-solving and analytical thinking skills.
Fluent in English, both written and spoken.
A passion for technological innovations and staying up-to-date with the latest trends.
Benefits & Perks:
At Booking.com, we believe in a Total Rewards Philosophy that goes beyond compensation. We offer a range of benefits to enhance your personal and professional life:
A brand new office located in the heart of Manchester.
Flexible hybrid working options.
Free breakfast and lunch.
25 days paid holiday plus bank holidays (increasing to 28 days after 3 years of service).
Pension contribution from the employer.
Industry-leading parental and adoption leave.
Great discounts on accommodation, car rentals, and other group benefits.
A dynamic and exciting work environment.
Health and well-being benefits, including mental health support, cycle to work schemes, and access to health insurance.
We believe in maintaining a work-life balance that works for you and the business. We offer flexibility around a 37.5-hour workweek and are open to considering other flexible working arrangements.
#ThinkInclusion: Wellbeing & Inclusion at Booking.com
Inclusion, diversity, belonging, wellbeing, and volunteering are at the core of our company culture. We embrace employees from diverse backgrounds, ethnicities, genders, and sexual orientations. We believe that inclusion is essential for creating a better and more inclusive travel experience for everyone. We provide reasonable adjustments for individuals with disabilities during the application process and throughout their employment. Please contact us to discuss any specific requirements you may have.
Booking.com is proud to be an equal opportunity workplace and is an affirmative action employer. We strive to create an environment where everyone can thrive, regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
Application Process:
We have a thorough application process that involves multiple stages. If your application is successful, our recruitment team will contact you for an initial screening call. The formal interview process includes a phone interview, competency interview, and technical interview. We will ensure that you are fully prepared for each stage of the interview process.
Pre-Employment Screening:
If your application is successful, your personal data may be used for pre-employment screening by a third party in compliance with applicable laws. The screening may include employment history, education, and other relevant information.
Show more
Show less","Big Data Engineering, Data Vault, Data Warehouses, Snowflake, Python, Apache Airflow, DBT, AWS, Data Modelling, Data Pipelines, Data Quality, Cloud Platform","big data engineering, data vault, data warehouses, snowflake, python, apache airflow, dbt, aws, data modelling, data pipelines, data quality, cloud platform","apache airflow, aws, big data engineering, cloud platform, data modelling, data quality, data vault, data warehouses, datapipeline, dbt, python, snowflake"
Data Cabling Engineer,Digital Waffle,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-at-digital-waffle-3732198097,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"Position:
Data Cabling Engineer (Temp/Contract/Freelance)
Department:
Information Technology / Network Infrastructure
Location:
Manchester, Greater Manchester, UK
Job Summary:
We are seeking a skilled and dedicated Data Cabling Engineer to join our Information Technology team. The successful candidate will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Jake on jake@digitalwaffle.co
Show more
Show less","Cat6, Cat6a, Copper cables, Cable trays, Conduits, Wall penetrations, Cable connectors, Outlets, Termination panels, RJ45 crimping tool, Punchdown tool, Cable tester, Cable certifier, Tone generator, Probe, Cable labels, Markers, Label printer, Tape measure, Ruler, Level, Cable ties, Velcro straps, Cable clips, Mounts, Power drill, Bits, Screwdrivers, Wall anchors, Safety glasses, Work gloves, Tool bag, Pouch, Pen, Notepad, Mobile device, Steeltoed boots, Hard hat, TIA/EIA, ISO/IEC, Network topologies, Protocols, Network equipment, Technical drawings, Schematics, Cabling diagrams","cat6, cat6a, copper cables, cable trays, conduits, wall penetrations, cable connectors, outlets, termination panels, rj45 crimping tool, punchdown tool, cable tester, cable certifier, tone generator, probe, cable labels, markers, label printer, tape measure, ruler, level, cable ties, velcro straps, cable clips, mounts, power drill, bits, screwdrivers, wall anchors, safety glasses, work gloves, tool bag, pouch, pen, notepad, mobile device, steeltoed boots, hard hat, tiaeia, isoiec, network topologies, protocols, network equipment, technical drawings, schematics, cabling diagrams","bits, cable certifier, cable clips, cable connectors, cable labels, cable tester, cable ties, cable trays, cabling diagrams, cat6, cat6a, conduits, copper cables, hard hat, isoiec, label printer, level, markers, mobile device, mounts, network equipment, network topologies, notepad, outlets, pen, pouch, power drill, probe, protocols, punchdown tool, rj45 crimping tool, ruler, safety glasses, schematics, screwdrivers, steeltoed boots, tape measure, technical drawings, termination panels, tiaeia, tone generator, tool bag, velcro straps, wall anchors, wall penetrations, work gloves"
Graduate Data Analyst,NielsenIQ,"Stockport, England, United Kingdom",https://uk.linkedin.com/jobs/view/graduate-data-analyst-at-nielseniq-3784375909,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"Job Description
The Data Market Measurement Analyst role is vital for the quality and integrity of our OPM model outputs. The analyst will be assigned to a specific OPM model but will work closely together in the Market Measurement team to make sure delivery deadlines on all OPM outputs are met and data quality meets CGA standards
Location-Stockport, UK
Requirements
Educated to degree level or equivalent with a strong analytical/thinking component (graduated and available to start immediately)
Comfortable working with and understanding various sources of information and deriving own conclusions
Highly numerate
Excellent communication with good written and spoken English language Skills
IT literate, comfortable with different software packages
Great rapport and builds strong relationships
Responsibilities
Taking individual ownership of a specific subject matter, working collaboratively as a member of a great team with a highly supportive and outgoing culture
Gain a detailed understanding of the full CGA product portfolio and how our class leading data helps clients uncover key insights to improve business performance
Processing, manipulating and interpreting various data sources
Work as part of a team to maintain and optimize a wide range of databases, regular reports and bespoke data projects in order to deliver a first-class service to the wider business
Working closely with experienced Senior Analysts you will rapidly develop your skills to ultimately play a crucial role in the delivery of outstanding market intelligence to our blue-chip customers
Involve in the creation and maintenance of CGAs class leading Market Measurement Services
We Offer
Generous Pension Scheme
Private Medical Insurance
Life Assurance
Gym Membership Discounts
Cycling Solutions Scheme
Holiday Entitlement plus additional Holiday Purchase Options
Refer a Friend Scheme
Local Discounts (Cinema tickets, retailers etc)
Additional Information
About NIQ
NIQ is the world’s leading consumer intelligence company, delivering the most complete understanding of consumer buying behavior and revealing new pathways to growth. In 2023, NIQ combined with GfK, bringing together the two industry leaders with unparalleled global reach. With a holistic retail read and the most comprehensive consumer insights—delivered with advanced analytics through state-of-the-art platforms—NIQ delivers the Full View™.
NIQ, is an Advent International portfolio company with operations in 100+ markets, covering more than 90% of the world’s population. For more information, visit NIQ.com.
Want to keep up with our latest updates?
Follow us on: LinkedIn | Instagram | Twitter | Facebook
Our commitment to Diversity, Equity, and Inclusion
NIQ is committed to reflecting the diversity of the clients, communities, and markets we measure within our own workforce. We exist to count everyone and are on a mission to systematically embed inclusion and diversity into all aspects of our workforce, measurement, and products. We enthusiastically invite candidates who share that mission to join us.
We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class. Our global non-discrimination policy covers these protected classes in every market in which we do business worldwide.
Learn more about how we are driving diversity and inclusion in everything we do by visiting the NielsenIQ News Center: https://nielseniq.com/global/en/news-center/diversity-inclusion/
NIQ or any of our subsidiaries will never ask you for money at any point of the recruitment or onboarding process.
Show more
Show less","Data Analysis, Communication, Data Manipulation, Data Interpretation, Database Management, Report Generation, Market Intelligence, Consumer Behavior Analysis, Microsoft Office Suite, Statistical Software, Data Visualization Tools","data analysis, communication, data manipulation, data interpretation, database management, report generation, market intelligence, consumer behavior analysis, microsoft office suite, statistical software, data visualization tools","communication, consumer behavior analysis, data interpretation, data manipulation, data visualization tools, dataanalytics, database management, market intelligence, microsoft office suite, report generation, statistical software"
Data Analyst,In Technology Group,"Stalybridge, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-in-technology-group-3775364271,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"Job Title:
Data Analyst
Location:
Stalybridge
Salary:
up to £50,000
Position Overview:
As the Sole Data Analyst you will be instrumental in harnessing the power of SAP to analyse and interpret data, generate reports, and contribute to the optimization of the business processes. You will work collaboratively with cross-functional teams to ensure the effective use of SAP data for informed decision-making.
Client Overview:
The client is a well-established manufacturer with more than 3000 employees worldwide. They work with cutting edge technologies, and have lots of opportunities for growth, progression and professional development.
Key Responsibilities:
Utilize Power BI to create comprehensive reports and dashboards.
interpret the Data in order to help make informed business decisions.
Collaborate with departments to understand their data needs, ensuring SAP solutions meet and exceed expectations.
Conduct data validation and cleansing within SAP to ensure accuracy and reliability.
Identify patterns, trends, and anomalies in datasets to inform strategic decision-making.
Contribute to the implementation and maintenance of SAP system upgrades and enhancements.
Qualifications:
Bachelor's degree in Data Science, Business Analytics, or a related field.
Proven experience as a Data Analyst with experience on SAP systems.
Proficiency in data analysis tools and languages (e.g., SQL, Python) for extracting and analysing SAP data.
Excellent analytical and problem-solving skills, with keen attention to detail.
Effective communication skills to convey complex technical concepts to non-technical stakeholders.
Experience using Power BI for data visualization and reporting
Ability to work collaboratively in a team environment and independently when necessary.
If you are interested in this role and have the relevant experience and qualifications, please apply as they are looking to get this role filled asap. For further details on the role, please contact me at
sean.porter@intechnologygroup-uk.com
Show more
Show less","Data Analysis, SAP, Power BI, Data Interpretation, Business DecisionMaking, Collaboration, Data Validation, Data Cleansing, Pattern Identification, Trend Analysis, Anomaly Detection, Strategic DecisionMaking, SAP System Upgrades, SAP System Enhancements, Data Science, Business Analytics, SQL, Python, Analytical Skills, Problem Solving, Communication Skills, Team Collaboration, Independent Work","data analysis, sap, power bi, data interpretation, business decisionmaking, collaboration, data validation, data cleansing, pattern identification, trend analysis, anomaly detection, strategic decisionmaking, sap system upgrades, sap system enhancements, data science, business analytics, sql, python, analytical skills, problem solving, communication skills, team collaboration, independent work","analytical skills, anomaly detection, business analytics, business decisionmaking, collaboration, communication skills, data interpretation, data science, data validation, dataanalytics, datacleaning, independent work, pattern identification, powerbi, problem solving, python, sap, sap system enhancements, sap system upgrades, sql, strategic decisionmaking, team collaboration, trend analysis"
Lead Data Engineer,ByteHire,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-bytehire-3767661710,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"Reference: DT-259p
Title: Lead Data Engineer
Job Type: Permanent
Salary: £80,000 - £100,000
Location: Fully Remote, (Must be located close to Manchester)
The Client
Industry: Global Aviation and Fuel Management technology business.
Purpose: Rationalise and support the use of fuel management for sustainability. This will add environmental impacts and financial impacts to the platforms customers.
The Role
Data Warehousing And Modelling
Design and implement scalable data warehousing solutions that cater to the client's analytics and reporting needs.
Regularly review and optimise the data model to accommodate evolving business requirements.
Ensure data accuracy and integrity throughout all data warehouses.
Data Pipelines Design And Operation
Develop, construct, test, and maintain architectures, such as Data Warehouse, Pipeline Orchestration, Data Transformation tooling systems.
Collaborate with data analytics engineers to produce optimised ETL/ELT processes and data pipelines.
Monitor and troubleshoot data ingestion and processing issues, ensuring timely and accurate data availability.
Data Infrastructure Observability And Monitoring
Implement monitoring tools and frameworks to oversee the health of the data infrastructure.
Establish alerting systems for potential issues and discrepancies in the data infrastructure.
Perform regular system audits to detect anomalies and ensure system resilience.
Development And Deployment Process Management
Lead the design and implementation of CI/CD processes for data pipelines and infrastructure.
Guide the team on best practices related to version control, testing, and deployment.
Collaborate with cross-functional teams to ensure smooth deployment and scaling of data solutions.
BI Reporting System Implementation And Maintenance
Partner with business stakeholders to design and roll out business intelligence solutions that meet their needs.
Maintain and optimise reporting systems, ensuring data accuracy and timely delivery.
Essential Skills
Data Architecture
Software and data architecture skills, understanding the bigger picture
Experienced in communicating with management and stakeholders around requirements
Python
Working with files, at scale
Working with data, at scale
Familiarity with Cloud Services APIs and SDKs
Unit Testing
DBT
Data warehousing, data modelling, data transformation, data quality
Databases / Storage
Strong SQL skills needed here (BigQuery and MongoDB)
Distributed Data Warehousing
Query Optimisation
Data Modelling
Role and User management
BigQuery monitoring and alerting
Apache Airflow
Good experience creating DAGS
Airflow monitoring and alerting
BI
Apache Superset (or Tableau or PowerBI)
Desirable Skills
GCP / Google Cloud
GKE (Google Kubernetes Engine)
Google Cloud Run (serverless)
Google Cloud Storage
Azure
Azure Container Apps
AKS (Azure Kubernetes Service)
Azure Blob Storage
Cloud
Kubernetes and Docker
Experienced in CI/CD, with Terraform for infrastructure provisioning
API
OpenAPI specification
Show more
Show less","Data Architecture, Software Architecture, Python, Cloud Services APIs, Unit Testing, Apache Airflow, BigQuery monitoring and alerting, SQL, BigQuery, MongoDB, Distributed Data Warehousing, Query Optimisation, Data Modelling, Role and User management, Apache Superset, Tableau, PowerBI, GCP / Google Cloud, GKE (Google Kubernetes Engine), Google Cloud Run, Google Cloud Storage, Azure, Azure Container Apps, AKS (Azure Kubernetes Service), Azure Blob Storage, Kubernetes, Docker, CI/CD, Terraform, API, OpenAPI specification","data architecture, software architecture, python, cloud services apis, unit testing, apache airflow, bigquery monitoring and alerting, sql, bigquery, mongodb, distributed data warehousing, query optimisation, data modelling, role and user management, apache superset, tableau, powerbi, gcp google cloud, gke google kubernetes engine, google cloud run, google cloud storage, azure, azure container apps, aks azure kubernetes service, azure blob storage, kubernetes, docker, cicd, terraform, api, openapi specification","aks azure kubernetes service, apache airflow, apache superset, api, azure, azure blob storage, azure container apps, bigquery, bigquery monitoring and alerting, cicd, cloud services apis, data architecture, data modelling, distributed data warehousing, docker, gcp google cloud, gke google kubernetes engine, google cloud run, google cloud storage, kubernetes, mongodb, openapi specification, powerbi, python, query optimisation, role and user management, software architecture, sql, tableau, terraform, unit testing"
Data Engineer - Arcadis Gen,Arcadis,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-arcadis-gen-at-arcadis-3770621485,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"Role: Data Engineer
Location: UK, Remote
As an Arcadian, you already help us deliver world leading sustainable design, engineering, and consultancy solutions for natural and built assets. You are part of our global business comprising 36,000 people, in over 70 countries, dedicated to improving quality of life.
Everyone has an important role to play. With the power of many curious minds, together we can continue to solve the world’s most complex challenges and deliver more impact together.
Role description:
The Data Engineer is responsible for migrating the data systems, delivery of the technical leadership in data migration, support of data migration strategies, tools, design and build. As well as formulating various methods to transform raw data into useful data systems. Overall, you’ll strive for efficiency by aligning data systems with business goals. This includes data ETL and modelling pipelines, hosting environments, automated testing and deployment, information storage, and retrieval and support enabling infrastructure across multiple client projects.
The Data Engineer will execute migration projects and may be a coach to other Junior Engineers within the Service Engineering Group (SEG). They will work together with Data architects, Data Consultants, and QA Consultants to enable technical solutions throughout the project lifecycle, including assessments, tool evaluations, demonstrations, requirements gathering, results presentation.
Role accountabilities:
Support and own the end-to-end migration journey on cloud-based products.
Technical delivery of ETL data migration with framework agreed.
Execute data migration runs.
Perform data migration using Snowflake, Dbt and Airflow or similar toolset.
Analyse and organize raw data; build data systems and pipelines.
Assess the compatibility with your cloud destination.
Ensure that the design fully meets the client’s needs.
Manage outputs and quality throughout delivery of a project ensuring all work packages align to the project scope.
Qualifications & Experience:
The ideal candidate will have a background in computer science and/or data engineering, ideally within a data migration consultancy or physical asset owning company context. They will have good leadership, communication, and organisational skills. They will have a strong understanding of how to deploy and scale machine learning and advance analytics techniques in a modern cloud environment. A keen interest in the latest technologies is important, to help ensure that the product (as well as Gen in general) stays on the cutting edge.
More importantly than these technical skills, however, we are looking for someone to be a data solution leader, pushing forward the boundaries of what we do, and the standards expected of the wider Gen data architecture. The successful candidate will need to be resilient and maintain a positive attitude when faced with challenges, be an excellent problem-solver, and be happy to be out of their comfort zone. They will need to have excellent communication skills, not just at a technical level, but also with more business-focused colleagues. They will not shy away from a challenge and will stand up and be counted. They will never be satisfied, and always look to find the next improvement in what Gen does, and how we do it.
We are looking for someone who possesses:
Must haves:
Experience in cloud-based data migration tools and validations
Experience using AWS and/or Azure
Experience using the following tools: Snowflake & DBT
Excellent understanding of SQL
Familiarity with DevOps processes/best practices
Part of assessment, tool options and data migration enablement
Experience of delivery of client facing projects, ideally in a data migration consultancy
Knowledge of programming languages (e.g., Java and Python)
You have experience in the use of automated testing, CI/CD, the use and creation of APIs and of virtual package environments to ensure repeatability of code.
Demonstrable skills in database management
Desirable:
Experience using Airflow
Experience using Azure Data Factory
Continue your career journey as an Arcadian.
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You do meaningful work, and no matter where your next role in Arcadis takes you, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark; on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We believe that by working together diverse people with different experiences develop the most innovative ideas. Equality, diversity and inclusion is at the heart of how we improve quality of life and we work closely with our people across six ED&I Workstreams: Age, Disability, Faith, Gender, LGBT+ and Race. A diverse and skilled workforce is essential to our success.
Show more
Show less","Data migration, ETL, Data modelling, Machine learning, Cloud computing, AWS, Azure, Snowflake, Dbt, Airflow, SQL, DevOps, Java, Python, Automated testing, CI/CD, APIs, Virtual package environments, Database management","data migration, etl, data modelling, machine learning, cloud computing, aws, azure, snowflake, dbt, airflow, sql, devops, java, python, automated testing, cicd, apis, virtual package environments, database management","airflow, apis, automated testing, aws, azure, cicd, cloud computing, data migration, data modelling, database management, dbt, devops, etl, java, machine learning, python, snowflake, sql, virtual package environments"
Senior MI Developer/ Senior Microsoft Data Engineer,BES Group,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-mi-developer-senior-microsoft-data-engineer-at-bes-group-3720028449,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"The BES Group are the leading end to end solution provider in the Testing, Inspection, Certification and Compliance sector. Our team of experts (hand-picked by our Careers Team) cover an extensive range of engineering services.
We’ve gone from strength to strength over recent years, welcoming the very best risk management businesses to our Group. As a result, we’ve grown both our team of experts and our suite of solutions in line with our customer’s needs. We’re private equity backed, which means we have the flexibility and support to move quickly and grow faster than any other company in our industry. We’re innovative and forward thinking and have the awards to prove it, but most of all we’re focused on helping make sure our customers leave nothing to chance.
Our head office is based in central Manchester and is the home to many of our central functions. To support our continued ambitious growth plans, we are delighted to be recruiting for a
Senior MI Developer/Senior Microsoft Data Engineer
to join our fantastic IS team.
What are we looking for?
This is a fantastic opportunity to join our growing Data Engineering and BI team, in a role that will allow you to both apply and grow your skills through the delivery of data and insights to help drive change in all areas of our business. You will form a key part of the team to continually improve our Microsoft Azure based strategic data platform, from wrangling data from Dynamics 365 and other key sources to producing dashboards for key business decisions and supporting predictive analysis projects.
Our operations are of a scale that will provide you with material data and technology solution challenges whilst allowing you to rapidly see the impact of the work that you do. Success in the role involves being customer centric, with the ability to innovate as well as being highly effective in the delivery of data and analytic solutions.
What will I be doing?
Reporting into the Head of Data Engineering you will own the development and implementation of data and reporting solutions using the latest Microsoft technologies: SQL Server Integration Services Azure Synapse Analytics, Azure Analysis Services, Azure Data Lake, Azure SQL Database, Azure Data Factory and Power BI (both dashboards and paginated reports)
You will be working with other members of the team or directly with our business users to understand and document business requirements, evaluate options, research, and propose suitable solutions. Using your stakeholder management skills to translate business requirements into design specifications.
You will ensure that all work is carried through the environments, source controlled with regularity and that deployment packages are robust and well organised. Ensuring all conflicts in this area are merged/escalated effectively whilst supporting the development and enhancement of our CI/CD pipelines.
You will take an active role in ensuring the highest quality of our processes and the data we provide, working with system and business owners to ensure a proactive approach supporting data governance as well as establishing best practice guidelines for code management and deployment between cloud MI environments
What makes BES Group a great place to work?
We genuinely care. It’s basic, human instinct that runs through every person that works at BES Group. Knowing we always do the best job we can with absolutely no compromise means everything.
If you work for us, you will get the below and, so much more:
A competitive salary from £55,000 (negotiable, depending on experience)
A flexible working approach, the role will be in our Manchester office with home based working available (you must have suitable Wi-Fi, and provide your own office set up).
Amazing head offices in the heart of Manchester city centre
Development opportunities across the business
Discretionary annual company bonus and yearly salary review
25 days leave per year plus bank holidays (and an extra day off on your birthday to celebrate you!)
Shiny new Surface Pro
Access to lots of discounts and benefits via our company benefits portal including; retail, fitness, holiday and cinema discounts
The support of a superb employee assistance programme
Electric vehicle salary sacrifice scheme
Opportunities to attend volunteer days
Company contribution to charity fundraising you participate in; we want to support charities you are passionate about!
Employee Suggestion programme, because we value input from everyone in the BES Group
Up to 10% pension
What experience do I need?
To Work For Us You Must Have The Right Attitude, Aptitude And Appetite For What We Do.  As a Minimum, You Will Have
Strong customer focus and communication skills.
Having delivered a modern data platform using relevant technologies and services in Microsoft Azure.
Expert in T-SQL including stored procedures, temp tables & CTE’s with performance tuning.
Proficient in the development of SQL Server Integration Services (SSIS) solutions and Azure Data Factory
Demonstrable experience in the use of GIT for CI/CD source control in azure across key areas of the stack
Proficient in developing and deploying Azure Analysis Services Models
Proficient in developing cascading, parameterized reports using SQL Server Reporting Services.
Report development of datasets and reporting in Microsoft Power BI
Ideally experience of using Data Bricks or Azure Synapse
Strong understanding of data modelling concepts
Experience of interpreting and analysing complex data
Knowledge and a proven track record in data / data quality management
Code management & deployment tools
Experience of Microsoft Dynamics CRM
Proficient in debugging, monitoring, tuning and troubleshooting BI solutions.
Ability to work in a matrix environment
To apply for this role, simply click “Apply”. We ask for some basic contact information and a CV or your LinkedIn profile, it’s that easy! Please keep in mind, successful candidates will be required to complete the relevant background checks as part of the recruitment process.
We want to inspire everyone to see how important safety is and we expect the same from our people. And if you’re on the same page as us about that and you’ve got the right skills, experience and attitude, you’ll fit right in. Simple. We’re all about diversity and inclusion and that means we want our people to be themselves. We’re delighted to be an equal opportunities employer and that will never change!
BES Group, Your Safety, Our Focus
Show more
Show less","SQL Server Integration Services, Azure Synapse Analytics, Azure Analysis Services, Azure Data Lake, Azure SQL Database, Azure Data Factory, Power BI, TSQL, Microsoft Azure, CI/CD, GIT, Azure Synapse, Data Bricks, Data modelling concepts, Microsoft Dynamics CRM, Data quality management, Code management & deployment tools","sql server integration services, azure synapse analytics, azure analysis services, azure data lake, azure sql database, azure data factory, power bi, tsql, microsoft azure, cicd, git, azure synapse, data bricks, data modelling concepts, microsoft dynamics crm, data quality management, code management deployment tools","azure analysis services, azure data factory, azure data lake, azure sql database, azure synapse, azure synapse analytics, cicd, code management deployment tools, data bricks, data modelling concepts, data quality management, git, microsoft azure, microsoft dynamics crm, powerbi, sql server integration services, tsql"
Senior Data Cable Engineer,Digital Waffle,"Warrington, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-cable-engineer-at-digital-waffle-3755107388,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"Position:
Senior Data Cabling Engineer (Full Time/Permanent)
Department:
Information Technology / Network Infrastructure
Location:
Warrington, Cheshire, UK
Job Summary:
We are seeking a skilled and dedicated Senior Data Cabling Engineer to join our Information Technology team. The successful candidate will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
10 years experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Senior Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Jake on jake@digitalwaffle.co
Requirements
10 Years Experience
Benefits
Company Van
Overtime Availability
Show more
Show less","Cat6, Cat6a, Copper cables, Cable pathways, Cable trays, Conduits, Wall penetrations, Cable connectors, Outlets, Termination panels, TIA/EIA, ISO/IEC, Cable testing tools, Cable certification tools, RJ45 crimping tool, Punchdown tool, Cable tester, Cable certifier, Tone generator, Probe, Cable labels, Markers, Label printer, Tape measure, Ruler, Level, Cable ties, Velcro straps, Cable clips, Cable mounts, Power drill, Screwdrivers, Safety glasses, Work gloves, Tool bag, Pen, Notepad, Steeltoed boots, Hard hat, Cable fish tape, Cable rods, Cable lubricant","cat6, cat6a, copper cables, cable pathways, cable trays, conduits, wall penetrations, cable connectors, outlets, termination panels, tiaeia, isoiec, cable testing tools, cable certification tools, rj45 crimping tool, punchdown tool, cable tester, cable certifier, tone generator, probe, cable labels, markers, label printer, tape measure, ruler, level, cable ties, velcro straps, cable clips, cable mounts, power drill, screwdrivers, safety glasses, work gloves, tool bag, pen, notepad, steeltoed boots, hard hat, cable fish tape, cable rods, cable lubricant","cable certification tools, cable certifier, cable clips, cable connectors, cable fish tape, cable labels, cable lubricant, cable mounts, cable pathways, cable rods, cable tester, cable testing tools, cable ties, cable trays, cat6, cat6a, conduits, copper cables, hard hat, isoiec, label printer, level, markers, notepad, outlets, pen, power drill, probe, punchdown tool, rj45 crimping tool, ruler, safety glasses, screwdrivers, steeltoed boots, tape measure, termination panels, tiaeia, tone generator, tool bag, velcro straps, wall penetrations, work gloves"
Data Insight Analyst,Salix,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-insight-analyst-at-salix-3776719145,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"Salix has recently gone through a period of significant growth and is still growing; Salix is in discussions with BEIS regarding Salix expanding its responsibilities to act as a Delivery Agent to manage the delivery of the Home Upgrade Grant (HUG) and Social Housing Decarbonisation Fund that was launched in July and September 2022. The schemes deliver energy efficiency and low-carbon heating upgrades to low-income households who live in social and private housing sectors across England. This role has been created to support the Assistant Directors of the Delivery Agent to deliver this.
Due to the expansion of the company and the move from managing loans to delivering large-scale grant programmes, we are seeking a Data Analyst to deliver data reporting and performance analysis for the housing schemes.
This is a newly created role, and we are looking for someone who is a team player, pays close attention to detail and has the personal motivation to perform the role to the highest standards. You are punctual, flexible and responsive in how you approach your work.
Key Performance Objectives;
Improve data accuracy by reviewing current procedures and implementing new ways of working.
Jointly prioritise data workloads across the Housing Team to ensure there is the capacity to deal with peaks and troughs of planned work and unplanned requests.
Continually improve the collection and presentation of data to ensure it meets the needs of senior stakeholders and operational decision-makers.
Improve turnaround time for data and management information requests.
Utilise Data Visualisation tools to help provide insight into internal processes and be able to make recommendations
Key Skills and Competencies;
Developing, enhancing, and monitoring metrics for Operational Excellence, Project Delivery and Business Process Effectiveness.
Outstanding verbal and written communication skills and the ability to express complex technical concepts effectively, both verbally and in writing.
Experience in designing, developing, and maintaining BI reports, and dashboards, optimizing efficiency in manipulating data and writing complex queries.
Experience with business applications & ERPs (NetSuite CRM, SUN Systems, SAP…).
Hands-on experience in using different DBMS (SQL Server, Oracle, MSQL).
Ability to comprehend business needs, convert them into BRD & TRD (Business/ Technical requirement document), develop implementation roadmap and execute on time.
Fosters collaborative and coordinated working across teams and in partnership with other organisations.
Presents a credible and positive image both internally and externally.
Pays close attention to detail, ensuring work is delivered on time and to a high standard.
Effective time management and ability to set realistic timescales for work delivery.
Self-motivated and good organisational skills.
Show more
Show less","Data Analysis, Data Reporting, Data Visualization, Performance Analysis, Operational Excellence, Project Delivery, Business Process Effectiveness, SQL Server, Oracle, MSQL, NetSuite CRM, SUN Systems, SAP, ERPs, Business Intelligence (BI), Data Manipulation, Business Applications, Business Requirements Document (BRD), Technical Requirements Document (TRD)","data analysis, data reporting, data visualization, performance analysis, operational excellence, project delivery, business process effectiveness, sql server, oracle, msql, netsuite crm, sun systems, sap, erps, business intelligence bi, data manipulation, business applications, business requirements document brd, technical requirements document trd","business applications, business intelligence bi, business process effectiveness, business requirements document brd, data manipulation, data reporting, dataanalytics, erps, msql, netsuite crm, operational excellence, oracle, performance analysis, project delivery, sap, sql server, sun systems, technical requirements document trd, visualization"
Data Analyst - SQL & Power BI,Nigel Frank International,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-sql-power-bi-at-nigel-frank-international-3780307888,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"A healthcare provider are looking for an Information Analyst with skills in SQL and either SSRS or Power BI to join their established Management Information team, where you'll be responsible for delivering timely and accurate data and reporting solutions for users across the organisation.
This organisation are headquartered in Hertfordshire, although this is a home-based role, so you can be based anywhere in the UK. It's likely you will be asked to travel to their Head Office around twice per year for full IT team meetings, and this will be fully expensed.
We are looking for an individual with strong SQL and T-SQL skills in order to effectively work with the data in their SQL and Snowflake database, as well as experience with either SSRS or Power BI to produce insightful reports and visualisations - currently they're on a journey to migrate most of their SSRS reports over to Power BI, with any new reports being created in Power BI.
This is a really exciting opportunity for an ambitious data professional to progress their career with an organisation who will be committed to your ongoing professional development, with the chance to work towards your Microsoft certifications - you'll have the opportunity to complete your Power BI and Snowflake certifications, and get your ITIL qualifications too.
Requirements:
Strong SQL and T-SQL skills - including complex queries, stored procedures, tables, views etc.
SSRS or Power BI Report builder experience (including DAX)
Excellent communication, problem solving and stakeholder management skills
Benefits:
Salary of £25-32,000, depending upon experience
25 days' annual leave plus bank holidays plus day off for your birthday (increasing by 1 day for every 2 years' service up to 28 days)
Pension with 5% employer contribution and 3% employee contribution
Enhanced maternity package
Death in service (2X salary)
Learning and development opportunities
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","SQL, TSQL, SSRS, Power BI, DAX, Snowflake, Microsoft Certifications, ITIL, Data Analysis, Reporting, Visualization, Communication, Problem Solving, Stakeholder Management","sql, tsql, ssrs, power bi, dax, snowflake, microsoft certifications, itil, data analysis, reporting, visualization, communication, problem solving, stakeholder management","communication, dataanalytics, dax, itil, microsoft certifications, powerbi, problem solving, reporting, snowflake, sql, ssrs, stakeholder management, tsql, visualization"
Senior Database Engineer (DB2 Technologies),"Pyrgos Advisors, LLC","Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-database-engineer-db2-technologies-at-pyrgos-advisors-llc-3758494020,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"Senior Database Engineer
United Kingdom (Virtual)
The Opportunity
Reporting to the Global Director of Infrastructure and Products you will design, configure, operate, and maintain systems hardware, software, and related infrastructure. This may include directing work of less senior engineers. You will help establish strategy and adoption of new technologies in a single domain. Provide engineering design and support for multi-tenant platforms in a single domain including hardware, software, automation, and monitoring.
Your Responsibilities Will Include
Ensure the global stability, integrity, and efficient operation of the systems and infrastructure that support departments, and to conduct/lead research on latest technologies for practical application to business problems and strategic plans.
Demonstrate advanced application of technical skills and interpersonal competencies.
Provide direct global technical leadership and support for all requirements in developing strategy, planning, coordination, scripting, and implementation of releases, upgrades, or changes to our distributed computing environment. This includes managing several small to medium-scale projects and tasks of several complexities across the enterprise.
Provide direct technical support for all requirements in the planning, coordination, scripting, and implementation of upgrades or changes to our Db2 database footprint.
Perform capacity planning, performance monitoring, tuning, and prediction.
Provide high-level support Db2 databases, ensuring change management procedures and methodologies are followed.
You Will Have The Following Experience
Experience installing and supporting DBb2 databases
Experience working with IBM z/OS
Knowledge of Db2 database internals (locking, partitioning, security, and cloning)
Trouble shoot ,mainframe and distributed processing problems in a Db2 z/OS environment
Db2 v11.X
Db2 v12.X
IBM z/OS: TSO, Utilities, JCL, VSAM, RACF
CA7, CA11: Host scheduling administration
BMC AMI and related Admin/utility software
Benefits & Training
Bonus Scheme
Pension Scheme
Flexible Working
Internal Wellbeing Programme
LinkedIn Learning
Learning and Development opportunities
At Assurant, diversity helps us inspire creativity in the global marketplace, and we are believers in the strategic value of inclusion and how it improves performance, creates growth opportunities, better aligns us to our clients, and enhances employee engagement.
We look forward to receiving your application!
Show more
Show less","Db2, IBM z/OS, TSO, Utilities, JCL, VSAM, RACF, CA7, CA11, BMC AMI, Capacity planning, Performance monitoring, Tuning, Prediction, Change management, Troubleshooting, Distributed processing, Mainframe, Scripting, Cloning, Highlevel support, Administration, Utilities software","db2, ibm zos, tso, utilities, jcl, vsam, racf, ca7, ca11, bmc ami, capacity planning, performance monitoring, tuning, prediction, change management, troubleshooting, distributed processing, mainframe, scripting, cloning, highlevel support, administration, utilities software","administration, bmc ami, ca11, ca7, capacity planning, change management, cloning, db2, distributed processing, highlevel support, ibm zos, jcl, mainframe, performance monitoring, prediction, racf, scripting, troubleshooting, tso, tuning, utilities, utilities software, vsam"
Customer Service Representative/Data Analyst/Data Entry Clerk,Jpmorganchaseco,"Stockport, England, United Kingdom",https://uk.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-jpmorganchaseco-3742324296,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : application@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data analysis, Statistical techniques, Data visualization, SQL, R, Python, Tableau, Power BI, Hypothesis testing, A/B testing, Data management, ETL, Data integrity, Data accuracy, Data completeness, Data collection, Data cleansing, Data manipulation, Datadriven decisionmaking, Business intelligence","data analysis, statistical techniques, data visualization, sql, r, python, tableau, power bi, hypothesis testing, ab testing, data management, etl, data integrity, data accuracy, data completeness, data collection, data cleansing, data manipulation, datadriven decisionmaking, business intelligence","ab testing, business intelligence, data accuracy, data collection, data completeness, data integrity, data management, data manipulation, dataanalytics, datacleaning, datadriven decisionmaking, etl, hypothesis testing, powerbi, python, r, sql, statistical techniques, tableau, visualization"
Lead Data Engineer,Adria Solutions Ltd,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-adria-solutions-ltd-3782016714,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Remote,"On behalf of our rapidly expanding UK-based client, we are recruiting for a talented Lead Data Engineer. As Lead Data Engineer, you will work closely with data analysts, data scientists, and other data consumers within the business to gather and populate the data warehouse table structure which will be optimized for reporting.
The Lead Data Engineer will be responsible for:
Strong Azure Experience
Day-to-day support of the data warehouse and troubleshoots of existing procedures and processes.
Defines and promotes the department’s best practices and design principles for data warehousing techniques and architecture.
Assists in the development of business intelligence, business data standards, and processes.
Guide the business in identifying any new data needs and delivery mechanisms for acquiring and reporting such information as well as addressing the actual needs.
Creates and supports the ETL to facilitate the accommodation of data in the warehouse.
Strong SQL / BI skills
Cloud experience - Azure
Team Leadership experience required
Benefits for the Lead Data Engineer - Manchester
This is a fantastic opportunity for a Lead Data Engineer to join an award-winning company, where you will work in an innovative and dynamic environment and play a vital part in the company’s transformational journey. For your hard work, you will be rewarded with a generous benefits package:
Competitive Salary
Flexible working hours with the opportunity for home working
Bonus
An additional day off for your birthday!
Company pension scheme and group life assurance
How to Apply if You’re Interested in this Job:
If this sounds like your perfect role, click Apply without delay!
If it’s not quite the job you’re looking for, details of all our current roles can be found on our
website, www.adriasolutions.co.uk
Lead Data Engineer - Manchester
Show more
Show less","Azure, Data Warehouse, SQL, BI, ETL, Cloud Computing, Team Leadership, Data Analytics, Data Science","azure, data warehouse, sql, bi, etl, cloud computing, team leadership, data analytics, data science","azure, bi, cloud computing, data science, dataanalytics, datawarehouse, etl, sql, team leadership"
Senior Database Engineer (UK Remote),Turnitin,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-database-engineer-uk-remote-at-turnitin-3693800359,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Remote,"Company Description
MUST BE UK BASED
100% REMOTE
When you join Turnitin, you'll be welcomed into a company that is a recognized innovator in the global education space. For more than 20 years, Turnitin has partnered with educational institutions to promote honesty, consistency, and fairness across all subject areas and assessment types. Over 16,000 academic institutions, publishers, and corporations use our services: Gradescope by Turnitin, iThenticate, Turnitin Feedback Studio, Turnitin Originality, Turnitin Similarity, ExamSoft, and ProctorExam.
Turnitin has offices in Australia, India, Indonesia, Japan, Korea, Mexico, the Netherlands, the Philippines, Ukraine, the United Kingdom, and the United States. Our diverse community of colleagues are all unified by a shared desire to make a difference in education. Come join us, and let's make change together.
Job Description
We are looking for an experienced Senior Data Engineer who thinks in clever ways to solve data problems of scale and load with elegant solutions.  Our team supports several PostgreSQL/SQLServer clusters that contain thousands of databases.  These data stores are up to 4+TB each with hundreds of millions of rows, as well as many on-premises and cloud data storage systems, such as Aurora, Redis, Memcached, Redshift, CockroachDB, DynamoDB, and Cassandra.  As a senior member of this team, you will work closely with our DevOps and Engineering Teams to help us maintain automation and stability in our data stores.
Key Responsibilities
Protect, tune, migrate, and administer On-premises and Cloud Data Stores.
Participate in a 24x7 on-call rotation.
Always perform in a manner that guarantees the Protection, Availability, and Performance of our Global Data Stores.
Be opinionated enough to speak up when you think we could be doing something better than we're doing it now -- and tactful and empathetic enough to communicate this in a way that brings people along instead of distancing them
Qualifications
Passion for data stores and a high sense of ownership while performing critical duties based on senior-level experience in Security, Disaster Recovery, and High Availability.
Ability to have a strong work ethic in a fast-paced environment with multiple priorities that may occasionally change.
Ability to work independently and perform under pressure.
Good interpersonal skills, friendly, and approachable.
Deep Linux experience.Strong SQL skills.
Expert in PostgreSQL/SQLServer tuning and best practices.
AWS experience, including Terraform.
Automated monitoring and alerting of On-premises and Cloud data technologies, such as Aurora, Redshift, Redis, CockroachDB, and Cassandra.
Kubernetes experience.
Working experience with configuration management tools, preferably Puppet and Terraform.
Additional Information
No agency submissions
Our Mission
is to ensure the integrity of global education and meaningfully improve learning outcomes.
Our Values
underpin everything we do.
Customer Centric -
We realize our mission to ensure integrity and improve learning outcomes by  putting educators and learners at the center of everything we do.
Passion for Learning -
We seek out teammates that are constantly learning and growing and build a workplace which enables them to do so.
I
ntegrity -
We believe integrity is the heartbeat of Turnitin. It shapes our products, the way we treat each other, and how we work with our customers and vendors.
Action & Ownership -
We have a bias toward action and empower teammates to make decisions.
One Team -
We strive to break down silos, collaborate effectively, and celebrate each other’s successes.
Global Mindset -
We respect local cultures and embrace diversity. We think globally and act locally to maximize our impact on education.
Global Benefits
Flexible/hybrid working
Remote First Culture
Health Care Coverage*
Tuition Reimbursement*
Competitive Paid Time Off
4 Self-Care Days per year
National Holidays*
2 Founder Days + Juneteenth Observed
Paid Volunteer Time*
Charitable contribution match*
Monthly Wellness Reimbursement/Home Office Equipment*
Access to Modern Health (mental health platform)
Parental Leave*
Retirement Plan with match/contribution*
*
varies by country
Seeing Beyond the Job Ad
At Turnitin, we recognize it’s unrealistic for candidates to fulfill 100% of the criteria in a job ad.  We encourage you to apply if you meet the majority of the requirements because we know that skills evolve over time. If you’re willing to learn and evolve alongside us, join our team!
Turnitin, LLC is committed to the policy that all persons have equal access to its programs, facilities and employment. All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","Data Engineering, PostgreSQL, SQLServer, Aurora, Redis, Memcached, Redshift, CockroachDB, DynamoDB, Cassandra, DevOps, Linux, SQL, Terraform, Kubernetes, Puppet","data engineering, postgresql, sqlserver, aurora, redis, memcached, redshift, cockroachdb, dynamodb, cassandra, devops, linux, sql, terraform, kubernetes, puppet","aurora, cassandra, cockroachdb, data engineering, devops, dynamodb, kubernetes, linux, memcached, postgresql, puppet, redis, redshift, sql, sqlserver, terraform"
Senior Data Engineer (Remote),Forward Role,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-remote-at-forward-role-3766815987,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Remote,"Senior Data Engineer (6 month contract)
Remote
£400 day rate outside IR35
What We're Looking For
Proven experience as a Data Engineer with expertise in AWS
Strong familiarity with AWS services, including Glue and Lambda.
Experience with data warehousing (Redshift) and data lake architecture
Previous experience managing and leading a team of engineer's
If you thinks this sounds like you, please reach out!
A leading Ecommerce brand based in Manchester is seeking a Senior Data Engineer for a 6-month contract. In this pivotal role, you will take lead and manage a team of data engineers as the company transitions away from Google for advertising. Your responsibilities will include crafting and implementing resilient data pipelines, utilizing AWS Glue to extract, transform, and load data from diverse sources into the Data Lake/Warehouse. Effective communication skills are essential, as you'll be required to articulate technical concepts to a range of stakeholders. Previous experience in team management and leadership is essential for any applicants.
This role can be remote based, paying a day rate circa £400 and falls outside IR35.
As an industry leading, nationwide Marketing, Digital, Analytics, IT and Design recruitment agency, we are continually receiving new assignments to work on, so keep a close eye on our website, Facebook, LinkedIn and Twitter pages for a full list of current permanent and interim opportunities as well as marketplace news and fun stuff.
Forward Role is operating as an employment agency.
Show more
Show less","AWS, AWS Glue, AWS Lambda, Redshift, Data warehousing, Data lake architecture, Data pipelines, Data extraction, Data transformation, Data loading, Data Lake, Data Warehouse, Communication, Team management, Leadership","aws, aws glue, aws lambda, redshift, data warehousing, data lake architecture, data pipelines, data extraction, data transformation, data loading, data lake, data warehouse, communication, team management, leadership","aws, aws glue, aws lambda, communication, data extraction, data lake, data lake architecture, data loading, data transformation, datapipeline, datawarehouse, leadership, redshift, team management"
Data Engineer,Venchr,"Manchester Area, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-venchr-3756377746,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"We are looking for an awesome Data Engineer to join a Manchester-based FinTech start-up growing all the time.
They are really going from strength to strength since their inception in 2016 and now contain a whole group of businesses across FinTech/Super Yachts and the entertainment sector!
Essentially we need an awesome Data Engineer who's great with
Python
and has
Databricks
experience. In addition, you will be creating and maintaining data pipeline architecture, super comfortable with large complex data sets, building analytics tools etc!
This role will report to the head and will need someone experienced and confident being somewhat left to their own devices at times.
They have a lovely new office but also a very flexible working policy.
Please apply now!
Venchr is an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
Show more
Show less","Python, Databricks, Data pipeline architecture, Analytics tools, Complex data sets","python, databricks, data pipeline architecture, analytics tools, complex data sets","analytics tools, complex data sets, data pipeline architecture, databricks, python"
Data Engineer,Talent,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-talent-3763525762,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"I am looking for talented Data Engineers looking to get hands on experience working with the Azure Cloud platform. For an established and growing organisation, who continue to expand their data capabilities and champion a data centric approach.
There are great opportunities to upskill within the position and scope to progress your career within the company.
Hybrid - 1-2 days - Manchester City Center
Salary - Up to £45k
What are we looking for?
Strong SQL and T-SQL experience
Knowledge and experience across data warehousing, the Kimball Methodology and data modelling.
A strong communicator, able to effectively gather requirements and translate them into viable solutions.
Motivated to learn new skills and technologies, and driven by consistent upskilling.
Bonus points for...
Any experience with Azure would be helpful
Skills in Python or R
Power BI Skills
Candidates must have permanent residency in the UK. We are unfortunately not able to offer sponsorship or consider candidates on temporary visas.
If you are interested in finding out more, please apply with a copy of your CV today.
Show more
Show less","Data Engineering, Azure Cloud Platform, SQL, TSQL, Data Warehousing, Kimball Methodology, Data Modeling, Communication Skills, Requirements Gathering, Solution Design, Upskilling, Python, R, Power BI","data engineering, azure cloud platform, sql, tsql, data warehousing, kimball methodology, data modeling, communication skills, requirements gathering, solution design, upskilling, python, r, power bi","azure cloud platform, communication skills, data engineering, datamodeling, datawarehouse, kimball methodology, powerbi, python, r, requirements gathering, solution design, sql, tsql, upskilling"
Data Engineer,Talent,"Manchester Area, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-talent-3778561904,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"Data Engineer - SQL / Power BI / Azure
I am looking for talented Data Engineers looking to get hands on experience working with the Azure Cloud platform. For an established and growing organisation, who continue to expand their data capabilities and champion a data centric approach.
There are great opportunities to upskill within the position and scope to progress your career within the company.
Salary - Up to £45k
What are we looking for?
Strong SQL and T-SQL experience
Knowledge and experience across data warehousing, the Kimball Methodology and data modelling.
A strong communicator, able to effectively gather requirements and translate them into viable solutions.
Motivated to learn new skills and technologies, and driven by consistent upskilling.
Bonus points for...
Any experience with Azure would be helpful
Skills in Python or R
Power BI Skills
Candidates must have permanent residency in the UK. We are unfortunately not able to offer sponsorship or consider candidates on temporary visas.
If this sounds like it could be your next step please apply today to find out more and send your CV to alex.richardson@talentinternational.com.
Show more
Show less","SQL, TSQL, Data Warehousing, Kimball Methodology, Data Modelling, Python, R, Power BI, Azure Cloud Platform","sql, tsql, data warehousing, kimball methodology, data modelling, python, r, power bi, azure cloud platform","azure cloud platform, data modelling, datawarehouse, kimball methodology, powerbi, python, r, sql, tsql"
Data Engineer,Corecom Consulting,"Warrington, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-corecom-consulting-3781859179,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"Data Engineer / Warrington – Hybrid / £50,000 DOE / Great benefits package and flexible working.
Corecom Consulting has partnered with an organisation specialising in Business services that happens to be one of the top 40 best places to work.
They are a collaborative network of dedicated individuals who pride themselves on integrity, reliability, and ethical principles. As well as being committed to helping businesses improve their service and customer experience by sharing insights and working together to address industry-wide issues.
As a Data Engineer, you will be a key member of the Data and Insights team reporting to the Data Engineering manager. This role will be vital in an upcoming transformation project they are about to embark on.
Key skills
Experience with End to End data issues
Data pipelines
Deployment and management of ETL processes
Strong Python skills
Azure
What’s on offer
On-site facilities include - Free canteen, Free gym, Yoga classes, Free parking + more
Health & Life Assurance options
26 days holiday + 5 days extra available to buy
Enhanced pension contribution
Hybrid working - 2 days on site
Please apply now, or send your CV directly to m.etienne@corecomconsulting.co.uk
“At Corecom, we don’t just accept differences, we celebrate them and thrive on them for the benefit of our employees, our clients and our candidates. Internally, we thrive from our difference and want our employees to be proud to be themselves and proud to be Corecom. Externally, we utilise those differences to help our clients and candidate strive for a more diverse and inclusive world.”
Show more
Show less","Data Engineering, Data Pipelines, ETL Processes, Python, Azure","data engineering, data pipelines, etl processes, python, azure","azure, data engineering, datapipeline, etl, python"
Data Engineer,Maxwell Bond®,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-maxwell-bond%C2%AE-3766699292,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"Data Engineer
45k - 55k
Manchester
Hybrid
No Visa Sponsorship Provided
I am currently partnering with a leading company that support small to medium sized businesses with finance allowing them to grow and they are seeking a Data Engineer passionate about Data Science and Machine Learning.
This is an innovative role, working with a small development team responsible for ensuring each business area is deriving the maximum value from the data held across their front and back office systems. Alongside reporting on the data they've collected, they also want to use their data to start making intelligent predictions about the future by leveraging Machine Learning technologies.
The successful Data Engineer will have:
2 years experience as a Data Engineer.
Working knowledge of Azure Data Factory/Synapse.
Working knowledge of Azure Databricks and Blob Storage/ADLS.
Keen interest in emerging ML techniques and their commercial value.
Strong Proficiency in Python, SQL, Azure ML, Git, Azure Cloud Services.
What is in it for you?
Flexible working for work/life balance.
Sponsorship for certifications.
Structure for progression and career development into ML Engineering and Data Science
Private healthcare package
Discounts package with retailers
And Many More!
This position does not offer Visa Sponsorship, even if you are currently covered by a post-study visa. Please refrain from applying if you need sponsorship at any stage.
If this sounds like you, please contact myself or apply directly with your CV!
“By applying for this role, you provide us with consent to process your data in line with our Privacy Policy, full details can be found on our website.
Show more
Show less","Data Science, Machine Learning, Azure Data Factory, Azure Synapse, Azure Databricks, Blob Storage, ADLS, Python, SQL, Azure ML, Git, Azure Cloud Services","data science, machine learning, azure data factory, azure synapse, azure databricks, blob storage, adls, python, sql, azure ml, git, azure cloud services","adls, azure cloud services, azure data factory, azure databricks, azure ml, azure synapse, blob storage, data science, git, machine learning, python, sql"
Data Engineer,Anson McCade,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-anson-mccade-3775987313,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"Data Engineer - Python & AWS
Location: Manchester
Type: Full-time, Permanent
Salary: Up to £58,000 + 10% Bonus
I’m working with a global consulting and technology firm, who are seeking a Senior Consultant Data Engineer to join their dynamic data & AI team In Manchester.
You will play a key part in shaping and delivering innovative data solutions for their clients. This is a fantastic opportunity for an experienced professional looking to advance their career in a leading consulting environment.
Responsibilities:
Design, develop, and maintain robust and scalable data engineering solutions using Python and AWS technologies.
Contribute to the design and implementation of data architecture, ensuring optimal performance and reliability.
Collaborate with cross-functional teams to integrate and transform data from various sources, ensuring data quality and accuracy.
Continuously optimize and enhance existing data pipelines to meet evolving business and technical requirements.
Work closely with clients to understand their business needs and provide data engineering expertise to support decision-making.
Qualifications:
Minimum of 5years of experience in a Data Engineering role, with a focus on Python and AWS.
Strong programming skills in Python, with experience in building and optimizing data pipelines.
Solid experience with AWS data services, such as S3, Redshift, Glue, Athena, etc.
Proficient in working with relational and non-relational databases.
Ability to analyze complex data challenges and develop effective solutions.
Previous experience in a consulting environment is highly desirable.
Excellent communication and client-facing skills to convey technical concepts to non-technical
What We Offer:
Competitive salary up to £75,000
10% bonus
Comprehensive benefits package
Opportunities for professional development and advancement
Collaborative and dynamic work environment
How to Apply:
If you are a seasoned Senior Data Engineer with a focus on Python, Spark and AWS, and you are excited about contributing to innovative data solutions, we would love to hear from you.
Data Engineer - Python & AWS
Location: Manchester
Type: Full-time, Permanent
Salary: Up to £58,000 + 10% Bonus
Show more
Show less","Python, AWS, S3, Redshift, Glue, Athena, Relational databases, Nonrelational databases","python, aws, s3, redshift, glue, athena, relational databases, nonrelational databases","athena, aws, glue, nonrelational databases, python, redshift, relational databases, s3"
Data Engineer,PA Consulting,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-pa-consulting-3760863564,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"Company Description
About us
We believe in the power of ingenuity to build a positive human future.
As strategies, technologies and innovation collide, we create opportunity from complexity.
Our diverse teams of experts combine innovative thinking and breakthrough use of technologies to progress further, faster. Our clients adapt and transform, and together we achieve enduring results.
An innovation and transformation consultancy, we are over 4000 specialists in consumer and manufacturing, defence and security, energy and utilities, financial services, government and public services, health and life sciences, and transport. Our people are strategists, innovators, designers, consultants, digital experts, scientists, engineers and technologists.
We operate globally from offices across the UK, US, Netherlands and Nordics.
PA. Bringing Ingenuity to Life.
Job Description
Flexible working - We are guided by our client work and needs; however, you have autonomy to manage your time and diary to suit your work/life balance.
Are you interested in collaborating with a high-achieving team that creates innovative solutions?
We're in search of an exceptional
Data Engineer
to join our Engineering Team. As a member of our dynamic and forward-thinking group, you'll have the chance to actively participate in a broad spectrum of digital projects, spanning cultural awareness, business continuity, and beyond.
Qualifications
We are hiring across a variety of grades from Consultant Analyst through to Senior Consultant. Due to the nature of client work for this role, you will need to be a
sole British national, eligible to work on secure client sites.
You will ideally have;
A minimum of 2 years’ experience using AWS and Azure cloud technologies for tasks like ETL pipeline development, data warehousing, data lake creation, and data movement.
Adeptness in utilising AWS and Azure data and analytics services (or their open-source equivalents), including but not limited to EMR, Glue, RedShift, Kinesis, Lambda, Data Factory, and Databricks.
A solid foundation in data engineering, data science, and product development, encompassing experience in both stream and batch processing.
Proficiency in the design and deployment of production data pipelines, involving languages like Java, Python, Scala, Spark, and SQL.
You should also have some, if not all, of the following;
Capability in scripting, data extraction via APIs, and the composition of SQL queries.
Practical experience in integrating data engineering components into production systems, along with expertise in data cleaning, data wrangling, data visualisation, and reporting.
Proficiency in handling substantial volumes of structured and unstructured data, including the integration of data through AWS and Azure cloud-based ingestion and curation processes.
Knowledge of data mining, machine learning, and natural language processing is considered an advantage.
Familiarity with Agile methodologies and participation in Scrum ceremonies.
Competence in the design and construction of solutions tailored for the AWS and Azure cloud environments, which may involve expertise in Platform-as-a-Service components, serverless architecture, and container technologies.
We know the skill-gap and ‘somewhat need to tick every box’ can get in the way of meeting brilliant candidates, so please don’t hesitate to apply – we’d love to hear from you.
Apply today by completing our online application!
Additional Information
Life At PA encompasses our peoples' experience at PA. It's about how we enrich peoples’ working lives by giving them access to unique people and growth opportunities and purpose led meaningful work.
We believe diversity fuels ingenuity. Diversity of thought brings exciting perspectives; diversity of experience brings a wealth of knowledge, and diversity of skills brings the tools we need. When we bring people together with diverse backgrounds, identities, and minds, embracing that difference through an inclusive culture where our people thrive; we unleash the power of diversity – bringing ingenuity to life.
Find out more about Life at PA here.
We are dedicated to supporting the physical, emotional, social and financial well-being of our people. Check out some of our extensive benefits:
Health and lifestyle perks accompanying private healthcare for you and your family
25 days annual leave (plus a bonus half day on Christmas Eve) with the opportunity to buy 5 additional days
Generous company pension scheme
Opportunity to get involved with community and charity-based initiatives
Annual performance-based bonus
PA share ownership
Tax efficient benefits (cycle to work, give as you earn)
We recruit, retain, reward and develop our people based solely on their abilities and contributions and without reference to their age, background, disability, genetic information, parental or family status, religion or belief, race, ethnicity, nationality, sex, sexual orientation, gender identity (or expression), political belief veteran status, or other by any other range of human difference brought about by identity and experience. We are on a journey towards ensuring our workforce is diverse at all levels and that our firm is representative of the world around us. We welcome applications from underrepresented groups.
Adjustments or accommodations
- Should you need any adjustments or accommodations to the recruitment process, at either application or interview, please contact us.
Show more
Show less","AWS, Azure, ETL, Data warehousing, Data lake, EMR, Glue, RedShift, Kinesis, Lambda, Data Factory, Databricks, Data engineering, Data science, Product development, Stream processing, Batch processing, Java, Python, Scala, Spark, SQL, Scripting, Data extraction, API, SQL queries, Data cleaning, Data wrangling, Data visualisation, Reporting, Data mining, Machine learning, Natural language processing, Agile methodologies, Scrum, PlatformasaService, Serverless architecture, Container technologies","aws, azure, etl, data warehousing, data lake, emr, glue, redshift, kinesis, lambda, data factory, databricks, data engineering, data science, product development, stream processing, batch processing, java, python, scala, spark, sql, scripting, data extraction, api, sql queries, data cleaning, data wrangling, data visualisation, reporting, data mining, machine learning, natural language processing, agile methodologies, scrum, platformasaservice, serverless architecture, container technologies","agile methodologies, api, aws, azure, batch processing, container technologies, data cleaning, data engineering, data extraction, data factory, data lake, data mining, data science, data visualisation, data wrangling, databricks, datawarehouse, emr, etl, glue, java, kinesis, lambda, machine learning, natural language processing, platformasaservice, product development, python, redshift, reporting, scala, scripting, scrum, serverless architecture, spark, sql, sql queries, stream processing"
Data Engineer,CDL Software,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-cdl-software-3671866239,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"Department:
Data Services
Employment Type:
Permanent - Full Time
Location:
North West UK - Hybrid
Description
We are looking for a talented Data Engineer who would relish the opportunity to work with our leading a cloud based, high-speed, secure data solution. Do you enjoy working as part of a collaborative, inspiring culture? If you are a Data Engineer who has experience working within cloud environment, we would like to hear from you!
We are looking for a Data Engineer to join our growing team of BI System and Data Engineers working on our cloud-based data solution product Kingfisher. Our Kingfisher product produces high-speed data management information dashboards, that replicates our core system data or any other complex data within minutes of changes taking place enabling clients and data scientists to make decisions.
About us:
CDL has established its position as a market leader in the highly competitive retail insurance sector by consistently creating powerful software solutions that deliver competitive and innovative advantage to our customers. We've custom-built our own modern, open offices set in our own landscaped campus in Stockport, employing over 600 staff, providing for over 100 internet brands (Asda, Tesco, Money Supermarket, Swinton, AA, and Sainsbury's) to name a few.
As part of the Data Services team you will work on CDL's high performance business intelligence (BI) and management information solution that enables customers to derive powerful insight and value from the data that they hold. Built using cloud technology, currently AWS, it provides performance, scalability and flexibility.
We have built a collaborative and creative culture where we employ agile techniques to deliver our pace-setting applications. We are also proud to have achieved the Top Employee 2019. We like to work in small, focused groups so you are always learning from people around you and you feel a part of the success of each project you are involved with.
Key Responsibilities
As a Data Engineer, you will have the opportunity to work in a fast-paced environment to continue to develop and support our data platform solution. Working to a clear vision and roadmap you will be part of a growing team, working collaboratively designing and developing the solution.
Skills Knowledge And Expertise
More than 3 years of strong experience with Cloud using AWS
Has AWS Solution Architect Certification
Knows AWS in depth to handle multiple AWS services including Database and networking
Experience working with Elasticsearch or Opensearch
Good experience in Terraform
Good exposure to Jenkins or similar tools
Good understanding and proven experience within a Linux/Unix environment.
Familiarity with at least one scripting or programming language (Bash, Python, Go).
Experience with Automation Tools, such as,.Jenkins
Strong analytical and problem-solving skills.
Good Team player
Self driven Individual with a positive attitude
Experience in complex and highly available software systems
Bonus Points
Any experience with Finance Domain would be advantageous.
Experience working with clients
Benefits
Hybrid working
Flexible hours
25 days holiday to start, increasing by 1 day per 1 year of service up to 30 days.
Top employer for 6th year running
Excellent training and development platform
Opportunities for career progression
Health and Wellbeing programme
Virtual activities, lunch and learns, coffee mornings and meetups.
Life and health assurance
Established Recognition Award System
Great Parental Benefits
Diversity and Inclusion network
Pension scheme
Community projects and volunteer days
Refer a friend bonus
Campus Benefits:
On-site free parking
Open plan modern offices
Quiet Spaces are also available
Shuttle bus to and from the station
Subsidised restaurant
Games room and chill out zone
Annual Hackathon Events
Social activities, regular social events through our social group Xtra, including a summer BBQ, competitions, bowling, go-karting, football games, Winter party and much more!
Car wash, barber, yoga classes, boot camp and massage services all on site
Cycle to work scheme
Shower and dry rooms
Diversity and inclusion
We're on a journey to keep innovating, that means welcoming new ideas and ways of thinking.
CDL recognises that building a diverse workforce is critical to the success of our business. We strongly encourage applications from a diverse talent pool and welcome the opportunity to discuss flexibility requirements and workplace adjustments with all our applicants.
Show more
Show less","Cloud Computing, AWS, AWS Solution Architect Certification, AWS Database, AWS Networking, Elasticsearch, Opensearch, Terraform, Jenkins, Linux, Unix, Bash, Python, Go, Automation Tools, Analytical Skills, ProblemSolving Skills, Teamwork, SelfDrive, Finance Domain, Client Experience","cloud computing, aws, aws solution architect certification, aws database, aws networking, elasticsearch, opensearch, terraform, jenkins, linux, unix, bash, python, go, automation tools, analytical skills, problemsolving skills, teamwork, selfdrive, finance domain, client experience","analytical skills, automation tools, aws, aws database, aws networking, aws solution architect certification, bash, client experience, cloud computing, elasticsearch, finance domain, go, jenkins, linux, opensearch, problemsolving skills, python, selfdrive, teamwork, terraform, unix"
Data Engineer,MRJ Recruitment,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-mrj-recruitment-3769249827,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"An exciting and rapidly expanding name within the eCommerce game that has over 80 years’ experience atop the industry, our client is a ground-breaking organisation that, since its formation in 1981, has remained true to its trademark family-owned, grassroots approach to business.
They’re currently on the lookout for an experienced Data Engineer to play a key role in the management, optimisation and transformation of the brand's data.
Working alongside a close-knit team of data specialists, It’s a great opportunity for a hands-on and detail-oriented figure that will see you working side by side with Data Architects on massive transformation projects including the creation of a bespoke data platform and more!
Salary:
Up to £35K-40k depending on experience
Location:
3 days a week in Manchester
Some of your other responsibilities include:
Utilise Azure services and work alongside cross functional teams to turn business requirements into actionable data engineering solutions
Establish close relationships with BI Developers, Product Owners, C Suite and more
Design and develop Power BI reporting solutions and play a key role in with data model design and DAX expression development.
Streamline the setup and enhancement of Azure Synapse environments, including workspaces, data lakes, pipelines, and data integration processes.
Create and set up efficient data structures like models, warehouses, and marts to support reporting, analytics, and data visualisation needs
Build effective data pipelines and ETL processes to move and transform data from different sources into Azure Synapse Analytics. Use tools like Azure Data Factory, Azure Databricks, or other relevant technologies for this purpose.
Establish strong data governance practices by incorporating data lineage, metadata management, data quality monitoring, and access controls.
Regularly watch and improve the Azure Synapse Analytics setup by fine-tuning queries, adjusting partitioning, optimizing indexing, and refining caching strategies.
Ensure you data troubleshoot and support through root cause analysis, performance bottleneck reduction and more
Keep on top of all relevant documentation (E.G data flow diagrams, data models, system architectures and more)
Some of the skills you’ll need:
Relevant commercial experience or qualifications in a similar computer science field (Azure Data Engineer, Azure Synapse Analytics)
Comfortable working with Azure services and Azure Synapse Analytics
Highly skilled in SQL with experience in data integration, ETL, and data pipeline development
Background working with data warehousing concepts, data modeling techniques, and database design principles
Proven programming skills in Python, Spark, PySpark, SparkSQL, with the ability to write efficient, maintainable code
Basic understanding of GDPR
Next Steps:
For more information regarding the company’s expansion plans and your integral way of delivering them, get in touch with us and we’ll be sure to give you the full rundown
Show more
Show less","Azure, Azure Synapse Analytics, Power BI, Data Factory, Databricks, Python, Spark, PySpark, SparkSQL, SQL, ETL, Data Warehousing, Data Modeling, Database Design, GDPR","azure, azure synapse analytics, power bi, data factory, databricks, python, spark, pyspark, sparksql, sql, etl, data warehousing, data modeling, database design, gdpr","azure, azure synapse analytics, data factory, database design, databricks, datamodeling, datawarehouse, etl, gdpr, powerbi, python, spark, sparksql, sql"
Data Engineer,hackajob,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-hackajob-3778540915,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"hackajob is a matching platform partnering with Bupa helping them to hire the best talent and build the future.
Role Overview
The Junior Data Solutions Designer will work across Bupa business to understand and translate business requirements into solutions that are fully aligned with the organisational, architectural and technology strategies and support delivery to make sure they are implemented correctly. As a Data subject matter expert, you will be responsible for driving best practice, educating Technology and business colleagues on data technologies and delivery, and feeding into Bupa’s data strategy.
This is a diverse role, and you will need to be adept at working across multiple projects, managing your team, own time and outputs with the help of the wider team. It is essential that you have a background in data and technology, adequate experience in gathering and interpreting requirements and are a good communicator who can work alongside existing team members to manage diverse stakeholders.
What you’ll do:
Liaise with stakeholders to gather, analyze, and manage business requirements.
Supporting effectively, taking accountability, building synergies, and collaborating effectively to drive outcomes.
Translating requirements into best-of-breed solutions (with the assistance of senior team members) that are aligned to the Bupa strategy and technology roadmap.
Work with the wider team to ensure solution designs are aligned with appropriate policies, rules, standards, and strategies.
Create artefacts and present designs to governance boards and design authorities to achieve appropriate sign offs.
Support delivery teams to ensure solution integrity and provide functional and technical guidance to stakeholders and delivery teams.
Develop and maintain documentation associated with the solution delivery.
Input with the support of the wider team, into the IT strategy and supporting architecture.
Aspire to build the capability to be a data subject matter expert and go-to person for all things data.
What you’ll bring:
Experience of data programs and data technologies, ideally gained within an enterprise-scale organisation or within technology or development team or similar.
Technical exposure to and/or experience in a range of technologies and architectural best practice concepts, specifically 1 or more of the following:
MS Azure Data Stack
Data Factory, Data Lake Storage, Azure SQL, Azure ML, Power BI
Snowflake knowledge would be desirable
Data Engineering Design (ETL, ELTs)
Data Modelling (Data Vault, Inmon, Kimball)
Data Science, Analytics & Reporting Design
Experience of gathering, interpreting, and managing business requirements and translating into market-leading data solutions and the full project lifecycle.
Foundational knowledge of cloud infrastructure (specifically MS Azure).
Exposure and passion for new and emerging data technologies and trends.
Exposure to concepts of Data Governance (Data Quality, Data Management, Data Lineage, Data Lifecycle Management, Data Security…)
A motivated and confident self-driven individual who uses initiative and drive to get things done.
A good communicator who can liaise easily with technical developers and non-technical colleagues alike.
Benefits
Our benefits are designed to make health happen for our people. Viva is our global well-being programme and includes all aspects of our health – from mental and physical, to financial, social and environmental well-being. We support flexible working and have a range of family-friendly benefits. Upon joining Bupa in this role you will receive the following benefits and more:
25 days holiday, increasing through length of service, with option to buy or sell
Bupa health insurance as a benefit in kind
An enhanced pension plan and life insurance
Annual performance-based bonus
Onsite gyms or local discounts where no onsite gym available
Various other benefits and online discounts
Show more
Show less","MS Azure Data Stack, Data Factory, Data Lake Storage, Azure SQL, Azure ML, Power BI, Snowflake, Data Engineering, Data Modelling, Data Vault, Inmon, Kimball, Data Science, Analytics, Reporting, Data Governance, Data Quality, Data Management, Data Lineage, Data Lifecycle Management, Data Security, Cloud Infrastructure","ms azure data stack, data factory, data lake storage, azure sql, azure ml, power bi, snowflake, data engineering, data modelling, data vault, inmon, kimball, data science, analytics, reporting, data governance, data quality, data management, data lineage, data lifecycle management, data security, cloud infrastructure","analytics, azure ml, azure sql, cloud infrastructure, data engineering, data factory, data governance, data lake storage, data lifecycle management, data lineage, data management, data modelling, data quality, data science, data security, data vault, inmon, kimball, ms azure data stack, powerbi, reporting, snowflake"
Data Engineer,BJSS,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-3438257392,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
About the Role
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","Software Engineering, DataOps, Python, Cloud Computing, AWS, Azure, GCP, Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion, CI/CD, Relational Databases, NonRelational Databases, Parallel Computing, Workflow Management, Agile, Data Pipelines, Data Storage, Data Processing","software engineering, dataops, python, cloud computing, aws, azure, gcp, databricks, data factory, synapse, kafka, redshift, glue, athena, bigquery, s3, cloud data fusion, cicd, relational databases, nonrelational databases, parallel computing, workflow management, agile, data pipelines, data storage, data processing","agile, athena, aws, azure, bigquery, cicd, cloud computing, cloud data fusion, data factory, data processing, data storage, databricks, dataops, datapipeline, gcp, glue, kafka, nonrelational databases, parallel computing, python, redshift, relational databases, s3, software engineering, synapse, workflow management"
Data Engineer,N Brown Group,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-n-brown-group-3782010724,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"N Brown – who we are and why work for us?
At N Brown, we’re committed to building a diverse workforce and creating an inclusive environment that values equality for all. Our vision is that by ‘championing inclusion, we’ll become the most loved and trusted fashion retailer’. Diversity, Equity, Inclusion and Belonging are, therefore, at the heart of our culture.
We’re a forward-thinking digital retailer with a financial services proposition to be proud of. We’re customer-obsessed, serving them through three core brands: JD Williams, Simply Be, and Jacamo. We’re experienced, with over 160 years of trading under our belt. We’re inclusive, as we believe in fashion without boundaries; and we’re sustainable, striving to make as little impact on the planet as possible.
The role
We are looking for a Data Engineer to support our Castle project team. You’ll be at the heart of a unique and exciting venture to assemble a new financial services platform as part of a
greenfield project
. You’ll leverage your industry knowledge to drive the key sourcing and commercials for the platform. Operating like an independent start-up, within an established organisation, you’ll be completely at home with agile frameworks and will fully embrace iterative and flexible commercial approaches.
What’s in it for you?
Hybrid working
24 days holiday (+ 8 bank holidays) with the option to buy an additional 10 days
Annual bonus scheme
Enhanced maternity and adoption leave
Access to Apricity, a self-funding IVF benefit at a reduced rate
Company pension with up to 8% N Brown contribution
Mental Health support both internally and externally, including access to our wellbeing champions and counselling services
A range of financial wellbeing support
Colleague discount across all N Brown brands
Onsite café with subsidised rates and local restaurant discounts!
Life Assurance and Private Medical Insurance
Paid volunteer time – all our colleagues can take a full day paid to volunteer for a charity of their choice
What will you do as an Data Engineer at N Brown?
Collaborate with a team of engineers to develop, maintain, and enhance our analytics platform.
Contribute to the data processing technical domain, including the implementation of frameworks that support N Brown's scalability in data processing. Liaise with the Architecture team to integrate state-of-the-art technologies that add value to our business.
Engage in the adoption of robust CICD practices to minimise deployment risks.
Adhere to best practices in coding and contribute to the team's knowledge sharing. Collaborate with the wider Data Engineering and Group Data team to align ways of working.
Enhance your software development skills to support the analytics platform more effectively. Advocate for test-driven, quality-first engineering practices.
Work with governance to develop scalable, robust solutions, ensuring the delivery of quality data to our customers.
Actively participate in cross-functional teams, driving best practices and focusing on delivering results.
Engage in an agile operating model, collaborating with different squads and teams.
Liaise with stakeholders to manage the Analytics Platform effectively, ensuring it is reliable, scalable, and secure. Adopt a proactive approach to monitoring.
What skills and experience will you have?
Excellent communication skills to effectively articulate technical concepts to business stakeholders and understand their needs.
Experience in implementing large-scale data projects, with a focus on end-to-end data architecture.
Deep interest in data engineering and its impact on enhancing experiences and informing business decisions.
Proficiency in working with Cloud Platforms; GCP experience is advantageous but not mandatory.
Eagerness to experiment and innovate, maintaining focus on outcomes.
Understanding of Agile principles, practices, and tools like Jira and Confluence.
What Software and Technology experience will you have?
Experience with Cloud products (Cloud SQL, BigQuery, RedShift, Snowflake, Apache Beam, Spark) or similar technologies.
Proficiency in open-source data-stack tools such as Airflow, Airbyte, DBT, Kafka, etc.
Familiarity with data visualization tools like PowerBI, Tableau, or Looker.
Knowledge of Teradata, Mainframe, and/or Google Analytics is beneficial.
Understanding of data governance, data management, analytics, science, and visualization workflows.
Awareness of the modern cloud data stack, headless BI, analytics engineering, data meshes, and lake-houses.
What to expect from our interview process?
You’ll hear from one of our Talent Team initially where they’ll talk you through the role and get to you better. You’ll then have a short call with the Head of Data Engineering to discuss the role and your experience in more detail then, if aligned, you’ll be asked to complete a panel interview meeting several stakeholders from across the business.
Ways of Working
We offer hybrid working which varies across the business depending on the role you’re in. For this role, the expectation would be 1 day per week. Our Head Office is located in the Northern Quarter in Manchester City Centre. So if you are travelling by train, tram or bus we’re perfectly located, plus we’re surrounded by cool cafes, trendy bars and the best places to eat!
Our working hours are 36.17 per week and our core working hours are between 10am - 4pm. Given we don’t have strict working hours you can find the working pattern that’s right for you.
Our promise to you:
We’re an equal opportunity employer and value diversity. We do not discriminate based on race, religion, colour, national origin, sex, gender, gender expression, sexual orientation, age, marital status, veteran status, or disability status.
What happens when you apply to a role at N Brown?
As soon as we receive your application, we’ll send you an email to let you know. We always aim to come back to you as soon as possible with an update and we really appreciate you taking the time to apply for a role with us. Good luck!
Show more
Show less","Data Engineering, Apache Beam, Spark, Cloud SQL, BigQuery, RedShift, Snowflake, Airflow, Airbyte, DBT, Kafka, PowerBI, Tableau, Looker, Teradata, Mainframe, Google Analytics, Agile, Jira, Confluence, Cloud Platforms, GCP, Cloud products, Data governance, Data management, Analytics, Science, Visualization, Modern cloud data stack, Headless BI, Analytics engineering, Data meshes, Lakehouses","data engineering, apache beam, spark, cloud sql, bigquery, redshift, snowflake, airflow, airbyte, dbt, kafka, powerbi, tableau, looker, teradata, mainframe, google analytics, agile, jira, confluence, cloud platforms, gcp, cloud products, data governance, data management, analytics, science, visualization, modern cloud data stack, headless bi, analytics engineering, data meshes, lakehouses","agile, airbyte, airflow, analytics, analytics engineering, apache beam, bigquery, cloud platforms, cloud products, cloud sql, confluence, data engineering, data governance, data management, data meshes, dbt, gcp, google analytics, headless bi, jira, kafka, lakehouses, looker, mainframe, modern cloud data stack, powerbi, redshift, science, snowflake, spark, tableau, teradata, visualization"
Senior Data Engineer,Anson McCade,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-anson-mccade-3775983997,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"Job Title: Senior Data Engineer
Location: Manchester (Hybrid Working – 2 days a week in the office)
Salary: up to £75,000
We are seeking a talented and security-clearable Data Engineer to join our dynamic team in London. As a key member of our data engineering team, you will play a crucial role in designing, developing, and maintaining robust data solutions for our clients. If you have a passion for data engineering, extensive experience with AWS, Java, and Python, and the ability to obtain security clearance, we want to hear from you.
This company is an innovation and transformation consultancy that accelerates new growth ideas from concept, through design and development to commercial success. Our multidisciplinary teams collaborate to solve complex challenges and drive meaningful impact for our clients.
Key Responsibilities:
• Design, develop, and maintain scalable and secure data solutions on AWS cloud infrastructure.
• Collaborate with cross-functional teams to understand data requirements and implement effective solutions.
• Ensure the security and integrity of data systems, complying with industry standards and regulations.
• Utilize Java and Python programming languages for efficient data processing and manipulation.
• Troubleshoot and optimize existing data pipelines for performance and reliability.
• Work closely with clients, understanding their needs, and providing on-site support as required (2 times a week).
Qualifications and Experience:
• Minimum of 5 years of experience in data engineering roles.
• Strong proficiency in AWS services and cloud-based data solutions.
• Expertise in Java and Python programming languages.
• Ability to obtain and maintain security clearance.
• Experience with data modelling, ETL processes, and data warehousing.
• Familiarity with big data technologies and tools.
• Excellent problem-solving and communication skills.
What We Offer:
• Competitive salary and benefits package.
• Opportunity for professional development and growth.
• A dynamic and collaborative work environment.
• Flexible working arrangements, with the ability to work from home and attend the office or client site 2 times a week.
Job Title: Senior Data Engineer
Location: Manchester (Hybrid Working – 2 days a week in the office)
Salary: up to £75,000
Show more
Show less","Java, Python, AWS, Data Modelling, ETL Processes, Data Warehousing, Big Data, Data Solutions, Data Pipelines, Data Security, Data Engineering, Cloud Computing, Databases, Programming","java, python, aws, data modelling, etl processes, data warehousing, big data, data solutions, data pipelines, data security, data engineering, cloud computing, databases, programming","aws, big data, cloud computing, data engineering, data modelling, data security, data solutions, databases, datapipeline, datawarehouse, etl, java, programming, python"
Senior Data Engineer,psd group,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-psd-group-3768701945,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer | IT Development | Microsoft | Azure | BI | DWH
My Client is giving you the chance to work with new cutting-edge technology maintaining your competitive advantage. Their vision is to enable the most effective data driven decision to carrying out meaningful insight to improve efficiency and recognise opportunities.
You will be supporting and working alongside the teams to transform and visualise the potential of the data and how it can be used effectively. We are looking for a Senior Data Engineer with a proven background in data warehouse architecture and implementation, alongside a track record of business intelligence and reporting to join the team, reporting to the Lead Data Engineer.
Responsibilities
Defining and building data warehouse and reporting solutions
High level documentation to be interpreted and understood by technical and non-technical audiences.
Explain technical concepts and the underlying principles to solutions during customer engagements.
Prioritise and effectively manage your time.
Expert understanding of architecture, languages, and IDEs to fulfil your duties.
High quality technical and consultancy level reports to be consumed by technical and non-technical audiences.
Confidently explain technical concepts and the underlying principles to solutions during engagements.
Identify the right approach and tooling to drive innovation and improvement of strategical goals.
Develop and deliver architectural blueprints and technical best practices underpinning core strategy set by the business.
Ensure the software meets architectural best practices and the needs of end users and achieves business goals.
Provide technical advice to the rest of the team.
Show more
Show less","Data warehousing, Data architecture, Reporting, Business intelligence, Technical documentation, Technical communication, Technical consulting, Innovation, Architectural blueprints, Software development, Team leadership, Technical advising","data warehousing, data architecture, reporting, business intelligence, technical documentation, technical communication, technical consulting, innovation, architectural blueprints, software development, team leadership, technical advising","architectural blueprints, business intelligence, data architecture, datawarehouse, innovation, reporting, software development, team leadership, technical advising, technical communication, technical consulting, technical documentation"
Senior Data Engineer,Anson McCade,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-anson-mccade-3778547485,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer - Python & AWS
Location: Manchester
Type: Full-time, Permanent
Salary: Up to £75,000 + 10% Bonus
I’m working with a global consulting and technology firm, who are seeking a Senior Consultant Data Engineer to join their dynamic data & AI team In Manchester.
You will play a key part in shaping and delivering innovative data solutions for their clients. This is a fantastic opportunity for an experienced professional looking to advance their career in a leading consulting environment.
Responsibilities:
Design, develop, and maintain robust and scalable data engineering solutions using Python and AWS technologies.
Contribute to the design and implementation of data architecture, ensuring optimal performance and reliability.
Collaborate with cross-functional teams to integrate and transform data from various sources, ensuring data quality and accuracy.
Continuously optimize and enhance existing data pipelines to meet evolving business and technical requirements.
Work closely with clients to understand their business needs and provide data engineering expertise to support decision-making.
Qualifications:
Minimum of 5years of experience in a Data Engineering role, with a focus on Python and AWS.
Strong programming skills in Python, with experience in building and optimizing data pipelines.
Solid experience with AWS data services, such as S3, Redshift, Glue, Athena, etc.
Proficient in working with relational and non-relational databases.
Ability to analyze complex data challenges and develop effective solutions.
Previous experience in a consulting environment is highly desirable.
Excellent communication and client-facing skills to convey technical concepts to non-technical
What We Offer:
Competitive salary up to £75,000
10% bonus
Comprehensive benefits package
Opportunities for professional development and advancement
Collaborative and dynamic work environment
How to Apply:
If you are a seasoned Senior Data Engineer with a focus on Python, Spark and AWS, and you are excited about contributing to innovative data solutions, we would love to hear from you.
Show more
Show less","Python, AWS, Data architecture, Data pipelines, Data quality, Data engineering, Relational databases, Nonrelational databases, S3, Redshift, Glue, Athena","python, aws, data architecture, data pipelines, data quality, data engineering, relational databases, nonrelational databases, s3, redshift, glue, athena","athena, aws, data architecture, data engineering, data quality, datapipeline, glue, nonrelational databases, python, redshift, relational databases, s3"
Data Analyst,In Technology Group,"Stalybridge, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-in-technology-group-3768705468,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"Job Title:
Data Analyst
Location:
Stalybridge
Salary:
up to £50,000
Position Overview:
As the Sole Data Analyst you will be instrumental in harnessing the power of SAP to analyse and interpret data, generate reports, and contribute to the optimization of the business processes. You will work collaboratively with cross-functional teams to ensure the effective use of SAP data for informed decision-making.
Client Overview:
The client is a well-established manufacturer with more than 3000 employees worldwide. They work with cutting edge technologies, and have lots of opportunities for growth, progression and professional development.
Key Responsibilities:
Utilize Power BI to create comprehensive reports and dashboards.
interpret the Data in order to help make informed business decisions.
Collaborate with departments to understand their data needs, ensuring SAP solutions meet and exceed expectations.
Conduct data validation and cleansing within SAP to ensure accuracy and reliability.
Identify patterns, trends, and anomalies in datasets to inform strategic decision-making.
Contribute to the implementation and maintenance of SAP system upgrades and enhancements.
Qualifications:
Bachelor's degree in Data Science, Business Analytics, or a related field.
Proven experience as a Data Analyst with experience on SAP systems.
Proficiency in data analysis tools and languages (e.g., SQL, Python) for extracting and analysing SAP data.
Excellent analytical and problem-solving skills, with keen attention to detail.
Effective communication skills to convey complex technical concepts to non-technical stakeholders.
Experience using Power BI for data visualization and reporting
Ability to work collaboratively in a team environment and independently when necessary.
If you are interested in this role and have the relevant experience and qualifications, please apply as they are looking to get this role filled asap. For further details on the role, please contact me at
sean.porter@intechnologygroup-uk.com
Show more
Show less","Data Analysis, SAP, Power BI, SQL, Python, Data Visualization, Data Cleansing, Data Validation, Problem Solving, Business Analytics, Analytical Skills, Effective Communication, Teamwork","data analysis, sap, power bi, sql, python, data visualization, data cleansing, data validation, problem solving, business analytics, analytical skills, effective communication, teamwork","analytical skills, business analytics, data validation, dataanalytics, datacleaning, effective communication, powerbi, problem solving, python, sap, sql, teamwork, visualization"
Data Analyst,NCC Group,"Manchester Area, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-ncc-group-3766637290,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"Role:
Data & Reporting Analyst
Location:
Manchester
Thanks for checking out our job opening; we are excited that YOU are interested in learning more about NCC Group.
We are on a mission to make society a safer and more secure place. Our people are the ones who make that possible; a global community of talented individuals working together towards a safer future.
We aim to create an environment where everyone can reach their full potential. We work together, we are brilliantly creative, we embrace difference and we want you to join in our mission, to make the world safer and more secure.
Take a look at our website here to learn more about why we’re one of the leading global Cyber Security and Risk Mitigation business… https://www.nccgroup.com/uk/
The Opportunity:
CC Group are recruiting for a
Data and Reporting Analyst
to join our in-house Data Reporting and MI team. We are looking for an talented and enthusiastic candidate with the ability to analyse, model and interpret data, who will manage requirements and be hands on to help develop and deliver solutions (utilising various technologies) for a wide range of data and/or reporting challenges. The successful applicant we be confident in requirements gathering workshops with at times, senior stakeholders and covering a broad range of data across a multitude of systems, business processes and areas of the organisation; with the aim of designing and delivering appropriate solutions which address any challenges at hand.
Working as part of our Global Technology Services Department (Global IT),within the Data and Reporting Team,you will support all areas of the business and be involved within a variety of projects that require development of data infrastructure (ETLs, data migrations, data models,
stored procedures and reporting outputs)with the end goal of providing clear and efficient analytics supported by robust data processes.
You will draw insights and shape processes, primarily using structured data to understand behaviour and interactions both throughout the business and relating to our internal
applications. You will be a go-to expert on data resources and tools whilst promoting
data-driven decision-making throughout the business.
Key Accountabilities:
Interpret business insights requirements through agile user story techniques to deliver technical solutions across our Data and Reporting toolsets (Azure Synapse, Power BI, SQL Data Warehouse) .
Mature and develop Data Warehouse insights through enhancement of stored procedures, views, data engineering routines, data structure and transformation processes. Strong SQL skills are a huge advantage.
Adapting existing data models within Power BI and developing new data models.
Creating advanced reporting insights and robust measures within Power BI.
Being a flexible member of a versatile team who are often the third line support for internal data issues and the team deployed for transformational projects relating to the data within our Global Applications stack.
A passion for IT and to Innovate by keeping abreast of latest technological advancements to further develop the teams capabilities.
Skills
Business Partnering
– Working hand-in-hand with stakeholders to develop business insight requirements. Able to communicate clearly and influence solution designs by demonstrating the strength of your expertise within the field of data.
Data Management
– Working knowledge of systematic databases, data warehousing, data cleansing and data transformation/migration best practices.
Data Architecture
- Create Database objects such as Tables, Views, Stored Procedures, Functions to support new product functionality and optimize existing ones for performance.
Database Understanding & Query Languages
– Demonstrable ability to use SQL (MySQL, Postgres, T-SQL, etc.) independently to query data objects with a level of comfort in transforming and aggregating information concisely and effectively.
Independence and Initiative
– Demonstrate the ability to retain information, conduct independent learning/research of new technologies as appropriate and understand complex requirements first-time. Being a self-starter who can understand, identify and execute the logical steps within the development of a solution is integral to high performance within this role.
Organizational Skills –
personal efficiency, time management skills, attention to detail and the ability to prioritize competing demands are key.
Adaptability and Agility
- Unperturbed managing activities across more than one project simultaneously. Able to adjust to the unexpected, intelligently and calmly assessing impact and options. Comfortable working in a fast-paced, dynamic environment with cross-functional teams both internally and externally.
Broad Understanding of Internal and External Data Sources
– Previous exposure to awide variety of different types of business information (e.g. Customer, Financial/transactional, HCM, Marketing) and the data models that support them.
Minimum Requirements
:
Direct Experience with Microsoft Azure, Azure Synapse and/or Microsoft DevOps would be seen as highly beneficial.
Experience of working with ERP and/or CRM systems (preferable Workday and Salesforce).
Experience with integration technologies and repository management
Ability to work in an Agile / Product-driven environments
Any prior development experience with JSON, Python or another applicable language
would be desirable although is not essential.
Desirable Requirements
:
Evidence of technical delivery in relation to Data, BI and Analytic solutions
Strong understanding of Transact-SQL or another similar query language and experience of using it to transform, manage and analyse data.
Experience Microsoft SQL Server Management Studio or a similar database management interface.
Experience working with at least one Business Intelligence (Power BI, DOMO, Tableau, etc.)
Good understanding of Data Warehousing principles and the technical capacity to aid in the growth and development of a data warehouse.
Behaviours
Focusing on Clients and Customers​
Working as One NCC
Always Learning
Being Inclusive and Respectful​
Delivering Brilliantly.
About NCC Group
The NCC Group family has over 2,200 members located all around the world, providing a trusted advisory service to 15,000 customers. Born in the UK, we have now have offices in North America, Canada, Europe, Asia- Pacific and United Arab Emirates.
We are passionate about helping our customers to protect their brand, value and reputation against the ever-evolving threat landscape. We fuel that passion with investment in our people and our business.
Our values and code of ethics are at the heart of how we operate – we work together, we are brilliantly creative and we embrace difference. We treat everyone and everything with equal respect.
We want to create an environment where all colleagues feel psychologically, emotionally and physically safe to be authentic, sharing their personal experiences to represent the diversity of the world they live in, and have equal opportunity to achieve their best.
About your application
We review every application received and will get in touch if your skills and experience match what we’re looking for. If you don’t hear back from us within 10 days, please don’t be too disappointed – we may keep your CV on our database for any future vacancies and we would encourage you to keep an eye on our career opportunities as there may be other suitable roles.
If you do not want us to retain your details, please email global.ta@nccgroup.com. All personal data is held in accordance with the NCC Group Privacy Policy. We are committed to diversity and flexibility in the workplace. If you require any reasonable adjustments to support you during the application process, please tell us at any stage.
Please note that this role involves mandatory pre-employment background checks due to the nature of the work NCC Group does. To apply, you must be willing and able to undergo the vetting process. This role being advertised will be subject to BS7858 screening as a mandatory requirement.
Show more
Show less","Azure Synapse, Power BI, SQL Data Warehouse, Microsoft DevOps, ERP systems, CRM systems, Workday, Salesforce, JSON, Python, TransactSQL, Microsoft SQL Server Management Studio, Business Intelligence, Data Warehousing, Data Architecture, Database Understanding, Query Languages, Data Management, Business Partnering, Adaptability and Agility, Organizational Skills, Independence and Initiative, Broad Understanding of Internal and External Data Sources","azure synapse, power bi, sql data warehouse, microsoft devops, erp systems, crm systems, workday, salesforce, json, python, transactsql, microsoft sql server management studio, business intelligence, data warehousing, data architecture, database understanding, query languages, data management, business partnering, adaptability and agility, organizational skills, independence and initiative, broad understanding of internal and external data sources","adaptability and agility, azure synapse, broad understanding of internal and external data sources, business intelligence, business partnering, crm systems, data architecture, data management, database understanding, datawarehouse, erp systems, independence and initiative, json, microsoft devops, microsoft sql server management studio, organizational skills, powerbi, python, query languages, salesforce, sql data warehouse, transactsql, workday"
Senior Data Engineer,iO Associates - UK/EU,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-io-associates-uk-eu-3766020472,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer
Location: Manchester (Hybrid Working Available)
Salary: £55,000 - £70,000 per annum
IO Associates are currently recruiting for a financial services company in Manchester, who are scaling their Data Engineering team.
They are looking to bring in a Senior Data Engineer to work alongside the lead data engineer and wider team to develop and implement advanced data and reporting solutions. Using the AWS Platform, you will be involved in optimising the use of data assets.
Requirements for the role:
· Experience in the AWS cloud platform with tools including S3, Glue, Lambda and Kinesis
· Programming with either Python or Typescript
· Experience with Databricks
· Strong SQL or PL/SQL experience
· ETL and pipeline development
· Knowledge of Financial Services
This role offers hybrid working (3 X Days remote) with salary between £55,000 - £70,000 depending on experience. The business also offer a comprehensive benefits suite including
· Holidays starting at 25 Days increasing to 30
· Private pension with employer contributions
· Private Health care
· Various cycling, car and tech purchase schemes.
If this is of interest, then please apply with an up-to-date CV or get in touch with Kieran Atherton (+44 1614166926) at iO associates for more information.
Show more
Show less","Data Engineering, AWS, S3, Glue, Lambda, Kinesis, Python, Typescript, Databricks, SQL, PL/SQL, ETL, Pipeline Development, Financial Services","data engineering, aws, s3, glue, lambda, kinesis, python, typescript, databricks, sql, plsql, etl, pipeline development, financial services","aws, data engineering, databricks, etl, financial services, glue, kinesis, lambda, pipeline development, plsql, python, s3, sql, typescript"
Junior Data Engineer,Pepper Mill,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-at-pepper-mill-3783938631,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"About Sparta Global
Embark on a transformative career journey with Sparta Global, where diversity, innovation, and passion for technology converge. We welcome individuals from all backgrounds, offering not just jobs, but dynamic careers in the tech industry. You'll work alongside enthusiastic professionals, receiving top-tier training and mentorship to hone your skills for success in both public and private sectors. Our commitment to designing impactful careers and coaching future leaders is evident in our over 10 prestigious awards in Learning & Development and Equality, Diversity & Inclusion. As a Top 20 Employer for Social Mobility and a proud B-Corp certified organisation, we're not just advancing careers; we're fostering a more diverse, equitable, and inclusive tech landscape. Join us in shaping the future of technology - where your growth is our mission, and your success, our pride. Apply now to be part of our award-winning team at Sparta Global.
About This Role
You'll become versatile in a wide array of tools across topics covering data focused coding, data visualisation, Cloud Services and Big Data.
You'll be designing, building, maintaining, and troubleshooting the data pipelines that enable organizations to store, process, and analyse their data, and ensuring the data is reliable through testing and debugging. Looking for efficiencies and optimising the data pipelines for scalability and performance will be a focus.
You'll be handling and working with large sets of structured and unstructured data and will be responsible for ensuring that the data is organized and available for data scientists and analysts to use.
Working with others is key, you could be working with other engineers, developers, data scientists, analysts and even stakeholders to understand their data needs.
We're not expecting you to have the proficiencies right away - that's where our award-winning Academy comes in. We are the experts in building skills and confidence in a fun and supportive environment that will not only challenge you but also develop your specialist capabilities ready to work on our clients' projects.
What we're looking for.
To be successful for this role you will demonstrate a level of ability in Python or similar. You will be passionate about technology and eager to learn programme development to an advanced level.
We're Seeking Candidates Who Can Exemplify Our Values
Empathy and Diversity: Integrity, respect, and a commitment to inclusivity.
Drive: A goal-oriented mindset with pride in exceeding targets.
Collaboration: A team-focused approach, fostering positive relationships.
Innovation: Curiosity, creativity, and openness to diverse ideas.
Flexibility: Adaptability and composure in the face of change.
As a national organisation with clients across the UK, we require flexibility and a willingness to relocate post-remote training. Deployment locations vary and cannot be guaranteed. We encourage applications from diverse backgrounds and experience levels. Eligibility to work in the UK by the start of employment is mandatory.
Why You Should Apply
Our environment is designed to nurture your talents and skills, your hard work and progress are not just appreciated - they're tangibly rewarded. We conduct performance-based reviews every six months, offering you the chance to increase your earning potential twice a year. This regular appraisal system is our way of ensuring that your efforts and achievements are consistently recognised and rewarded.
We Also Provide
20 days annual leave + bank holidays.
An extra day off for your birthday.
Pension.
Discounted gym membership.
Eye care.
Death in service cover.
Cycle to work scheme.
Season ticket loan.
Employee assistance program.
Yearly budget for personal development.
Access to alumni and community networks.
Opportunities to be brand ambassadors.
Being employed by Sparta Global is an investment in your future that pays dividends along the way. We give you breadth of experience and skills, along with increasing opportunities to develop further and earn more. No two career paths look the same at Sparta.
Our Recruitment Process
Begin your journey via our supportive recruitment process. Apply online and our team will promptly review your application, contacting successful candidates within 48 hours to initiate the next steps. If you pass our initial screening, candidates will proceed to online assessments which vary depending on the opportunity you are applying for. The final stage is a competency interview, here you'll have the opportunity to impress us with your ability to communicate effectively and exhibit behavioural competencies through relevant examples. We're looking for candidates who can demonstrate a collaborative spirit and a growth mindset.
Your dedicated Talent Team member will be with you every step of the way to support and answer any questions you have. You can also visit our YouTube channel to gain valuable insights and expert advice on virtual interviews, strategies to manage nerves, and tips on nonverbal communication.
We look forward to receiving your application - good luck!
Show more
Show less","Python, Data visualization, Cloud services, Big data, Data pipelines, Data engineering, Data analysis, Data science, Data integration, Data quality, Data security, Data governance","python, data visualization, cloud services, big data, data pipelines, data engineering, data analysis, data science, data integration, data quality, data security, data governance","big data, cloud services, data engineering, data governance, data integration, data quality, data science, data security, dataanalytics, datapipeline, python, visualization"
Junior Data Engineer,Sparta Global,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-at-sparta-global-3783937441,2023-12-17,Greater Manchester, United Kingdom,Mid senior,Hybrid,"About Sparta Global
Embark on a transformative career journey with Sparta Global, where diversity, innovation, and passion for technology converge. We welcome individuals from all backgrounds, offering not just jobs, but dynamic careers in the tech industry. You'll work alongside enthusiastic professionals, receiving top-tier training and mentorship to hone your skills for success in both public and private sectors. Our commitment to designing impactful careers and coaching future leaders is evident in our over 10 prestigious awards in Learning & Development and Equality, Diversity & Inclusion. As a Top 20 Employer for Social Mobility and a proud B-Corp certified organisation, we're not just advancing careers; we're fostering a more diverse, equitable, and inclusive tech landscape. Join us in shaping the future of technology - where your growth is our mission, and your success, our pride. Apply now to be part of our award-winning team at Sparta Global.?
About This Role
You'll become versatile in a wide array of tools across topics covering data focused coding, data visualisation, Cloud Services and Big Data.
You'll be designing, building, maintaining, and troubleshooting the data pipelines that enable organizations to store, process, and analyse their data, and ensuring the data is reliable through testing and debugging. Looking for efficiencies and optimising the data pipelines for scalability and performance will be a focus.
You'll be handling and working with large sets of structured and unstructured data and will be responsible for ensuring that the data is organized and available for data scientists and analysts to use.
Working with others is key, you could be working with other engineers, developers, data scientists, analysts and even stakeholders to understand their data needs.
We're not expecting you to have the proficiencies right away - that's where our award-winning Academy comes in. We are the experts in building skills and confidence in a fun and supportive environment that will not only challenge you but also develop your specialist capabilities ready to work on our clients' projects.
What we're looking for.
To be successful for this role you will demonstrate a level of ability in Python or similar. You will be passionate about technology and eager to learn programme development to an advanced level.
We're seeking candidates who can exemplify our values:??
Empathy and Diversity: Integrity, respect, and a commitment to inclusivity.??
Drive: A goal-oriented mindset with pride in exceeding targets.??
Collaboration: A team-focused approach, fostering positive relationships.??
Innovation: Curiosity, creativity, and openness to diverse ideas.??
Flexibility: Adaptability and composure in the face of change.
As a national organisation with clients across the UK, we require flexibility and a willingness to relocate post-remote training. Deployment locations vary and cannot be guaranteed. We encourage applications from diverse backgrounds and experience levels. Eligibility to work in the UK by the start of employment is mandatory.
Why you should apply:?
Our environment is designed to nurture your talents and skills, your hard work and progress are not just appreciated - they're tangibly rewarded. We conduct performance-based reviews every six months, offering you the chance to increase your earning potential twice a year. This regular appraisal system is our way of ensuring that your efforts and achievements are consistently recognised and rewarded.?
We also provide:??
20 days annual leave + bank holidays.??
An extra day off for your birthday.??
Pension.??
Discounted gym membership.??
Eye care.??
Death in service cover.??
Cycle to work scheme.??
Season ticket loan.??
Employee assistance program.??
Yearly budget for personal development.??
Access to alumni and community networks.??
Opportunities to be brand ambassadors.??
Being employed by Sparta Global is an investment in your future that pays dividends along the way. We give you breadth of experience and skills, along with increasing opportunities to develop further and earn more. No two career paths look the same at Sparta.??
Our Recruitment Process:?
Begin your journey via our supportive recruitment process. Apply online and our team will promptly review your application, contacting successful candidates within 48 hours to initiate the next steps. If you pass our initial screening, candidates will proceed to online assessments which vary depending on the opportunity you are applying for. The final stage is a competency interview, here you'll have the opportunity to impress us with your ability to communicate effectively and exhibit behavioural competencies through relevant examples. We're looking for candidates who can demonstrate a collaborative spirit and a growth mindset.??
Your dedicated Talent Team member will be with you every step of the way to support and answer any questions you have. You can also visit our YouTube channel to gain valuable insights and expert advice on virtual interviews, strategies to manage nerves, and tips on nonverbal communication.???
We look forward to receiving your application - good luck!???
Show more
Show less","Data Coding, Data Visualization, Cloud Services, Big Data, Data Pipelines, Data Engineering, Data Analysis, Programming, Python, Data Structures, Machine Learning, Scalability, Performance Optimization, Structured Data, Unstructured Data, Data organization, Data availability, SQL, NoSQL, Hadoop, Spark, AWS, Azure, GCP, Soft Skills, Empathy, Diversity, Integrity, Respect, Inclusivity, Goaloriented, Target Exceeding, Teamfocused, Collaboration, Positive Relationships, Curiosity, Creativity, Openness to Diversity, Adaptability, Composure, Change Management","data coding, data visualization, cloud services, big data, data pipelines, data engineering, data analysis, programming, python, data structures, machine learning, scalability, performance optimization, structured data, unstructured data, data organization, data availability, sql, nosql, hadoop, spark, aws, azure, gcp, soft skills, empathy, diversity, integrity, respect, inclusivity, goaloriented, target exceeding, teamfocused, collaboration, positive relationships, curiosity, creativity, openness to diversity, adaptability, composure, change management","adaptability, aws, azure, big data, change management, cloud services, collaboration, composure, creativity, curiosity, data availability, data coding, data engineering, data organization, data structures, dataanalytics, datapipeline, diversity, empathy, gcp, goaloriented, hadoop, inclusivity, integrity, machine learning, nosql, openness to diversity, performance optimization, positive relationships, programming, python, respect, scalability, soft skills, spark, sql, structured data, target exceeding, teamfocused, unstructured data, visualization"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Yonkers, NY",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783190174,2023-12-17,Stamford,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Yonkers-DataResearchAn.016
Show more
Show less","Python, JavaScript, JSON, Generative AI, Data Science, Product Development, OOP, R, Stakeholder Management, Written and Verbal Communication, Data Analytics, Agile Development, SQL, Linux, Cloud Computing","python, javascript, json, generative ai, data science, product development, oop, r, stakeholder management, written and verbal communication, data analytics, agile development, sql, linux, cloud computing","agile development, cloud computing, data science, dataanalytics, generative ai, javascript, json, linux, oop, product development, python, r, sql, stakeholder management, written and verbal communication"
"Senior Data Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Yonkers, NY",https://www.linkedin.com/jobs/view/senior-data-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783185434,2023-12-17,Stamford,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Senior Data Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Yonkers-SeniorDataAnal.007
Show more
Show less","Python, JavaScript, JSON, Generative AI, Machine Learning, Natural Language Processing, Data Analysis, Data Science, Product Development, Research, Strategic Problem Solving, Collaboration, Communication, OOP Languages, ObjectOriented Programming, Data Analytics, HighPerformance Coaching, Education Technology (EdTech), Online Learning, Remote Work","python, javascript, json, generative ai, machine learning, natural language processing, data analysis, data science, product development, research, strategic problem solving, collaboration, communication, oop languages, objectoriented programming, data analytics, highperformance coaching, education technology edtech, online learning, remote work","collaboration, communication, data science, dataanalytics, education technology edtech, generative ai, highperformance coaching, javascript, json, machine learning, natural language processing, objectoriented programming, online learning, oop languages, product development, python, remote work, research, strategic problem solving"
Staff Data Platform Engineer,NBCUniversal,"Englewood Cliffs, NJ",https://www.linkedin.com/jobs/view/staff-data-platform-engineer-at-nbcuniversal-3770035980,2023-12-17,Stamford,United States,Mid senior,Remote,"Company Description
We create world-class content, which we distribute across our portfolio of film, television, and streaming, and bring to life through our theme parks and consumer experiences. We own and operate leading entertainment and news brands, including NBC, NBC News, MSNBC, CNBC, NBC Sports, Telemundo, NBC Local Stations, Bravo, USA Network, and Peacock, our premium ad-supported streaming service. We produce and distribute premier filmed entertainment and programming through Universal Filmed Entertainment Group and Universal Studio Group, and have world-renowned theme parks and attractions through Universal Destinations & Experiences. NBCUniversal is a subsidiary of Comcast Corporation.
Here you can be your authentic self. As a company uniquely positioned to educate, entertain and empower through our platforms, Comcast NBCUniversal stands for including everyone. Our Diversity, Equity and Inclusion initiatives, coupled with our Corporate Social Responsibility work, is informed by our employees, audiences, park guests and the communities in which we live. We strive to foster a diverse, equitable and inclusive culture where our employees feel supported, embraced and heard. Together, we’ll continue to create and deliver content that reflects the current and ever-changing face of the world.
Job Description
As part of the Operations & Technology Engineering organization, the Data & Analytics team (D&A) supports NBCU’s vast portfolio of brands - from broadcast, cable, news, streaming service, and sports networks to film studios, world-renowned theme parks and a diverse suite of digital properties. We take pride in supplying our business groups with data to advise and shape strategic business decisions related to our content.
As the Staff Data Platform Engineer, you’ll have the opportunity to work with internal stakeholders, data engineers, visualization experts, data scientists, and other technologists across the business to ensure the delivery of data across major platforms at scale. If you’re someone with expertise in cloud, dev/data ops, and operational processes and can maintain the fine balance of business acumen and deep technical knowledge, you’ve come to the right place. Here you can create the extraordinary. Join us!
Build Cloud Tagging frameworks, and work with engineering teams on implementing & monitoring it.
Responsible for developing a robust program for Snowflake Cloud and BigQuery management frameworks that includes but is not limited to monitoring, alerting, and governance model, budgets and change management, cost optimization lifecycle, show-backs, and charge-backs
Work on Cloud Usage reporting to measure cloud cost related insights, metrics and KPIs
Work on automation solutions for Cloud usage using GCP, AWS and Python as needed
Work on BigQuery optimizations at warehouse and database level
Coordinate cloud issue resolution with Operation teams
Setup and monitor data share-in capabilities within GCP and cross cloud technologies
Communicate with leadership on Cloud usage trends, issues, and optimization techniques
Collaborate with the D&A team to help with Cloud IAM & Role Management
Ability to write complex SQL to analyze and troubleshoot data anomalies
Coordinate with Cybersecurity to adhere to NBCU security guidelines
Coordinate VPC Networking, IP Whitelisting etc
Participate in the forecasting process; build and improve forecasts and explore trends in cloud spend using internal and external tools
Work on building Cloud Governance policies, standards, and documentation
Provide guidance and support to internal and federated Solution Architects to ensure compliance to expected technical and quality standards.
Qualifications
8+ years hands-on data platform experience with at least 1+ years working in the Google Cloud environment
Experience across GCP and Cloud-based databases on IAM & Role Management
Experience in Data Sharing, Cloning, Data Exchange, DR replication features in Snowflake and BigQuery
Experience with GCP, AWS and Python
Experience with GIT development, deployment, and support of data processes and procedures
Detailed understanding of AWS, GCP, Azure, Snowflake resources (e.g. S3, Glue, Blob, EC2, Containers, etc.)
Experience with Linux & Windows environments
Proven experience and expertise in Cloud operational monitoring and administration
Experience with high volume data processing using Snowflake, Redshift, Databricks, BigQuery and other relevant cloud technologies and designing solutions for reliability and performance at scale.
Experience with BI tools
Familiarity with cataloging and data governance
Experience in partnering with Engineering and Product teams on cloud optimization, costs, and their impact on the company’s bottom line.
Effective analytical, troubleshooting, and problem-solving skills
Experience working cross-functionally to roll out and manage large initiatives.
Advanced Microsoft Excel, Outlook, Word, and PowerPoint skills
Desired Characteristics
Bachelor’s Degree Mathematics, Computer Science, or related field
Prior work experience in media preferred
Google cloud, certified, AWS Solution Architect or Snowpro Core certification a major plus.
Strong interpersonal, communication, presentation, and customer service skills
Proactive, great attention to detail, and results-oriented problem solver
This position is eligible for company sponsored benefits, including medical, dental and vision insurance, 401(k), paid leave, tuition reimbursement, and a variety of other discounts and perks. Learn more about the benefits offered by NBCUniversal by visiting the Benefits page of the Careers website. Salary range: $130,000 - $170,000
(bonus eligible)
Additional Information
NBCUniversal's policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.
If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access nbcunicareers.com as a result of your disability. You can request reasonable accommodations in the US by calling 1-818-777-4107 and in the UK by calling +44 2036185726.
Show more
Show less","Cloud Tagging, Snowflake, Amazon Web Services (AWS), BigQuery, Google Cloud Environment, Google Cloud Platform (GCP), Data Sharing, Cloning, Data Exchange, Disaster Recovery (DR), Linux, Windows, Data Governance, Microsoft Excel, Microsoft Word, Microsoft PowerPoint","cloud tagging, snowflake, amazon web services aws, bigquery, google cloud environment, google cloud platform gcp, data sharing, cloning, data exchange, disaster recovery dr, linux, windows, data governance, microsoft excel, microsoft word, microsoft powerpoint","amazon web services aws, bigquery, cloning, cloud tagging, data exchange, data governance, data sharing, disaster recovery dr, google cloud environment, google cloud platform gcp, linux, microsoft excel, microsoft powerpoint, microsoft word, snowflake, windows"
Senior Data Analytics Specialist,City of New York,"Queens, NY",https://www.linkedin.com/jobs/view/senior-data-analytics-specialist-at-city-of-new-york-3728292935,2023-12-17,Stamford,United States,Mid senior,Hybrid,"The NYC Department of Design and Construction, Project Controls Division, seeks a Senior Data Analytics Specialist. The selected candidate will perform advanced data analysis to proactively solve highly complex problems in support of capital project delivery by identifying drivers of project performance and uncover correlations between project data. The selected candidate will communicate insights to senior staff and review work performed by other data analysts and data specialists. Responsibilities include collecting, transforming, analyzing, and interpreting large, complex datasets across projects using statistical and machine learning techniques to identify patterns and insights that can help project management team make data-driven decisions. The selected candidate will design data models and tools to support analysis, develop predictive models, uncover correlations between project data, identify drivers of project performance, use appropriate statistical and machine learning techniques for predictive analytics, and develop visualizations based on project management needs. Effort will require review and selection of best available technology such as Azure Analytics services, Python libraries and data science algorithms such as random forest.
Minimum Qualifications
A baccalaureate degree from an accredited college and four years of satisfactory full-time experience related to the area(s) required by the particular position; or,
Education and/or experience which is equivalent to ""1"" above.
Preferred Skills
Strong analytical skills Background in computer science, mathematics and/or statistics desirable Experience with machine learning algorithms and statistical modeling Experience with programming languages such as Python, R, and SQL Experience with Azure services Experience with data management technology architecture Strong communication skills to explain complex findings to non-technical stakeholders.
Public Service Loan Forgiveness
As a prospective employee of the City of New York, you may be eligible for federal loan forgiveness programs and state repayment assistance programs. For more information, please visit the U.S. Department of Education’s website at https://studentaid.gov/pslf/
Residency Requirement
New York City Residency is not required for this position
Additional Information
The City of New York is an inclusive equal opportunity employer committed to recruiting and retaining a diverse workforce and providing a work environment that is free from discrimination and harassment based upon any legally protected status or protected characteristic, including but not limited to an individual's sex, race, color, ethnicity, national origin, age, religion, disability, sexual orientation, veteran status, gender identity, or pregnancy.
$75,000.00 – $118,063.00
Show more
Show less","Data Analysis, Statistics, Machine Learning, Azure Analytics, Python, R, SQL, Data Management, Predictive Analytics, Data Modeling, Data Visualization","data analysis, statistics, machine learning, azure analytics, python, r, sql, data management, predictive analytics, data modeling, data visualization","azure analytics, data management, dataanalytics, datamodeling, machine learning, predictive analytics, python, r, sql, statistics, visualization"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Brownsville, TX",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783188341,2023-12-17,Brownsville,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Brownsvi-DataResearchAn.008
Show more
Show less","Python, JavaScript, JSON, Generative AI, Educational tools, OOP languages, Data science, R, English communication skills, Data analytics, AI outputs, Coaching, Git","python, javascript, json, generative ai, educational tools, oop languages, data science, r, english communication skills, data analytics, ai outputs, coaching, git","ai outputs, coaching, data science, dataanalytics, educational tools, english communication skills, generative ai, git, javascript, json, oop languages, python, r"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Brownsville, TX",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783187379,2023-12-17,Brownsville,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Brownsvi-DataScientist.009
Show more
Show less","Python, JavaScript, JSON, R, OOP (ObjectOriented Programming), Machine Learning, Data Analytics, Data Science, Generative AI, Product Development, Research, Technical Writing, Communication, Stakeholder Management, Project Management, SQL","python, javascript, json, r, oop objectoriented programming, machine learning, data analytics, data science, generative ai, product development, research, technical writing, communication, stakeholder management, project management, sql","communication, data science, dataanalytics, generative ai, javascript, json, machine learning, oop objectoriented programming, product development, project management, python, r, research, sql, stakeholder management, technical writing"
People M&A Data Analyst,LSEG (London Stock Exchange Group),"New York, NY",https://www.linkedin.com/jobs/view/people-m-a-data-analyst-at-lseg-london-stock-exchange-group-3779371657,2023-12-17,Staten Island,United States,Associate,Hybrid,"Role Summary
Working within the People Function M&A team, this role offers the opportunity to support across the full deal life cycle, being a key member of a small dynamic global team. To be successful the role holder will need the ability to partner with teams worldwide to effectively work through deal processes, identifying and managing risks, and working to remove barriers to execution.
What You'll Be Doing
Provide support to People Function M&A team on transactions and development of M&A materials, including the People M&A Playbook
Provide support through end-to-end transaction management, including pre-sign analysis, analysis of people policies and programmes, communications and consultation processes and integration related activities.
Coordinate with internal teams and liaise with People Function members around the globe, ensuring aligned processes and efficient project management.
Manage creation of documents for employee on-boarding and separation processes
Accountable for gathering and managing the People baseline data during each phase of a transaction
Support creation of all background People information needed to support dispositions, including data rooms.
Create dashboards and presentations using PowerPoint
What You'll Bring
Strong attention to detail and high degree of accuracy
Ability to partner with individuals within and outside of People Function
Ability to function and flex in a fast-paced environment spanning multiple time-zones
Experience operating within a large corporate People Function
Some experience of performing data analysis, manipulation and management using Excel
Curiosity to learn about both M&A activity and other areas of the People Function, together with a commitment to problem-solving.
Knowledge of working across different teams with a People Function and experience of People M&A activities would be an advantage
You Receive
Development: e-learning platform, a range of workshops and international projects
Onboarding: structured training and buddy support
Information: access to real-time economic data
Diversity: colleagues from 40 countries [working in Gdynia]
Charity: 2 additional days off for voluntary jobs
Perks: multisport card, private medical healthcare, employee assistance program, pension plan,
Office: chillout room with video games, massage chairs and ping-pong table.
Activities: sport teams and social events to join
Future: corporate pension plan and employee insurance
LSEG is a leading global financial markets infrastructure and data provider. Our purpose is driving financial stability, empowering economies and enabling customers to create sustainable growth.
Our purpose is the foundation on which our culture is built. Our values of
Integrity, Partnership
,
Excellence
and
Change
underpin our purpose and set the standard for everything we do, every day. They go to the heart of who we are and guide our decision making and everyday actions.
Working with us means that you will be part of a dynamic organisation of 25,000 people across 65 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.
LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.
We are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone’s race, religion, colour, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants' and employees' religious practices and beliefs, as well as mental health or physical disability needs.
Please take a moment to read this privacy notice carefully, as it describes what personal information London Stock Exchange Group (LSEG) (we) may hold about you, what it’s used for, and how it’s obtained, your rights and how to contact us as a data subject.
If you are submitting as a Recruitment Agency Partner, it is essential and your responsibility to ensure that candidates applying to LSEG are aware of this privacy notice.
Show more
Show less","Microsoft PowerPoint, Excel, Data analysis and management, Communication, Negotiation, Teamwork, Problemsolving, Attention to detail, Accuracy, Flexibility, Time management, Project management, Microsoft Office Suite, Data visualization, Data rooms, Employee onboarding, Separation processes, M&A Playbook, People Function, Employee assistance programs, Multinational work environment, Diversity and inclusion","microsoft powerpoint, excel, data analysis and management, communication, negotiation, teamwork, problemsolving, attention to detail, accuracy, flexibility, time management, project management, microsoft office suite, data visualization, data rooms, employee onboarding, separation processes, ma playbook, people function, employee assistance programs, multinational work environment, diversity and inclusion","accuracy, attention to detail, communication, data analysis and management, data rooms, diversity and inclusion, employee assistance programs, employee onboarding, excel, flexibility, ma playbook, microsoft office suite, microsoft powerpoint, multinational work environment, negotiation, people function, problemsolving, project management, separation processes, teamwork, time management, visualization"
Sr Aviation Ops Data Analyst,MAG Aerospace,"Lexington Park, MD",https://www.linkedin.com/jobs/view/sr-aviation-ops-data-analyst-at-mag-aerospace-3757505837,2023-12-17,Saint Marys City,United States,Mid senior,Onsite,"Position Summary
As a Senior Research Specialist – Aviation Operational Data Analyst for the Analysis and Reporting Tool (ART) engineering and analysis team, you will be joining a strong team providing real time data analysis to support aviation operations and operational test at the Joint Simulation Environment (JSE), Patuxent River, MD and other elite aviator training facilities. You will be supporting decision-makers by quantifying naval warfighting capabilities and limitations under realistic employment conditions; developing empirical or theoretical estimates for the performance of fleet systems and platforms; and developing creative solutions to complex tactical and force design challenges.
MUST BE A US CITIZEN**
Essential Duties And Responsibilities
Apply standard research methodologies to gather, process, and analyze data. Use relevant database software to effectively organize and present data and generate reports.
Generate compelling visualizations of quantitative datasets.
Develop institutional knowledge of the mission of MSEA, including the team’s major customers.
Participate in meetings with the team. Support preparations for customer interactions.
Present and summarize data effectively in oral and written communication.
Requirements
Required Experience:
10 years of relevant work experience in data collection and metrics creation.
Skilled in programming (R, Python), statistics, SQL, data visualization, and other data analysis fundamentals.
Analyze, interpret, summarize, and present data effectively.
Demonstrated ability to apply advanced principles, theories, and concepts to solve uniquely complex problems.
Conceptualize and develop solutions, formulate problem statements, and develop analysis methods and approaches.
Excellent communication skills, both oral and written; experience in packaging analytic results in report format as well as in the development and presentation of oral briefings; interact with and advise senior levels of leadership.
Ability to work in an unstructured environment.
Education / Certifications
MS in Scientific Discipline; Data Science, Computer Science, Engineering, Information Technology, Mathematics, or Physics or equivalent experience.
PHD preferred.
Desired Skill, Experience, Or Certifications
Experience with supporting Special Access Program (SAP) efforts.
Aviation experience
Clearance
Must be able to obtain DoD Secret security clearance.
Show more
Show less","Data Collection, Metrics Creation, R Programming, Python Programming, Statistics, SQL, Data Visualization, Data Analysis, Advanced Principles, Theories, Concepts, ProblemSolving, Solution Development, Report Writing, Oral Briefings, Leadership Interaction, Unstructured Environment, Scientific Discipline, Data Science, Computer Science, Engineering, Information Technology, Mathematics, Physics, Special Access Program (SAP), Aviation, DoD Secret Security Clearance","data collection, metrics creation, r programming, python programming, statistics, sql, data visualization, data analysis, advanced principles, theories, concepts, problemsolving, solution development, report writing, oral briefings, leadership interaction, unstructured environment, scientific discipline, data science, computer science, engineering, information technology, mathematics, physics, special access program sap, aviation, dod secret security clearance","advanced principles, aviation, computer science, concepts, data collection, data science, dataanalytics, dod secret security clearance, engineering, information technology, leadership interaction, mathematics, metrics creation, oral briefings, physics, problemsolving, python programming, r programming, report writing, scientific discipline, solution development, special access program sap, sql, statistics, theories, unstructured environment, visualization"
Data Systems Analyst – Advance,Chenega MIOS SBU,"Patuxent River, MD",https://www.linkedin.com/jobs/view/data-systems-analyst-%E2%80%93-advance-at-chenega-mios-sbu-3639936174,2023-12-17,Saint Marys City,United States,Mid senior,Hybrid,"Patuxent River, MD
Are you ready to enhance your skills and build your career in a rapidly evolving business climate? Are you looking for a career where professional development is embedded in your employer’s core culture? If so, Chenega Military, Intelligence & Operations Support (MIOS) could be the place for you! Join our team of professionals who support large-scale government operations by leveraging cutting-edge technology and take your career to the next level!
Chenega Agile Real-Time Solutions (CARS) was created with the purpose of providing integrated enterprise IT support to Federal customers both CONUS and OCONUS. CARS employs Subject Matter Experts (SMEs) with decades of experience working in the Federal marketplace.
Summary
Chenega Agile Real-Time Solutions (CARS) is looking for Data Systems Analyst – Advance our team in the Patuxent River area. This team supports a Department of Defense (DoD) client.
Duties And Responsibilities
Discover opportunities for data acquisition and data systems integration; perform research, design, and development for databases and data processing systems; develop data systems, architectures, and models.
Leverage high-performance computing infrastructures into systems architecture.
Recommend the best architectures, tools, and technologies to address organizational needs.
Other duties as assigned.
Minimum Qualifications
Graduate degree and 7+ years of relevant experience OR
High school diploma or GED equivalent, active CSWF Certifications: CSWF Code 421, 422, or CISM, CISSP, and GSLC
Active Top Secret w/SCI eligible clearance
Knowledge, Skills, And Abilities
Ability to obtain Secret clearance
Ability to work independently and yet be effective within a team setting
Must be capable of managing multiple efforts with time-related constraints in a fast-paced contracting environment.
Demonstrated ability to effectively communicate and collaborate with diverse internal and external stakeholder groups and individuals.
Friendly presence, helpful attitude, good interpersonal skills, and ability to work well with others.
Excellent skills in Microsoft Word, Excel, and other Office applications.
Experience working in a home office setting as well as the ability to train end users on frequently asked technical issues.
Ability to provide technical assistance and support over the phone with good phone skills and a professional demeanor.
Previous customer service experience strongly desired.
Good problem-solving skills with the ability to visualize a problem or situation and think abstractly to solve it.
How you’ll grow
At Chenega MIOS, our professional development plan focuses on helping our team members at every level of their careers to identify and use their strengths to do their best work every day. We believe there’s always room to learn from entry-level employees to senior leaders.
We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs, our professionals have a variety of opportunities to continue to grow throughout their careers.
Benefits
At Chenega MIOS, we know that great people make a great organization. We value our team members and offer them a broad range of benefits.
Learn more about what working at Chenega MIOS can mean for you.
Chenega MIOS’s culture
Our positive and supportive culture encourages our team members to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them be healthy, centered, confident, and aware. We offer well-being programs and continuously look for new ways to maintain a culture where we excel and lead healthy, happy lives.
Corporate citizenship
Chenega MIOS is led by a purpose to make an impact that matters. This purpose defines who we are and extends to our relationships with our clients, team members, and communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities.
Learn more about Chenega’s impact on the world.
Chenega MIOS News- https://chenegamios.com/news/
Tips from your Talent Acquisition team
We Want Job Seekers Exploring Opportunities At Chenega MIOS To Feel Prepared And Confident. To Help You With Your Research, We Suggest You Review The Following Links
Chenega MIOS web site - www.chenegamios.com
Glassdoor - https://www.glassdoor.com/Overview/Working-at-Chenega-MIOS-EI_IE369514.11,23.htm
LinkedIn - https://www.linkedin.com/company/1472684/
Facebook - https://www.facebook.com/chenegamios/
Chenega Corporation and family of companies is an EOE.
Equal Opportunity Employer/Veterans/Disabled
Native preference under PL 93-638.
We participate in the E-Verify Employment Verification Program.
Show more
Show less","Data Systems Analyst, Data acquisition, Database, Data systems integration, Data processing systems, Architectures, Models, Highperformance computing infrastructures, Microsoft Word, Microsoft Excel, Office applications, Customer service, Problemsolving, Communication, Collaboration, Visualization, Abstract thinking, Secret clearance, Top Secret w/SCI eligible clearance, CSWF Certifications: CSWF Code 421 422 or CISM CISSP and GSLC, CSWF, CISM, CISSP, GSLC","data systems analyst, data acquisition, database, data systems integration, data processing systems, architectures, models, highperformance computing infrastructures, microsoft word, microsoft excel, office applications, customer service, problemsolving, communication, collaboration, visualization, abstract thinking, secret clearance, top secret wsci eligible clearance, cswf certifications cswf code 421 422 or cism cissp and gslc, cswf, cism, cissp, gslc","abstract thinking, architectures, cism, cissp, collaboration, communication, cswf, cswf certifications cswf code 421 422 or cism cissp and gslc, customer service, data acquisition, data processing systems, data systems analyst, data systems integration, database, gslc, highperformance computing infrastructures, microsoft excel, microsoft word, models, office applications, problemsolving, secret clearance, top secret wsci eligible clearance, visualization"
Junior Data Analyst with Security Clearance,ClearanceJobs,"Patuxent River, MD",https://www.linkedin.com/jobs/view/junior-data-analyst-with-security-clearance-at-clearancejobs-3786801886,2023-12-17,Saint Marys City,United States,Mid senior,Hybrid,"Spalding Consulting, Inc. is seeking a Junior Data Analyst in Patuxent River, MD. Spalding Consulting, Inc. is a professional services company delivering cutting-edge solutions to the Department of Defense since 2001. Our expert-level solutions include software development, information technology, program management, financial management and business intelligence services. Spalding Consulting offers competitive compensation, career development, flexible work schedules and excellent benefits. Position Type: Full-Time Work Location: Hybrid remote/in-office (see on-site requirements below) **On-Site Requirements: On-boarding will require 1-2 visits to Lexington Park, MD. Telework is authorized, however on-site support will be required approximately 1 day per week. Future on-site/telework requirements/schedules may change as additional client direction is received. Essential Functions:
Perform data collection, data analysis, testing, and troubleshooting on large quantities of corporate operations data.
Extract, transform, and load data using TOAD, SQL, and/or Informatica.
Provide configuration and/or data management for data, information, files, and documents.
Other duties as assigned or required. Qualifications and Experience: Required:
Experience creating/executing SQL queries in Business Objects, TOAD, Cognos, or similar tool required.
Proficiency in Excel (pivot tables, macros, VBA) required. Desired:
Familiarity with visualization tools such as Qlik or Tableau desired.
Experience developing/creating reports with Business Objects or another BI query tool desired.
Experience providing support to a NAVAIR IT organization/environment desired. Security Clearance: Must be eligible to obtain a Secret clearance, active or interim Secret is preferred. Requirements to obtain a clearance include US Citizenship, security investigation, etc. Education: BS/BA OR AA/AS degree and 1-year related experience OR HS and 2 years related experience. Spalding Consulting, Inc. is committed to providing equal employment opportunities to all applicants and employees. We will not discriminate against any employee or applicant on the basis of race, color, ethnic origin, national origin, creed, religion, political belief, sex (including pregnancy), sexual orientation, gender identity, marital status, age, military status, physical or mental disability, or any other legally protected basis, in accordance with applicable federal, state or local laws. Spalding Consulting, Inc. is an Affirmative Action/Equal Opportunity Employer and encourages minorities, women, disabled, and veterans to apply for job openings within our company. Accessibility: If you are interested in applying for employment with Spalding Consulting, Inc. and need special assistance or an accommodation to apply for a posted position, contact our Recruiting department via email at or call us at 301-737-0150 Ext. 500. Pay Transparency Employee Polygraph Protection Act Employee Rights under the Family and Medical Leave Act Equal Employment Opportunity Know Your Rights: Workplace Discrimination is Illegal
Show more
Show less","SQL, Excel, Business Objects, Cognos, TOAD, Informatica, Qlik, Tableau, VBA, Data collection, Data analysis, Data troubleshooting, Data transformation, Data loading, Data management, Information management, File management, Document management, Report development, Report creation, BI query tools, IT support, NAVAIR IT","sql, excel, business objects, cognos, toad, informatica, qlik, tableau, vba, data collection, data analysis, data troubleshooting, data transformation, data loading, data management, information management, file management, document management, report development, report creation, bi query tools, it support, navair it","bi query tools, business objects, cognos, data collection, data loading, data management, data transformation, data troubleshooting, dataanalytics, document management, excel, file management, informatica, information management, it support, navair it, qlik, report creation, report development, sql, tableau, toad, vba"
Sr. Research Specialist/Data Analyst (Aviation) - Ability to Obt with Security Clearance,ClearanceJobs,"Patuxent River, MD",https://www.linkedin.com/jobs/view/sr-research-specialist-data-analyst-aviation-ability-to-obt-with-security-clearance-at-clearancejobs-3753450050,2023-12-17,Saint Marys City,United States,Mid senior,Hybrid,"Are you a highly motivated Senior Research Specialist/Data Analyst? Do you have a passion for developing creative solutions to complex tactical and force design challenges? Then AMERICAN SYSTEMS Senior Research Specialist/Aviation Operational Data Analyst position is for you! Join us today! As a Senior Research Specialist/Data Analyst, you will
Join an engaged, 100% employee-owned company
Work with a team of supportive, driven, highly skilled Subject Matter Experts
Provide real time data analysis to support aviation operations and operational test at the Joint Simulation Environment (JSE), Patuxent River, MD and other elite aviator training facilities
Receive a robust benefits package that includes our Employee Stock Ownership Plan! (ESOP) A week in the life of the Senior Research Specialist/Data Analyst includes:
Apply standard research methodologies to gather, process, and analyze data. Use relevant database software to effectively organize and present data and generate reports.
Generate compelling visualizations of quantitative datasets.
Develop institutional knowledge of the mission of Modeling & Simulation Enterprise Applications (MSEA), including the team's major customers.
Participate in meetings with the team. Support preparations for customer interactions.
Present and summarize data effectively in oral and written communication. Job Requirements Job Requirements
Ability to obtain a Secret Clearance (US Citizenship required)
Master's degree in Scientific Discipline; Data Science, Computer Science, Engineering, Information Technology, Mathematics, or Physics or equivalent experience.
At least 10 years of relevant work experience Required Knowledge, Skills, & Abilities:
Data collection and metrics creation
Skilled in programming (R, Python), statistics, SQL, data visualization, and other data analysis fundamentals
Able to analyze, interpret, summarize, and present data effectively
Demonstrated ability to apply advanced principles, theories, and concepts to solve uniquely complex problems
Able to conceptualize and develop solutions, formulate problem statements, and develop analysis methods and approaches
Excellent communication skills, both oral and written; experience in packaging analytic results in report format as well as in the development and presentation of oral briefings; interact with and advise senior levels of leadership
Ability to work in an unstructured environment Founded in 1975, AMERICAN SYSTEMS is one of the largest employee-owned companies in the United States. We are a government services contractor focused on delivering Strategic Solutions to complex national priority programs with 100+ locations worldwide. Through our focus on quality, strong cultural beliefs and innovation we deliver excellence every day. Company Awards:
Forbes National Best Midsize Companies
Energage National Best Workplaces, National
Washington Post Best Workplaces, Veteran Hiring Awards:
GOLD AWARD by U.S. Department of Labor HIRE Vets Medallion
BEST FOR VETS: EMPLOYER by Military Times
TOP 10 MILITARY FRIENDLY EMPLOYER by MilitaryFriendly.com AMERICAN SYSTEMS is committed to pay transparency for our applicants and employee-owners. The salary range for this position is$100,000-135,000. Actual compensation will be determined based on several factors permitted by law. AMERICAN SYSTEMS provides for the welfare of its employees and their dependents through a comprehensive benefits program by offering healthcare benefits, paid leave, retirement plans, insurance programs, and education and training assistance. EOE Minorities/Women/Disabled/Veterans/Gender Identity/Sexual Orientation
Show more
Show less","Data Analysis, Data Visualization, R, Python, SQL, Statistics, Data Collection, Metrics Creation, Data Presentation, Data Summarization, Data Interpretation, Advanced Principles, Problem Solving, Solution Development, Problem Statement Formulation, Analysis Methods, Oral Communication, Written Communication, Report Writing, Oral Briefing, Unstructured Environment","data analysis, data visualization, r, python, sql, statistics, data collection, metrics creation, data presentation, data summarization, data interpretation, advanced principles, problem solving, solution development, problem statement formulation, analysis methods, oral communication, written communication, report writing, oral briefing, unstructured environment","advanced principles, analysis methods, data collection, data interpretation, data presentation, data summarization, dataanalytics, metrics creation, oral briefing, oral communication, problem solving, problem statement formulation, python, r, report writing, solution development, sql, statistics, unstructured environment, visualization, written communication"
Sr Aviation Ops Data Analyst with Security Clearance,ClearanceJobs,"Lexington Park, MD",https://www.linkedin.com/jobs/view/sr-aviation-ops-data-analyst-with-security-clearance-at-clearancejobs-3760175892,2023-12-17,Saint Marys City,United States,Mid senior,Hybrid,"Position Summary As a Senior Research Specialist - Aviation Operational Data Analyst for the Analysis and Reporting Tool (ART) engineering and analysis team, you will be joining a strong team providing real time data analysis to support aviation operations and operational test at the Joint Simulation Environment (JSE), Patuxent River, MD and other elite aviator training facilities. You will be supporting decision-makers by quantifying naval warfighting capabilities and limitations under realistic employment conditions; developing empirical or theoretical estimates for the performance of fleet systems and platforms; and developing creative solutions to complex tactical and force design challenges. **MUST BE A US CITIZEN** Essential Duties and Responsibilities
Apply standard research methodologies to gather, process, and analyze data. Use relevant database software to effectively organize and present data and generate reports.
Generate compelling visualizations of quantitative datasets.
Develop institutional knowledge of the mission of MSEA, including the team's major customers.
Participate in meetings with the team. Support preparations for customer interactions.
Present and summarize data effectively in oral and written communication. Requirements Required Experience:
10 years of relevant work experience in data collection and metrics creation.
Skilled in programming (R, Python), statistics, SQL, data visualization, and other data analysis fundamentals.
Analyze, interpret, summarize, and present data effectively.
Demonstrated ability to apply advanced principles, theories, and concepts to solve uniquely complex problems.
Conceptualize and develop solutions, formulate problem statements, and develop analysis methods and approaches.
Excellent communication skills, both oral and written; experience in packaging analytic results in report format as well as in the development and presentation of oral briefings; interact with and advise senior levels of leadership.
Ability to work in an unstructured environment. Education / Certifications
MS in Scientific Discipline; Data Science, Computer Science, Engineering, Information Technology, Mathematics, or Physics or equivalent experience.
PHD preferred. Desired Skill, Experience, or Certifications
Experience with supporting Special Access Program (SAP) efforts.
Aviation experience Clearance
Must be able to obtain DoD Secret security clearance.
Show more
Show less","Aviation, Data Analysis, Realtime Data, Simulation, Naval Operations, Warfighting Capabilities, Fleet Systems, Tactical Design, Research Methodologies, Data Collection, Metrics Creation, Data Visualization, Data Presentation, SQL, R, Python, Statistics, Data Analysis Fundamentals, Scientific Discipline, Data Science, Computer Science, Engineering, Information Technology, Mathematics, Physics, Special Access Program (SAP), DoD Secret Security Clearance","aviation, data analysis, realtime data, simulation, naval operations, warfighting capabilities, fleet systems, tactical design, research methodologies, data collection, metrics creation, data visualization, data presentation, sql, r, python, statistics, data analysis fundamentals, scientific discipline, data science, computer science, engineering, information technology, mathematics, physics, special access program sap, dod secret security clearance","aviation, computer science, data analysis fundamentals, data collection, data presentation, data science, dataanalytics, dod secret security clearance, engineering, fleet systems, information technology, mathematics, metrics creation, naval operations, physics, python, r, realtime data, research methodologies, scientific discipline, simulation, special access program sap, sql, statistics, tactical design, visualization, warfighting capabilities"
Data Engineer,Little Caesars Pizza,United States,https://www.linkedin.com/jobs/view/data-engineer-at-little-caesars-pizza-3752877354,2023-12-17,Kitchener, Canada,Associate,Remote,"Build a Bigger, Better, Bolder Future
Imagine working for a company that measures its success based off the growth of its colleagues, a company that invests in its future by investing in you. Little Caesars is a company where our colleagues make an impact.
Your Mission:
In this role, you will be responsible for moderately complex designing and growing our Business Intelligence product in all aspects of our Business Intelligence solution, such as building new BI features, data modeling, machine integration, dashboard creation, Analyze Business Intelligence project requirements and perform the design, coding, testing, and documentation steps required to complete the project. A responsibility of this role will be providing technical assistance to less experienced staff.
How You’ll Make an Impact:
Perform moderately complex data analysis and data profiling against source systems and data warehouse.
Apply data warehousing best practices to define, design, and develop data transformation rules.
Perform intermediate ETL design, development, and support for moving large data volumes from various sources into the various destinations after significant cleansing, transformation, and processing.
Handle job stream design, development, and implementation.
Experienced partnering with the business to develop reports and dashboards to meet requirements, including: Develop test plans, test scripts, and test conditions based on the business and system requirements, in addition to testing solutions to validate whether requirements have been met, Identify project tasks and providing estimates for those tasks.
Provide end-user support in post deployment phase; assess and evaluate all feedback to ensure that the requirements necessary to correct issues are addressed.
Analyze business requirements and work with client areas to produce functional specifications for use as reference in creation of the reporting and analytical solutions.
Develop and maintain relationships with business customers to identify and develop additional analytic and reporting capabilities.
Define and prepare advanced data delivery designs and provide development support to leverage technology investments to support business needs.
Coordinate prototype reviews and validating solutions meet requirements and service-level agreements.
Continue to apply concepts and develop technical knowledge of features and functions of business intelligence tools.
Determine the level of effort for development work given a set of requirements and determine estimate to complete through implementation.
Understand and identify the correct resources to help define and validate both requirements and specifications.
Resolve system issues and respond to suggestions for improvements.
Communicate and monitor compliance with existing practices.
Maintain existing business reporting and analytical solutions by providing production support for troubleshooting and regular processing needs.
Train and assist users with use of the business intelligence toolset.
Provide guidance in the creation of reporting and analytical solutions.
Who You Are:
Bachelor’s in Computer Science, equivalent degree, or related course work in systems or computer programming. Equivalent experience may be considered in lieu of formal degree.
Minimum of five (5) years of programming experience with advanced SQL experience leveraging complex SQL statements, PL/SQL, and analytics functions to perform data profiling, data validation, and performance tuning.
Minimum of three (3) years’ experience with Microsoft BI Platform (SQL Server, SSIS, SSAS, SSRS, Integration, Reporting, and Analysis tools.
Demonstrated experience with other Business Intelligence tools (Cognos, Business Objects, Tableu, Qlikview) is a plus.
Evidence of excellent analytical and problem-solving skills. Ability to find creative solutions to solve problems while staying within practice guidelines.
Possess In-depth knowledge of data warehousing and business intelligence concepts and methods. Sound understanding of business intelligence best practices, relational data structures, dimensional data modeling, and data warehouse and reporting techniques.
Proof of experience contributing to and creating data warehouse project deliverables such as source system analysis, business definitions, data dictionary, source to target mappings, and code.
In-depth experience preparing advanced report and dashboard designs.
Evidence of Tableau Server administration and dashboard development experience.
Evidence of ability to document complex business processes.
Demonstrated excellence in communication skills working with business areas and other non-technical groups, translating requests into requirements with the ability to express ideas effectively, verbally, and in writing.
Experience working in a client-facing role with the ability to build strong relationships.
Must have the ability to work independently and within a team environment.
Evidence of ongoing pursuit of technical knowledge and experience.
Ability to participate in an on-call rotation performing weekend and after-hours support.
Ability to work various schedule to support a 24 X 7 shift which can include nights, weekends and holidays.
Where You’ll Work:
In a normal office environment where there is no physical discomfort due to temperature, noise, dust and the like.
Show more
Show less","SQL, PL/SQL, Data Profiling, Data Validation, Performance Tuning, Microsoft BI Platform, SQL Server, SSIS, SSAS, SSRS, Integration, Reporting, Analysis, Cognos, Business Objects, Tableu, Qlikview, Data Warehousing, Business Intelligence, Dimensional Data Modeling, Data Dictionary, Tableau Server, Documentation, Communication, ClientFacing Role, Team Environment, OnCall Rotation, 24 X 7 Shift","sql, plsql, data profiling, data validation, performance tuning, microsoft bi platform, sql server, ssis, ssas, ssrs, integration, reporting, analysis, cognos, business objects, tableu, qlikview, data warehousing, business intelligence, dimensional data modeling, data dictionary, tableau server, documentation, communication, clientfacing role, team environment, oncall rotation, 24 x 7 shift","24 x 7 shift, analysis, business intelligence, business objects, clientfacing role, cognos, communication, data dictionary, data profiling, data validation, datawarehouse, dimensional data modeling, documentation, integration, microsoft bi platform, oncall rotation, performance tuning, plsql, qlikview, reporting, sql, sql server, ssas, ssis, ssrs, tableau server, tableu, team environment"
Data Engineer - NBC Sports Next,NBC Sports Next,"Orlando, FL",https://www.linkedin.com/jobs/view/data-engineer-nbc-sports-next-at-nbc-sports-next-3773999986,2023-12-17,Kitchener, Canada,Associate,Remote,"Company Description
We create world-class content, which we distribute across our portfolio of film, television, and streaming, and bring to life through our theme parks and consumer experiences. We own and operate leading entertainment and news brands, including NBC, NBC News, MSNBC, CNBC, NBC Sports, Telemundo, NBC Local Stations, Bravo, USA Network, and Peacock, our premium ad-supported streaming service. We produce and distribute premier filmed entertainment and programming through Universal Filmed Entertainment Group and Universal Studio Group, and have world-renowned theme parks and attractions through Universal Destinations & Experiences. NBCUniversal is a subsidiary of Comcast Corporation.
Here you can be your authentic self. As a company uniquely positioned to educate, entertain and empower through our platforms, Comcast NBCUniversal stands for including everyone. Our Diversity, Equity and Inclusion initiatives, coupled with our Corporate Social Responsibility work, is informed by our employees, audiences, park guests and the communities in which we live. We strive to foster a diverse, equitable and inclusive culture where our employees feel supported, embraced and heard. Together, we’ll continue to create and deliver content that reflects the current and ever-changing face of the world.
Come join us as we work together as one team to innovate and deliver what’s Next.
NBC Sports Next is where sports and technology intersect. We’re a subdivision of NBC Sports and home to all NBCUniversal digital applications in sports and technology within our three groups: Youth & Recreational Sports and Golf.
At NBC Sports Next we’re fueled by our mission to innovate, create larger-than-life events and connect with sports fans through technology that provides the ultimate in immersive experiences.
Golf fuses the team behind products and services like GolfNow, TeeOff and GolfPass, which better connects golfers and golf facilities around the world through innovative solutions like cloud-based golf course management and SmartPlay contactless technology and services that create optimum golfing experiences.
Job Description
GolfNow
has an exciting opportunity for an experienced Data Engineer. Working alongside the Data Engineering Team, you'll manage the full lifecycle of our data warehousing needs. You'll read and write complex queries, demonstrate the ability to create database objects (tables, views, stored procedures, user-defined functions), and create and maintain ETL pipelines. Our data warehouse and data operations are built on top of Microsoft and AWS technologies including MS SQL Server, SSIS, PowerShell, and other AWS cloud technologies. To perform this job successfully, an individual would need to be able to understand complex business processes, gather requirements, work efficiently, and verify their results.
Responsibilities Include But Are Not Limited To
Work within a small team of passionate data engineers and data scientists.
Compile user requirements and specifications for reports.
Contribute to the management of the day-to-day operations of running our Data Warehouse.
Build, analyze, and manage reports and dashboards for business stakeholders.
Respond to users to troubleshoot and/or improve existing reports.
Collaborate with internal QA on customer acceptance testing.
Develop SQL scripts and objects to support reporting functionality and performance.
Build data pipelines and ETLs for loading source system data into the data warehouse for further reporting and analysis.
Assist in building scalable data models to support reporting and tracking of key business and product metrics.
Help identify better practices, tools, and relevant trends that can positively influence the data operations across the business.
Other duties may be assigned as needed by management.
Strong ability to actively engage in a remote workspace - including, but not limited to, virtual team meetings, connection via team channels (Teams, Slack and Outlook) and ad hoc meetings as requested.
Experience working through complex issues to completion
Other duties may be assigned as needed by management.
Qualifications
All candidates must meet the qualifications below:
A minimum of 2+ years of data engineering experience is required
Bachelor’s Degree in Computer Science or related field/relevant industry experience in data engineering
Working experience developing and refactoring SQL Stored Procedures
A minimum of 1 years working experience with Python
Solid experience and knowledge of T-SQL and Microsoft SQL Server Database Platforms
1-2 years experience with AWS cloud environment
Experience with AWS ETL tools such as Glue and Lambda
Experience with Apache Airflow
Experience using source control with Github or Team Foundation Server.
Experience with modeling data structures in both transactional and analytical platforms.
Experience with PowerShell scripting is a plus
Experience with SSIS is a plus
Experience with one of the following BI Tools. (SSRS, Tableau, Power BI)
Desired Qualifications Are As Follows
Experience working in Agile environment
Experience managing SDLC process with Atlassian tools. (Jira, Confluence)
Able and eager to learn new technologies.
Able to easily transition between high-level strategy and day-to-day implementation.
Excellent teamwork and collaboration skills.
Results-oriented and self-motivated.
Fully Remote:
This position has been designated as fully remote, meaning that the position is expected to contribute from a non-NBCUniversal worksite, most commonly an employee’s residence.
Additional Information
NBCUniversal's policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.
If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access nbcunicareers.com as a result of your disability. You can request reasonable accommodations in the US by calling 1-818-777-4107 and in the UK by calling +44 2036185726.
Show more
Show less","Data Engineering, Python, SQL, Microsoft SQL Server, AWS cloud environment, AWS ETL tools, Apache Airflow, GitHub, Team Foundation Server, SSRS, Tableau, Power BI, Agile, Atlassian tools, Jira, Confluence","data engineering, python, sql, microsoft sql server, aws cloud environment, aws etl tools, apache airflow, github, team foundation server, ssrs, tableau, power bi, agile, atlassian tools, jira, confluence","agile, apache airflow, atlassian tools, aws cloud environment, aws etl tools, confluence, data engineering, github, jira, microsoft sql server, powerbi, python, sql, ssrs, tableau, team foundation server"
Data Engineer,KellyMitchell Group,"Ohio, United States",https://www.linkedin.com/jobs/view/data-engineer-at-kellymitchell-group-3768701778,2023-12-17,Kitchener, Canada,Associate,Remote,"Job Summary:
Our client is seeking a Data Engineer to join their team in Ohio. Remote candidates will be considered. This candidate will be accountable for developing and delivering technological responses to targeted business outcomes. This role will also analyze, design, and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise.
Duties:
Utilize enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Ensure there is clarity between ongoing projects, escalating when necessary, including direct collaboration with 84.51
Leverage innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state
Contribute to the development of cost-benefit analysis for leadership to shape sound architectural decisions
Analyze technology environments to detect critical deficiencies and recommend solutions for improvement
Promote the reuse of data assets, including the management of the data catalog for reference
Draft architectural diagrams, interface specifications, and other design documents
Desired Skills/Experience:
Advanced working SQL knowledge and experience working with relational databases and query authoring
Experience building and optimizing 'big data' pipelines, architectures, and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with unstructured data sets
Experience with build processes supporting data transformation, data structures, metadata, dependency, and workload management
Successful history of manipulating, processing, and extracting value from large, disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores
Strong project management and organizational skills
Benefits:
Medical, Dental, & Vision Insurance Plans
401K offered
KMID: 129316
Show more
Show less","SQL, NoSQL, Cloudbased data platforms, Data pipelines, Data architecture, Data analysis, Data mining, Data visualization, Data catalog, Data governance, Message queuing, Stream processing, Big data, Project management, Organizational skills","sql, nosql, cloudbased data platforms, data pipelines, data architecture, data analysis, data mining, data visualization, data catalog, data governance, message queuing, stream processing, big data, project management, organizational skills","big data, cloudbased data platforms, data architecture, data catalog, data governance, data mining, dataanalytics, datapipeline, message queuing, nosql, organizational skills, project management, sql, stream processing, visualization"
Data Platform Engineer,Upstart,United States,https://www.linkedin.com/jobs/view/data-platform-engineer-at-upstart-3764619895,2023-12-17,Kitchener, Canada,Associate,Remote,"About Upstart
Upstart is a leading AI lending marketplace partnering with banks and credit unions to expand access to affordable credit. By leveraging Upstart's AI marketplace, Upstart-powered banks and credit unions can have higher approval rates and lower loss rates across races, ages, and genders, while simultaneously delivering the exceptional digital-first lending experience their customers demand. More than two-thirds of Upstart loans are approved instantly and are fully automated.
Upstart is a digital-first company, which means that most Upstarters can live and work anywhere in the U.S. We also have offices in San Mateo, California; Columbus, Ohio; and Austin, Texas.
Most Upstarters join us because they connect with our mission of enabling access to effortless credit based on true risk. If you are energized by the impact you can make at Upstart, we’d love to hear from you!
The Team
The
Data Engineering team
(Part of Upstart's core platform vertical) provides developer tools, frameworks, and scalable data infrastructure as shared services across Upstart. The team's primary objective is to provide Data Analysts, Software Engineers, and ML scientists access to high-quality data and developer tools to create business metrics for respective product verticals,
You will join the subdivision focusing on
Data Capture
.
As the
Data Platform Engineer
on the
Data Capture team
, you will get the opportunity to contribute to 3 areas:
Expand the existing developer tools that capture business events, operational data and third party data into the Lakehouse.
Expand the self-serve and orchestration framework that developers use to build pipelines.
Improve the observability and SLAs for the above services and raw layer data in the Lakehouse.
We would love to hear from you if you are passionate about building data products!
Position Location -
This role is available in the following locations: Remote, San Mateo, Columbus, Austin
Time Zone Requirements -
This team operates on the East/West Coast time zones.
Travel Requirements -
This team has periodic on-site collaboration sessions twice or thrice a year. Upstart will cover all travel-related expenses if you need to travel to make these meetups.
How you’ll make an impact:
Build and Improve the tools owned by the Capture team
Own the service/raw layer data observability and SLAs
Participate in planning and prioritization by collaborating with stakeholders across the various product verticals and functions (ML, Analytics, Finance) to ensure our architecture aligns with the overall business objectives.
Collaborate with stakeholders such as Software Engineering, Machine Learning, Machine Learning Platform and Analytics teams to ingest well curated data into the Lakehouse and adopt the developer tools
Participate in code reviews and architecture discussions to exchange actionable feedback with peers.
Contribute to engineering best practices and mentor junior team members.
Help break down complex projects and requirements into sprints.
Continuously monitor and improve data platform performance, reliability, and security.
Stay up-to-date with emerging technologies and industry best practices in data engineering.
What we’re looking for:
Minimum requirements:
A bachelor's degree in Computer Science, Data Science, Engineering, or a related field.
3+ years of experience in data engineering or related fields, with a strong focus on data quality, governance, and data infrastructure.
Proficiency in data engineering tech stack; Databricks / PostgreSQL / Python / Spark / Kafka/ Streaming / SQL / AWS / Airflow/ DBT / containers and orchestration (Docker, Kubernetes) and others.
Ability to approach problems with first principles thinking, embrace ambiguity, and enjoy collaborative work on complex solutions.
Preferred qualifications:
Strong foundation in algorithms and data structures and their real-world use cases.
Experience and understanding of distributed systems, data architecture design, and big data technologies (e.g., Spark Streaming / Batch, Kafka Streaming, Lakehouse, Databricks, Redshift).
Experience with AWS technologies ( e.g., AWS Lambda, Redshift, RDS, S3, etc.).
Knowledge of data ingestion, software engineering and data security best practices.
Good knowledge of DevOps engineering using Continuous Integration/Delivery tools like Kubernetes, Jenkins, Terraform, etc., thinking about automation, alerting, monitoring, security, and other declarative infrastructure.
Ability to explain complex concepts in easy-to-understand ways and navigate environments where problems are not well-defined (and evolve quickly).
What you'll love:
Competitive Compensation (base + bonus & equity)
Comprehensive medical, dental, and vision coverage with Health Savings Account contributions from Upstart
401(k) with 100% company match up to $4,500 and immediate vesting and after-tax savings
Employee Stock Purchase Plan (ESPP)
Life and disability insurance
Generous holiday, vacation, sick and safety leave
Supportive parental, family care, and military leave programs
Annual wellness, technology & ergonomic reimbursement programs
Social activities including team events and onsites, all-company updates, employee resource groups (ERGs), and other interest groups such as book clubs, fitness, investing, and volunteering
Catered lunches + snacks & drinks when working in offices
At Upstart, your base pay is one part of your total compensation package. The anticipated base salary for this position is expected to be within the below range. Your actual base pay will depend on your geographic location–with our “digital first” philosophy, Upstart uses compensation regions that vary depending on location. Individual pay is also determined by job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.
In addition, Upstart provides employees with target bonuses, equity compensation, and generous benefits packages (including medical, dental, vision, and 401k).
United States | Remote - Anticipated Base Salary Range
$130,100—$180,000 USD
Upstart is a proud Equal Opportunity Employer. We are dedicated to ensuring that underrepresented classes receive better access to affordable credit, and are just as committed to embracing diversity and inclusion in our hiring practices. We celebrate all cultures, backgrounds, perspectives, and experiences, and know that we can only become better together.
If you require reasonable accommodation in completing an application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please email
candidate_accommodations@upstart.com
https://www.upstart.com/candidate_privacy_policy
Show more
Show less","Databricks, PostgreSQL, Python, Spark, Kafka, Streaming, SQL, AWS, Airflow, DBT, Docker, Kubernetes, Data engineering, Data quality, Data governance, Data infrastructure, Algorithms, Data structures, Distributed systems, Data architecture design, Big data technologies, AWS technologies, Data ingestion, Software engineering, Data security, DevOps engineering, Continuous Integration/Delivery, Kubernetes, Jenkins, Terraform, Automation, Alerting, Monitoring, Security, Declarative infrastructure","databricks, postgresql, python, spark, kafka, streaming, sql, aws, airflow, dbt, docker, kubernetes, data engineering, data quality, data governance, data infrastructure, algorithms, data structures, distributed systems, data architecture design, big data technologies, aws technologies, data ingestion, software engineering, data security, devops engineering, continuous integrationdelivery, kubernetes, jenkins, terraform, automation, alerting, monitoring, security, declarative infrastructure","airflow, alerting, algorithms, automation, aws, aws technologies, big data technologies, continuous integrationdelivery, data architecture design, data engineering, data governance, data infrastructure, data ingestion, data quality, data security, data structures, databricks, dbt, declarative infrastructure, devops engineering, distributed systems, docker, jenkins, kafka, kubernetes, monitoring, postgresql, python, security, software engineering, spark, sql, streaming, terraform"
Data Engineer,Rightworks,"Hudson, NH",https://www.linkedin.com/jobs/view/data-engineer-at-rightworks-3780794360,2023-12-17,Kitchener, Canada,Associate,Remote,"Rightworks offers the only intelligent cloud purpose-built for accounting firms and professionals. Backed by award-winning around-the-clock US-based support, our fully managed IT and applications ensure customers have secure, reliable, on-demand access to their technology. We provide a curated software ecosystem that simplifies the complexity of running an accounting firm or small business, supported by a community of thought leaders, peer networks and educational resources. Our success is made possible by leveraging decades of specialized experience in leading accounting firms and technology companies. Thousands of Firms and SMBs count on us to run their business every day.
We have a great team, we’re growing fast, and have a winning culture based on innovation, teamwork, and mutual respect.
Job Overview
Rightworks is looking for a Data Engineer to help our Technology organization design, implement, and operate a data lakehouse across our product systems to facilitate more advanced analytical capabilities into our tech stack, accelerating the delivery of data intelligence to our customers. This person will be responsible for assisting in the design, build, population and maintaining of an analytical data platform.
Responsibilities
Work closely with Data Engineering, Data Analysis and Data Architecture teams to design, build, populate and maintain a data platform to provide insight into Product Data
Develop data integrations, data storage, warehousing and analytics as it pertains to building a data platform
Understand and document business requirements for reporting and analysis and identify the source data and define transformation logic requirements and test criteria
Design, develop, deploy, and monitor data pipelines using Azure based technologies
Design, develop and test ETL logic
Define test criteria and writing automated test plans
Monitor and maintain data pipelines proactively to ensure high service availability
Work with engineering teams and architects to define features to improve data quality and governance to foster analysis
Requirements
2+ years of relevant experience in Data Engineering or Software Engineering in a data role
Participated in building, scaling and managing large scale data systems
Knowledge of cloud data ecosystems for compute and storage
4-year degree in Computer Science, Engineering, Management Information Systems, Mathematics/Statistics, or similar field
Basic understanding of data systems, analytics architectures, and cloud solutions in the data space
Participated in building, scaling and managing large scale data systems
Proficiency in software engineering with strong coding and design skills, including .Net, SQL, Databricks and Python preferred
Experience with Azure cloud technologies
Experience with the Atlassian product suite including Jira, Confluence
Experience delivering with modern technology solutions, for example data warehouses, data lakehouse and business intelligence platforms
Familiarity with big data technology concepts including streaming, data security, storage, compliance and responsible data usage
Basic knowledge of data governance, data quality and systems architecture
Proficiency in SDLC, IAC and DevOps best practices
Basic understanding of data architecture and database concepts including physical and logical modeling
Basic understanding of REST, GraphQL and other related API concepts
Thrives in an agile, collaborative, and distributed environment
Exceptional communication skills, capable of conveying intricate technical concepts to diverse audiences
Participate in on-call rotation for off-hours support
This role will require some light travel in support of team objectives
Benefits
To provide best-in-class solutions, we need a best-in-class team. We offer competitive salaries to recruit the best talent. We provide company-paid short and long-term disability insurance, life insurance, and a generous 401K match. We offer highly affordable medical, dental, vision coverage, and many other valuable benefits. We offer a generous PTO bank, and numerous paid holidays, affording you the time to be there for what is important in your life. We encourage giving back to our communities by providing volunteer paid time off. We are proud to be an Equal Opportunity Employer!
This job description may not be inclusive of all assigned duties, responsibilities, or aspects of the job described, and may be amended at any time at the sole discretion of the employer.
Show more
Show less","Data Lakehouse, Data Engineering, Data Integration, Data Storage, Data Warehousing, Data Analytics, Business Requirements, Reporting, Analysis, Source Data, Transformation Logic, Test Criteria, Data Pipelines, Azure Technologies, ETL Logic, Test Plans, Data Governance, Data Quality, SDLC, IAC, DevOps, Data Architecture, Database Concepts, Physical Modeling, Logical Modeling, REST, GraphQL, API, Communication","data lakehouse, data engineering, data integration, data storage, data warehousing, data analytics, business requirements, reporting, analysis, source data, transformation logic, test criteria, data pipelines, azure technologies, etl logic, test plans, data governance, data quality, sdlc, iac, devops, data architecture, database concepts, physical modeling, logical modeling, rest, graphql, api, communication","analysis, api, azure technologies, business requirements, communication, data architecture, data engineering, data governance, data integration, data lakehouse, data quality, data storage, dataanalytics, database concepts, datapipeline, datawarehouse, devops, etl logic, graphql, iac, logical modeling, physical modeling, reporting, rest, sdlc, source data, test criteria, test plans, transformation logic"
Data Visualization Engineer,ProFocus Technology,United States,https://www.linkedin.com/jobs/view/data-visualization-engineer-at-profocus-technology-3778865438,2023-12-17,Kitchener, Canada,Associate,Remote,"TITLE:
Data Visualization Engineer
LOCATION:
Remote
PAY:
Target pay for this role is between $60-65 but may vary based on experience
ENGAGEMENT TYPE:
Contract 6-Months
WHAT YOU’LL BE DOING
Data Visualization Engineer will work on data analytics, and data visualization delivering valuable data-driven insights, data trends, and patterns within large, complex data sets. Create dashboards and visualization that convey data insights in a meaningful way. Translate the story behind the data in an accessible way to non-technical audiences
Overview of responsibilities
Integrate, analyze complex data and develop innovative insights from that data
Work with business users and analysts to help identify data trends, and patterns within large, complex data sets
Translate the story behind the data in an accessible way to non-technical audiences
Create visually appealing and easily understandable dashboards and visualizations that convey data insights in a concise and meaningful way
Prototype and develop advanced data-driven visualization components and interfaces that can be easily consumed by internal users as well as external customers and injured workers
Research modern visualization and data access tools, evaluate fit for needs of the organization, facilitate, promote adoption among self service business users
WHO WE’RE LOOKING FOR
5 or more years of experience in data analytics and data visualization.
Strong analytical and problem-solving skills, with a track record of delivering valuable data-driven insights.
Expertise in data visualization tools, such as Tableau, Power BI, or similar tools.
Understanding of data modeling and database design principles.
Proficient in SQL and programming languages such as Python or similar.Strong communication skills.
Authorization to work in the US is a precondition of employment - we cannot sponsor work visas
Please NO Third Party Applicants
ABOUT OUR CLIENT
Our client is a mission-driven enterprise organization providing critical products and services to Oregon workers.
WHY ProFocus:
Candidates come first.
ProFocus is a six-time winner of Best in Staffing for Talent Satisfaction.
Quality process.
We invest the time to understand your background and career goals. We only introduce you to opportunities that are the right fit.
Access to hiring managers.
We have close relationships with some of the most respected local companies, from small businesses to Fortune 500 companies. These relationships give is access to roles that may not be available anywhere else.
Excellent benefits.
We offer medical, dental, vision, 401k match, education reimbursement, sick leave, and employer-paid disability and life insurance. Review our detailed benefits here.
Want to learn more?
Contact one of our recruiters here or email info@ProFocusTechnology.com.
Want to apply?
Email your resume to Resume@ProFocusTechnology.com.
Visit our Job Seekers page to learn more and review other opportunities.
ProFocus is an equal opportunity employer. We value diversity in our workplace and encourage all qualified applicants regardless of race, color, age, sex, religion, national origin, physical or mental disability, pregnancy, marital status, veteran or military status, genetic information, sexual orientation, or any other characteristic protected by federal, state, or local laws.
Show more
Show less","Data Visualization, Data Analytics, DataDriven Insights, Data Trends, Data Patterns, Dashboards, Visualization, Tableau, Power BI, SQL, Python, Data Modeling, Database Design","data visualization, data analytics, datadriven insights, data trends, data patterns, dashboards, visualization, tableau, power bi, sql, python, data modeling, database design","dashboard, data patterns, data trends, dataanalytics, database design, datadriven insights, datamodeling, powerbi, python, sql, tableau, visualization"
Data Engineer,"ISC (Integrated Specialty Coverages, LLC)",United States,https://www.linkedin.com/jobs/view/data-engineer-at-isc-integrated-specialty-coverages-llc-3768723452,2023-12-17,Kitchener, Canada,Associate,Remote,"About Integrated Specialty Coverages
Integrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.
Backed by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and “Main Street USA”, we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we’re building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.
Our ISC Engineering Team consists of a diverse group of people who have passion for the products we create and for the environment we’ve built to ensure their success. One thing is for sure, you’ll be surrounded with engineers, analysts, product owners and other peers who like to dive deep, enjoy a challenge and have fun while doing it! We’ve also created a fun, casual work environment that fosters openness, a sense of accomplishment and creativity. It’s a place where you can mentor others to grow, and also be mentored by some of the top technologists and visionaries in our industry.
Job Summary
The
Data Engineer
role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.
Job Responsibilities
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python.
Work with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
Work with data science and analytics teams to strive for greater functionality in our data systems.
Minimum Qualifications:
Advanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)
Experience building and optimizing data pipelines, architecture and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
A successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)
Strong analytic skills related to working with unstructured datasets.
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Ability to mentor/guide/collaborate with other team members.
We are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:
Experience with relational and MPP databases, including Snowflake and MySQL.
Experience developing software in an agile environment from the requirements stage to production.
Experience with version control: git
Experience with container technologies: Docker
Experience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc.
Experience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift
Experience with other cloud services: Snowflake, Airflow
Experience with object-oriented/object function scripting languages: Python, Java, C++, etc.
Experience with data modeling and data warehouse design
Experience with data visualization tools (PowerBI, QuickSight)
Additional Information
The starting pay scale for this position is $100,000 - $115,000. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.
ISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.
Benefits of Working at ISC
Competitive vacation and flexible working arrangements
Comprehensive and inclusive health benefits
A variety of professional development and mentorship opportunities
Choice of technology whether at home or in the office
ISC’s Ownership Behaviors
*Do the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes
*Try Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination
Applicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.
ISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a “can-do” attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).
Diversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law.
Show more
Show less","Python, Java, C++, AWS, Snowflake, MySQL, git, Docker, Airflow, Jenkins, AWS Glue, Azkaban, Luigi, EC2, ECS, Batch, S3, EMR, RDS, Redshift, Data pipeline, Data visualization, Data architecture, Data modeling, Data warehouse design, Data engineering, Data analysis, Data science, Machine learning, AI","python, java, c, aws, snowflake, mysql, git, docker, airflow, jenkins, aws glue, azkaban, luigi, ec2, ecs, batch, s3, emr, rds, redshift, data pipeline, data visualization, data architecture, data modeling, data warehouse design, data engineering, data analysis, data science, machine learning, ai","ai, airflow, aws, aws glue, azkaban, batch, c, data architecture, data engineering, data pipeline, data science, data warehouse design, dataanalytics, datamodeling, docker, ec2, ecs, emr, git, java, jenkins, luigi, machine learning, mysql, python, rds, redshift, s3, snowflake, visualization"
Data Engineer,Talent Groups,United States,https://www.linkedin.com/jobs/view/data-engineer-at-talent-groups-3785576842,2023-12-17,Kitchener, Canada,Associate,Remote,"Location:
Remote
Duration:
2 months to start – temp to perm
Our client is only able to work with W2 candidates at this time (US Citizen or Green Card Perm Residents)
Job Description
As a Data Engineer, you will be responsible for driving the analytics product life cycle, expanding our analytics products while optimizing our data architecture, and developing best practices and governance for administrative reporting, data visualization, and data flow for cross-functional teams. We are looking for a candidate who is experienced in all aspects of data from development to implementation.
You will support various stakeholders and ensure consistent optimal product delivery throughout ongoing projects. You will also support non-technical colleagues in collecting and appropriately using administrative data. The ideal candidate must be self-directed, comfortable supporting the data needs of multiple teams, systems, and products, and excited about re-designing our client’s data architecture to support our next generation of products and data initiatives.
Job Responsibilities:
Conduct data modeling by evaluating structured and unstructured data and determining the most appropriate schema for new fact tables, data marts, etc.
Collaborate with colleagues across the enterprise to scope requests, extract data from various data sources, validate results, create relevant data visualizations, and share with the requester in Tableau. Develop dashboards and automate refreshes as appropriate in Tableau Server.
Adhere to and contribute to data governance standards and educate and support colleagues in best practices to ensure that data is used appropriately.
Collaborate and act as the voice of the customer to offer concrete feedback and project requests as well as an advocate for analytics from within the business units themselves.
Assemble large, complex data sets that meet functional/non-functional business requirements.
Identify, design, and implement internal process improvements, such as automating manual processes, optimizing data delivery, and re-designing infrastructure for greater scalability.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources (including ground, hybrid cloud, and cloud) using SQL and various programming technologies.
Develop analytics tools that utilize data resources to provide actionable insights, operational efficiency, and other key business performance metrics.
Work with stakeholders including the Executive, Administrative, and Analyst teams to assist with data-related technical issues and support their data infrastructure needs.
Develop optimized tools for analytics and data scientist team members that assist them in building and optimizing projects into an innovative industry leaders.
Experience in data analysis, design, and development using Tableau
Strong understanding of data modeling, data warehousing, and data integration
Proficient in SQL for data retrieval, manipulation, and analysis
Experience integrating predictive and prescriptive models into applications and processes
Develop processes supporting data transformation, data structures, metadata, dependency, and workload management
Perform root cause analysis on internal and external data and processes to identify opportunities for improvement
Skills:
Experience with relational SQL and NoSQL databases such as IBM PDA (Netezza), MS SQL Server, and HBase
Experience with data integration tools such as Informatica, MS Integration Services, and Sqoop
Experience with API consumption and building
Knowledge of object-oriented programming languages such as Python, Java, C++, and Scala
Familiarity with statistical data analysis tools like R, SAS, and SPSS
Proficiency in visual analytics tools including QlikView, Tableau, and Power BI
Familiarity with Agile methodology for development.
Education:
Proficiency in appropriate Business Intelligence/Data Warehousing technology or subject domain.
Bachelor’s degree in computer/Analytics/Data Science related field.
3-6 years of Business Intelligence/Data Warehousing experience, preferably in a healthcare & research environment.
Preferred Qualifications:
Tableau certification and a strong portfolio on Tableau Public
Experience with other data visualization tools like Power BI, QlikView, or Domo
Knowledge of ETL processes and tools such as Informatica or Talend
At least 5 years of experience in Data Engineering, Business Intelligence, or Data Warehousing
Show more
Show less","Data modeling, Tableau, Data visualization, SQL, Data integration tools, Python, Java, C++, Scala, Statistical data analysis tools, Agile methodology, Business Intelligence, Data Warehousing, Tableau certification, Power BI, QlikView, Domo, Informatica, Talend","data modeling, tableau, data visualization, sql, data integration tools, python, java, c, scala, statistical data analysis tools, agile methodology, business intelligence, data warehousing, tableau certification, power bi, qlikview, domo, informatica, talend","agile methodology, business intelligence, c, data integration tools, datamodeling, datawarehouse, domo, informatica, java, powerbi, python, qlikview, scala, sql, statistical data analysis tools, tableau, tableau certification, talend, visualization"
Data Engineer,SurveyMonkey,United States,https://www.linkedin.com/jobs/view/data-engineer-at-surveymonkey-3782561910,2023-12-17,Kitchener, Canada,Associate,Remote,"SurveyMonkey
is a global leader in online surveys and forms that empowers people with the insights they need to make decisions with speed and confidence.
Our fast, intuitive feedback management platform connects millions of users worldwide with real-time AI-powered insights that drive meaningful decisions. We provide answers to more than 20 million questions every day so that people and organizations can attract new audiences, delight customers, create advocates, and extend their competitive advantage in the marketplace. Our vision is to raise the bar for human experiences by amplifying individual voices. Learn more at surveymonkey.com.
What We’re Looking For
We are looking for an experienced data Engineer to join a team of highly skilled data engineers and data architects to build and manage the end-to-end data pipelines (batch and near real-time) using modern cloud technologies. This is a role in the central data organization at SurveyMonkey that provides actionable insights into all key business functions of the organization
As a Data Engineer, you will work on data engineering initiatives and build end-to-end analytical solutions that are highly available, scalable, stable, secure, and cost-effective.
What You’ll Be Working On
Design, architect and build data pipelines to support existing data models
Data quality: Build quality checks in the end-to-end data pipelines
Build new Data models (Fact vs Dimension). Write performant/idempotent transformations in Snowflake
Build data pipeline using Python scripting (in a modular/loop context) Write well-tested, production-ready code in Python and Snowflake SQL
Hands-on experience implementing ETL (or ELT) best practices
Translate business requirements, to technical specifications, form project scope, and deliver deployable code.
Write complex data engineering Snowflake - SQL jobs that perform sophisticated queries on the entirety of our datasets
Collaborate closely with stakeholders, upstream and downstream partners
Document our systems for internal and external stakeholders
Monitor and debug data pipelines running on Airflow
Participate in code reviews
We’d love to hear from people with
4+ years experience in data engineering and Data warehousing technologies
2+ years experience in Snowflake/ETL or similar technologies like Redshift
Experience with AWS cloud services: S3, EC2, RDS, Spark, EMR etc
Experience with object-oriented/object function scripting languages: Python (preferred), Java, Scala, etc.
Experience in orchestrating, automating, and deploying production data pipelines using Airflow/Luigi, etc
Experience with DevOPS: Git, Github actions, CI/CD pipelines, Terraform, etc
Experience building Data infrastructure or Data Platform Framework Preferred
Experience with tools such as DBT or other similar technologies
Experience with transforming, and developing data structures, metadata, dependency, and data workflows to support an Analytics function
In-depth knowledge of Data lakes, EDW concepts, and data modeling (Star, Snowflake, and Galaxy schemas)
The base pay provided for this position ranges from $106,880 / year - $160,320 / year depending on the geographic market and assuming a full-time schedule. Actual base pay is based on a number of factors including market location, job-related knowledge, education or training, skills, and experience.
Bonuses and commissions may also be offered as part of the total compensation package, in addition to a competitive benefits package including medical, dental, vision, life, and disability insurance; 401(k) retirement plan; flexible spending & health savings account; paid holidays; paid time off; employee assistance program; and other company benefits.
Why SurveyMonkey? We’re glad you asked
SurveyMonkey is a place where the curious come to grow. We’re building an inclusive workplace where people of every background can excel no matter their time zone. At SurveyMonkey, we weave employee feedback into everything we do to create forward-looking benefits policies, employee programs, and an award-winning culture, including best workplace for parents, our annual holiday refresh, our annual week of service, and our C.H.O.I.C.E Fund. In addition, we’ve reimagined the way we work to allow employees to choose what works best for them -- working in-person, fully remote, or a hybrid model that combines the two through our Choice Model.
Our commitment to an inclusive workplace
SurveyMonkey is an equal opportunity employer committed to providing a workplace free from harassment and discrimination. We celebrate the unique differences of our employees because that is what drives curiosity, innovation, and the success of our business. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, gender identity or expression, age, marital status, veteran status, disability status, pregnancy, parental status, genetic information, political affiliation, or any other status protected by the laws or regulations in the locations where we operate. Accommodations are available for applicants with disabilities.
Show more
Show less","Data Engineering, Data Architecture, Cloud Technologies, Data Pipelines, Data Quality, Data Modeling, Snowflake, Python, ETL, Airflow, DevOps, Git, Github, CI/CD, Terraform, DBT, Data Lakes, EDW, Data Modeling, Star Schema, Snowflake Schema, Galaxy Schema","data engineering, data architecture, cloud technologies, data pipelines, data quality, data modeling, snowflake, python, etl, airflow, devops, git, github, cicd, terraform, dbt, data lakes, edw, data modeling, star schema, snowflake schema, galaxy schema","airflow, cicd, cloud technologies, data architecture, data engineering, data lakes, data quality, datamodeling, datapipeline, dbt, devops, edw, etl, galaxy schema, git, github, python, snowflake, snowflake schema, star schema, terraform"
Data Center Engineer,Cloudflare,United States,https://www.linkedin.com/jobs/view/data-center-engineer-at-cloudflare-3732382791,2023-12-17,Kitchener, Canada,Associate,Remote,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
Data Center Operations Engineer
About the department
In this role, you will be focused on maintaining the Clou dflare global network. You 'll work closely with Cloudflare’s SRE (Site Reliability Engineering) team, Network Engineering team, Network Deployment Engineering team and with various vendors and partners (including hardware vendors, datacenter and network providers, and ISPs) to maintain and improve our global infrastructure. You will further be responsible for the development and implementation of consistent processes and visibility measurements for consistent and effective management of our infrastructure. This is a highly visible position that requires deep technical understanding of datacenter infrastructure, networking (physical), and basic experience with data analysis and project management.
To be successful in this position, you should have excellent technical skills, communication skills, and be able to navigate a range of challenges and constraints (e.g. schedule adherence, time zones, and cultures). You will have the opportunity to (literally) build a faster, safer Internet for our millions of users and the billions of web surfers that visit their sites each month.
Who You Are
You will thrive in a hypergrowth engineering environment and be self driven with a keen attention to detail. You will come with a deep technical understanding of Data Center colocation environments, network architecture and server technologies. You will be used to working through partners to support infrastructure delivery to a number of remote locations. You will have had experience managing operational environments, and used to developing new approaches to improve delivery efficiency or operational stability.
What You'll Do
Collaborating with internal teams (Infrastructure, Network Engineering and SRE). Create documentation and manage remote contractors to complete datacenter tasks, working with hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 300+ datacenter locations
Maintain Data Center environment operational availability
Creating and maintaining documentation, plans, SOP’s, MOP’s etc.
Support and configure network infrastructure where required
Providing feedback to internal teams to support internal tools and external vendor partnerships
Required Experience
Minimum of 5 yrs of Linux systems administration
Experience with Juniper, Cisco and DWDM network equipment
Experience managing and instructing remote contractors
Familiarity with work required to stand up infrastructure in remote colocation facilities
Experience running and improving operational processes, including automation tooling, in a rapidly changing environment
Familiarity with day-to-day tasks and projects common to Data Center Operations (deployment, migration, decommissioning etc.)
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Incident management
Other Responsibilities May Include
Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure
Assist with the definition, documentation and implementation of consistent processes across all region
Limited travel
Examples Of Desirable Skills, Knowledge And Experience
Bachelor’s degree; technical background in engineering, computer science, or MIS
Direct experience executing on complex data center/infrastructure projects
Previous experience installing / maintaining data center (and other IT) infrastructure and DCIM tools
Experience running and improving operational processes in a rapidly changing environment
Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills
Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously
Ability to manage MS excel and Google spreadsheets
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Must be a team player
Bonus Points
Multi-lingual; experience working with infrastructure in multiple countries
Comfortable with remote “lights-out” and out-of-band access to data center resources
Linux certifications (RHCSA etc.)
Network certifications (CCNA, JNCIA or higher)
Compensation
Compensation may be adjusted depending on work location.
For Colorado-based hires: Estimated annual salary of $ 111,000 - $ 135,000 .
For New York City, Washington, and California (excluding Bay Area) based hires: Estimated annual salary of $ 135,000 - $ 165,000
For Bay Area-based hires: Estimated annual salary of $ 142,000 - $ 174,000 .
Equity
This role is eligible to participate in Cloudflare’s equity plan.
Benefits
Cloudflare offers a complete package of benefits and programs to support you and your family. Our benefits programs can help you pay health care expenses, support caregiving, build capital for the future and make life a little easier and fun! The below is a description of our benefits for employees in the United States, and benefits may vary for employees based outside the U.S.
Health & Welfare Benefits
Medical/Rx Insurance
Dental Insurance
Vision Insurance
Flexible Spending Accounts
Commuter Spending Accounts
Fertility & Family Forming Benefits
On-demand mental health support and Employee Assistance Program
Global Travel Medical Insurance
Financial Benefits
Short and Long Term Disability Insurance
Life & Accident Insurance
401(k) Retirement Savings Plan
Employee Stock Participation Plan
Time Off
Flexible paid time off covering vacation and sick leave
Leave programs, including parental, pregnancy health, medical, and bereavement leave
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Data Center Operations, Linux Systems Administration, Juniper Networks, Cisco Systems, DWDM, Project Management, Automation Tooling, Incident Management, JIRA, Microsoft Excel, Google Spreadsheets, Troubleshooting, Communication Skills, Remote Work, Multilingual, Linux Certifications, Network Certifications","data center operations, linux systems administration, juniper networks, cisco systems, dwdm, project management, automation tooling, incident management, jira, microsoft excel, google spreadsheets, troubleshooting, communication skills, remote work, multilingual, linux certifications, network certifications","automation tooling, cisco systems, communication skills, data center operations, dwdm, google spreadsheets, incident management, jira, juniper networks, linux certifications, linux systems administration, microsoft excel, multilingual, network certifications, project management, remote work, troubleshooting"
Senior Data Engineer,Medasource,"Indiana, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-medasource-3778816598,2023-12-17,Kitchener, Canada,Associate,Remote,"Job Title:
Senior Data Engineer
Location:
Remote
Start Date:
ASAP
Duration:
6-months with strong possibility of extension
Minimum Qualifications:
·
SQL:
wide variety of development from simple ad hoc queries to complex stored procedures and multidimensional analysis
·
Data integration:
ETL and ELT development paradigms and platforms – Airflow, Azure Data Factory, Apache Spark, Databricks
·
Software development methodologies/concepts
: SDLC, Agile, Scrum, Git, Continuous Integration
·
MPP Data Warehouse Platforms:
Azure Synapse (SQL DW), Amazon Redshift, Snowflake, Teradata, Azure Data Lake
·
Programming/scripting languages:
Scala, Python, and PowerShell
·
Cloud Platforms:
Azure Data Lake, Azure SQL DW, Azure Databricks, Apache Spark, AWS (Redshift)
·
Data profiling/quality:
Informatica, Microsoft
·
Performance Tuning:
Expert in performance tuning of ETL/SQL code
· Required experience in documentation of code or data models and data mapping, definition of process/guidelines/standards, create data dictionaries, etc.
· Experience with history tables, fact vs. dimension tables (slowly changing dimensions, SCD type 2), Git and Source Control, Azure cloud, Data Lake, Data Factory, Synapse
· Working knowledge of SOLID development principles and common data models such as HL7, OMOP, FHIR, OMOP on FHIR, etc.
· Expertise in SQL
Responsibilities:
The senior data engineer is responsible for the successful delivery of business intelligence information to the entire organization and is experienced in Data Warehouse and BI development implementations. He/she will be part of the team to review, analyze, modify, and create ETLs, testing, debugging, integration and implementation processes. Tasks will also include documenting technical needs for ETL processes and databases, and ensuring optimal technical infrastructure is utilized. This position involves the delivery of big data integration projects and requires leveraging best practices expertise and experience at implementation; as well as effective technology know-how to support the ongoing activity of those working within business intelligence and analytics teams.
ESSENTIAL FUNCTIONS:
· Design, construct, install, test and maintain highly scalable data management systems
· Develop and maintain logical & physical data model designs, data management standards and conventions, data naming standards and metadata (normalized, de-normalized and STAR-based structures)
· Perform reverse engineering of physical data models from databases and SQL scripts
· Evaluate data models and physical databases for variances and discrepancies
· Knowledge of the Healthcare Ecosystem and Use Cases - e.g., Care Workflows, Careplans, Prior Auth, Population Health, Segmentation/Stratification, Risk Computation, etc.
· Understanding and knowledge of Terminology Standards such as ICD10, CPT, Snomed, LOINC, RXNorm, ERGs, RERGs, DRGs
· Experience in a healthcare setting with deep exposure to Payer and Provider oriented terminologies (Nice to Have).
· Validate business data objects for accuracy and completeness
Show more
Show less","SQL, Data integration, ETL, ELT, Airflow, Azure Data Factory, Apache Spark, Databricks, SDLC, Agile, Scrum, Git, Continuous Integration, MPP Data Warehouse Platforms, Azure Synapse, SQL DW, Amazon Redshift, Snowflake, Teradata, Azure Data Lake, Scala, Python, PowerShell, Azure Data Lake, Azure SQL DW, Azure Databricks, AWS, Redshift, Data profiling, Informatica, Microsoft, Performance Tuning, SOLID development principles, HL7, OMOP, FHIR, OMOP on FHIR","sql, data integration, etl, elt, airflow, azure data factory, apache spark, databricks, sdlc, agile, scrum, git, continuous integration, mpp data warehouse platforms, azure synapse, sql dw, amazon redshift, snowflake, teradata, azure data lake, scala, python, powershell, azure data lake, azure sql dw, azure databricks, aws, redshift, data profiling, informatica, microsoft, performance tuning, solid development principles, hl7, omop, fhir, omop on fhir","agile, airflow, amazon redshift, apache spark, aws, azure data factory, azure data lake, azure databricks, azure sql dw, azure synapse, continuous integration, data integration, data profiling, databricks, elt, etl, fhir, git, hl7, informatica, microsoft, mpp data warehouse platforms, omop, omop on fhir, performance tuning, powershell, python, redshift, scala, scrum, sdlc, snowflake, solid development principles, sql, sql dw, teradata"
Senior Data Engineer,NBCUniversal,"Englewood Cliffs, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-nbcuniversal-3770036819,2023-12-17,Kitchener, Canada,Associate,Remote,"Company Description
We create world-class content, which we distribute across our portfolio of film, television, and streaming, and bring to life through our theme parks and consumer experiences. We own and operate leading entertainment and news brands, including NBC, NBC News, MSNBC, CNBC, NBC Sports, Telemundo, NBC Local Stations, Bravo, USA Network, and Peacock, our premium ad-supported streaming service. We produce and distribute premier filmed entertainment and programming through Universal Filmed Entertainment Group and Universal Studio Group, and have world-renowned theme parks and attractions through Universal Destinations & Experiences. NBCUniversal is a subsidiary of Comcast Corporation.
Here you can be your authentic self. As a company uniquely positioned to educate, entertain and empower through our platforms, Comcast NBCUniversal stands for including everyone. Our Diversity, Equity and Inclusion initiatives, coupled with our Corporate Social Responsibility work, is informed by our employees, audiences, park guests and the communities in which we live. We strive to foster a diverse, equitable and inclusive culture where our employees feel supported, embraced and heard. Together, we’ll continue to create and deliver content that reflects the current and ever-changing face of the world.
Job Description
As part of the global Operations & Technology organization, the D&A team is focused on data and analytics strategies for the future. We support NBCU’s vast portfolio of brands - from broadcast, cable, news, and sports networks to film studios, world-renowned theme parks, and a diverse suite of digital properties. We take pride in supplying our business groups with data to advise and shape strategic business decisions related to our content.
We're looking for a passionate problem solver who’s looking to build the next generation of data pipelines and applications. Working across one or more of our main subject areas – research, marketing, engineering frameworks – the Senior Data Engineer role is right for you if you’re a “hands-on” coder who can build and cleanse large datasets in order to report out actionable insights.
In addition, you’ll be working with internal stakeholders, data engineers, visualization experts, data scientists, and other technologists across the business. If you’re someone who loves to take large, disparate data sets and build them into flexible and scalable analytics applications and databases, you’ve come to the right place. Here you can create the extraordinary. Join us!
Collaborate with business leaders, engineers, and product managers to understand data needs.
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using cloud-native data engineering principles
Design, build, and scale data pipelines across a variety of source systems and streams (internal, third-party, as well as cloud-based), distributed/elastic environments, and downstream applications and/or self-service solutions
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience
Participate and lead in development sprints, demos, and retrospectives, as well as release and deployment
Build and manage relationships with supporting IT teams in order to effectively deliver work products to production
Qualifications
5+ years of experience in a data engineering or related role
Direct experience designing and building data modeling, ETL/ELT development principles, or data warehousing concepts
Strong knowledge of data management fundamentals and data storage principles
Deep experience in building data pipelines using Python/SQL
Deep experience in Airflow or similar orchestration engines
Deep experience in applying CI/CD principles and processes to data engineering solutions.
Strong understanding of cloud data engineering design patterns and use cases
Bachelor's degree in Computer Science, Data Science, Statistics, Informatics, Information Systems, Mathematics, Computer Engineering, or quantitative field.
This position is eligible for company sponsored benefits, including medical, dental and vision insurance, 401(k), paid leave, tuition reimbursement, and a variety of other discounts and perks. Learn more about the benefits offered by NBCUniversal by visiting the Benefits page of the Careers website. Salary range: $115,000 - $145,000.
Additional Information
NBCUniversal's policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.
If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access nbcunicareers.com as a result of your disability. You can request reasonable accommodations by emailing AccessibilitySupport@nbcuni.com.
Show more
Show less","data engineering, data modeling, ETL/ELT development principles, data warehousing concepts, data management fundamentals, data storage principles, Python, SQL, Airflow, CI/CD principles, cloud data engineering design patterns, Computer Science, Data Science, Statistics, Informatics, Information Systems, Mathematics, Computer Engineering","data engineering, data modeling, etlelt development principles, data warehousing concepts, data management fundamentals, data storage principles, python, sql, airflow, cicd principles, cloud data engineering design patterns, computer science, data science, statistics, informatics, information systems, mathematics, computer engineering","airflow, cicd principles, cloud data engineering design patterns, computer engineering, computer science, data engineering, data management fundamentals, data science, data storage principles, data warehousing concepts, datamodeling, etlelt development principles, informatics, information systems, mathematics, python, sql, statistics"
Data Engineer III - NBC Sports Next,NBC Sports Next,"Orlando, FL",https://www.linkedin.com/jobs/view/data-engineer-iii-nbc-sports-next-at-nbc-sports-next-3770036816,2023-12-17,Kitchener, Canada,Associate,Remote,"Company Description
NBC Sports Next is where sports and technology intersect. We’re a subdivision of NBC Sports and home to all NBCUniversal digital applications in sports and technology within our three groups: Youth & Recreational Sports; Golf; and Betting, Gaming & Emerging Media.
At NBC Sports Next, we make playing sports better through innovative technology and immersive experiences for athletes, coaches, players and fans. We equip more than 30MM players, coaches, athletes, sports administrators and fans in 40 countries with more than 25 sports solution products, including SportsEngine, the largest youth sports club, league and team management platform; GolfNow, the leading online tee time marketplace and provider of golf course operations technology; GolfPass the ultimate golf membership that connects golfers to exclusive content, tee time credits, and coaching, tips; TeamUnify, swim team management services; GoMotion, sports and fitness business software solutions; and NBC Sports Edge, a leading platform for fantasy sports information and betting-focused tools.
At NBC Sports Next we’re fueled by our mission to innovate, create larger-than-life events and connect with sports fans through technology that provides the ultimate in immersive experiences.
Golf fuses the team behind products and services like GolfNow, TeeOff and GolfPass, which better connects golfers and golf facilities around the world through innovative solutions like cloud-based golf course management and SmartPlay contactless technology and services that create optimum golfing experiences.
Come join us as we work together as one team to innovate and deliver what’s Next.
Job Description
GolfNow has an exciting opportunity for an experienced Data Engineer III. In this role as part of the Data Engineering Team, you work to manage the full lifecycle of our data warehousing needs. You will read and write complex queries, demonstrate the ability to create database objects (tables, views, stored procedures, user-defined functions) and create and maintain ETL pipelines. Our data warehouse and data operations are built on top of Microsoft and AWS technologies including MS SQL Server, SSIS, PowerShell, and other AWS cloud technologies. To perform this job successfully, an individual would need to be able to understand complex business processes, gather requirements, work efficiently, and verify their results.
Responsibilities Include But Are Not Limited To
Work within a small team of passionate data engineers and data scientists.
Compile user requirements and specifications for reports.
Contribute to the management of the day-to-day operations of running our Data Warehouse.
Build, analyze and manage reports and dashboards for business stakeholders.
Respond to users to troubleshoot and/or improve existing reports.
Collaborate with internal QA on customer acceptance testing.
Develop SQL scripts and objects to support reporting functionality and performance.
Build data pipelines and ETLs for loading source system data into the data warehouse for further reporting and analysis.
Assist in building scalable data models to support reporting and tracking of key business and product metrics.
Help identify better practices, tools, and relevant trends that can positively influence the data operations across the business.
Other duties may be assigned as needed by management.
Qualifications
All candidates must meet the qualifications below:
A minimum of 5 years of data engineering experience is required.
Bachelor’s Degree in Computer Science or related field/relevant industry experience in data engineering.
Strong understanding of AWS architecture and ETL processing
Advanced knowledge of TSQL tuning
5+ years of experience working with Python
Advanced experience and knowledge of T-SQL Microsoft SQL Server Database Platforms.
Experience working with, assessing and improving cloud frameworks
Experience understanding and enhancing cloud data engineering and analytics data strategies
Experience with AWS cloud environment.
Experience with AWS ETL tools including Glue, Lambda and step functions
Experience with Apache Airflow
Experience working with Terraform
Working experience developing and refactoring SQL Stored Procedures.
Experience using source control with Git or Team Foundation Server.
Experience with modeling data structures in both transactional and analytical platforms.
Experience with one of the following BI Tools: Tableau, Power BI
Desired Qualifications Are As Follows
Experience with SSIS is a plus
Experience with PowerShell scripting is a plus
Strong experience mentoring junior data engineers
Experience working in Agile environment
Experience working with SSRS reports
Experience managing SDLC process with Atlassian tools. (Jira, Confluence)
Able and eager to learn new technologies.
Able to easily transition between high-level strategy and day-to-day implementation.
Excellent teamwork and collaboration skills.
Results-oriented and self-motivated.
Fully Remote:
This position has been designated as fully remote, meaning that the position is expected to contribute from a non-NBCUniversal worksite, most commonly an employee’s residence.
Additional Information
NBCUniversal's policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.
If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access nbcunicareers.com as a result of your disability. You can request reasonable accommodations in the US by calling 1-818-777-4107 and in the UK by calling +44 2036185726.
Show more
Show less","SQL, TSQL, AWS, ETL, Python, Terraform, Apache Airflow, Git, Tableau, Power BI, SSIS, PowerShell, Agile, SSRS, Atlassian, Jira, Confluence","sql, tsql, aws, etl, python, terraform, apache airflow, git, tableau, power bi, ssis, powershell, agile, ssrs, atlassian, jira, confluence","agile, apache airflow, atlassian, aws, confluence, etl, git, jira, powerbi, powershell, python, sql, ssis, ssrs, tableau, terraform, tsql"
Data Engineer,Alto Pharmacy,United States,https://www.linkedin.com/jobs/view/data-engineer-at-alto-pharmacy-3756084142,2023-12-17,Kitchener, Canada,Associate,Remote,"Alto Pharmacy is a full-service, digitally-powered pharmacy that makes it simple to live your healthiest life by providing an easier, more supportive and more affordable pharmacy experience. We’re redefining what a pharmacy can do, with fast and reliable prescription delivery, tools like treatment reminders and medication bundling, direct access to care specialists, and support with insurance and cost savings. By focusing on the person behind the prescription, our model boosts adherence, improves health outcomes, and keeps our customers returning month after month. Learn more at www.alto.com.
The Data Engineering team at Alto owns the platform and infrastructure that enables data driven decision making, powers innovative pharmacy specific machine learning applications, and provides a high quality experience for internal users (analysts, data scientists, business partners) and external customers (other businesses Alto partners with to realize our mission to improve the quality of life for our patients).
Accelerate Your Career as You
Develop the next generation of our analytics and reporting platform to enable low latency insights
Refine our dimensional models and improve data self service
Partner with data scientists to improve the usability and performance of Kubeflow, our ML platform
Partner with analysts to improve the usability and performance of our modeling pipelines
Minimum Qualifications
A Bit About You
3+ years of data engineering experience
Strong technical proficiency with Python and SQL
Experience with data modeling and dimensionalization best practices
Experience with ETL pipelines, data analysis, BI tools, and cloud services
Strong sense of ownership and pride in your work
Comfortable working at startup pace and focus
Additional Physical Job Requirements
Read English, comprehend, and follow simple oral and written instructions. The worker is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading. Assessing the accuracy, neatness and thoroughness of the work assigned.
Communicating with others to exchange information. Expressing or exchanging ideas by means of the spoken word; those activities where detailed or important spoken instructions must be conveyed to other workers accurately, loudly, or quickly.
Perceiving the nature of sounds at normal speaking levels with or without correction, and having the ability to receive detailed information through oral communication, and making fine discriminations in sound.
Frequent repeating motions required to operate a computer that may include the wrists, hands and/or fingers.
Sedentary work: Sitting most of the time, exerting up to 10 pounds of force occasionally and/or a negligible amount of force frequently or constantly to lift, carry, push, pull or otherwise move objects, including the human body. Walking and standing are required only occasionally.
Salary And Benefits
Salary Range
: $145,000 - $165,000
Commission Eligible:
No
Equity Eligible
: Yes
Travel
: Yes. Up to 15% of the time.
Location Requirement:
Employment at Alto is limited to individuals residing in the following states: Washington, California, Nevada, Colorado, Texas, and New York.
Employment Requirement:
Applicants must be authorized to work for any employer in the U.S. At this time, Alto is unable to sponsor or take over sponsorship of an employment Visa.
Benefits
: Full-time: Medical, Dental, Vision, 401(k), Group Life, AD&D, Employer paid STD/LTD, generous PTO and parental leave.
Alto Pharmacy is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. We are an E-Verify company.
To learn about Alto's privacy practices including compliance with applicable privacy laws, please click here
Show more
Show less","Data Engineering, Python, SQL, Data Modeling, Dimensionalization, ETL, Data Analysis, BI Tools, Cloud Services, Kubeflow, Machine Learning, Data Science","data engineering, python, sql, data modeling, dimensionalization, etl, data analysis, bi tools, cloud services, kubeflow, machine learning, data science","bi tools, cloud services, data engineering, data science, dataanalytics, datamodeling, dimensionalization, etl, kubeflow, machine learning, python, sql"
Data Engineer,SurveyMonkey,"Portland, OR",https://www.linkedin.com/jobs/view/data-engineer-at-surveymonkey-3782561884,2023-12-17,Kitchener, Canada,Associate,Remote,"SurveyMonkey
is a global leader in online surveys and forms that empowers people with the insights they need to make decisions with speed and confidence.
Our fast, intuitive feedback management platform connects millions of users worldwide with real-time AI-powered insights that drive meaningful decisions. We provide answers to more than 20 million questions every day so that people and organizations can attract new audiences, delight customers, create advocates, and extend their competitive advantage in the marketplace. Our vision is to raise the bar for human experiences by amplifying individual voices. Learn more at surveymonkey.com.
What We’re Looking For
We are looking for an experienced data Engineer to join a team of highly skilled data engineers and data architects to build and manage the end-to-end data pipelines (batch and near real-time) using modern cloud technologies. This is a role in the central data organization at SurveyMonkey that provides actionable insights into all key business functions of the organization
As a Data Engineer, you will work on data engineering initiatives and build end-to-end analytical solutions that are highly available, scalable, stable, secure, and cost-effective.
What You’ll Be Working On
Design, architect and build data pipelines to support existing data models
Data quality: Build quality checks in the end-to-end data pipelines
Build new Data models (Fact vs Dimension). Write performant/idempotent transformations in Snowflake
Build data pipeline using Python scripting (in a modular/loop context) Write well-tested, production-ready code in Python and Snowflake SQL
Hands-on experience implementing ETL (or ELT) best practices
Translate business requirements, to technical specifications, form project scope, and deliver deployable code.
Write complex data engineering Snowflake - SQL jobs that perform sophisticated queries on the entirety of our datasets
Collaborate closely with stakeholders, upstream and downstream partners
Document our systems for internal and external stakeholders
Monitor and debug data pipelines running on Airflow
Participate in code reviews
We’d love to hear from people with
4+ years experience in data engineering and Data warehousing technologies
2+ years experience in Snowflake/ETL or similar technologies like Redshift
Experience with AWS cloud services: S3, EC2, RDS, Spark, EMR etc
Experience with object-oriented/object function scripting languages: Python (preferred), Java, Scala, etc.
Experience in orchestrating, automating, and deploying production data pipelines using Airflow/Luigi, etc
Experience with DevOPS: Git, Github actions, CI/CD pipelines, Terraform, etc
Experience building Data infrastructure or Data Platform Framework Preferred
Experience with tools such as DBT or other similar technologies
Experience with transforming, and developing data structures, metadata, dependency, and data workflows to support an Analytics function
In-depth knowledge of Data lakes, EDW concepts, and data modeling (Star, Snowflake, and Galaxy schemas)
The base pay provided for this position ranges from $106,880 / year - $160,320 / year depending on the geographic market and assuming a full-time schedule. Actual base pay is based on a number of factors including market location, job-related knowledge, education or training, skills, and experience.
Bonuses and commissions may also be offered as part of the total compensation package, in addition to a competitive benefits package including medical, dental, vision, life, and disability insurance; 401(k) retirement plan; flexible spending & health savings account; paid holidays; paid time off; employee assistance program; and other company benefits.
Why SurveyMonkey? We’re glad you asked
SurveyMonkey is a place where the curious come to grow. We’re building an inclusive workplace where people of every background can excel no matter their time zone. At SurveyMonkey, we weave employee feedback into everything we do to create forward-looking benefits policies, employee programs, and an award-winning culture, including best workplace for parents, our annual holiday refresh, our annual week of service, and our C.H.O.I.C.E Fund. In addition, we’ve reimagined the way we work to allow employees to choose what works best for them -- working in-person, fully remote, or a hybrid model that combines the two through our Choice Model.
Our commitment to an inclusive workplace
SurveyMonkey is an equal opportunity employer committed to providing a workplace free from harassment and discrimination. We celebrate the unique differences of our employees because that is what drives curiosity, innovation, and the success of our business. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, gender identity or expression, age, marital status, veteran status, disability status, pregnancy, parental status, genetic information, political affiliation, or any other status protected by the laws or regulations in the locations where we operate. Accommodations are available for applicants with disabilities.
Show more
Show less","Python, Snowflake, AWS cloud services, S3, EC2, RDS, Spark, EMR, Java, Scala, Airflow, Luigi, Git, Github actions, CI/CD pipelines, Terraform, DBT, Data lakes, EDW concepts, Data modeling, Star schemas, Snowflake schemas, Galaxy schemas","python, snowflake, aws cloud services, s3, ec2, rds, spark, emr, java, scala, airflow, luigi, git, github actions, cicd pipelines, terraform, dbt, data lakes, edw concepts, data modeling, star schemas, snowflake schemas, galaxy schemas","airflow, aws cloud services, cicd pipelines, data lakes, datamodeling, dbt, ec2, edw concepts, emr, galaxy schemas, git, github actions, java, luigi, python, rds, s3, scala, snowflake, snowflake schemas, spark, star schemas, terraform"
Big Data Engineer (No C2C),Aptonet Inc,"Atlanta, GA",https://www.linkedin.com/jobs/view/big-data-engineer-no-c2c-at-aptonet-inc-3768097748,2023-12-17,Kitchener, Canada,Associate,Remote,"Big Data Engineer (No C2C)
REMOTE on EST Schedule
Job Summary
As a Big Data Engineer, you will be responsible for designing, building, and optimizing big data pipelines. You will work closely with data scientists and other stakeholders to support the data needs of the organization.
Responsibilities
Design and implement high-velocity, high-volume data streaming solutions using Apache Kafka and Spark Streaming.
Develop real-time data processing and streaming techniques using Spark Structured Streaming and Kafka.
Troubleshoot and optimize Spark applications for peak performance.
Work closely with Python and/or Scala for data processing tasks (PySpark/Scala-Spark).
Utilize Databricks for cloud-based big data solutions.
Build, test, and optimize big data ingestion pipelines, architectures, and datasets.
Deploy data platforms on Azure or AWS and manage serverless technologies like S3, Kinesis/MSK, Lambda, and Glue.
Operate messaging platforms like Kafka, Amazon MSK, TIBCO EMS, or IBM MQ Series for asynchronous data communication.
Manage Databricks Notebooks, work with Delta Lake using both Python and Spark SQL, and manage Delta Live Tables and the Unity Catalog.
Ingest data from various formats including JSON, XML, and CSV.
Work with NoSQL databases like HBASE and/or Cassandra.
Perform shell scripting and other tasks on Unix/Linux platforms.
Work with other database solutions like Kudu/Impala or Delta Lake.
Qualifications - must have
Must have hands-on experience with high-velocity, high-volume stream processing: Apache Kafka and Spark Streaming.
Experience with real-time data processing and streaming techniques using Spark structured streaming and Kafka.
Deep knowledge of troubleshooting and tuning Spark applications.
Must have hands-on experience with Python and/or Scala (PySpark/Scala-Spark).
Must have experience with Databricks.
Must have hands-on experience building, testing, and optimizing big data ingestion pipelines, architectures, and datasets.
Experience in successfully building and deploying a new data platform on Azure/AWS.
Experience in Azure/AWS Serverless technologies, like S3, Kinesis/MSK, Lambda, and Glue.
Strong knowledge of Messaging Platforms like Kafka, Amazon MSK & TIBCO EMS or IBM MQ Series.
Experience with Databricks UI, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL, Delta Live Tables, Unity Catalog.
Experience with data ingestion of different file formats like JSON, XML, CSV.
Experience with NoSQL databases, including HBASE and/or Cassandra.
Knowledge of Unix/Linux platform and shell scripting is a must.
Experience with database solutions like Kudu/Impala, or Delta Lake.
Show more
Show less","Big Data, Apache Kafka, Spark Streaming, Spark Structured Streaming, Python, Scala, PySpark, ScalaSpark, Databricks, Azure, AWS, Serverless technologies, S3, Kinesis, Lambda, Glue, Kafka, Amazon MSK, TIBCO EMS, IBM MQ Series, Databricks Notebooks, Delta Lake, Unity Catalog, JSON, XML, CSV, HBASE, Cassandra, Unix/Linux, Kudu/Impala","big data, apache kafka, spark streaming, spark structured streaming, python, scala, pyspark, scalaspark, databricks, azure, aws, serverless technologies, s3, kinesis, lambda, glue, kafka, amazon msk, tibco ems, ibm mq series, databricks notebooks, delta lake, unity catalog, json, xml, csv, hbase, cassandra, unixlinux, kuduimpala","amazon msk, apache kafka, aws, azure, big data, cassandra, csv, databricks, databricks notebooks, delta lake, glue, hbase, ibm mq series, json, kafka, kinesis, kuduimpala, lambda, python, s3, scala, scalaspark, serverless technologies, spark, spark streaming, spark structured streaming, tibco ems, unity catalog, unixlinux, xml"
Data Engineer-Marketing Solutions (Remote),Vericast,"San Antonio, TX",https://www.linkedin.com/jobs/view/data-engineer-marketing-solutions-remote-at-vericast-3785888378,2023-12-17,Kitchener, Canada,Associate,Remote,"Company Description
Vericast is a premier marketing solutions company that accelerates profitable revenue growth for thousands of businesses businesses it serves directly by influencing consumer purchasing and transaction behavior at scale while engaging with over 120 million households daily. We are recognized as leading providers of incentives, advertising, marketing services, transaction solutions, customer data and cross-channel campaign management, and intelligent media delivery that create millions of customer touch points annually for their clients. For more information, visit http://www.vericast.com or follow Vericast on LinkedIn.
Job Description
Vericast is seeking for a motivated Data Engineer to build big data pipelines, marketing campaign automations, and support existing analytics and reporting data requirements. This role will be responsible for expanding our data infrastructure and data pipeline architecture, as well as optimizing data flow and collection towards our Data Lake environment.
The Data Engineer will work closely with our analytics and reporting as well as data scientists on data initiatives and will ensure that optimal data delivery architecture is consistent and always available throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
This position is a perfect fit for someone who really likes to work in an agile team and to contribute their skills for building high quality data-driven marketing products, working collaboratively with other developers and business partners.
Key Duties/Responsibilities
Develop scalable data pipelines and builds out new integrations to support continuing increases in data volume and complexity.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making across the organization.
Implements processes and systems to monitor data quality, ensure availability and accuracy of production data for key stakeholders and business processes that depend on it.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Provide post-deployment support and to quickly respond to and resolve unexpected service problems in production.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Qualifications
EDUCATION:
Bachelor of Technology in Computer Science/Information Technology or any related field.
OR
Master of Science in Computer Science/Information Technology or any related field (preferred).
Experience
3+ years of experience in Data Engineering or ETL Development role.
Strong experience with PySpark for building solid data pipelines.
Hands-on experience with relational databases and SQL queries.
Experience with Agile Software Development methodologies.
Experience with GitLab, CI/CD process and ServiceNow etc.
Solid programming skills in object-oriented/functional scripting languages like Python, PySpark for building data pipelines with experience in testing, logging to ensure quality of code and data observability. (Required)
Experience in distributed systems and parallel data processing using big data tools such as Spark, PySpark, Hadoop, Kafka, Hive. (Required)
Proficiency in querying with relational databases. (Required)
Strong knowledge of Linux/Unix-based computer systems.
Experience in building Data Processing pipelines using ETL tools like Talend, SSIS etc. (Required)
Understanding of Machine Learning models and algorithms interfacing with Data Science team. (preferred)
Proficiency in data visualization tools to showcase insights using Tableau, matplotlib etc. (preferred)
Nice to have AWS cloud experiences in Redshift, Lambda, Sage Maker, Glue etc.(preferred)
Experience with building Rest API. (preferred)
Excellent data analytical, conceptual, and problem-solving skills.
Excellent communication skills to promote cross-team collaboration.
Additional Information
Salary:
$90,000 - $110,000
The ultimate compensation offered for the position will depend upon several factors such as skill level, cost of living, experience, and responsibilities.
All team members are responsible for demonstrating the company's Core Values at all times and for using Performance Excellence principles to continuously improve effectiveness, efficiency, products, and services. This includes, but is not limited to, participating on improvement teams, recommending, and implementing improvement ideas, and participating in training and other activities to keep up to date on processes, information, etc.
All team members are responsible for supporting and complying with safety and security policies to promote a healthy working environment.
Vericast offers a generous total rewards benefits package that includes medical, dental and vision coverage, 401K and flexible PTO. A wide variety of additional benefits like life insurance, employee assistance and pet insurance are also available, not to mention smart and friendly coworkers!
At Vericast, we don’t just accept differences - we celebrate them, we support them, and we thrive on them for the benefit of our employees, our clients, and our community. As an Equal Opportunity employer, Vericast considers applicants for all positions without regard to race, color, creed, religion, national origin or ancestry, sex, sexual orientation, gender identity, age, disability, genetic information, veteran status, or any other classifications protected by law. Applicants who have disabilities may request that accommodations be made in order to complete the selection process by contacting our Talent Acquisition team at talentacquisition@vericast.com. EEO is the law. To review your rights under Equal Employment Opportunity please visit: www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf.
Show more
Show less","PySpark, Spark, Hadoop, Kafka, Hive, Python, Linux, Data Engineering, ETL Development, Databases, SQL, Agile, GitLab, CI/CD, ServiceNow, Machine Learning, Tableau, matplotlib, Rest API, Data Visualization","pyspark, spark, hadoop, kafka, hive, python, linux, data engineering, etl development, databases, sql, agile, gitlab, cicd, servicenow, machine learning, tableau, matplotlib, rest api, data visualization","agile, cicd, data engineering, databases, etl development, gitlab, hadoop, hive, kafka, linux, machine learning, matplotlib, python, rest api, servicenow, spark, sql, tableau, visualization"
Data Management Developer,Donyati,United States,https://www.linkedin.com/jobs/view/data-management-developer-at-donyati-3780001865,2023-12-17,Kitchener, Canada,Associate,Remote,"Seeking Data Management Developer with experience using multiple applications. Must have 5+ years of Financial Data Quality Management (FDMEE)/Data Management experience.
· Should have knowledge and be able to demonstrate the capability to Integrate multiple sources
· Experience integrating with EPM Products. i.e. Consolidation, Planning, Essbase,)
· Should be able to provide recommendations on Data Management usage
· LCM Migration of FDMEE/Data Management and other Oracle Product Artifacts
· Experience with Jython import, custom, and event scripting
· Extensive experience troubleshooting data load issues
· Complex mapping inside Data Management, multi-dimension, leveraging import format
· Experience creating and modifying reports for Data Management
· Good understanding of the Data Management table structure
· Basic understanding of ODI
· Good understanding of location and intermediate security
· Experience with Batch processing including before/after batch scripting and scheduling
· Experience troubleshooting and modifying Excel templates
Compensation & Benefits
Donyati offers competitive compensation packages, as well as a robust benefits package including medical, dental, vision, and life / disability insurance. Also offered is a 401k match and unlimited paid time off with a focus on work life flexibility and a people first culture.
About Donyati
Donyati was founded as a challenge to the status quo of business and technology consulting. At Donyati we leverage technology to solve business challenges and deliver innovative solutions with expertise, hard work and passion. We listen, advise, and deliver on a continuous basis to achieve our client’s desired outcomes. We always have our client’s best interests at heart.
Show more
Show less","Data Management, Jython, ODI, Batch Processing, Data Quality Management (FDMEE), Essbase, Oracle Product Artifacts, EPM Products, Consolidation, Planning","data management, jython, odi, batch processing, data quality management fdmee, essbase, oracle product artifacts, epm products, consolidation, planning","batch processing, consolidation, data management, data quality management fdmee, epm products, essbase, jython, odi, oracle product artifacts, planning"
Data Analyst,Interactive Resources - iR,United States,https://www.linkedin.com/jobs/view/data-analyst-at-interactive-resources-ir-3778689285,2023-12-17,Kitchener, Canada,Associate,Remote,"Sr Data Analyst
Seeking a Senior Data Analyst to join our dynamic team. As a Senior Data Analyst, you will play a critical role in analyzing and interpreting large-scale data sets related to our customer base, payment systems, pricing, logistics, and customer cohorts. Your insights and recommendations will directly contribute to optimizing our ecommerce operations and enhancing customer experiences. This is an excellent opportunity for a seasoned Sr Data Analyst to develop into a Data Scientist.
Key Responsibilities
Conduct in-depth data analysis to identify trends, patterns, and anomalies in large datasets.
Collaborate with various teams to gather and consolidate data from multiple sources, ensuring data quality and accuracy.
Develop and maintain data models to support business requirements and reporting needs.
Create interactive and meaningful data visualizations, dashboards, and reports to communicate insights to stakeholders.
Apply statistical techniques to analyze data and provide recommendations.
Develop and implement predictive models to support forecasting and decision-making.
Utilize advanced data mining techniques to discover hidden insights within our data
Ensure data quality, integrity, and security by defining and implementing data governance best practices.
Collaborate with various teams, including business units, IT, and executives, to understand their data needs and provide data-driven solutions.
Mentor and provide guidance to junior data analysts on best practices, data analysis
Stay up-to-date with the latest industry trends, tools, and technologies in data analytics.
Qualifications & Skills
Bachelor's or Master's degree in a quantitative field such as Data Analytics, Statistics, Mathematics, Computer Science, or a related field.
3+ years of experience as a Data Analyst, preferably in an ecommerce or related industry.
Proficiency in using Python and SQL to query data, generate visualizations, and run statistical analysis.
Strong analytical and problem-solving skills and attention to detail with the ability to think critically and propose innovative solutions to complex business challenges.
Excellent communication skills with the ability to effectively present data-driven insights to technical and non-technical stakeholders.
Ability to work independently and collaboratively in a fast-paced environment.
Strong project management skills, including the ability to prioritize and meet deadlines.
Knowledge of data governance and best practices.
Experience working with large-scale datasets and conducting statistical analysis.
Knowledge of ecommerce operations and customer behavior is a plus.
If you are a detail-oriented data analyst with a passion for uncovering valuable insights and a desire to contribute to the success of a rapidly growing company, we would love to hear from you. Join our team and help us shape the future of our ecommerce operations through data-driven decision-making.
Show more
Show less","Data Analysis, Data Visualization, Data Mining, Machine Learning, Data Governance, Python, SQL, Statistical Analysis, Data Warehousing, Data Modeling, Data Reporting, Ecommerce, Customer Behavior","data analysis, data visualization, data mining, machine learning, data governance, python, sql, statistical analysis, data warehousing, data modeling, data reporting, ecommerce, customer behavior","customer behavior, data governance, data mining, data reporting, dataanalytics, datamodeling, datawarehouse, ecommerce, machine learning, python, sql, statistical analysis, visualization"
Data Analyst,e-Emphasys Technologies,United States,https://www.linkedin.com/jobs/view/data-analyst-at-e-emphasys-technologies-3778875329,2023-12-17,Kitchener, Canada,Associate,Remote,"About Us
We are a leading ERP software and solutions provider to dealers and rental companies of heavy equipment. We specialize in working with the construction, mining, forestry, material handling, and agriculture industries. We aim to be the ERP thought leader in our space and a trusted IT advisor to all dealers and rental companies. Today, we have over 700 employee associates, offices on four continents, and customers in over 20 countries. We are privately held, and our headquarters are in beautiful Cary, NC. We are actively seeking talented individuals to join our team and help us aggressively grow our North American footprint for both our on-premises and 100% cloud-based ERP solutions.
Position Overview
The ideal candidate will be highly skilled in all aspects of data analytics, including mining, generation, and visualization. In this role, we are looking for a self-starter who is a team player and able to build a strong Business Analytics eco-system using Power BI. Additionally, this person should be committed to transforming data into readable, goal-oriented reports that drive innovation and growth. Develop, implement, and maintain leading-edge analytics systems, taking complicated problems and building simple frameworks. Identify trends and opportunities for growth through analysis of complex datasets. Evaluate organizational methods and provide source-to-target mappings and information-model specification documents for datasets. Create best-practice reports based on data mining, analysis, and visualization. Work directly with stakeholders to gather requirements, provide status updates, and build relationships.
Required Skills & Education
Three or more years of experience mining data as a data analyst.
Strong experience in Power BI as analytics and reporting platform.
Proven analytics skills, including mining, evaluation, and visualization.
Technical writing experience in relevant areas, including queries, reports, and presentations.
Strong SQL and Excel skills, with aptitude for learning other analytics tools.
Experience in building data warehouse for the organization by aggregating various data sources and building co-relation between datasets.
Strong communication skills with ability to interact with senior management.
Preferred Skills
Bachelor’s degree (or equivalent) in mathematics, computer science, economics, or statistics
Experience with database and model design and segmentation techniques.
Proven success in a collaborative, team-oriented environment.
Experience in other analytics tools such as Tableau.
Responsibilities
Work closely with stakeholders both internal and external to understand and maintain focus on their analytics needs, including critical metrics and KPIs, and deliver actionable insights to relevant decision-makers.
Proactively analyze data to answer key questions for stakeholders, with an eye on what drives business performance, and investigate and communicate which areas need improvement in efficiency and productivity.
Create and maintain rich interactive visualizations through data interpretation and analysis, with reporting components from multiple data sources.
Define and implement data acquisition and integration logic, selecting an appropriate combination of methods and tools within the defined technology stack.
Develop and maintain databases by acquiring data from primary sources and build scripts that will make our data evaluation process more flexible or scalable across datasets.
e-Emphasys is an Equal Opportunity Employer
Show more
Show less","Data Analytics, Data Mining, Data Visualization, Power BI, SQL, Excel, Data Warehouse, Business Analytics, Technical Writing, Data Aggregation, Database Design, Model Design, Tableau","data analytics, data mining, data visualization, power bi, sql, excel, data warehouse, business analytics, technical writing, data aggregation, database design, model design, tableau","business analytics, data aggregation, data mining, dataanalytics, database design, datawarehouse, excel, model design, powerbi, sql, tableau, technical writing, visualization"
Data Analyst,Burtch Works,United States,https://www.linkedin.com/jobs/view/data-analyst-at-burtch-works-3764337410,2023-12-17,Kitchener, Canada,Associate,Remote,"**W2 REQUIRED-Sponsorship is not available**
In this role the
Data Analyst
is responsible for collecting, processing, and analyzing large sets of data to extract meaningful insights and support informed decision-making within an organization. Their key responsibilities include exploring and interpreting data, designing and generating reports and visualizations, ensuring data quality and accuracy, and collaborating with various teams to address analytical needs. Data Analysts use statistical methods and programming languages to perform analyses, identify trends and patterns, and communicate findings through clear presentations. They play a crucial role in transforming raw data into actionable information that contributes to business strategy and operational efficiency.
Responsibilities:
· Apply advanced statistical and machine learning techniques to extract valuable insights from large datasets.
· Collaborate with cross-functional teams to understand business needs and develop data-driven solutions.
· Build and deploy predictive models to enhance decision-making processes.
· Design, develop, and maintain robust data pipelines for efficient data extraction, transformation, and loading (ETL).
· Optimize and scale existing data infrastructure to support evolving business requirements.
· Collaborate with stakeholders to ensure data accuracy, availability, and security.
Position Qualifications:
· Bachelor’s/Master’s degree in Computer Science, Statistics, or a related field.
· Proven experience in data science, machine learning, and statistical analysis (for Data Scientist role).
· Strong proficiency in programming languages such as Python, R, or Java.
· Experience with data engineering tools and technologies (e.g., Apache Spark, Hadoop, SQL).
· Familiarity with cloud platforms (AWS, Azure, GCP) is a plus.
Show more
Show less","Data Analysis, Machine Learning, Data Science, Statistical Analysis, Python, R, Java, Apache Spark, Hadoop, SQL, AWS, Azure, GCP","data analysis, machine learning, data science, statistical analysis, python, r, java, apache spark, hadoop, sql, aws, azure, gcp","apache spark, aws, azure, data science, dataanalytics, gcp, hadoop, java, machine learning, python, r, sql, statistical analysis"
Data Analyst,Cypress HCM,United States,https://www.linkedin.com/jobs/view/data-analyst-at-cypress-hcm-3778852592,2023-12-17,Kitchener, Canada,Associate,Remote,"Analyst, Decision Sciences, Strategy and Insights
As a Decision Sciences Analyst on our Strategy and Insights team, you will be asked to answer a variety of complex questions utilizing a wide range of analytical methods and assist in assessing the business impact for our clients. With the ultimate goal to help them decide how to improve the performance of current and future campaigns. This role contributes to profitable growth by driving client ROI and confidence, to drive strong campaign renewal and retention rates.
What You’ll Do:
Independently coordinate, execute, and summarize internal customized research to answer business questions
Organize and analyze a wide array of data using syndicated query tools, Microsoft Excel, Python, R or Juia
Develop and present findings and recommendations to internal clients
Develop and present case studies that enable our sales teams to show pharmaceutical, biotech, and retail clients the benefits of messaging at the point of care
Desired Qualifications
Bachelor’s Degree is required
1-2 years of experience in research, statistics, healthcare economics, business intelligence, and/or related fields
Comfort working with large data sets (1,000-500,000 records)
Demonstrated skills in Excel (intermediate), PowerPoint (intermediate), and Word (intermediate)
Demonstrated skills with SQL
Demonstrated skills with at least one analytical program. Python, R, or Julia preferred
Demonstrated strong analytical skills and practical knowledge of statistics
What You'll Need to Succeed:
Self-motivation, strong ambition, and interest in directly impacting business results.
Resourcefulness, multi-tasking skills and creative problem-solving skills.
Curiosity and a strong passion for data, trend analysis and storytelling through data.
Resiliency and ability to overcome challenges, sound business judgment.
Passion for relationship building and building trusted partnerships.
Deep curiosity
Ability to prioritize across multiple internal and external priorities
Attention to detail
Ability to balance collaboration and independence in a high-energy environment
Ability to translate complex ideas in a way that is simply to understand
Passion for driving the success of our clients through analytics and storytelling
Compensation: $35.00 - $46.67 per hour
Show more
Show less","Decision Sciences, Strategy, Insights, Analytical Methods, Microsoft Excel, Python, R, Julia, SQL, Statistics, Data Analysis, Data Visualization, Business Intelligence, Healthcare Economics, PowerPoint, Word","decision sciences, strategy, insights, analytical methods, microsoft excel, python, r, julia, sql, statistics, data analysis, data visualization, business intelligence, healthcare economics, powerpoint, word","analytical methods, business intelligence, dataanalytics, decision sciences, healthcare economics, insights, julia, microsoft excel, powerpoint, python, r, sql, statistics, strategy, visualization, word"
Data Analyst,Shift Digital,United States,https://www.linkedin.com/jobs/view/data-analyst-at-shift-digital-3768069430,2023-12-17,Kitchener, Canada,Associate,Remote,"Shift Digital is the leader in digital marketing program optimization. Established in 2008 and headquartered in Birmingham, Michigan, our comprehensive solutions help national brands and their local retailers measure, manage, and improve the performance of their digital marketing and sales initiatives.
We are looking for a Data Analyst who is passionate about collaboration, problem solving, data analysis, data sciences, data architecture, data engineering, and data capture.
This is a full time, direct hire position with Shift Digital. We offer an extraordinary benefit package, very competitive compensation, paid holidays, generous paid time off (PTO) and other employee perks. This is an opportunity to join a highly interactive and advanced engineering team, where culture and technology are priority.
Responsibilities
Our team is responsible for providing guidance and direction for data stewardship and consumption, based on producer/consumer needs. our practices are constantly evolving so the ideal candidate should be very comfortable in a dynamic environment and should be a self-motivated learner who isn’t afraid of learning new tools and technologies. We drive data architecture/design solutions for complex business problems and lead data design and integration work efforts, considering risks, mitigations, performance, and user experience.
Data Analysts apply emerging technology solutions to increase efficiency and effectiveness. We love and use groundbreaking platforms such as Databricks, TensorFlow, sci-kit, Ballista, etc. We can’t wait to see what you do!
Join us!
Qualifications
Competitive candidate will bring the following required skills:
· Ability to work with business partners and other stakeholders to understand business requirements
· Experience in providing guidance and direction for data stewardship and consumption
· Experience in developing data strategy, data governance, data architecture and data design
· Mathematics / statistics and a bachelor's degree in a STEM area is preferred; but not required
· Strong competency with SQL
· Experience in working with “Big Data” platforms such as Hadoop, Databricks, or Snowflake
· Experience with data visualization platforms such as Tableau and Power BI
· Experience with data sciences fields and practices
· Experience working with python and/or Jupyter Notebooks
· Experience in defining and testing data requirements for movement, replication, synchronization and validation of data
· Experience in the marketing or advertising industries are a plus
· Ability to communicate effectively with technical and business partners
· Applicants are required to be eligible to lawfully work in the U.S. immediately; employer will not sponsor applicants for U.S. work authorization (e.g. H-1B visa) for this opportunity**
At Shift, many factors are taken into account when determining a reasonable compensation range for any given role. Some of these factors include, but are not limited to: skillset, location, years of experience, licenses, certifications, and business/organizational needs. The following compensation range is specific to New York City, Colorado, Nevada, and California residents
only
.
While we do provide an estimated salary range, it is not typical for an individual to be hired near the top range for their role. A reasonable estimate for this role is: $80,000 - $95,000.
Show more
Show less","SQL, Tableau, Power BI, Python, Data Sciences, Data Analysis, Data Engineering, Data Architecture, Data Capture, Data Governance, Data Design, Data Visualization, Big Data, Hadoop, Databricks, Snowflake, TensorFlow, scikit, Ballista, Jupyter Notebooks","sql, tableau, power bi, python, data sciences, data analysis, data engineering, data architecture, data capture, data governance, data design, data visualization, big data, hadoop, databricks, snowflake, tensorflow, scikit, ballista, jupyter notebooks","ballista, big data, data architecture, data capture, data design, data engineering, data governance, data sciences, dataanalytics, databricks, hadoop, jupyter notebooks, powerbi, python, scikit, snowflake, sql, tableau, tensorflow, visualization"
Senior Data Analyst,BioSpace,"North Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-analyst-at-biospace-3770939752,2023-12-17,Newport,United States,Mid senior,Onsite,"Our Business Technology Solutions (BTS) team shapes the digital transformation accelerating the future of medicine at AbbVie—and we do it together, asking bold questions and taking on tough challenges through deep, honest collaboration. In the role of Senior Data Analyst, you will join a diverse, global team that invests in your customized career growth and uses technology and data to positively impact people’s lives.
The Senior Data Analyst will guide AbbVie’s global IT quality organization by providing process knowledge and statistical expertise to drive data-driven decision making. The incumbent will utilize data science methodology (e.g., machine learning, AI, advanced analytics, etc.) to determine the impact of IT incidents on applications, processes and products, and perform trend monitoring of IT quality data. The individual will engage in process improvement opportunities that can lead to efficiencies such as reductions of cycle time, and cost reduction, all within the framework of preserving product quality. The senior data analyst will also drive the approaches and technology used for trend monitoring and data science initiatives in technical and quality areas.
Responsibilities
Identify process improvement opportunities through data science, machine learning, manufacturing site engagement, and applied global supply chain and process knowledge. Quantify potential savings versus costs, prioritize opportunities, and champion implementation.
Perform root cause analysis in support of complaint and process investigations. Guide teams through DMAIC process, communicate status during project execution, and document results.
Author/co-author/review/approve scientific reports and presentations to support and document process deviations/investigations and recommendations for corrective actions, determine impact in product quality, and support regulatory submissions. Report consumers vary but can include quality managers, senior management, internal technical groups, and regulatory agencies.
Establish commercial data footprints for commercial and pipeline products. Anticipate potential sources of manufacturing and post market issues based on FMEAs, supply chain variability, and process controls. Ensure critical data is available and linkable across geographically diverse manufacturing networks.
Perform ongoing monitoring and baseline activities to ensure that product quality trending complies with internal procedures and regulatory expectations.
Manage process performance metrics and actions/alerts generated from trend monitoring systems.
Work closely with product performance teams (PPTs) and BTS to build automated reporting systems that efficiently translate raw data into consumable information. Serve as a bridge between these groups to translate business needs into IT language and to help stakeholders leverage emerging information technologies.
Provide statistical and software training for relevant personnel across the organization as the need arises to improve awareness and understanding of data analysis offerings and techniques.
Create and manage ETL procedures and lead data discovery processes for data model creation and multidimensional models. Manage data model connections for automated data pulls and dashboard updates. Ensure that data models will meet the requirements of analytics and visualization tools to inform business decisions.
Create automated dashboards that meet business requirements and deliver value across the organization using data from data models spanning multiple systems and databases.
Bachelor’s degree with 5 years experience; Or Masters Degree with 4 years experience; Or PhD with 0 years experience.
Experience in business analytics, manufacturing data analytics, data mining, or statistical modeling in a cGMP related industry.
Knowledge of global regulatory requirements for pharmaceutical, medical devices, and combination products. Knowledge of FDA Quality Systems, pharmaceutical products, and Medical Device Reporting regulations (21 CFR 803, 820 and 211) is preferred.
Ability to execute analytical investigations methodically while outputting reproducible insights and analyses.
Software experience – Demonstrated proficiency with at least one statistical programming package (e.g., R, SAS, JMP, Minitab). Familiarity in Python, SQL, relational databases (Teradata, Oracle etc.), and/or BI tools (PowerBI, Qliksense). Experience with data visualization/pipelining tools (e.g., Spotfire, Tableau, Pipeline Pilot etc.) and automated statistical process control platforms (e.g., Discoverant, Statistica, NWA, etc.) is preferred.
Proficiency in SQL, cloud architecture, database administration, and relational database management with the capability to create automated ETL processes and data models.
Strong problem solving and interpersonal skills and ability to work as part of a diverse team including data engineers, IT, and business analytics teams.
Strong communication skills, written and verbal, and strong attention to detail.
Lean Six Sigma Greenbelt/Blackbelt certification is preferred.
Why Business Technology Solutions
For anyone who wants to use technology and data to make a difference in people’s lives, shape the digital transformation of a leading biopharmaceutical company, and secure sustainable career growth within a diverse, global team: we’re ready for you.
AbbVie is committed to operating with integrity, driving innovation, transforming lives, serving our community, and embracing diversity and inclusion. It is AbbVie’s policy to employ qualified persons of the greatest ability without discrimination against any employee or applicant for employment because of race, color, religion, national origin, age, sex (including pregnancy), physical or mental disability, medical condition, genetic information, gender identity or expression, sexual orientation, marital status, status as a protected veteran, or any other legally protected group status.
Show more
Show less","Data Science, Machine Learning, AI, Advanced Analytics, Statistical Programming, R, SAS, JMP, Minitab, Python, SQL, Teradata, Oracle, PowerBI, Qliksense, Spotfire, Tableau, Pipeline Pilot, Discoverant, Statistica, NWA, Lean Six Sigma, DMAIC, FMEA, ETL, Data Visualization, Data Mining, Statistical Modeling, Data Analysis, Business Analytics, Manufacturing Data Analytics, Cloud Architecture, Database Administration, Relational Database Management","data science, machine learning, ai, advanced analytics, statistical programming, r, sas, jmp, minitab, python, sql, teradata, oracle, powerbi, qliksense, spotfire, tableau, pipeline pilot, discoverant, statistica, nwa, lean six sigma, dmaic, fmea, etl, data visualization, data mining, statistical modeling, data analysis, business analytics, manufacturing data analytics, cloud architecture, database administration, relational database management","advanced analytics, ai, business analytics, cloud architecture, data mining, data science, dataanalytics, database administration, discoverant, dmaic, etl, fmea, jmp, lean six sigma, machine learning, manufacturing data analytics, minitab, nwa, oracle, pipeline pilot, powerbi, python, qliksense, r, relational database management, sas, spotfire, sql, statistica, statistical modeling, statistical programming, tableau, teradata, visualization"
Junior Data Scientist,DigitalGenius,United Kingdom,https://uk.linkedin.com/jobs/view/junior-data-scientist-at-digitalgenius-3746745148,2023-12-17,Preston, United Kingdom,Associate,Remote,"Description
DigitalGenius is a venture-backed artificial intelligence company bringing practical applications of deep learning and AI to some of the largest customer service operations in the world as well as high growth companies. We’re a dedicated team of thoughtful and hard-working people committed to transforming customer service through the application of artificial intelligence. Our team is fully remote with colleagues based in the UK, US and Europe
Role
We are looking for a Junior NLP Data Scientist who will be part of a three-member Machine Learning Team. The rapidly growing number of clients requires us to scale our Machine Learning team, so we are looking for a new member to take on part of the day-to-day tasks.
As a Data Scientist at DigitalGenius, you will be responsible for leveraging our existing machine learning solutions, APIs, and analytics to maximise the product’s performance for our customers.
The Machine Learning team at DigitalGenius owns all Data Science related projects. This includes everything from research, implementation, API management, testing, production, data analysis and error reporting.
Responsibilities
Proactively work with team members and clients on data analysis, machine learning model evaluations and reporting
Data Wrangling: collection of closed data sources and data processing
Be responsible for all clients’ data analysis
Work closely with our labellers
Manage/Tune existing and legacy machine learning models
Participate in Research and development projects
Requirements
Requirements
Strong Knowledge of Python
Good knowledge of NLP techniques such as message cleaning, vectorising, clustering etc
Basic understanding of machine learning techniques and algorithms such as supervised and unsupervised learning techniques, recommendation algorithms, predictive modelling and statistics
Knowledgeable of machine learning and data handling libraries such as PyTorch, Scikit-learn, Pandas, Numpy
Excellent organisation skills, work independently and ability to deliver results for deadlines
A proactive, innovative and pragmatic approach to problem solving, and an ability to think critically and objectively
Good customer-facing skills and ability to clearly communicate technical issues to both technical and non-technical audiences
UK working hours
Benefits
Benefits
Competitive Salary & Equity Package
Generous vacation time (25 days of holiday annually)
Free Lunch & Delivery every Friday
Yearly ""Free Week"" in addition to annual leave allowance
$500 One-Time Home Office Allowance
Freedom to experiment with your own ideas
Environment to develop yours skills without bureaucracy or red-tape
Monthly fitness stipend of $210 or fully paid Third Space Membership
We are an equal-opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, colour, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Machine Learning, NLP, Python, PyTorch, Scikitlearn, Pandas, Numpy, Message cleaning, Vectorising, Clustering, Supervised learning, Unsupervised learning, Recommendation algorithms, Predictive modeling, Statistics, Data wrangling, Data collection, Data processing, Data analysis, Model evaluation, Model tuning, Research and development, Data labeling, Problem solving, Critical thinking, Communication, Customer service","machine learning, nlp, python, pytorch, scikitlearn, pandas, numpy, message cleaning, vectorising, clustering, supervised learning, unsupervised learning, recommendation algorithms, predictive modeling, statistics, data wrangling, data collection, data processing, data analysis, model evaluation, model tuning, research and development, data labeling, problem solving, critical thinking, communication, customer service","clustering, communication, critical thinking, customer service, data collection, data labeling, data processing, data wrangling, dataanalytics, machine learning, message cleaning, model evaluation, model tuning, nlp, numpy, pandas, predictive modeling, problem solving, python, pytorch, recommendation algorithms, research and development, scikitlearn, statistics, supervised learning, unsupervised learning, vectorising"
Data Engineer,Actica Consulting,United Kingdom,https://uk.linkedin.com/jobs/view/data-engineer-at-actica-consulting-3724349226,2023-12-17,Preston, United Kingdom,Associate,Hybrid,"Department:
Consultancy
Employment Type:
Full Time
Location:
Guildford
Description
This is an exciting time in Actica's development. We will soon have a permanent headcount of over 200 people and our plans are to grow our business organically at around 20% per annum - a rate that we believe is sustainable and will not compromise our focus on quality and trust with our clients. In addition, we will make selective acquisitions that build our capabilities and, critically, are aligned with our culture and values.
Growth of our data practice is a core element of our plans. This practice provides our clients with hands-on experience to help them achieve more with data by inspecting, cleansing, transforming, categorising, manipulating and modelling data to identify useful information. We create Extract, Transform and Load (ETL) pipelines and use scientific methods processes and algorithms to unlock knowledge and insight from structured and unstructured data. We create dashboards and other data visualisation methods to help our clients exploit the unlocked information.
Actica Consulting is a growing Technology Consultancy that works on some of the most nationally critical technology projects. We specialise in delivering complex projects to limited timescales. Our customers include a wide range of public sector organisations; such as, the Home Office, the Ministry of Defence, HM Treasury, the Ministry of Justice, Police Forces, Fire Services and the NHS; as well as a range of national and international companies across the private sector.
We are recruiting in order to facilitate continued growth of the business and as a result multiple positions are available across Consultant, Senior Consultant, and Principal Consultant grades. The advertised role is for employment on a permanent basis; however, we are happy to discuss Associate (contract) opportunities if that is your preference.
Collaborate with application teams to assess existing on-premises or legacy applications and plan their migration to the cloud.
Have a high-level knowledge of security and compliance requirements throughout the design process. Then implement appropriate security controls, such as identity and access management, encryption, and network security, to protect data and resources.
Optimize existing cloud-based systems for performance and scalability, understanding bottlenecks and then implement solutions such as caching mechanisms, load balancing, and auto-scaling capabilities efficiently.
Collaboration with cross-functional teams, including developers, operations personnel, department stakeholders.
Candidates must be eligible and willing to gain high level UK Security Clearance to apply for this position.
Key Responsibilities
We are looking for people who know what the future data platforms will need to look like and how they will interact with legacy client architectures. We ask you to provide your ideas, expertise, enthusiasm and commitment. In return, we will provide stimulating work that improves peoples' lives; opportunities to work in a team with more experienced subject matter specialists as well as guiding those with less experience; and a supportive culture where you will be able to achieve your full potential.
Skills, Knowledge And Expertise
The successful candidates will have:
Experience of working in Agile teams using modern collaborative working tools (e.g. GIT, JIRA, Confluence).
Good coding skills in one or more of the following languages: Python, Java, Go, PowerShell, C#, or Ruby and familiarity with best practice coding guidelines and secure development principles.
Experience in using continuous integration and automated deployment pipelines.
Basic experience of automation languages such as Kubernetes/HELM, Terraform, Ansible, Puppet, Bash.
Some exposure to advanced analytic techniques.
Familiarity with modern data platform architectures including containerisation.
Experience in one or more of the following is desirable but not essential:
Building and managing fully automated pipelines (ETL, ELT, ELTL) including:
Designing and building data interfaces to source systems
Combining and transforming data into appropriate formats for storage
Developing data sets for analytics purposes
Developing pipelines that can handle common errors in an automated way
Cloud analytic platforms such as the relevant AWS, Azure or GCP services
Data visualisation tools e.g., Power BI, Qlik Sense, Kibana;
Databases such as Elastic, Mongo, Neo4j, Postgres.
The position represents a unique opportunity to join an ambitious and impressive team part-way through a period of significant expansion. A competitive remuneration package will be provided. More details will be provided during the interview process.
Benefits
25 days of paid leave per annum plus 8 UK bank holidays
Participation in a discretionary bonus scheme
Participation in our stakeholder pension scheme
Participation in cycle to work scheme
Employee Assistance Programme
Electric Car leasing Scheme
Show more
Show less","Python, Java, Go, PowerShell, C#, Ruby, GIT, JIRA, Confluence, Kubernetes/HELM, Terraform, Ansible, Puppet, Bash, ETL, ELT, ELTL, AWS, Azure, GCP, Power BI, Qlik Sense, Kibana, Elastic, Mongo, Neo4j, Postgres","python, java, go, powershell, c, ruby, git, jira, confluence, kuberneteshelm, terraform, ansible, puppet, bash, etl, elt, eltl, aws, azure, gcp, power bi, qlik sense, kibana, elastic, mongo, neo4j, postgres","ansible, aws, azure, bash, c, confluence, elastic, elt, eltl, etl, gcp, git, go, java, jira, kibana, kuberneteshelm, mongo, neo4j, postgres, powerbi, powershell, puppet, python, qlik sense, ruby, terraform"
Data Analyst / Scientist,Actica Consulting,United Kingdom,https://uk.linkedin.com/jobs/view/data-analyst-scientist-at-actica-consulting-3724350140,2023-12-17,Preston, United Kingdom,Associate,Hybrid,"Department:
Consultancy
Employment Type:
Full Time
Location:
Guildford
Description
As a Data Analyst or Data Scientist at Actica, you will have the opportunity to use and develop your data skills to unlock business problems for a wide range of high-profile UK public sector organisations. Your help will enable them to get the maximum benefit from their investments in technology, information and facilities, and in doing so, play a key role in nationally critical projects which make a real difference to people's everyday lives.
Actica recognises that Data Analytics and Data Science are different disciplines, though closely related. When we talk about Data Analytics, we think primarily of exploiting existing data to generate deeper insights and support faster, more assured decision-taking.
When we talk about Data Science, our focus is on scoping, designing, and developing innovative ways to capture and exploit new or previously unexploited data. Actica colleagues tend to move between the two, depending on experience, interest, and client requirements.
In either role, you would be expected to translate business problems into data queries, applying data standards and principles to add fresh understanding. You should be able to provide advice on how best to set up data exploitation to assure success, including consideration of a range of different languages and toolsets. You will need to relate your work to the clients' wider data environment, so understanding of some of the key principles of data architecture is also important.
This role will require you to work flexibly as part of teams of varying sizes and compositions – including data specialists and others with different skillsets. Over time, you may move on to leading assignments and take responsibility for supervising direct reports whilst ensuring the overall success of the engagement.
Actica Consulting is a growing Technology Consultancy that works on some of the most nationally critical technology projects. We specialise in delivering complex projects to limited timescales. Our customers include a wide range of public sector organisations; such as, the Home Office, the Ministry of Defence, HM Treasury, the Ministry of Justice, Police Forces, Fire Services and the NHS; as well as a range of national and international companies across the private sector.
We are recruiting in order to facilitate continued growth of the business and as a result multiple positions are available across Consultant, Senior Consultant, and Principal Consultant grades. The advertised role is for employment on a permanent basis; however, we are happy to discuss Associate (contract) opportunities if that is your preference.
Collaborate with application teams to assess existing on-premises or legacy applications and plan their migration to the cloud.
Have a high-level knowledge of security and compliance requirements throughout the design process. Then implement appropriate security controls, such as identity and access management, encryption, and network security, to protect data and resources.
Optimize existing cloud-based systems for performance and scalability, understanding bottlenecks and then implement solutions such as caching mechanisms, load balancing, and auto-scaling capabilities efficiently.
Collaboration with cross-functional teams, including developers, operations personnel, department stakeholders.
Candidates must be eligible and willing to gain high level UK Security Clearance to apply for this position.
Key Responsibilities
Working either individually or – more usually - as part of a larger team, you will undertake assignments such as:
Creating genuinely useful data outcomes to help deliver client goals in major business transformation programmes;
Hands-on analytics, including working with clients, users and other team members to address data availability and quality issues before they become blockers;
Carrying out 'pre-discovery' to establish which business problems, or which parts of business problems, are best addressed through data analytics / data science;
Translating business problems into data exploitation queries;
Specifying data analytics and data science environments;
Understanding data flows, i.e., which parts of the organization generate data, which require data to function, how data flows are managed, and how data changes in transition;
Collaborating and coordinating with multiple departments, stakeholders, partners, and external vendors.
Skills, Knowledge And Expertise
We work predominantly within the UK Defence and Public Sectors so experience within those sectors is desirable, preferably in roles which have used data to solve business problems. The fast-moving nature of the Public Sector technology environment, together with the need to resource multiple, ad-hoc assignments, also requires our data analysts and data scientists to be quick thinkers, proactive and self-motivated – with the ability to apply a structured approach to often unfamiliar subject matter.
The following attributes and areas of experience will make you particularly suited to this role with Actica:
Coding eg expertise in Python or R;
Collaborative, team-based development;
Cloud analytics platforms eg relevant AWS and Azure platform services;
Data visualisation tools eg Power BI, Qlik Sense;
Data science approaches and tooling eg Hadoop, Spark;
Data engineering approaches;
Database management, eg MySQL, Postgress;
Government Digital Service approaches, ie the Government Service Design Manual and the Service Standard;
Software development methods and techniques eg Agile methods such as SCRUM;
Software change management, notably familiarity with git;
Public sector best practice guidance, eg ITIL, OGC toolkit.
You will also need to hold or be eligible to hold UK security clearance; more information about the clearances can be found here: United Kingdom Security Vetting: clearance levels - GOV.UK (www.gov.uk)
Benefits
25 days of paid leave per annum plus 8 UK bank holidays
Participation in a discretionary bonus scheme
Participation in our stakeholder pension scheme
Participation in cycle to work scheme
Employee Assistance Programme
Electric Car leasing Scheme
Show more
Show less","Data Analytics, Data Science, Python, R, AWS, Azure, Power BI, Qlik Sense, Hadoop, Spark, MySQL, Postgress, Agile, SCRUM, Git, ITIL, OGC toolkit, Public sector best practice guidance, UK Security Clearance","data analytics, data science, python, r, aws, azure, power bi, qlik sense, hadoop, spark, mysql, postgress, agile, scrum, git, itil, ogc toolkit, public sector best practice guidance, uk security clearance","agile, aws, azure, data science, dataanalytics, git, hadoop, itil, mysql, ogc toolkit, postgress, powerbi, public sector best practice guidance, python, qlik sense, r, scrum, spark, uk security clearance"
T-SQL Data Engineer,Hydrogen Group,"Preston, England, United Kingdom",https://uk.linkedin.com/jobs/view/t-sql-data-engineer-at-hydrogen-group-3774844000,2023-12-17,Preston, United Kingdom,Mid senior,Onsite,"Job title:
T-SQL Data Engineer
Location:
Preston or Frimley - Hybrid 2/3 day weekly split.
Salary:
Up to £51K + benefits
What You'll Be Doing
Develop, implement and optimise complex T-SQL scripts independently to support data ingestion processes and reporting
Monitoring and do proactive validations and verifications of ETL development and monitoring of data loads into the data warehouse
Maintaining existing ETL solutions and identifying improvements
Analyse existing SQL queries for performance improvements
Conducting peer code reviews and UAT validation and create well defined test cases
Adhering to company source control, validation and promotion policy
Working within change control environment
Writing and Review technical documentation
Essential
Your skills and experiences:
Previous experience in a similar role Excellent in T-SQL programming and experience using SQL Server Management Studio and Microsoft BI stack (SSIS/SSRS)
Excellent understanding of relational databases and normalisation
Solid experience of working with Visual Studio to develop, debug and publish SSIS packages and reports and to proactive provide training to juniors and peers
Desirable
Knowledge of PowerShell scripting
Appreciation of data warehousing architecture techniques, best practised and able to deploy packages for production to be at industrial strength
Show more
Show less","TSQL, SQL Server Management Studio, Microsoft BI stack (SSIS/SSRS), Relational databases, Normalization, Visual Studio, PowerShell, Data warehousing architecture techniques, SSIS packages, Reports","tsql, sql server management studio, microsoft bi stack ssisssrs, relational databases, normalization, visual studio, powershell, data warehousing architecture techniques, ssis packages, reports","data warehousing architecture techniques, microsoft bi stack ssisssrs, normalization, powershell, relational databases, reports, sql server management studio, ssis packages, tsql, visual studio"
Lead Data Engineer,Robert Walters,"Preston, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-robert-walters-3775810136,2023-12-17,Preston, United Kingdom,Mid senior,Onsite,"A leading global business are looking for a Lead Data Engineer to join their team. You will be working on a number of exciting projects and will have the ability to upskill and progress your career. An outline of the responsibilities and requirements are below: As a Lead Data Engineer, you will play a crucial role in developing, implementing, and optimizing complex T-SQL scripts. Your responsibilities will include: * Monitoring and performing proactive validations and verifications of ETL development. * Ensuring efficient data loads into the data warehouse. * Maintenance of existing ETL solutions and identification of opportunities for improvement. * Analysis of existing SQL queries for performance enhancements. * Conducting peer code reviews and User Acceptance Testing (UAT) validation. * Adhering to the company's source control, validation, and promotion policies. * Working within a change control environment. * Writing and reviewing technical documentation. Essential skills: * Previous experience in a similar role. * Excellent proficiency in T-SQL programming. * Experience using SQL Server Management Studio and Microsoft BI stack (SSIS/SSRS). * Strong understanding of relational databases. * Proactive attitude towards providing training to juniors and peers The business are looking for the Lead Data Engineer to be in their Preston office 2/3 times per week If you are a Data Engineer looking to take the next step in your career and work for a global leader, I'd love to hear from you!
A leading global business are looking for a Lead Data Engineer to join their team.
Responsibilities
You will be working on a number of exciting projects and will have the ability to upskill and progress your career. An outline of the responsibilities and requirements are below:
As a Lead Data Engineer, you will play a crucial role in developing, implementing, and optimizing complex T-SQL scripts. Your responsibilities will include:
Monitoring and performing proactive validations and verifications of ETL development.
Ensuring efficient data loads into the data warehouse.
Maintenance of existing ETL solutions and identification of opportunities for improvement.
Analysis of existing SQL queries for performance enhancements.
Conducting peer code reviews and User Acceptance Testing (UAT) validation.
Adhering to the company's source control, validation, and promotion policies.
Working within a change control environment.
Writing and reviewing technical documentation.
Essential Skills
Previous experience in a similar role.
Excellent proficiency in T-SQL programming.
Experience using SQL Server Management Studio and Microsoft BI stack (SSIS/SSRS).
Strong understanding of relational databases.
Proactive attitude towards providing training to juniors and peers
The business are looking for the Lead Data Engineer to be in their Preston office 2/3 times per week
If you are a Data Engineer looking to take the next step in your career and work for a global leader, I'd love to hear from you!
Robert Walters Operations Limited is an employment business and employment agency and welcomes applications from all candidates
Show more
Show less","TSQL, SQL Server Management Studio, ETL, Data Warehouse, Relational Databases, SQL, SSIS, SSRS, Data Engineering, Technical Documentation, Source Control, Change Control","tsql, sql server management studio, etl, data warehouse, relational databases, sql, ssis, ssrs, data engineering, technical documentation, source control, change control","change control, data engineering, datawarehouse, etl, relational databases, source control, sql, sql server management studio, ssis, ssrs, technical documentation, tsql"
Senior Software Data Engineer,Expleo Group,"Preston, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-software-data-engineer-at-expleo-group-3711866216,2023-12-17,Preston, United Kingdom,Mid senior,Onsite,"Expleo is a global company and trusted partner for end-to-end, integrated engineering, quality services and management consulting for digital transformation. We help businesses harness unrelenting technological change to successfully deliver innovations that will help them gain a competitive advantage and improve the everyday lives of people around the globe.
We are looking for a highly skilled senior developer with a data focus to work as an expert in a team of consultants for client projects. This is an exciting position for someone looking to use a wide range of skills, and be involved in the technical development and direction of our clients’ projects. You will be expected to understand and extend the technical architecture, work in an agile manner and lead by example using test driven development (TDD). This is an excellent opportunity to stretch yourself demonstrating leadership abilities in the delivery of client projects.
Your specific technical stack experience is less important; enthusiasm, initiative, and the ability to switch between different technologies is the essential fit here; we pride ourselves on our start-up style culture, with great flexibility and countless opportunities to grow. Thus, even if you don’t have experience in all the areas we work in, you will be provided on-the-job training and coaching to make sure that we succeed.
As part of the Expleo Digital and Emerging Technology (DET) team, you will report to the Head of Software Development (Engineering) and support recruitment and development of sector strategy.
Responsibilities
Be a senior member of a team of consultants on a diverse range of projects for our clients
Be the expert and go to person for data analysis and data-led projects
Mentor and encourage a culture of agile development in your teammates and contribute to an iterative approach to delivery
Promote software quality thorough testing
Communicate effectively and clearly with stakeholders, your team, and the wider business
Be part of our success through developing trust with our clients
Work closely with the technical lead to progress opportunities with clients
Explore ways to enhance data quality and reliability
Develop analytical tools and programs
Collaborate with data scientists and architects on several projects
Qualifications
Degree in Computer Science, IT, or similar field; a Master’s is a plus
Essential Skills
Experienced developer with expertise in more than one language including python
Experience with data warehousing, ETLs and data management (Apache Kafka, Snowflake, AWS Kinesis)
Experience in data analysis, machine learning or artificial intelligence
Worked with agility and the associated tools (JIRA, Confluence etc.)
Good cloud knowledge and experience, especially with data ingestion and processing
Knowledge of software quality techniques such as TDD, Red-Green-Refactor, SOLID
Experience in working as a member of a team of engineers
Keen learner, with a great attitude to work and team environment
Desired Skills
Experience of being in a consultancy-led environment
Be innovative, and keen to bring own knowledge to the table
Experience of CI/CD tools and pipelines
Experience
At least 5 years of software development experience including 2 years of data engineering experience
Benefits
Collaborative working environment – we stand shoulder to shoulder with our clients and ours peers through good times and challenges.
We empower all passionate technology loving professionals by allowing them to expand their skills and take part in inspiring projects.
Excellent opportunity for career progression.
Expleo Academy - enables you to acquire and develop the right skills by delivering a suite of accredited training courses.
Competitive company benefits such Flexible Compensation Plan.
Flexible working hours and remote working possible.
Always working as one team, our people are not afraid to think big and challenge the status quo
We are an equal opportunities employer and welcome applications from all suitably qualified persons regardless of their race, sex, disability, religion/belief, sexual orientation or age.
Show more
Show less","Python, Data warehousing, ETLs, Data management, Apache Kafka, Snowflake, AWS Kinesis, Data analysis, Machine learning, Artificial intelligence, Agile development, JIRA, Confluence, Cloud knowledge, Data ingestion, Data processing, Software quality techniques, TDD, RedGreenRefactor, SOLID, CI/CD tools, Pipelines","python, data warehousing, etls, data management, apache kafka, snowflake, aws kinesis, data analysis, machine learning, artificial intelligence, agile development, jira, confluence, cloud knowledge, data ingestion, data processing, software quality techniques, tdd, redgreenrefactor, solid, cicd tools, pipelines","agile development, apache kafka, artificial intelligence, aws kinesis, cicd tools, cloud knowledge, confluence, data ingestion, data management, data processing, dataanalytics, datawarehouse, etls, jira, machine learning, pipelines, python, redgreenrefactor, snowflake, software quality techniques, solid, tdd"
Customer Service Representative/Data Analyst/Data Entry Clerk,Dollargeneral,"Wigan, England, United Kingdom",https://uk.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-dollargeneral-3741441467,2023-12-17,Preston, United Kingdom,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : application@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Advanced Statistical Techniques, SQL, R, Python, Tableau, Power BI, Data Visualization, Data Mining, Data Modeling, Data Quality Management, Data Integrity, Data Manipulation, Data Cleansing, Data Analysis, DataDriven Decision Making, Business Intelligence, Reporting, A/B Testing, Statistical Modeling, Hypothesis Testing, Data Management, ETL Processes","advanced statistical techniques, sql, r, python, tableau, power bi, data visualization, data mining, data modeling, data quality management, data integrity, data manipulation, data cleansing, data analysis, datadriven decision making, business intelligence, reporting, ab testing, statistical modeling, hypothesis testing, data management, etl processes","ab testing, advanced statistical techniques, business intelligence, data integrity, data management, data manipulation, data mining, data quality management, dataanalytics, datacleaning, datadriven decision making, datamodeling, etl, hypothesis testing, powerbi, python, r, reporting, sql, statistical modeling, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk/Urgent,Townepaucekltd,"Southport, England, United Kingdom",https://uk.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-urgent-at-townepaucekltd-3742326130,2023-12-17,Preston, United Kingdom,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk/Urgent will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : application@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data Analysis, SQL, R, Python, A/B Testing, Data Visualization, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, Data Management, ETL","data analysis, sql, r, python, ab testing, data visualization, tableau, power bi, statistical modeling, hypothesis testing, data management, etl","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Senior Data Engineer,Current Health,United Kingdom,https://uk.linkedin.com/jobs/view/senior-data-engineer-at-current-health-3770400063,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"Who is Current Health?
Current Health is a leader in the movement to transition more healthcare into the home. We’re a technology company, but beyond our best-in-class hardware and software capabilities, we help healthcare organisations operationalise care at home and deliver excellent patient experiences through helping with the last mile of care logistics and services. We partner with leading healthcare organisations to advance home-based care in the US, the UK, and around the world.
More than doubling in size in the last year, we’re building a global team united in one purpose: helping people live healthier, longer lives by predicting disease and enabling earlier treatment. At Current Health, we seek to put patients first, solve problems with curiosity and creativity, pursue uncommon collaboration, and never stop growing. We’re a remote-first company, which means that everything from how we work, communicate, and socialise is shaped by tools and culture that prioritise collaboration and support independence.
What does a Data Engineer at Current Health do?
We are in the process of revamping our patient admissions/enrollment capabilities - we’d like to introduce support for net new fields captured within that process, as well as define the most appropriate locations for that data to be stored so that it can be used within different workflows of the future. As a specialist in data engineering, you will help us throughout the process by assisting software engineers and drive the project from a data perspective.
As well as helping support our services across our software platform and applications, you will also help support and evolve our existing data products and provide analytical support to external clients and Current Health teams.
You will also be working closely with the data team who are building a new data platform whereby we will make all our data accessible in one place and build data products that use this data thus allowing our engineers, data scientists, bio-medical engineers, and clinical researchers to perform analytics which will help us to drive patient engagement, clinical impact, and client growth.
How We Work As a Team
We would describe ourselves as self-organising and collaborative with a continuous improvement mindset. We value knowledge sharing and making decisions as a team, regardless of role or level of seniority. We encourage collaboration via e.g.pair programming and holding regular knowledge share sessions. We seek to continuously improve as a team, and we also look to support each other's growth.
Requirements
We want to hear from you if…
You meet some or many of the points below. This is some of the experience we would be looking for to help you be successful in our team:
Experience in real-time stream or batch processing
Designing and implementing systems with scale in mind
Deep knowledge of at least one modern programming language (we use Python and Java)
Proficiency in data manipulation using e.g. SQL, Spark, pandas or similar
An eagerness to learn new languages and technologies as required
Writing code with a focus on testability
Be confident reviewing and troubleshooting code
Have implemented solutions in a cloud computing and networking environment (we use AWS along with terraform/IaC)
Collaborating outside your own team. We work closely with data engineers, product teams and other consumers of data products
Technologies we use
Querying and analytics: Python, SQL, dbt, Databricks, AWS Athena, AWS Glue
Orchestration: Airflow
Storage: PostgreSQL(RDS), Redshift, DynamoDB, AWS S3
Monitoring: DataDog, CloudWatch
Benefits
Salary Exchange Pension scheme (5% employee, 3% employer contribution)
Private Medical Insurance through Vitality
Competitive bonus scheme
2 x Life Assurance cover
Critical Illness cover
Employee Assistance Program
£10 pcm flex pot to use toward benefits in our Benni benefits portal
On call allowance (Only payable if and for so long as you provide on call services)
Flexible, autonomous working environment
Bike to work scheme
Give as you earn through payroll
Show more
Show less","Python, Java, SQL, Spark, Pandas, dbt, Databricks, AWS Athena, AWS Glue, Airflow, PostgreSQL, Redshift, DynamoDB, AWS S3, DataDog, CloudWatch","python, java, sql, spark, pandas, dbt, databricks, aws athena, aws glue, airflow, postgresql, redshift, dynamodb, aws s3, datadog, cloudwatch","airflow, aws athena, aws glue, aws s3, cloudwatch, databricks, datadog, dbt, dynamodb, java, pandas, postgresql, python, redshift, spark, sql"
Senior Data Engineer,CloudQuery,United Kingdom,https://uk.linkedin.com/jobs/view/senior-data-engineer-at-cloudquery-3770803440,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"As a Senior Data Engineer at CloudQuery, you will play a pivotal role in working on CloudQuery transformations powered by dbt and cloudquery plugins. The ideal candidate will have significant experience in data analysis, a strong analytical mindset, and a proven track record of delivering high-quality results.
Responsibilities
Work on security, compliance and cost queries and transformations by utilizing CloudQuery AWS, GCP, and Azure source plugins
Gain understanding of the underlying cloud products, use cases in addition to the raw data
Create cross database queries and transformations for PostgreSQL, BigQuery and snowflake
Collaborate with cross-functional teams to understand business requirements and provide analytical support
Requirements
Bachelor's or advanced degree in a relevant field (e.g., Statistics, Mathematics, Computer Science)
Extensive experience as a Data Engineer or a relevant position
Significant experience with Python
Expertise in data manipulation, analysis, and visualization tools (e.g., SQL, Python, Snowflake, BigQuery, DBT)
Strong statistical analysis skills and the ability to draw meaningful insights from data
Excellent communication skills with the ability to translate complex data findings into actionable insights
Team player and experience in mentoring and guiding junior analysts
Familiarity with ETL or ELT products is a plus
Benefits
Remote-first company!
Competitive pay with significant options pack upside
Remote-friendly environment and culture that nurtures company and team events to stay connected
Lots of responsibilities with tons of opportunities for advancement!
About Us
CloudQuery is a high-performance data integration framework built for developers. Our open-source project is widely used by SREs, DevOps & Security engineers around the world. Come work with us to build the future of cloud-infrastructure for developers.
Show more
Show less","Data analysis, Data manipulation, Data visualization, Python, Snowflake, BigQuery, DBT, Statistics, SQL, ETL, ELT, CloudQuery","data analysis, data manipulation, data visualization, python, snowflake, bigquery, dbt, statistics, sql, etl, elt, cloudquery","bigquery, cloudquery, data manipulation, dataanalytics, dbt, elt, etl, python, snowflake, sql, statistics, visualization"
Senior Data and Backend Engineer,Urban,United Kingdom,https://uk.linkedin.com/jobs/view/senior-data-and-backend-engineer-at-urban-3782501667,2023-12-17,Preston, United Kingdom,Mid senior,Remote,"The Role
At Urban, data driven decision making is key to the operation of our marketplace business, enabling us to effectively match customer demand and practitioner supply. This role will have ownership of our data infrastructure, providing hands-on maintenance of the data pipeline to ensure it addresses the needs of our evolving business. This role will sit within the Engineering team and collaborate with cross-functional teams, including a data analyst, to ensure the efficient and up to date flow of data throughout the company. Being champion in the data domain, this role would also actively contribute to initiatives requiring backend development, collaborating with other senior backend engineers.
As a product team, we are highly-skilled multi-disciplined engineers, passionate about quality of our product, about the experience all of our users will have using our product. We work closely with each other across design, engineering, and testing. We embrace strong development standards, continuous integration, peer review, test automation, and replicable deployment processes.
Being champion in the data domain, this role would also actively contribute to deliver business requirements across a few dozens of backend microservices, along with Senior Backend Engineers. To evolve our architecture and to keep it up to date with the latest technologies. Will be applying best engineering practices and continue our journey towards engineering excellence.
You will be working as part of small international engineering team distributed across multiple countries and time zones. You will get access to different cultures and opinions and will be releasing changes to the production environment in small chunks, daily.
Requirements
As the owner of our data infrastructure, actively develop and maintain the Airflow Data Pipeline and Data Warehouse on Google Cloud Platform, including any bug fixes or adapting the pipeline to upstream changes
Co-own and maintain our dataflow upstream (Segment, Zapier, etc) and our behavioural tracking (Segment, Google Analytics and other analytical products). Behavioural data includes events, sessions and attributions on our platforms ranging from iOS/Android Apps to Website
Ensure the quality of data in our data warehouse
Maiasting impact on the analytics and architecture landscape.
Apply now and be a driving force in defining what the future of data looks like!
Show more
Show less","Data Engineering, Python, SQL, SSIS/SSAS, Azure, Azure Synapse, Data Warehouse, Analytics, Machine Learning, Data Pipeline, Development, Data Architecture, Project Management, Change Management","data engineering, python, sql, ssisssas, azure, azure synapse, data warehouse, analytics, machine learning, data pipeline, development, data architecture, project management, change management","analytics, azure, azure synapse, change management, data architecture, data engineering, data pipeline, datawarehouse, development, machine learning, project management, python, sql, ssisssas"
Field Data Engineer,Ascend.io,"London Area, United Kingdom",https://uk.linkedin.com/jobs/view/field-data-engineer-at-ascend-io-3783648367,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Ascend's mission is to enable every organization to innovate faster by harnessing the power of automation. We help customers spend more time bringing data innovations to life by simplifying and accelerating data orchestration.
As a Field Data Engineer at Ascend you will help our customers accelerate their data projects, focused primarily on the initial technical implementation of Ascend. While you will have direct responsibility for building parts of the overall solution, you will also focus intently on up-leveling the customer in their ability to design and build data systems themselves. When you are not working directly with a customer, you will be part of the broader field engineering team working on customer POC's, staying current on leading edge technologies, helping other customers, and creating content. If you love a challenge, working with insanely bright and motivated people, and learning more faster than you thought possible, join Ascend, recently named a Gartner Cool Vendor!
What you'll do:
Support our customers in region
With the help of a Solution Architect, you will design cutting-edge data solutions specific to customer requirements.
Implement aspects of the solutions while providing support and guidance to customer developers on the team.
Help our customer developers become raving fans of Ascend.
Provide feedback to the product team on new features that will drive adoption
Assist/drive POC's with new potential customers.
Research and implement technologies that are adjacent to and complementary to Ascend.
Generate content that will accelerate customer learnings and promote self-sufficiency.
Build additional features / extensions for the Ascend platform.
What we look for:
Deep desire to learn/improve and be part of a winning team
3+ years of Software or Data Engineering experience.
Experienced with at least one cloud (AWS, GCP, Azure)
Hands-on with at least 1 scripting language (such as Python, Ruby, Javascript)
Education
: Bachelor degree in Computer Science or Math.
Skills: Must have a software engineering/developer background.
What we offer:
Medical/Dental/Vision
Long-term & Short-term disability
Life Insurance
Stock options
401(k)
Flexible PTO
Remote Friendly
Don’t meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Ascend we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Ascend.io is an equal opportunity employer. We are committed to treating all applicants fairly based on their abilities, achievements, and experience without regard to race, national origin, sex, age, disability, veteran status, sexual orientation, gender identity, or any other classification protected by law.
Show more
Show less","Data Engineering, Software Engineering, AWS, Azure, GCP, Python, Ruby, Javascript, Computer Science, Math","data engineering, software engineering, aws, azure, gcp, python, ruby, javascript, computer science, math","aws, azure, computer science, data engineering, gcp, javascript, math, python, ruby, software engineering"
Senior Data Engineer - GCP,CUBE,"London Area, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-gcp-at-cube-3754984442,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Role:
Senior Data Engineer
Location:
United Kingdom
Recently listed as a ""RegTech Top Performer"" in Market Fintech's RegTech Supplier Performance Report, CUBE is pioneering the development of machine automated compliance.
We are a global RegTech business defining and implementing the gold standard of regulatory intelligence and change for the financial services industry. We deliver our services through a SaaS platform, powered by an innovative combination of AI and proprietary data ontology, to simplify the complex and everchanging world of compliance for our clients.
CUBE is creating the future and are a company rooted in strong values, team spirit and commitment to our customers and wider communities. We serve some of the largest financial institutions globally and are expanding our footprint very fast. As we do so, we are keen for new talent to join us and realize their full potential to grow into leadership positions within the business.
Our Products:
RegPlatform
is an Enterprise grade technology platform that streamlines regulatory change management. It provides firms with a one-stop, continuously maintained inventory of global regulations, with effortless horizon scanning, integration capabilities and workflow management. RegPlatform combines industry leading AI technology with expert validated insights to simplify the complexities of multi-jurisdictional regulatory content.
RegAssure
caters for nimble, lean financial organizations that want assured, seamless compliance without the burden of set-up costs and implementation processes. RegAssure provides fast, automated regulatory intelligence that intuitively knows your business needs and can grow with you.
Role mission:
Establish a strong data architecture in GCP for the backend of AI/ML/DS Services supporting a SaaS product.
Build cost-efficient, low latency data pipelines into and within GCP.
Represent the interests of the RegBrain AI team in the establishment of a global data strategy at CUBE.
Assist ML engineering team to build SOTA applications on structured and unstructured data, at scale, by providing fitted data solutions.
Ensure seamless integration of RegBrain’s product features to analytics via CUBE’s data lake.
Continuously educate the ML team regarding best data practices.
Ensure reliability, completeness and quality of datasets replicated from internal sources.
Own the data governance needs of RegBrain, educate the ML Engineers on the subject.
Improve and manage ETL/ELT pipelines to the graph database(s) supporting CUBE's Knowledge Graph.
Responsibilities:
Create, orchestrate and monitor strong ETL pipelines for core datasets in the business from on-prem to GCP.
Build monitoring solutions for data quality within GCP, with reporting to management stakeholders.
Support creation, orchestration and monitoring of data pipelines to graph database.
Support the establishment of a data catalogue.
Coordinate with the Infrastructure team to ensure seamless creation of needed solutions.
Manage data lifecycle and operational costs.
What we’re looking for:
Cloud champion. You must have extensive experience in
GCP
specifically. None of
the secrets of BigQuery, DataFlow, Pub/Sub, Composer etc... elude you. You can quickly decide which solution is suited to a given use case.
You are at ease with all the tools in GCP: CloudSQLBigQuery,DataFlow/Fusion/Proc, Cloud Composer.
A specialist in their domain. You must be able to identify CUBE/RegBrain's needs, communicate planned solutions efficiently and carry them out.
You must be able to deal with:
-> Data transport
-> NoSql
-> Caching
-> Message Buses
->Data cleansing, esp. for textual data.
You not only merely think about data as bytes to be moved around, but as valuable information about the real world.
You are proactive. We will be relying on you to set the standards and raise them. Your work should enable others in the team.
An appetite for leadership. When the team grows from its success, you must be eager to assist its composition and lead more junior data engineers.
A can-do attitude. As a mid-size company, pragmatism is key. We need to enable short iteration cycles within the ML team while slowly establishing strong data foundations for its future growth.
You like working remotely as much as in-person.
Why Us?
🌍
Globally, we are one of a kind!
CUBE is a well-established player in Regtech (we were around before Regtech was even a thing!), and our category-defining product is used by leading financial institutions around the world (including Revolut, Citi, and HSBC). We cover over 5,000 issuing bodies in 712 jurisdictions across 180 countries. Substantially more than our closest competitors.
🗽
Freedom, flexibility & progression.
We are the market leader within Regtech, and yet we are constantly evolving and you will have a significant influence over how we develop moving forwards. We will help you realise your full potential and grow into leadership positions across CUBE.
🗣️
Internationally collaborative culture
CUBE has more than 300 CUBERs across 11 locations in Europe, the Americas and APAC and we are united by our strong team-oriented culture. You will have the opportunity to work with likeminded colleagues from all over the world, gaining exposure to different cultures and business practices.
🌱
Innovative & meaningful work
We are always exploring new technologies and innovations that can help clients solve complex problems. If you’re someone seeking the opportunity to work with cutting edge technology and work on challenging projects that have a real impact then we want to hear from you!
💻
Work life balance
CUBE is a remote first business, you'll be able to design your home office and choose your own work equipment. Unable to work from home one week, or desperate for in-person interaction with colleagues? No problem—book a room in a coworking space or join one of our department monthly team meets (they’re happening all the time!).
🤝
Diversity, Equity and Inclusion:
We believe in equal opportunities and encourage applicants to apply to our open roles regardless of gender, marital status, race, nationality, ethnicity, religion, neurodiversity, sexual orientation and/or age and are committed to providing a working environment where everyone is supported to be their authentic best selves.
At CUBE we do our best to implement inclusive hiring processes to build a culture where we value diversity. We are a values driven company who care about growth and learning, therefore recognise we still have long way to go from where we want to be in regards to DE&I.
As part of the application, we ask for personal data for our internal diversity and inclusion bench-marking, so that we can tailor our DE&I strategies and identify opportunities to improve as a business.
Show more
Show less","BigQuery, DataFlow, Pub/Sub, Composer, Data Lake, Hadoop, NoSQL, CloudSQLBigQuery, DataFusion, Cloud Composer, Data Catalog, Caching, Message Buses, Data Governance, Data Integration, Data Quality, Data pipelines, Machine Learning, Data Analytics, Data Architecture, Data Engineering, ETL, ELT, Artificial Intelligence, Cloud Computing, Data Science, Programming, Software Development, GCP, SaaS, RegTech","bigquery, dataflow, pubsub, composer, data lake, hadoop, nosql, cloudsqlbigquery, datafusion, cloud composer, data catalog, caching, message buses, data governance, data integration, data quality, data pipelines, machine learning, data analytics, data architecture, data engineering, etl, elt, artificial intelligence, cloud computing, data science, programming, software development, gcp, saas, regtech","artificial intelligence, bigquery, caching, cloud composer, cloud computing, cloudsqlbigquery, composer, data architecture, data catalog, data engineering, data governance, data integration, data lake, data quality, data science, dataanalytics, dataflow, datafusion, datapipeline, elt, etl, gcp, hadoop, machine learning, message buses, nosql, programming, pubsub, regtech, saas, software development"
Lead Data Engineer // £75k plus package // Remote working // Financial Services,Mentmore,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-%C2%A375k-plus-package-remote-working-financial-services-at-mentmore-3765284265,2023-12-17,Slough, United Kingdom,Mid senior,Remote,"Job title
– Lead Data Engineer
Salary
- £75k plus package
Working Model
– Fully remote with minimal requirements to attend the office.
Industry
– Financial Services
A mid-size financial services company looking to onboard a highly skilled Lead Data Engineer to help mature the current data engineering function. My client is looking for someone who is highly proficient in Redshift & can help develop junior members of the team.
Key responsibilities:
Technical leadership of the data engineering department.
Work closely with the CTO and head of Software Product to define and implement the data strategies.
Strong communication skills.
Make recommendations for the MI reports, processes, and standards.
Mentor junior members of the team.
Proficient in AWS Redshift.
Experience working with Python scripting, Glue & QuickSIght.
Proficient in data modeling.
Experience developing ETL & ETL processes from multiple data sources.
Exposure to working in Agile environments.
If you require visa sponsorship please could I ask you not to apply as my client is currently unable to provide sponsorship, thank you.
Show more
Show less","Data Engineering, Data Leadership, Redshift, Python Scripting, Glue, Quicksight, Data Modeling, ETL Processes, Agile","data engineering, data leadership, redshift, python scripting, glue, quicksight, data modeling, etl processes, agile","agile, data engineering, data leadership, datamodeling, etl, glue, python scripting, quicksight, redshift"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Newyorkuniversity,"Parksville, British Columbia, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-newyorkuniversity-3757207271,2023-12-17,Nanaimo, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, SQL, R, Python, DataDriven DecisionMaking, Data Modeling, Algorithms, A/B Testing, Data Quality, Data Manipulation, Data Visualization, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL Processes","data analysis, statistical techniques, sql, r, python, datadriven decisionmaking, data modeling, algorithms, ab testing, data quality, data manipulation, data visualization, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, algorithms, data manipulation, data quality, dataanalytics, datadriven decisionmaking, datamodeling, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Data Analyst,Infinity-Outsourcing,"McLean, VA",https://www.linkedin.com/jobs/view/data-analyst-at-infinity-outsourcing-3741171745,2023-12-17,Sterling,United States,Associate,Onsite,"Role: Data Analyst
Location: McLean, VA
Pay rate /salary to candidate : $47-$50/Hr or $95,000-$100,000 /Annum
Client: Freddie Mac
experience: +10
Data Analyst
Analyze the current source of data, for key data elements, understand the current usage and identify the new/alternative source for these data elements
Experience in Sybase and DB2 is needed and knowledge of ECDA is desired.
Identify customer needs and intended use of requested data in the development of database requirements and support the planning and engineering of databases.
Maintain comprehensive knowledge of database technologies, complex coding languages, and computer system skills.
Collaborate with developers and project managers to understand integration requirements.
Assist in designing and implementing API Integrations between different software systems in Modeling applications.
Write and maintain API Documentation for internal and external stakeholders.
Troubleshoot and debug integration issues, identifying and resolving technical obstacles.
Stay up to date with industry trends and best practices in API Integration.
Work with people with different functional expertise respectfully and cooperatively to work toward a common goal
Skilled in documentation and database reporting for the purposes of analysis, data discovery, and decision-making with the use of relevant software such as Crystal Reports, Excel, or SSRS
Experience in the process of analyzing data to identify trends or relationships to inform conclusions about the data
Show more
Show less","Data Analysis, Sybase, DB2, ECDA, API Integration, Software Modeling, API Documentation, Troubleshooting, Industry Trends, Crystal Reports, Excel, SSRS, Data Mining, Data Trends","data analysis, sybase, db2, ecda, api integration, software modeling, api documentation, troubleshooting, industry trends, crystal reports, excel, ssrs, data mining, data trends","api documentation, api integration, crystal reports, data mining, data trends, dataanalytics, db2, ecda, excel, industry trends, software modeling, ssrs, sybase, troubleshooting"
"Data Engineer, Sr.",Atlas Research,"Washington, DC",https://www.linkedin.com/jobs/view/data-engineer-sr-at-atlas-research-3111073000,2023-12-17,Sterling,United States,Associate,Onsite,"<span style=""font-family:""Arial"",sans-serif"">Data Engineer, Senior
<span style=""font-family:""Arial"",sans-serif"">
Atlas Research is seeking qualified candidates for a Data Analyst position to join an emerging technology and innovation practice.  Primary objectives involve participating with a team responsible for data analysis and analytics utilizing database management software/tools against various database platforms. In this role you will assist in the development of automated queries and data extraction across multiple computing environments. You will develop technical expertise in data mining techniques and tools to ensure deliverables meet the needs of both internal and external business customers.  This position serves as an excellent starting point for becoming a Data Scientist.
<span style=""font-family:""Arial"",sans-serif"">
Essential Duties and Responsibilities
<span style=""font-family:""Arial"",sans-serif"">
Joins and combines various data sources, systems, databases, reports, to come up with a single data output.
<span style=""font-family:""Arial"",sans-serif"">
Convert business requirements into specifications, jobs, process, scripts, programs, and queries;  Writes basic to intermediate queries (like CRUD statements, joins, stored procedure, functions) and views to extract required data
<span style=""font-family:""Arial"",sans-serif"">
Enhances and optimizes advanced query, views and procedures to improve efficiency and reliability of data manipulation (i.e. SSIS, VBA, Excel Macro, VB, .Net, SQL).
<span style=""font-family:""Arial"",sans-serif"">
Design and develop, build and modify, and maintain complex reports using data visualization tools (i.e. MSSQL, SSRS, Power BI, Tableu, Excel)
<span style=""font-family:""Arial"",sans-serif"">
Provide management with status reports of current ETL (extract, transform, load) processes
<span style=""font-family:""Arial"",sans-serif"">
Create visually rich presentations using various forms, charts, and other graphical elements into dashboards
<span style=""font-family:""Arial"",sans-serif"">
Produce clear, concise, relevant, and accurate documentation as needed
<span style=""font-family:""Arial"",sans-serif"">
Import and export data in different formats and various sources for data manipulation, wrangling, modeling, migration, and reporting
<span style=""font-family:""Arial"",sans-serif"">
Key Requirements:
<span style=""font-family:""Arial"",sans-serif"">
Bachelor's degree in Information Technology, Computer Science, or related Field
<span style=""font-family:""Arial"",sans-serif"">
Must have 5+ years of relevant data analysis experience
<span style=""font-family:""Arial"",sans-serif"">
Must be proficient MSSQL stored procedure creation, job creation, and general report automation
<span style=""font-family:""Arial"",sans-serif"">
Familiarity with advanced MSSQL features (linked servers, dB mail, bulk copy program (bcp) utility) to use in automating data delivery, file creation, notifications
<span style=""font-family:""Arial"",sans-serif"">
Familiarity with Open Database Connectivity (ODBC) creation/usage
<span style=""font-family:""Arial"",sans-serif"">
Familiarity with Microsoft Access table creation, queries, macros, forms and reports, Visual Basic for Applications (VBA)
<span style=""font-family:""Arial"",sans-serif"">
Familiarity with MSSQL (2000/2005/2008) and Oracle (10G/11G) databases
<span style=""font-family:""Arial"",sans-serif"">
Must possess strong organizational skills with Must be able to work under tight deadlines efficiently demonstrated attention to detail
<span style=""font-family:""Arial"",sans-serif"">
Must flexible and able to adapt in a changing business environment
<span style=""font-family:""Arial"",sans-serif"">
Must possess strong written and verbal communication
<span style=""font-family:""Arial"",sans-serif"">
Must be resourceful, industrious, and willing to take on new tasks and proactively learn new technologies to keep up with business needs for example developing a proficiency in R or Python
<span style=""font-family:""Arial"",sans-serif"">
Knowledge or familiarity to VA data management systems to include but not limited to Corporate Data Warehouse (CDW), Veterans Health Administration Support Services (VSSC), Computerized Patient Record System (CPRS) is a plus
<span style=""font-family:""Arial"",sans-serif"">
Special skills/must be proficient in the following or willing to improve proficiency in these areas:
<span style=""font-family:""Arial"",sans-serif"">
Structured Query Language (SQL)
<span style=""font-family:""Arial"",sans-serif"">
Cursory knowledge of programming and scripting languages (.Net C#, C++, VBS, etc.)
<span style=""font-family:""Arial"",sans-serif"">
Microsoft Office 2007/2010 (Excel, Access, Word, PowerPoint)
<span style=""font-family:""Arial"",sans-serif"">
Database structure and design
<span style=""font-family:""Arial"",sans-serif"">
Excellent written and oral communication skills
Show more
Show less","Data analysis, Data mining, Data visualization, SQL, MSSQL, SSRS, Power BI, Tableu, Data wrangling, Data modeling, Data migration, Data reporting, Data management, Data Scientist, Data extraction, Data integration, Data transformation, Data cleansing, Data loading, Data warehousing, Data mining techniques, Data manipulation, Data delivery, Data automation, Data migration, Data modeling, Data reporting, Data visualization, Relational databases, NoSQL databases, Big data technologies, Cloud computing, Machine learning, Artificial intelligence, Data governance, Data security, Data privacy","data analysis, data mining, data visualization, sql, mssql, ssrs, power bi, tableu, data wrangling, data modeling, data migration, data reporting, data management, data scientist, data extraction, data integration, data transformation, data cleansing, data loading, data warehousing, data mining techniques, data manipulation, data delivery, data automation, data migration, data modeling, data reporting, data visualization, relational databases, nosql databases, big data technologies, cloud computing, machine learning, artificial intelligence, data governance, data security, data privacy","artificial intelligence, big data technologies, cloud computing, data automation, data delivery, data extraction, data governance, data integration, data loading, data management, data manipulation, data migration, data mining, data mining techniques, data privacy, data reporting, data scientist, data security, data transformation, data wrangling, dataanalytics, datacleaning, datamodeling, datawarehouse, machine learning, mssql, nosql databases, powerbi, relational databases, sql, ssrs, tableu, visualization"
Senior Data Analyst,Accroid Inc,"Reston, VA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-accroid-inc-3768300248,2023-12-17,Sterling,United States,Associate,Onsite,"12+ Months
VA
Technical skills: SQL (required), Tableau (required), Python (nice to have), some AWS exposure desired (Redshift, s3 Bucket)
Personality: ambitious, self-starter, proactive, great communication skills, unafraid to ask questions/curious
Show more
Show less","SQL, Tableau, Python, AWS, Redshift, S3 Bucket","sql, tableau, python, aws, redshift, s3 bucket","aws, python, redshift, s3 bucket, sql, tableau"
Sr. Data Analyst,eStaffing Inc.,"Washington, DC",https://www.linkedin.com/jobs/view/sr-data-analyst-at-estaffing-inc-3740219360,2023-12-17,Sterling,United States,Associate,Onsite,"Job Title -
Sr. Data Analyst:
Washington, DC
Location-
Washington, DC
Job Type- Full Time
Shift- Day
Job Requirements-
An Active DoD Issued Secret Clearance at the time of application.
Bachelor’s Degree in an applied mathematical discipline such as physics, engineering, computer science, or similar, and 5 to 7 years of relevant experience is required.
Experience supporting senior leaders and collaborating across large enterprises.
Excellent communication (i.e., oral, writing, presentation) skills for briefing Flag/SES leaders; specifically, presentation skills.
Experience using at least one statistical analytic package (Minitab, SPSS, etc.) and SQL
Experience with data visualization using Tableau.
NAVSEA or Navy experience, either active duty officer or as a contractor Project Manager and/or Ship Building Specialist.
Apply for full job position and will share full job details in your email id.
#DataAnalyst #SecurityClearance #DoDIssuedSecretClearance #BachelorDegree #Mathematics #Engineering #ComputerScience #Collaboration #CommunicationSkills #PresentationSkills #StatisticalAnalysis #DataVisualization #SQL #NAVSEA #NavyExperience #MachineLearning #DataScience #DataAnalytics #DataManagement #MetricsAnalysis #DashboardDevelopment #Teleworking #DataVisualization #Tableau #DataAnalysisTools #BusinessValue #DataMining #PredictiveAnalysis #DataInterpretation #MachineLearningMethods #StatisticalMethods #BusinessSolutions #AnalyticsJobs #SeniorDataAnalyst #DataAnalyticsCareer #GovernmentJobs #WashingtonDCJobs #JobOpportunity #DataScienceJobs #BusinessAnalysis #MaintenanceMetrics #AdvancedAnalytics #LeadershipBriefs #DataManagementSchemas #ProductivityImprovement #StatisticalAnalysis #PredictiveModeling #DataQuality #DecisionMaking #StakeholderEngagement #BusinessProcesses
Show more
Show less","Secret Clearance, Bachelor's Degree, Mathematics, Engineering, Computer Science, Collaboration, Communication Skills, Presentation Skills, Statistical Analysis, Data Visualization, SQL, NAVSEA, Navy Experience, Data Analytics, Data Management, Metrics Analysis, Dashboard Development, Teleworking, Tableau, Data Analysis Tools, Business Value, Data Mining, Predictive Analysis, Data Interpretation, Machine Learning Methods, Statistical Methods, Business Solutions, Analytics Jobs, Data Analytics Career, Government Jobs, Data Science Jobs, Business Analysis, Maintenance Metrics, Advanced Analytics, Leadership Briefs, Data Management Schemas, Productivity Improvement, Predictive Modeling, Data Quality, Decision Making, Stakeholder Engagement, Business Processes","secret clearance, bachelors degree, mathematics, engineering, computer science, collaboration, communication skills, presentation skills, statistical analysis, data visualization, sql, navsea, navy experience, data analytics, data management, metrics analysis, dashboard development, teleworking, tableau, data analysis tools, business value, data mining, predictive analysis, data interpretation, machine learning methods, statistical methods, business solutions, analytics jobs, data analytics career, government jobs, data science jobs, business analysis, maintenance metrics, advanced analytics, leadership briefs, data management schemas, productivity improvement, predictive modeling, data quality, decision making, stakeholder engagement, business processes","advanced analytics, analytics jobs, bachelors degree, business analysis, business processes, business solutions, business value, collaboration, communication skills, computer science, dashboard development, data analysis tools, data analytics career, data interpretation, data management, data management schemas, data mining, data quality, data science jobs, dataanalytics, decision making, engineering, government jobs, leadership briefs, machine learning methods, maintenance metrics, mathematics, metrics analysis, navsea, navy experience, predictive analysis, predictive modeling, presentation skills, productivity improvement, secret clearance, sql, stakeholder engagement, statistical analysis, statistical methods, tableau, teleworking, visualization"
