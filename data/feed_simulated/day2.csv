job_title,company,job_location,job_link,first_seen,search_city,search_country,job level,job_type,job_summary,job_skills,job_skills_raw,job_skills_cleaned
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744395043,2023-12-17,Layton,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Data engineering, Data science, Apache Airflow, Kubernetes, Docker, Helm, Apache Spark, PySpark, SQL, Snowflake, Kafka, Storm, Spark Streaming, Data Warehouses, ETL pipelines, Data management tools, Data classification, Retention","python, data engineering, data science, apache airflow, kubernetes, docker, helm, apache spark, pyspark, sql, snowflake, kafka, storm, spark streaming, data warehouses, etl pipelines, data management tools, data classification, retention","apache airflow, apache spark, data classification, data engineering, data management tools, data science, data warehouses, docker, etl pipelines, helm, kafka, kubernetes, python, retention, snowflake, spark, spark streaming, sql, storm"
Sr. Data Analyst,Varo Bank,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/sr-data-analyst-at-varo-bank-3748838419,2023-12-17,Layton,United States,Mid senior,Remote,"Varo is an entirely new kind of bank. All digital, mission-driven, FDIC insured and designed for the way our customers live their lives. A bank for all of us.
The Analytics team is lean, high functioning and impact-focused. If you can see different angles for business/product opportunities, synthesize large amounts of data, connect the dots amidst ambiguity, and want to deliver clear and measurable impact from day one, Varo welcomes you!
What You'll Be Doing
Develop a deep understanding of the existing business and share insights around the key metrics that drive the company
Conduct data analysis, using different analytical and statistical approaches, to make business recommendations
Analysis areas might include (but not limited to): commercial analytics, product feature analysis, portfolio credit exposure, delinquency propensity models, forecasting)
Create & automate reports, iteratively build & prototype dashboards to provide insights at scale, solving for business priorities
Work with Product, Engineering and cross functional teams to provide analytical support for ongoing needs and large initiatives
Deliver effective presentations of findings and recommendations to multiple levels of stakeholders, creating visual displays of quantitative information.
You Bring The Following Required Skills And Experiences
4+ years of experience in an analytical role providing business analysis, developing dashboards, and presenting results to leadership
4+ years of experience in SQL queries, Tableau (and other BI tools), and day-to-day use of R/Python is a plus
Experience articulating business questions, scripting with SQL/Python to pull data from datasets, and adapting quantitative techniques to solve complex problems
Experience with experimentation, risk assessment, measuring ROI of initiatives, and automating dashboards and reports to track business performance
Track record of successfully communicating data-driven insights
Solid understanding of statistical analysis
Distinctive problem-solving skills and impeccable business judgment
We recognize not everyone will have all of these requirements. If you meet most of the criteria above and you’re excited about the opportunity and willing to learn, we’d love to hear from you!
About Varo
Varo launched in 2017 with the vision to bring the best of fintech into the regulated banking system. We’re a new kind of bank – all-digital, mission-driven, FDIC-insured, and designed around the modern American consumer.
As the first consumer fintech to be granted a national bank charter in 2020, we make financial inclusion and opportunity for all a reality by empowering everyone with the products, insights, and support they need to get ahead. Through our core product offerings and suite of customer-first features, we aim to address a broad range of consumer needs while profitably serving underserved communities that have been historically excluded from the traditional financial system.
We are growing quickly in our hub locations of San Francisco, Salt Lake City, and Charlotte along with colleagues located across the country. We have been recognized among Fast Company’s Most Innovative Companies, Forbes’ Fintech 50, and earned the No. 7 spot on Inc. 5000’s list of fastest-growing companies across the country.
Varo. A bank for all of us.
Our Core Values
Customers First
Take Ownership
Respect
Stay Curious
Make it Better
Learn More About Varo By Following Us
Facebook - https://www.facebook.com/varomoney
Instagram - www.instagram.com/varobank
LinkedIn - https://www.linkedin.com/company/varobank
Twitter - https://twitter.com/varobank
Engineering Blog - https://medium.com/engineering-varo
SoundCloud - https://soundcloud.com/varobank
Varo is an equal opportunity employer. Varo embraces diversity and we are committed to building teams that represent a variety of backgrounds, perspectives, and skills. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.
Beware of fraudulent job postings!
Varo will never ask for payment to process documents, refer you to a third party to process applications or visas, or ask you to pay costs. Never send money to anyone suggesting they can provide work with Varo. If you suspect you have received a phony offer, please e-mail careers@varomoney.com with the pertinent information and contact information.
CCPA Notice At Collection For California Employees And Applicants
https://varomoney.box.com/s/q7eockvma9nd2b0utwryruh4ze6gf8eg
Show more
Show less","Data analysis, SQL, Tableau, R, Python, BI tools, Experimentation, Risk assessment, Statistical analysis","data analysis, sql, tableau, r, python, bi tools, experimentation, risk assessment, statistical analysis","bi tools, dataanalytics, experimentation, python, r, risk assessment, sql, statistical analysis, tableau"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773086808,2023-12-17,Layton,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Pipelines, Statistical Analysis, Data Visualization, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, NoSQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Complex Data Projects, Data Classification, Data Retention, Data Governance, Risk and Compliance Initiatives, Text Data Sets, NLP, Conversational AI APIs, Recommender Systems","data engineering, machine learning, data pipelines, statistical analysis, data visualization, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, complex data projects, data classification, data retention, data governance, risk and compliance initiatives, text data sets, nlp, conversational ai apis, recommender systems","airflow, applied machine learning, aws, azure, bash, complex data projects, conversational ai apis, data classification, data engineering, data governance, data retention, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, nlp, nosql, python, recommender systems, risk and compliance initiatives, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, text data sets, visualization"
Senior Data Engineer,"Resolution Technologies, Inc.","La Vergne, TN",https://www.linkedin.com/jobs/view/senior-data-engineer-at-resolution-technologies-inc-3777073358,2023-12-17,Lawton,United States,Mid senior,Hybrid,"Senior Data Engineer Job Description:
This individual will work to deliver enterprise grade software solutions with high customer impact. They will lead architecture and development activities with a specialization in at least one major enterprise IT application, one major database platform (e.g, MySQL, Oracle, SQL Server), and one major operating system (e.g. Linux). The Senior Data Engineer also performs all aspects of the development life cycle. They will act as the senior technical programmer for the assigned enterprise system and/or application of responsibility. Finally, they will deliver results through independent contributions and through mentoring of junior engineers.
Senior Data Engineer Minimum Qualifications:
Bachelor’s Degree in Computer Science or related field or directly related year for year experience
6 years’ experience in designing, developing, implementing, and supporting enterprise level IT solutions
Senior Data Engineer Preferred Skills:
6+ years of experience with writing and optimizing existing complex SQL queries
6+ years of database application development experience
Advanced knowledge of SQL relational databases, query authoring (SQL)
Experience with Vertica (projections, segmentation, columnar data)
Experience with columnar databases or non-relational databases
Knowledge of common tools for CentoOS Linux (logs, piping, redirections, grep, sed, yum)
Knowledge of Linux scripting (Python, Perl, shell scripts) and/or advanced stored procedures
Experience with architecting data modeling and meeting requirements for data visualization or reporting tools
Experience with collaborating in a cross-functional capacity across teams, building consensus and executing the necessary vision for application and other analytical needs.
Knowledge of developing in Visual Studio, SSMS and DB Visualizer
Knowledge of JIRA and Confluence
Ability to take business requirements and transpose them into technical details
Senior Data Engineer Key Responsibilities:
Serves as Designer/Architect/Engineer for a major enterprise IT application.
Creates, develops, modifies, and maintains data models for internal and external facing application as part of an Agile/SCRUM engineering team
Assembles large and complex data sets that meet business requirements.
Coordinates and communicates with users, developers, and product owners to gather and understanding requirements.
Develops new design patterns, standards, documentation, etc. and works with other developers for implementation
#RT
Show more
Show less","SQL, Vertica, Columnar databases, Linux, Python, Perl, Visual Studio, SSMS, DB Visualizer, JIRA, Confluence","sql, vertica, columnar databases, linux, python, perl, visual studio, ssms, db visualizer, jira, confluence","columnar databases, confluence, db visualizer, jira, linux, perl, python, sql, ssms, vertica, visual studio"
Data Warehouse Engineer - 77043,Swoon,"Midvale, UT",https://www.linkedin.com/jobs/view/data-warehouse-engineer-77043-at-swoon-3766966252,2023-12-17,Lawton,United States,Mid senior,Hybrid,"Swoon is actively seeking a Data Warehousing Engineer to join the team!
What your day-to-day will look like?
Design, coding, integration testing, deployment, operations support, and documentation using Agile methodologies
Partner with architects, engineers, information analysts, business and technology stakeholders for developing and deploying enterprise grade platforms that enable data-driving solutions
Analyze and design technical solutions to address business needs
Develop, test, and modify software to improve efficiency of data platforms and applications
What’s Required? / Technical Skills
Experience with data warehousing, data technologies and ETL solutions
4+ years of experience in ETL development and big data distributed systems
3+ years of experience in programming languages like python
Experience in data migration, data analysis, data transformations, conversion, interface, or large volume data loading
Experience working with cloud or on-prem big data/MPP analytics platforms
Education / Certifications
Bachelor’s degree
What Else you should know?
Job Title:
Data Warehousing Engineer
Direct-Hire
– Full-time Opportunity
Location:
Midvale, Utah 84047
Hybrid -
3 days a week in office – 2 days a week work from home
What’s Next?
Apply Now! -- Email questions to Kathryn.Jackson@swoontech.com
Show more
Show less","Data Warehousing, ETL Solutions, Data Migration, Data Analysis, Data Transformations, Conversion, Interface, Large Volume Data Loading, Cloud, Onprem Data Analytics Platforms, MPP Analytics Platforms, Python, Agile Methodologies","data warehousing, etl solutions, data migration, data analysis, data transformations, conversion, interface, large volume data loading, cloud, onprem data analytics platforms, mpp analytics platforms, python, agile methodologies","agile methodologies, cloud, conversion, data migration, data transformations, dataanalytics, datawarehouse, etl solutions, interface, large volume data loading, mpp analytics platforms, onprem data analytics platforms, python"
Data Services Analyst Co-Op -Summer 2024,Crown Equipment Corporation,"New Bremen, OH",https://www.linkedin.com/jobs/view/data-services-analyst-co-op-summer-2024-at-crown-equipment-corporation-3755147076,2023-12-17,Van Buren,United States,Mid senior,Onsite,"Location:
New Bremen, OH, US, 45869
Company Description
Crown Equipment Corporation is a leading innovator in world-class forklift and material handling equipment and technology. As one of the world’s largest lift truck manufacturers, we are committed to providing the customer with the safest, most efficient and ergonomic lift truck possible to lower their total cost of ownership.
Job Responsibilities
Assist in the following items: Perform data cleaning and statistical data analysis using R, SQL and business intelligence (BI) tool to determine new trends and patterns that might affect lift truck performance. Use different data sources (SAP ERP, Baan, columnar DB, SQL - InfoLink, flat files) to proactively generate reports; recommend and develop new metrics that may benefit Crown engineering, sales, service and customers.
Analyze data, find patterns and develop machine learning algorithm to model alarm functioning based on fleet manager feedback. Build and manage solutions that can be used by engineering and Crown service to improve vehicle health. Work with Fleet Managers to transform business requirements into technical requirements for business improvement
Proactively collaborate with different teams and customers to analyze functional requirements. Must be able to interpret results and communicate status reports/important findings to management and decision-makers. Communicate project expectations and status to team members and stakeholders in a clear, concise, and timely fashion. Write codes as per the coding conventions and deliver solutions as defined by documentation standards. Should be unafraid to prevent true facts and capable of defending recommendations logically
Research and learn new tools/technologies in the analytics world which can be integrated to the present system for planning and building new products/solution. Evaluate different tools available in the market and provide recommendations based on different criteria.
Job Qualifications
Experience doing data analysis
R, SQL, PowerBI,Tableau/QLIK a must, Map-Reduce/Hadoop a plus
Experience in data mining and statistical modeling techniques on R (supervised and unsupervised machine learning)
Enrolled in a Bachelor degree program for Business, Computer Science, MIS or related field.
Must have valid driving privileges
Work Authorization
Crown will only employ those who are legally authorized to work in the United States. This is not a position for which sponsorship will be provided. Individuals with temporary visas or who need sponsorship for work authorization now or in the future, are not eligible for hire.
No agency calls please.
EO/AA Employer Minorities/Females/Protected Veterans/Disabled
Nearest Major Market:
Lima
Nearest Secondary Market:
Findlay
Job Segment:
Summer Internship, Computer Science, Database, SQL, Data Mining, Entry Level, Technology
Show more
Show less","R, SQL, PowerBI, Tableau, QLIK, MapReduce, Hadoop, Machine Learning, Data Mining, Statistical Modeling, Data Analysis, Business Intelligence, Coding Conventions, Documentation Standards, SAP ERP, Baan, Columnar DB, InfoLink, Fleet Manager, Analytics","r, sql, powerbi, tableau, qlik, mapreduce, hadoop, machine learning, data mining, statistical modeling, data analysis, business intelligence, coding conventions, documentation standards, sap erp, baan, columnar db, infolink, fleet manager, analytics","analytics, baan, business intelligence, coding conventions, columnar db, data mining, dataanalytics, documentation standards, fleet manager, hadoop, infolink, machine learning, mapreduce, powerbi, qlik, r, sap erp, sql, statistical modeling, tableau"
RESEARCH DATA SPECIALIST II,Caltrans,"San Luis Obispo County, CA",https://www.linkedin.com/jobs/view/research-data-specialist-ii-at-caltrans-3782430868,2023-12-17,San Luis Obispo,United States,Mid senior,Onsite,"Equal Opportunity Employer
The State of California is an equal opportunity employer to all, regardless of age, ancestry, color, disability (mental and physical), exercising the right to family care and medical leave, gender, gender expression, gender identity, genetic information, marital status, medical condition, military or veteran status, national origin, political affiliation, race, religious creed, sex (includes pregnancy, childbirth, breastfeeding and related medical conditions), and sexual orientation.
It is an objective of the State of California to achieve a drug-free work place. Any applicant for state employment will be expected to behave in accordance with this objective because the use of illegal drugs is inconsistent with the law of the State, the rules governing Civil Service, and the special trust placed in public servants.
Position Details
Job Code #:
JC-406616
Position #(s):
905-800-5758-XXX
Working Title:
Research Data Specialist II
Classification:
RESEARCH DATA SPECIALIST II
$6,658.00 - $8,583.00
Shall Consider:
RESEARCH DATA SPECIALIST I
$6,061.00 - $7,817.00
# of Positions:
Multiple
Work Location:
San Luis Obispo County
Telework:
Hybrid
Job Type:
Permanent, Full Time
Department Information
Caltrans Mission: Provide a safe and reliable transportation network that serves all people and respects the environment.
Caltrans Vision: A brighter future for all through a world-class transportation network.
The Caltrans workforce is made up of diverse and unique individuals who contribute to our organizational success. Caltrans is about celebrating diversity, valuing one another, and recognizing that Caltrans is strong not in spite of the diverse attributes of our workforce, but because of our diversity.
Department Website: www.dot.ca.gov
Frequently Asked Questions for an Applicant: http://dot.ca.gov/jobs/docs/faq-ct-applicants-081617.pdf
Director’s EEO Policy : https://dot.ca.gov/programs/equal-employment-opportunity
Director’s EEO Policy Statement: https://dot.ca.gov/programs/equal-employment-opportunity
Job Description And Duties
Will Consider Research Data Specialist I
Under the general direction of a Senior Transportation Engineer in Advance Planning, the Research Data Specialist II (RDS II) is a team member at full journey person level and has primary responsibility in the Division of Transportation Planning for a variety of tasks associated with the application of travel demand modeling, transportation data, statistical analysis and Geographic Information Systems (GIS) for various Caltrans transportation engineering, project development, and planning needs.This is a technical planning position that requires the incumbent to determine the scope and parameters of research criteria and perform complex research work.The RDS II will collaborate closely with Traffic Operations on the development of planning and project studies and related data collection efforts. The RDS II also works cooperatively with internal and external partners including the Regional Transportation Planning Agencies and Metropolitan Planning Organization on planning studies and related projects to support the Department's mission, goals, and objectives.
The RDS II may be assigned by Supervisor to be a Lead-worker to train, direct, and manage the work of other staff within assigned branch.
PARF 05-4-827 / JC - 406616
Eligibility for hire is determined by your score on the Research Data Specialist II Exam. You must be on the state examination list to be eligible for these positions. The Research Data Specialist II Exam is located here: https:// www.calcareers.ca.gov/CalHrPublic/Exams/Bulletin.aspx?examCD=8PB40
Eligibility for hire is determined by your score on the Research Data Specialist I Exam. You must be on the state examination list to be eligible for these positions. The Research Data Specialist I Exam is located here: https:// www.calcareers.ca.gov/CalHrPublic/Exams/Bulletin.aspx?examCD=8PB39
T he Human Resources Contact is available to answer questions regarding the application process. The Hiring Unit Contact is available to answer questions regarding the position.
You will find additional information about the job in the
Duty Statement
.
Special Requirements
Possession of a valid driver’s license is required when operating a state owned or leased vehicle.
Possession of Minimum Qualifications will be verified prior to interview and/or appointment. If you are meeting Minimum Qualifications with education, you must include your unofficial transcript(s)/diploma for verification. Unofficial, original or official sealed transcripts will be required upon appointment. Applicants with foreign transcripts/degrees must provide a transcript/degree U.S. equivalency report evaluation that indicates the number of units and degree to which the foreign coursework is equivalent. Here is a list of evaluation agencies: https://www.naces.org/members .
Application Instructions
Completed applications and all required documents must be received or postmarked by the Final Filing Date in order to be considered. Dates printed on Mobile Bar Codes, such as the Quick Response (QR) Codes available at the USPS, are not considered Postmark dates for the purpose of determining timely filing of an application.
Final Filing Date: 1/2/2024
Who May Apply
Individuals who are currently in the classification, eligible for lateral transfer, eligible for reinstatement, have list eligibility, are in the process of obtaining list eligibility, or have SROA and/or Surplus eligibility (please attach your letter, if available). SROA and Surplus candidates are given priority; therefore, individuals with other eligibility may be considered in the event no SROA or Surplus candidates apply. Individuals who are eligible for a Training and Development assignment may also be considered for this position(s).
Applications will be screened and only the most qualified applicants will be selected to move forward in the selection process. Applicants must meet the Minimum Qualifications stated in the Classification Specification(s).
How To Apply
Complete Application Packages (including your Examination/Employment Application (STD 678) and applicable or required documents) must be submitted to apply for this Job Posting. Application Packages may be submitted electronically through your CalCareer Account at www.CalCareers.ca.gov. When submitting your application in hard copy, a completed copy of the Application Package listing must be included. If you choose to not apply electronically, a hard copy application package may be submitted through an alternative method listed below:
Address for Mailing Application Packages
You may submit your application and any applicable or required documents to:
Department of Transportation
Attn: Caltrans DHR Contact
Certification Services MS-90
P O Box 168036
Sacramento , CA 95816-8036
Address for Drop-Off Application Packages
You may drop off your application and any applicable or required documents at:
Department of Transportation
Caltrans DHR Contact
Classification and Hiring Unit - ECOS
1727 30th Street, MS 90
Sacramento , CA 95816
Closed on weekends and State Holidays
08:00 AM - 05:00 PM
Required Application Package Documents
The following items are required to be submitted with your application. Applicants who do not submit the required items timely may not be considered for this job:
Current version of the State Examination/Employment Application STD Form 678 (when not applying electronically), or the Electronic State Employment Application through your Applicant Account at www.CalCareers.ca.gov. All Experience and Education relating to the Minimum Qualifications listed on the Classification Specification should be included to demonstrate how you meet the Minimum Qualifications for the position.
Resume is optional. It may be included, but is not required.
Applicants requiring reasonable accommodations for the hiring interview process must request the necessary accommodations if scheduled for a hiring interview. The request should be made at the time of contact to schedule the interview. Questions regarding reasonable accommodations may be directed to the EEO contact listed on this job posting.
Show more
Show less","GIS, Geographic Information Systems, Research, Statistical analysis, Transportation planning, Travel demand modeling, Data analysis, Data collection, Project development, Engineering, Statistics, SQL, Python, R, Hadoop, Spark, Tableau, Power BI","gis, geographic information systems, research, statistical analysis, transportation planning, travel demand modeling, data analysis, data collection, project development, engineering, statistics, sql, python, r, hadoop, spark, tableau, power bi","data collection, dataanalytics, engineering, geographic information systems, gis, hadoop, powerbi, project development, python, r, research, spark, sql, statistical analysis, statistics, tableau, transportation planning, travel demand modeling"
Senior Data Engineer,VeeAR Projects Inc.,"Menlo Park, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-veear-projects-inc-3743867807,2023-12-17,Santa Clara,United States,Mid senior,Onsite,"Data Engineer Responsibilities:
Partner with leadership, engineers, program managers and data scientists to understand data needs.
Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.
Communicate, at scale, through multiple mediums: Presentations, dashboards, company-wide datasets, bots and more.
Educate your partners: Use your data and analytics experience to 'see what's missing', identifying and addressing gaps in their existing logging and processes.
Leverage data and business principles to solve large scale web, mobile and data infrastructure problems.
Build data expertise and own data quality for your areas.
Minimum Qualifications:
5+ years of Python development experience.
5+ years of SQL experience.
3+ years' experience with Data Modeling.
5+ years' experience in custom ETL design, implementation and maintenance.
Preferred Qualifications:
Experience with more than one coding language.
Experience with designing and implementing real-time pipelines.
Experience with data quality and validation.
Experience with SQL performance tuning and E2E process optimization.
Experience with anomaly/outlier detection.
Experience with notebook-based Data Science workflow.
Experience with Airflow.
Experience querying massive datasets using Spark, Presto, Hive, Impala, etc.
Show more
Show less","Python, SQL, Data Modeling, ETL Design, ETL Implementation, ETL Maintenance, Coding Language, Realtime Pipelines, Data Quality, Data Validation, SQL Performance Tuning, E2E Process Optimization, Anomaly Detection, Notebookbased Data Science Workflow, Airflow, Spark, Presto, Hive, Impala","python, sql, data modeling, etl design, etl implementation, etl maintenance, coding language, realtime pipelines, data quality, data validation, sql performance tuning, e2e process optimization, anomaly detection, notebookbased data science workflow, airflow, spark, presto, hive, impala","airflow, anomaly detection, coding language, data quality, data validation, datamodeling, e2e process optimization, etl design, etl implementation, etl maintenance, hive, impala, notebookbased data science workflow, presto, python, realtime pipelines, spark, sql, sql performance tuning"
Senior Data Services Engineer,Mr. Cooper,"Lewisville, TX",https://www.linkedin.com/jobs/view/senior-data-services-engineer-at-mr-cooper-3786376138,2023-12-17,Arlington,United States,Mid senior,Onsite,"Who We Are
We are Xome, a real estate services company headquartered in the Dallas, Texas area. As a subsidiary of Mr. Cooper Group, we employ over 1,200 team members nationwide. The nation’s largest financial services companies look to us for integrated and scalable business solutions that help simplify the mortgage and real estate process.
At the heart of everything we do is our purpose: To keep the dream of home ownership alive. If that sounds like a big, lofty goal, that’s because it really is. And we can’t do it alone. Our entire team is focused on helping create a stable and healthy housing industry. And, making sure the process of buying/selling a home doesn’t undermine the excitement of home ownership. That’s why we battle every day against the mediocrity of the status quo to simplify the complex world of mortgage servicing, lending and banking. We see ourselves as the experts who make doing business easier. While others bring complexity and a lack of transparency, we offer simplicity, trust, and visibility across the entire property lifecycle. And we deliver radical customer service.
Now, you might be wondering; how exactly do you pronounce Xome? Simple. ZOM (like home, if it started with a z)!
The Senior Data Services Engineer is responsible for designing and developing data repositories for enterprise business operations to be used for providing business reporting and analysis. The Senior Data Services Engineer will support the analytics team by providing accurate, timely and relevant data to meet their diverse requirements. The Senior Data Services Engineer will also work closely with internal teams to support and build business insight tools.
Essential Job Functions
Designs data architectures and builds relational/dimensional databases and establish methods to improve functional reporting data content and completeness of data.
Design and develop robust, re-usable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming.
Provide technical skills in designing and developing BI/Data projects in the areas of optimal design patterns, models, standards and code to ensure consistency and realize benefits of a high-performing, secure, reliable and scalable architecture
Collaborate with IT architecture/data team/ data scientists to develop a practical end state and reference architectures for BI/Data with considerations for distributed data, in-memory computing, cloud computing, visualization tools/platforms
Provide technical leadership in areas such as master data management and reference data management to reduce duplication and redundancy for core data objects
Work with the data governance team to ensure alignment of data definition, quality specifications, models and meta data management to technical implementations.
Develops relationships with the larger development teams that promote trust and increase efficiency and effectiveness
Participate in application validation and QA efforts as they pertain to reporting, data, metrics, and report creation and execution.
Other duties as assigned.
Education / Experience Requirements
Bachelor's Degree or Foreign equivalent in Information Technology, Computer Science, Computer Information Systems, Engineering or related field and 5 years of progressively responsible experience OR a Master’s Degree in Information Technology, Computer Science, Computer Information Systems, Engineering or related field (foreign equivalent acceptable) and 3 years of progressively responsible experience.
5+ progressive years of experience in Data Warehouse, SQL scripting, SQL Tuning and Business Intelligence technologies.
5+ progressive years of experience in Python programming language for scripting and ETL development.
5+ progressive years of strong data engineering, SQL scripting and tuning experience.
Years of progressive experience should include strong knowledge of any of the ETL tools like SSIS, Azure Data Factory, Informatica; any of the Data Modeling tools like Erwin, ER/Studio, Toad Data Modeler; any of the BI tools like SSRS, Tableau, Microsoft Power BI.
Must be experienced in designing and implementing high performance data pipelines using Databricks for data analytics with any major cloud platform like AWS or Azure.
Must be experienced in developing and implementing data quality and data governance standards.
Must be experienced implementing high performing technical solutions related to ETL with large source environments and patterns related to ODS, MDM, Landing/Staging areas and EDW/Data Mart.
Experience in managing data and analytics programs (people, process, tools) through the full lifecycle: strategic recommendation; design of experiments, testing, communication, pilot, implementation, etc.
Knowledge of developing and maintaining formal documentation that describes the data and data structures including data modeling.
Ability to work with senior technical and business resources providing technical guidance related to data architecture and governance.
In-depth knowledge of IT concepts, strategies and methodologies and their application to business opportunities.
Ability to mentor junior Data Engineers.
Mortgage or Finance industry experience is a big plus.
Xome is committed to nurturing a diverse and inclusive environment where every employee is empowered to be their authentic self. We know that a large part of our success as a business is directly tied to our ongoing efforts to attract and retain diverse talent and maintain an inclusive environment where each employee can thrive. Embracing and leveraging diversity through an inclusive work environment fosters new ideas, new insights, and constant innovation. We strive to weave the principles of diversity and inclusion throughout the fabric of how we work, how we interact, and how we engage with our customers and the community.
Job Requisition ID
021044
Job Category
Information Technology
Primary Location City:
Lewisville
Primary Location Region:
Texas
Primary Location Postal Code:
75067
Primary Location Country:
United States of America
Additional Posting Location(s):
Show more
Show less","Data Structures, Business Intelligence, Data Architecture, Data Engineering, ETL Development, SQL Scripting, SQL Tuning, Python, Data Modeling, Data Warehousing, Cloud Computing, AWS, Azure, SSIS, Azure Data Factory, Informatica, Erwin, ER/Studio, Toad Data Modeler, SSRS, Tableau, Microsoft Power BI, Databricks, ODS, MDM, Landing/Staging Areas, EDW/Data Mart, Data Governance, Data Quality","data structures, business intelligence, data architecture, data engineering, etl development, sql scripting, sql tuning, python, data modeling, data warehousing, cloud computing, aws, azure, ssis, azure data factory, informatica, erwin, erstudio, toad data modeler, ssrs, tableau, microsoft power bi, databricks, ods, mdm, landingstaging areas, edwdata mart, data governance, data quality","aws, azure, azure data factory, business intelligence, cloud computing, data architecture, data engineering, data governance, data quality, data structures, databricks, datamodeling, datawarehouse, edwdata mart, erstudio, erwin, etl development, informatica, landingstaging areas, mdm, microsoft power bi, ods, python, sql scripting, sql tuning, ssis, ssrs, tableau, toad data modeler"
Senior Data Engineer,Aditi Consulting,"Fort Worth, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-aditi-consulting-3762448756,2023-12-17,Arlington,United States,Mid senior,Onsite,"Collaborative team environment
Wants to be on the cutting edge, working with cutting edge technologies, open to new ideas and strategies
Team access and collaboration
The reason for hiring is they have expanded and restructured- taken the old BI team and moved into Data Engineering as well as brought on a director of engineering.
This candidate will report to the Director of Data Engineering and will be working on a team of 2 that has been approved to grow into a team of 6.
Designs and implements self-service data product deployment strategies
Promotes BorrowWorks’ cloud strategy and design cloud-native data engineering workflows
Develops tooling to facilitate data product development, deployment, and monitoring
Develops automated workflows for data engineering pipelines
Collaborates with data engineers, software engineers, business analysts, and data scientists to develop data pipelines and automation solutions
Creates and promotes best practices in data operations
Helps contribute to a collaborative, open developer environment
Leads improvements in methodology or initiatives to address capability gaps or increase efficiency
Offers advice and guidance to junior associates for sake of continuous improvement.
Required
Five (5) years of experience related to Data Engineering
Demonstrated experience with data and software engineering best practices and implementing data and software development lifecycles.
Demonstrated experience with implementing data ingestion/testing automation solutions.
Demonstrated success in one or more of the following programming languages: SQL, Python, Java, Scala.
Experience developing RESTful APIs
Experience with Docker/Kubernetes
Experience delivering and scaling data products in production.
Compensation:
The pay rate range above is the base hourly pay range that Aditi Consulting reasonably expects to pay someone for this position (compensation may vary outside of this range depending on a number of factors, including but not limited to, a candidate’s qualifications, skills, competencies, experience, location and end client requirements).
Benefits and Ancillaries:
Medical, dental, vision, PTO benefits and ancillaries may be available for eligible Aditi Consulting employees and vary based on the plan options selected by the employee.
Show more
Show less","Data Engineering, Cloudnative data engineering, Data product development, Data product deployment, Data pipeline automation, Data operations best practices, Collaborative developer environment, Data and software engineering best practices, Data and software development lifecycles, Data ingestion automation, Testing automation solutions, RESTful APIs, Docker, Kubernetes, Data products in production, SQL, Python, Java, Scala","data engineering, cloudnative data engineering, data product development, data product deployment, data pipeline automation, data operations best practices, collaborative developer environment, data and software engineering best practices, data and software development lifecycles, data ingestion automation, testing automation solutions, restful apis, docker, kubernetes, data products in production, sql, python, java, scala","cloudnative data engineering, collaborative developer environment, data and software development lifecycles, data and software engineering best practices, data engineering, data ingestion automation, data operations best practices, data pipeline automation, data product deployment, data product development, data products in production, docker, java, kubernetes, python, restful apis, scala, sql, testing automation solutions"
"Data Quality Engineer -- 1 year contract -- Dallas, TX (Day 1 Onsite)",Lorven Technologies Inc.,"Dallas, TX",https://www.linkedin.com/jobs/view/data-quality-engineer-1-year-contract-dallas-tx-day-1-onsite-at-lorven-technologies-inc-3783919854,2023-12-17,Arlington,United States,Mid senior,Onsite,"Job Title: Data Quality Engineer
Location: Dallas, TX (Day 1 Onsite)
Duration: 1 year contract
Mandatory Skills: DQLabs
Responsibilities
As a Data Quality Engineer specializing in DQLabs, you will be instrumental in evaluating and implementing data quality tools, conducting pilot tests, and ensuring the effectiveness of the data quality processes. Your role will involve collaboration with cross-functional teams to assess and select the right tools while running pilot tests to validate their suitability for our unique financial datasets.
Assess and evaluate various data quality tools, with a focus on DQLabs, to determine their suitability for their environment.
Collaborate with stakeholders to understand data quality requirements and help select tools that align with organizational goals.
Design and execute pilot tests for selected data quality tools to assess their performance in a real-world data context.
Evaluate the effectiveness of tools in identifying and rectifying data quality issues through systematic testing.
Work with cross-functional teams to develop and implement a comprehensive data quality framework tailored to financial datasets.
Ensure that the chosen tools integrate seamlessly with the overall data quality
Develop and implement automated data quality checks and validations using scripting or coding languages (e.g., Python, SQL) within the DQ Labs environment.
Streamline data quality processes and workflows to enhance efficiency.
Collaborate closely with financial analysts, data scientists, and other stakeholders to understand data quality needs and address specific requirements.
Provide technical expertise and support during pilot tests and tool implementation.
Qualifications
Bachelor's degree in a relevant field (e.g., Computer Science, Information Systems, Finance).
Proven experience as a Data Quality Engineer, with a focus on DQ Labs tools and methodologies.
Solid understanding of financial data concepts, regulations, and industry best practices.
Proficiency in scripting or coding languages (e.g., Python, SQL) for data quality automation.
Solid understanding of financial data concepts, regulations, and industry best practices.
Strong analytical and problem-solving skills with meticulous attention to detail.
Excellent communication and collaboration skills within a financial context.
Show more
Show less","Data Quality, DQLabs, Data Quality Tools, Data Quality Framework, Python, SQL, DQ Labs, Financial Data Concepts, Financial Regulations, Industry Best Practices, Scripting Languages, Coding Languages, Data Quality Automation, Data Quality Checks, Data Quality Validation, Financial Analysts, Data Scientists, Technical Expertise","data quality, dqlabs, data quality tools, data quality framework, python, sql, dq labs, financial data concepts, financial regulations, industry best practices, scripting languages, coding languages, data quality automation, data quality checks, data quality validation, financial analysts, data scientists, technical expertise","coding languages, data quality, data quality automation, data quality checks, data quality framework, data quality tools, data quality validation, data scientists, dq labs, dqlabs, financial analysts, financial data concepts, financial regulations, industry best practices, python, scripting languages, sql, technical expertise"
"Software Engineer - Core Data Engineering Platform (Dallas, TX)",Goldman Sachs,"Dallas, TX",https://www.linkedin.com/jobs/view/software-engineer-core-data-engineering-platform-dallas-tx-at-goldman-sachs-3754984618,2023-12-17,Arlington,United States,Mid senior,Onsite,"Job Description
What We Do
At Goldman Sachs, our Engineers don’t just make things – we make things possible. Change the world by connecting people and capital with ideas. Solve the most challenging and pressing engineering problems for our clients. Join our engineering teams that build massively scalable software and systems, architect low latency infrastructure solutions, proactively guard against cyber threats, and leverage machine learning alongside financial engineering to continuously turn data into action. Create new businesses, transform finance, and explore a world of opportunity at the speed of markets.
Engineering, which is comprised of our Technology Division and global strategists groups, is at the critical center of our business, and our dynamic environment requires innovative strategic thinking and immediate, real solutions. Want to push the limit of digital possibilities? Start here.
About Data Engineering
Data plays a critical role in every facet of the Goldman Sachs business. The Data Engineering group is at the core of that offering, focusing on providing the platform, processes, and governance, for enabling the availability of clean, organized, and impactful data to scale, streamline, and empower our core businesses.
Within Data Engineering, we focus on offering a comprehensive data platform, Legend, which we have made available as an open-source product. Legend includes a full data modeling environment, as well as the execution of data access and controls, and a vast set of value-add products which allow our business users to operate more efficiently.
Leveraging our own Legend offering, our engineers build efficient data solutions that source, curate, and distribute critical data to our businesses, including financial product, pricing, transaction, and client reference data. Our engineers collaborate closely with the business to design and curate business-specific data models, and to transform and distribute data for optimal storage and retrieval.
Who We Look For
Goldman Sachs Engineers are innovators and problem-solvers, building solutions in risk management, big data, mobile and more. We look for creative collaborators who evolve, adapt to change and thrive in a fast-paced global environment.
As a
Full-stack Software Engineer
on the Data Engineering team, you will be responsible for helping improve the Legend data platform, our curated data offerings, and how the business uses data. We tackle some of the most complex engineering problems across distributed software development, optimizing data access and delivery, enabling core access controls via well-defined security paradigms, building UIs to enable data visualization, using machine learning to curate data, or engaging with businesses to ensure their data needs are met, and we react quickly to new demands by rapidly evolving our data solutions.
How You Will Fulfill York Potential
Design & develop modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies
Contribute to an open-source technology - https://github.com/finos/legend
Drive adoption of cloud technology for data processing and warehousing
Engage with data consumers and producers in order to design appropriate models to suit enable the business
Relevant Technologies
: Java, Python, AWS, React
Basic Qualifications
A Bachelor or Master degree in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)
2-7+ years of relevant work experience in a team-focused environment
2-7+ years of experience in distributed system design
2-7+ years of experience using Java, Python, and/or React
2-7+ years of experience or interest in functional programming languages
Strong object-oriented design and programming skills and experience in OO languages (Java)
Strong experience with cloud infrastructure (AWS, Azure, or GCP) and infrastructure as code (Terraform, CloudFormation, or ARM templates).
Proven experience applying domain driven design to build complex business applications
Deep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes
In-depth knowledge of relational and columnar SQL databases, including database design
Expertise in data warehousing concepts (e.g. star schema, entitlement implementations, SQL v/s NoSQL modeling, milestoning, indexing, partitioning)
Experience in REST and/or GraphQL
Experience in creating Spark jobs for data transformation and aggregation
Comfort with Agile operating models (practical experience of Scrum / Kanban)
General knowledge of business processes, data flows and the quantitative models that generate or consume data
Excellent communications skills and the ability to work with subject matter experts to extract critical business concepts
Independent thinker, willing to engage, challenge or learn
Ability to stay commercially focused and to always push for quantifiable commercial impact
Strong work ethic, a sense of ownership and urgency
Strong analytical and problem solving skills
Establish trusted partnerships with key contacts and users across business and engineering teams
Preferred Qualifications
Financial Services industry experience
Experience with Pure / Legend / Alloy
Working knowledge of open-source tools such as AWS lambda, Prometheus
About Goldman Sachs
At Goldman Sachs, we commit our people, capital and ideas to help our clients, shareholders and the communities we serve to grow. Founded in 1869, we are a leading global investment banking, securities and investment management firm. Headquartered in New York, we maintain offices around the world.
We believe who you are makes you better at what you do. We're committed to fostering and advancing diversity and inclusion in our own workplace and beyond by ensuring every individual within our firm has a number of opportunities to grow professionally and personally, from our training and development opportunities and firmwide networks to benefits, wellness and personal finance offerings and mindfulness programs. Learn more about our culture, benefits, and people at GS.com/careers.
We’re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https://www.goldmansachs.com/careers/footer/disability-statement.html
© The Goldman Sachs Group, Inc., 2023. All rights reserved.
Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Veteran/Sexual Orientation/Gender Identity
Show more
Show less","Java, Python, AWS, React, Git, Scrum, Kanban, CloudFormation, ARM templates, Terraform, REST, GraphQL, Spark, SQL, NoSQL, Git, Data modeling, Machine Learning, Legend, Alloy, Pure, Prometheus, Lambda","java, python, aws, react, git, scrum, kanban, cloudformation, arm templates, terraform, rest, graphql, spark, sql, nosql, git, data modeling, machine learning, legend, alloy, pure, prometheus, lambda","alloy, arm templates, aws, cloudformation, datamodeling, git, graphql, java, kanban, lambda, legend, machine learning, nosql, prometheus, pure, python, react, rest, scrum, spark, sql, terraform"
Insurance - Data Analyst - REMOTE,Wahve LLC,"Dallas, TX",https://www.linkedin.com/jobs/view/insurance-data-analyst-remote-at-wahve-llc-3785427375,2023-12-17,Arlington,United States,Mid senior,Remote,"Put your Insurance Experience to work - FROM HOME!
At
Wahve
, we value significant insurance experience and want to revolutionize the way people think about
phasing into
retirement
by offering qualified candidates the opportunity to continue their career working from home. As we say -
retire from the office but not from work
. Our unique platform provides you with
real
work/life balance and allows you to customize your own work schedule while continuing to utilize your insurance expertise in
a remote, long-term position
.
What You’ll Love About Wahve
We created a welcoming place to work with friendly and professional leadership. We are known for the great care we take with our staff and our clients. We are passionate and determined about delivering the best customer service, preserving insurance industry knowledge, and making a difference by the work that we do.
What We Are Seeking
We have assignments available to help our
insurance industry
clients in
Data Analyst positions. Responsibilities include:
Build and maintain data warehouse, new reports, and ad hoc reports.
Work with user groups to identify reporting issues/enhancements and document business requirements.
Will serve as a member of a project team and/or work independently on projects.
Support and train internal users as needed.
Compile and prepare data for customer analysis.
Experience in C#, Visual Studio, JavaScript, CSS, and current web technologies such as .NET, ASP, JSON, and XML.
Experience with ANY of the following technologies: SQL Server Reporting Services (SSRS), SSIS Reporting, Power BI, Dynamics CRM, Dynamics GP, Share point, Excel, Power Query, Power Pivot.
Ability to compile data results and author commentary on industry studies is a plus.
Insurance or financial services industry experience required.
TO BECOME A WORK-AT-HOME VINTAGE EXPERT, WE REQUIRE
25 years of full-time work experience
Experience working in a data analysis role in the insurance or financial services industry - required
Benefits Of Becoming a Wahve Vintage Expert
Retire from the office but not from work.
Eliminate the office stress and the commute.
Choose the work you would like to do now.
Customize your schedule - full or part time.
Continue to earn an income.
Utilize your years of insurance industry knowledge.
Be part of our dynamic yet virtual team environment and connect with other experienced insurance professionals like yourself!
How To Get Started
Click
APPLY NOW
to complete our simple preliminary profile. Be sure to include your preferred contact information as one of our Qualification Specialists will connect with you promptly.
WE LOOK FORWARD TO MEETING YOU!
Show more
Show less","Insurance, Data Analysis, C#, Visual Studio, JavaScript, CSS, .NET, ASP, JSON, XML, SQL Server Reporting Services (SSRS), SSIS Reporting, Power BI, Dynamics CRM, Dynamics GP, SharePoint, Excel, Power Query, Power Pivot","insurance, data analysis, c, visual studio, javascript, css, net, asp, json, xml, sql server reporting services ssrs, ssis reporting, power bi, dynamics crm, dynamics gp, sharepoint, excel, power query, power pivot","asp, c, css, dataanalytics, dynamics crm, dynamics gp, excel, insurance, javascript, json, net, power pivot, power query, powerbi, sharepoint, sql server reporting services ssrs, ssis reporting, visual studio, xml"
Licensed Civil Engineer - Data Center (Remote),Olsson,"Dallas, TX",https://www.linkedin.com/jobs/view/licensed-civil-engineer-data-center-remote-at-olsson-3784206324,2023-12-17,Arlington,United States,Mid senior,Remote,"Company Description
We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As a Licensed Civil Engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Qualifications
You are passionate about:
Working collaboratively with others
Having ownership in the work you do
Using your talents to positively affect communities
Solving problems
Providing excellence in client service
You bring to the team:
Strong communication skills
Ability to contribute and work well on a team
Bachelor's Degree in civil engineering
At least 6 years of related civil engineering experience
Proficient in Civil 3D software
Must be a registered professional engineer
Additional Information
Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
Engage in work that has a positive impact in communities
Receive an excellent 401(k) match
Participate in a wellness program promoting balanced lifestyles
Benefit from a bonus system that rewards performance
Have the possibility for flexible work arrangements
Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
Show more
Show less","Civil Engineering, Project Management, Design Engineering, Planning Documents, Design Calculations, Quality Assurance, Quality Control, Client Coordination, Civil 3D, Registered Professional Engineer","civil engineering, project management, design engineering, planning documents, design calculations, quality assurance, quality control, client coordination, civil 3d, registered professional engineer","civil 3d, civil engineering, client coordination, design calculations, design engineering, planning documents, project management, quality assurance, quality control, registered professional engineer"
"Sr. Software Engineer, Data (Starlink)",SpaceX,"Hawthorne, CA",https://www.linkedin.com/jobs/view/sr-software-engineer-data-starlink-at-spacex-3715421226,2023-12-17,Compton,United States,Mid senior,Onsite,"SpaceX was founded under the belief that a future where humanity is out exploring the stars is fundamentally more exciting than one where we are not. Today SpaceX is actively developing the technologies to make this possible, with the ultimate goal of enabling human life on Mars.
SR. SOFTWARE ENGINEER, DATA (STARLINK)
At SpaceX, we're leveraging our experience in building rockets and spacecraft to deploy Starlink, the world's most advanced broadband internet system. Starlink is the world's largest satellite constellation and is providing fast, reliable internet to 1M+ users worldwide. We design, build, test, and operate all parts of the system – thousands of satellites, consumer receivers that allow users to connect within minutes of unboxing, and the software that brings it all together. We've only begun to scratch the surface of Starlink's potential global impact and are looking for best-in-class engineers to help maximize Starlink's utility for communities and businesses around the globe.
As a Software Engineer, you will be responsible for developing the strategy, key metrics, tools, software services, and processes for assessing how well key aspects of Starlink are scaling and how effective the Starlink Network is itself, in serving millions of users around the globe. You will work with operators, subsystem responsible engineers, software engineers, and network engineers inside the Starlink organization as well as key contacts with various major external partners to help ensure the growth of this program.
RESPONSIBILITIES:
Build and maintain mission-critical infrastructure, tools, processes, and custom software to objectively assess growth areas for the Starlink program
Automate the aggregation of metrics and detection of widespread application issues across Starlink
Establish and maintain a relationships with key third-party application/content owners
Lead technical investigations about chronic application-level issues
Build ground-based software systems that ingest, transform, and store data
Apply data analytics, models, and techniques to data products created by space vehicles
Create catalogs of data and tools that can be used by you and other teams to perform analytics
Fuse data from multiple sources to create usable information
BASIC QUALIFICATIONS:
Bachelor's degree in computer science, data science, physics, mathematics, or a STEM discipline and 5+ years of professional experience in data engineering; OR 7+ years of professional experience in data engineering in lieu of a degree
Professional experience building solutions with object-oriented software languages including C#, Java, Python, or C++
Professional experience working with in-stream, big data processing and analytics using Apache Kafka, Spark, Flink, or similar
PREFERRED SKILLS AND EXPERIENCE:
Programming experience in Python, C#, Java, Scala, Go or similar languages
Experience with relational and non-relational databases, data lakes e.g. HBase, Hive, Delta Lake, PostgreSQL, CockroachDB or similar
Experience with data exploration tools like Grafana, Jupyter Notebooks, Metabase, PowerBI or similar
Good understanding of version control, testing, continuous integration, build, deployment and monitoring
Experience building predictive models and machine learning pipelines (clustering analysis, prediction, anomaly detection)
Ability to work effectively in a dynamic environment that includes working with changing needs and requirements
Ability to take on projects that require taking initiative and developing new expertise
ADDITIONAL REQUIREMENTS:
Must be willing to work extended hours and weekends as needed
COMPENSATION AND BENEFITS:
Pay range:
Software Engineer/Senior: $160,000.00 - $220,000.00/per year
Your actual level and base salary will be determined on a case-by-case basis and may vary based on the following considerations: job-related knowledge and skills, education, and experience.
Base salary is just one part of your total rewards package at SpaceX. You may also be eligible for long-term incentives, in the form of company stock, stock options, or long-term cash awards, as well as potential discretionary bonuses and the ability to purchase additional stock at a discount through an Employee Stock Purchase Plan. You will also receive access to comprehensive medical, vision, and dental coverage, access to a 401(k)-retirement plan, short & long-term disability insurance, life insurance, paid parental leave, and various other discounts and perks. You may also accrue 3 weeks of paid vacation & will be eligible for 10 or more paid holidays per year. Exempt employees are eligible for 5 days of sick leave per year.
ITAR REQUIREMENTS:
To conform to U.S. Government export regulations, applicant must be a (i) U.S. citizen or national, (ii) U.S. lawful, permanent resident (aka green card holder), (iii) Refugee under 8 U.S.C.
1157, or (iv) Asylee under 8 U.S.C.
1158, or be eligible to obtain the required authorizations from the U.S. Department of State. Learn more about the ITAR here.
SpaceX is an Equal Opportunity Employer; employment with SpaceX is governed on the basis of merit, competence and qualifications and will not be influenced in any manner by race, color, religion, gender, national origin/ethnicity, veteran status, disability status, age, sexual orientation, gender identity, marital status, mental or physical disability or any other legally protected status.
Applicants wishing to view a copy of SpaceX's Affirmative Action Plan for veterans and individuals with disabilities, or applicants requiring reasonable accommodation to the application/interview process should notify the Human Resources Department at (310) 363-6000.
Show more
Show less","Python, C#, Java, Scala, Go, Kafka, Spark, Flink, Grafana, Jupyter Notebooks, Metabase, PowerBI, HBase, Hive, Delta Lake, PostgreSQL, CockroachDB, Data engineering, Machine learning, Clustering analysis, Prediction, Anomaly detection, Version control, Testing, Continuous integration, Build, Deployment, Monitoring","python, c, java, scala, go, kafka, spark, flink, grafana, jupyter notebooks, metabase, powerbi, hbase, hive, delta lake, postgresql, cockroachdb, data engineering, machine learning, clustering analysis, prediction, anomaly detection, version control, testing, continuous integration, build, deployment, monitoring","anomaly detection, build, c, clustering analysis, cockroachdb, continuous integration, data engineering, delta lake, deployment, flink, go, grafana, hbase, hive, java, jupyter notebooks, kafka, machine learning, metabase, monitoring, postgresql, powerbi, prediction, python, scala, spark, testing, version control"
Lead Data Engineer,GoodRx,"Santa Monica, CA",https://www.linkedin.com/jobs/view/lead-data-engineer-at-goodrx-3771704111,2023-12-17,Compton,United States,Mid senior,Onsite,"GoodRx is America’s healthcare marketplace. Each month, millions of people visit goodrx.com to find reliable health information and discounts for their healthcare — and we’ve helped people save $60 billion since 2011. We provide prescription discounts that are accepted at more than 70,000 pharmacies in the U.S., as well as telehealth services including doctor visits and lab tests. Our services have been positively reviewed by Good Morning America, The New York Times, NBC News, AARP, and many others.
Our goal is to help Americans find convenient and affordable healthcare. We offer solutions for consumers, employers, health plans, and anyone else who shares our desire to provide affordable prescriptions to all Americans.
About The Role
GoodRx is looking for extremely smart and innovative data engineers, who are deft at working with a wide variety of languages, such as Python and SQL, a variety of raw data formats, such as parquet and CSV, in a fast-paced and friendly environment. You will collaborate and work with teams across GoodRx to build an outstanding data platform that supports hundreds of data pipelines which move big data accurately and quickly to guide enterprise data decisions.
Responsibilities
Collaborate with product managers, data scientists, data analysts and engineers to define features needed for a data platform
Provide mentorship and technical leadership for a team
Work closely with other engineers to scale infrastructure, improve reliability and efficiency
Improve developer tooling with a focus on reliability and efficiency
Write good technical documentation
Perform large system upgrades and migrations
Maintenance and improvement of multiple CI/CD pipelines
Act as an in-house data expert who makes recommendations regarding standards for code quality and pipeline architecture
Develop, deploy and maintain data processing pipelines using cloud technology such as AWS, Kubernetes, Lambda, Kafka, Airflow, Redshift, S3, Glue, and EMR
Make smart engineering and infra decisions based on data auditing and collaboration
Lead and architect cloud-based data infrastructure solutions to meet stakeholder needs
Skills & Qualifications
8+ years of professional experience in any one of the Cloud providers such as AWS, Azure or GCP
8+ years experience in engineering data pipelines using data technologies (Python, Databricks, pySpark, Kafka) on large scale data sets
Experience building or maintaining a Data Platform that supports multiple engineering teams and processes big data
Ability to quickly learn complex domains and new technologies
Innately curious and organized with the drive to analyze data to identify deliverables, anomalies and gaps and propose solutions to address these findings
Experience designing data models that have been implemented in production
Strong experience in writing complex SQL and ETL development with experience processing large data sets
Familiarity with AWS services (Redshift, RDS, EKS, S3, EMR, Glue, Lambda)
Experience using GitHub, Docker, Terraform, CodeFresh, Jira
Experience contributing to full lifecycle deployments with a focus on quality and scalability
Good to Have
Experience with customer data platform tools such as Segment
Experience contributing to full lifecycle deployments with a focus on testing and quality
Experience with data quality processes, data quality checks, validations, data quality metrics definition and measurement
AWS/Kafka/Databricks or similar certifications
At GoodRx, pay ranges are determined based on work locations and may vary based on where the successful candidate is hired. The pay ranges below are shown as a guideline, and the successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, and other relevant business and organizational factors. These pay zones may be modified in the future. Please contact your recruiter for additional information.
San Francisco Office
$180,000.00 - $288,000.00
New York And Seattle Offices
$165,000.00 - $264,000.00
Santa Monica Office
$150,000.00 - $240,000.00
Other Office Locations:
$135,000.00 - $216,000.00
GoodRx also offers additional compensation programs such as annual cash bonuses and annual equity grants for most positions as well as generous benefits. Our great benefits offerings include medical, dental, and vision insurance, 401(k) with a company match, an ESPP, unlimited vacation, ""Take Care of Yourself"" days, 11 paid holidays, and 72 hours of sick leave. GoodRx also offers additional benefits like mental wellness and financial wellness programs, fertility benefits, supplemental life insurance for you and your dependents, company-paid short-term and long-term disability, and more!
We’re committed to growing and empowering a more inclusive community within our company and industry. That’s why we hire and cultivate diverse teams of the best and brightest from all backgrounds, experiences, and perspectives. We believe that true innovation happens when everyone has a seat at the table and the tools, resources, and opportunities to excel.
With that said, research shows that women and other underrepresented groups apply only if they meet 100% of the criteria. GoodRx is committed to leveling the playing field, and we encourage women, people of color, those in the LGBTQ+ communities, and Veterans to apply for positions even if they don’t necessarily check every box outlined in the job description. Please still get in touch - we’d love to connect and see if you could be good for the role!
GoodRx is America's healthcare marketplace. The company offers the most comprehensive and accurate resource for affordable prescription medications in the U.S., gathering pricing information from thousands of pharmacies coast to coast, as well as a telehealth marketplace for online doctor visits and lab tests. Since 2011, Americans with and without health insurance have saved $60 billion using GoodRx and million consumers visit goodrx.com each month to find discounts and information related to their healthcare. GoodRx is the #1 most downloaded medical app on the iOS and Android app stores. For more information, visit www.goodrx.com.
Show more
Show less","Python, SQL, Parquet, CSV, AWS, Kubernetes, Lambda, Kafka, Airflow, Redshift, S3, Glue, EMR, Databricks, PySpark, GitHub, Docker, Terraform, CodeFresh, Jira, Segment, Salesforce, Tableau, BigQuery, GCP, Azure, Jenkins, Splunk, Hadoop, Hive, Pig, HBase, Oozie, Sqoop, Flume, Chukwa, Cassandra, HDFS, Yarn, ZooKeeper","python, sql, parquet, csv, aws, kubernetes, lambda, kafka, airflow, redshift, s3, glue, emr, databricks, pyspark, github, docker, terraform, codefresh, jira, segment, salesforce, tableau, bigquery, gcp, azure, jenkins, splunk, hadoop, hive, pig, hbase, oozie, sqoop, flume, chukwa, cassandra, hdfs, yarn, zookeeper","airflow, aws, azure, bigquery, cassandra, chukwa, codefresh, csv, databricks, docker, emr, flume, gcp, github, glue, hadoop, hbase, hdfs, hive, jenkins, jira, kafka, kubernetes, lambda, oozie, parquet, pig, python, redshift, s3, salesforce, segment, spark, splunk, sql, sqoop, tableau, terraform, yarn, zookeeper"
Big Data SDET,Software Technology Inc.,"Santa Monica, CA",https://www.linkedin.com/jobs/view/big-data-sdet-at-software-technology-inc-3778237092,2023-12-17,Compton,United States,Mid senior,Onsite,"Hi ,
Hope you are doing safe and healthy!!
We're #hiring. Know anyone who might be interested?
We have an urgent requirement for a
""
Big Data SDET "".
If you are available and interested in this position, then please #share your updated resume at
Raghava.P@stiorg.com
or you can #call me at
609-416-8027 x 127.
Job Title: Big Data SDET
Location: Santa Monica, CA
Duration: Full Time
Job Description
Must have Strong Python Experience along with the Big Data Testing
Basic Qualifications
The DEE Technology team in Santa Monica is seeking a Sr. Software Development in Test Engineer to join our Engineering Services team to build test automation and processes to ensure the quality of our advertising systems. When our systems work properly, they can reliably deliver relevant ads to all of our viewers; driving higher revenue for our business and helping our customers to discover brands and products that are relevant to their interests. Defects in our ad systems can negatively impact revenue and deliver advertisements that detract from the viewer experience due to irrelevance, repetition, and other factors. This is a unique opportunity to join an excellent team and have a meaningful impact on the QE and automation process and culture, as well as the products we release.
Preferred Qualifications
Preferred Qualifications
Responsibilities include:
Work closely with Software Engineers to understand the complex advertising ecosystem in place at DEE Technology
Develop automated test frameworks and suites on UI, API and Integration levels of testing using python or other OO language.
Participate in design discussions for our platform to help evolve the platform in a way that enables richer testing scenarios that simplify defect detection and prevention.
Assist with triage, diagnosis, and resolution of issues discovered across teams.
Contribute to end-to-end acceptance tests
Where necessary, develop and execute manual test cases to detect issues that cannot be detected through automated means
Drive the conversion of manual tests to automated whenever possible
Responsibilities And Duties Of The Role
Summarize job responsibilities, core deliverables and major duties. What is required for the position to exist?
Focus on major areas of work, typically 20% or more of role % of Time
Part of product teams in building architectures which are robust, fault-tolerant, and cloud- native. Builds solutions for problems of sizeable scope and complexity that have been successfully deployed to customers/users. Influences and drives software engineering best practices within the team 25%
Technically lead and deliver multiple projects utilizing an Agile methodology while reviewing team member’s code. Participates in developing technical and/or business approaches; and new/enhanced technical tools. 25%
Owns the design of software programs or systems within the team, and within the organization. Writes codes that establishes and enhances frameworks. Reviews code for the design, testability and clear usability. Builds solutions that scale and perform. Identifies opportunities to improve the system/product/services with each iteration. 50%
Required Education, Experience/Skills/Training
Minimum and Preferred. Inclusive of Licenses/Certs (include functional experience as well as behavioral attributes and/or leadership capabilities)
Minimum of 4 years of hands-on software test development experience, including both functional and non-functional test development
Passion around driving best practices in the testing space
Proficiency with Python or other OO language
Knowledge of software engineering practices and agile approaches
Strong desire for establishing and improving product quality.
Experience building or improving test automation frameworks .
Proficiency CICD integration and pipeline development in Jenkins, Spinnaker or other similar tools.
Proficiency in UI automation ( Selenium, Robot, Watir)
Experience in Gherkin ( BDD /TDD )
Willingness to take challenges head on while being part of a team
B.S. in Computer Science (or equivalent degree or work experience)
Required Education
BA/BS Degree
Thanks,
Raghava Sharma
Sr. US IT Recruiter
Direct:
609-416-8027 X 127
Email:
raghava.p@stiorg.com |
Web:
www.stiorg.com
Linkedin
:
https://www.linkedin.com/in/raghava-sharma-04641b5a/
100 Overlook Center, Suite 200
Princeton, NJ 08540.
Show more
Show less","Big Data, SDET, Python, UI, API, Agile, Jenkins, Spinnaker, Selenium, Robot, Watir, Gherkin, BDD, TDD, CICD","big data, sdet, python, ui, api, agile, jenkins, spinnaker, selenium, robot, watir, gherkin, bdd, tdd, cicd","agile, api, bdd, big data, cicd, gherkin, jenkins, python, robot, sdet, selenium, spinnaker, tdd, ui, watir"
Principal Data Engineer,Freestar,"Los Angeles, CA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-freestar-3595100324,2023-12-17,Compton,United States,Mid senior,Onsite,"Principal Engineer - Data Platform (Remote - US /Canada based W2 position)
You will be owning the entire data strategy and Data Platform of Freestar that interfaces with multiple products such as Pubfig, Prebid Server and Mediation Platform. At Freestar, the Data Platform team manages real time streaming for event processing and batch processing for aggregation (roll-up) and data ingestion using google managed technologies such as PubSub, Cloud dataflow, Cloud Composer, Bigquery, etc. You will be mentoring and leading a 6-8 member team and closely working with an engineering manager to deliver the data engineering technical roadmaps and supporting data system monitoring, cost optimization and performance.
Pathway to Success
Take the needs and challenges of the business requirements and formulate the technical roadmap and technology solution that will support their business strategies and goals.
Provide architectural recommendations, solution and approach given tradeoffs and ability to communicate that to the business.
Quickly gain an understanding of the landscape of tools and data frameworks so as to recommend next steps, approach (such as real-time streaming, batch, workflows, etc.)
Credentialize roadmap and architecture
Enhance Data Engineering capability through coaching, mentoring and leadership
Co-create and shape strategy and approach to engagements to achieve the desired business outcomes
Collaborate with cross departmental team in the org to learn and share best practices and techniques
Provide technical leadership in an enterprise environment to ensure delivery of exceptional technical solutions.
Mentor on approach and execution of solutions, coach on technologies and establishing a team-wide comprehension of solution capabilities and direction.
Ensure technical expectations of deliverables are met.
Maintaining strong expertise and knowledge of current and emerging technologies and products.
Qualifications *
You are equally happy coding as leading a team to implement a solution!
You have a track record of innovation and execution in Data Engineering
You have a deep understanding of data modeling and experience with data engineering and have built large-scale data pipelines and data-centric applications using Big Data tooling platforms such as Kafka, Pub/Sub, and Cloud Dataflow, Apache Airflow, Apache Beam, Hadoop, Spark, Hive, Kinesis, Redshift, S3 and/or HDFS, BigQuery, Dataprep, Composer, etc.
You have demonstrated experience designing and implementing complex data pipelines in a cloud environment such as GCP, AWS or Azure.
Hands-on experience with event streaming with modern event streaming tooling like Google Dataflow, Pub/Sub, Kafka, Kinesis, Glue, etc
Understanding of when streaming vs. batch processing is appropriate, and tradeoffs in a given context
Hands-on experience with MPP query engines like Presto, BigQuery, and Spark SQL.
You are comfortable applying data security strategy to solve business problems
You are able to contrast the use of managed services vs custom built ones
""Good to Have"" experience
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Big Table, etc)
Experience with schema design, dimensional data modeling and data engineering concepts.
Experience with analytics stacks include Looker, Tableau, Redash, etc
Experience designing, building, and maintaining data processing systems
Previous background in the ad tech or media landscape (linear, digital, or social) is a plus
What You Can Expect In Return
Full-Time, Salaried Position
Working remotely
Generous Medical, Dental, and Vision benefits
401K with company match, vested immediately
The opportunity to be part of something high value, high impact, and high growth...Who doesn't love that!
Freestar is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
Show more
Show less","Data strategy, Data Platform, Realtime streaming, Event processing, Batch processing, Data aggregation, Data ingestion, PubSub, Cloud Dataflow, Cloud Composer, Bigquery, Hadoop, Spark, Hive, Kinesis, Redshift, S3, HDFS, Dataprep, Presto, BigQuery, Spark SQL, Google Cloud data products, CloudSQL, Spanner, Cloud Storage, Big Table, Looker, Tableau, Redash, Data processing systems, Ad tech, Media landscape","data strategy, data platform, realtime streaming, event processing, batch processing, data aggregation, data ingestion, pubsub, cloud dataflow, cloud composer, bigquery, hadoop, spark, hive, kinesis, redshift, s3, hdfs, dataprep, presto, bigquery, spark sql, google cloud data products, cloudsql, spanner, cloud storage, big table, looker, tableau, redash, data processing systems, ad tech, media landscape","ad tech, batch processing, big table, bigquery, cloud composer, cloud dataflow, cloud storage, cloudsql, data aggregation, data ingestion, data platform, data processing systems, data strategy, dataprep, event processing, google cloud data products, hadoop, hdfs, hive, kinesis, looker, media landscape, presto, pubsub, realtime streaming, redash, redshift, s3, spanner, spark, spark sql, tableau"
Associate Data Analyst,Plenty®,"Compton, CA",https://www.linkedin.com/jobs/view/associate-data-analyst-at-plenty%C2%AE-3766013381,2023-12-17,Compton,United States,Mid senior,Onsite,"About Plenty
Plenty is on a mission to make fresh, pesticide-free fruits and vegetables more accessible to everyone. Our cutting-edge indoor, vertical farms produce extraordinary yields using a fraction of the land and water outdoor farms need. Nutrient-rich, mouth-watering and locally grown, our crops help people, communities and our planet to be healthier and more resilient.
The people and teams who make up Plenty are at the core of how we do this. We are a collaborative and innovative bunch. We are passionate about our contributions to Plenty, the communities around us and our planet, but never take ourselves too seriously. We’re down to Earth, we grow together, and we always find a way. Most importantly, we are growing for a better tomorrow, today.
About The Role
Plenty is looking for an Associate Data Analyst to join our Farm Operations team that will work fully on-site in our office. In this role, you will engage in data analysis activities in a modern, high-technology vertical farm. You will play a pivotal role making data accessible to drive innovation, optimize processes, and empower data-informed decision-making. In a dynamic environment, you will identify insights from datasets, leveraging your analytical prowess to shape the future of our organization. Your work will directly impact the scalability of sustainable farming in a fast-paced, dynamic environment.
What You'll Do
Gather, clean, and analyze datasets to extract insights
Develop and maintain data visualizations and reports to effectively communicate findings from our production sites to stakeholders
Collaborate with cross-functional teams to identify business opportunities, optimize processes through data-driven solutions
Create and maintain data pipelines and workflows, ensuring data accuracy, consistency, and availability
Other duties as assigned. Schedules will be reviewed periodically and are subject to change based on business needs
What You'll Bring To The Table
Familiarity with data analysis
Versatile thinker with a proven track of being oriented and driving data-driven decision-making
Excellent attention-to-detail
Experience in spreadsheet software and data entry (Google Sheets or Excel)
Experience in presenting and communicating data and results
Exceptionally collaborative approach with excellent communication skills
Type of role
Full-Time; Onsite
What We Can Offer
Compensation: we’ll pay you a competitive wage on a biweekly basis, give you a stake in the company via equity and contribute toward your 401K
Health & well-being: we’ll invest in your physical and mental well-being with comprehensive medical, dental, & vision benefits from day one, and an Employee Assistance Program to help with mental health, financial & legal matters
Food: in-office lunches, snacks and access to our leafy greens & new produce in development -- are you up for a side gig as a taste tester?
Grow together: learning & development opportunities including unlimited access to our Upstart University platform, LinkedIn Learning, lunch & learns, training certifications, and more
Equipment: we’ll provide you with the equipment you need to be successful, including $75/month toward your home internet and phone bills and $250 every 2 years to upgrade your phone
Paid time off: we know you’ll do your best if you’re taking time to recharge, so we offer 2 weeks of paid time off + 6 days of paid sick leave per year in addition to the paid holidays we observe
Parental leave: 12 weeks off for all new parents - fully paid so you can focus your energy on your newest addition
Food & Safety Requirements
Comply with all Plant, State and Federal OSHA, EPA, FDA, HACCP, and SQF regulations
All applicants must be 18 years or older to apply
Comply with GMP/GAPS in the Facility
Conduct monitoring activities to assure finished product comply with food safety and quality specifications
Assist in the maintenance of Food Safety and Quality system
Plenty’s target compensation range for this role is $31.39/hr - $34.62/hr for this location. Actual compensation is commensurate with experience and alignment with role requirements.
Show more
Show less","Data analysis, Data visualization, Data reporting, Data cleaning, Data mining, Data pipelines, Data workflows, Datadriven decisionmaking, Spreadsheet software, Data entry, Google Sheets, Microsoft Excel, Data communication, Data presentation, Collaborative approach, Communication skills, Upstart University platform, LinkedIn Learning","data analysis, data visualization, data reporting, data cleaning, data mining, data pipelines, data workflows, datadriven decisionmaking, spreadsheet software, data entry, google sheets, microsoft excel, data communication, data presentation, collaborative approach, communication skills, upstart university platform, linkedin learning","collaborative approach, communication skills, data cleaning, data communication, data entry, data mining, data presentation, data reporting, data workflows, dataanalytics, datadriven decisionmaking, datapipeline, google sheets, linkedin learning, microsoft excel, spreadsheet software, upstart university platform, visualization"
"Senior Software Engineer, Data Product","Enigma Technologies, Inc.","New York, NY",https://www.linkedin.com/jobs/view/senior-software-engineer-data-product-at-enigma-technologies-inc-3736678731,2023-12-17,Eastchester,United States,Mid senior,Remote,"The Opportunity:
Join Enigma at a pivotal moment as we continue to provide valuable solutions for small businesses. We're seeking an experienced Data Product Engineer to join our team and help develop and build the iteration of small business data products . Your work will directly impact the accuracy of small business profiles, which influence decisions for companies that employ half the U.S. workforce!
The Role:
As a Data Product Engineer, you will design and develop data products that solve critical customer pain points. Your impact will be measured by your ability to deliver scalable, high-quality data products that customers love. To succeed in this role, you will bring together three distinct capabilities
Understand acute customer needs and extract common problem structures across customers
Analyze and extract value from data at scale
Build efficient, maintainable production-grade data pipelines
We are looking for someone who:
Operates with a bias for action and knows how to deliver value in the short, medium and long term
Loves talking to customers and works hard to solve their problems in a repeatable way
Adopts a principled, metrics-driven approach to difficult data problems and demonstrates excellence in their analytics and engineering craft
Operates transparently, collaboratively and with low ego—loves learning from others and having their ideas questioned and challenged
What Makes This Job Exciting:
Impact: Develop products that take an innovative data-first approach to solving high-value customer problems.
Technical Challenge: Tackle complex data and engineering problems while balancing customer impact, reliability, scalability, data quality, and an ambitious forward development plan.
Ownership: You'll work directly with customers. You and your teammates will design and build products based on your learnings .
Bonus Points If You:
Have experience building data products at scale.
Bring prior experience in Databricks or Spark
Have worked on data products in the marketing, kyb or credit underwriting space.
About Us:
At Enigma, we're building the single, most reliable source of data on businesses to power the future of financial services. By engineering better data from hundreds of public and third-party sources, we aim to tell the complete story of every business, so that companies of every size can access the financial services they need to grow and thrive. Our core values – generosity, curiosity, ingenuity, & drive – guide everything we do, from how we make our most important product decisions to how we work with and support one another on a daily basis. We're a team of curious, driven individuals with diverse backgrounds and skills, but we're all passionate about engineering deeper understanding through data—together. If this resonates, we would love to hear from you!
We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.
Salary Range: $160,000-$210,000
A note on salary ranges: we pride ourselves on paying competitively for our size and industry. Salary is one piece of a total Enigma compensation package that includes additional benefits and opportunities. All of our compensation packages include equity because we believe 100% of Enigma employees should have the option to purchase ownership in the company and benefit from the value we're creating together
Show more
Show less","Data Product Engineering, Data Pipelines, Spark, Databricks, Data Quality, Analytics, Scalability, Customer Problem Solving, Customer Pain Points, Data Products","data product engineering, data pipelines, spark, databricks, data quality, analytics, scalability, customer problem solving, customer pain points, data products","analytics, customer pain points, customer problem solving, data product engineering, data products, data quality, databricks, datapipeline, scalability, spark"
"Senior Data Analyst, Product - Remote | WFH",Get It Recruit - Information Technology,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-analyst-product-remote-wfh-at-get-it-recruit-information-technology-3776622784,2023-12-17,Eastchester,United States,Mid senior,Remote,"We foster a vibrant creative environment, empowering music enthusiasts with the tools to bring their artistic visions to life. Boasting an extensive catalog of licensed samples, cutting-edge AI technology, and affordable plugins and DAWs, we propel sound discovery, inspire creativity, and supercharge the output of music creators worldwide.
How We Work
We champion DISCO – an ethos that embodies collaboration, accountability, and unity. We prioritize being Direct, Inclusive, Splice Together, Creator Centric, and Optimistic. Our collective success hinges on our ability to support one another, collaborate effectively, and communicate transparently. Embracing flexibility and a unified approach, we navigate challenges with resilience.
Our company embraces a culture of remote work, bringing together colleagues from across the US and the UK. To foster team synergy, we engage in regular communication through Town Halls, departmental All Hands, and virtual gatherings.
When you become part of our company, you join a network of colleagues, peers, and collaborators. Are you ready?
JOB TITLE: Senior Data Analyst (Product)
LOCATION: Remote
The Role
As a Senior Data Analyst embedded in our product team, you will play a pivotal role in shaping the future of our company. Your primary responsibility is to ensure that decisions regarding product development are informed by robust and accurate data insights. In this senior-level role within a small team, you will act as a ""data translator,"" utilizing your expertise to convert product goals into actionable metrics and deliver analytics results that guide key decisions.
What You'll Do
Collaborate with the product team to provide insights and develop reusable or ad hoc data views that influence product strategy, design, and development.
Translate complex product questions into actionable analytics-driven metrics.
Design, develop, and maintain dashboards and reports using SQL, Looker, or scripting languages, ensuring real-time visibility into product performance.
Evaluate the success metrics of product launches, features, and the outcomes of A/B tests.
Communicate the business implications of our product decisions to relevant stakeholders.
Investigate, highlight, and propose fixes for gaps or errors in our analytics events, and recommend potential additions to enhance product insights.
Job Requirements
Demonstrated experience with BI platforms (e.g., Looker, Tableau, PowerBI, Qlik).
Strong knowledge of statistical concepts, including trend analysis, hypothesis testing, and confidence intervals.
Advanced SQL ability: Comfortable with advanced concepts, including CTEs, UDFs, and managing query costs across large datasets.
Basic proficiency in scripting languages such as Python or R to automate tasks and conduct advanced analyses.
Previous experience working within a product organization, with a solid understanding of product development dynamics.
Nice To Haves
As a company serving musicians and producers, some knowledge of the music-production process is an asset. If this topic is new to you, that's okay—you should be open to learning about it.
The national pay range for this role is $132,500 - $157,500. Individual compensation will be commensurate with the candidate's experience.
Our company is an Equal Opportunity Employer. We provide equal employment opportunities to all employees and applicants for employment, prohibiting discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.
Employment Type: Full-Time
Show more
Show less","Data Analyst, Data Insights, Data Visualization, Business Intelligence, Product Strategy, Statistical Concepts, SQL, Looker, Scripting Languages, Python, R, Dashboards, Reporting, Product Development, Music Production","data analyst, data insights, data visualization, business intelligence, product strategy, statistical concepts, sql, looker, scripting languages, python, r, dashboards, reporting, product development, music production","business intelligence, dashboard, data insights, dataanalytics, looker, music production, product development, product strategy, python, r, reporting, scripting languages, sql, statistical concepts, visualization"
Data Engineer,"U.S. Venture, Inc.","Appleton, WI",https://www.linkedin.com/jobs/view/data-engineer-at-u-s-venture-inc-3774214590,2023-12-17,Wisconsin,United States,Associate,Onsite,"POSITION SUMMARY
Our People Analytics team is expanding! The Data Engineer I - People Analytics, is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.
This position is based in Appleton, WI.
JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.
QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming
2+ years of experience in utilizing Python or PySpark or Scala for data engineering and transformation
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated
#LI-Onsite
Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101.
Show more
Show less","Data Warehouse, Cloud Storage, Data Analytics, ETL, Python, Microsoft SQL Server Integration Services (SSIS), SQL, PySpark, Scala, AWS, Azure, Google Cloud, Power BI, Tableau, MicroStrategy, Business Objects, DAX, Power Query","data warehouse, cloud storage, data analytics, etl, python, microsoft sql server integration services ssis, sql, pyspark, scala, aws, azure, google cloud, power bi, tableau, microstrategy, business objects, dax, power query","aws, azure, business objects, cloud storage, dataanalytics, datawarehouse, dax, etl, google cloud, microsoft sql server integration services ssis, microstrategy, power query, powerbi, python, scala, spark, sql, tableau"
Sr. Edge Data Center Engineer - Remote | WFH,Get It Recruit - Information Technology,"Oshkosh, WI",https://www.linkedin.com/jobs/view/sr-edge-data-center-engineer-remote-wfh-at-get-it-recruit-information-technology-3782246339,2023-12-17,Wisconsin,United States,Mid senior,Remote,"Join our dynamic team of technology innovators! We're looking for a passionate and experienced Edge Data Center Engineer to design, implement, and support our distributed network of data centers across the enterprise.
About The Role
In this mid-level position, you'll be a key player in keeping our edge infrastructure running smoothly, ensuring optimal performance and reliability for critical business operations. You'll work closely with the Digital Technology organization and collaborate with various teams like server, storage, field services, and facilities to optimize efficiency and maintain the highest standards.
What You'll Do
Be the technical go-to: Provide day-to-day support for production sites and systems, troubleshoot operational issues, and offer proactive recommendations for improvement.
Become the infrastructure guru: Design and maintain site drawings, assess power, space, and air handling requirements, and manage facility monitoring solutions.
Master the tech: Configure and manage server, storage, and network infrastructure, including LAN, WAN, and Wi-Fi systems.
Champion collaboration: Lead projects involving technology components of new site deployments, refreshes, and migrations. Work effectively with diverse teams to build strong relationships across the organization.
Always stay ahead: Actively seek opportunities to automate tasks and implement best practices through scripting and automation tools.
Be the problem solver: Resolve incidents, coordinate third-party support, and oversee external technology vendors.
Own your success: Participate in an on-call rotation to provide 24/7 support and contribute to achieving business outcomes.
What You Bring
Minimum 3 years of experience in designing, implementing, and supporting edge data centers.
Solid understanding of server room power, space, and air handling requirements.
Proven ability to detect and troubleshoot server room operational issues.
Optional: Industry certifications, experience with scripting tools, and knowledge of edge technologies (VMware, Windows Server, Cisco networking).
Bonus Points
Excellent communication and interpersonal skills.
Strong understanding of the business impact of IT.
Ability to travel domestically and internationally (20%-30%).
Why Join Us?
Make a real impact: Drive the success of our critical infrastructure and enable seamless operations across the enterprise.
Develop your skills: Work with cutting-edge technologies and expand your knowledge in a dynamic and collaborative environment.
Join a winning team: Be part of a passionate and dedicated team focused on innovation and excellence.
Enjoy flexibility: This role offers a remote-friendly work environment with on-site collaboration opportunities.
Ready to become the hero of our distributed data center network? Apply today
Employment Type: Full-Time
Show more
Show less","Edge Data Center Design, Implementation, Support, Production Site Support, Systems Troubleshooting, Server Configuration, Storage Infrastructure, Network Infrastructure, LAN, WAN, WiFi, Project Management, Site Deployment, Refresh, Migration, Automation, Scripting, Problem Solving, ThirdParty Support, VMware, Windows Server, Cisco Networking, Communication, Interpersonal Skills, Business Impact of IT, Domestic and International Travel","edge data center design, implementation, support, production site support, systems troubleshooting, server configuration, storage infrastructure, network infrastructure, lan, wan, wifi, project management, site deployment, refresh, migration, automation, scripting, problem solving, thirdparty support, vmware, windows server, cisco networking, communication, interpersonal skills, business impact of it, domestic and international travel","automation, business impact of it, cisco networking, communication, domestic and international travel, edge data center design, implementation, interpersonal skills, lan, migration, network infrastructure, problem solving, production site support, project management, refresh, scripting, server configuration, site deployment, storage infrastructure, support, systems troubleshooting, thirdparty support, vmware, wan, wifi, windows server"
Data Analyst Lead,Northwestern Mutual,"Wisconsin, United States",https://www.linkedin.com/jobs/view/data-analyst-lead-at-northwestern-mutual-3765980808,2023-12-17,Wisconsin,United States,Mid senior,Remote,"At Northwestern Mutual, we are strong, innovative and growing. We invest in our people. We care and make a positive difference.
Primary Duties & Responsibilities
Leads expert level quantitative and qualitative data analysis and reporting of patterns, insights, and trends to decision-makers in order to drive business decisions and address business questions
Creates and is accountable for high quality, value driven work that drives business outcomes; places the user in the center of decision making; and collaborates with team for speed, agility and innovation in work
Delivers results to Business Clients and Sr. Dept Leaders
Delivers results, insights and recommendations (written, verbal, presentations, etc) to business audience to support business decisions
Produce actionable reports that show key performance indicators, identify areas of improvement into current operations, display root cause analysis of problems, and recommend solutions
Deep expertise in a least one business area or domain, with a broad understanding of the business and domains surrounding the main focus. Ability to articulate this knowledge as an expert at the enterprise level and may be called on to provide insight at the senior leadership level to provide detailed analysis.
Seeks to understand business process, user tasks, and as necessary, captures refined process documents
Shares best practices with peers through coaching and mentoring around core methodologies, patterns, standards and processes. Contribute to the development there of.
Takes initiatives to design and develop deliverables based on interpretation of findings and business client needs on a wide range of highly complex analytical topics
Provides consultation to business clients (including senior leaders) and participates in enterprise-wide teams to address business issues
Identifies & captures business requirements, develops KPI frameworks
Perform business use case opportunity identification & sizing
Regularly applies new perspectives, creative problem solving, and inter-departmental connections to improve enterprise analytical capabilities
Actively promote learning & curiosity.
Access, gather, and analyze data from multiple internal and external sources to drive insights into complex business problems, decisions, and performance. Creates and produces forecasts, reports, dashboards, etc. to tell a story through data
Leads the creation of forecasts, recommendations and strategic/tactical plans based on business data and market knowledge. Utilizes analytics and metrics to improve processes and provide data-driven forecasts that impact the business (costs, risks, etc).
Leads value driven work that drives business outcomes; places the user in the center of decision making; and collaborates with team for speed, agility and innovation in work.
Qualifications
Master's degree in computer science, MIS, math, statistics, business or related field.
At least 6 years of professional experience
At least 5 years of experience building analytics models, visualizations and delivering insights
Ability to provide design direction
Ability to apply data visualization best practices to work deliverables.
7+ years of relevant experience with a proven track record in research and data analysis, application of statistical research techniques, report or application development that support business decisions/outcomes.
Expertise in development and analytical tools such as SAS, SQL, MS Excel, SPSS or other tool
Visualization tooling like PowerBI or Tableau
Compensation Range
Pay Range - Start:
$89,040.00
Pay Range - End
$165,360.00
Northwestern Mutual pays on a geographic-specific salary structure and placement in the salary range for this position will be determined by a number of factors including the skills, education, training, credentials and experience of the candidate; the scope, complexity as well as the cost of labor in the market; and other conditions of employment. At Northwestern Mutual, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. Please note that the salary range listed in the posting is the standard pay structure. Positions in certain locations (such as California) may provide an increase on the standard pay structure based on the location. Please click here for additional information relating to location-based pay structures.
Grow your career with a best-in-class company that puts our client’s interests at the center of all we do. Get started now!
We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.
If you work or would be working in California, Colorado, New York City, Washington or outside of a Corporate location, please click here for information pertaining to compensation and benefits.
FIND YOUR FUTURE
We’re excited about the potential people bring to Northwestern Mutual. You can grow your career here while enjoying first-class perks, benefits, and commitment to diversity and inclusion.
Show more
Show less","SAS, SQL, MS Excel, SPSS, PowerBI, Tableau, Data Analysis, Quantitative Analysis, Qualitative Analysis, Reporting, Data Visualization, Business Intelligence, Statistical Analysis, Research, Analytical Modeling, Data Mining, Machine Learning, Artificial Intelligence, Business Process Management, Project Management, Communication, Presentation Skills, Problem Solving, Critical Thinking, Decision Making, Teamwork, Collaboration, Innovation, Creativity","sas, sql, ms excel, spss, powerbi, tableau, data analysis, quantitative analysis, qualitative analysis, reporting, data visualization, business intelligence, statistical analysis, research, analytical modeling, data mining, machine learning, artificial intelligence, business process management, project management, communication, presentation skills, problem solving, critical thinking, decision making, teamwork, collaboration, innovation, creativity","analytical modeling, artificial intelligence, business intelligence, business process management, collaboration, communication, creativity, critical thinking, data mining, dataanalytics, decision making, innovation, machine learning, ms excel, powerbi, presentation skills, problem solving, project management, qualitative analysis, quantitative analysis, reporting, research, sas, spss, sql, statistical analysis, tableau, teamwork, visualization"
Data Scientist,Emerald Resource Group,"Lake County, OH",https://www.linkedin.com/jobs/view/data-scientist-at-emerald-resource-group-3787375093,2023-12-17,Cleveland Heights,United States,Mid senior,Onsite,"Data Scientist - Supply Chain
Direct Hire - Salary ( 100-115K)
In office - You must live in NE Ohio
The company will not pay for relocation and prefers local talent
You must be a current US Citizen or Green Card Holder
Data Scientist
Attention exceptional
Data Scientists
with experience and knowledge of
Python
,
DevOps
, SQL, R, and ML algorithms !
Emerald Resource Group is currently seeking top talent for a
Data Scientist
position in the Greater Cleveland Area.
Required to be considered:
-US Citizen or Green Card Holder
-Minimum of Bachelor's Degree with a Masters or Ph.D preferred.
-Python, SQL, R, BI reporting experience (PowerBI , Tableau, and Excel are currently used)
-Supply Chain experience as a Data Scientist, Data Analyst, Data Engineer, or similar will be considered first.
Why make the switch?
Our client offers an exceptional opportunity for personal and professional growth. As a
Data Scientist
, you'll be valued for your unique experience, background, and perspective. We know that work-life balance, recognition, and learning and career development are all part of a rewarding career, and we're committed to delivering on those promises.
Objectives of the role:
As a
Data Scientist
, you'll be responsible for leading and participating in projects that utilize large amounts of data to develop recommendations and solutions. This is a supply chain industry role. You'll manage projects of varying complexity, with some being very complex and typically focused on profitability, revenue growth, and/or expense reduction. Working under limited supervision, you'll have the opportunity to make a significant impact on the organization and be recognized for your contributions.
Ready to create a career that no one could have planned for?
Apply now
and join our client's team of top-tier Data Scientists. We're looking forward to hearing from you!
A day in the life:
As a
Data Scientist, you will:
Develop and/or researches, learns, and then applies novel techniques to create solutions for problems
Select optimal approach then develops predictive and prescriptive models to surface insights and drive action
Use statistical testing and experimental design where appropriate
Apply artificial intelligence and machine learning to enhance decisions embedded in business processes
Use effective project planning techniques to break down projects into tasks and ensure deadlines are kept
Define success measures and tracks performance of models
Actively research and implement innovations to new and existing AI/ML models and processes
Develop predictive and prescriptive models of varying complexity including very complex models to surface insight and drive action
Perform regular model maintenance
Initiate then drives projects to completion with minimal guidance
Provide some thought leadership
Gather data from disparate sources and ensures data quality (e.g. correlation, detection of outliers, imputation of missing values, data standardization and deduplication) throughout all stages of collection/acquisition, annotation and processing, normalization and transformation
Actively use Git version control system and has knowledge of DevOps and agile development practices
Must have skillset (specific experiences can be concurrent):
Must have experience in at least 3 of the following: predictive analytics, machine learning, unsupervised learning, optimization, simulation
Proficient and proven experience scripting in a high-level programming language (e.g., Python, R)
Proficient in SQL including experience writing well-structured and efficient queries
Experience working with data from a variety of sources (e.g., databases, JSON, text-based, image, and unstructured data)
Experience with data visualization tools or data visualization within a high-level programming language (e.g., Python, R)
Strong storytelling skill set including significant experience communicating complex technical information to a non-technical audience
Hands on project management experience
High intellectual curiosity with ability to develop new knowledge and skills and use new concepts, methods, digital systems and processes to improve performance
Valid driver’s license and a driving record that conforms to company standards
To succeed in this role, you’ll also need:
Ability to work effectively in an office environment for 40+ hours per week (including sitting, standing, and working on a computer for extended periods of time)
Ability to communicate effectively in a collaborative work environment utilizing various technologies such as: telephone, computer, web, voice, teleconferencing, e-mail, etc.
Ability to travel as required
Ability to operate an automobile within the parameters of the driving policy
Education:
Bachelor's degree with quantitative focus in Econometrics, Statistics, Operations Research, Mathematics, Computer Science, Actuarial Science or related field OR commensurate experience
Job type:
Full-time
For any follow up questions email your Recruiter Tom Gaebelein directly at tom@emeraldresourcegroup.com or call at 440-627-6925.
INTERESTED BUT NOT SURE?
We know the confidence gap can get in the way of meeting spectacular candidates, so please
don’t hesitate to apply
–
we’d love to hear from you!
Looking for a different role?
Check out hot vacancies here
www.emeraldresourcegroup.com
and navigate to our ""Careers"" page.
**WHY EMERALD RESOURCE GROUP**
We are more than just a talent scouting agency. We believe that the key to a company's success is its people, and our mission is to match exceptional candidates with exceptional companies. With over 25 years of experience, we have built a reputation as an
industry leader in IT recruiting
, working with a diverse range of clients from
startups
to
Fortune 500
companies. This means more and better opportunities for you to take your career to the next level.
Choose Emerald Resource Group for excellence in IT recruiting.
Let’s keep in touch!
Twitter: @EmeraldRG
LinkedIn: @Emerald Resource Group
Facebook: @Emerald Resource Group
**FOR HIRING MANAGERS - Are you struggling to find top IT talent for your team?
Let us take the burden off your shoulders. Simply send us the job description for the open positions you're trying to fill, and our expert recruiters will work tirelessly to find the perfect fit for your company. Don't miss out on the opportunity to bring on exceptional IT professionals -
contact us today!
Show more
Show less","Python, DevOps, SQL, R, ML, BI reporting, PowerBI, Tableau, Excel, Supply Chain, Statistics, Experimental design, Artificial intelligence, Machine learning, Project management, Data visualization, Git, Agile development, Predictive analytics, Unsupervised learning, Optimization, Simulation, Econometrics, Operations Research, Mathematics, Computer Science, Actuarial Science","python, devops, sql, r, ml, bi reporting, powerbi, tableau, excel, supply chain, statistics, experimental design, artificial intelligence, machine learning, project management, data visualization, git, agile development, predictive analytics, unsupervised learning, optimization, simulation, econometrics, operations research, mathematics, computer science, actuarial science","actuarial science, agile development, artificial intelligence, bi reporting, computer science, devops, econometrics, excel, experimental design, git, machine learning, mathematics, ml, operations research, optimization, powerbi, predictive analytics, project management, python, r, simulation, sql, statistics, supply chain, tableau, unsupervised learning, visualization"
"Sr Analyst, People Analytics and Data",Pacific Dental Services,"Henderson, NV",https://www.linkedin.com/jobs/view/sr-analyst-people-analytics-and-data-at-pacific-dental-services-3736261537,2023-12-17,Las Vegas,United States,Mid senior,Onsite,"Now is the time to join Pacific Dental Services. You will have opportunities to learn new skills from our team of experienced professionals. If you're ready to take your career to the next level and gain valuable experience, apply today!
The primary responsibility of the Sr. Analyst, People Analytics and Data is to provide analytical expertise identifying, evaluating, and developing strategic analytics for all people systems technology. They are responsible for building the story behind the data to turn into tactical results. The Sr Analyst will be required to use judgment to determine the most appropriate methods to extract and analyze company data and proposed strategic analysis which provides actional intel for internal customers. Reporting to the People System Director, this role sits within the HRIS function and will be the go-to person for all People System reporting and data accuracy. In addition, is responsible to integrate for facilitating and working on projects.
Responsibilities
Analyze people data to generate meaningful business insights for strategic analysis.
Identify/build tools to bring our workforce insights to our NSC leaders and a wide audience to drive actions from people data and insights.
Use Data to help PDS understand our internal workforce, our potential external future workforce and know how we can best manage our team members to drive real business results and value.
Produce and deliver standard (monthly, quarterly, and annual) and customized People reports.
Collaborates with systems team and develops the framework on how People data is structured and how it interacts with systems outside of the People team, while providing recommendations to improve data quality.
Works with various internal customers to adequately define their problems and document business requirements. These will be problems and questions that range from mundane tohighly complex.
Understands the various types of company data and platforms, how it is created and thepotential for bad data.
Develops effective and practical approaches to solving complex problems and takes a new perspective on existing solutions.
Documents requirements, approach, and assumptions. Performs gap analysis by comparing current state to future/desired state.
Develops departmental and executive level reports and presentations to communicatework status and results.
Partners with Data Warehouse team to understand how people data is being captured and used to present an accurate story for people data.
Partners and collaborates with all departments and all levels of management within thecompany and conducts themselves in a highly professional manner.
Will participate on larger projects representing people systems Data & Analytics. May provide project coordination, and coordination of user acceptance testing, including Alphas, Betas, and full Deployment.
Other duties as assigned by Management.
Subject matter expert in development of reports and dashboards.
Qualifications
Bachelor’s degree in business administration, Data Management, Statistics, Decision Science, or related field. In lieu of degree, +5 years of professional experience is required.
4-6 years of proven successful experience performing business analysis.
Minimum 5 years of experience in building and maintaining Advanced Matrix and composite reports and dashboards within Workday Report Writer and minimal intermittent level in core Workday architecture and configuration.
Excellent analytics ability with a focus on analyzing, interpreting, and extracting insights from large amounts of data, and developing recommendations & improvement opportunities.
Configuration experience with Power BI reporting tool
Familiarity with Robotic Process Automation (RPA).
Advanced skills in Office 365.
Preferred
Master’s degree in business administration.
Advanced degree in mathematics, statistics, or a related quantitative field.
Familiarity with inferential statistics.
Certified Workday Reporting PRO.
Advanced Power BI skills.
Knowledge/Skills/Abilities
Proficient in process, system, and financial business analysis.
Develop and maintain SQL, Power BI reporting solutions for cross functional department usage and data management.
Excels at problem solving, probability and statistical analysis.
Ability to multi-task effectively without compromising the quality of the work.
Excellent communication and presentation skills.
Problem solving skills to gather & analyze information to identify and resolve problems in a timely manner arrive at valid conclusions, recommendations, and plans of action.
Detail oriented, organized, process focused, problem solver, self-motivated, proactive, ambitious, customer service focused.
Must have ownership mentality and ability to work independently and as part of a team.
Ability to draw conclusions and make independent decisions with limited information.
Ability to respond to common inquiries from customers, staff, regulatory agencies, vendors, and other members of the business community.
Self-motivated, reliable individual capable of working independently as well as part of a team.
Project management knowledge, SDLC capabilities under SCRUM / Agile methodologies.
Benefits
Medical, dental, and vision insurance
Paid time off
Tuition Reimbursement
401K
Paid time to volunteer in your local community
Pacific Dental Services is an Equal Opportunity Employer. We celebrate diversity and are united in our mission to create healthier and happier team members.
Show more
Show less","Data Analysis, Business Analysis, Reporting, Data Visualization, SQL, Power BI, Workday, Data Management, Process Improvement, Problem Solving, Communication, Presentation Skills, Decision Making, Project Management, Agile Methodologies, SCRUM, Statistical Analysis, Inferential Statistics, Robotics Process Automation (RPA), Office 365","data analysis, business analysis, reporting, data visualization, sql, power bi, workday, data management, process improvement, problem solving, communication, presentation skills, decision making, project management, agile methodologies, scrum, statistical analysis, inferential statistics, robotics process automation rpa, office 365","agile methodologies, business analysis, communication, data management, dataanalytics, decision making, inferential statistics, office 365, powerbi, presentation skills, problem solving, process improvement, project management, reporting, robotics process automation rpa, scrum, sql, statistical analysis, visualization, workday"
Senior Software Engineer - Data Science & Analytics,"Skyworks Solutions, Inc.","Mexicali, Baja California, Mexico",https://mx.linkedin.com/jobs/view/senior-software-engineer-data-science-analytics-at-skyworks-solutions-inc-3769586238,2023-12-17,Calexico,United States,Mid senior,Onsite,"The Data Science & Analytics team is an integral part of Skyworks’ Technology and Manufacturing Group, responsible for advanced technology development, central engineering, quality, manufacturing operations, sourcing, and supply chain. Our mission is to drive digital transformation across all aspects of the business. This is an exceptional opportunity to play a pivotal role in the rapidly evolving semiconductor industry. The Data Science & Analytics team is dedicated to empowering business users with cutting-edge tools and technology to facilitate information discovery and analysis.
Position Summary:
As a Senior Software Engineer within the Data Science & Analytics team, you will play a critical role in supporting, administering, developing, and deploying new and existing data analytics tools and technologies. These tools are designed to facilitate information discovery and analysis, with a primary focus on manufacturing data from both internal and external factories. You will collaborate closely with engineers across functional teams and locations to drive data-driven decision-making processes.
Responsibilities:
In this role, your key responsibilities will include:
Developing, enhancing, testing, and deploying advanced analytics solutions. This involves creating software, scripts, templates, and other tools to continually improve information discovery and analysis capabilities for business users.
Monitoring and administering commercial and internally developed software tools, ensuring their reliability and efficiency.
Providing training and support to business users, both locally and remotely, to ensure they can effectively utilize the analytics tools.
Creating and maintaining user and administrative documentation as needed to support the tools and technologies in use.
Participating in or leading projects focused on the development and deployment of new tools and technologies for information discovery and analysis.
Requirements:
To excel in this role, you should possess:
A Master's or PhD degree in Data Science, Data Analytics, Computer Engineering, Electrical Engineering, or a related field.
A minimum of 3 years of relevant experience, demonstrating your expertise in data analytics and software development.
Strong backend programming skills using languages such as Python, Java, C++, or C#.
Knowledge of software architecture, design patterns, and system design, ensuring scalable, efficient, and robust solutions.
Experience with database systems like PostgreSQL, MongoDB, or similar.
Proficiency in writing Unit tests and Integration tests to ensure software quality.
Familiarity with Agile methodologies and the Scrum development process.
Mostrar más
Mostrar menos","Data Science, Analytics, Software Engineering, Software Development, SQL, Python, Java, C++, C#, PostgreSQL, MongoDB, Unit Testing, Integration Testing, Agile, Scrum","data science, analytics, software engineering, software development, sql, python, java, c, c, postgresql, mongodb, unit testing, integration testing, agile, scrum","agile, analytics, c, data science, integration testing, java, mongodb, postgresql, python, scrum, software development, software engineering, sql, unit testing"
Master Data Analyst,Amcor,"Mexicali, Baja California, Mexico",https://mx.linkedin.com/jobs/view/master-data-analyst-at-amcor-3739617661,2023-12-17,Calexico,United States,Mid senior,Remote,"Join Amcor and you can be part of the team that makes the packaging of the future better for people and our planet.
Here at Amcor, we work together every day to make a positive impact on the lives of millions of people across the globe by providing packaging for essential products, including food, beverages, pharmaceutical, medical, home and personal care. We are sustainability leaders in our industry, and we were the first global packaging company to commit to making all our products recyclable or reusable by 2025. In addition, our products play a vital role in a circular economy as we are working toward 30% recycled material across our portfolio by 2030, and we are one of few packaging companies committed to using science-based targets and achieving net zero emissions by 2050.
We are always looking for talented and passionate individuals who are motivated to make a difference. Working at Amcor means you will have a unique opportunity to be a part of an organization that is committed to innovating and driving new solutions to create more sustainable packaging solutions.
To find out more about our commitment to sustainability and about Amcor, visit www.amcor.com I LinkedIn I Glassdoor I Facebook I YouTube
Amcor Rigid Packaging
Master Data Analyst
Mexico, Argentina or Colombia
Position Overview
Lead efforts to manage and continuously improve the center led global master data function at Amcor Rigid Packaging. The Master Data Analyst is responsible for leading, defining, processing, documenting, and monitoring master data requirements for key business data domains with the goal of supporting continuity of business operations and improving business analytics with clean and trustworthy data. This position will be an integral part of the Digital Procurement Operations Team and the scope will include custodianship of all master data worldwide including but not limited to business partners, material master records in SAP/MDG and Product Hierarchies, Bills of Materials, Production Versions, Source Lists, Purchasing Info Records in SAP ECC, and additional master data initiatives.
Essential Responsibilities And Duties
Manage and prioritize Amcor’s requirements for set-up (manual entry or mass loads in SAP, or via SAP-MDG workflow approval), maintenance and cleansing/enrichment of business master data identified above to ensure continuity of commercial and manufacturing business operations.
Lead the cleansing, transformation, conversion, and remediation of data analytics efforts across Source to Pay, Order to Cash, Record to Report, Production Planning, Materials Management business processes.
Transform and maintain business rules books consisting of business glossary and technical metadata for key master data domains in Amcor’s business systems and analytical applications.
Participate future master data initiatives aligned to Digital Procurement roadmap.
Lead audit of master data in Amcor’s business systems for compliance with data standards, governance policies and procedures.
Apply advance tools to analyze large set of data to report business compliance against critical metrics and recommend actions to improve business performance.
Manage publication of management reports capturing procurement savings, progress against digital transformation and data quality improvement initiatives.
Identify opportunities to automate process between master data and procure to pay process in Ariba.
Assess and remediate effectiveness of business requestors and approvers in their ability to comply with business service level agreements on turnaround and quality.
Design and maintain end user documentation related to training, Standard Operating Procedures that reflects latest business operations.
Provide routine training of end users and approvers with focus on compliance to Amcor master data rules book, governance processes and Key Performance Indicators (KPIs).
Coordinate with Amcor IT to audit and manage mapping of Amcor co-workers to appropriate business roles.
Any other responsibilities as assigned by Procurement Manager – Analytics and Data Management
Travel 5%
Relationships
Reports directly to the Procurement Manager – Analytics/Data Management
Partner with the following internal customers/other colleagues:
Procurement Leadership Team,
Leaders from Commercial, Finance, Supply Chain and Manufacturing Operations.
Digital Procurement Manager
Procurement Operations Manager
Procurement Category Managers
Master Data Analysts, Procurement Analysts, Procurement Buyers
Amcor IT
Skills Required
Demonstrated proficiency in one or more functional domains in SAP ECC and SAP MDG are required, including aptitude for functional configuration
Effective analytical and problem-solving abilities
Knowledge of manufacturing and supply chain processes in consumer business, packaging, automobile, life sciences or energy industry sectors
Deep customer centric and positive attitude to drive and influence change
Excellent interpersonal, leadership, verbal, and written communication skills
Ability to lead requirements gathering, future state design and fit-gap analysis workshops with business stakeholders
Ability to plan, measure and report progress on projects or business initiatives
Excellent command of spoken and written English
Knowledge And Experience
Bachelor’s degree required, with concentration in Industrial, Computer Science or Mechanical Engineering, Business Administration, Supply Chain Management, Information System.
Minimum of 3 years’ work experience with SAP Master Data Management (MM) and Production Planning (PP) modules
At least 3 years’ experience with SAP EIM tools such as MDG, NetWeaver MDM, in the capacity of a functional/business analyst
Minimum 3 Years of experience with reporting and visualization tools such as – SAP BW, Informatica, PowerBI, Tableau
At least 3 years of experience of using data quality and profiling tool such as information Steward, or Informatica for data profiling, automated data cleansing and proactive management of the quality of the data
Must have demonstrated abilities to perform functional configuration of master data domains – Production Materials, BOMs, Production Version, MRO, Services, Vendor, Customer, Employee
Hands-on experience with mass data loading and maintenance tools (e.g. Winshuttle, LSMW) are required
In depth knowledge of SAP organization structures, metadata management, data dictionary and data structures
Advanced to expert level knowledge of Microsoft Word, PowerPoint, Excel, Visio and MS Project.
Competencies:
Amcor Leadership Framework Competencies
Core Competencies:
Customer Focus
Learning on the Fly
Interpersonal Savvy
Drive for Results
3-5 Applicable ALF Competencies:
Organizing
Priority Setting
Functional/Technical Skills
Choose an item.
Choose an item.
IND123
Amcor is a global leader in developing and producing responsible packaging solutions for food, beverage, pharmaceutical, medical, home and personal-care, and other products. Amcor works with leading companies around the world to protect their products and the people who rely on them, differentiate brands, and improve supply chains through a range of flexible and rigid packaging, specialty cartons, closures, and services. The company is focused on making packaging that is increasingly lighter weight, recyclable and reusable, and made using an increasing amount of recycled content. In fiscal year 2022, 44,000 Amcor people generated $15 billion in annual sales from operations that span 220 locations in 43 countries. NYSE: AMCR; ASX: AMC
Mostrar más
Mostrar menos","SAP ECC, SAP MDG, SAP BW, Informatica, PowerBI, Tableau, Winshuttle, LSMW, Microsoft Office Suite, Data quality and profiling tools, Data analytics, Data management, Data cleansing, Data enrichment, Data governance, Data migration, Data modeling, Data reporting, Data visualization, Master data management, Production planning, Supply chain management, Procurement, Manufacturing, Engineering, Business analysis, Project management, Communication, Leadership, Teamwork, Problemsolving, Analytical skills","sap ecc, sap mdg, sap bw, informatica, powerbi, tableau, winshuttle, lsmw, microsoft office suite, data quality and profiling tools, data analytics, data management, data cleansing, data enrichment, data governance, data migration, data modeling, data reporting, data visualization, master data management, production planning, supply chain management, procurement, manufacturing, engineering, business analysis, project management, communication, leadership, teamwork, problemsolving, analytical skills","analytical skills, business analysis, communication, data enrichment, data governance, data management, data migration, data quality and profiling tools, data reporting, dataanalytics, datacleaning, datamodeling, engineering, informatica, leadership, lsmw, manufacturing, master data management, microsoft office suite, powerbi, problemsolving, procurement, production planning, project management, sap bw, sap ecc, sap mdg, supply chain management, tableau, teamwork, visualization, winshuttle"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Drmartens,"Blainville, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-drmartens-3756473180,2023-12-17,Sainte-Foy, Canada,Mid senior,Hybrid,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Data Interpretation, Statistical Techniques, Data Visualization, Data Management, ETL Processes, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing","data analysis, data interpretation, statistical techniques, data visualization, data management, etl processes, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing","ab testing, data interpretation, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Senior Data Engineer,Adroit Software Inc.,"Durham, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-adroit-software-inc-3667476827,2023-12-17,Durham,United States,Mid senior,Onsite,"For a financial client we need Senior Data Engineer. This position is based in Durham, NC or Westlake, TX. We are Primarily looking for W2 Candidates and not looking for Third Party Candidates.
The Expertise And Skills You Bring
10+ years of development experience in Database Development
Writing SQL queries and debugging stored procedures within an Oracle environment.
Strong hands-on working knowledge in Scripting
Experience and/or certification with Amazon Web Services, Google Cloud Platform, or Microsoft Azure is a plus.
Knowledge of Informatica and/or ETL tools
Assist in identification, isolation, resolution, and communication of problems within the production and nonproduction environment and perform troubleshooting.
Professional in scripting with the ability to develop automation tools.
Define, maintain, and support our enterprise products
Perform troubleshooting and triaging in Assist in production and nonproduction environment
Standout colleague, self-starter, collaborative, innovative and eager to learn every day.
Excellent communication and documentation skills.
Enjoy experimental development solutions
Ability to multi-task within various initiatives if needed
The Skills You Bring
Define, maintain, and support our enterprise products
Perform troubleshooting and triaging in Assist in production and nonproduction environment
Standout colleague, self-starter, collaborative, innovative and eager to learn every day.
Excellent communication and documentation skills.
Enjoy experimental development solutions
Ability to multi-task within various initiatives if needed
The Value You Deliver
Accountable for consistent delivery of functional software sprint to sprint, release to release
Excellence in software development practices and procedures
Participates in application level architecture
Develops original and creative technical solutions to on-going development efforts
Responsible for QA readiness of software deliverables (end-to-end tests, unit tests, automation)
Responsible for supporting implementation of moderate-scope projects or major initiatives
Works on complex assignments and often multiple phases of a project
Show more
Show less","Database Development, SQL, Stored Procedures, Oracle, Scripting, Amazon Web Services, Google Cloud Platform, Microsoft Azure, Informatica, ETL, Automation, Enterprise Software, Troubleshooting, Production Environment, Communication, Documentation, Experimental Development, Multitasking","database development, sql, stored procedures, oracle, scripting, amazon web services, google cloud platform, microsoft azure, informatica, etl, automation, enterprise software, troubleshooting, production environment, communication, documentation, experimental development, multitasking","amazon web services, automation, communication, database development, documentation, enterprise software, etl, experimental development, google cloud platform, informatica, microsoft azure, multitasking, oracle, production environment, scripting, sql, stored procedures, troubleshooting"
Senior Software Engineer - Data Path,VAST Data,"Raleigh, NC",https://www.linkedin.com/jobs/view/senior-software-engineer-data-path-at-vast-data-3623805244,2023-12-17,Durham,United States,Mid senior,Onsite,"VAST Data is looking for a Senior Software Engineer -Data Path to join our growing team!
This is a great opportunity to be part of the fast-growing infrastructure company in history. In just a few short years, we’ve shaken up the industry by challenging traditional architecture models and introduced a revolutionary set of storage possibilities through our Universal Storage platform.
Our success has been built through incredible leadership and motivated employees who want to leverage their skills & experiences to make a real impact. This is an opportunity to be a key player at a pivotal time in our company’s growth.
Come join the disruption, make your mark, and be a part of the amazing team introducing the new era of data storage!
Requirements:
Strong experience in C/C++ software engineer design, coding, integration, and debugging
BSc/MSc degree in Computer Science, Engineering or equivalent
Understanding of Linux operating system
Solid networking knowledge (OSI network layers, TCP/IP) — advantage
Knowledge block-level Storage / file systems — advantage\
Python scripting — advantage
Show more
Show less","C/C++, Software Design, Coding, Integration, Debugging, Linux, Networking, TCP/IP, Storage, File Systems, Python","cc, software design, coding, integration, debugging, linux, networking, tcpip, storage, file systems, python","cc, coding, debugging, file systems, integration, linux, networking, python, software design, storage, tcpip"
Data Analyst/Modeler - Hybrid,"ActiveSoft, Inc","Durham, NC",https://www.linkedin.com/jobs/view/data-analyst-modeler-hybrid-at-activesoft-inc-3787729737,2023-12-17,Durham,United States,Mid senior,Onsite,"Looking for a Data modeler who came from a SQL development/data analyst background
SQL- Any analysis SQL skills, Querying databases. Some coding experience.
GAP analysis of existing databases. Creating sample data sets for consumer for to
Physical Modeling using Erwin or SAP Power designer.
What are the NICE to have skills: Brokerage or Financial Services
Powered by JazzHR
LD0sltcs1l
Show more
Show less","SQL, Database querying, Coding, Data modeling, GAP analysis, Erwin, SAP PowerDesigner, NICE","sql, database querying, coding, data modeling, gap analysis, erwin, sap powerdesigner, nice","coding, database querying, datamodeling, erwin, gap analysis, nice, sap powerdesigner, sql"
Big Data Senior Developer / Lead,Wise Skulls,"Cary, NC",https://www.linkedin.com/jobs/view/big-data-senior-developer-lead-at-wise-skulls-3667468815,2023-12-17,Durham,United States,Mid senior,Onsite,"Job Title: Big Data Senior Developer / Lead
Location: Cary, NC
Duration: 12 months
Implementation: TCS
End Client: To be Disclosed
Job Description
Minimum 8+ years of experience
Technical
Hadoop, Hive, Scala, Spark, SQL, HBase
Scripting knowledge (Python or Shell)
Functional
Banking Financial Services and Insurance domain
Non-Technical
Experience in BFSI Information technology or equivalent
Possesses mastery level understanding of standards, business processes, workflows, methodologies, and leading practices
Must have in-depth knowledge of software development lifecycles including Agile development and testing.
Demonstrated ability in applying automation for repeatable tasks
Strong management, communication, technical and remote collaboration skill are a must
Experience in large cross system, cross team initiatives
Experience Required
Strong hands-on experience
Ability to build, deliver and operate complex systems
Excellent verbal and written communication skills with focused attention to details, as well as, demonstrated professionalism and time/task management skills; ability to establish strong relationships
Strong Software Development Life Cycle knowledge and experience, or comparable methodologies
Strong problem-solving skills with demonstrated ability to apply analysis to actionable insights
Roles & Responsibilities
Analyzing, designing, developing, and driving solutions for major components and features
Ensuring secure, high code quality across the entire team
Interfacing with key stakeholders and business leaders
Resolving inconsistencies to gain consensus
Mentoring Junior Software Engineers
Show more
Show less","Hadoop, Hive, Scala, Spark, SQL, HBase, Python, Shell, Agile Development, Automation, Software Development Life Cycle, ProblemSolving Skills, Analysis, System Design, Code Quality Management, Stakeholder Management, Communication, Teamwork, Mentoring","hadoop, hive, scala, spark, sql, hbase, python, shell, agile development, automation, software development life cycle, problemsolving skills, analysis, system design, code quality management, stakeholder management, communication, teamwork, mentoring","agile development, analysis, automation, code quality management, communication, hadoop, hbase, hive, mentoring, problemsolving skills, python, scala, shell, software development life cycle, spark, sql, stakeholder management, system design, teamwork"
Senior Clinical Data Analyst,Parexel,"Durham, NC",https://www.linkedin.com/jobs/view/senior-clinical-data-analyst-at-parexel-3765200433,2023-12-17,Durham,United States,Mid senior,Remote,"The Senior Clinical Data Analyst (Sr. CDAn) is capable of leading Clinical Data Analyst programming activities with minimal support and oversight. The Sr. CDAn is capable of creating, leading, and driving activities to assess and improve the Analytics and Reporting programming infrastructure.
Programs metrics and reports in a data visualization tool/analyst software (e.g., Power Business Intelligence (BI), Spotfire).
Ability to independently develop innovative and complex reports to support activities including, but not limited to, data review and surveillance.
Develops and maintains the Analytics and Reporting Global Library to ensure consistency across programs and studies and concordance with the Electronic Data Capture (EDC) Global Library.
Trains Clinical Data Managers and Clinical Data Scientists on how to generate output (e.g., data review tool, metrics).
Works with Clinical Data Manager, Lab Data Specialist, Medical Monitor, and Protocol Lead (and other study team members as appropriate) to develop new or improve existing data review tools.
Coordinates with Clinical Data Managers and Clinical Data Scientists regarding timelines and deliverables to ensure all reports are working as expected.
Mentors less proficient Clinical Data Analysts as appropriate.
Independently manages processes and applications while evaluating for improvements; lead working groups to develop and implement new processes and applications.
May contribute to the development, review and implementation of Standard Operational Procedures (SOPs), templates and processes and other departmental and/or cross functional initiatives.
Utilizes experience and leadership skills to provide guidance to other team members. Takes ownership of process resources available to team.
Complies with required training curriculum.
Completes timesheets accurately as required.
Submits expense reports as required.
Updates CV as required.
Maintains a working knowledge of and complies with Parexel processes, ICH-GCPs and other applicable requirements.
5-10 years of experience with Analyst software (e.g., Spotfire, Power BI, Tableau), preferably in the pharmaceutical or biotechnology industry.
Technical expertise in database and report development.
Experience with programming languages (e.g., Structured Query Language (SQL), Python).
Experience in managing projects as well as effective verbal and written communication skills.
Ability to work independently, organize tasks, time and priorities, ability to multitask.
Knowledge of and experience with clinical databases, electronic data capture systems, quality control processes and auditing procedures.
Strong critical thinking skills.
Familiarity with Good Clinical Practice (GCP), International Council of Harmonization (ICH), and Food & Drug Administration (FDA) requirements as they apply to clinical data.
Strong well-rounded technical skills (EDC systems, Microsoft Word, Excel, PowerPoint).
Regular interaction with Regional Clinical Trial Operations and Medical Management groups.
Regular interaction with other groups within Clinical Data Management, including Clinical Data Programmers, EDC Programmers, and Data Scientists.
Show more
Show less","Clinical Data Analysis, Power BI, Spotfire, Tableau, Python, SQL, Database Development, Report Development, Project Management, Clinical Databases, Electronic Data Capture, Quality Control, Auditing, Good Clinical Practice, International Council of Harmonization, Food and Drug Administration, Microsoft Office Suite, EDC Systems, Programming, Data Visualization, Data Review, Data Surveillance, Standard Operational Procedures","clinical data analysis, power bi, spotfire, tableau, python, sql, database development, report development, project management, clinical databases, electronic data capture, quality control, auditing, good clinical practice, international council of harmonization, food and drug administration, microsoft office suite, edc systems, programming, data visualization, data review, data surveillance, standard operational procedures","auditing, clinical data analysis, clinical databases, data review, data surveillance, database development, edc systems, electronic data capture, food and drug administration, good clinical practice, international council of harmonization, microsoft office suite, powerbi, programming, project management, python, quality control, report development, spotfire, sql, standard operational procedures, tableau, visualization"
Data Engineer III - Network,Crown Castle,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-engineer-iii-network-at-crown-castle-3772656693,2023-12-17,Peoria,United States,Mid senior,Remote,"Position Title:
Data Engineer III, Network (P3)
Company Summary
Crown Castle is the nation’s largest provider of shared communications infrastructure: towers, small cells and fiber. Whenever you make a call, track a workout or stream music and videos, we’re the ones providing the communications infrastructure that makes it possible to transform the way we live and work. From 5G and the internet of things to drones, autonomous vehicles and AR/VR, we enable the technologies that help people stay safe, connected and ready for the future. Crown Castle is publicly traded on the S&P 500, and one of the largest Real Estate Investment Trusts in the US.
Role
The Network Automation and Platforms (NAP) team provides innovative solutions using open-source and commercial technologies on-premises and in the cloud, to deliver critical data, geo-spatial, and automation capabilities to our customers. You will work in a highly collaborative team consisting of software architects, developers, database admins, network planners, and network engineers. As a Data Engineer III, you will interpret Network Engineering data requirements, procure datasets, and build reporting & analytical capabilities that will enable data-driven decisions leading to increased revenue and greater efficiencies for the network business. You will also assist in setting the data engineering strategy and implementation roadmap necessary to achieve our vision of a next generation, end-to-end autonomous network.
Position Summary
The Data Engineer III is responsible for automating the ingestion, transformation, and integration of data between applications and managing, advancing, and delivering business intelligence solutions based on that data to meet the business objectives of the organization's Network Engineering teams with minimal supervision.
Essential Job Functions
Meet with subject matter experts in network engineering areas of the business, be able to quickly learn concepts and language relevant to their departments and develop an understanding of how their data supports their business processes.
Develop and maintain efficient enterprise-grade dashboards, reports and datasets adhering to industry best practices.
Review, interpret, troubleshoot, and optimize complex SQL queries. Develop maintainable code via proper structures, comments, design using best practices.
Provide database development and data integration support, developing database application code and data integration layers using database programming languages, ETL tools, APIs, and scripting languages.
Provide feedback as needed to internal and external resources that provide source data.
Continuously demonstrate proficiency in the use of business intelligence technologies.
Perform source data analysis – data profiling, validation, conceptual and logical data modeling, etc. – to determine the suitability of the source data for meeting the reporting requirements.
Research and understand business objectives and provide guidance, options, and proof of concepts (if appropriate) to solve business needs.
Work with third-party software vendors to manage both support and feature requests to improve the end-user experience.
Participate in weekly standups and provide code reviews for team projects.
Well organized and capable of executing on multiple high-level projects at the same time.
Adheres to project methodology, change management, and departmental procedures.
Develop documentation including but not limited to flowcharts and entity-relationship diagrams, as well as requirements and solutions.
Exceptional ability to interact in a team environment with peers and members of other teams that might be located remotely.
Education/Certifications
BS Degree in Engineering, Computer Science, or related technical discipline.
Code-camp with a portfolio and additional years of experience may be considered in lieu of a degree (put your GitHub, etc. link in your resume)
Experience/Minimum Requirements
5+ years of experience with SQL database technologies such as PostgresSQL, Oracle, SQL Server, and MySQL including developing database-specific SQL queries, data modeling, migrations, and integration strategies.
3+ years of design experience with BI reporting technologies in the creation of data-rich dashboards, such as Grafana and/or Power BI.
Proficiency with operational database and data warehouse design, ETL development, scalable data pipeline design, API integration and automation.
Proficiency with scripting languages such as Python (preferred), Ruby, and Go.
Experience with software development best practices, including coding standards, code reviews, source control management, automated build processes, testing, and operations.
Experience with modern ETL pipelines and schedulers a plus (e.g., Airflow, etc.)
Reports to:
Principal Architect
We offer a total benefits package and professional growth development for teammates in any stage of their career. Along with caring for our teammates, we’re an active member in the communities where we live, work and do business. We have a responsibility to give back, which we do through our Connected by Good program. Giving back allows us to improve public spaces where people connect, promote public safety and advance access to education and technology
For New York City, Colorado, California, and Washington state residents:
The hiring range offered for this position is $101,800 - $146,400 annually. In addition to salary, employees are eligible for an annual bonus of up to 15% of annual salary and restricted stock. Employees (and their families) are eligible for medical, dental, vision, and basic life insurance. Employees are able to enroll in our company’s 401k plan. Employees will also receive 18 days of paid time off each year and 12 paid holidays throughout the calendar year.
Show more
Show less","SQL, Python, Ruby, Go, Grafana, Power BI, Oracle, SQL Server, MySQL, Scalable data pipeline design, ETL, Airflow, PostgresSQL, Data modeling, Software development best practices, Code reviews, API integration, Automation","sql, python, ruby, go, grafana, power bi, oracle, sql server, mysql, scalable data pipeline design, etl, airflow, postgressql, data modeling, software development best practices, code reviews, api integration, automation","airflow, api integration, automation, code reviews, datamodeling, etl, go, grafana, mysql, oracle, postgressql, powerbi, python, ruby, scalable data pipeline design, software development best practices, sql, sql server"
Sr. Data Analyst,Aston Carter,"Phoenix, AZ",https://www.linkedin.com/jobs/view/sr-data-analyst-at-aston-carter-3788824242,2023-12-17,Peoria,United States,Mid senior,Remote,"Description:*
Essential Job Functions:*
Support clients: Work and communicate closely with their clients throughout the report/dashboard development process. Provide status updates. Help clients understand their report/dashboard. Provide on-going maintenance and support.
Support projects: Participate in outcome evaluation, program evaluation, and quality improvement studies. Attend meetings, answer data-related questions, and offer suggestions.
Create reports and dashboards: Create ad-hoc and routine reports. Design and develop dashboards to display key metrics and trends.
Manage data: Collect, organize, store, and share a wide variety of data.
Transform data: Clean and optimize data for analyses.
Ensure data quality: Audit data, data transformation processes, workflow, deliverables and outputs.
Perform analyses: Perform statistical analyses (descriptive and inferential analyses).
Present findings: Present data and findings in a clear and concise manner, using appropriate reporting and data visualization tools.
Create and maintain documentation: Create FDD, document report requirement, business logic and workflow. Create data dictionaries. Ensure documentation is up-to-date.
Maintain up-to-date knowledge on information management systems, processes and data.
Manage compliance reporting: Maintain up-to-date knowledge of CMS, DHCS and internal compliance reporting requirement. Translate reporting requirement into reports. Work with clients to ensure accuracy of data. Submit report to external and internal agencies in a timely manner. Attend compliance trainings, meetings, and data validation webinars.
Support system enhancement/implementation: Perform data-related research and testing. Stay informed of system and process changes. Identify impact on existing reports and dashboards. Modify existing reports and dashboards accordingly.
Prioritize work and keep supervisor informed: Work on multiple projects at the same time. Organize and prioritize work effectively. Inform management when requirement or due date cannot be met.
Adheres to all quality, compliance and regulatory standards to achieve organization outcomes.
Required Education: *
Bachelor’s or Master’s degree in Analytics, Healthcare Informatics, Statistics, Computer Science, or related field.
Required Experience:*
At least 5 years of experience analyzing and compiling data, preferably in a health plan setting.
Required Skills/Abilities:*
Ability to manipulate and analyze data to produce accurate results. Present results in data visualizations, dashboards, and reports.
Knowledge in CMS Medicare Advantage (Part C), CMS Prescription Drug Coverage (Part D), and CMS Special Needs Plan (SNP), and DHCS Medi-Cal Managed Care reporting requirements.
Knowledge in authorization, claims, and encounter data. Clinical code knowledge (ICD, CPT, etc) related to utilization data.
Advanced skills in Microsoft Office, SQL Transactional SQL (T-SQL), SQL Server Reporting Services (SSRS), and Tableau.
Experience in SQL Server Integration Services (SSIS), Visual Basic for Applications (VBA).
Must have analytical, communication, documentation, interpersonal, planning, presentation, problem-solving and research skills.
About Aston Carter:
Please Note: Scammers are posing as Aston Carter. We'll never contact you via Gmail, Telegram, or WhatsApp and we'll never solicit money from you.
At Aston Carter, we’re dedicated to expanding career opportunities for the skilled professionals who power our business. Our success is driven by the talented, motivated people who join our team across a range of positions – from recruiting, sales and delivery to corporate roles. As part of our team, employees have the opportunity for long-term career success, where hard work is rewarded and the potential for growth is limitless. Established in 1997, Aston Carter is a leading staffing and consulting firm, providing high-caliber talent and premium services to more than 7,000 companies across North America. Spanning four continents and more than 200 offices, we extend our clients’ capabilities by seeking solvers and delivering solutions to address today’s workforce challenges. For organizations looking for innovative solutions shaped by critical-thinking professionals, visit [AstonCarter.com.](AstonCarter.com) Aston Carter is a company within Allegis Group, a global leader in talent solutions. The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law. If you would like to request a reasonable accommodation, such as the modification or adjustment of the job application process or interviewing process due to a disability, please call 888-237-6835 or email [astoncarteraccommodation@astoncarter.com](mailto:%20astoncarteraccommodation@astoncarter.com) for other accommodation options. However, if you have questions about this position, please contact the Recruiter located at the bottom of the job posting. The Recruiter is the sole point of contact for questions about this position.
Show more
Show less","Microsoft Excel, SQL, SQL SSRS, Tableau, SQL SSIS, VBA, Data Visualization, Reporting, Data Analysis, Data Management, Data Transformation, Data Quality, Statistical Analysis, Data Warehousing, Data Mining, Data Integration, Business Intelligence, Data Governance, Data Security, Healthcare Informatics, Clinical Coding, CMS Regulations, DHCS Regulations, Utilization Management, Quality Improvement, Risk Adjustment, Outcome Measurement, Program Evaluation, Project Management, Communication, Documentation, Problem Solving, Research, Analytical Skills","microsoft excel, sql, sql ssrs, tableau, sql ssis, vba, data visualization, reporting, data analysis, data management, data transformation, data quality, statistical analysis, data warehousing, data mining, data integration, business intelligence, data governance, data security, healthcare informatics, clinical coding, cms regulations, dhcs regulations, utilization management, quality improvement, risk adjustment, outcome measurement, program evaluation, project management, communication, documentation, problem solving, research, analytical skills","analytical skills, business intelligence, clinical coding, cms regulations, communication, data governance, data integration, data management, data mining, data quality, data security, data transformation, dataanalytics, datawarehouse, dhcs regulations, documentation, healthcare informatics, microsoft excel, outcome measurement, problem solving, program evaluation, project management, quality improvement, reporting, research, risk adjustment, sql, sql ssis, sql ssrs, statistical analysis, tableau, utilization management, vba, visualization"
Insurance - Data Analyst - REMOTE,Wahve LLC,"Phoenix, AZ",https://www.linkedin.com/jobs/view/insurance-data-analyst-remote-at-wahve-llc-3785424777,2023-12-17,Peoria,United States,Mid senior,Remote,"Put your Insurance Experience to work - FROM HOME!
At
Wahve
, we value significant insurance experience and want to revolutionize the way people think about
phasing into
retirement
by offering qualified candidates the opportunity to continue their career working from home. As we say -
retire from the office but not from work
. Our unique platform provides you with
real
work/life balance and allows you to customize your own work schedule while continuing to utilize your insurance expertise in
a remote, long-term position
.
What You’ll Love About Wahve
We created a welcoming place to work with friendly and professional leadership. We are known for the great care we take with our staff and our clients. We are passionate and determined about delivering the best customer service, preserving insurance industry knowledge, and making a difference by the work that we do.
What We Are Seeking
We have assignments available to help our
insurance industry
clients in
Data Analyst positions. Responsibilities include:
Build and maintain data warehouse, new reports, and ad hoc reports.
Work with user groups to identify reporting issues/enhancements and document business requirements.
Will serve as a member of a project team and/or work independently on projects.
Support and train internal users as needed.
Compile and prepare data for customer analysis.
Experience in C#, Visual Studio, JavaScript, CSS, and current web technologies such as .NET, ASP, JSON, and XML.
Experience with ANY of the following technologies: SQL Server Reporting Services (SSRS), SSIS Reporting, Power BI, Dynamics CRM, Dynamics GP, Share point, Excel, Power Query, Power Pivot.
Ability to compile data results and author commentary on industry studies is a plus.
Insurance or financial services industry experience required.
TO BECOME A WORK-AT-HOME VINTAGE EXPERT, WE REQUIRE
25 years of full-time work experience
Experience working in a data analysis role in the insurance or financial services industry - required
Benefits Of Becoming a Wahve Vintage Expert
Retire from the office but not from work.
Eliminate the office stress and the commute.
Choose the work you would like to do now.
Customize your schedule - full or part time.
Continue to earn an income.
Utilize your years of insurance industry knowledge.
Be part of our dynamic yet virtual team environment and connect with other experienced insurance professionals like yourself!
How To Get Started
Click
APPLY NOW
to complete our simple preliminary profile. Be sure to include your preferred contact information as one of our Qualification Specialists will connect with you promptly.
WE LOOK FORWARD TO MEETING YOU!
Show more
Show less","C#, Visual Studio, JavaScript, CSS, .NET, ASP, JSON, XML, SQL Server Reporting Services (SSRS), SSIS Reporting, Power BI, Dynamics CRM, Dynamics GP, Share point, Excel, Power Query, Power Pivot, SQL, Data Warehousing, Data Analysis, Project Management, Customer Service, Insurance Industry Knowledge, Financial Services Industry Knowledge, Data Compilation, Commentary Writing, Industry Studies","c, visual studio, javascript, css, net, asp, json, xml, sql server reporting services ssrs, ssis reporting, power bi, dynamics crm, dynamics gp, share point, excel, power query, power pivot, sql, data warehousing, data analysis, project management, customer service, insurance industry knowledge, financial services industry knowledge, data compilation, commentary writing, industry studies","asp, c, commentary writing, css, customer service, data compilation, dataanalytics, datawarehouse, dynamics crm, dynamics gp, excel, financial services industry knowledge, industry studies, insurance industry knowledge, javascript, json, net, power pivot, power query, powerbi, project management, share point, sql, sql server reporting services ssrs, ssis reporting, visual studio, xml"
"Data Conversion Developer, Senior Associate",PwC,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749940328,2023-12-17,Peoria,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Python, PySpark, Scala, SQL, Azure Data Engineer Associate, Databricks Certified Data Engineer Associate, IBM Maximo, IBM DB2, Oracle, Microsoft SQL Server, Maximo's Integration Framework (MIF), Azure ADF, AWS Glue, SSIS, DataBricks","python, pyspark, scala, sql, azure data engineer associate, databricks certified data engineer associate, ibm maximo, ibm db2, oracle, microsoft sql server, maximos integration framework mif, azure adf, aws glue, ssis, databricks","aws glue, azure adf, azure data engineer associate, databricks, databricks certified data engineer associate, ibm db2, ibm maximo, maximos integration framework mif, microsoft sql server, oracle, python, scala, spark, sql, ssis"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Fulton, CA",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783186241,2023-12-17,Santa Rosa,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Fulton-DataScientist.007
Show more
Show less","Python, JavaScript, JSON, Generative AI, Data engineering, Machine learning, R, Algorithms, OOP languages, English, Data science","python, javascript, json, generative ai, data engineering, machine learning, r, algorithms, oop languages, english, data science","algorithms, data engineering, data science, english, generative ai, javascript, json, machine learning, oop languages, python, r"
Senior Data Engineer,Professional Diversity Network,"Washington, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-professional-diversity-network-3788312391,2023-12-17,Wenatchee,United States,Mid senior,Onsite,"Responsibilities
The Senior Data Engineer responsibilites include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of Seattle Children's Enterprise Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatement, etc.
Requirements
Required Education/Experience: - Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competency - Minimum of five (5) years technology industry or related experience, including items such as: - Build highly scalable, scaled-out architectures on large scale database platforms - Experience working in a complex data infrastructure environment - Five (5) years of experience in a data engineering role - Extensive and in depth data pipeline development experience with industry standard data integration tools - Advanced competency in SQL with ability to perform query optimization in large scale database platforms - Experience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support. - Experience with any industry standard tool for Source Control and Project Management - Experience wrting test cases and test scripts for data quality assurance - Experience creating stored procedures and functions - Experience developing dimensional data model with any industry standard tool. Required Credentials: - N/A Preferred: - Experience in Healthcare or related industry - Experience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plus - Experience productizing/automating predictive models that use R, SAS, Python, SPSS, etc. - Continuous delivery and deployment automation for analytic solutions - Familiarity with Agile framework and test driven development methodology for analytic solutions - API development - Data visualization and/or dashboard development
Min to Max Hourly Salary
$59.18 - $88.77 /hr
Min to Max Annual Salary
$123,094.00 - $184,640.00 /yr
Salary Information
This compensation range was calculated based on full-time employment (2080 hours worked per calendar year). Offers are determined by multiple factors including equity, skills, experience, and expertise, and may vary within the range provided.
The union pay ranges can be found on the Seattle Children's website here: WSNA - UFCW
Seattle Children's offers annual incentive pay based upon performance that is commensurate with the level of the position.
Disclaimer for Out of State Applicants
This compensation range is specific to Seattle, positions located outside of Seattle may be compensated differently depending on various factors
.
Benefits Information
Seattle Children's offers a generous benefit package, including medical, dental, and vision plans, 403(b), life insurance, paid time off, tuition reimbursement, and more. Click here for more information.
About Us
Hope. Care. Cure. These three simple words capture what we do at Seattle Children's -- to help every child live the healthiest and most fulfilling life possible. Are you ready to engage with a mission-driven organization that is life-changing to many, and touches the hearts of all? #HOPECARECURE
Our founding promise to the community is as valid today as it was over a century ago: we will care for all children in our region, regardless of the families' ability to pay. Together, we deliver superior patient care, advance new discoveries and treatments through pediatric research, and serve as the pediatric and adolescent, academic medical center for Washington, Alaska, Montana and Idaho - the largest region of any children's hospital in the country.
In 2022, U.S. News & World Report once again ranked Seattle Children's among the nation's best children's hospitals - for the 30th year in a row. For more than a decade, Seattle Children's has been nationally ranked in all 10 specialty areas evaluated by U.S. News & World Report. We are honored to be the top-ranked pediatric hospital in Washington and the Pacific Northwest.
As a Magnet designated institution, and classified among America's best large employers by Forbes, we recognize the importance of hiring and developing great talent to provide best-in-class care to the patients and families we serve. Our organizational DNA takes form in our core values: Compassion, Excellence, Integrity, Collaboration, Equity and Innovation. Whether it's delivering frontline care to our patients in a kind and caring manner, practicing the highest standards of quality and safety, or being relentlessly curious as we work towards eradicating childhood diseases, these values are the fabric of our culture and community. The future starts here.
Our Commitment to Diversity
Our community welcomes diverse experiences, backgrounds, and thoughts as this is what drives our spirit of inquiry and allows us to better connect with our increasingly diverse patients and families. Our organization recruits, employs, trains, compensates, and promotes without regard to race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
The people who work at Seattle Children's are members of a community that seeks to respect and celebrate all the qualities that make each of us unique. Each of us is empowered to be ourselves within this community, which cultivates and promotes equity, diversity, and inclusion at all levels.
Seattle Children's is proud to be an Equal Opportunity Workplace and Affirmative Action Employer.
PDN-9a3eeb2f-0d7e-42c0-98aa-ee15bd90aa1f
Show more
Show less","Data Engineering, Data Modeling, Data Integration, Data Warehousing, Data Processing, SQL, SDLC, Cloud Computing, Agile, Test Driven Development, Netezza, Datastage, BitBucket, JIRA, Confluence, R, SAS, Python, SPSS, API Development, Data Visualization, Dashboard Development","data engineering, data modeling, data integration, data warehousing, data processing, sql, sdlc, cloud computing, agile, test driven development, netezza, datastage, bitbucket, jira, confluence, r, sas, python, spss, api development, data visualization, dashboard development","agile, api development, bitbucket, cloud computing, confluence, dashboard development, data engineering, data integration, data processing, datamodeling, datastage, datawarehouse, jira, netezza, python, r, sas, sdlc, spss, sql, test driven development, visualization"
Sr Data Engineer,Jefferson Frank,"Washington, United States",https://www.linkedin.com/jobs/view/sr-data-engineer-at-jefferson-frank-3743793638,2023-12-17,Wenatchee,United States,Mid senior,Onsite,"My client is a small startup company that is the source of truth for patent intelligence! They provide SaaS platforms that correlate multiple data sets (patent, financial, and market data) using supervised deep learning, in order to answer fundamental questions related to patent and innovation strategy.
They are on the hunt for a Senior Data Engineer to assist in creating a 2.0 version of their existing platform. This includes developing a fresh front end driven by new algorithms and building scalable data science models that leverage data from diverse sources such as patents, financial data, and personnel information.
Top Requirements
3-5 years of extensive experience as a data engineer in the AWS ecosystem, encompassing structured and unstructured data, scalable computing, and real-time processing.
Proficiency in complex SQL queries and data correlation analysis.
Deep familiarity with AWS tools like RedShift, Glue, and other resources for distributed computing. Substantial expertise in multi-modal databases and ETL pipelines, including graph databases.
If interested, please send me a message or an email at: e.gardner@jeffersonfrank.com
Show more
Show less","Data Engineering, Supervised Deep Learning, AWS Ecosystem, Structured and Unstructured Data, Scalable Computing, RealTime Processing, SQL Queries, Data Correlation Analysis, AWS Tools, RedShift, Glue, Distributed Computing, Multimodal Databases, ETL Pipelines, Graph Databases","data engineering, supervised deep learning, aws ecosystem, structured and unstructured data, scalable computing, realtime processing, sql queries, data correlation analysis, aws tools, redshift, glue, distributed computing, multimodal databases, etl pipelines, graph databases","aws ecosystem, aws tools, data correlation analysis, data engineering, distributed computing, etl pipelines, glue, graph databases, multimodal databases, realtime processing, redshift, scalable computing, sql queries, structured and unstructured data, supervised deep learning"
Azure Data Engineer,HTC Global Services,"Washington, United States",https://www.linkedin.com/jobs/view/azure-data-engineer-at-htc-global-services-3769549613,2023-12-17,Wenatchee,United States,Mid senior,Onsite,"About the job
About HTC Global Services:
Shaping careers since 1990 - our long tenured employees are a testimony of the work culture. Join our global employee base of 12,000 and help us bring human expertise to tech in order to deliver purposeful solutions that amplify value.
Tech Stack
Python
SQL and NoSQL databases
Scala
Spark-SQL
Experience with Azure: ADLS, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates
Qualifications
Previous experience as an Azure Data Engineer or similar role
Experience building and optimising ‘big data’ data pipelines, architectures, and data sets.
Strong analytic skills related to working with unstructured datasets.
The ability to design and implement well written code.
Ability to work to tight deadlines.
Ability to test the data from source to the presentation layer.
Ability to support/troubleshoot data pipelines.
Confident and concise communication skills, with the ability to drive alignment, collaboration, and efficiency.
Benefits:
At HTC Global Services our associates have access to a comprehensive benefits package that includes Health, Dental, Vision, Paid-Time-Off, Paid Holidays, 401K matching, Life and Accidental Death Insurance, Short- & Long-Term Disability Insurance, and a variety of other offerings.
Diversity & Inclusion
Our success as a company is built on practicing inclusion and embracing diversity. HTC Global Services is committed to providing a work environment free from discrimination and harassment, where all employees are treated with respect and dignity. Together we work to create and maintain an environment where everyone feels valued, included, and respected. At HTC Global Services, our differences are embraced and celebrated. HTC is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills, and experiences within our workforce. HTC is proud to be recognized as a National Minority Supplier.
EEO/M/F/V/H
Show more
Show less","Python, SQL, NoSQL, Scala, SparkSQL, Azure, ADLS, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates, Data Pipelines, Data Architectures, Data Sets, Data Engineering, Big Data, Analytic Skills, Unstructured Datasets, Code Design, Code Implementation, Tight Deadlines, Data Testing, Data Troubleshooting, Communication Skills, Alignment, Collaboration, Efficiency","python, sql, nosql, scala, sparksql, azure, adls, databricks, stream analytics, sql dw, cosmos db, analysis services, azure functions, serverless architecture, arm templates, data pipelines, data architectures, data sets, data engineering, big data, analytic skills, unstructured datasets, code design, code implementation, tight deadlines, data testing, data troubleshooting, communication skills, alignment, collaboration, efficiency","adls, alignment, analysis services, analytic skills, arm templates, azure, azure functions, big data, code design, code implementation, collaboration, communication skills, cosmos db, data architectures, data engineering, data sets, data testing, data troubleshooting, databricks, datapipeline, efficiency, nosql, python, scala, serverless architecture, sparksql, sql, sql dw, stream analytics, tight deadlines, unstructured datasets"
Data / Metrics Consultant,Oil and Gas Job Search Ltd,"Washington, United States",https://www.linkedin.com/jobs/view/data-metrics-consultant-at-oil-and-gas-job-search-ltd-3767601331,2023-12-17,Wenatchee,United States,Mid senior,Onsite,"ABSG Consulting, Inc. is seeking a Data /Metrics Consultant, to support the U.S. Department of Homeland Security (DHS).
The Cyber Defense Education and Training (CDET) Metrics position is responsible for the collection, aggregation, improvement of, and communication of program metrics for CDET's various component programs. The Metrics position works closely with CDET's various program leaders to ensure accurate, reliable, relevant metrics/data for reporting purposes and to help demonstrate the impact of CDET's effort on the national cyber workforce.
What You Will Do
Create and maintain strategic metrics reporting through use of Power BI, Excel, and other products for 10 CDET programs, including Continuous Diagnostics and Mitigation (CDM), ICS, IR, and Skilling Academy.
Collaborate on strategy for improved CDET-wide, program level data collection, storage, presentation.
Create a series of strategic datapoints to be collected by CDET programs with the goal of helping to demonstrate the impact CDET has on the national cyber workforce.
Work with CDET program leaders to customize this series of datapoints to fit with the goals, operation, capabilities of selected CDET programs.
Work with these key CDET programs and leaders to acquire this new, strategic information.
Present key data and metrics deliverables as needed/when requested.
Maintain and improve legacy metrics storage and reporting products.
Continuously work with CDET program leads with the goal of continuing improvement of metrics processes as desired.
Specific Tasks Include
Collect monthly metrics from CDET's training programs to aggregate in a monthly summary PowerPoint.
Regularly provide data for quarterly CDET program 'placemat' for distribution to convey success stories and various programs successes to external audiences.
Maintain Power BI dashboards for President's Cup Cybersecurity Competition.
Assist in developing metrics collection, storage, and presentation capabilities for FCDSA.
Develop post-module and post-cohort surveys for Federal Cyber Defense Skilling Academy (FCDSA).
Assist in developing and running FCDSA focus groups to solicit detailed, qualitative student feedback about experiences within the program, as needed.
What You Will Need
Education and Experience
1-4 years experience creating data visualizations using PowerBI (required) or equivalent (recommended)
1-4 years experience with Microsoft Office Suite software including ability to use Microsoft Excel to create clear data organization and presentations, charts & graphs, as well as PowerPoint for composing and delivering presentations (required)
Bachelor's Degree required
Knowledge, Skills, And Abilities
Familiarity with a statistical programming language such as ""R"" (highly desirable)
Good oral presentation skills and etiquette
US citizenship required; eligibility to obtain DHS Suitability
About Us
We set out more than 160 years ago to promote the security of life and property at sea and preserve the natural environment. Today, we remain true to our mission and continue to support organizations facing a rapidly evolving seascape of challenging regulations and new technologies. Through it all, we are anchored by a vision and mission that help our clients find clarity in uncertain times.
ABS is a global leader in marine and offshore classification and other innovative safety, quality, and environmental services. We're at the forefront of supporting the global energy transition at sea, the application of remote and autonomous marine systems, cutting-edge technical solutions, and many more exciting advancements. Our commitment to safety, reliability, and efficiency is ever-present, guiding our clients to safer and more efficient operations.
About Our Benefits
ABS Group proudly offers a variety of industry-leading benefits designed to enhance the life and well-being of our employees and their families. These benefits include, but are not limited to, medical insurance (PPO and HD), dental and vision insurance, Health Savings account (HSA), Flexible Savings Account (FSA), life insurance, accidental death and dismemberment insurance, disability leave programs, parental leave program, paid holidays, and paid vacation time. The Company provides an Employee Assistance Plan (EAP) that offers additional support in personal wellness, including work-life services. ABS Group also offers a 401K plan with a generous company match, subject to plan requirements.
Equal Opportunity
The ABS Group of Companies is committed to the equal employment opportunity of its employees and prohibits discrimination against any employee or qualified applicant based on race, color, creed, religion, national origin, sex, gender identity, age, disability, marital status, sexual orientation, citizenship status or veteran status, or other non-work-related characteristics that may be protected under the law of the Federal Government or specific state employment laws.
Notice
ABS and Affiliated Companies (ABS) will not pay a fee to any third-party agency without a valid ABS Master Service Agreement (MSA) authorized and signed by Human Resources. Any resume, CV, application, or other forms of candidate submission provided to any employee of ABS without a valid MSA on file will be considered property of ABS, and no fee will be paid.
Other
This job description is not intended, and should not be construed, to be an all-inclusive list of responsibilities, skills, efforts or working conditions associated with the job of the incumbent. It is intended to be an accurate reflection of the principal job elements essential for making a fair decision regarding the pay structure of the job. #ogjs
Show more
Show less","Power BI, Excel, Microsoft Office Suite, PowerPoint, R, Statistical programming language, Data visualization, Data aggregation, Data presentation, Metrics collection, Metrics reporting, Data storage, Data analysis, Data interpretation, Data communication, Presentation skills, Citizenship (US), Eligibility (DHS Suitability)","power bi, excel, microsoft office suite, powerpoint, r, statistical programming language, data visualization, data aggregation, data presentation, metrics collection, metrics reporting, data storage, data analysis, data interpretation, data communication, presentation skills, citizenship us, eligibility dhs suitability","citizenship us, data aggregation, data communication, data interpretation, data presentation, data storage, dataanalytics, eligibility dhs suitability, excel, metrics collection, metrics reporting, microsoft office suite, powerbi, powerpoint, presentation skills, r, statistical programming language, visualization"
"Senior Software Engineer, Data Transcoding - Autonomous Vehicles",NVIDIA,"Washington, United States",https://www.linkedin.com/jobs/view/senior-software-engineer-data-transcoding-autonomous-vehicles-at-nvidia-3761518112,2023-12-17,Wenatchee,United States,Mid senior,Remote,"NVIDIA is seeking to hire senior software engineers to develop and scale its Data Transcode framework! Our Data Transcode framework is the highly optimized transcoding engine widely used by AV Curation and Perception teams for data export and operations. In order to support this activity, you will need to have strong programming skills in C/C++/GoLang, and a decent knowledge of video coding fundamentals and mathematics. In addition, it is desired that you have experience with different types of sensor technologies and an understanding of cloud-based technologies.
What You'll Be Doing
Write flawless C++/Go code.
Design and build scalable data transcoding services that ensure the performance and robustness of the service.
Analyze root cause and triage issues.
Closely collaborate with our partner's team to implement new features and improve existing ones.
What We Need To See
Masters or equivalent experience in Computer Science, Electrical Engineering or related field with 12+ years of Work or Research Experience.
Strong programming background that incorporates methodologies like data structures, design patterns, OOP, and test-driven development.
Advanced programming skills to build efficient solutions optimized for runtime and memory footprint.
A specialist programmer in C/C++ and knowledge of Go.
Experience with or solid understanding of hardware and sensor technologies.
Switch effectively between long-term strategic management and near-term tactical management.
Highly motivated with strong interpersonal skills, you have the ability to work successfully with multi-functional teams, principles and architects and coordinate effectively across organizational boundaries and geographies.
Ways To Stand Out From The Crowd
Experience working with automotive sensor technologies.
Experience with hardware-accelerated video codecs API.
Strong mathematical skills.
A go-getter attitude to dive deep and understand technical requirements.
With competitive salaries and a generous benefits package, NVIDIA is widely considered to be one of the technology industry's most desirable employers. We have some of the most forward-thinking and talented people in the world working with us and our engineering teams are growing fast in some of the most impactful fields of our generation: Deep Learning, Artificial Intelligence, and Autonomous Vehicles. If you're a creative engineer who enjoys autonomy and shares our passion for technology, we want to hear from you!
The base salary range is 216,000 USD - 333,500 USD. Your base salary will be determined based on your location, experience, and the pay of employees in similar positions.
You will also be eligible for equity and benefits .
NVIDIA accepts applications on an ongoing basis.
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.
Show more
Show less","C++, GoLang, Data Transcode, Video Coding, Sensor Technologies, CloudBased Technologies, Data Structures, Design Patterns, OOP, TestDriven Development, Runtime Optimization, Memory Optimization, HardwareAccelerated Video Codecs, Mathematical Skills","c, golang, data transcode, video coding, sensor technologies, cloudbased technologies, data structures, design patterns, oop, testdriven development, runtime optimization, memory optimization, hardwareaccelerated video codecs, mathematical skills","c, cloudbased technologies, data structures, data transcode, design patterns, golang, hardwareaccelerated video codecs, mathematical skills, memory optimization, oop, runtime optimization, sensor technologies, testdriven development, video coding"
Senior Data Protection Operations Engineer,BECU,"Washington, United States",https://www.linkedin.com/jobs/view/senior-data-protection-operations-engineer-at-becu-3749156951,2023-12-17,Wenatchee,United States,Mid senior,Remote,"As the nation's largest community credit union, we begin every day focused on delivering superior financial products and services for our 1.3 million members and more than $30 billion in managed assets. Our work has an economic impact as we support our members' financial goals. We are unapologetic about being devoted to our members and the communities we serve. Our business is guided by our people helping people philosophy – which includes our team members.
BECU has been in business for more than 85 years, driven by unwavering core values and a dedication to improving the communities we serve. While we have a rich history, the future of our company, accelerated by business and technology transformation, is even brighter. There's never been a better time to work for BECU.
To learn more visit becu.org/careers.
PAY RANGE
The Target Pay Range for this position is $130,200-$159,200 annually. The full Pay Range is $101,100-$188,300 annually. At BECU, compensation decisions are determined using factors such as relevant job-related skills, experience, and education or training. Should an offer for employment be made, we will consider individual qualifications. In addition to your salary, compensation incentives are available for the hired applicant. Incentives are performance based and targets vary by role.
Benefits
Employees and their eligible family members have access to a wide array of employee benefits, such as medical, dental, vision and life insurance coverage. Employees have access to disability and AD&D insurance. We also offer health care and dependent care flexible spending accounts, as well as health savings accounts, to eligible employees. Employees are able to enroll in our company’s 401k plan and employer-funded retirement plan. Newly hired employees accrue 6.16 hours of paid time off (PTO) on a per pay period basis based on hours worked (up to a maximum of 160 PTO hours per year) and receive ten paid holidays throughout the calendar year. Additional details regarding BECU Benefits can be found here.
Impact You’ll Make
Are you looking to take your data protection career to a new level? Monitor. Detect. Analyze. Could you be our next guardian on our forefront team of defense as a Senior Data Protection Operations Engineer? In this role, you'll safekeep our digital realms, proactively monitoring and swiftly analyzing events in real time to keep our data fortress unbreachable.
You’ll use our tools and craft scripts crucial in rapid threat detection and response. You'll also sculpt and maintain systems ensuring top-tier data protection. Collaborate with fellow tech enthusiasts, sharing knowledge and elevating our protective expertise. You’ll maintain the defenses that will shape the future of our data safety.
What You’ll Do
Compliance Captain & Relationship Builder:
Uphold BECU standards and stay compliant while handling data protection. Collaborate and guide others, such as data owners and data analysts. Use your exceptional problem solving and interpersonal skills to effectively communicate the complex in simple terms as you provide customer service.
Design Lead:
Take the lead in shaping and rolling out data protection patterns for vendor solutions, ensuring sensitive info is always safeguarded. Stay updated on tech threats and emerging security trends. Review existing cloud security architecture and recommend improvement road map.
Systems Steward & Data Privacy:
Seamlessly manage and recover systems that track sensitive data, focusing on a user-friendly experience. Champion for enhancing our data privacy practices, boosting team efficiency in the process.
Risk Analyst & Incident Partner:
Dive deep into risk assessments, considering asset value, potential threats, and existing controls. Engage in incident responses, directing vital alerts to the Security Operations Center.
Tech Advisor & Vendor Liaison:
Guide Tech teams on best practices and tools within our tech stack to shield crucial info. Quick learner, Adjunct IT professor, team leadership experience, Gov tenant. Bridge the gap with our data protection vendors, ensuring we're always in good hands.
And yes, there might be a few extra tasks thrown in, and maybe some afterhours work because cybersecurity is always an ongoing threat, but you've got this! If this sounds like the next step in your career, let’s connect.
Basic Qualifications
Bachelor’s degree in a relevant technology or business management area or equivalent work experience.
Minimum of seven years working with security tools and/or information technology operations
Minimum of four years working with data tools, including performing deployment and configuration, and maintaining operations and content development
Implementation knowledge of Microsoft defender for endpoint, Microsoft defender for Identity, Microsoft, cloud App Security, Microsoft defender for O365
Preferred Qualifications
Advanced scripting skills (e.g., PowerShell, Python, JAVA) preferred.
Configured Microsoft Cloud App Security (MCAS) for customer cloud workload discovery, MCAS integrations with AWS, ServiceNow, Workday, Data loss prevention policies and integrated with AIP labels, Office 365 Advanced threat protection, Azure logs for AIP label analytics, and endpoint solutions for antivirus, antimalware, host-based intrusion prevention, and EDR.
Cloud security, Azure Sentinel, Splunk, CyberArk, IAAS, PAAS, Identity mgmt., Azure MFA,
eDiscovery, Compliance experience
Perform hands-on activities involved in creating cloud infrastructure components.
Deployed AIP/MIP labels using Crawl, Walk, Run
Deployed and configured on-premises AIP scanner to discover data
High level knowledge and configuration of Microsoft Defender ATP, Microsoft Threat Experts, EDR
Advanced understanding of Cyber Security Operations (monitoring, detection, incident response, forensics) required.
Advanced scripting skills (e.g., PowerShell, Python, JAVA)
Knowledge of TCP/IP and OSI network protocol stack required.
EEO Statement
BECU is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, veteran status, disability, sexual orientation, gender identity, or any other protected status.
Show more
Show less","Microsoft Defender, Microsoft Defender for Identity, Microsoft Cloud App Security, Microsoft Defender for O365, PowerShell, Python, JAVA, Cloud security, Azure Sentinel, Splunk, CyberArk, IAAS, PAAS, Identity mgmt., Azure MFA, eDiscovery, Compliance, Cloud infrastructure components, AIP/MIP labels, Onpremises AIP scanner, Microsoft Defender ATP, Microsoft Threat Experts, EDR, Cyber Security Operations, TCP/IP, OSI network protocol stack","microsoft defender, microsoft defender for identity, microsoft cloud app security, microsoft defender for o365, powershell, python, java, cloud security, azure sentinel, splunk, cyberark, iaas, paas, identity mgmt, azure mfa, ediscovery, compliance, cloud infrastructure components, aipmip labels, onpremises aip scanner, microsoft defender atp, microsoft threat experts, edr, cyber security operations, tcpip, osi network protocol stack","aipmip labels, azure mfa, azure sentinel, cloud infrastructure components, cloud security, compliance, cyber security operations, cyberark, ediscovery, edr, iaas, identity mgmt, java, microsoft cloud app security, microsoft defender, microsoft defender atp, microsoft defender for identity, microsoft defender for o365, microsoft threat experts, onpremises aip scanner, osi network protocol stack, paas, powershell, python, splunk, tcpip"
"Senior Business Data Analyst, Revenue Cycle *Remote*",Providence Health & Services,"Washington, United States",https://www.linkedin.com/jobs/view/senior-business-data-analyst-revenue-cycle-remote-at-providence-health-services-3756147146,2023-12-17,Wenatchee,United States,Mid senior,Remote,"Description
Providence St. Joseph Health is calling a Senior Business Data Analyst, Revenue Cycle to work remotely within our footprint states: AK, CA, MT, OR, TX and/or WA.
This position will be a contributor to the RCS Analytics Operations team which supports efforts of the Data&Insights team. The role (and team) supports the RCS Analytics team, leadership, and its stakeholders/customers. This position is seen as an accountable Business Analyst Sr for analytics-related activities. This role also supports various administrative and customer facing activities including, but not limited to intake/triage management, artifact management, policies&procedures, effort/time estimation, product training, Data Governance&Evangelism, inventory review with customers/leadership, resource onboarding, analytics ecosystem management, etc. The role may also serve in cross functional capacity on projects and initiatives.
Providence caregivers are not simply valued – they’re invaluable. Join our team at Revenue Cycle Business Services and thrive in our culture of patient-focused, whole-person care built on understanding, commitment, and mutual respect. Your voice matters here, because we know that to inspire and retain the best people, we must empower them.
Required qualifications:
Bachelor's Degree Computer science, mathematics, statistics, information technology, health informatics, public health, or related subject. -Or-
Bachelor's Degree Data Science, Analytics, Finance, Engineering, Mathematics, Computer Science, Business Administration, Health Administration, Public Health, Public Administration, or related discipline
2 – 3 years Revenue Cycle Experience
5 years Strong SQL programming skills. Some combination of experience with: Snowflake, SQL Server, Oracle SQL or similar platforms.
5 years Progressive experience in practicing analytics as a data analyst, report analyst, business analyst, BI developer, etc.
2 year's Experience with two or more provider-based Revenue Cycle vert
3 years Proven experience creating, managing, and validating large data sets.
Demonstrated PC skills, i.e., proficient with MS Office products including Word, Excel, Teams, SharePoint, Power Point and Outlook
Preferred qualifications:
Master's Degree Computer science, mathematics, statistics, information technology, health informatics, public health, or related subject.
Experience with EPIC: reporting data out of Epic, reviewing Revenue Cycle related account detail.
Report writing skills in Tableau or Power BI
Salary Range by location:
NorCal (Napa, Sonoma)
Min: $42.53 Max: $68.67
Southern California, NorCal (Humboldt) Alaska (Kodiak, Seward, Valdez)
Min: $37.91, Max: $61.20
WA Puget Sound Oregon (Portland) Alaska (Anchorage)
Min: $36.37, Max: $58.72
Oregon (Hood River, Medford, Seaside)
Min: $32.90, Max: $54.74
Eastern Washington (Richland, Spokane, Walla Walla)
Min: $32.36, Max: $52.25
Montana
Min:$29.28, Max: $47.27
Texas
Min:$27.74, Max: $44.78
Why Join Providence?
Our best-in-class benefits are uniquely designed to support you and your family in staying well, growing professionally, and achieving financial security. We take care of you, so you can focus on delivering our Mission of caring for everyone, especially the most vulnerable in our communities.
About Providence
At Providence, our strength lies in Our Promise of “Know me, care for me, ease my way.” Working at our family of organizations means that regardless of your role, we’ll walk alongside you in your career, supporting you so you can support others. We provide best-in-class benefits and we foster an inclusive workplace where diversity is valued, and everyone is essential, heard and respected. Together, our 120,000 caregivers (all employees) serve in over 50 hospitals, over 1,000 clinics and a full range of health and social services across Alaska, California, Montana, New Mexico, Oregon, Texas and Washington. As a comprehensive health care organization, we are serving more people, advancing best practices and continuing our more than 100-year tradition of serving the poor and vulnerable.
The amounts listed are the base pay range; additional compensation may be available for this role, such as shift differentials, standby/on-call, overtime, premiums, extra shift incentives, or bonus opportunities.
Check out our benefits page for more information about our Benefits and Rewards.
About The Team
Providence Shared Services is a service line within Providence that provides a variety of functional and system support services for our family of organizations across Alaska, California, Montana, New Mexico, Oregon, Texas and Washington. We are focused on supporting our Mission by delivering a robust foundation of services and sharing of specialized expertise.
We are committed to the principle that every workforce member has the right to work in surroundings that are free from all forms of unlawful discrimination and harassment.
We are committed to cultural diversity and equal employment for all individuals. It is our policy to recruit, hire, promote, compensate, transfer, train, retain, terminate, and make all other employment-related decisions without regard to race, color, religious creed (including religious dress and grooming practices), national origin (including certain language use restrictions), ancestry, disability (mental and physical including HIV and AIDS), medical condition (including cancer and genetic characteristics), genetic information, marital status, age, sex (which includes pregnancy, childbirth, breastfeeding and related medical conditions), gender, gender identity, gender expression, sexual orientation, genetic information, and military and veteran status or any other applicable legally protected status. We will also provide reasonable accommodation to known physical or mental limitations of an otherwise qualified caregiver or applicant for employment, unless the accommodation would impose undue hardship on the operation of our business.
We are a community where all people, regardless of differences, are welcome, secure, and valued. We value respect, appreciation, collaboration, diversity, and a shared commitment to serving our communities. We expect that all workforce members in our community will act in ways which reflect a commitment to and accountability for, racial and social justice and equality in the workplace. As such, we will maintain a workplace free of discrimination and harassment based on any applicable legally protected status. We also expect that all workforce members will maintain a positive workplace free from any unacceptable conduct which creates an intimidating, hostile, or offensive work environment.
Requsition ID:
224357
Company:
Providence Jobs
Job Category:
Strategy&Planning
Job Function:
Administration
Job Schedule:
Full time
Job Shift:
Career Track:
Department:
4001 SS RC RPTNG ANALYTICS
Address:
WA Liberty Lake 24001 E Mission Ave
Work Location:
Liberty Lake-Liberty Lake
Pay Range:
$See posting - $See posting
The amounts listed are the base pay range; additional compensation may be available for this role, such as shift differentials, standby/on-call, overtime, premiums, extra shift incentives, or bonus opportunities.
Check out our benefits page for more information about our Benefits and Rewards.
Providence is proud to be an Equal Opportunity Employer. Providence does not discriminate on the basis of race, color, gender, disability, veteran, military status, religion, age, creed, national origin, sexual identity or expression, sexual orientation, marital status, genetic information, or any other basis prohibited by local, state, or federal law.
Show more
Show less","SQL, Snowflake, SQL Server, Oracle SQL, Data Analyst, Report Analyst, Business Analyst, BI Developer, SAS, Tableau, Power BI, MS Office (Word Excel Teams SharePoint Power Point Outlook), EPIC, Healthcare, Revenue Cycle, Data Governance, Analytics, Databases, Data Warehousing, Data Mining, Business Intelligence, Reporting, Data Visualization, Data Modeling, Data Management","sql, snowflake, sql server, oracle sql, data analyst, report analyst, business analyst, bi developer, sas, tableau, power bi, ms office word excel teams sharepoint power point outlook, epic, healthcare, revenue cycle, data governance, analytics, databases, data warehousing, data mining, business intelligence, reporting, data visualization, data modeling, data management","analytics, bi developer, business analyst, business intelligence, data governance, data management, data mining, dataanalytics, databases, datamodeling, datawarehouse, epic, healthcare, ms office word excel teams sharepoint power point outlook, oracle sql, powerbi, report analyst, reporting, revenue cycle, sas, snowflake, sql, sql server, tableau, visualization"
Data Center Architect,Ascendion,"Washington, United States",https://www.linkedin.com/jobs/view/data-center-architect-at-ascendion-3774819398,2023-12-17,Wenatchee,United States,Mid senior,Remote,"About Ascendion
Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next.
Ascendion | Engineering to elevate life
We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:
Build the coolest tech for world’s leading brands
Solve complex problems – and learn new skills
Experience the power of transforming digital engineering for Fortune 500 clients
Master your craft with leading training programs and hands-on experience
Experience a community of change makers!
Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.
About the Role:
Job Title: AWS Data Center Project Architect
Daily Roles & Responsibilities:-
Travel for site assessments, internal design meetings, construction review, and interfacing with design consultants. Anticipated travel not to exceed 25%.
Communicate conceptual designs and create/maintain project documentation before, during, and after construction.
Maintenance of Basis of Design, prototype design, and template specifications for architectural elements.
Review and inform design RFPs.
Manage our external design consultants through the design and construction process.
Coordinate with internal and external Civil, Structural, Mechanical, Electrical, Controls, Cabling, and Security design engineers.
Effectively communicate design standards to internal and external project partners.
Manage multiple fast paced projects simultaneously.
Think outside of the box to find innovative solutions prior to and during the construction process to reduce costs without negative impacts on quality or reliability.
Requirements:-
Excellent communication skills and attention to detail.
NCARB recognized architecture license.
6+ years of design experience in commercial / industrial / other complex technical projects.
1+ years of Data Center or Mission Critical facility design experience.
1+ years leading sub-consultants and project teams.
Proficiency in building codes, regulations, and standards including IBC or equivalent.
Salary Range: The salary for this position is between $160,000 – $170,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.
Benefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]
Want to change the world? Let us know.
Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!
Show more
Show less","AWS, Data Center, Project Architect, Site assessments, Design meetings, Construction review, Conceptual designs, Project documentation, Basis of Design, Prototype design, Template specifications, Architectural elements, Design RFPs, External design consultants, Civil engineering, Structural engineering, Mechanical engineering, Electrical engineering, Controls engineering, Cabling engineering, Security engineering, Design standards, Project management, Communication skills, Attention to detail, NCARB architecture license, Commercial architecture, Industrial architecture, Complex technical projects, Data Center design, Mission Critical facility design, Subconsultants management, Project teams leadership, Building codes, Regulations, Standards, IBC","aws, data center, project architect, site assessments, design meetings, construction review, conceptual designs, project documentation, basis of design, prototype design, template specifications, architectural elements, design rfps, external design consultants, civil engineering, structural engineering, mechanical engineering, electrical engineering, controls engineering, cabling engineering, security engineering, design standards, project management, communication skills, attention to detail, ncarb architecture license, commercial architecture, industrial architecture, complex technical projects, data center design, mission critical facility design, subconsultants management, project teams leadership, building codes, regulations, standards, ibc","architectural elements, attention to detail, aws, basis of design, building codes, cabling engineering, civil engineering, commercial architecture, communication skills, complex technical projects, conceptual designs, construction review, controls engineering, data center, data center design, design meetings, design rfps, design standards, electrical engineering, external design consultants, ibc, industrial architecture, mechanical engineering, mission critical facility design, ncarb architecture license, project architect, project documentation, project management, project teams leadership, prototype design, regulations, security engineering, site assessments, standards, structural engineering, subconsultants management, template specifications"
Senior Data Insights Analyst,Cambia Health Solutions,"Washington, United States",https://www.linkedin.com/jobs/view/senior-data-insights-analyst-at-cambia-health-solutions-3770779771,2023-12-17,Wenatchee,United States,Mid senior,Hybrid,"Remote Within WA, OR, UT or ID
Primary Job Purpose
The Senior Data Insights Analyst is responsible for developing and providing analytics. These roles provide regular reporting and analytics on the performance and value of apps, web and mobile platforms and perform ad hoc analyses that inform site optimization and user experience design. These roles are also responsible for working with our product management and client service teams to perform analyses of impact of platforms which entails analysis method design, consulting, execution of the statistical analysis, mathematical modelling and reporting.
General Functions And Outcomes
Works cross functionally to understand business objectives, and provide analyses on routine and ad hoc projects that are directly relatable to business stakeholders.
Designs and executes analytics projects (conceptualization, method, data sourcing, analysis structure, and report/data visualization).
Ensures the timely delivery of accurate project reporting and analyses.
Provides insights to clients on tracking deliverables and ad hoc analysis projects which leverage insights from consumer digital site behavior with campaign, survey and consumer activity data, and claims data.
Performs analyses that move beyond an explanation of ‘what happened’ to ‘why it happened’ and ‘what should be done’. The successful analyst turns data observations into insights, insights into recommended actions, and ultimately developing strategy and site optimization.
Provide reporting to meet stakeholder agreements.
Creates, manages and drives analyses, KPI definition/measurement and analytics in support of corporate initiatives.
Collaborates with team members to define goals and KPI’s and identifies the best way to track metrics within digital platforms to assess success of new and existing site features and functionality in order to shape the strategy of future plans.
Collaborates with various internal and external teams to identify and prioritize recommendations based on insights uncovered from data analysis.
Collaborates with team members to develop analytical and statistical methods, best practices and systemic processes that enable standardization and automation of tracking and reporting and client self-service for routine reports.
Drive the continued development of customer understanding via quantitative and qualitative methodologies as it pertains to both online behavior as well as the intersection of online behavior and all other customer touch points.
Builds models that represent the consumer digital data in a manner that promotes understanding of influences on digital behavior.
Able to provide recommendations to client on appropriate methodology.
Provide guidance to level I analysts.
Provides work team leadership for cross functional projects.
Provides review of data sourcing and manipulation strategy, analytical methods and computational integrity.
Provide mentoring for developing Digital Insights Analysts, assisting in project design, method selection and review of analyses and quality.
Able to work with clients and/or senior leadership to make recommendations which allow clients and/or leadership to gain greater understanding regarding impacts of features/ functionality.
Able to guide company wide data strategy including recommendations for enhanced data capture and how to leverage data and analytics for greater impact to company
Minimum Requirements
Whole-brain thinking: Possessing the ability to think creatively and demonstrate analytical skills, analyzing complex situations both alone and as part of a team, learning quickly and synthesizing solutions, options and action plans.
Experience with tools for data mining, statistics, analysis, and scripting (e.g., Python, R, SAS, Scala, MATLAB, Ruby).
Demonstrated experience creating complex SQL queries for standard as well as ad hoc data mining purposes, analyze large amounts of data, interpret qualitative data (research, feedback) and incorporate into analyses.
Experience with data reporting tools- Tableau, Micro-strategy, SPSS, SQL Server Reporting Service)
Demonstrated understanding of research and ability to develop methodologies in data mining and other innovative statistical/mathematical approaches.
Ability to collaborate with team members and business partners to define, develop, and deliver web and digital analytics that meet the needs of the business to manage routine operations, identify tactical decisions, and inform strategic direction.
Ability to present complex data, analysis or findings to teams in a way that is clear and understandable and supports the overall business decisions and program efforts.
Demonstrated experience analyzing consumer web, app and mobile behavior.
Experience with web analytics software- Web-trends, Google Analytics, Adobe Analytics
Ability to work independently, managing deliverable timelines and clients.
Ability to develop and produce accurate, error free final deliverables and to participate in and support a peer review culture.
Experience with statistical modelling of results to drive enhanced decision-making.
Advanced skills in data extraction and manipulation.
Experience leading work teams both internal to the department and comprised of multi-disciplinary staff.
Expert in data extraction design and manipulation, coaching junior level analysts in best practices.
Normally to be proficient in the competencies listed above
Digital Insights Analyst Senior would have a Master’s degree in Statistics, Mathematics, Computer Science, Information Technology, or Economics or related field and 7+ years of related work experience or equivalent combination of education and experience.
The expected hiring range for a Senior Data Insights Analyst is $97,000 - $132,000 depending on skills, experience, education, and training; relevant licensure / certifications; performance history; and work location. The bonus target for this position is 15% . The current full salary range for this role is $91,500 - $149,000
Benefits
Base pay is just part of the compensation package at Cambia that is supplemented with an exceptional 401(k) match, bonus opportunity and other benefits. In keeping with our Cause and vision, we offer comprehensive well-being programs and benefits, which we periodically update to stay current. Some highlights:
medical, dental, and vision coverage for employees and their eligible family members
annual employer contribution to a health savings account ($1,200 or $2,500 depending on medical coverage, prorated based on hire date)
paid time off varying by role and tenure in addition to 10 company holidays
up to a 6% company match on employee 401k contributions, with a potential discretionary contribution based on company performance (no vesting period)
up to 12 weeks of paid parental time off (eligible day one of employment if within first 12 months following birth or adoption)
one-time furniture and equipment allowance for employees working from home
up to $225 in Amazon gift cards for participating in various well-being activities. for a complete list see our External Total Rewards page.
We are an Equal Opportunity and Affirmative Action employer dedicated to workforce diversity and a drug and tobacco-free workplace. All qualified applicants will receive consideration for employment without regard to race, color, national origin, religion, age, sex, sexual orientation, gender identity, disability, protected veteran status or any other status protected by law. A background check is required.
If you need accommodation for any part of the application process because of a medical condition or disability, please email CambiaCareers@cambiahealth.com. Information about how Cambia Health Solutions collects, uses, and discloses information is available in our Privacy Policy. As a health care company, we are committed to the health of our communities and employees during the COVID-19 pandemic. Please review the policy on our Careers site.
Show more
Show less","Data mining, Statistics, Analysis, Scripting, Python, R, SAS, Scala, MATLAB, Ruby, SQL, Data reporting, Tableau, Microstrategy, SPSS, SQL Server Reporting Service, Research, Methodologies, Data mining, Statistical, Mathematical, Collaboration, Presentation, Consumer web, App, Mobile behavior, Web analytics software, Webtrends, Google Analytics, Adobe Analytics, Data extraction, Manipulation, Statistical modelling, Decisionmaking, Data extraction, Design, Manipulation, Coaching, Master's degree, Statistics, Mathematics, Computer Science, Information Technology, Economics, 401(k) match, Bonus opportunity, Medical coverage, Dental coverage, Vision coverage, Health savings account, Paid time off, Parental time off, Furniture and equipment allowance, Amazon gift cards, Equal Opportunity, Affirmative Action, Workforce diversity, Drug and tobaccofree workplace, Background check, Privacy Policy","data mining, statistics, analysis, scripting, python, r, sas, scala, matlab, ruby, sql, data reporting, tableau, microstrategy, spss, sql server reporting service, research, methodologies, data mining, statistical, mathematical, collaboration, presentation, consumer web, app, mobile behavior, web analytics software, webtrends, google analytics, adobe analytics, data extraction, manipulation, statistical modelling, decisionmaking, data extraction, design, manipulation, coaching, masters degree, statistics, mathematics, computer science, information technology, economics, 401k match, bonus opportunity, medical coverage, dental coverage, vision coverage, health savings account, paid time off, parental time off, furniture and equipment allowance, amazon gift cards, equal opportunity, affirmative action, workforce diversity, drug and tobaccofree workplace, background check, privacy policy","401k match, adobe analytics, affirmative action, amazon gift cards, analysis, app, background check, bonus opportunity, coaching, collaboration, computer science, consumer web, data extraction, data mining, data reporting, decisionmaking, dental coverage, design, drug and tobaccofree workplace, economics, equal opportunity, furniture and equipment allowance, google analytics, health savings account, information technology, manipulation, masters degree, mathematical, mathematics, matlab, medical coverage, methodologies, microstrategy, mobile behavior, paid time off, parental time off, presentation, privacy policy, python, r, research, ruby, sas, scala, scripting, spss, sql, sql server reporting service, statistical, statistical modelling, statistics, tableau, vision coverage, web analytics software, webtrends, workforce diversity"
Senior Database Engineer,Belay Technologies,"Annapolis Junction, MD",https://www.linkedin.com/jobs/view/senior-database-engineer-at-belay-technologies-3787909079,2023-12-17,Baltimore,United States,Mid senior,Onsite,"Belay Technologies has been voted Baltimore Business Journal's (BBJ) Best Places to Work 2019, runner up in 2020 and a finalist in 2021!
Belay Technologies is seeking a Senior Database Engineer to join our intel team to upgrade and modernize the system. The ideal candidate shall have experience with Oracle database administration. Experience with Extract, Translate, and Load (ETL) operations and an understanding of dataflow into Oracle. Understanding of TechSIGINT data.
Candidates should have the following qualifications:
TS/SCI with Full Scope polygraph is required.
Twenty (20) years of experience in Database Engineering or in similar programs or contracts of similar scope, type, and complexity
Bachelor’s degree in a technical discipline from an accredited college or university
Candidates are required to have the following skills:
Experience with Oracle database administration.
Experience with Extract, Translate, and Load (ETL) operations and an understanding of dataflow into Oracle.
Understanding of TechSIGINT data.
Perks and Benefits:
8 weeks paid leave - 4 weeks of personal leave, 3 Yay! days, take off on your birthday,11 paid holidays and optional leave up to 6 days through Belay's volunteer program
10% matching in 401(k) contributions vested on day one
$5,000 annual training/tuition
Student Loan Repayment Program
100% company funded HSA
Rich medical coverage (100% coinsurance)
Dental coverage including orthodontia
Up to $420,000 in life insurance, premiums 100% company funded
Amazon Prime, gym reimbursement, monthly lunches, games and prizes
Pet adoption program, generous referral bonus program, fun events, and more!
Belay Technologies is a certified Service-Disabled Veteran-Owned Small Business located in Columbia, Maryland (Baltimore/Washington area). Belay Technologies specializes in systems automation and full stack development. Belay Technologies provides leading technology and engineering solutions to the DoD, as well as state-of-the-art commercial products. We hire software engineers, web designers, test engineers, systems engineers, systems administrators, database engineers and other tech services. We are an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, physical or mental disability, genetic factors, military/veteran status or other characteristics protected by law.
Powered by JazzHR
5V9nQpJbDo
Show more
Show less","Oracle database administration, Extract Translate and Load (ETL) operations, Dataflow into Oracle, TechSIGINT data, Full Scope polygraph, Database Engineering, Systems automation, Full stack development, ServiceDisabled VeteranOwned Small Business","oracle database administration, extract translate and load etl operations, dataflow into oracle, techsigint data, full scope polygraph, database engineering, systems automation, full stack development, servicedisabled veteranowned small business","database engineering, dataflow into oracle, extract translate and load etl operations, full scope polygraph, full stack development, oracle database administration, servicedisabled veteranowned small business, systems automation, techsigint data"
Software Engineer 1 (Data Management),"Farfield Systems, Inc","Linthicum, MD",https://www.linkedin.com/jobs/view/software-engineer-1-data-management-at-farfield-systems-inc-3787767548,2023-12-17,Baltimore,United States,Mid senior,Onsite,"About Farfield Systems, Inc
At Farfield we are committed to delivering trusted expertise to our government clients. As we grow, our focus is on increasing opportunities for you to grow with us while still delivering the same excellence customers have grown to expect from us. We continually evaluate our environment to provide a place where your career is packed with opportunities to grow and you have the ability to demonstrate your passion to our customers. We focus on building a Team where each employee is a valued member.
Farfield provides support to multiple agencies across the United States Government in many locations. That means many different opportunities to follow your career path without changing companies every few years.
""Employee driven...customer focused."" We build, operate and secure networks and infrastructure.
*** Requires a Top Secret/SCI clearance with a polygraph and U.S. Citizenship***
Software Engineer 1
(Data Management):
Data Management - The analytics team is focused on delivering reliable, accurate data to our end users.
Primary Skills:
- PIG
- Py-Spark
Secondary Skills:
- NiFi
- PressureWave
Powered by JazzHR
P1M3PMEQug
Show more
Show less","PIG, PySpark, NiFi, PressureWave","pig, pyspark, nifi, pressurewave","nifi, pig, pressurewave, spark"
Senior Data Engineer/Architect,RBR-Technologies,"Fort Meade, MD",https://www.linkedin.com/jobs/view/senior-data-engineer-architect-at-rbr-technologies-3687613118,2023-12-17,Baltimore,United States,Mid senior,Hybrid,"RBR-Technologies is seeking a
Senior Data Engineer/Architect
to support our DCSA customer from our Odenton, MD office. Our specific work, which revolves around technology modernization for Industrial security, is an ongoing effort to add an analytic capability for industry partners and Government analysts that collects, stores, automates and centralizes information supporting risk analysis for entities in the Defense Industrial Base (DIB). The candidate will support DCSA Industrial Security technology modernization by delivering data analytic services that will integrate with existing web application development to enhance and extract insights from specific commercial and government data.
As part of the RBR PM NISS Enterprise Engineering Team, the
Senior Data Engineer/Architect
will be responsible for leading design discussions, gathering and analyzing data requirements, documentation, and processes to discover and optimize operational solutions. Deliverables include enterprise database design, system architecture, requirement documentation, and process development. This position is based in the Odenton, MD area with a need to commute into our office two (2) times a week.
Responsibilities
Define technical standard of data security cross data modeling, data storage, data access, data retention, data reporting etc.
Design and delivery execution of library, API, or applications across the industrial security enterprise that enable and ensure data security.
Design and integration of tools that enable identification of data that can be integrated into Industrial Security systems.
Establish and maintain an Enterprise Data Model that describes how data is processed, stored, and utilized.
Manage the data structures required to support the enterprise.
Provide the means of system integration by understanding the source and target data and build the data mapping.
Determine architectural approaches for data environments and help ensure that the data needs of the enterprise are being met.
Responsible for developing and maintaining a formal description of the data and data structures - this can include data definitions and data models. Build strategy and design for managing data history.
Assist in the development of the DCSA Data Governance process.
Manage Internal Process Governance (Data acquisition and tool/analytic development to support DCSA mission).
Provide SME consulting for projects and planning with relation to data and data collection for Industrial Security systems.
Collect and track requirements tied to specific data sets.
Requirements
Must have an active DoD Secret clearance
Bachelor's degree and 12+ years of relevant experience or 16+ years of relevant experience
8+ years experience in data governance and data acquisition activities
Good oral and written communication skills
Experience with various Microsoft technologies such as MS Office 2013 (Word, Excel, and PowerPoint) and SharePoint
Demonstrated ability to work with limited supervision
5 years of direct experience managing/working with DoD IT systems
Robust understanding of the Software Development Life Cycle (SDLC), to include requirements collection, validation and decomposition
Strong proficiency in briefing senior leaders/executives
Good understanding of agile software development methodologies
Desired Qualifications
Agile Scrum/Kanban principles, PMI/ACP, ScrumMaster, Product Owner, PMI PMP
Experience with the Atlassian Suite of tools to include JIRA & Confluence
KM Platforms (SharePoint, MS Teams, Wiki tools) experience
Strong background in DoD Acquisition policy, to include DoD Instruction 5000.87 - Software Acquisition Pathway
Oversight and execution of DoD testing, specifically DISA and DCSA test policies
Understanding of DoD cybersecurity requirements for IT systems and cloud-based platforms
Top Secret clearance
Equal Employment Opportunity has been, and will continue to be, a fundamental principle at RBR-Technologies, where employment is based on personal capabilities and qualifications without discrimination because of race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any protected characteristic as established by law. This policy of Equal Employment Opportunity applies to all policies and procedures relating to recruitment, employment, promotion, transfer, training, working conditions, compensation, benefits, termination, and all other conditions of employment.
Show more
Show less","Data governance, Data acquisition, Data engineering, Data architecture, Enterprise Data Model, Agile Scrum/Kanban principles, Atlassian Suite of tools, KM Platforms (SharePoint MS Teams Wiki tools), DoD Acquisition policy, Testing policies, Cybersecurity requirements, MS Office 2013 (Word Excel PowerPoint), SharePoint","data governance, data acquisition, data engineering, data architecture, enterprise data model, agile scrumkanban principles, atlassian suite of tools, km platforms sharepoint ms teams wiki tools, dod acquisition policy, testing policies, cybersecurity requirements, ms office 2013 word excel powerpoint, sharepoint","agile scrumkanban principles, atlassian suite of tools, cybersecurity requirements, data acquisition, data architecture, data engineering, data governance, dod acquisition policy, enterprise data model, km platforms sharepoint ms teams wiki tools, ms office 2013 word excel powerpoint, sharepoint, testing policies"
Senior Data Services Engineer,Mr. Cooper,"Lewisville, TX",https://www.linkedin.com/jobs/view/senior-data-services-engineer-at-mr-cooper-3786372704,2023-12-17,Denton,United States,Mid senior,Onsite,"Who We Are
We are Xome, a real estate services company headquartered in the Dallas, Texas area. As a subsidiary of Mr. Cooper Group, we employ over 1,200 team members nationwide. The nation’s largest financial services companies look to us for integrated and scalable business solutions that help simplify the mortgage and real estate process.
At the heart of everything we do is our purpose: To keep the dream of home ownership alive. If that sounds like a big, lofty goal, that’s because it really is. And we can’t do it alone. Our entire team is focused on helping create a stable and healthy housing industry. And, making sure the process of buying/selling a home doesn’t undermine the excitement of home ownership. That’s why we battle every day against the mediocrity of the status quo to simplify the complex world of mortgage servicing, lending and banking. We see ourselves as the experts who make doing business easier. While others bring complexity and a lack of transparency, we offer simplicity, trust, and visibility across the entire property lifecycle. And we deliver radical customer service.
Now, you might be wondering; how exactly do you pronounce Xome? Simple. ZOM (like home, if it started with a z)!
There’s no place like XomeTM! At Xome, we believe the process of buying and selling a home shouldn’t undermine the excitement of home ownership, so we’ve reimagined the real estate experience to create a bridge­­­­­­ between the offline and online world. Xome is the only platform that digitally connects every major touch-point in the real estate process— giving our customers unique visibility and access into all parts of the real estate transaction process.
Our culture encourages collaboration, breakthrough thinking and work that makes a lasting difference. We reward initiative and informed decisions and empower you to act in the best interests of our customers and our company. Our decisions are guided by our key principles of: Putting Customers First, having a Bias for Action, Failing Fast, Innovating and Breaking Paradigms, and Creating Sustainable results.
Here at Xome, we recognize that our past, present, and future success is driven by the passionate and talented people we employ, and we are always looking for smart, dedicated people to join our teams. We put a priority on not simply hiring incredible people, but retaining them with engaging, challenging jobs – as well as a respect for the importance of balance and work-life integration.
If you are excited by the idea of building revolutionary products to simplify real estate nationwide and want to help us raise the bar – we look forward to hearing from you.
Job Summary
The Senior Data Services Engineer is responsible for designing and developing data repositories for enterprise business operations to be used for providing business reporting and analysis. The Senior Data Services Engineer will support the analytics team by providing accurate, timely and relevant data to meet their diverse requirements. The Senior Data Services Engineer will also work closely with internal teams to support and build business insight tools.
Essential Job Functions
Designs data architectures and builds relational/dimensional databases and establish methods to improve functional reporting data content and completeness of data.
Design and develop robust, re-usable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming.
Provide technical skills in designing and developing BI/Data projects in the areas of optimal design patterns, models, standards and code to ensure consistency and realize benefits of a high-performing, secure, reliable and scalable architecture
Collaborate with IT architecture/data team/data scientists to develop a practical end state and reference architectures for BI/Data with considerations for distributed data, in-memory computing, cloud computing, visualization tools/platforms
Provide technical leadership in areas such as master data management and reference data management to reduce duplication and redundancy for core data objects
Work with the data governance team to ensure alignment of data definition, quality specifications, models and meta data management to technical implementations.
Develops relationships with the larger development teams that promote trust and increase efficiency and effectiveness
Participate in application validation and QA efforts as they pertain to reporting, data, metrics, and report creation and execution.
Other duties as assigned.
Education / Experience Requirements
Bachelor's Degree or Foreign equivalent in Information Technology, Computer Science, Computer Information Systems, Engineering or related field and 4 years of progressively responsible experience OR a Master’s Degree in Information Technology, Computer Science, Computer Information Systems, Engineering or related field (foreign equivalent acceptable) and 2 years of progressively responsible experience.
4+ progressive years of experience in Data Warehouse, SQL scripting, SQL Tuning and Business Intelligence technologies.
4+ progressive years of experience in Python programming language for scripting and ETL development.
4+ progressive years of strong data engineering, SQL scripting and tuning experience.
Years of progressive experience should include strong knowledge of any of the ETL tools like SSIS, Databricks, Azure Data Factory, Informatica; any of the Data Modeling tools like Erwin, ER/Studio, Toad Data Modeler; any of the BI tools like SSRS, Tableau, Microsoft Power BI.
Must be experienced in designing and implementing high performance data pipelines using Databricks for data analytics with any major cloud platform like AWS or Azure.
Must be experienced in developing and implementing data quality and data governance standards.
Must be experienced implementing high performing technical solutions related to ETL with large source environments and patterns related to ODS, MDM, Landing/Staging areas and EDW/Data Mart.
Experience in managing data and analytics programs (people, process, tools) through the full lifecycle: strategic recommendation; design of experiments, testing, communication, pilot, implementation, etc.
Knowledge of developing and maintaining formal documentation that describes the data and data structures including data modeling.
Ability to work with senior technical and business resources providing technical guidance related to data architecture and governance.
In-depth knowledge of IT concepts, strategies and methodologies and their application to business opportunities.
Familiarity with Real Estate terminology and standards.
Ability to mentor junior Data Engineers.
Mortgage or Finance industry experience is a big plus.
Xome is committed to nurturing a diverse and inclusive environment where every employee is empowered to be their authentic self. We know that a large part of our success as a business is directly tied to our ongoing efforts to attract and retain diverse talent and maintain an inclusive environment where each employee can thrive. Embracing and leveraging diversity through an inclusive work environment fosters new ideas, new insights, and constant innovation. We strive to weave the principles of diversity and inclusion throughout the fabric of how we work, how we interact, and how we engage with our customers and the community.
Job Requisition ID
020964
Job Category
Information Technology
Primary Location City:
Lewisville
Primary Location Region:
Texas
Primary Location Postal Code:
75067
Primary Location Country:
United States of America
Additional Posting Location(s):
Home - California
Pay Range: $110,000.00 - $138,000.00
Show more
Show less","AWS, Azure, Azure Data Factory, BI, Business Intelligence, Cloud computing, Data architecture, Data Engineering, Data Factory, Data governance, Data Mart, Data Modeling, Data Pipeline, Data Quality, Data Science, Data Structures, Data Warehouse, Databricks, EDW, ETL, Informatica, IT, Landing Zones, Machine Learning, Master Data Management, MDM, Microsoft Power BI, Mortgage, ODS, Python, Real Estate, Reference Data Management, SSIS, SSRS, Staging Areas, SQL, SQL scripting, SQL Tuning, Tableau, Toad Data Modeler","aws, azure, azure data factory, bi, business intelligence, cloud computing, data architecture, data engineering, data factory, data governance, data mart, data modeling, data pipeline, data quality, data science, data structures, data warehouse, databricks, edw, etl, informatica, it, landing zones, machine learning, master data management, mdm, microsoft power bi, mortgage, ods, python, real estate, reference data management, ssis, ssrs, staging areas, sql, sql scripting, sql tuning, tableau, toad data modeler","aws, azure, azure data factory, bi, business intelligence, cloud computing, data architecture, data engineering, data factory, data governance, data mart, data pipeline, data quality, data science, data structures, databricks, datamodeling, datawarehouse, edw, etl, informatica, it, landing zones, machine learning, master data management, mdm, microsoft power bi, mortgage, ods, python, real estate, reference data management, sql, sql scripting, sql tuning, ssis, ssrs, staging areas, tableau, toad data modeler"
"Sr. Data Analyst || Hybrid Role (Smithfield, RI, Durham, NC, or Westlake, TX)",Steneral Consulting,"Westlake, TX",https://www.linkedin.com/jobs/view/sr-data-analyst-hybrid-role-smithfield-ri-durham-nc-or-westlake-tx-at-steneral-consulting-3647813123,2023-12-17,Denton,United States,Mid senior,Onsite,"Title: Sr. Data Analyst
Location: hybrid 5 days onsite/month. Smithfield, RI, Durham, NC, or Westlake, TX!
Duration: Through end of 2023, possible extension
We are currently sourcing for a Sr. Data Analyst to work at Fidelity's location in Smithfield, RI, Durham, NC, or Westlake, TX!
Must Have
7+ years experience with Data Modeling
5+ years experience working with Data Warehouse
Experience working with AWS
Nice To Have
Financial industry exposure would be really nice to have since this would really help when talking to our business stakeholders
Experience working with Data Lakes within Snowflake
Show more
Show less","Data Modeling, Data Warehouse, AWS, Data Lakes, Snowflake","data modeling, data warehouse, aws, data lakes, snowflake","aws, data lakes, datamodeling, datawarehouse, snowflake"
"Principal Engineer – Database, SRE, & Cloud Engineering",Genuine Parts Company,"Coppell, TX",https://www.linkedin.com/jobs/view/principal-engineer-%E2%80%93-database-sre-cloud-engineering-at-genuine-parts-company-3700370896,2023-12-17,Denton,United States,Mid senior,Onsite,"Company Background:
Genuine Parts Company (“GPC” or the “Company”), founded in 1928 and based in Atlanta, Georgia, is a leading distributor of automotive and industrial replacement parts and value-added services. The Company operates a global portfolio of businesses with more than 10,000 locations across the world. GPC has approximately 50,000 global employees. The Company has operations in the United States, Canada, Mexico, Australia, New Zealand, Indonesia, Singapore, France, the U.K., Germany, Poland, the Netherlands, Belgium, Spain and China.
Position Purpose:
Seeking world-class talent to join the world’s leading distributor of automotive and industrial replacement parts and value-added services operating 5,500+ locations and servicing more than 20,000 locations in the U.S and Canada.
This individual must be a network technologist & engineer at heart and be comfortable in defining the technology direction and being hands on with the execution of the strategy. She/He must exhibit a deep understanding of modern technology stack and agile delivery models, demonstrated focus on emerging technologies, including cloud-based solutions, and must have a proven track record of modernizing complex networking infrastructures at scale.
Close collaboration and alignment with a wide variety of both internal stakeholders and external vendors will be required. As such, exceptional abilities in building and maintaining strong working relationships will be required. High level communication, expert level technical acumen, and project oversight skills are ideal.
The Principal Engineer is ultimately responsible for shepherding a project team, contributing to the solution design, provide assistance with configuration development, and ensuring efficient and precise deployment within the complex GPC Enterprise Network environment.
The engineering process is highly collaborative. In addition to pairing, Principal Engineers field questions from other teams and organizations, encourage cross-team collaboration, and serve as the Subject Matter Expert for all Network related inquiries. They also play an active role working with 3rd party vendors as well as the open-source community.
Principal Engineer’s create foundational processes and technical focus that aligns with the forward-looking GPC communications and connectivity strategy. The Principal Engineer will also be responsible for the organization, presentation, and remediation of network architectural diagrams and other solution-related documentation. In addition, Principal Network Engineers will be involved in solution configuration, performance tuning and testing as well as production monitoring.
As Principal Engineer, you will be the technology Subject Matter Expert within the GPC Network Engineering Team and are expected to build and grow the skillsets of the more junior engineers and collaborate with Network Engineering Team management to devise and deliver unique and innovative networking solutions that optimize GPC’s products, applications, and service offerings.
Major Tasks, Responsibilities And Key Accountabilities
~40% Execution & 30% Engineering Leadership
Collaborates and pairs with other network and product/application team members to deliver secure, reliable, and scalable network solutions
Develops comprehensive processes and procedures geared to the sustainment of the GPC network
Delivers technology solutions and recommendations providing high reliability and resiliency in the networking environment.
Performs as the GPC Network Team Subject Matter Expert with all solutions and architectures deployed within the network.
Collaborates with the Network Team management group to efficiently delivery new and innovative solutions to GPC networking.
Provides mentoring and technical oversight to the GPC Engineering Team during solution delivery and configuration.
Creates meaningful architecture diagrams and other documentation essential to the operation and management of the GPC Network.
~20% – Support & Enablement:
Fields questions and inquiries concerning the GPC Network from other entities throughout the company.
Collaborates with the Network Support team for the efficient and effective resolution of issues encountered within the network.
Oversees and implements architectural modifications and improvements to the network environment.
Works with vendors to help identify and implement solutions and capabilities essential to the GPC service offering.
Proactively reviews the Engineering Team skillsets, capabilities, and performance in the network and provides recommendations for improvements.
Triages high priority issues and outages as they arise
~10% – Learning:
Moderates and manages Network Team development activities including cross-functionality training, mentoring, and general strategy discussions.
Learns, through reading, tutorials, and videos, new technologies and best practices being used within other technology organizations
Attends conferences and learns how to apply new technologies where appropriate
Desired Qualifications & Experiences:
7-10+ years continued experience with Network Engineering and Support
Consistent track record of teamwork, and delivering high impact results
Experience with public cloud (AWS, Azure, GCP)
Strong communication skills
Network Certifications (Cisco CCIE, AWS Solutions Architect, Azure, etc.)
Specific Experience with the following solutions and technologies:
Cisco Routing and Switching Solutions – (WAN, LAN, ACI)
Cisco Security Solutions – (ASA, Umbrella, FTD)
Cisco Meraki Wireless Solutions
Fortinet Firewall Solutions – (VPN, Virtual Appliances, Firewalls)
Load Balancing Solutions and techniques – (F5, VMWare Avi, I-Rules)
Automation and Scripting – (Ansible, API, Python, JSON)
Not the right fit? Let us know you're interested in a future opportunity by joining our Talent Community on jobs.genpt.com or create an account to set up email alerts as new job postings become available that meet your interest!
GPC conducts its business without regard to sex, race, creed, color, religion, marital status, national origin, citizenship status, age, pregnancy, sexual orientation, gender identity or expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. GPC's policy is to recruit, hire, train, promote, assign, transfer and terminate employees based on their own ability, achievement, experience and conduct and other legitimate business reasons.
Show more
Show less","Network engineering, Network architecture, Cloud computing, Agile methodology, Collaboration, Communication, Technical leadership, Problem solving, Troubleshooting, Continuous learning, Cisco Routing and Switching Solutions, Cisco Security Solutions, Cisco Meraki Wireless Solutions, Fortinet Firewall Solutions, Load Balancing, Automation, Scripting, Ansible, API, Python, JSON","network engineering, network architecture, cloud computing, agile methodology, collaboration, communication, technical leadership, problem solving, troubleshooting, continuous learning, cisco routing and switching solutions, cisco security solutions, cisco meraki wireless solutions, fortinet firewall solutions, load balancing, automation, scripting, ansible, api, python, json","agile methodology, ansible, api, automation, cisco meraki wireless solutions, cisco routing and switching solutions, cisco security solutions, cloud computing, collaboration, communication, continuous learning, fortinet firewall solutions, json, load balancing, network architecture, network engineering, problem solving, python, scripting, technical leadership, troubleshooting"
Azure Data Lead Analyst,Kubota Tractor Corporation,"Grapevine, TX",https://www.linkedin.com/jobs/view/azure-data-lead-analyst-at-kubota-tractor-corporation-3784645147,2023-12-17,Denton,United States,Mid senior,Onsite,"For Earth For Life
BASIC PURPOSE AND SCOPE OF POSITION
This position requires a collaboration with business users/analysts/scientists to identify the required data and bring them into Azure analytics platform.
PRINCIPAL ACTIVITIES
: This position does the following in accordance with all applicable Federal, State and local laws / regulations and the Company’s policies, procedures and guidelines:
Design, build, and maintain datasets in Azure Data lake/SQL DB/Synapse by extracting data from various source systems
Design and develop data processing
Monitor and optimize data storage and data processing
Work with Security Engineers to implement data security
Actively involved with Production Support and troubleshoot the issues in live system by engaging appropriate internal and external stakeholders, being responsive, leading the root cause analysis and thoughtful on upstream and downstream impacts of the change. Provide quick resolution of the problems ensuring minimal downtime to the business and adhering to the change control process of the organization
SPECIAL PROJECTS
As assigned.
Minimum Qualifications
EDUCATION, CERTIFICATIONS, AND TRAINING:
Bachelor's degree in Computer Science, Information systems, Engineering or a related discipline; or equivalent combination of education and experience
Certification of Data Engineering on Microsoft Azure is preferred
Skills And Background
Overall 5+ years of experience in the Data and Analytics domain, performing Data Engineering on Business Intelligence, Digital and Data Warehouse projects.
At least 2 full life-cycle Azure implementations preferred
Must be an expert in implementing data warehouse, data lakes, big data, data streaming and ingestion, data retention and archiving, ETL, etc.
Should have experience with Azure Storage/Data Lake, Azure Data Factory, Azure Databricks, Data processing languages SQL and Python or, Data Wrangling, Azure Synapse, and S/4 data structure, preferred
Participates in multiple design activities with authority to make independent choices free from supervision
Requires strong communication skills to work internally with end user departments, management and peers. Develops and provides written and verbal presentations to end users, peers, and support personnel
Language Requirements
Must be able to read, write and communicate in English.
EQUIPMENT OPERATION
(% of time, description, nature of service):
Office equipment including computer, copier, fax, phone, printer
Physical Requirements
Typical office environment.
Additional Information
DISCLAIMER:
The information provided in the description has been designed to indicate the general nature and level of work performed by incumbents within the classification. This description is not intended to be a comprehensive inventory of all duties, responsibilities, qualifications and working conditions required of employees assigned to this job/classification. This job is intended to include the current essential functions of the job. Management reserves the right to add or modify the duties and responsibilities and to designate other functions as essential at any time.
Kubota is an equal opportunity at will employer and does not discriminate against any employee or applicant for employment because of age, race, religion, color, disability, sex, sexual orientation or national origin.
Apply Now
Show more
Show less","Azure, SQL, Python, Data engineering, Data warehouse, Big data, Data streaming, Data ingestion, Data retention, Data archiving, ETL, Azure storage, Azure Data Lake, Azure Data Factory, Azure Databricks, Data processing, Azure Synapse, S/4 data structure","azure, sql, python, data engineering, data warehouse, big data, data streaming, data ingestion, data retention, data archiving, etl, azure storage, azure data lake, azure data factory, azure databricks, data processing, azure synapse, s4 data structure","azure, azure data factory, azure data lake, azure databricks, azure storage, azure synapse, big data, data archiving, data engineering, data ingestion, data processing, data retention, data streaming, datawarehouse, etl, python, s4 data structure, sql"
Lead Data Engineer,Dice,"Lewisville, TX",https://www.linkedin.com/jobs/view/lead-data-engineer-at-dice-3788098613,2023-12-17,Denton,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Braintree Technology Solutions, is seeking the following. Apply via Dice today!
Lead Data Engineer
Location: Lewisville, TX USA, This is Onsite role
Job Functions / Responsibilities
Build and document automated data pipelines from a wide range of data sources with an emphasis on automation and scale
Develop highly available applications and APIs to support near real time integrations using an AWS based technology stack Ensure product and technical features are delivered to spec and on time in a DevOps fashion Contribute to overall architecture, framework, and design patterns to store and process high data volumes
Develop solutions to measure, improve, and monitor data quality based on business requirements
Design and implement reporting and analytics feature in collaboration with product owners, reporting analysts / data analysts, and business partners within an Agile / Scrum methodology
Proactively support product health by building solutions that are automated, scalable, and sustainable ¿ be relentlessly focused on minimizing defects and technical debt Provide post implementation production support for data pipelines
Qualifications
Bachelors' degree in Computer Science, Informatics, or a related field required Masters¿ degree in Computer Science preferred
3+ years of experience in a data engineering role
2+ years of experience with AWS and related services (e.g., EC2, S3, SNS, Lambda, IAM, Snowflake)
Hands on experience with ETL tools and techniques (Desirable) Basic proficiency with a dialect of ANSI SQL, APIs, and Python Knowledge of and experience with RDBMS platforms, such as MS SQL Server, MySQL, NoSQL, Postgres
Show more
Show less","Data Engineering, AWS, EC2, S3, SNS, Lambda, IAM, Snowflake, ETL tools and techniques, SQL, Python, RDBMS, MS SQL Server, MySQL, NoSQL, Postgres, Agile, Scrum, DevOps","data engineering, aws, ec2, s3, sns, lambda, iam, snowflake, etl tools and techniques, sql, python, rdbms, ms sql server, mysql, nosql, postgres, agile, scrum, devops","agile, aws, data engineering, devops, ec2, etl tools and techniques, iam, lambda, ms sql server, mysql, nosql, postgres, python, rdbms, s3, scrum, snowflake, sns, sql"
Staff Data Engineer,Recruiting from Scratch,"Watauga, TX",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744397184,2023-12-17,Denton,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data classification, data retention","airflow, business intelligence, data classification, data engineering, data retention, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, python, schema design, snowflake, spark, sparkstreaming, sql, storm"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Watauga, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748831276,2023-12-17,Denton,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, ETL, Data Warehouses, Data Management Tools, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, etl, data warehouses, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Senior Data Engineer,HTC Global Services,"Addison, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-htc-global-services-3780003338,2023-12-17,Denton,United States,Mid senior,Hybrid,"Job Title: Senior Data Engineer
Location: Addison, TX
Length: Fulltime
About HTC Global Services:
Shaping careers since 1990 - our long tenured employees are a testimony of the work culture. Join our global employee base of 12,000 and help us bring human expertise to tech in order to deliver purposeful solutions that amplify value.
Requirements:
Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and micro-services.
Develop new services in AWS using server-less and container-based services. Work with Spark clusters and Bigdata ecosystem tools on-prem and in the cloud.
Minimum Qualifications:
Proficient in Python and Spark
Hands-on experience with Azure/AWS/GCP
Hands-on experience with Data Lake or Data Warehouse
Intermediate to advanced SQL skills
Experience in using Serverless Development
Should have the ability to work and contribute beyond defined responsibilities
Excellent communication/inter-personal skills a must
Attitude and aptitude to learn new technologies in a fast-paced environment
Effective problem-solving skills
Ability to work in a fast-paced environment with a ""can do"" attitude
Preferred Qualifications:
8+ Years of working experience in relevant technologies.
Besides Minimum Qualification below will be considered added advantages
Working experience on Python, Airflow, Apache Spark , Apache Beam, Apache Flink, Kubernetes etc.
Experience with CI/CD and DevOps is added advantage
Working Knowledge of OpenShift
Familiarity in using AIOPS platforms like mlFlow, AutoML
Knowledge on Kafka is added advantage
Bachelors in Computer Engineering and/or Computer Science and/or Information Technology.
Benefits:
At HTC Global Services our associates have access to a comprehensive benefits package that includes Health, Dental, Vision, Paid-Time-Off, Paid Holidays, 401K matching, Life and Accidental Death Insurance, Short- & Long-Term Disability Insurance, and a variety of other offerings.
Move ahead:
Our success as a company is built on practicing inclusion and embracing diversity. HTC Global Services is committed to providing a work environment free from discrimination and harassment, where all employees are treated with respect and dignity. Together we work to create and maintain an environment where everyone feels valued, included, and respected. At HTC Global Services, our differences are embraced and celebrated. HTC is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills, and experiences within our workforce. HTC is proud to be recognized as a National Minority Supplier and an equal opportunity employer of protected veterans.
Show more
Show less","Python, Spark, Azure, AWS, GCP, Data Lake, Data Warehouse, Serverless Development, Airflow, Apache Spark, Apache Beam, Apache Flink, Kubernetes, OpenShift, AIOPS, mlFlow, AutoML, Kafka, CI/CD, DevOps","python, spark, azure, aws, gcp, data lake, data warehouse, serverless development, airflow, apache spark, apache beam, apache flink, kubernetes, openshift, aiops, mlflow, automl, kafka, cicd, devops","aiops, airflow, apache beam, apache flink, apache spark, automl, aws, azure, cicd, data lake, datawarehouse, devops, gcp, kafka, kubernetes, mlflow, openshift, python, serverless development, spark"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Frisco, TX",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759710407,2023-12-17,Denton,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Mining, Data Cleaning, Normalization, Modeling, Pandas, R, Airflow, KubeFlow, NLP, Python, Java, bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Classification, Data Retention, 401K","data engineering, machine learning, data mining, data cleaning, normalization, modeling, pandas, r, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, data retention, 401k","401k, airflow, applied machine learning, aws, azure, bash, data classification, data cleaning, data engineering, data mining, data retention, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, modeling, nlp, normalization, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Frisco, TX",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773088708,2023-12-17,Denton,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Pipeline Design, Data Preprocessing, Data Postprocessing, ML Datasets, Statistical Analysis, Data Visualization, Distributed Systems, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, Relational Databases, NoSQL Databases, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine Learning, Data Classification, Data Retention, 401K, Equity Programs","data pipeline design, data preprocessing, data postprocessing, ml datasets, statistical analysis, data visualization, distributed systems, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, relational databases, nosql databases, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data classification, data retention, 401k, equity programs","401k, airflow, aws, azure, bash, data classification, data pipeline design, data postprocessing, data preprocessing, data retention, distributed systems, docker, dynamodb, equity programs, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, ml datasets, nlp, nosql databases, python, relational databases, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Sr Data Solutions Analyst,National Life Group,"Addison, TX",https://www.linkedin.com/jobs/view/sr-data-solutions-analyst-at-national-life-group-3771069738,2023-12-17,Denton,United States,Mid senior,Hybrid,"Senior Data Solutions Analyst
Overview
At National Life Group we are in the process of changing how we think about, organize, and use data. In order to get the most value from our data it is critical that we have timely, accurate, consistent and well-defined data throughout the organization. Success depends on strong commitment & collaboration from all areas of the organization. The Analytical Governance Team is key to enabling this transformation.
The Senior Data Solutions Analyst Is a Key Member Of Our Analytics Center Of Excellence, Which Provide Two Main Functions To The Organization
Facilitation of enterprise reporting & analytics by administering tools, providing business ready data sets, & assisting in delivering high quality information to decision makers.
Support of data projects & the data governance framework through advising on deliverables related to new and existing analytical solutions.
Obviously, we are looking for someone who loves data! Beyond that, a successful candidate will embrace the type of work that comes with a transformative effort. They will also contribute to the culture of National Life – where servant leadership, company values & applying learnings to performance are valued above all else.
The Senior Data Solutions Analyst role is a great opportunity to join a fast-growing team that is central to National Life’s data transformation. You will be exposed to different teams who work as part of the data delivery chain. Through this work you will get exposure and guidance from senior and executive leaders across the enterprise and a chance to collaborate with peers from other departments.
Responsibilities
Job Responsibilities:
After getting up to speed on National Life’s processes they will support the two functions above, primarily through owning end-to-end one of our analytical applications and providing ad-hoc analysis for key stakeholders. A typical day-to-day experience will vary, but will be some combination of:
Owning the end-to-end experience of one of our analytical applications, including user training, user experience, incidents, upgrade support, etc.
Providing general support of data & analytics as a subject matter expert for a specific functional domain
Creating, enhancing, enforcing & promoting data governance policies & practices
Owning functions within the analytical governance processes to facilitate delivering data solutions to production environments
Requirements & Qualifications
Bachelor's degree in Math, Economics, Engineering, or related quantitative field
3-5 years of experience with data governance, data management or data analytics
Strong communication skills, and the ability to explain technical solutions to non-technical audiences
Experience with relational databases (e.g. Microsoft SQL) and writing queries
Ability to learn data & technology as it relates to business functions
Analytical and critical thinking skills, i.e. a problem solver’s mindset
Experience administering Analytical tools, specifically Tableau & Alteryx, preferred
The base compensation range represents the low and high end of the range for this position. Actual compensation will vary and may be above or below the range based on various factors including but not limited to qualifications, skills, competencies, location, and experience. The range listed is just one component of our total compensation package for employees.
Other rewards may include an annual bonus, quarterly bonuses, commissions, and other long-term incentive compensation, depending on the position. National Life offers a competitive total rewards package which includes: a 401(k) retirement plan match; medical, dental, and vision insurance; a company funded wellness account for director and below employees; 10 paid holidays; a generous paid time off plan (22 days of combined time-off for non-exempt employees and exempt employees have discretion in managing their time, including scheduling time off in the normal course of business, but in no event will exempt employees receive less sick time than required by state or local law); 6 weeks of paid parental leave; and 6 weeks of paid family leave after a year of full-time employment
National Life Group® is a trade name of National Life Insurance Company, Montpelier, VT – founded in 1848, Life Insurance Company of the Southwest, Addison, TX – chartered in 1955, and their affiliates. Each company of National Life Group is solely responsible for its own financial condition and contractual obligations. Life Insurance Company of the Southwest is not an authorized insurer in New York and does not conduct insurance business in New York. Equity Services, Inc., Member FINRA/SIPC, is a Broker/Dealer and Registered Investment Adviser affiliate of National Life Insurance Company. All other entities are independent of the companies of National Life Group.
Fortune 1000 status is based on the consolidated financial results of all National Life Group companies.
National Life Group
1 National Life Dr
Montpelier, VT 05604
Social Media Policy
Site Disclosure and Privacy Policy
Show more
Show less","Data governance, Data management, Data analytics, Data solutions, Data quality, Data modeling, Data integration, Data mining, Data visualization, Data warehousing, Data architecture, Data security, Relational databases, SQL, Tableau, Alteryx, Analytical tools, Communication skills, Problem solving skills, Analytical thinking, Critical thinking, Mathematics, Economics, Engineering","data governance, data management, data analytics, data solutions, data quality, data modeling, data integration, data mining, data visualization, data warehousing, data architecture, data security, relational databases, sql, tableau, alteryx, analytical tools, communication skills, problem solving skills, analytical thinking, critical thinking, mathematics, economics, engineering","alteryx, analytical thinking, analytical tools, communication skills, critical thinking, data architecture, data governance, data integration, data management, data mining, data quality, data security, data solutions, dataanalytics, datamodeling, datawarehouse, economics, engineering, mathematics, problem solving skills, relational databases, sql, tableau, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Carrollton, TX",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759706916,2023-12-17,Denton,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, ML pipelines, Data mining, Data cleaning, Data normalizing, Data modeling, Data pre/post processing, Pandas, R, Airflow, KubeFlow, NLP, Python, Java, SQL, Bash, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning","data engineering, ml pipelines, data mining, data cleaning, data normalizing, data modeling, data prepost processing, pandas, r, airflow, kubeflow, nlp, python, java, sql, bash, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning","airflow, aws, azure, bash, data cleaning, data engineering, data mining, data normalizing, data prepost processing, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, ml pipelines, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Watauga, TX",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707765,2023-12-17,Denton,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","ML Data Engine, ML Data OPs, Data pre/post processing pipelines, Data mining, Data cleaning, Data normalization, Data modeling, Data platforms, Data frameworks, Big data, Data governance, Data risk, Data compliance, Data infrastructure, Pandas, R, Airflow, KubeFlow, Python, Java, bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied machine learning, Data classification, Data retention, Data management tools","ml data engine, ml data ops, data prepost processing pipelines, data mining, data cleaning, data normalization, data modeling, data platforms, data frameworks, big data, data governance, data risk, data compliance, data infrastructure, pandas, r, airflow, kubeflow, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, data retention, data management tools","airflow, applied machine learning, aws, azure, bash, big data, data classification, data cleaning, data compliance, data frameworks, data governance, data infrastructure, data management tools, data mining, data normalization, data platforms, data prepost processing pipelines, data retention, data risk, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, ml data engine, ml data ops, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Watauga, TX",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773088699,2023-12-17,Denton,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Airflow, AWS, Apache Spark, Azure, Bash, Data Mining, Data Modeling, Data Pipelines, Data Preprocessing, Data Processing, Docker, Git, Helm, Java, Kubernetes, KubeFlow, Kafka, LLM, ML Data OPs, Natural Language Processing, NoSQL, Pandas, Python, R, Relational Databases, Snowflake, SQL, Storm, SparkStreaming","airflow, aws, apache spark, azure, bash, data mining, data modeling, data pipelines, data preprocessing, data processing, docker, git, helm, java, kubernetes, kubeflow, kafka, llm, ml data ops, natural language processing, nosql, pandas, python, r, relational databases, snowflake, sql, storm, sparkstreaming","airflow, apache spark, aws, azure, bash, data mining, data preprocessing, data processing, datamodeling, datapipeline, docker, git, helm, java, kafka, kubeflow, kubernetes, llm, ml data ops, natural language processing, nosql, pandas, python, r, relational databases, snowflake, sparkstreaming, sql, storm"
Insurance - Data Analyst - REMOTE,Wahve LLC,"Charlotte, NC",https://www.linkedin.com/jobs/view/insurance-data-analyst-remote-at-wahve-llc-3785427368,2023-12-17,Gastonia,United States,Mid senior,Remote,"Put your Insurance Experience to work - FROM HOME!
At
Wahve
, we value significant insurance experience and want to revolutionize the way people think about
phasing into
retirement
by offering qualified candidates the opportunity to continue their career working from home. As we say -
retire from the office but not from work
. Our unique platform provides you with
real
work/life balance and allows you to customize your own work schedule while continuing to utilize your insurance expertise in
a remote, long-term position
.
What You’ll Love About Wahve
We created a welcoming place to work with friendly and professional leadership. We are known for the great care we take with our staff and our clients. We are passionate and determined about delivering the best customer service, preserving insurance industry knowledge, and making a difference by the work that we do.
What We Are Seeking
We have assignments available to help our
insurance industry
clients in
Data Analyst positions. Responsibilities include:
Build and maintain data warehouse, new reports, and ad hoc reports.
Work with user groups to identify reporting issues/enhancements and document business requirements.
Will serve as a member of a project team and/or work independently on projects.
Support and train internal users as needed.
Compile and prepare data for customer analysis.
Experience in C#, Visual Studio, JavaScript, CSS, and current web technologies such as .NET, ASP, JSON, and XML.
Experience with ANY of the following technologies: SQL Server Reporting Services (SSRS), SSIS Reporting, Power BI, Dynamics CRM, Dynamics GP, Share point, Excel, Power Query, Power Pivot.
Ability to compile data results and author commentary on industry studies is a plus.
Insurance or financial services industry experience required.
TO BECOME A WORK-AT-HOME VINTAGE EXPERT, WE REQUIRE
25 years of full-time work experience
Experience working in a data analysis role in the insurance or financial services industry - required
Benefits Of Becoming a Wahve Vintage Expert
Retire from the office but not from work.
Eliminate the office stress and the commute.
Choose the work you would like to do now.
Customize your schedule - full or part time.
Continue to earn an income.
Utilize your years of insurance industry knowledge.
Be part of our dynamic yet virtual team environment and connect with other experienced insurance professionals like yourself!
How To Get Started
Click
APPLY NOW
to complete our simple preliminary profile. Be sure to include your preferred contact information as one of our Qualification Specialists will connect with you promptly.
WE LOOK FORWARD TO MEETING YOU!
Show more
Show less","Data Analyst, Data Warehouse, Reporting, C#, Visual Studio, JavaScript, CSS, .NET, ASP, JSON, XML, SQL Server Reporting Services (SSRS), SSIS Reporting, Power BI, Dynamics CRM, Dynamics GP, Share point, Excel, Power Query, Power Pivot, SQL, Data Analysis, Insurance, Financial Services","data analyst, data warehouse, reporting, c, visual studio, javascript, css, net, asp, json, xml, sql server reporting services ssrs, ssis reporting, power bi, dynamics crm, dynamics gp, share point, excel, power query, power pivot, sql, data analysis, insurance, financial services","asp, c, css, dataanalytics, datawarehouse, dynamics crm, dynamics gp, excel, financial services, insurance, javascript, json, net, power pivot, power query, powerbi, reporting, share point, sql, sql server reporting services ssrs, ssis reporting, visual studio, xml"
Junior Data Engineers SQL,Belmont Lavan,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineers-sql-at-belmont-lavan-3675687783,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"Are you a capable SQL data engineer who is passionate about the power of data to solve environmental issues?
We are looking for an SQL data engineer to shape delivery by collaborating with data architects and modellers to contribute to the acquisition of corporate data requirements, documenting them according to the required standards utilising the prescribed methods and Azure data tools, to build our centralised data platform.
This is a permanent role, with responsibility for developing, constructing, testing, and maintaining architectures such as data pipelines and large-scale data processing warehouses. They will leverage industry best practice while delivering changes, such as agile backlogs, code repositories, automated builds, testing, and releases.
They will be responsible for ensuring data scientists can pull relevant data sets for their analyses, and implement data pipelines to connect operational systems, data for analytics and BI systems.
The post holder will provide clean, usable data to the business through the data platform in accordance with governance and execute and evaluate data requirements to support business activities and projects. The post holder will be central in ensuring the delivery of world-class digital products and changing the delivery culture in our company.
Requirements
Key responsibilities include:
Managing the investigation of corporate data requirements, documenting them according to the required standards utilising the prescribed methods and tools
Implementing data flows to connect operational systems, data for analytics and BI systems
Ensuring that those using the data structures and associated components have a good understanding and that any queries are dealt with promptly and efficiently
In liaison with the information management or IT management functions, contributing to the development and maintenance of corporate data standards
Required skills and experience:
Strong technical process understanding regardless of technology
Core SQL Competencies - SSMS, SSIS, T-SQL, Stored Procedures
High attention to detail
Strong communication skills
Structured problem-solving techniques
Efficient in building ETL and ELT processes for enterprise solutions
Strong software delivery methods and knowledge
Desired Skills And Experience
Digital delivery - has a track record of working on DevOps delivery
Exposure in Climate Change data legislation, practices and stakeholders
Experience in Environmental related industries i.e Water, Energy, Forestry related
Presentation skills
Azure skills (i.e. Azure Devops, Azure Data Factory, Azure Data Bricks, Azure SQL)
Performance-tuning skills
This is a permanent full-time role,
which will be delivered remotely to start with.
The post holder will be required to travel to our London office from time to time when it reopens.
Show more
Show less","SQL, SSMS, SSIS, TSQL, Stored Procedures, Data pipelines, ETL, ELT, DevOps, Azure Devops, Azure Data Factory, Azure Data Bricks, Azure SQL, Performance tuning, Agile, Code repositories, Automated builds, Testing, Releases, BI, Data governance, Data standards, Data architecture, Data modeling, Data processing, Data warehouses, Data analysis","sql, ssms, ssis, tsql, stored procedures, data pipelines, etl, elt, devops, azure devops, azure data factory, azure data bricks, azure sql, performance tuning, agile, code repositories, automated builds, testing, releases, bi, data governance, data standards, data architecture, data modeling, data processing, data warehouses, data analysis","agile, automated builds, azure data bricks, azure data factory, azure devops, azure sql, bi, code repositories, data architecture, data governance, data processing, data standards, data warehouses, dataanalytics, datamodeling, datapipeline, devops, elt, etl, performance tuning, releases, sql, ssis, ssms, stored procedures, testing, tsql"
Junior SQL Data Engineer Permanent Contract,Belmont Lavan,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-sql-data-engineer-permanent-contract-at-belmont-lavan-3590315132,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"Are you a capable SQL data engineer who is passionate about the power of data to solve environmental issues?
We are looking for an SQL data engineer to shape delivery by collaborating with data architects and modellers to contribute to the acquisition of corporate data requirements, documenting them according to the required standards utilising the prescribed methods and Azure data tools, to build our centralised data platform.
This is a permanent role, with responsibility for developing, constructing, testing, and maintaining architectures such as data pipelines and large-scale data processing warehouses. They will leverage industry best practice while delivering changes, such as agile backlogs, code repositories, automated builds, testing, and releases.
They will be responsible for ensuring data scientists can pull relevant data sets for their analyses, and implement data pipelines to connect operational systems, data for analytics and BI systems.
The post holder will provide clean, usable data to the business through the data platform in accordance with governance and execute and evaluate data requirements to support business activities and projects. The post holder will be central in ensuring the delivery of world-class digital products and changing the delivery culture in our company.
Requirements
Key responsibilities include:
Managing the investigation of corporate data requirements, documenting them according to the required standards utilising the prescribed methods and tools
Implementing data flows to connect operational systems, data for analytics and BI systems
Ensuring that those using the data structures and associated components have a good understanding and that any queries are dealt with promptly and efficiently
In liaison with the information management or IT management functions, contributing to the development and maintenance of corporate data standards
Required skills and experience:
Strong technical process understanding regardless of technology
Core SQL Competencies - SSMS, SSIS, T-SQL, Stored Procedures
High attention to detail
Strong communication skills
Structured problem-solving techniques
Efficient in building ETL and ELT processes for enterprise solutions
Strong software delivery methods and knowledge
Desired Skills And Experience
Digital delivery - has a track record of working on DevOps delivery
Exposure in Climate Change data legislation, practices and stakeholders
Experience in Environmental related industries i.e Water, Energy, Forestry related
Presentation skills
Azure skills (i.e. Azure Devops, Azure Data Factory, Azure Data Bricks, Azure SQL)
Performance-tuning skills
This is a permanent full-time role,
which will be delivered remotely to start with.
The post holder will be required to travel to our London office from time to time when it reopens.
Show more
Show less","SQL data engineering, Environmental data analysis, DevOps, Agile methodologies, Data pipeline development, Data warehousing, Data governance, Data architecture, Data modeling, Data quality assurance, Data integration, SSMS, SSIS, TSQL, Stored procedures, ETL/ELT processes, Software delivery methods, Azure Devops, Azure Data Factory, Azure Data Bricks, Performance tuning, Climate change data legislation, Environmental industries data, Presentation skills, High attention to detail, Strong communication skills, Structured problem solving","sql data engineering, environmental data analysis, devops, agile methodologies, data pipeline development, data warehousing, data governance, data architecture, data modeling, data quality assurance, data integration, ssms, ssis, tsql, stored procedures, etlelt processes, software delivery methods, azure devops, azure data factory, azure data bricks, performance tuning, climate change data legislation, environmental industries data, presentation skills, high attention to detail, strong communication skills, structured problem solving","agile methodologies, azure data bricks, azure data factory, azure devops, climate change data legislation, data architecture, data governance, data integration, data pipeline development, data quality assurance, datamodeling, datawarehouse, devops, environmental data analysis, environmental industries data, etlelt processes, high attention to detail, performance tuning, presentation skills, software delivery methods, sql data engineering, ssis, ssms, stored procedures, strong communication skills, structured problem solving, tsql"
Data Engineer,INEOS Automotive,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-ineos-automotive-3590321506,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"INEOS Automotive - Grenadier - Built On Purpose
It's a big task to launch a best-in-class 4x4 for those who depend on a vehicle as a working tool, and it's our responsibility to do the best job possible. We're building the Grenadier from the ground up, component by component. No corners cut and no easy options.
To make this vision a reality, we've assembled a team of world-class experts who are willing to roll up their sleeves and get stuck in. We need more doers that think big. More thinkers that dive in and do. More people that make things happen. We're a diverse workforce of tenacious, straight-talking experts with engineering at our core. We're growing our world-class team and looking for spirited innovators and disruptors - those who thrive on a gritty challenge and will work through adversity in the pursuit of success. We're doing things differently.
If this sounds like you, let's talk.
OVERVIEW:
We are looking for a Data Engineer to join the INEOS Automotive Information Technology department. The department sits at the heart of the business developing and supporting robust, easy to use, scalable tools and providing data, analysis, and insight to help shape the company's strategy
Ideally you come from a solid technical background in data engineering, and are now looking for the challenge of implementing, developing, managing, and supporting the data engineering application estate. Your ambition and passion for technical delivery will be a critical factor for success in this role
Our data engineering will be powered by a blend of Qlik Data Integration (QDI) suite namely, Attunity Replicate and Compose and Azure Data Factory so you will have a degree of expertise in these products but may also be familiar with the wider landscape of ETL products such as Informatica, Talend, Matillion, SAP Data Integrator, SQL Server Information Services (SSIS) etc
You will be familiar with environments concerning Operational and Management Analytics and will have expertise in developing optimal data pipelines using large volumes of data with varying variety and veracity and ideally have exposure to industry standard methodologies such as Boyce-Codd, Innmon, Kimball and Data Vault
Requirements
RESPONSIBILITIES
:
Responsible for designing, building, and deploying data pipelines using the functionality available in Qlik Replicate, Compose and Azure Data Factory
Responsible for designing and building database schemas and associated data pipelines (extract, transform and load) routines / processes to facilitate the end-user data and analytics requirements
Responsible for understanding database structures and articulating these using industry standard methodologies such as Boyce-Codd (Relational), Innmon (Data Marts), Kimball (Data Warehouse) and Dan Lindst (Data Vault)
Responsible for scoping, designing, building, servicing, and supporting all new and existing data pipelines within the organisation with the goal of providing robust analytical solutions that support the strategic aims
Responsible for gathering, inspecting, cleaning, transforming, and modelling data with the goal of discovering useful information, suggesting conclusions, and supporting decision-making
Responsible for capturing data requirements to design, extend or maintain, logical and physical data models which support new and existing business initiatives
Responsible for creation and maintenance of the data artifacts (data catalogue and data dictionary) and system level documentation
Responsible for understanding functional and non-functional requirements to create appropriate and effective technical solutions
Responsible for designing and building workflows to facilitate process automation
Responsible for providing end user support, service, and delivery management
Responsible for the data acquisition estate and conducting approved technology upgrades, providing technical advice, product information and 3rd line support
Responsible for coding SQL and stored procedures, including testing and troubleshooting queries, execution plans, optimising models, and tuning code
Responsible managing software licences, application security (roles and permissions) and application workspaces within the Qlik Replicate, Compose and Azure Data Factory Platforms
DESIRED EXPERIENCE & SKILLS:
Min 5 years of experience with the development of data warehouse solutions
Min 5 years of experience of working on complex business intelligence programmes across divisions
Min 5 years of experience of designing, developing, and implementing data pipelines, data lineage, taxonomies, catalogue, and metadata management processes
Min 5 years of experience of supporting and maintaining data pipelines, data lineage, taxonomies, catalogue, and metadata management processes
Min 5 years of experience with various forms of data storage, data warehouses and data lakes
Min 5 years of experience working with a wide variety of database platforms such as, Oracle, Postgres, SQL Server, Snowflake, Redshift etc
Min 5 years of experience in time and project management using Agile or Waterfall principals
Must be highly proficient with Microsoft suite including Word, PowerPoint, Excel, etc. especially Excel and PowerPoint. Will be expected to compile polished content including presentations
Show more
Show less","Data Engineering, Data Pipelines, ETL, Data Integration, Qlik Data Integration Suite, Attunity Replicate, Attunity Compose, Azure Data Factory, Informatica, Talend, Matillion, SAP Data Integrator, SQL Server Information Services (SSIS), BoyceCodd, Innmon, Kimball, Data Vault, Database Schemas, Data Warehouse, Data Marts, Data Vault, Data Lake, SQL, Stored Procedures, Software Licenses, Application Security, Agile, Waterfall, Microsoft Suite, Word, PowerPoint, Excel","data engineering, data pipelines, etl, data integration, qlik data integration suite, attunity replicate, attunity compose, azure data factory, informatica, talend, matillion, sap data integrator, sql server information services ssis, boycecodd, innmon, kimball, data vault, database schemas, data warehouse, data marts, data vault, data lake, sql, stored procedures, software licenses, application security, agile, waterfall, microsoft suite, word, powerpoint, excel","agile, application security, attunity compose, attunity replicate, azure data factory, boycecodd, data engineering, data integration, data lake, data marts, data vault, database schemas, datapipeline, datawarehouse, etl, excel, informatica, innmon, kimball, matillion, microsoft suite, powerpoint, qlik data integration suite, sap data integrator, software licenses, sql, sql server information services ssis, stored procedures, talend, waterfall, word"
Big Data Developer,Information Tech Consultants,"London Area, United Kingdom",https://uk.linkedin.com/jobs/view/big-data-developer-at-information-tech-consultants-3780070824,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"Have you dreamed about the Future, now you can be part of that future?
Are you looking for an exciting opportunity to work on the cutting-edge of technological innovation at the Most Important Companies in the UK? Are you inquisitive and excited about venturing into untested waters that will take society into the next era? If you are passionate about solving problems using new approaches, programming to create new possibilities, find solutions where everybody finds problems, a career in the field of Big Data may be for you.
Information Tech Consultants Ltd (ITC) is an IT consulting firm based in London. At Information Tech Consultants Ltd (ITC) t we value professionals with degrees in Computer Science or engineering, as well as 2-3 years of experience in programming or data analysis. If you have the right education, expertise, or skills, we will train you to become a successful professional in the field of Big Data.
Our comprehensive
Consultant Program
is a 6-7-week intensive workshop led by field experienced subject matter experts. Training prepares you to be an effective Big Data consultant.
Training includes:
4-6 hours a day of applied data science classroom training
Involvement with diverse Big Data tools such as Spark/Kafka/NiFi/HDFS/Hadoop/Docker/ELK/AWS
Great work environment
Assignments to enhance your classroom learning.
· Personalized coaching from our SMEs
· Real Life Scenarios
· Industry Coaching
Consultant training seminars focusing on professional and soft skills.
· Training materials
· Guidance about the trending technologies in the industry
· Knowledge in the industry trending technologies
You will also be coached on how to land consulting projects with our high-profile Fortune 500 partners such as:
· Microsoft
· Intel
· Amazon
· Apple
· Google
· Facebook
We currently have only 3 slots open for this exciting opportunity to become a Big Data consultant. Contact our Technical Recruiter to walk you through the 3-step hiring process:
1.
Initial pre-screen phone call with your Recruiter
2. Hacker Rank Technical Test
3. Spark hire Interview
To qualify for our amazing opportunity to advance your career, the following is required:
REQUIRED
• MUST have experience with coding/programing Java, C# or Python
• Experience with OOP Concepts
• Analytical skills and problem-solving ability
• Must have a bachelor’s degree
• Ability to translate objectives to a project plan with milestones, and resource/technology requirements, and teach, lead, and manage projects/people/clients to successful execution.
• Strong written and verbal communication skills
NICE TO HAVE
• Experience in SQL and Linux- but not required
• Server’s infrastructure/Architecture knowledge
• Business Intelligence, AWS/Azure, & ETL tool knowledge are a plus to have for this position but not required.
• You should be entitled to work in the UK and must be willing to travel within the UK as per project/client demands. Travel expenses will be covered.
Show more
Show less","Big Data, Java, C#, Python, OOP, Spark, Kafka, NiFi, HDFS, Hadoop, Docker, ELK, AWS, SQL, Linux, Business Intelligence, ETL, Server's infrastructure, Architecture","big data, java, c, python, oop, spark, kafka, nifi, hdfs, hadoop, docker, elk, aws, sql, linux, business intelligence, etl, servers infrastructure, architecture","architecture, aws, big data, business intelligence, c, docker, elk, etl, hadoop, hdfs, java, kafka, linux, nifi, oop, python, servers infrastructure, spark, sql"
"Software Engineer (Data), London",Isomorphic Labs,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/software-engineer-data-london-at-isomorphic-labs-3778533572,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"Software Engineer, Data Engineering, London or Switzerland
We are looking for data engineers with various levels of experience - Mid through to Senior, Principal, Staff or equivalent levels.
Isomorphic Labs is a new Alphabet company that is reimagining drug discovery through a computational- and AI-first approach.
We are on a mission to accelerate the speed, increase the efficacy and lower the cost of drug discovery. You'll be working at the cutting edge of the new era of 'digital biology' to deliver a transformative social impact for the benefit of millions of people.
Come and be part of a multi-disciplinary team driving groundbreaking innovation and play a meaningful role in contributing towards us achieving our ambitious goals, while being a part of an inspiring, collaborative and entrepreneurial culture.
Your impact
This is an exciting opportunity for you to work on a greenfield ML-based software platform that will transform the biopharmaceutical world as we know it.
Working in a highly creative, iterative environment, you will be partnering with leading engineers, scientists and ML researchers to build the critical platform driving that transformation. This is a newly created role and you will need to use your previous experience and show initiative in order to fully carve out your contribution.
What you will do
Develop and operate computational pipelines for ingestion of reference, external, and internal scientific data sets, including experimental and synthetic data.
Develop data stores for effective storage and computational use of the data by machine learning models and other data-driven products.
Develop and operate data generation services.
Develop and support data management toolkits in use by the wider organisation.
Develop and execute processes for effective and secure data handling and version control.
Partner and collaborate with a diverse set of teams incl. science, research, product, business development and operations
Skills and qualifications
Essential
Experience in data engineering at scale
Strong knowledge of data management tools and technologies including ETL frameworks, database engines, and file and object stores
Experience with modern ML systems, frameworks, and data lifecycle
Experience building secure/scalable platforms/products on cloud
Experience with life sciences data, including bioinformatics, computational chemistry, and biomedical research
Experience partnering with research and product teams
Strong software engineering experience with software design / architecture skills
Extensive enterprise programming experience writing production code using any mainstream programming languages, e.g. Java, C++, Python, Go
Demonstrate ongoing career progression / trajectory and a passion for learning
Either a Bachelor’s degree in Computer Science, a related technical field, or equivalent practical experience
Strong understanding of data structures and algorithms
Nice to have
Strong experience in Python
Basic knowledge of biology / chemistry
Experience working with biomedical data
Knowledge of the pharmaceutical industry, ideally with a focus on drug discovery
Experience building, deploying and maintaining production systems on GCP
Exposure to data curation
Culture and values
What does it take to be successful at IsoLabs? It's not about finding people who think and act in the same way, but we do have some shared values:
Thoughtful
Thoughtful at Iso is about curiosity, creativity and care. It is about good people doing good, rigorous and future-making science every single day.
Brave
Brave at Iso is about fearlessness, but it’s also about initiative and integrity. The scale of the challenge demands nothing less.
Determined
Determined at Iso is the way we pursue our goal. It’s a confidence in our hypothesis, as well as the urgency and agility needed to deliver on it. Because disease won’t wait, so neither should we.
In this together
Together at Iso is about connection, collaboration across fields and catalytic relationships. It’s knowing that transformation is a group project, and remembering that what we’re doing will have a real impact on real people everywhere.
Creating an inclusive company
We realise that to be successful we need our teams to reflect and represent the populations we are striving to serve. We’re working to build a supportive and inclusive environment where collaboration is encouraged and learning is shared. We value diversity of experience, knowledge, backgrounds and perspectives and harness these qualities to create extraordinary impact.
We are committed to equal employment opportunities regardless of sex, race, religion or belief, ethnic or national origin, disability, age, citizenship, marital, domestic or civil partnership status, sexual orientation, gender identity, pregnancy or related condition (including breastfeeding) or any other basis protected by applicable law. If you have a disability or additional need that requires accommodation, please do not hesitate to let us know.
Hybrid working
It’s hugely important for us to be able to share knowledge and establish relationships with each other, and we find it easier to do this if we spend time together in person. This is why we’ve decided to follow a hybrid model, and would
require you to be able to come into the office 3 days a week
(currently Tue, Wed, and one other day depending on which team you’re in). As an equal opportunities employer we are committed to building an equal and inclusive team. If you have additional needs that would prevent you from following this hybrid approach, we’d be happy to talk through these if you’re selected for an initial screening call.
Please note that when you submit an application, your data will be processed in line with our privacy policy .
>> Click to view other open roles at Isomorphic Labs
Show more
Show less","Data Engineering, ML Systems, Frameworks, ETL Frameworks, Database Engines, File and Object Stores, Data Lifecycle, Cloud Platforms, Bioinformatics, Computational Chemistry, Biomedical Research, Software Design, Software Architecture, Java, C++, Python, Go, Data Structures, Algorithms, Biology, Chemistry, Biomedical Data, Pharmaceutical Industry, Drug Discovery, GCP, Data Curation","data engineering, ml systems, frameworks, etl frameworks, database engines, file and object stores, data lifecycle, cloud platforms, bioinformatics, computational chemistry, biomedical research, software design, software architecture, java, c, python, go, data structures, algorithms, biology, chemistry, biomedical data, pharmaceutical industry, drug discovery, gcp, data curation","algorithms, bioinformatics, biology, biomedical data, biomedical research, c, chemistry, cloud platforms, computational chemistry, data curation, data engineering, data lifecycle, data structures, database engines, drug discovery, etl frameworks, file and object stores, frameworks, gcp, go, java, ml systems, pharmaceutical industry, python, software architecture, software design"
Distributed Systems Engineer - Analytical Database Platform,Cloudflare,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/distributed-systems-engineer-analytical-database-platform-at-cloudflare-3516880374,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
About Role
We are looking for an experienced and highly motivated engineer to join our team and contribute to our analytical database platform. The platform is a critical component of Cloudflare Analytics which provides real-time visibility into the health and performance of Cloudflare customers' online properties.
The team builds and maintains a high-performance, scalable database platform powered by ClickHouse, optimized for analytical workloads. We help our customers, both internal and external, to gain a deeper understanding of their online properties, identify trends and patterns, and make informed decisions about how to optimize their web performance, security, and other key metrics. Our mission is to empower customers to leverage their data to drive better outcomes for their business.
As a Distributed systems engineer - Analytical Database Platform, you will:
Develop and implement new platform components for the Cloudflare Analytical Database Platform to improve functionality and performance.
Add more database clusters to accommodate the growing volume of data generated by Cloudflare products and services.
Monitor and maintain the performance and reliability of existing database platform clusters, and identify and troubleshoot any issues that may arise.
Work to identify and remove bottlenecks within the analytics database platform, including optimizing query performance and streamlining data ingestion processes.
Collaborate with the ClickHouse open-source community to add new features and functionality to the database, as well as contribute to the development of the upstream codebase.
Collaborate with other teams across Cloudflare to understand their data needs and build solutions that empower them to make data-driven decisions.
Participate in the development of the next generation of the database platform engine, including researching and evaluating new technologies and approaches that can improve the database's performance and scalability.
Key qualifications:
3+ years of experience working in software development covering distributed systems, and databases.
Strong programming skills (C++ is preferable), as well as a deep understanding of software development best practices and principles.
Strong knowledge of SQL and database internals, including experience with database design, optimization, and performance tuning.
A solid foundation in computer science, including algorithms, data structures, distributed systems, and concurrency.
Ability to work collaboratively in a team environment, as well as communicate effectively with other teams across Cloudflare.
Strong analytical and problem-solving skills, as well as the ability to work independently and proactively identify and solve issues.
Experience with ClickHouse is a plus.
Experience with SALT or Terraform is a plus.
Experience with Linux container technologies, such as Docker and Kubernetes, is a plus.
If you're passionate about building scalable and performant databases using cutting-edge technologies, and want to work with a world-class team of engineers, then we want to hear from you! Join us in our mission to help build a better internet for everyone!
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Distributed Systems, Databases, C++, SQL, Database Design, Database Optimization, Algorithms, Data Structures, Concurrency, Linux, Docker, Kubernetes, ClickHouse, SALT, Terraform","distributed systems, databases, c, sql, database design, database optimization, algorithms, data structures, concurrency, linux, docker, kubernetes, clickhouse, salt, terraform","algorithms, c, clickhouse, concurrency, data structures, database design, database optimization, databases, distributed systems, docker, kubernetes, linux, salt, sql, terraform"
"Lead Big Data Developer (Spark, Python, Databricks)",EPAM Systems,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-big-data-developer-spark-python-databricks-at-epam-systems-3781867336,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"Description
Are you passionate about big data development? Do you enjoy working with both technical and business stakeholders to translate vision and designs into sustainable, customer-focused solutions?
Can you communicate efficiently, and influence quicker deliveries? If yes, we have new position just opened up for a Lead Data Developer with expertise in Spark and Databricks to join our Data Practice team and work with EPAM strategic clients. As Lead Data Engineer you will be building data systems and applications to support decision making. You will focus on developing and maintaining data pipelines in Microsoft Azure. You will be participating in data migration from on-premise to Azure and building components for central enterprise data platforms.
This position will require advanced technical depth and experience, technical leadership, and multi-faceted communication skills.
What You’ll Do
Contributing to the success of our customer projects
Lead and coach team members, including communicating team goals and identifying areas for growth
Provide technology solutions that will solve business problems and strengthen our position as digital leaders
Design, plan and deliver sustainable solutions using modern programming languages
Help us in our journey as we move to Azure and build a new Azure-native application
Help lead our growing team of analysts and engineers to build a new application
Provide technical expertise and recommendations in assessing requirements and initiatives to support and enhance our existing applications
Conduct code reviews and test software as needed, along with participating in application architecture and design and other phases of SDLC
See that proper operational controls and procedures are implemented
Requirements
5+ years of experience in data development using Spark, preferably in an enterprise environment on Microsoft Azure
8+ years of experience in building enterprise big data / data engineering applications using continuous integration tools like Azure DevOps
Extensive knowledge and experience with Databricks
Strong understanding of reusable software design patterns
Experience in Azure Data Factory, Azure Data Lake and Azure Log Analytics, Azure KeyVault and Git
Design and implementation of production-grade solutions
Delivering high-quality code, focusing on simplicity, performance, maintainability and scalability
Practical experience in applying an agile way of working and in applying DevOps methodologies
Excellent analytical and conceptual skills to understand complex technology stacks and their dependencies
Nice to have
Expert knowledge of Python and Spark
Enterprise system integration experience
Expert knowledge of Databricks
Docker
We Offer
We offer a range of benefits including
A competitive group pension plan, life assurance and income protection
Private medical insurance, private dental care and critical illness cover
Cycle scheme Tech scheme and season ticket loan
Employee assistance program
Unlimited access to LinkedIn learning solutions
EPAM Employee Stock Purchase Plan (ESPP) (subject to certain eligibility requirements)
Various perks such as Gym discount, Friday lunch, on-site massage and regular social events
Some of these benefits may be available only after you have passed your probationary period
About EPAM
EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential
Apply
Show more
Show less","Spark, Databricks, Microsoft Azure, Python, Azure Data Factory, Azure Data Lake, Azure Log Analytics, Azure KeyVault, Git, Docker, DevOps, Agile, Continuous integration tools, Productiongrade solutions, Reusable software design patterns, Enterprise system integration, Software architecture, Software design, Software development, Software testing, SDLC, Analytical skills, Conceptual skills","spark, databricks, microsoft azure, python, azure data factory, azure data lake, azure log analytics, azure keyvault, git, docker, devops, agile, continuous integration tools, productiongrade solutions, reusable software design patterns, enterprise system integration, software architecture, software design, software development, software testing, sdlc, analytical skills, conceptual skills","agile, analytical skills, azure data factory, azure data lake, azure keyvault, azure log analytics, conceptual skills, continuous integration tools, databricks, devops, docker, enterprise system integration, git, microsoft azure, productiongrade solutions, python, reusable software design patterns, sdlc, software architecture, software design, software development, software testing, spark"
Consulting Engineer - Data Security (DLP & CASB),Palo Alto Networks,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/consulting-engineer-data-security-dlp-casb-at-palo-alto-networks-3757256908,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"Company Description
Our Mission
At Palo Alto Networks® everything starts and ends with our mission:
Being the cybersecurity partner of choice, protecting our digital way of life.
Our vision is a world where each day is safer and more secure than the one before. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.
Our Approach to Work
We lead with flexibility and choice in all of our people programs. We have disrupted the traditional view that all employees have the same needs and wants. We offer personalization and offer our employees the opportunity to choose what works best for them as often as possible - from your wellbeing support to your growth and development, and beyond!
At Palo Alto Networks, we believe in the power of collaboration and value in-person interactions. This is why our employees generally work from the office three days per week, leaving two days for choice and flexibility to work where you feel most effective. This setup fosters casual conversations, problem-solving, and trusted relationships. While details may evolve, our goal is to create an environment where innovation thrives, with office-based teams coming together three days a week to collaborate and thrive, together!
Job Description
Your Career
We are currently seeking a seasoned Consulting Engineer (CE) to join our EMEA & LATAM team. We will consider applicants in the UK, Germany or France.
This CE will help drive the adoption of our portfolio for Data Security within our customers. In this position, you will play a key role in evangelizing, architecting, and simplifying the effort to sell these solutions to help customers become more secure.
The EMEA Consulting Engineer will work with our customer-facing sales teams (Systems Engineers and Account Executives) and internal stakeholders (Product Management, Marketing & others). The goal is to ensure that the solution aligns with our market goals, product capabilities, and roadmaps. The CE will also produce material and tools to educate the Systems Engineering community and work alongside them to gain technical preference at the customer.
Your Impact
Present to customers as our expert in your technical domain at all levels in the customer hierarchy from technician to CIO
Lead conversations about industry trends and emerging changes to the security landscape that every customer needs to be aware of
Discuss, with credibility, competitive offers in the marketplace and position ours as the best alternative
Lead and support customer demonstrations, with our System Engineers, that showcase our unique value
Interact locally and remotely with customers in an equally persuasive manner
Qualifications
Your Experience
5+ years of positioning and successfully selling Data Loss Protection & Cloud Security Access Broker solutions for customers
Strong knowledge of data privacy laws
Pre-sales experience
Proven ability to work within small creative teams as well as cross-functional teams
Strong communication skills (written and verbal)
Strong presentation skills
Self motivated with solid organizational skills
""Whatever it takes"" attitude
Willingness to travel as required (30% of time)
BS technical degree or equivalent or equivalent military experience required
Additional Information
The Team
Palo Alto Networks has brought technology to market that is reshaping the cybersecurity threat and protection landscape. Our ability to protect digital transactions is limited only by our ability to establish relationships with our potential customers and help them understand how our products can protect their environments. This is where our sales teams come in. Our sales team members work together with large organizations to keep their digital information safe. Our passionate sales teams educate, inspire, and empower our potential clients.
As part of our sales team, you are empowered with unmatched systems and tools, constantly updated research and sales libraries, and a team built on joint success. You won’t find someone at Palo Alto Networks that isn’t committed to your success – with everyone pitching in to assist when it comes to solutions selling, learning, and development. As a member of our sales team, you are motivated by a solutions-focused sales environment and find fulfillment in working with clients to resolve incredibly complex cyberthreats. You’re an amazing sales person – you’re just looking for something more substantial and challenging as your next step.
Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.
We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.
All your information will be kept confidential according to EEO guidelines.
Please note that we will not sponsor applicants for work visas for this position.
Show more
Show less","Data Loss Prevention, Cloud Security Access Broker, Data Privacy Laws, PreSales Experience, Communication Skills, Verbal Skills, Presentation Skills, Organizational Skills, Sales Libraries, SolutionsFocused Sales, Cyberthreats","data loss prevention, cloud security access broker, data privacy laws, presales experience, communication skills, verbal skills, presentation skills, organizational skills, sales libraries, solutionsfocused sales, cyberthreats","cloud security access broker, communication skills, cyberthreats, data loss prevention, data privacy laws, organizational skills, presales experience, presentation skills, sales libraries, solutionsfocused sales, verbal skills"
Lead Data Software Engineer (Azure),EPAM Systems,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-software-engineer-azure-at-epam-systems-3781865703,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"Description
As a lead team member of the Data platform engineering team, the successful candidate will be responsible for the design, implementation and functionality of data products. This role combines technical and people management skills for data platform, as well as knowledge of financial data, big data processing and management, and cloud-based data warehouse solutions (Azure).
In this role you will:
Work on the team responsible for Data Engineering including design, development, and creating a data product to help commercial data and analytics products.
Participate in strategy and design discussion for modernizing the data product and distribution channels.
Drive the entire software development lifecycle process in support of continuous data delivery, while leading the evaluation and adoption of emerging technologies.
This position will require advanced technical depth and experience, technical leadership, and multi-faceted communication skills.
Responsibilities
Provide partnership and support to SME’s and Tech Leads to ensure delivery on commitments
Build and maintain secure and compliant production data processing pipelines by using different tools and techniques (cloud services, Python/Scala)
Ensure that data pipelines and data stores are high-performing, efficient, organized, and reliable, given a set of business requirements and constraints. They deal with unanticipated issues swiftly, and they minimize data loss
Design, implement, monitor, and optimize data platforms to meet the data pipelines needs
Design and implement data engineering, ingestion, and curation functions on Azure cloud using Azure native or custom programming. Create data transformation and loading workflows
Responsible for data-related implementation tasks that include provisioning data storage services, ingesting streaming, and batch data, transforming data, implementing security requirements, implementing data retention policies, identifying performance bottlenecks, and accessing external data sources
Design and operationalize large scale enterprise data solutions and applications using one or more Azure data and analytics services
Implement data solutions that use the following Azure services: Azure Cosmos DB, Azure Synapse Analytics, Azure Data Factory, Azure Stream Analytics, Azure Databricks, and Azure Blob storage, Microsoft Purview etc
Responsible for integrating, transforming, and consolidating data from various structured and unstructured data systems into a structure that is suitable for building analytics solutions
Requirements
Depending on seniority, relevant experience in Data Platforms (in Financial Services industry) and Azure’s PaaS offerings (Synapse, Purview, ADF, Databricks, Azure Data Lake Storage etc.)
Strong experience with Azure: Synapse Analytics, Data Factory, Data Lake, Databricks, Microsoft Purview, Monitor, SQL Database, SQL Managed Instance, Stream Analytics, Cosmos DB, Storage Services, ADLS, Azure Functions, Log Analytics, Serverless Architecture, ARM Templates
Proficient in data processing languages such as SQL, JAVA, C# or Scala
Strong notions of security best practices (e.g., using Azure Key Vault, IAM, RBAC, Monitor etc.)
Proficient in integrating, transforming, and consolidating data from various structured and unstructured data systems into a structure that is suitable for building analytics solutions
Understand the data through exploration, experience with processes related to data retention, validation, visualization, preparation, matching, fragmentation, segmentation, and enhancement
We Offer
We offer a range of benefits including
A competitive group pension plan, life assurance and income protection
Private medical insurance, private dental care and critical illness cover
Cycle scheme Tech scheme and season ticket loan
Employee assistance program
Unlimited access to LinkedIn learning solutions
EPAM Employee Stock Purchase Plan (ESPP) (subject to certain eligibility requirements)
Various perks such as Gym discount, Friday lunch, on-site massage and regular social events
Some of these benefits may be available only after you have passed your probationary period
About EPAM
EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential
Apply
Show more
Show less","Data Engineering, Data Platform, Design, Development, Cloud Services, Data Product, Python, Scala, Azure, Azure Synapse Analytics, Azure Data Factory, Azure Stream Analytics, Azure Databricks, Azure Blob Storage, Microsoft Purview, Azure Cosmos DB, SQL, JAVA, C#, Data Exploration, Data Retention, Data Validation, Data Visualization, Data Preparation, Data Matching, Data Fragmentation, Data Segmentation, Data Enhancement, IAM, RBAC","data engineering, data platform, design, development, cloud services, data product, python, scala, azure, azure synapse analytics, azure data factory, azure stream analytics, azure databricks, azure blob storage, microsoft purview, azure cosmos db, sql, java, c, data exploration, data retention, data validation, data visualization, data preparation, data matching, data fragmentation, data segmentation, data enhancement, iam, rbac","azure, azure blob storage, azure cosmos db, azure data factory, azure databricks, azure stream analytics, azure synapse analytics, c, cloud services, data engineering, data enhancement, data exploration, data fragmentation, data matching, data platform, data preparation, data product, data retention, data segmentation, data validation, design, development, iam, java, microsoft purview, python, rbac, scala, sql, visualization"
Senior Digital Data Analyst,Frieze,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-digital-data-analyst-at-frieze-3766933897,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"Who We Are:
Frieze is the world’s leading platform for modern and contemporary art, dedicated to artists, galleries, collectors and art lovers alike. Frieze comprises three magazines – frieze, Frieze Masters Magazine and Frieze Week – and five international art fairs – Frieze London, Frieze Masters, Frieze New York, Frieze Los Angeles and Frieze Seoul. In October 2021, Frieze also launched No.9 Cork Street, a permanent space for visiting international galleries in the heart of Mayfair, London. Frieze is part of the IMG network.
About the Role
Frieze is in the process of transforming how data systems and digital platforms drive the business. The position of Digital Data Analyst sits at the heart of this transition within the Data and Insights Team harnesses the huge potential in data to drive value from our user base, to optimise our fair operations and to develop our suite of Digital Products. Frieze has an unrivalled opportunity within our market to build up a world-leading data-driven CRM program for high-net-worth art professionals, art enthusiasts and collectors. Further, we are building a membership programme based on a subscription model, critical to which is a firm understanding of how our content and experience motivates and converts audience segments within our market.
Key Responsibilities Include:
Product owner of the data warehouse, planning the goals of the warehouse against the department’s roadmap and architecting the technical developments required to deliver their product strategy.
Creating data pipelines between all data sources within data eco-system and the data warehouse, specifically ETL processes using python (primarily using pandas library) and mySQL stored procedures.
Error handling and database maintenance, including designing data architecture for new and existing sources.
Ensuring that best practice and compliance is adhered to in the collection, storage, and use of data within our data warehouse.
Developing and maintaining a robust suite of reporting dashboards within PowerBI for internal stakeholders.
Using data visualisation to bridge any data knowledge gaps across the organisation.
Delivering bespoke analysis on various data sets from across the business.
Presenting findings to senior level management to ensure that decisions regarding audiences and products are data-driven and insight led.
Technical ownership of Google Analytics and tag management.
Automating manual data related tasks through the data warehouse to ensure efficiency and efficacy in the department.
You Will Have The Following Strengths:
Demonstrable proficiency with SQL and Python.
Experience of Google Analytics and Google Tag Manager, preferably within a digital media context and with experience in context of mixed ad-funded and subscription / recurring revenue models
Building statistical and forecasting models to meet reporting, business, and product needs.
Experience of developing reporting within Power BI, or another similar data visualisation platform.
Inform, set, and track results for products against KPIs.
Strong working experience in consuming data from external APIs
Experience working with a variety of data formats (e.g. xml, csv, json, xlsx)
Advanced user of MS Excel
Strong understanding of relational database modelling and development.
Awareness of non-relational database architecture and NoSQL.
Experience of Data Warehousing technologies and architecture, specifically mySQL.
Working knowledge of HTML/CSS/Javascript is an advantage.
Endeavor unites and brings people together in our love of sport, culture, and entertainment. We understand this can only be accomplished when we lead with a lens of diversity, equity, and inclusion in everything we do. As a global company that drives culture, we strive to reflect the world’s diverse voices.
Endeavor is an equal opportunities employer and encourages applications from suitably qualified and eligible candidates regardless of sex, race, disability, age, sexual orientation, or religion or belief.
Show more
Show less","Data Analysis, Data Visualization, SQL, Python, pandas, MySQL, PowerBI, Google Analytics, Google Tag Manager, Statistical Modeling, Forecasting, Data Warehousing, Relational Database Modeling, NoSQL, HTML, CSS, Javascript","data analysis, data visualization, sql, python, pandas, mysql, powerbi, google analytics, google tag manager, statistical modeling, forecasting, data warehousing, relational database modeling, nosql, html, css, javascript","css, dataanalytics, datawarehouse, forecasting, google analytics, google tag manager, html, javascript, mysql, nosql, pandas, powerbi, python, relational database modeling, sql, statistical modeling, visualization"
Data Centre Technician,JLL,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-centre-technician-at-jll-3761770027,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"Job Description - Duties And Responsibilities
This role is in support of Data Centre Operations for Proximity Services based between Telehouse Docklands Data Centre. Out of hours we look after Paternoster Square Head Office and 5 Canada Square where the aim is to deliver service to the best possible standard for all infrastructure and processes managed within the data centre. The Data Centre engineer will be responsible for providing technical assistance and overall support to our customers providing 24/7 365 cover through a 12 hour shift pattern days and nights within the data Centre (07:00 to 19:00 and 19:00 to 07:00).
The Data Centre Engineer will also be responsible for the installation of racks, housing, network and computer equipment. Each Data Centre Engineer is also responsible for managing and implementing installations as required, including shelves, power strips, rails, cable management and customer IT equipment. When required identify issues with the hardware or cabling, which can include replacing internal components and testing, tracing and labelling of cabling.
Liaising with counterparts within the company globally and 3rd parties, on hardware related issues and processes.
The Data Centre Engineer will also share responsibility for upholding the standards of tidiness, cleanliness, security, asset information and installation standards within the computer suites.
Job Specifications - Essential Skills, Knowledge And Experience
Computer engineering experience and knowledge in order to achieve optimum methods of working.
Cabling experience and knowledge, to achieve optimum methods of working.
Installation of rack mount kit including HP and CISCO, Dell, Arista, Synergy 1200,
Hardware maintenance – swapping of failed customer replaceable parts such as cache battery, HDD, Power Supply.
Highly motivated individual, with a positive & pro-active attitude to work.
Willingness to make changes to improve operational efficiency through innovation, process, and procedures, adopting and adapting ideas and practices from elsewhere.
Ability to act rapidly and logically under pressure and making effective use of others in resolving problems.
Capable of working with the minimum of supervision.
Good written and verbal communication skills (English).
Good working knowledge of Outlook, Word, and Excel
Excellent team skills, with an ability to listen and contribute to discussions and meetings.
Customer and service focused, with determination to meet their needs.
Flexible
Job Specifications - Desirable Skills, Knowledge And Experience
ITIL Accreditation
Service Now
Must hold a valid UK driving licence.
Show more
Show less","Computer engineering, Cabling, Installation, Rack mount kit, HP, CISCO, Dell, Arista, Synergy 1200, Hardware maintenance, Outlook, Word, Excel, ITIL Accreditation, Service Now","computer engineering, cabling, installation, rack mount kit, hp, cisco, dell, arista, synergy 1200, hardware maintenance, outlook, word, excel, itil accreditation, service now","arista, cabling, cisco, computer engineering, dell, excel, hardware maintenance, hp, installation, itil accreditation, outlook, rack mount kit, service now, synergy 1200, word"
Data Centre Technician,JLL,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-centre-technician-at-jll-3761770025,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"Job Description - Duties And Responsibilities
This is a float role to support Data Centre Operations for Proximity Services based atTelehouse Docklands Data Centre but working between Docklands and Paternoster Square in London where required. Also out of hours we look after Paternoster Square Head Office and 5 Canada Square where the aim is to deliver service to the best possible standard for all infrastructure and processes managed within the data centre. The Data Centre engineer will be responsible for providing technical assistance and overall support to our customers. Telehouse Docklands shift is 24/7 365 cover through a 12 hour shift pattern days and nights within the data Centre (07:00 to 19:00 and 19:00 to 07:00). Paternoster Square is buisness hours 07:00 to 15:00 and 11:00 to 19:00.
The Data Centre Engineer will also be responsible for the installation of racks, housing, network and computer equipment. Each Data Centre Engineer is also responsible for managing and implementing installations as required, including shelves, power strips, rails, cable management and customer IT equipment. When required identify issues with the hardware or cabling, which can include replacing internal components and testing, tracing and labelling of cabling.
Liaising with counterparts within the company globally and 3rd parties, on hardware related issues and processes.
The Data Centre Engineer will also share responsibility for upholding the standards of tidiness, cleanliness, security, asset information and installation standards within the computer suites.
Job Specifications - Essential Skills, Knowledge And Experience
Computer engineering experience and knowledge in order to achieve optimum methods of working.
Cabling experience and knowledge, to achieve optimum methods of working.
Installation of rack mount kit including HP and CISCO, Dell, Arista, Synergy 1200,
Hardware maintenance – swapping of failed customer replaceable parts such as cache battery, HDD, Power Supply.
Highly motivated individual, with a positive & pro-active attitude to work.
Willingness to make changes to improve operational efficiency through innovation, process, and procedures, adopting and adapting ideas and practices from elsewhere.
Ability to act rapidly and logically under pressure and making effective use of others in resolving problems.
Capable of working with the minimum of supervision.
Good written and verbal communication skills (English).
Good working knowledge of Outlook, Word, and Excel
Excellent team skills, with an ability to listen and contribute to discussions and meetings.
Customer and service focused, with determination to meet their needs.
Flexible
Job Specifications - Desirable Skills, Knowledge And Experience
ITIL Accreditation
Service Now
Must hold a valid UK driving licence.
Show more
Show less","Computer engineering, Cabling, Installation of rack mount kit, Hardware maintenance, Problem resolution, Communication skills, Outlook, Word, Excel, Teamwork, Customer focus, Flexibility, ITIL Accreditation, Service Now","computer engineering, cabling, installation of rack mount kit, hardware maintenance, problem resolution, communication skills, outlook, word, excel, teamwork, customer focus, flexibility, itil accreditation, service now","cabling, communication skills, computer engineering, customer focus, excel, flexibility, hardware maintenance, installation of rack mount kit, itil accreditation, outlook, problem resolution, service now, teamwork, word"
PK/PD Data Scientist,Roche,"Welwyn, England, United Kingdom",https://uk.linkedin.com/jobs/view/pk-pd-data-scientist-at-roche-3737991102,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"The Position
In Roche’s Pharmaceutical Research and Early Development organization (pRED), we make transformative medicines for patients in order to tackle some of the world’s toughest unmet healthcare needs. At pRED, we are united by our mission to transform science into medicines. Together, we create a culture defined by curiosity, responsibility and humility, where our talented people are empowered and inspired to bring forward extraordinary life-changing innovation at speed.
This position is located in the Predictive Modeling and Data Analytics chapter, a chapter within the Pharmaceutical Sciences function, where generating insights from data is a primary focus area. We closely collaborate with our therapeutic areas and functions to convert hypotheses into innovative therapeutics.
Nature of the contract
Permanent or temporary (2 years) contract - to be discussed
Job mission
As a member of our team of data 2 insights experts you support the organization to generate deep insights from our data, driving our journey into becoming a data savvy, efficient and data driven organization. You will contribute to our prime responsibility of developing safe and efficacious drugs creating exceptional value for our portfolio and for patients.
Your impact
Lead the creation of pooled and ready to use pharmacokinetic/pharmacodynamic (PK/PD) modeling datasets (e.g. NONMEM® dataset) and descriptive datasets in compliance with Roche internal guidelines, standard operating procedures (SOPs) and Health Authority guidelines through high quality programs
Lead the creation of PK non-compartmental analysis (NCA) datasets in compliance with Roche internal guidelines, SOPs and Health Authority guidelines through high quality programs
Conduct exploratory graphical analyses and model-based graphical analyses using existing PK/PD models (e.g. bayesian feedback analysis) in consultation with the clinical pharmacometrician
Contribute to process-improvement initiatives
Seek for improvement in our data (e.g. via automation, data processing, data management) and decision makings (data visualization, data interpretation) processes
Your profile
Master of Science (M.S.) or Bachelor of Science (BSc) or equivalent experience in Mathematics and Statistics
Efficient in SAS® and/or R
Ability to process information, analyze data, and reach conclusions based on sound reasoning
Good understanding of drug development process including global clinical trial practices, procedures, methodologies
Highly effective verbal and written communication/presentation skills in English
You are obsessed with meeting customer needs. You put yourself in your customers place and act boldly with them to bring better results to patients.
You exhibit a growth mindset. You ask for feedback and act on it. You embrace opportunities to gain new skills and perspectives and provide honest feedback to others to help them grow.
Your location
This position is located in Basel or Welwyn
Our commitment
Roche commits to recognising talent and aptitude. We prioritise encouraging and supporting our employees on their personal journeys by providing a safe, creative space to help them reflect, make decisions and grow in their career.
We are confident that we find the most innovative solutions by gaining different perspectives, asking and answering hard questions, and challenging the status quo. Roche embraces diversity and equal opportunity in a serious yet enthusiastic way; we are devoted to building a team that represents a range of backgrounds, perspectives, and skills. The more inclusive we are, the better our work will be.
Who we are
At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.
Basel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.
Besides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.
We believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.
Roche is an Equal Opportunity Employer.
Show more
Show less","SAS, R, Statistics, Mathematics, Data visualization, Data interpretation, NONMEM, Pharmacokinetics, Pharmacodynamics, PK/PD modeling, PK noncompartmental analysis, Bayesian feedback analysis","sas, r, statistics, mathematics, data visualization, data interpretation, nonmem, pharmacokinetics, pharmacodynamics, pkpd modeling, pk noncompartmental analysis, bayesian feedback analysis","bayesian feedback analysis, data interpretation, mathematics, nonmem, pharmacodynamics, pharmacokinetics, pk noncompartmental analysis, pkpd modeling, r, sas, statistics, visualization"
"Shift Engineer, Data Centres",JLL,"Slough, England, United Kingdom",https://uk.linkedin.com/jobs/view/shift-engineer-data-centres-at-jll-3787346987,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"Electrical Shift Engineer
Duties of the Electrical Shift Engineer
As the post holder, you will be expected to have extensive understanding of Critical Facilities/Data Centre infrastructure (Mechanical, Electrical, Plumbing & associated assets). As an integrated member of the engineering shift team, you will responsible for effective inspection, operation and maintenance of all associated MEP assets within the critical facility, with a primary focus on critical facilities risk management, enhancing the quality of the service delivery through effective technical engineering capabilities.
As an electrical biased engineer you expected to take the lead in the Planned, Corrective and Reactive Maintenance tasks as well as other associated assets when required. You will have the ability to issue PTW, ensuring compliance with method statements, risk assessments
Duties to include:
Operational understanding of all MEP systems within the Critical Facility/Data Centre in a competent, and confident manner (Biased towards Electrical within the facility). Making recommendations for improvement and providing concise technical reports when required.
Ensure that Specialist Vendors service visits are completed efficiently, and quality of work has been inspected and recommendations are communicated to the duty Shift Leader/Assist Chief Engineer.
To provide backup as engineering shift relief cover in the event of annual leave, training or other absence within the engineering team. Ensuring minimum staffing arrangements are always maintained and covering your opposite shift team members when required.
Accurately reporting of all completed work orders, including precise recording of any critical spares/stock items used during these tasks.
Attend Weekly Team meetings and Quarterly Team Meetings when scheduled.
Completing the shift handover reports, ensuring that significant events or activities are noted and effectively communicated to oncoming shifts and management.
Actively participate and complete Health and Safety compliance activities, such as Hazard/Near Miss Reporting and regular attendance/completion of scheduled Toolbox Talks.
Liaise with the supply chain, when ordering M&E components for remedial/corrective works.
Take ownership of areas (including all plant rooms) under control, ensuring they are brought up too and maintained to the required show site standards.
Evaluate and escalate any potential risks, with an assessment of impact, probable causes and mitigation opportunities.
Actively contribute to continuous improvement of safe systems of work, ensuring compliance with relevant Health and Safety policy and procedures.
Complete Reactive, Planned & Corrective Work Orders in line with SFG20 and ensure reliability of assets through excellent standards of maintenance delivery.
Escalation of all ONWH Alarm Notifications in line with agreed escalation procedures for EMEA MEP operations.
Accurate completion of all Corrigo Work Orders that have been allocated for completion by the CMMS. Including accurate capture of time spent on tasks and clear/concise details of works undertaken. With no errors input onto the system
Efficient and Effective completion of Corrigo Work Orders that have been allocated, ensuring that Work Orders have been acknowledged and completed within SLA.
Personal Specification:
Minimum of 5 years relevant experience in Critical Environments such as Banking HQ and Data Centres.
Electrical Engineering apprenticeship or similar (Experienced in Mechanical Engineering – desirable)
HVAC Engineering Experience
Excellent communication skills and the ability to deal with all levels of staff.
Proactive in achieving the highest standard of operation.
Have good IT and report writing skills.
Ability to work under pressure whilst remaining calm, clear thinking and able to deliver the required services to the client within given time constraints.
Individuals who can demonstrate commitment and previous experience; but not meet all the required technical qualifications may be considered subject to Operations Directors approval.
Demonstrate a willingness to attend on and off-site training for MEP assets, this may require nights away from home due to the nature of the specialist training such as HV Authorised Person Training
A high level of initiative, with drive to continuously improve the operation.
Advanced communication skills with a full understanding of customer needs & expectations
Ability to comprehend and act upon both verbal and written instruction.
Enthusiasm and demonstration of pro-activity, diligence and alertness, with the willingness to participate in appropriate internal training programmes.
Integrity, honesty and punctuality is also expected
Show more
Show less","Electrical Engineering, Mechanical Engineering, HVAC Engineering, Plumbing, Critical Facilities, Data Centres, MEP, Planned Maintenance, Corrective Maintenance, Reactive Maintenance, PTW, Risk Assessments, Method Statements, Specialist Vendors, M&E Components, SFG20, Corrigo Work Orders, CMMS, IT, Report Writing, HV Authorised Person Training","electrical engineering, mechanical engineering, hvac engineering, plumbing, critical facilities, data centres, mep, planned maintenance, corrective maintenance, reactive maintenance, ptw, risk assessments, method statements, specialist vendors, me components, sfg20, corrigo work orders, cmms, it, report writing, hv authorised person training","cmms, corrective maintenance, corrigo work orders, critical facilities, data centres, electrical engineering, hv authorised person training, hvac engineering, it, me components, mechanical engineering, mep, method statements, planned maintenance, plumbing, ptw, reactive maintenance, report writing, risk assessments, sfg20, specialist vendors"
"Shift Engineer, Data Centres",JLL,"Slough, England, United Kingdom",https://uk.linkedin.com/jobs/view/shift-engineer-data-centres-at-jll-3787351127,2023-12-17,Greater London, United Kingdom,Associate,Onsite,"Electrical Shift Engineer
We are looking to recruit for a Electrical Shift Engineer
Duties of the Electrical Shift Engineer
As the post holder, you will be expected to have extensive understanding of Critical Facilities/Data Centre infrastructure (Mechanical, Electrical, Plumbing & associated assets). As an integrated member of the engineering shift team, you will responsible for effective inspection, operation and maintenance of all associated MEP assets within the critical facility, with a primary focus on critical facilities risk management, enhancing the quality of the service delivery through effective technical engineering capabilities.
As an electrical biased engineer you expected to take the lead in the Planned, Corrective and Reactive Maintenance tasks as well as other associated assets when required. You will have the ability to issue PTW, ensuring compliance with method statements, risk assessments
Duties to include:
Operational understanding of all MEP systems within the Critical Facility/Data Centre in a competent, and confident manner (Biased towards Electrical within the facility). Making recommendations for improvement and providing concise technical reports when required.
Ensure that Specialist Vendors service visits are completed efficiently, and quality of work has been inspected and recommendations are communicated to the duty Shift Leader/Assist Chief Engineer.
To provide backup as engineering shift relief cover in the event of annual leave, training or other absence within the engineering team. Ensuring minimum staffing arrangements are always maintained and covering your opposite shift team members when required.
Accurately reporting of all completed work orders, including precise recording of any critical spares/stock items used during these tasks.
Attend Weekly Team meetings and Quarterly Team Meetings when scheduled.
Completing the shift handover reports, ensuring that significant events or activities are noted and effectively communicated to oncoming shifts and management.
Actively participate and complete Health and Safety compliance activities, such as Hazard/Near Miss Reporting and regular attendance/completion of scheduled Toolbox Talks.
Liaise with the supply chain, when ordering M&E components for remedial/corrective works.
Take ownership of areas (including all plant rooms) under control, ensuring they are brought up too and maintained to the required show site standards.
Evaluate and escalate any potential risks, with an assessment of impact, probable causes and mitigation opportunities.
Actively contribute to continuous improvement of safe systems of work, ensuring compliance with relevant Health and Safety policy and procedures.
Complete Reactive, Planned & Corrective Work Orders in line with SFG20 and ensure reliability of assets through excellent standards of maintenance delivery.
Escalation of all ONWH Alarm Notifications in line with agreed escalation procedures for EMEA MEP operations.
Accurate completion of all Corrigo Work Orders that have been allocated for completion by the CMMS. Including accurate capture of time spent on tasks and clear/concise details of works undertaken. With no errors input onto the system
Efficient and Effective completion of Corrigo Work Orders that have been allocated, ensuring that Work Orders have been acknowledged and completed within SLA.
Personal Specification:
Minimum of 5 years relevant experience in Critical Environments such as Banking HQ and Data Centres.
Electrical Engineering apprenticeship or similar (Experienced in Mechanical Engineering – desirable)
HVAC Engineering Experience
Excellent communication skills and the ability to deal with all levels of staff.
Proactive in achieving the highest standard of operation.
Have good IT and report writing skills.
Ability to work under pressure whilst remaining calm, clear thinking and able to deliver the required services to the client within given time constraints.
Individuals who can demonstrate commitment and previous experience; but not meet all the required technical qualifications may be considered subject to Operations Directors approval.
Demonstrate a willingness to attend on and off-site training for MEP assets, this may require nights away from home due to the nature of the specialist training such as HV Authorised Person Training
A high level of initiative, with drive to continuously improve the operation.
Advanced communication skills with a full understanding of customer needs & expectations
Ability to comprehend and act upon both verbal and written instruction.
Enthusiasm and demonstration of pro-activity, diligence and alertness, with the willingness to participate in appropriate internal training programmes.
Integrity, honesty and punctuality is also expected
Show more
Show less","Electrical engineering, HVAC engineering, Critical facilities/Data Centre infrastructure, MEP (Mechanical Electrical Plumbing), PTW (Permit to Work), Method statements, Risk assessments, Corrigo work orders, CMMS (Computerized Maintenance Management System), SFG20, HV Authorised Person Training, IT skills, Report writing skills, Communication skills, Problemsolving skills, Teamwork skills, Leadership skills, Time management skills, Stress management skills","electrical engineering, hvac engineering, critical facilitiesdata centre infrastructure, mep mechanical electrical plumbing, ptw permit to work, method statements, risk assessments, corrigo work orders, cmms computerized maintenance management system, sfg20, hv authorised person training, it skills, report writing skills, communication skills, problemsolving skills, teamwork skills, leadership skills, time management skills, stress management skills","cmms computerized maintenance management system, communication skills, corrigo work orders, critical facilitiesdata centre infrastructure, electrical engineering, hv authorised person training, hvac engineering, it skills, leadership skills, mep mechanical electrical plumbing, method statements, problemsolving skills, ptw permit to work, report writing skills, risk assessments, sfg20, stress management skills, teamwork skills, time management skills"
Data Engineer,Sika,"Welwyn Garden City, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-sika-3774566691,2023-12-17,Greater London, United Kingdom,Associate,Remote,"Company Description
Sika is a specialty chemicals company with a leading position in the development and production of systems and products for waterproofing, bonding, sealing, damping, reinforcing, and protecting in the building sector and motor vehicle industry. Sika has subsidiaries in 101 countries around the world and manufactures in over 300 factories. Its more than 27,000 employees generated annual sales of CHF 9.3 billion in 2021.
Job Description
Sika Data Science is a young and growing division of Sika UK. Having begun as an arm of the UK R&D department, it has rapidly developed into a team that works on a much larger and more diverse scale: ranging from using traditional data science techniques to optimise the efficiency of operations and monitoring product quality and consistency, to harnessing the power of state-of-the-art AI solutions to identify novel construction materials and service Sika global with LLM technologies. The objectives of the team have evolved equally rapidly: we now aim to use data science to improve the productivity and sustainability of every Sika department.
Purpose of the Role
Globally, Sika R&D is undergoing a significant transformation in our approach to data recording and storage, which demands the expertise of a data engineer who is skilled in change management. This transition exposes significant opportunities for the application of data science to R&D on both the small and large scales; six-sigma approaches can be used to expedite individual development pipelines, and, over the long term, structured data portions can be identified as key candidates for data engineering and the training of machine learning solutions. As the Data Science team seeks to contribute more to Sika global, such deployments will become increasingly valuable for product catalogue optimisation with the ever-growing pool of R&D data.
What’s In it for You?
The Opportunity
You will steward the transition of Sika Concrete R&D into our new approach to data management, providing bespoke support, data cleaning and infrastructure engineering services.
You will actively seek further data sources which can be valorised through common data science initiatives, and support in the construction of large-scale solutions to harness these repositories.
You will provide support to Sika’s global R&D community by accelerating R&D processes through data science approaches, such as design of experiments, simple modelling and six-sigma methodologies.
The Rewards
You will be rewarded with a competitive salary, and a sweep of great benefits. We pride ourselves on our total reward package:
Bonus Scheme
Holiday 25 days rising to 28 days with service, plus bank holidays
Hybrid Working
Core Hours Working
A company pension contribution of up to 10%
Private healthcare for you, with option to add family
Group life cover at 6x your annual salary
Global Income Protection (PHI)
Enhanced maternity and paternity pay
Staff discount platform & Cycle to Work scheme
Dedicated Service Awards for reaching key milestones
Selected discounted Sika products
The Support
As this role requires a deep knowledge of Sika’s data infrastructure platforms, you will be trained to the point of excellence in these systems, with full support from our existing team. We are a small team who prioritise satisfaction and excellence in our work, and we will do our utmost to ensure that you are content in your role, accommodating for your needs. We aim to foster an honest and democratic culture, where all voices are equally important, and everyone is treated with respect.
The Skills
This role requires a significant commitment to professional and personal development from day one. You must be hungry to learn and eager to expand both your technical and interpersonal skillsets, focusing on the nuances of platform-specific functionalities and managing team-specific approaches. You’ll need a keen nose for where data science can be harnessed to drive productivity, both in the small and large scales, and where new sources of information can be harnessed and centralised. To these ends, your affinity for organisation and structure needs to be unmatched, and we’ll require that your attention to detail is scrupulous.
The Future
Potential Career Routes: Data Scientist, Data Science Team Leader
Working at Sika bestows you with the opportunity to progress within a global company across a variety of business areas. You’ll learn about different sectors of the business, getting the chance to build networks with various departments across the world. As your technical and interpersonal skills develop, you will increasingly contribute more to global scope, business-transformative initiatives, which will yield you the opportunity to rise through the ranks of Sika.
Qualifications
What we need from you?
Highly flexible approach to communication, able to perceptively understand and deftly adapt to the different approaches of different teams.
Scrupulous attention to detail and precise, meticulous approach to data organisation.
2+ years of experience in data engineering and Python scripting are mandatory for this role.
Experience in change management and application of data science solutions to scientific experimentation are ideal but not essential.
Experience with data systems such as SCADA peripheries and SAP is valuable.
Show more
Show less","Data Engineering, Python, Data Science, Data Cleaning, Infrastructure Engineering, Design of Experiments, Simple Modelling, SixSigma Methodologies, SCADA, SAP","data engineering, python, data science, data cleaning, infrastructure engineering, design of experiments, simple modelling, sixsigma methodologies, scada, sap","data cleaning, data engineering, data science, design of experiments, infrastructure engineering, python, sap, scada, simple modelling, sixsigma methodologies"
Data and Audio Visual Installation Field Engineer,"Stone Group Ltd, A Converge Company","England, United Kingdom",https://uk.linkedin.com/jobs/view/data-and-audio-visual-installation-field-engineer-at-stone-group-ltd-a-converge-company-3774704952,2023-12-17,Greater London, United Kingdom,Associate,Remote,"Job Title:
Data and Audio Visual Cabling Engineer
Department:
Technical Services
Base location:
Mobile w/travel centered around the Centre & North of England.
Reporting to:
AV Installations Manager
Salary:
Salary + Vehicle + Benefits
Stone Technologies Limited, a Converge company, is an innovative, highly accredited market leading technology provider with high employee engagement working to a clear set of values with sustainability central to its activities.
Everyone is welcome at Stone and will always be supported in their career with us. We encourage our people to bring their authentic selves to work and we will always ensure everyone here has an equal opportunity to reach their full potential.
At Stone we support employees and embrace diversity. This goes beyond policies and procedures to nurturing a culture of inclusivity and engagement. From comprehensive training programmes including apprenticeships, to volunteer days and community engagement, we support every employee.
Built around sustainable business methods, dedication to our customers, delivering the best possible service, believing in our people, always working with integrity and continuously innovating ourselves, we understand that taking inclusion & diversity seriously as an organisation is imperative to ensure we live up to our values
We are seeking a
Data and
Audio Visual Engineer
to join our
Professional Services
team.
Job Purpose
We are looking for someone to deliver a range of AV/Structured cabling installations as part of a team with the AV Installations Manager and also lead installs when teamed with other engineers.
This job will be reporting to the AV Installations Manager in Stone's Technical Services Department, providing higher margins than if this work was subcontracted to a 3rd party supplier.
Key Accountabilities
To provide greater capacity to in source AV/Structured cabling installation projects providing higher margins than if this work was subcontracted to a 3rd party supplier.
Main Responsibilities
To deliver AV/Structured Cabling installations across the Centre & North of England (roles based on location of successful candidates), as well as further afield dependent on business needs.
Measures of Performance
· Complete all AV/structured cabling installations within project time/budget allowance.
· Comply with Stone’s internal management systems and safety procedures
· To undertake any other ad hoc duties or projects as requested by the reporting Manager.
Person Specification
Essential Qualifications & Experience
· Audio Visual installations and Data / Fibre Cabling running and termination.
· Cabinet patching and clean installations.
· Access point installations.
Desired Qualifications & Experience
· Experience with the main brands requiring certification (e.g Promethean).
· Data Cabling capability and fibre installation / certification.
· Experience working on Wireless site surveys & Data surveys
Competencies/Behaviours
· Passion for the brand - Contagious enthusiasm and commitment to our business brand, our vision and values
· Growth mindset - A genuine desire for personal and business growth to deliver outstanding results.
· Teamwork - Believes in building effective and collaborative working relationships in order to achieve our goals
· Customer focus - Proactively and consistently provides the best service that responds to what customers want
Job Details:
This is a full-time position - 37 hours per week
The working days are Monday to Friday
Home-based position with daily travel to customer sites required
If you meet the criteria for this role, please apply today for an initial informal discussion. These are business-critical positions so don't delay in applying!
We reserve the right to close this vacancy early if we are in receipt of sufficient applications for this position.
No terminology in the advert you have seen is intended to discriminate on the grounds of age, disability, gender reassignment, marriage and civil partnership, pregnancy and maternity, race, religion or beliefs, sex or sexual orientation, and we will gladly accept applications from all sections of the community. We have an equality, diversion, and inclusion policy.
Show more
Show less","AV installations, Data cabling, Fibre cabling, Structured cabling, Cabinet patching, Access point installations, Wireless site surveys, Data surveys, Project management, Safety procedures, Teamwork, Customer focus","av installations, data cabling, fibre cabling, structured cabling, cabinet patching, access point installations, wireless site surveys, data surveys, project management, safety procedures, teamwork, customer focus","access point installations, av installations, cabinet patching, customer focus, data cabling, data surveys, fibre cabling, project management, safety procedures, structured cabling, teamwork, wireless site surveys"
Data Engineer,Pret A Manger,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-pret-a-manger-3776727272,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"People at Pret work hard, have fun, learn a lot and really grow. Right now, we’re looking for a passionate Data Engineer to join us.
About Pret
Pret is an international on-the-go food and drink retailer founded in 1986. Our purpose is to make every day a little bit brighter, through organic coffee, freshly prepared food, and exceptional customer service to millions of people. Pret runs hundreds of shops across the UK, France, USA, and Hong Kong, with additional shops run by franchise partners around the world.
Pret is transforming from a bricks-and-mortar focussed retailer to a truly customer-centric digital business with a leading consumer brand that aims to stand for more than just ‘freshly made, healthy food, to go’. We’re aiming to better understand our customers and their preferences, so now is an exciting time to join us!
The Role
As a Data Engineer, you will join a team helping to realise value from Pret’s data. You will be working collaboratively to deliver strategic data-informed initiatives, following modern data engineering best practices. You will play a part in designing and building big data pipelines and enabling us to create the efficient data flows required in a wide variety of areas across the business. You will work in a culture that supports learning, collaboration, transparency, and inclusiveness, to influence the future of Pret. You will work in a hybrid approach, normally working in the office 2 or 3 days per week and from home 2 or 3 days per week (Monday to Friday).
Responsibilities
Translate business requirements into technical solutions
Design, write and deploy high-volume data applications to ingest, transform, and store data in a common data model to be used in analytics and reporting
Write and deploy ETL applications and scripts across a range of data sources and stores
Practice disciplined software engineering (e.g. automated testing, code reviews, and writing quality code)
Promote skills and knowledge sharing
Learn new data technologies and be keen to adapt
Work in a hybrid environment
Essential skills and experience
Experience working with big data as a Data Engineer, following modern software engineering best practices
Proficient in cloud-based data services and platforms, including Snowflake
Proficient in using Python
Proficient in using SQL
Proficient in writing automated tests
Proficient in using Git version control with a remote repository service such as GitHub
Excellent verbal and written communication skills (English)
Desirable skills and experience
Experience using Azure
Experience using PySpark
Experience of Terraform
Experience with PostgreSQL
Experience of creating CI/CD pipelines using GitHub Actions
Experience developing software using TDD (Test-Driven Development)
Experience acquiring new software skills and working with new technologies and approaches
Experience of working with agile methods
Pret Behaviours
Passion – drive, enthusiasm, and pride
Clear Communication – clear and appropriate communication, great at listening
Team Working – helpful, sociable, and respectful
Great Execution – high standards, well planned, efficient
Open to Change – flexible, keen to learn
Realising Potential – supportive and challenging
One Pret – collaborative, whole business focused
Pret offers:
Competitive salary and annual bonus
33 days holiday a year including Bank Holidays
Private healthcare
Life assurance
Pret pension scheme
Season ticket loan
Free lunch and drinks
50% discount in Pret shops worldwide
Great reward and recognition events
Legendary parties
The deadline for applications for this role is
Friday 15th December 2023.
Show more
Show less","Data Engineering, Python, SQL, Git, Snowflake, Automated Testing, TDD, Agile, CloudBased Data Services, CI/CD Pipelines, PostgreSQL, Terraform, Azure, PySpark","data engineering, python, sql, git, snowflake, automated testing, tdd, agile, cloudbased data services, cicd pipelines, postgresql, terraform, azure, pyspark","agile, automated testing, azure, cicd pipelines, cloudbased data services, data engineering, git, postgresql, python, snowflake, spark, sql, tdd, terraform"
Data Engineer,GOAT Interactive,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-goat-interactive-3771397223,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"GOAT Interactive
is looking for a Data Engineer to join our data engineering team, this role will be responsible for the development, testing, and maintenance of data pipelines and data storage systems on Google Cloud Platform (GCP). You will be working with technologies such as Apache Airflow, BigQuery, Python, and SQL to transform and load large data sets, ensuring high data quality and accessibility for business intelligence and analytics purposes.
Our business is growing quickly and with that so the scale of our data, we are currently working on several initiatives to improve our data solution and you would become an integral part of this.
Key Responsibilities:
Design, construct, install, test, and maintain data pipelines.
Ensure systems meet business requirements and industry practices for data integrity and quality.
Manage ETL and ELT pipelines across many data sources (CSV/parquet files, API endpoints, etc)
Design and build data models for the business end users.
Write complex SQL queries for standard as well as ad hoc data retrieval needs.
Use Python for data manipulation and automation tasks.
Troubleshoot and resolve issues within the data pipelines and databases.
Participate in code reviews, learning from and teaching others to improve team knowledge and practices.
Experience:
Python3 experience in a business environment
SQL experience in a business environment
Knowledge of Data engineering principles and practices
Experience working in a fast-paced, high-growth environment where requirements can change quickly
Understanding of cloud computing ideally GCP
Understanding of software development principles and CI/CD
Knowledge of Git
Strong analytical skills and problem-solving attitude.
* * We operate a Hybrid Working policy, with 3 days per week at our office in Southwark and 2 days from home! Please ensure this split works for you when applying
Show more
Show less","Data Engineering, Data Pipelines, Data Storage Systems, Google Cloud Platform (GCP), Apache Airflow, BigQuery, Python, SQL, ETL, ELT, Data Models, Data Manipulation, Automation, Troubleshooting, Databases, Code Reviews, Git, Cloud Computing, Software Development Principles, CI/CD","data engineering, data pipelines, data storage systems, google cloud platform gcp, apache airflow, bigquery, python, sql, etl, elt, data models, data manipulation, automation, troubleshooting, databases, code reviews, git, cloud computing, software development principles, cicd","apache airflow, automation, bigquery, cicd, cloud computing, code reviews, data engineering, data manipulation, data models, data storage systems, databases, datapipeline, elt, etl, git, google cloud platform gcp, python, software development principles, sql, troubleshooting"
Data Science Engineer (Junior),Understanding Recruitment,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-science-engineer-junior-at-understanding-recruitment-3780706879,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"Data Science Engineer (Junior)
Are you ready to join the crypto tech revolution in the heart of London? We are partnered with a venture-backed crypto technology company, is on the hunt for a Data Scientist.
👾 More details about the company:
They are shaping the future of decentralized finance.
They're one of Ethereum's top block producers
Their founders are seasoned pros
🌟 What will you get up to?
Elevate the internal monitoring and post-trade analytics.
Supercharge infrastructure performance monitoring.
Dive into market and competitor analysis.
Craft dazzling monitoring dashboards.
Collaborate with the engineering team and dive into debugging.
Contribute to cutting-edge research, prototyping, and system design.
🚀 We are looking for the following skills:
Solid analytical, numerical, and quantitative skills.
At least one year of Python programming experience.
Ace at database management (mongoDB and SQL).
Command of Linux/Unix, AWS, Git, Docker.
A deep understanding of blockchain fundamentals and MEV. 💡
Don't miss this chance to be part of something big! Apply now! 💰🌍
Show more
Show less","Python, MongoDB, SQL, Linux/Unix, AWS, Git, Docker, Blockchain, MEV","python, mongodb, sql, linuxunix, aws, git, docker, blockchain, mev","aws, blockchain, docker, git, linuxunix, mev, mongodb, python, sql"
Data Engineer - Retail,Freshminds,"England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-retail-at-freshminds-3780382569,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"A household retailer is undertaking a full company transformation programme and is in the process of assembling their transformation function, filled with people who are passionate about delivering results and making an impact on day one.
This is an exciting and challenging role that offers the opportunity to manage the delivery of projects worth tens of millions of pounds and support one of retail's largest upcoming transformation programmes in the UK.
They are seeking an experienced Data Engineer to work across various teams focused on developing data products which sit at the heart of the commercial and data strategies. You will work closely with the data product owner, data scientists, and business stakeholders.
Requirements:
University degree in a relevant STEM discipline
c.3-5 years’ work experience as a Data Engineer working with big data
Experience building data products in the cloud, preferably Azure, with Pyspark and Databricks
Excellent PySpark, Databricks, and SQL skills
Experience with productionising data products with CI/CD pipelines is desirable
Details:
Start date: ASAP
Duration: 3 months contract, with possibility of going permanent
Salary: Up to £200/day on contract, £50,000 per annum
Location: Can be remote, with some occasional travel to office
Show more
Show less","Data Engineering, Data Science, Cloud Computing, Azure, PySpark, Databricks, SQL, CI/CD Pipelines","data engineering, data science, cloud computing, azure, pyspark, databricks, sql, cicd pipelines","azure, cicd pipelines, cloud computing, data engineering, data science, databricks, spark, sql"
Data Engineer,Understanding Recruitment,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-understanding-recruitment-3780362809,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"Are you excited about joining a collaborative team that's driving innovation to revolutionise e-commerce?
We're currently seeking a Data Engineer to be part of our dynamic tech team, working on cutting-edge projects in the e-commerce space.
As a Data Engineer, your focus will be on constructing and maintaining robust data pipelines within the AWS ecosystem, particularly leveraging the innovative Delta Lakehouse Platform on Databricks.
2x days in-office (Newcastle) per week
Benefits:
Competitive Salary
Inclusive and Dynamic Culture
Opportunities for Growth and Progression
Engage with Cutting-Edge Technology
If you're ready for an exciting opportunity, apply now! 🚀
Show more
Show less","AWS, Databricks, Delta Lakehouse Platform","aws, databricks, delta lakehouse platform","aws, databricks, delta lakehouse platform"
Data Engineer,MindGym,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-mindgym-3782065000,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"Data Engineer
Location: London; Hybrid
The Job
MindGym uses the latest behavioural science to transform the performance of companies and the lives of people who work in them. Most of FTSE100/S&P100 are clients and over 2 million professionals in 60 countries have taken part in a live Mind Gym experience, whilst many more have connected digitally.
We’re growing rapidly and have successfully floated on the London Stock Exchange. This is just the start. We have a bold vision to redefine how companies and individuals flourish, and so disrupt the market for behavioural change.
We have some big plans and ambitions for 2024 (and beyond), and to deliver on these we’re going to be growing our passionate, diverse and ambitious team of technologists to help make our mission a reality… If you’re determined to make a difference, voraciously curious and brimming with entrepreneurial spirit, we’d like to hear from you.
Day to Day:
Work with stakeholders throughout the organisation as part of a cross functional “mission team” to identify opportunities for leveraging data to drive business solutions.
Identify valuable data sources, automate collection processes, and organise the raw data
Explore ways to enhance data quality and reliability
Undertake preprocessing of structured and unstructured data
Run, operate and take care for the data platform.
Contribute to the attainment of the quarterly OKRs of the team.
Follow lean and agile principles, maximising the work
not
done and only building what you need.
Get involved with agile ceremonies that prepare, co-ordinate and reflect on the team’s throughput.
Help us build a generative team culture with high co-operation, shared risks, encouraged bridging and blame free post-mortems.
About you
Essential
You love being a data engineer and have been doing it for at least 1 year
Experience working with diverse and complex data structures
Experience with different data storage / retrieval, encoding and evolution techniques
Experience with Linux and basic command line tools
Experience with Python and Pandas in a containerised environment
Proficiency in advanced SQL scripting and adherence to best practices
Cloud native in AWS or GCP
Experience with CI/CD practices and tools and data pipelines
Desirable
Experience of working in a distributed and high-volume environment
Experience with modern analytical databases (e.g. Amazon Redshift, Google BigQuery, Snowflake, Firebolt)
Experience of data modelling techniques and query languages (e.g. SQL, pandas, R data.table)
Your stack extends to Docker, Kubernetes and Kafka
Experience in back-end development
Benefits
25 days annual leave
Private medical insurance
Critical life insurance
Income protection
Pension (5% company contribution)
Travel insurance
Cycle to work scheme
Season ticket loan
Charity work (two days paid annually)
Gym subsidy
Yoga, mindfulness and massages
Unlimited mental health support, 24/7 unlimited remote G
Working at MindGym
We want people to do their best work every day and that's why we ensure everyone at MindGym works in an environment that challenges them to be their best, while maintaining a strong sense of wellbeing.
MindGym is committed to diversity, equity and inclusion. We offer equal employment opportunities to all applicants regardless of age, gender, ethnicity, disability, sexual orientation, religious beliefs, marital or parental status.
We support flexible working arrangements for all roles unless operational requirements require otherwise. We are committed to providing a working environment where everyone's individuality and unique contributions are recognised, valued, and respected.
Show more
Show less","Data Engineering, Linux, Python, Pandas, SQL, AWS, GCP, CI/CD, Data Pipelines, Amazon Redshift, Google BigQuery, Snowflake, Firebolt, Docker, Kubernetes, Kafka, Backend Development","data engineering, linux, python, pandas, sql, aws, gcp, cicd, data pipelines, amazon redshift, google bigquery, snowflake, firebolt, docker, kubernetes, kafka, backend development","amazon redshift, aws, backend development, cicd, data engineering, datapipeline, docker, firebolt, gcp, google bigquery, kafka, kubernetes, linux, pandas, python, snowflake, sql"
Intermediate Data Engineer,Hippo,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/intermediate-data-engineer-at-hippo-3770166407,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"About The Role
Hippo Digital is recruiting for an
Intermediate Data Engineer
to join our Hippo Herd. Our Intermediate Data Engineers work in multi-disciplinary teams that build, support & maintain User-Centred digital solutions that offer real value and work for everyone.
As an
Intermediate Data Engineer
, you will be working as part of a multidisciplinary team to build solutions that make data accessible to enable solutions to be optimised by using an evidence-based approach. Engaging with our clients you will design and implement data solutions ensuring solutions are integrated with internal systems and business processes.
Your Role In a Nutshell
Help architects realise data system design such as Meshes, Warehouses and Event based systems
Implement data flows to connect operational systems, data for analytics and business intelligence (BI) systems
Re-engineer, develop and optimise code to ensure processes perform optimally
Build great relationships with your team and stakeholders
Work with the Hippo community to share best practice to ensure high standards
Promote Hippo’s Engineering Herd internally and externally (for example through writing Blogs, Workshops, Seminars or Conferences)
About The Candidate
Proficiency in Python and some experience with another core language
Experience with one Cloud Platform (AWS, GCP or Azure)
Experience in R, Bash, Java/.NET and/or PowerShell desirable
Working knowledge of SQL and/or NoSQL
Knowledge of cloud architectures, networking and distributed computing systems
Experience of data system design such as Data Lakes, Data Meshes and Data Warehouses.
Experience in a range of data sources such as unstructured, structured, SQL, NoSQL, etc.
Experience in infrastructure delivery on any data platform (Snowflake, Elastic, Redshift, Data Bricks, Splunk, etc).
Demonstrable experience writing regular expressions and/or JSON parsing, etc.
Experience in log processing (Cribl, Splunk, Elastic, Apache NiFi etc.)
Experience in the production of dashboard/insight delivery
Be able to demonstrate a reasonable level of security awareness (An understanding of basic security best practices, OAuth, MFA, TLS, etc.)
Experience in the processing of large datasets
A firm understanding of data modelling and normalisation concepts
Good estimation skills for times, latencies and costs.
Experience working within multidisciplinary teams desirable
Teamwork and presentation skills, experience of mentor led working practices
About The Company
As well as a competitive salary which we’re transparent about from the outset, you can also expect a range of benefits:
Contributory pension scheme (Hippo 6% with employee contributions of 2%)
25 days holiday plus UK public holidays
Perkbox access for a wide range of discounts
Critical illness cover
Life assurance and death in service cover
Volunteer days
Cycle-to-work scheme for the avid cyclists
Salary sacrifice electric vehicles scheme
Season ticket loans
Financial and general wellbeing sessions
Flexible benefits scheme with options of:
private health cover
private dental cover
additional company pension contributions
additional holidays (up to an extra 2 days)
wellbeing contribution
charity contributions
tree planting
Hi, we’re Hippo Digital.
At Hippo Digital, we design with empathy and build for impact. We do this by combining data-informed evidence, human-centred design and software engineering. We're a digital services partner who is genuinely invested in helping our clients thrive as modern organisations. Our delivery methodology is truly agile, from concept to reality, supporting innovation and continuous improvement to achieve your desired outcomes.
We firmly believe that technology should serve humanity, not the other way around. We take a human-centred approach to everything we do because we understand that complex problems require a service design approach. This means understanding how users behave and ensuring our solutions work for them in the real world.
Our combination of data, design, and engineering delivers bespoke digital services that make a positive and meaningful impact on organisations and society. We're confident in our abilities, authentic in our approach, and passionate about what we do. If you're looking for a digital services partner that can deliver real results, let us help you build for the future and make a lasting impact.
Hippo locations
We are headquartered in Leeds and located across the UK in Edinburgh, Manchester, Birmingham, London and Bristol. We're on the lookout for top talent nationwide, so don't let location hold you back from applying. We welcome candidates from all over the UK who possess the flexibility to work from any of our locations. Plus, we offer a generous relocation support package of up to £8k to help make your move a smooth one. It's worth noting that, given the dynamic nature of our business, you may be required to work on-site at client locations or from your own home.
Diversity and Inclusion at Hippo Digital: A commitment to lasting change
At Hippo Digital, we're wholeheartedly dedicated to promoting diversity, equality and inclusion. For us, it's not about checking off boxes but about making a true, lasting difference. Having a diverse team unlocks our capacity for innovation, creativity and problem-solving while fostering an inclusive environment that benefits everyone. We strongly encourage applicants from all backgrounds to apply and join us on this important journey.
Show more
Show less","Python, Java, .NET, R, PowerShell, Bash, SQL, NoSQL, AWS, GCP, Azure, Snowflake, Elastic, Redshift, Data Bricks, Splunk, Cribl, Apache NiFi, Data Lakes, Data Meshes, Data Warehouses, Cloud Architectures, Networking, Distributed Computing Systems, Data System Design, Data Sources, Infrastructure Delivery, JSON Parsing, Regular Expressions, Log Processing, Dashboard/Insight Delivery, Security Awareness, Data Modelling, Normalization Concepts, Estimation Skills, Multidisciplinary Teams, Teamwork, Presentation Skills, Mentor Led Working Practices","python, java, net, r, powershell, bash, sql, nosql, aws, gcp, azure, snowflake, elastic, redshift, data bricks, splunk, cribl, apache nifi, data lakes, data meshes, data warehouses, cloud architectures, networking, distributed computing systems, data system design, data sources, infrastructure delivery, json parsing, regular expressions, log processing, dashboardinsight delivery, security awareness, data modelling, normalization concepts, estimation skills, multidisciplinary teams, teamwork, presentation skills, mentor led working practices","apache nifi, aws, azure, bash, cloud architectures, cribl, dashboardinsight delivery, data bricks, data lakes, data meshes, data modelling, data sources, data system design, data warehouses, distributed computing systems, elastic, estimation skills, gcp, infrastructure delivery, java, json parsing, log processing, mentor led working practices, multidisciplinary teams, net, networking, normalization concepts, nosql, powershell, presentation skills, python, r, redshift, regular expressions, security awareness, snowflake, splunk, sql, teamwork"
Python Engineer - Data ESG/RI,Man Group,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/python-engineer-data-esg-ri-at-man-group-3778542018,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"The Role
We are seeking a highly motivated and experienced engineer to join the Data Engineering team within Man Technology. You will play a pivotal role in our strategic build-out of cutting-edge technology to achieve our ESG goals. Collaborating with ESG thematic researchers and our Responsible Investment (RI) and Stewardship and Corporate Social Responsibility (CSR) teams, you will contribute to the development of our quant-driven RI strategies and discretionary RI portfolios.
Your Responsibilities
Onboarding and analysing the best vendor ESG datasets for research, analysis, and reporting in collaboration with our Data Science team.
Implementing quantitative methods to establish evidence-based views of security's ESG alignment.
Creating Python APIs to facilitate easy access to ESG data and analysis for systems and researchers.
Developing tools and reports to ensure actionable ESG data is available firm-wide, including interactive tools, static reports, and Jupyter notebooks.
Integrating ESG data into Man's back-office systems.
Your Impact and Growth
In this role, you will gain a deep understanding of the ESG space and contribute to the investment and ESG research conducted by our teams. You will have the opportunity to apply technology and automation strategically, shaping our technology landscape and making a significant impact from the start.
Our Technology
We primarily work with Linux and Python, utilizing the full scientific stack (numpy, scipy, pandas, scikit-learn) extensively. MongoDB serves as our primary storage solution. We leverage Docker, Kubernetes, and Airflow for streamlined deployments, while OpenFin and React drive our front-end development.
Working Here
At Man Tech, we foster a small company, no-attitude culture that is transparent, collaborative, and open. You will have ample opportunities for growth and the chance to make a real difference. We actively engage with the broader technology community, hosting and sponsoring London's PyData and Machine Learning Meetups. We also contribute to open-source projects and share our technology innovations.
In our fantastic open-plan office overlooking the River Thames, we organize regular social events ranging from photography to climbing, karting, and wine tasting. We offer competitive compensation, a generous holiday allowance, flexible benefits, and a commitment to continuous learning and development through coaching, mentoring, and sponsoring professional qualifications.
Technology and Business Skills
We strive to hire only the brightest, best and most highly skilled, passionate technologists.
Essential
Extensive programming experience, preferably in Python.
Proficiency in handling large structured and unstructured datasets.
Advocate for collaborative software engineering techniques (agile development, continuous integration, code review, etc.).
Proficient with Linux platforms and scripting languages.
Working knowledge of one or more relevant database technologies (MongoDB, PostgreSQL, Snowflake, Oracle, Microsoft SQL Server).
Proficiency with open-source frameworks and development tools (NumPy/SciPy/Pandas, Spark, Jupyter).
Advantageous
Prior experience with financial market data or alternative data.
Familiarity with the ESG space and investment technology/approaches.
Experience in data visualization and modern web app frameworks (e.g., React).
Proficiency with git.
Personal Attributes
Strong academic background with a degree in Computer Science, Mathematics, Engineering, or Physics from a leading university.
Commitment to engineering excellence and a passion for technology.
Analytical problem-solving skills.
Strong time management and organization skills.
Excellent interpersonal and communication skills.
Show more
Show less","Python, Linux, NumPy, SciPy, Pandas, Scikitlearn, MongoDB, Docker, Kubernetes, Airflow, OpenFin, React, Agile development, Continuous integration, Code review, Git, SQL, Snowflake, Oracle, Microsoft SQL Server, Spark, Jupyter, Data visualization, Web app frameworks","python, linux, numpy, scipy, pandas, scikitlearn, mongodb, docker, kubernetes, airflow, openfin, react, agile development, continuous integration, code review, git, sql, snowflake, oracle, microsoft sql server, spark, jupyter, data visualization, web app frameworks","agile development, airflow, code review, continuous integration, docker, git, jupyter, kubernetes, linux, microsoft sql server, mongodb, numpy, openfin, oracle, pandas, python, react, scikitlearn, scipy, snowflake, spark, sql, visualization, web app frameworks"
"Business Data Engineer, BI Developer (Snr Assoc), Credit Fund, London",Millar Associates,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/business-data-engineer-bi-developer-snr-assoc-credit-fund-london-at-millar-associates-3779162370,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"Credit / FIxed Income, SQL, Data Warehouse, BI Developer, CUBE, etc.
Up to £130k Total + Benefits
This large Credit Hedge Fund manages nearly $50 billion across a range of Structured Credit, CLOs, Loans, High Yield, etc., They seek a Data Engineer to report to the UK Head of Data and support development, implementation and integration of data, platforms, processes across the firm.
Working closely with Business Development, the PMs, Fund Control, & IT, there will be day to day interaction with other business areas and an understanding of the business context is vital. You’ll need strong Data experience gained within Credit / Fixed Income Financial Services, and great analytical, and communication skills.
KEY RESPONSIBILITIES:
Continue the development of core Data Warehousing infrastructure
Deliver data processing pipelines for the Portfolio Analytics Team.
Assist in a firmwide effort to update & optimize current dataflows, reporting capabilities.
Assess potential systems/platforms to determine relevance for current processes.
Support the development of an automated, dynamic, and self-service reporting platform
Assist the implementation of new platforms, processes, & reporting across: design; requirements gathering; data integrity
ESSENTIAL SKILLS & EXPERIENCE:
2-5 years of experience as a SQL or Data Warehouse Developer or 2-3 yrs in a quantitative role in financial services
A strong understanding of Data Warehousing concepts
Strong SQL coding ability (stored procedures / CTEs / subqueries etc.)
Superb, persuasive communication skills in English (spoken & written)
Degree in a numerical subject (maths / physics / engineering, etc.)
DESIRABLE ATTRIBUTES:
Good Credit / Fixed Income knowledge
Python & SQL Server Analysis Services (SSAS)
ETL tools: Airflow / SSIS / Alteryx
Project management experience
Show more
Show less","SQL, Data Warehouse, BI Development, CUBE, Data Engineering, Business Development, Fund Control, Analytical skills, Communication skills, Data Warehousing infrastructure, Portfolio Analytics, Dataflows, Reporting capabilities, Dynamic reporting, Selfservice reporting, Data integrity, Python, SQL Server Analysis Services (SSAS), ETL tools, Airflow, SSIS, Alteryx, Project management","sql, data warehouse, bi development, cube, data engineering, business development, fund control, analytical skills, communication skills, data warehousing infrastructure, portfolio analytics, dataflows, reporting capabilities, dynamic reporting, selfservice reporting, data integrity, python, sql server analysis services ssas, etl tools, airflow, ssis, alteryx, project management","airflow, alteryx, analytical skills, bi development, business development, communication skills, cube, data engineering, data integrity, data warehousing infrastructure, dataflows, datawarehouse, dynamic reporting, etl tools, fund control, portfolio analytics, project management, python, reporting capabilities, selfservice reporting, sql, sql server analysis services ssas, ssis"
Software Engineer - Market Data,TradingHub,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/software-engineer-market-data-at-tradinghub-3754311868,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"About TradingHub
Founded in 2010, we have grown from a united vision shared between two people to a team of over 140 across London, Toronto, New York and Singapore. We have achieved scale by building the best-in-class surveillance tooling, where our analytics bring the front office risk mindset to the compliance function.
Though we have developed in the trade surveillance arena, we have always been more than that. At heart, we are a finance-focused big data firm. Our goal is to continue creating the world’s leading financial markets analytics platform.
The Role
We are seeking a data-driven Software Engineer to help design, build and maintain the market data and associated analytics that will be used throughout the Trading Hub products and platforms.
Successful candidates will work alongside Pricing Quants and Risk Strats to productionise new data and help industrialise the current market data environment which will vastly improve some key profit-driving new features for the company, which can vastly stimulate our continued growth.
The role will require candidates to have at least some exposure to financial markets, whether through university projects/industry experience or demonstrable hobbyist endeavours. You will work with experienced professionals who will help shape your career and hone your skill set as you become more adept to TradingHub’s products. This role will attract someone who is comfortable working with complex architectures and are keen to apply their development and problem-solving skills in a practical setting that will make a genuine difference to a fast-growth FinTech.
Responsibilities:
Working alongside Pricing, Risk and Trading Hub product teams to develop, support and productionise data and analytics that will be used in all TradingHub’s products
Develop a deep understanding of financial markets and how market data is used within our products
Research and development of strategic data storage and processing for large data sets
Utilise our in-house big data language for the large-scale modelling and analysis
Solve and understand critical production issues
Develop an excellent understanding of TradingHub’s key products such as MAST, AMLA, and TEAM and work closely with their teams to integrate our data into their analytics
Requirements
STEM degree OR relevant industry experience within a programming position within a financial context
Evidence of exceptional analytical and problem-solving skills
High attention to detail and ability to work on multiple fast-paced projects simultaneously
Proficiency with C#, or any object-oriented programming language
Experience of databases and proficiency with SQL
Experience using version control systems such as git
Proactive and curious mindset, with accountability as a core personal value
Benefits
Why should you apply?
Ambition:
Extremely fast-growing company with an uncapped potential, offering every colleague a broad range of experience and plenty of opportunities for internal movement, as well as rapid career progression. Vibrant company culture full of uniquely talented and friendly colleagues with regular social perks to build camaraderie.
Flexibility:
25 days holiday + bank holidays, informal dress code, generous maternity/parental leave policies (up to 6 months fully paid maternity leave, and enhanced paternity and shared parental leave policies). We also offer a flexible working policy (up to 2 days a week remote).
Reward:
Highly competitive compensation plus annual discretionary bonus and discretionary EMI scheme (company share option scheme).
Support:
Company budget for personal training and a number of available resources, 5 days study leave per year, private healthcare including dental and vision, healthcare cash plan, subsidised counselling sessions, company pension plan, death in service coverage, cycle to work scheme & tech scheme.
TradingHub is an equal opportunities employer. We do not discriminate based on race, religion, ethnic or national origins, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, socio-economic background, responsibilities for dependants, physical or mental disability or other applicable legally protected characteristics. TradingHub selects candidates for interview based solely on their skills, qualifications and experience.
TradingHub is committed to making our recruitment process accessible to all and we encourage candidates to inform us of any required adjustments.
A full copy of our diversity equity and inclusion policy will be made available to you upon request.
Show more
Show less","C#, SQL, Databases, Version control systems (Git), Objectoriented programming, Data storage, Data processing, Data modelling, Data analysis, Financial markets, Market data, Analytics, MAST, AMLA, TEAM","c, sql, databases, version control systems git, objectoriented programming, data storage, data processing, data modelling, data analysis, financial markets, market data, analytics, mast, amla, team","amla, analytics, c, data modelling, data processing, data storage, dataanalytics, databases, financial markets, market data, mast, objectoriented programming, sql, team, version control systems git"
Data Engineer - Health Analytics,Remit Resources,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-health-analytics-at-remit-resources-3736395052,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"Data Engineer needed for this first class Healthcare Analytics firm, dedicated to delivering top-tier consultancy services and developing innovative data products for their clients. Their dual focus encompasses the meticulous delivery of one-time projects and the crafting of repeatable propositions to elevate market presence.
Why this role? :
🚀
Innovate in Healthcare
: Be at the forefront of transforming healthcare through data.
🔄
Diverse Projects
: Engage in a mix of unique projects and development of market propositions.
🌟
Career Growth
: Thrive in an environment that nurtures your professional growth and values your contributions.
Your Role Will Involve:
📊
Analytics & Engineering
: Navigate through both analytical and engineering landscapes, with a predominant focus on Data Engineering.
🛠️
Tool Mastery
: Employ modern data engineering tools like Azure Data Factory, Databricks, and Git to develop and optimize data products.
📈
Data Management
: Harness your knowledge and skills in SQL, R, or Python to manage and manipulate data effectively.
We Are Seeking:
🎯
Experience
: Up to one year of professional experience in a relevant field.
🧰
Tool Proficiency
: Familiarity with modern data engineering tools and platforms.
📊
Data Expertise
: Experience or a keen interest in working with data, utilising SQL, R, or Python.
☁️
Cloud Computing
: Experience with cloud computing platforms like Azure, AWS, or GCP is a plus!
Your Impact:
Your expertise will not only drive the successful delivery of projects but also play a pivotal role in the development and refinement of their data products, directly influencing their market propositions and client success.
Let’s Connect:
To delve deeper into this opportunity and discuss your potential fit in confidence, please connect with Tori Amarnani at Remit Resources.
Show more
Show less","Healthcare Analytics, Data Engineering, Azure Data Factory, Databricks, Git, SQL, R, Python, Cloud Computing, Azure, AWS, GCP","healthcare analytics, data engineering, azure data factory, databricks, git, sql, r, python, cloud computing, azure, aws, gcp","aws, azure, azure data factory, cloud computing, data engineering, databricks, gcp, git, healthcare analytics, python, r, sql"
Data Analyst,KDR Talent,"Leatherhead, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-kdr-talent-3780685576,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"Data Analyst | Leatherhead, UK | Salary up to £40,000
Are you a Data Analyst who wants to use your skills to make a real impact in the Education world?
Do you enjoy seeing your work bringing people and data together in the best ways?
Are you the person who wants to move forward in a greenfield space?
Then this could be the role for you..
Our client really is all about making the lives of our Teachers and Children better within their Education. We all know how important it is to have the best support possible during this time, which means you’ll have the opportunity to be a part of something truly amazing – helping support the lives of thousands of people. Their programs ensure that key guidance and structure is always provided to all involved, while forever learning and adapting to create the very best future.
This role will see you in the thick of it, being the key asset to bring all of their data together. The work you’ll be a part of will help understand and define the strategy and vision going forward, allowing you to work in a greenfield environment! So, with your Data support, you’ll be the glue that really helps create the future in a role you can truly be proud of!
So, what are we looking for:
👉 Experience in utilising and analysing large Data sets
👉 Ability to clean, define and show Data
👉 Database Design and Modelling
👉 Experience working in a CRM environment
👉 Ability and confidence to liaise with multiple departments and Stakeholders
👉 Sales and/or Marketing experience would be desirable
👉 Someone who is passionate about doing great things with Data!
If you’re looking to be part of something amazing and to make a real difference, hit that ‘Apply Now’ button!
Show more
Show less","Data Analysis, Data Visualization, Database Design, Database Modeling, CRM, Stakeholder Engagement, Sales, Marketing","data analysis, data visualization, database design, database modeling, crm, stakeholder engagement, sales, marketing","crm, dataanalytics, database design, database modeling, marketing, sales, stakeholder engagement, visualization"
Senior Data Engineer,Full Circle Recruitment Ltd,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-full-circle-recruitment-ltd-3787127965,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"Senior Data Engineer,
£70,000-90,000
Digital advertising product development
London 3 days/week: WFH 2 days/week
This rapidly growing analytics firm provides clients with a market-leading data product to optimise their media supply chains. To facilitate continued growth through 2024, they need to hire a lead data engineer/architect to work on back-end development and drive product improvements, focusing on:
Architecting and integrating new user-facing features
Build code libraries and harnessing open source where advantageous; keeping up to date with the latest trends in large-scale data processing
Analysis, monitoring and optimisation of data storage and retrieval techniques, including updates to Airflow / Google Cloud Composer
Contributing regularly to strategy discussions regarding process and system improvements; driving discussions as to what to build next
Mentoring of junior technical staff and training of other non-technical staff
Experience sought
At least
5 years in software development
, including at least
3 years in Adtech/Martech
, building big data analytical products
Leadership, product design and architecture skills and experience
Technical skills must include:
Python
knowledge, including pip package management and pytest
Apache Airflow ETL framework
, ideally including experience of Google Cloud Composer
Git, used in a team environment, including the use of a cloud-based repository, e.g.
Github, Bitbucket, Gitlab
Continuous Integration concepts and tools, e.g. Circle CI, 1 Drone, Google Cloud Build
Extensive knowledge of containerisation, including Docker and orchestration frameworks such as Kubernetes or Mesos/Marathon
What’s in it for you
This is a very hands-on, senior technical role in a leading and growing firm, where you wil work closely with founders, clients and product teams, and get to make a real mark in the technical direction of the business
Working with best-of-breed technology, incorporating bought and open source, across a range of different client platforms
Opportunity to take a lead in architectural decisions
£70-90,000 base salary, plus pension
Show more
Show less","Python, Pip, Pytest, Apache Airflow, Google Cloud Composer, Git, GitHub, Bitbucket, GitLab, Continuous Integration, Circle CI, Drone, Google Cloud Build, Docker, Kubernetes, Mesos/Marathon","python, pip, pytest, apache airflow, google cloud composer, git, github, bitbucket, gitlab, continuous integration, circle ci, drone, google cloud build, docker, kubernetes, mesosmarathon","apache airflow, bitbucket, circle ci, continuous integration, docker, drone, git, github, gitlab, google cloud build, google cloud composer, kubernetes, mesosmarathon, pip, pytest, python"
Data Engineer,trilitech,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-trilitech-3767385730,2023-12-17,Greater London, United Kingdom,Associate,Hybrid,"Data Engineer
Location:
London, Soho
Industry:
Blockchain | Tech
Salary Range:
Industry leading base, crypto LTIP (Tezos tokens) and great benefits
About Us:
Trilitech is a Tezos ecosystem company based in London. We cooperate with other companies, partners, and projects on a variety of Tezos blockchain matters. These include core protocol development, application development, and business development in three key verticals: Art, Gaming, and DeFi. We recently supported the launch of Manchester United's digital collectibles, which is based on white label NFT store technology supplied by us.
Our corporate partners are McLaren Racing, Ubisoft, Societe Generale and many others. Some of the exciting projects built on the Tezos blockchain include Hic Et Nunc, Objkt.com, OneOf and Kukai. The ambition of Trilitech is to be a centre of excellence in everything we do; we are hence looking to scale the team with the best of the best in their respective fields.
Responsibilities:
Work with Senior Leadership to create and own the data strategy defining success measures and maintaining a roadmap
Build the infrastructure required for extraction, loading, and transformation of our data to support business outcomes
Work with the technical teams to integrate data pipelines to drive new insights
Take ownership of ambiguous problems in a structured way with solutions that add value
Support teams with data-related technical issues and support their needs through the data infrastructure
Prioritise moving fast and value through experimentation, with decision-making led by impact
Drive communication with stakeholders in a clear and concise way, ensuring everyone remains aligned on direction
Essential Requirements:
Worked as a Data Engineer working with large scale, high volume, data-centric, applications and infrastructure
Comfortable with SQL, ETL, data modelling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.)
Knowledge of one or more of the following: data processing automation, data quality, data warehousing, data governance, business intelligence, data visualisation, data privacy
Prior experience working within Blockchain industry
Benefits
Double matching of pension contribution up to 10% of gross salary
Top PMI for you and your family
Up to £2,700 pa gross contribution towards office travel
Life assurance policy
25 days holiday (plus an additional day for your birthday)
Workplace perks, such as free lunch, snacks and drinks, and an onsite gym
Culture:
We care deeply about our culture and have developed it across five key pillars:
Autonomy:
we believe in hiring great people, then giving them the space and flexibility to work in the way that’s best for them. We will agree on goals, and then expect you to decide how you want to pursue those goals.
Collaboration:
as a fast growing small business we all work together across multiple workstreams. We have a flat structure and encourage frank discussion, honesty and openness.
Learning and development:
the blockchain space is developing at an incredibly fast rate. Having a growth mentality and being open to continuous learning is an important part of our culture. It’s always OK to not know the answer to something and have to do some research! We cement this by offering more formal perks to help fund employees’ continued education.
Diversity and Inclusion:
inclusiveness is one of our blockchain’s core strengths, and in the same way we are committed to inclusiveness across all diversity dimensions. We are also actively looking for talent outside of the immediate blockchain space. What we primarily look for is general markers of excellence and ambition.
Mission driven:
we believe that the Tezos blockchain is going to change the world and our goal is to promote its adoption. Blockchain technology will power new positive change, increase the democratisation of previously restricted systems and create transparency and trust.
Show more
Show less","SQL, ETL, Data modeling, Python, C++, C#, Scala, Data processing automation, Data quality, Data warehousing, Data governance, Business intelligence, Data visualization, Data privacy, Blockchain","sql, etl, data modeling, python, c, c, scala, data processing automation, data quality, data warehousing, data governance, business intelligence, data visualization, data privacy, blockchain","blockchain, business intelligence, c, data governance, data privacy, data processing automation, data quality, datamodeling, datawarehouse, etl, python, scala, sql, visualization"
Data Engineer (£65k-81k),sequel,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-%C2%A365k-81k-at-sequel-3766416955,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"! We are not working with recruiters on this role !
Sequel is a community of the world’s best athletes investing in the world’s best startups, making a positive impact.
We are on a mission to help athletes build legacies for their families and for the world.
We are a seed-stage company founded and backed by experienced serial entrepreneurs with several successful exits.
A world-class team is looking for a world-class full-time Data engineer to help us build and improve our VC and founder facing tools.
What You Will Be Working On
Implement tools that surface trends in the investment world
Strengthen our data-driven methods of identifying startups
Create tools that let us apply data insights you generate
We are here to win
- not just to take part. Just like our clients who operate at the elite level of sport, we understand that in order to win; we need to put in the work. This means going the extra mile, being willing to do what others will not and holding ourselves to a higher standard. Some companies play to not lose, we play to win. We are proactive, decisive and have the courage to take risks.
We pay it forward -
we are always helpful. To customers, to partners and most importantly to each other. Not because it will ‘pay back’, because it’s the right thing to do.
We are adaptable -
we understand in a world that is changing faster than ever before - the only constant is change. The average lifespan of a Fortune 100 company in 1958 was 61 years, now it is 18 years. We adapt to a changing world, constantly iterating, moving forward and staying relevant.
Yes first -
we are on a mission to build something which has never been done before from the ground up. It will be a hard journey full of challenges which are currently unsolved. Our culture is defined by the fact that when we are confronted with challenges, we work from the assumption that it is possible to overcome them.
We are transparent -
we challenge each other. Regardless of title, seniority or reporting lines. We give feedback and we are hungry to receive it. We value honest feedback delivered with kindness, and we value people who ask questions.
We always trust, respect and care for each other -
it’s no easy feat working in a startup, let alone one with goals as ambitious as ours. At sequel we believe no one is ever too busy to support fellow team members and that we should always be helpful, inclusive and selfless in the way we work. Diversity of background and thought is celebrated as we respect and appreciate what makes us all different. We don’t tolerate free riders, egos or workplace politics.
Requirements
🏆
Who You Are
You are interested in data science and research but also in applying your insights
You previously worked in the field of VC/hedge data or data scraping
You know either Python or TypeScript
Bonus: experience with React and Retool
Bonus: You know your way around Firebase and GCP
An avid learner constantly exploring new technologies and tinkering with LLMs & vector databases at the moment
You have a big interest in building systems that handle millions of data records
You don't shy away from making data come to life working on the back- or frontend
A brilliant communicator - you are not afraid to tell if something is going in the wrong direction and you want to correct early on
You have an existing passion for or an interest in learning more about the worlds of startup investing and professional sport
Benefits
💰 Meaningful options package
🏥 Private healthcare
🧘 We offer perks through Juno, a holistic perks platform
🍼 Generous parental leave policy
🏢 We believe our best work is done together in the office, but we offer flexibility for team members to work up to 1 day per week at home
💻 Choose your own equipment
🏝 25 days off + bank holidays
📚 £500 per year learning and development budget
🏋️ Gym Membership Contributions
Show more
Show less","Python, TypeScript, React, Retool, Firebase, GCP, Data analysis, Machine learning, Statistical modeling, Data visualization, Data mining, Data engineering, Data science, Research, Vector databases","python, typescript, react, retool, firebase, gcp, data analysis, machine learning, statistical modeling, data visualization, data mining, data engineering, data science, research, vector databases","data engineering, data mining, data science, dataanalytics, firebase, gcp, machine learning, python, react, research, retool, statistical modeling, typescript, vector databases, visualization"
Data Engineer,LTIMindtree,"London Area, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-ltimindtree-3769004262,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Role: Data Architect
Location: London, UK
Maps source data and builds integration routines and migration rules to load and transport data in support of integrity checks and analytical exploitation.
Enables access to key information to enable one-time or regular data acquisition, determines mechanisms e.g. Stream, virtualise or ETL.
Candidate must have at least 5-8 yrs experience in design and development of ETL solutions in Azure stack
Azure Databricks / Pyspark is mandatory for this requirement.
Strong understanding of Azure service components like ADF, Databricks, SQL DB / SQL Server, Azure Blob storage, AAS and Power BI. Should be able to show working experience across all these technologies.
Should be experienced in migrating on-prem and legacy DWH to Azure stack
Must have a working experience in Azure DevOps and CI/CD tools at each phase of the SDLC cycle. All project management and test coordination will happen through DevOps and should be able to manage and drive deliverables and report status through DevOps.
Candidate must be familiar with Azure security model and Storage accounts.
Candidate must have Azure Databrick expertise with Python / Spark (Scala would be great). Strong understanding of Data Bricks background architecture, and advanced concepts like Security and productionalization.
Should be able to develop end to end automations in Azure stack – for ETL workflows, data quality validations and reporting, SIT automations through DevOps etc,
Must have good understanding of understanding of provisioning of Databricks and Azure resources
Good understanding of coding standards (code modularization, refactoring), testing automation
Familiarity with Airflow will be an added advantage.
Candidate must have at least 2-3 yrs experience in productionized applications.
Show more
Show less","Data Architecture, Data Integration, ETL, Azure, Azure Databricks, Pyspark, Azure ADF, SQL DB / SQL Server, Azure Blob Storage, AAS, Power BI, Azure DevOps, CI/CD, DevOps, Azure Security Model, Storage Accounts, Data Bricks, Python, Spark, Scala, Data Bricks Architecture, Security, Productionalization, Azure Stack, Code Modularization, Refactoring, Testing Automation, Airflow","data architecture, data integration, etl, azure, azure databricks, pyspark, azure adf, sql db sql server, azure blob storage, aas, power bi, azure devops, cicd, devops, azure security model, storage accounts, data bricks, python, spark, scala, data bricks architecture, security, productionalization, azure stack, code modularization, refactoring, testing automation, airflow","aas, airflow, azure, azure adf, azure blob storage, azure databricks, azure devops, azure security model, azure stack, cicd, code modularization, data architecture, data bricks, data bricks architecture, data integration, devops, etl, powerbi, productionalization, python, refactoring, scala, security, spark, sql db sql server, storage accounts, testing automation"
SQL Data Engineer,Belmont Lavan,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/sql-data-engineer-at-belmont-lavan-3590311930,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Key responsibilities include:
{ Managing the investigation of corporate data requirements, documenting them according to the required standards utilising the prescribed methods and tools.
{ Implementing data flows to connect operational systems, data for analytics and BI systems.
{ Ensuring that those using the data structures and associated components have a good understanding and that any queries are dealt with promptly and efficiently.
{ In liaison with the information management or IT management functions, contributing to the development and maintenance of corporate data standards.
Required skills and experience:
{ 3+ years Strong technical process understanding regardless of technology.
{ Core SQL Competencies - SSMS, SSIS, T-SQL, Stored Procedures.
{ High attention to detail.
{ Strong communication skills.
{ Structured problem-solving techniques.
{ Performance-tuning skills
{ Efficient in building ETL and ELT processes for enterprise solutions
{ Strong software delivery methods and knowledge
Desired Skills And Experience
{ Exposure to Python
{ Digital delivery - has a track record of working on DevOps delivery
{ Exposure in Climate Change data legislation, practices and stakeholders
{ Experience in Environmental related industries i.e Water, Energy and Forestry related
{ Presentation skills
{ Azure skills (i.e. Azure Devops, Azure Data Factory, Azure Data Bricks, Azure SQL)
This is a permanent full-time role, reporting to the Head of Data Engineering,
Benefits
30 days holiday plus bank holidays. Generous non-contributory pension provision, annual discretionary bonus (depending on company performance), Employee Assistance Program, life assurance, Training and development, Flexible working opportunities and others.
Show more
Show less","Data investigation, Data standards, SSMS, SSIS, SQL, TSQL, Stored Procedures, High attention to detail, Strong communication skills, Structured problemsolving techniques, Performance tuning, ETL, ELT, Software delivery methods, Python, DevOps, Azure Devops, Azure Data Factory, Azure Data Bricks, Azure SQL, Data Engineering, Data analytics, BI systems, Corporate data requirements, Data structures, Climatic Change data legislation, Environmental related industries, Presentation skills","data investigation, data standards, ssms, ssis, sql, tsql, stored procedures, high attention to detail, strong communication skills, structured problemsolving techniques, performance tuning, etl, elt, software delivery methods, python, devops, azure devops, azure data factory, azure data bricks, azure sql, data engineering, data analytics, bi systems, corporate data requirements, data structures, climatic change data legislation, environmental related industries, presentation skills","azure data bricks, azure data factory, azure devops, azure sql, bi systems, climatic change data legislation, corporate data requirements, data engineering, data investigation, data standards, data structures, dataanalytics, devops, elt, environmental related industries, etl, high attention to detail, performance tuning, presentation skills, python, software delivery methods, sql, ssis, ssms, stored procedures, strong communication skills, structured problemsolving techniques, tsql"
Junior Data Engineer / Data Analyst,Energy Jobline,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-data-analyst-at-energy-jobline-3772913751,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Hybrid / London Office
*This client is not able to sponsor potential candidates*
Location: London, United Kingdom
Our client is a successful VC firm who invest in start-ups with innovative technologies and business models that can accelerate digitization and efficiency.
Position Overview
They are seeking a detail-oriented, analytical, and curious Data Analyst / Data Engineer with a broad technical background to join their data and research team.
The ideal candidate will have strong technical data engineering and analytics skills, either from their educational background or commercial experience.
Key Responsibilities
Acquire and process data from primary and secondary data sources and maintain databases/data systems.
Monitor and audit data quality.
Identify, analyze, and interpret trends or patterns in complex data sets by applying ML and NLP models.
Collaborate with investment and research teams to prioritize business and information needs.
Assist in the design and delivery of dashboards and data-driven insights to stakeholders.
Participate in the evaluation of new data analytics tools and platforms.
Develop productivity tools for internal teams to engage with the data.
Mandatory Qualifications/Skills
MASTERS DEGREE IN DATA SCIENCE OR COMPUTER SCIENCE OR SIMILAR.
Ideally 2 years of experience in data analysis or computer science related field.
Strong knowledge of and experience with databases (SQL, NoSQL, etc.), data structures (XML, JSON), and programming (e.g., Python, graph libraries such as Plotly, ETL frameworks, software engineering, ML libraries, APIs).
Experience with data visualization tools, such as Tableau, Power BI, or similar.
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Up to date with latest technological innovation.
Adept at queries, report writing, and presenting findings.
Preferred Qualifications/Skills
Familiarity with Microsoft Azure cloud computing and security infrastructure.
Using a version management system such as GitHub.
Familiarity with venture capital, other financial data, or have prior experience in the venture ecosystem.
Soft Skills
Excellent written and verbal communication skills.
Curiosity and a drive to learn and master new technologies, financial concepts, and techniques.
Demonstrated ability to work collaboratively within a team and with stakeholders across the organization.
Problem-solving aptitude.
Ability to manage multiple tasks and projects simultaneously
Show more
Show less","Data Analytics, Data Engineering, Machine Learning, Natural Language Processing, Databases (SQL NoSQL), Data Structures (XML JSON), Programming (Python Plotly), ETL Frameworks, Software Engineering, ML Libraries, APIs, Data Visualization Tools (Tableau Power BI), Microsoft Azure, GitHub, Venture Capital, Financial Data","data analytics, data engineering, machine learning, natural language processing, databases sql nosql, data structures xml json, programming python plotly, etl frameworks, software engineering, ml libraries, apis, data visualization tools tableau power bi, microsoft azure, github, venture capital, financial data","apis, data engineering, data structures xml json, data visualization tools tableau power bi, dataanalytics, databases sql nosql, etl frameworks, financial data, github, machine learning, microsoft azure, ml libraries, natural language processing, programming python plotly, software engineering, venture capital"
Data Engineer,The People Network,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-the-people-network-3779197303,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Data Engineer
- required on a permanent basis for a Financial Service company based in London that uses data and machine learning technologies to help companies prevent financial crime and terrorism.
This will be a hybrid role with 2-3 days a week in the London office and a salary up to £85,000.
Responsibilities:
Contributing production quality code and unit tests to our Data Collection Hub
Contributing improvements to the test and build pipelines
Considering the impact and implications of changes and communicating these clearly
Helping to support the data processing pipelines as needed
Modelling data in the best way for specific business needs
Staying abreast of the latest developments in Data Engineering to contribute to best practices
Tech Stack:
You will be using Python (specifically Pyspark) and Node.js for processing data
You will be using Hadoop stack technologies such as HDFS and HBase
Experience using MongoDB and Elasticsearch for indexing smaller datasets would be beneficial
Experience using Airflow & Nifi to co-ordinate the processing of data would be beneficial
You will be using Ansible to manage configuration and deployments
AWS
Linux
If you are a Data Engineer looking for a new challenge then please APPLY NOW for immediate consideration.
Show more
Show less","Python, Pyspark, Node.js, Hadoop, HDFS, HBase, MongoDB, Elasticsearch, Airflow, Nifi, Ansible, AWS, Linux","python, pyspark, nodejs, hadoop, hdfs, hbase, mongodb, elasticsearch, airflow, nifi, ansible, aws, linux","airflow, ansible, aws, elasticsearch, hadoop, hbase, hdfs, linux, mongodb, nifi, nodejs, python, spark"
Data Engineer,Vector Recruitment Ltd,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-vector-recruitment-ltd-3780717017,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"We are seeking a Data Engineer to work for a fast-growing company in central London. You will be joining a team of over 700 engineers who are at the forefront of technology, developing next generation connected products within consumer electronics and IoT devices.
You will be a highly skilled Data Engineer with experience working with big Data with a solid background in designing, developing, and maintaining data infrastructure. You will perform an important role, working closely with cross-functional teams to build and maintain data pipelines, optimize data workflows, and support data-driven decision-making across the organisation.
In return you will be working for a company which fosters new ideas, encourages personal development and is a true investor in their employees. You will be offered a very competitive salary with excellent benefits and bonus scheme.
Requirements of Data Engineer
Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.
Recent and proven experience as a Data Engineer, with a track record of successfully delivering data projects
Experience and ability in the design, implementation, and maintaining of robust and scalable data pipelines to collect, process, and store data from various sources, ensuring data quality and integrity.
Skilled in the development and maintenance of ETL processes to transform and cleanse data for analytical purposes, while adhering to best practices.
Proficient in data engineering tools and technologies such as Apache Spark, Hadoop, AWS, Azure, GCP, and ETL frameworks.
Strong knowledge of SQL and NoSQL databases, data modeling, and database design.
Programming skills in languages like Python, C++, C#, Java, or Scala.
Experience working with TB to PB scale data.
Familiarity with data quality and data governance best practices.
Strong problem-solving skills and the ability to work in a dynamic, fast-paced environment.
Excellent communication and teamwork skills.
Salary:
£70k - £90k + Excellent Bens and bonus
Location:
Central London
Show more
Show less","Apache Spark, Hadoop, AWS, Azure, GCP, SQL, NoSQL, Python, C++, C#, Java, Scala, Data pipelines, Data quality, Data governance, ETL processes, Data modeling, Database design","apache spark, hadoop, aws, azure, gcp, sql, nosql, python, c, c, java, scala, data pipelines, data quality, data governance, etl processes, data modeling, database design","apache spark, aws, azure, c, data governance, data quality, database design, datamodeling, datapipeline, etl, gcp, hadoop, java, nosql, python, scala, sql"
Data Engineer - 3 Days Per Week Onsite,Energy Jobline,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-3-days-per-week-onsite-at-energy-jobline-3776810309,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Job Title: Data Engineer - 3 Days Per Week Onsite
Location: London/Birmingham/Sunderland (hybrid/3 days per week in the office)
Salary/Rate: Up to £500 per day INSIDE IR35
Start Date: 18/12/2023
Job Type: Contract
Company Introduction
We have an exciting opportunity now available with one of our sector-leading financial services clients! They are currently looking for a skilled Data Engineer to join their team for a six-month contract.
Required Skills/Experience
The ideal candidate will have the following:
Hands on experience with one of the programming languages: Java (DSA problems - String/Array(Medium)), Python (DSA), SQL
Big Data Tech Stack: Spark, General stack
Data Warehousing: Dimensional Modelling, Data Lake/DWH storage design, Snowflake
Cloud: Classic Web services, Athena, S3, IAM, Networking, Redshift
If you are interested in this opportunity, please apply now with your updated CV in Microsoft Word/PDF format.
Disclaimer
Notwithstanding any guidelines given to level of experience sought, we will consider candidates from outside this range if they can demonstrate the necessary competencies.
Square One is acting as both an employment agency and an employment business, and is an equal opportunities recruitment business. Square One embraces diversity and will treat everyone equally. Please see our website for our full diversity statement
Show more
Show less","Java, Python, SQL, Spark, General stack, Dimensional Modelling, Snowflake, Classic Web services, Athena, S3, IAM, Networking, Redshift","java, python, sql, spark, general stack, dimensional modelling, snowflake, classic web services, athena, s3, iam, networking, redshift","athena, classic web services, dimensional modelling, general stack, iam, java, networking, python, redshift, s3, snowflake, spark, sql"
Lead Data Engineer,Logidot,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-logidot-3737036790,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"About Logidot:
Logidot is a privately-owned company founded in 2018 and born out of a research programme at Imperial College London, to develop a system for high performance real-time location tracking of mobile assets in industrial warehouses. Logidot is a pioneer and manufacturer of award winning industrial real-time location system (RTLS) and services to help warehouse and factory operators.
Our mission is to bring people, processes, assets and industrial spaces into optimal flow. Logidot creates a digital twin of the warehouse, factory or yard operations by tracking mobile assets (eg vehicles, tools, stock, robots) in realtime with our uniquely high-performance, plug&play and low-cost location tracking sensors. Our Smart Warehouse platform, in turn, gives managers the ability to analyse real-time and historical material flows and optimize the efficiency, safety and sustainability of operations through predictions, locationbased process automation and alerts. For example, increase efficiency by dynamically optimising picking routes, reduce misplaced stock, predict and right-size asset requirements, and improve safety by detecting hazardous behaviours and anomalies to prevent accidents. We are looking for a passionate and talented data engineer to lead the development of our spatial intelligence platform.
Your role:
You will have a strong background in data analysis, machine learning and a proven track record of delivering production software in a collaborative environment. You will be expected to take ownership of our cloud-based data analytics stack, which analyses raw data collected by our hardware (as well as 3rd party) to provide actionable insights to our end users. Your work will involve maintaining the existing codebase as well as extending its functionality in response to feedback from our clients. You will also lead the development of our spatial intelligence platform.
Requirements
Main Requirements:
The ideal candidate is expected to have:
An Msc or PhD in Computer Science, Mathematics, Statistics, or related technical field
Fluency in Python and data analysis libraries (such as numpy, scipy, etc)
Experience with machine learning, optimisation and process mining in Python
Familiarity with MLOps principles and practices, including CI/CD, model versioning, monitoring, and deployment
Experience working with Linux, Docker, AWS, Kafka and git
Adopt best practices for software development, testing, and deployment, as well as MLOps and Cloud computing principles
The ability to think and work independently
Superb team working and communication skills (in English)
A track record of successful projects
A passion for technology and a desire to push the boundaries (not looking for a 9/5 job)
Bonus Requirements:
Strong knowledge of cloud computing platforms (e.g., AWS, Azure, GCP) and experience building cloud-based machine learning systems
Experience with dynamic routing, scheduling problems and constrained optimisation techniques
Familiarity with ERP/WMS software
Benefits
While the role is mostly remote, the candidate has to be UK based and is expected to travel to events and client sites approximately on a monthly basis. He/she will be remunerated with a competitive base salary as well as a significant stake of options in the company.
Show more
Show less","Python, NumPy, SciPy, Machine learning, Optimization, Process mining, MLOps, CI/CD, Model versioning, Monitoring, Deployment, Linux, Docker, AWS, Kafka, Git, Software development, Testing, Deployment, Cloud computing, Cloudbased machine learning systems, Dynamic routing, Scheduling problems, Constrained optimization techniques, ERP/WMS software","python, numpy, scipy, machine learning, optimization, process mining, mlops, cicd, model versioning, monitoring, deployment, linux, docker, aws, kafka, git, software development, testing, deployment, cloud computing, cloudbased machine learning systems, dynamic routing, scheduling problems, constrained optimization techniques, erpwms software","aws, cicd, cloud computing, cloudbased machine learning systems, constrained optimization techniques, deployment, docker, dynamic routing, erpwms software, git, kafka, linux, machine learning, mlops, model versioning, monitoring, numpy, optimization, process mining, python, scheduling problems, scipy, software development, testing"
Principal Data Engineer,Cabinet Office,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/principal-data-engineer-at-cabinet-office-3787003970,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Location
Bristol, Glasgow, London, Manchester, York. We embrace flexible working. There will be a requirement for office working in line with new civil service guidelines. We are happy to discuss requirements at interview. If London is not your base location, we would expect you to come into the London office 1-2 times a month.
About The Job
Job summary
Key focus on building data pipelines that enable data from across government to be ingested and transformed to enable cutting-edge AI initiatives to leverage these data holdings
About The Team
The Incubator for Artificial Intelligence (i.AI) - is an AI for public good programme to harness the opportunities presented by AI to improve the lives of citizens. It will focus on a number of priority projects in home affairs, health, education and government efficiency projects. The i.AI team will use ethical and secure methods at all times to help deliver better public services.
Building on the work of No10 Data Science Team (10DS) and other leading technology teams across the civil service i.AI will experiment and prove what is possible in improving the use of AI across the government.
You can see more about our work on ai.gov.uk
Job Description
Role Responsibilities
Lead the strategic design and build of Data pipelines for the ingestion and transformation of data from across government to support the wider AI initiatives
You will be using and developing Data Engineering capability provided within the AWS environment.
You will be required to review, document and streamline current datasets and ETL processes to integrate systems and bring multiple data sources together for analysis.
You will deliver capability using an appropriate structured process. Agile and DevOps experience would be an advantage.
You will be acquiring data sets from multiple sources at varying levels of maturity. You will curate and catalogue this data in partnership with the Data Scientists and analysts. Solutions will be developed using the Gov PaaS environment as well as AWS. You will be using and developing Data Engineering capability provided within the AWS environment. There is extensive use of infrastructure as code within a structured development.
There is extensive use of infrastructure as code within a structured development environment. You will lead on delivering capability using an appropriate structured process. Agile and DevOps experience would be an advantage.
You can produce data models and understand where to use different types of data models. You have experience of a wide range of data engineering tools. You understand industry-recognised data-modelling patterns and standards.
Further Information About The Role
Do you have a question about the roles or you want to learn more about the team, the team will be holding a meet the team event on Tuesday 28th November 13.00-14.00. You can join on https://meet.google.com/zda-mvke-mvh
Person specification
Senior of experience in infrastructure engineering, DevOps or large scale data platforms.
Strong coding background in Python or similar and strong SQL skills
Experience in building data pipelines (ETL and/or analytical pipelines)
ETL Pipeline development experience using Lambda, Glue, S3, Athena, SQL, EC2, Cloudwatch, EventBridge, Redshift, Sagemaker, IAM
Expertise in Data Modelling
Experience in cleansing, managing and transforming high volume data
Strong understanding of the importance Data Observability and how to implement the same
We welcome a broad range of applicants and encourage you to apply. Strong candidates come from many different backgrounds. Studies show that talented people, especially those from groups underrepresented in their field, are more likely to doubt themselves and feel like an ""imposter"". Unique perspectives enrich teams, so we urge you to have confidence in your potential contributions. If aspects of this role resonate with you, please apply. We look forward to your application highlighting your skills and experiences
Behaviours
We'll assess you against these behaviours during the selection process:
Working Together
Seeing the Big Picture
Delivering at Pace
Making Effective Decisions
Communicating and Influencing
We only ask for evidence of these behaviours on your application form:
Working Together
Technical Skills
We'll assess you against these technical skills during the selection process:
Technical assessment at interview will be based on the listed essential criteria and will be confirmed to candidates at shortlisting stage
Benefits
Alongside your salary of £90,000, Cabinet Office contributes £24,300 towards you being a member of the Civil Service Defined Benefit Pension scheme. Find out what benefits a Civil Service Pension provides.
Learning and development tailored to your role.
An environment with flexible working options.
A culture encouraging inclusion and diversity.
A Civil Service Pension which provides an attractive pension, benefits for dependants and average employer contributions of 27%.
A minimum of 25 days of paid annual leave, increasing by one day per year up to a maximum of 30.
Things you need to know
Selection process details
This vacancy is using Success Profiles (opens in a new window) , and will assess your Behaviours, Experience and Technical skills.
To submit an application, you will need to provide an up to date CV. We would also like you to submit a 500 word personal statement. Please ensure your CV and personal statement demonstrates how you meet the following criteria:
Senior of experience in infrastructure engineering, DevOps or large scale data platforms.
Strong coding background in Python or similar and strong SQL skills
Experience in building data pipelines (ETL and/or analytical pipelines)
ETL Pipeline development experience using Lambda, Glue, S3, Athena, SQL, EC2, Cloudwatch, EventBridge, Redshift, Sagemaker, IAM
Expertise in Data Modelling
Experience in cleansing, managing and transforming high volume data
Strong understanding of the importance Data Observability and how to implement the same
Should a large number of applications be received, an initial sift may be undertaken using the lead Behaviour,
Working Together.
Assessment and interview process
Your application will be reviewed by a panel. If you have been shortlisted, you will be invited to complete a technical assessment.
Technical assessment
Candidates will be sent through a problem and asked to solve it. You will present your solution technical panel for 30 minutes. We are looking to understand your thought process and hands-on experience.
If you pass the technical assessment, the final stage will be an interview where you will be asked questions on behaviours:
Working together
Seeing the Big Picture,
Delivering at pace
Making Effective Decisions and
Communicating & Influencing
Candidates that do not pass the interview but have demonstrated an acceptable standard may be considered for similar roles at a lower grade.
Depending on the amount of application we get there might be an additional technical assessment.
Expected timeline (subject to change)
Expected sift date – 11th December 2023
Expected tech assessment - 18thDecember -1st January 2024
Expected interview date/s –8th January 2024
Reasonable Adjustment
If a person with disabilities is put at a substantial disadvantage compared to a non-disabled person, we have a duty to make reasonable changes to our processes.
If you need a change to be made so that you can make your application, you should:
Contact Government Recruitment Service via cabinetofficerecruitment.grs@cabinetoffice.gov.uk as soon as possible before the closing date to discuss your needs.
Complete the ‘Assistance required’ section in the ‘Additional requirements’ page of your application form to tell us what changes or help you might need further on in the recruitment process. For instance, you may need wheelchair access at interview, or if you’re deaf, a Language Service Professional.
Further information
If you are experiencing accessibility problems with any attachments on this advert, please contact the email address in the 'contact point for applicants' section.
Please note that this role requires SC clearance, which would normally need 5 years’ UK residency in the past 5 years. This is not an absolute requirement, but supplementary checks may be needed where individuals have not lived in the UK for that period. This may mean your security clearance (and therefore your appointment) will take longer or, in some cases, not be possible.
Please note terms and conditions are attached. Please take time to read the document to determine how these may affect you.
Any move to Cabinet Office from another employer will mean you can no longer access childcare vouchers. This includes moves between government departments. You may however be eligible for other government schemes, including Tax Free Childcare. Determine your eligibility at: https://www.childcarechoices.gov.uk.
A reserve list will be held for a period of 12 months, from which further appointments can be made.
If successful and transferring from another Government Department a criminal record check may be carried out.
In order to process applications without delay, we will be sending a Criminal Record Check to Disclosure and Barring Service/Disclosure Scotland on your behalf.
However, we recognise in exceptional circumstances some candidates will want to send their completed forms direct. If you will be doing this, please advise Government Recruitment Service of your intention by emailing Pre-EmploymentChecks.grs@cabinetoffice.gov.uk stating the job reference number in the subject heading.
For further information on the Disclosure Scotland confidential checking service telephone: the Disclosure Scotland Helpline on 0870 609 6006 and ask to speak to the operations manager in confidence, or email Info@disclosurescotland.co.uk
New entrants are expected to join on the minimum of the pay band.
Applicants who are successful at interview will be, as part of pre-employment screening, subject to a check on the Internal Fraud Database (IFD). This check will provide information about employees who have been dismissed for fraud or dishonesty offences. This check also applies to employees who resign or otherwise leave before being dismissed for fraud or dishonesty had their employment continued. Any applicant’s details held on the IFD will be refused employment.
A candidate is not eligible to apply for a role within the Civil Service if the application is made within a 5 year period following a dismissal for carrying out internal fraud against government.
Existing Civil Servants and applicants from accredited NDPBs are eligible to apply, but will only be considered on loan basis (Civil Servants) or secondment (accredited NDPBs). Prior agreement to be released on a loan basis must be obtained before commencing the application process. In the case of Civil Servants, the terms of the loan will be agreed between the home and host department and the Civil Servant. This includes grade on return.
Please note, these roles are for Cabinet Office. The GDS Recruitment team is supporting with the recruitment process.
Feedback will only be provided if you attend an interview or assessment.
Security
Successful candidates must undergo a criminal record check.
Successful candidates must meet the security requirements before they can be appointed. The level of security needed is security check (opens in a new window) . See our vetting charter (opens in a new window) .
People working with government assets must complete baseline personnel security standard (opens in new window) checks.
Nationality Requirements
This job is broadly open to the following groups:
UK nationals
nationals of the Republic of Ireland
nationals of Commonwealth countries who have the right to work in the UK
nationals of the EU, Switzerland, Norway, Iceland or Liechtenstein and family members of those nationalities with settled or pre-settled status under the European Union Settlement Scheme (EUSS) (opens in a new window)
nationals of the EU, Switzerland, Norway, Iceland or Liechtenstein and family members of those nationalities who have made a valid application for settled or pre-settled status under the European Union Settlement Scheme (EUSS)
individuals with limited leave to remain or indefinite leave to remain who were eligible to apply for EUSS on or before 31 December 2020
Turkish nationals, and certain family members of Turkish nationals, who have accrued the right to work in the Civil Service
Further information on nationality requirements (opens in a new window)
Working for the Civil Service
The Civil Service Code (opens in a new window) sets out the standards of behaviour expected of civil servants.
We recruit by merit on the basis of fair and open competition, as outlined in the Civil Service Commission's recruitment principles (opens in a new window) .
The Civil Service embraces diversity and promotes equal opportunities. As such, we run a Disability Confident Scheme (DCS) for candidates with disabilities who meet the minimum selection criteria.
The Civil Service also offers a Redeployment Interview Scheme to civil servants who are at risk of redundancy, and who meet the minimum requirements for the advertised vacancy.
Apply and further information
This vacancy is part of the Great Place to Work for Veterans (opens in a new window) initiative.
Once this job has closed, the job advert will no longer be available. You may want to save a copy for your records.
Contact point for applicants
Job contact :
Name : GDS Recruitment Team
Email : gds-recruitment@digital.cabinet-office.gov.uk
Recruitment team
Email : cabinetofficerecruitment.grs@cabinetoffice.gov.uk
Further information
Appointment to the Civil Service is governed by the Civil Service Commission’s Recruitment Principles.
If you feel that your application has not been treated in accordance with the Recruitment Principles, and wish to make a complaint, then in the first instance you should contact Government Recruitment Service at: cabinetofficerecruitment.grs@cabinetoffice.gov.uk.
If you are not satisfied with the response that you receive, then you can contact the Civil Service Commission at: info@csc.gov.uk.
For further information on the Recruitment Principles, and bringing a complaint to the Civil Service Commission, please visit their website at: https://civilservicecommission.independent.gov.uk.
Show more
Show less","Python, SQL, AWS, Redshift, Athena, Glue, Lambda, Cloudwatch, EventBridge, IAM, S3, ETL, Dev Ops, Data Modeling, Data Observability, Data Pipelines, Data Transformation","python, sql, aws, redshift, athena, glue, lambda, cloudwatch, eventbridge, iam, s3, etl, dev ops, data modeling, data observability, data pipelines, data transformation","athena, aws, cloudwatch, data observability, data transformation, datamodeling, datapipeline, dev ops, etl, eventbridge, glue, iam, lambda, python, redshift, s3, sql"
Lead Data Engineer,Cabinet Office,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-cabinet-office-3772391910,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Location
Bristol, Glasgow, London, Manchester, York. We embrace flexible working. There will be a requirement for office working in line with new civil service guidelines. We are happy to discuss requirements at interview. If London is not your base location, we would expect you to come into the London office 1-2 times a month.
About The Job
Job summary
Key focus on building data pipelines that enable data from across government to be ingested and transformed to enable cutting-edge AI initiatives to leverage these data holdings
About The Team
The Incubator for Artificial Intelligence (i.AI) - is an AI for public good programme to harness the opportunities presented by AI to improve the lives of citizens. It will focus on a number of priority projects in home affairs, health, education and government efficiency projects. The i.AI team will use ethical and secure methods at all times to help deliver better public services.
Building on the work of No10 Data Science Team (10DS) and other leading technology teams across the civil service i.AI will experiment and prove what is possible in improving the use of AI across the government.
You can see more about our work on ai.gov.uk
Job Description
Role Responsibilities
Have a key focus on building Data pipelines for the ingestion and transformation of data from across government to support the wider AI initiatives
You will be using and developing Data Engineering capability provided within the AWS environment.
You will be required to review, document and streamline current datasets and ETL processes to integrate systems and bring multiple data sources together for analysis.
You will deliver capability using an appropriate structured process. Agile and DevOps experience would be an advantage.
You will be acquiring data sets from multiple sources at varying levels of maturity. You will curate and catalogue this data in partnership with the Data Scientists and analysts. Solutions will be developed using the Gov PaaS environment as well as AWS. You will be using and developing Data Engineering capability provided within the AWS environment. There is extensive use of infrastructure as code within a structured development.
There is extensive use of infrastructure as code within a structured development environment. You will deliver capability using an appropriate structured process. Agile and DevOps experience would be an advantage.
You can produce data models and understand where to use different types of data models. You have experience of a wide range of data engineering tools. You understand industry-recognised data-modelling patterns and standards.
Further Information About The Role
Do you have a question about the roles or you want to learn more about the team, the team will be holding a meet the team event on Tuesday 28th November 13.00-14.00. You can join on https://meet.google.com/zda-mvke-mvh
Person specification
Senior of experience in infrastructure engineering, DevOps or large scale data platforms.
Strong coding background in Python or similar and strong SQL skills
Experience in building data pipelines (ETL and/or analytical pipelines)
ETL Pipeline development experience using Lambda, Glue, S3, Athena, SQL, EC2, Cloudwatch, EventBridge, Redshift, Sagemaker, IAM
Expertise in Data Modelling
Experience in cleansing, managing and transforming high volume data
Strong understanding of the importance Data Observability and how to implement the same
We welcome a broad range of applicants and encourage you to apply. Strong candidates come from many different backgrounds. Studies show that talented people, especially those from groups underrepresented in their field, are more likely to doubt themselves and feel like an ""imposter"". Unique perspectives enrich teams, so we urge you to have confidence in your potential contributions. If aspects of this role resonate with you, please apply. We look forward to your application highlighting your skills and experiences
Behaviours
We'll assess you against these behaviours during the selection process:
Working Together
Seeing the Big Picture
Delivering at Pace
Making Effective Decisions
Communicating and Influencing
We only ask for evidence of these behaviours on your application form:
Working Together
Technical Skills
We'll assess you against these technical skills during the selection process:
Technical assessment at interview will be based on the listed essential criteria and will be confirmed to candidates at shortlisting stage
Benefits
Alongside your salary of £64,700, Cabinet Office contributes £17,469 towards you being a member of the Civil Service Defined Benefit Pension scheme. Find out what benefits a Civil Service Pension provides.
Learning and development tailored to your role.
An environment with flexible working options.
A culture encouraging inclusion and diversity.
A Civil Service Pension which provides an attractive pension, benefits for dependants and average employer contributions of 27%.
A minimum of 25 days of paid annual leave, increasing by one day per year up to a maximum of 30.
Things you need to know
Selection process details
This vacancy is using Success Profiles (opens in a new window) , and will assess your Behaviours, Experience and Technical skills.
To submit an application, you will need to provide an up to date CV. We would also like you to submit a 500 word personal statement. Please ensure your CV and personal statement demonstrates how you meet the following criteria:
Senior of experience in infrastructure engineering, DevOps or large scale data platforms.
Strong coding background in Python or similar and strong SQL skills
Experience in building data pipelines (ETL and/or analytical pipelines)
ETL Pipeline development experience using Lambda, Glue, S3, Athena, SQL, EC2, Cloudwatch, EventBridge, Redshift, Sagemaker, IAM
Expertise in Data Modelling
Experience in cleansing, managing and transforming high volume data
Strong understanding of the importance Data Observability and how to implement the same
Should a large number of applications be received, an initial sift may be undertaken using the lead Behaviour,
Working Together.
Assessment and interview process
Your application will be reviewed by a panel. If you have been shortlisted, you will be invited to complete a technical assessment.
Technical assessment
Candidates will be sent through a problem and asked to solve it. You will present your solution technical panel for 30 minutes. We are looking to understand your thought process and hands-on experience.
If you have passed the technical assessment, the final stage will be an interview where you will be asked questions on behaviours:
Working together
Seeing the Big Picture,
Delivering at pace
Making Effective Decisions and
Communicating & Influencing
Candidates that do not pass the interview but have demonstrated an acceptable standard may be considered for similar roles at a lower grade.
Depending on the amount of application we get there might be an additional technical assessment.
Expected timeline (subject to change)
Expected sift date – 18th December 2023
Technical assessment - 1st-8th January 2024
Expected interview date/s – 15th January 2024
Reasonable Adjustment
If a person with disabilities is put at a substantial disadvantage compared to a non-disabled person, we have a duty to make reasonable changes to our processes.
If you need a change to be made so that you can make your application, you should:
Contact Government Recruitment Service via cabinetofficerecruitment.grs@cabinetoffice.gov.uk as soon as possible before the closing date to discuss your needs.
Complete the ‘Assistance required’ section in the ‘Additional requirements’ page of your application form to tell us what changes or help you might need further on in the recruitment process. For instance, you may need wheelchair access at interview, or if you’re deaf, a Language Service Professional.
Further information
If you are experiencing accessibility problems with any attachments on this advert, please contact the email address in the 'contact point for applicants' section.
Please note that this role requires SC clearance, which would normally need 5 years’ UK residency in the past 5 years. This is not an absolute requirement, but supplementary checks may be needed where individuals have not lived in the UK for that period. This may mean your security clearance (and therefore your appointment) will take longer or, in some cases, not be possible.
Please note terms and conditions are attached. Please take time to read the document to determine how these may affect you.
Any move to Cabinet Office from another employer will mean you can no longer access childcare vouchers. This includes moves between government departments. You may however be eligible for other government schemes, including Tax Free Childcare. Determine your eligibility at: https://www.childcarechoices.gov.uk.
A reserve list will be held for a period of 12 months, from which further appointments can be made.
If successful and transferring from another Government Department a criminal record check may be carried out.
In order to process applications without delay, we will be sending a Criminal Record Check to Disclosure and Barring Service/Disclosure Scotland on your behalf.
However, we recognise in exceptional circumstances some candidates will want to send their completed forms direct. If you will be doing this, please advise Government Recruitment Service of your intention by emailing Pre-EmploymentChecks.grs@cabinetoffice.gov.uk stating the job reference number in the subject heading.
For further information on the Disclosure Scotland confidential checking service telephone: the Disclosure Scotland Helpline on 0870 609 6006 and ask to speak to the operations manager in confidence, or email Info@disclosurescotland.co.uk
New entrants are expected to join on the minimum of the pay band.
Applicants who are successful at interview will be, as part of pre-employment screening, subject to a check on the Internal Fraud Database (IFD). This check will provide information about employees who have been dismissed for fraud or dishonesty offences. This check also applies to employees who resign or otherwise leave before being dismissed for fraud or dishonesty had their employment continued. Any applicant’s details held on the IFD will be refused employment.
A candidate is not eligible to apply for a role within the Civil Service if the application is made within a 5 year period following a dismissal for carrying out internal fraud against government.
Existing Civil Servants and applicants from accredited NDPBs are eligible to apply, but will only be considered on loan basis (Civil Servants) or secondment (accredited NDPBs). Prior agreement to be released on a loan basis must be obtained before commencing the application process. In the case of Civil Servants, the terms of the loan will be agreed between the home and host department and the Civil Servant. This includes grade on return.
Please note, these roles are for Cabinet Office. The GDS Recruitment team is supporting with the recruitment process.
Feedback will only be provided if you attend an interview or assessment.
Security
Successful candidates must undergo a criminal record check.
Successful candidates must meet the security requirements before they can be appointed. The level of security needed is security check (opens in a new window) . See our vetting charter (opens in a new window) .
People working with government assets must complete baseline personnel security standard (opens in new window) checks.
Nationality Requirements
This job is broadly open to the following groups:
UK nationals
nationals of the Republic of Ireland
nationals of Commonwealth countries who have the right to work in the UK
nationals of the EU, Switzerland, Norway, Iceland or Liechtenstein and family members of those nationalities with settled or pre-settled status under the European Union Settlement Scheme (EUSS) (opens in a new window)
nationals of the EU, Switzerland, Norway, Iceland or Liechtenstein and family members of those nationalities who have made a valid application for settled or pre-settled status under the European Union Settlement Scheme (EUSS)
individuals with limited leave to remain or indefinite leave to remain who were eligible to apply for EUSS on or before 31 December 2020
Turkish nationals, and certain family members of Turkish nationals, who have accrued the right to work in the Civil Service
Further information on nationality requirements (opens in a new window)
Working for the Civil Service
The Civil Service Code (opens in a new window) sets out the standards of behaviour expected of civil servants.
We recruit by merit on the basis of fair and open competition, as outlined in the Civil Service Commission's recruitment principles (opens in a new window) .
The Civil Service embraces diversity and promotes equal opportunities. As such, we run a Disability Confident Scheme (DCS) for candidates with disabilities who meet the minimum selection criteria.
The Civil Service also offers a Redeployment Interview Scheme to civil servants who are at risk of redundancy, and who meet the minimum requirements for the advertised vacancy.
Apply and further information
This vacancy is part of the Great Place to Work for Veterans (opens in a new window) initiative.
Once this job has closed, the job advert will no longer be available. You may want to save a copy for your records.
Contact point for applicants
Job contact :
Name : GDS Recruitment
Email : gds-recruitment@digital.cabinet-office.gov.uk
Recruitment team
Email : cabinetofficerecruitment.grs@cabinetoffice.gov.uk
Further information
Appointment to the Civil Service is governed by the Civil Service Commission’s Recruitment Principles.
If you feel that your application has not been treated in accordance with the Recruitment Principles, and wish to make a complaint, then in the first instance you should contact Government Recruitment Service at: cabinetofficerecruitment.grs@cabinetoffice.gov.uk.
If you are not satisfied with the response that you receive, then you can contact the Civil Service Commission at: info@csc.gov.uk.
For further information on the Recruitment Principles, and bringing a complaint to the Civil Service Commission, please visit their website at: https://civilservicecommission.independent.gov.uk.
Show more
Show less","Python, SQL, AWS, Apache Airflow, Athena, Cloudwatch, EC2, EventBridge, Glue, Gov PaaS, IAM, Lambda, Redshift, Sagemaker, S3, Agile, DevOps, ETL Pipelines, Data Observability, Data Modelling, Data Transformation, Data Engineering","python, sql, aws, apache airflow, athena, cloudwatch, ec2, eventbridge, glue, gov paas, iam, lambda, redshift, sagemaker, s3, agile, devops, etl pipelines, data observability, data modelling, data transformation, data engineering","agile, apache airflow, athena, aws, cloudwatch, data engineering, data modelling, data observability, data transformation, devops, ec2, etl pipelines, eventbridge, glue, gov paas, iam, lambda, python, redshift, s3, sagemaker, sql"
Data Engineer Python SQL,Client Server,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-python-sql-at-client-server-3779852309,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Data Engineer (Python SQL) London / WFH to £60k
Are you a tech savvy Data Engineer looking to work on a modern tech stack with daily challenges and learning opportunities?
You could be joining a tech driven real-time market data and research provider for the Energy markets as part of a small collaborative team of software and finance professionals.
As a Data Engineer you'll collaborate with Energy Market Analysts to build the platforms and technology they need; you'll have a broad scope of projects including Data Engineering, building and maintaining ETL pipelines in Python, deploying to GCP and contributing to the overall Data team activities.
You'll be joining a supportive and collaborative team where you'll be encouraged to improve your market knowledge and technical skills.
There's perks such as the latest MacBook Pro, licenses to any software you want to use, training and personal improvement budget, in-house bar open on Fridays and an annual company retreat.
Location / WFH:
You'll join colleagues in the London office three days a week with flexibility to work from home the other two days.
Requirements:
You have strong Python and SQL coding skills
You have experience of building reliable, maintainable data pipelines
You have a good understanding of best practice in data development: documentation, version control, testing and automation
You have strong understanding of mathematics and Data Analysis
You're collaborative with great communication skills
Ideally you will also have experience with GCP and Airflow
What's in it for you?
Up to £60k salary plus bonus and share options
Hybrid working (2 days a week remote)
MacBook Pro upon joining
Free breakfast in the office
Annual companywide retreats
Open in-house bar on Friday's
Learning and development budget
Apply now
or call to find out more about this Data Engineer (Python SQL GCP) opportunity.
At Client Server we believe in a diverse workplace that allows people to play to their strengths and continually learn. We're an equal opportunities employer whose people come from all walks of life and will never discriminate based on race, colour, religion, sex, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The clients we work with share our values.
Show more
Show less","Python, SQL, GCP, Airflow, Data Engineering, ETL pipelines, Data Analysis, Data Development, Documentation, Version Control, Testing, Automation, Mathematics, Communication","python, sql, gcp, airflow, data engineering, etl pipelines, data analysis, data development, documentation, version control, testing, automation, mathematics, communication","airflow, automation, communication, data development, data engineering, dataanalytics, documentation, etl pipelines, gcp, mathematics, python, sql, testing, version control"
Data Center Engineer,Evermore Global,"Slough, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-center-engineer-at-evermore-global-3779208944,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Data Centre Engineer / Shift Work / Hardware / HP Kit / Cabling
Location: Slough
Salary: £35,000 - £37,000 + Shift Allowance
Permanent
Currently recruiting for a leading blue-chip client with a global presence who are looking to hire a data centre engineer ideally with some good hardware skills to join their expanding team. You will need to happy to work day and night shifts (2 days, 2 nights, 4 off – 12 hour shifts) and have worked within a data centre environment previously.
Any experience with Dell or HP kit would be highly desirable.
Please apply for more information on the client and full JD.
Responsibilities / Essential Skills
To receive, respond, resolve and complete service requests and Break / Fix tickets within the agreed SLA timeframe or escalate promptly if they cannot be resolved within an acceptable timely manner.
To receive, assess and complete patching / de-patching requests as well as other cable tasks such as installation and running (both copper and fibre) within the agreed SLA timeframe and to applicable internal, manufacture and industry standards.
To receive, assess and complete IT equipment installation, relocation and decommission requests within the agreed SLA timeframes and to applicable internal, manufacture and industry standards.
Ensure all deployments are installed to applicable internal, manufacture and industry standards.
Provide remote hands problem solving support (server reboots, backups, visual inspections etc).
Cabling experience and knowledge
Installation of rack mount kit including HP and CISCO
Hardware maintenance – swapping of failed customer replaceable parts such as cache battery, HDD, Power Supply.
Highly motivated individual, with a positive & pro-active attitude to work
.
Willingness to make changes to improve operational efficiency through innovation, process and procedures, adopting and adapting ideas and practices from elsewhere.
Ability to act rapidly and logically under pressure and making effective use of others in resolving problems.
Any experience with HP Service Manager 9 highly desirable
Show more
Show less","Data centre engineering, Hardware skills, Dell kit, HP kit, Cabling, Patching, Depatching, IT equipment installation, IT equipment relocation, IT equipment decommissioning, Remote hands problem solving, Rack mount kit installation, Hardware maintenance, HP Service Manager 9","data centre engineering, hardware skills, dell kit, hp kit, cabling, patching, depatching, it equipment installation, it equipment relocation, it equipment decommissioning, remote hands problem solving, rack mount kit installation, hardware maintenance, hp service manager 9","cabling, data centre engineering, dell kit, depatching, hardware maintenance, hardware skills, hp kit, hp service manager 9, it equipment decommissioning, it equipment installation, it equipment relocation, patching, rack mount kit installation, remote hands problem solving"
Senior Data Engineer,Fractal,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-fractal-3786207300,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"It's fun to work in a company where people truly BELIEVE in what they are doing!
We're committed to bringing passion and customer focus to the business.
Client Collaboration: Establish a strong rapport with our client team and actively engage in discussions to comprehend their specific project requirements and goals.
Code Development: Design, develop, and deploy code solutions that align with client needs while ensuring the highest coding standards and best practices.
Timely Delivery: Adhere to project timelines and milestones, consistently delivering high-quality code within the specified time frame.
Problem Solving: Tackle complex technical challenges, troubleshoot issues, and provide innovative solutions to meet client objectives.
Communication: Maintain open and effective communication with both internal teams and clients, ensuring transparency throughout the development process.
Documentation: Create comprehensive documentation for code, processes, and project updates, facilitating knowledge transfer and future maintenance. Mandatory Technical Skills required (Python, Pyspark, Spark and GCP services)
If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us!
Not the right fit? Let us know you're interested in a future opportunity by clicking
Introduce Yourself
in the top-right corner of the page or create an account to set up email alerts as new job postings become available that meet your interest!
Show more
Show less","Python, Pyspark, Spark, GCP services, Client Collaboration, Code Development, Timely Delivery, Problem Solving, Communication, Documentation","python, pyspark, spark, gcp services, client collaboration, code development, timely delivery, problem solving, communication, documentation","client collaboration, code development, communication, documentation, gcp services, problem solving, python, spark, timely delivery"
Lead Data and Analytics Engineer,Alevio Consulting,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-and-analytics-engineer-at-alevio-consulting-3719298580,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Our client is a specialist bank serving the blended financial needs of business owners, property entrepreneurs and family businesses. They put enduring relationships at the heart of what they do. Providing quality products, services and expertise to help their customers manage their interdependent professional and personal financial requirements, whilst embracing the exciting new opportunities available in the industry.
They are looking to add a Lead Data and Analytics engineer to their team, who will perform a pivotal role in helping to build the company’s new data and analytics architecture. Working closely with data architects, other data engineers and business stakeholders, the role offers a chance to shape the D&A landscape, from data modelling to tool selection. This role requires broad technical expertise and hands-on ability.
Responsibilities
Lead the design, development, testing, and maintenance of data analytics systems and tools
Collect, process, and perform statistical analyses on large sets of structured and unstructured data
Build and maintain analytics dashboards and reports that provide meaningful insights to the clients
Develop and maintain ETL pipelines that integrate multiple data sources
Collaborate with cross-functional teams to ensure data consistency and quality
Keep up-to-date with the latest developments in data analytics technologies and techniques
Lead the data team, help coach and guide the data engineers to develop best practices and guardrails
This is a hybrid role based in St Pauls, London, requiring 3 days office attendance.
Requirements
At least 8 years of experience in data analytics and engineering
Strong experience in programming languages such as Python, R, or SQL
Experience of migrating data from on-perm to the Cloud
Proven experience in designing and implementing complex data pipelines using tools such as Apache Beam, Google Dataflow or DBT
Experience with data warehousing, data modelling, and ETL
Experience with data governance tooling - Alation would be advantageous
Demonstrated ability to communicate effectively with technical and non-technical stakeholders
Benefits
Salary: up to £80k
Show more
Show less","Data analytics, Data engineering, Python, R, SQL, Apache Beam, Google Dataflow, DBT, Data warehousing, Data modelling, ETL, Data governance, Alation, Data architecture","data analytics, data engineering, python, r, sql, apache beam, google dataflow, dbt, data warehousing, data modelling, etl, data governance, alation, data architecture","alation, apache beam, data architecture, data engineering, data governance, data modelling, dataanalytics, datawarehouse, dbt, etl, google dataflow, python, r, sql"
Senior Data Analytics Engineer,Benivo,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analytics-engineer-at-benivo-3771512409,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"About:
Benivo is disrupting a 32 billion dollar industry with technology and data. Our HR and Fintech solution is used by tech giants like Microsoft and Google, and large consulting firms like Wipro, CGI and Kearney. Our mission is to transform the mobility industry with smart technology.
Benivo is currently seeking a highly skilled and motivated Data Analytics Engineer to join our team. As a Data Analytics Engineer, you will play a pivotal role in transforming raw data into valuable insights that drive informed decision-making within our organization. You will work with a variety of data sources and technologies, including SQL, Zoho, Azure Data Factory, Power BI, and ETL processes.
Key Responsibilities:
Develop and maintain efficient ETL (Extract, Transform, Load) processes to collect, cleanse, and transform data from multiple sources
Design and implement data models to support analytics and reporting needs
Write complex SQL queries to extract and manipulate data from relational databases
Work with Zoho applications and tools to integrate data and enhance business processes
Build and manage data pipelines using Azure Data Factory to automate data workflows
Create interactive and insightful dashboards and reports using Power BI or similar tools
Ensure data quality, accuracy, and consistency by implementing data governance best practices
Collaborate with squads to understand their data needs and provide analytical solutions
Identify and implement performance improvements in data processing and reporting
Document data pipelines, processes, and reports for knowledge sharing and future reference
Requirements
Bachelor's degree in Computer Science, Data Science, or a related field
Proven track record of at least 7 years in data engineering, data analytics, or a related role
Exceptional proficiency in SQL for data extraction, manipulation, and reporting
Familiarity with Zoho applications and integration
Deep and extensive experience with Azure Data Factory is a must
Proficiency in data visualization tools, particularly Zoho analytics, Zoho CRM, and Power BI
Knowledge of data modeling, warehousing, and database management
Excellent problem-solving skills and attention to detail
Strong communication and collaboration skills
Ability to work independently and in a team-oriented environment
Benefits
Hybrid working: one to two days per week in the office (King’s Cross)
33 days paid annual leave, including bank holidays which are flexible - you may choose to work on bank holidays and take the day off elsewhere in the year
Sponsorship for those who require a work permit
Flexible start-end times (core hours are 10 am - 4 pm)
Learning & development - every Friday afternoon is a half day to focus on your self-development and interests
A yearly generous well-being cash allowance
PayLater cash advance - financial assistance for large personal expenses (e.g moving house, furniture, etc)
Work remotely from 30+ countries for one month per calendar year
Enhanced employer pension contributions of up to 5%
Share options in the employee’s options plan
Company social events and team celebrations
Benivo is an equal-opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
We encourage individuals from all backgrounds to apply and are eager to create a workforce that reflects the diversity of our community.
Show more
Show less","Data Analytics, ETL, SQL, Zoho applications, Azure Data Factory, Power BI, Data visualization, Data modeling, Warehousing, Database management, Problemsolving, Communication, Collaboration","data analytics, etl, sql, zoho applications, azure data factory, power bi, data visualization, data modeling, warehousing, database management, problemsolving, communication, collaboration","azure data factory, collaboration, communication, dataanalytics, database management, datamodeling, datawarehouse, etl, powerbi, problemsolving, sql, visualization, zoho applications"
"Senior Data Engineer (AWS, SQL, Python)",Houseful,"South Bank, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-aws-sql-python-at-houseful-3768317604,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Multiple open positions in Houseful’s Central team and trusted brand Hometrack.
At Houseful, we’re here to help everyone make intelligent decisions about their home
Do the best work of your life!
Houseful is home to trusted brands Zoopla, Alto, Hometrack, Calcasa, Mojo and Prime location. Together were creating the connections that power better property decisions, by unlocking the combined strength of software, data and insight.
We make moves with head and heart to achieve our big ambitions, and to drive progress in the property market. There’s never been a better time to join us.
At Hometrack, we’re redefining the mortgage journey for lenders, brokers and borrowers. We deliver market-leading valuation and risk evaluation services across the property technology and financial technology industries.
Our customers include 9 of the top 10 mortgage providers, as well as many others in financial services. Founded in 1999, we made our name with our Automated Valuation Model (AVM) and now provide more than 50 million automated valuations every year.
We want to make Houseful more welcoming, fair and representative every day. We’ll consider everyone who applies for this role in the same way, regardless of your ethnicity, colour, national origin, religion, sexual orientation, gender, gender identity, age, physical disability, neurodiversity status, family or parental status, or how long you’ve spent unemployed. We’re reimagining the property industry for everyone, so we want our team to represent people from all walks of life. We actively welcome your application if your demographics or background are underrepresented in the technology or property sectors.
We need Senior Data Engineer's to join our team's and help us build our next generation data platform and pipelines, and inspire best practice for data across Houseful.
Your first challenge will be to join the team responsible for building and reimagining the company's core valuation models.
As a senior data engineer you will have an exciting and varied set of responsibilities:
Building out the core data platform on Databricks (writing terraform, deploying infrastructure)
Replatforming some of the companies core datasets migrating them from SQL Server to the Databricks platform (we use DLT to build out robust data pipelines)
Supporting Data Scientists in improving our models by ensuring access to data and supporting in the deployment of models to production
Desirable Tech Skills
Python, Spark
Experience in the Cloud, preferably AWS
Terraform, or similar IaC
SQL, NoSQL
Experience building out data warehouses and building data pipelines
Experience with Data platform solutions: ideally Databricks, or Snowflake
Nice to have: Docker (ECS/Kubernetes) & Orchestration tools (AWS Step Functions, Airflow, Dagster)
We want our new joiners to relate to and champion our
Houseful behaviours
:
Build Together: you collaborate, you support and mentor colleagues
Set the Bar Higher: with your professional experience and personal passion
Know your Audience: you’re driven to solve customer problems
Own It: comfortable in a dynamic environment, with a degree of uncertainty
Re-imagine: comfortable learning new technologies and tools on the job
There’s always room to grow and learn with our roles so please don’t be put off if you don’t have all of these skills and experiences. It’s more important that you’re passionate about our mission to improve the home moving and owning experience for everyone.
Benefits
Everyday Flex - greater flexibility over where and when you work
25 days annual leave + extra days for years of service
Day off for volunteering & Digital detox day
Festive Closure - business closed for period between Christmas and New Year
Cycle to work and electric car schemes
Free Calm App membership
Enhanced Parental leave
Fertility Treatment Financial Support
Group Income Protection and private medical insurance
Gym on-site in London – or membership in regional offices
7.5% pension contribution by the company
Discretionary annual bonus up to 10% of base salary
Talent referral bonus up to £5K
Show more
Show less","Python, Spark, Cloud Computing, AWS, Terraform, InfrastructureasCode (IaC), SQL, NoSQL, Data Warehousing, Data Pipelines, Data Platform Solutions, Databricks, Snowflake, Docker, ECS, Kubernetes, Orchestration Tools, AWS Step Functions, Airflow, Dagster","python, spark, cloud computing, aws, terraform, infrastructureascode iac, sql, nosql, data warehousing, data pipelines, data platform solutions, databricks, snowflake, docker, ecs, kubernetes, orchestration tools, aws step functions, airflow, dagster","airflow, aws, aws step functions, cloud computing, dagster, data platform solutions, databricks, datapipeline, datawarehouse, docker, ecs, infrastructureascode iac, kubernetes, nosql, orchestration tools, python, snowflake, spark, sql, terraform"
Mobile Data Centre Engineer,Excelerate360,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/mobile-data-centre-engineer-at-excelerate360-3724531124,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Our client is a highly successful IT and data centre service provider that operates predominantly with multinational enterprises, data centre operators and channel partners. Most of our client's work is in the South-East however this does extend internationally, into Europe, America, and Asia. Our client has a well-established brand and standing within the industry and a reputation for delivering outstanding services and quality.
Our Client offers a wide range of services, so expect every day to be different with a new challenge to face. They have regular contracted clients who they work with on a weekly basis as well as clients with big projects that can take months to complete.
Responsibilities:
Lifecycle Support - Equipment & Connectivity Auditing, Install, Moves & Changes, Relocation, Decommissioning & Disposal, Patching, Troubleshooting
Managed Data Centre Services - Data Centre Migration, Capacity Management, Rollout & Deployment
Infrastructure - Structured Cabling, Rack Deployment & Aisle Containment, Wireless Networking
Requirements
This is a field-based engineering role which will require you to travel to client sites daily. We are based in Chelmsford, Essex and you would also be required, from time to time, to attend HQ to load/unload materials, for job preparation works and training days.
It is essential that you have worked in data centres and with critical infrastructure and are comfortable and confident in those environments
Able to work unsociable hours and weekends (rota)
Proficient with Microsoft packages, particularly Excel
You will be a practical, hands-on, individual as this is a field-based skilled role that will involve thinking on your feet
Proven experience and exposure of the majority, if not all of the skills below:
Hardware, cable and connection auditing
Hardware/cabling/patching troubleshooting and testing
Data centre IMAC works
Data cable patching and patch schedules
Copper and fibre optic structured cabling installation in data centre and commercial environments with applicable certificates
Office and data centre migrations
Large infrastructure moves and installations of rack, PDU’s, cabling containment
Working to strict timescale and quality standards
CNCI, IPAF, CSCS, IPAF, WAH, PASMA
Full UK Driving licence and experience in driving a van
Benefits
A competitive salary basic starting salary of £30,000 - £40,000 (based on experience),
Pension and life assurance
Flexible hours – 40 hours per week, our work can occur in out-of-hours periods (evenings and weekends)
25 days holiday + bank holidays.
Opportunity for overtime
Additional increased rate of pay for unsociable hours and working away from home.
Company van
Training
Potential to work on projects in overseas data centres
Team away days
Gym membership.
Perkbox
Show more
Show less","Hardware Auditing, Cable Auditing, Connection Auditing, Hardware Troubleshooting, Cabling Troubleshooting, Patching Troubleshooting, Data Centre IMAC, Data Cable Patching, Patch Schedules, Structured Cabling Installation, Copper Cabling Installation, Fibre Optic Cabling Installation, Office Migrations, Data Centre Migrations, Rack Deployment, PDU Installation, Cabling Containment Installation, Microsoft Excel, Microsoft Office Suite, CNCI Certification, IPAF Certification, CSCS Certification, WAH Certification, PASMA Certification","hardware auditing, cable auditing, connection auditing, hardware troubleshooting, cabling troubleshooting, patching troubleshooting, data centre imac, data cable patching, patch schedules, structured cabling installation, copper cabling installation, fibre optic cabling installation, office migrations, data centre migrations, rack deployment, pdu installation, cabling containment installation, microsoft excel, microsoft office suite, cnci certification, ipaf certification, cscs certification, wah certification, pasma certification","cable auditing, cabling containment installation, cabling troubleshooting, cnci certification, connection auditing, copper cabling installation, cscs certification, data cable patching, data centre imac, data centre migrations, fibre optic cabling installation, hardware auditing, hardware troubleshooting, ipaf certification, microsoft excel, microsoft office suite, office migrations, pasma certification, patch schedules, patching troubleshooting, pdu installation, rack deployment, structured cabling installation, wah certification"
Data Production Engineer,Hudson River Trading,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-production-engineer-at-hudson-river-trading-3668462591,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Hudson River Trading (HRT) is looking for a Data Production Engineer to join our Data team. Data is at the core of everything we do at HRT; we excel at deriving deep insights from all types of data, allowing us to achieve consistent success in a dynamic market.
This role is an opportunity to work directly with live trading teams to support one of the largest automated trading systems in the world. You will write automation, explore data, work closely with our research and trading teams, and interact with a variety of external partners such as data providers, brokers, and exchanges. In addition to being a critical part of the trading process, you will have the opportunity to acquire, analyze, and prepare data for quantitative research.
Responsibilities
Data Engineering: Write tools to classify, onboard, and reconcile data. Onboard datasets, explore data, and automate tasks using a modern Python data stack
Data Analysis: Parse, analyze, and understand data sets. Perform data reconciliations, validations, and quality checks. Identify and develop new processes within the data request process to enrich data. Assist our researchers in cleaning and featurizing data
Data Debugging: Find anomalies in derived datasets and trace the issues back to their source. This can include using a mix of deductive reasoning, technical analysis, and communicating with multiple stakeholders in a data pipeline
Production Support: Provide proactive oversight of our data pipeline, handle inquiries from internal customers, and resolve issues under efficient turnaround times
Profile
Track record of being detail-oriented and thorough
You excel in problem solving and researching large datasets to resolve complex issues
You have a collaborative attitude that lends itself to cross-team customers and projects
You thrive in the fast-paced environment of a daily live trading operation
Qualifications
2+ years of experience in a data engineering/science role OR a degree in data science or a similar discipline
Experience in Python strongly preferred
Experience managing ETL pipelines is a plus
Experience with financial datasets (e.g. Refinitiv, S&P, Bloomberg) is a big plus
Comfortable with the Linux command line
Experienced in at least one SQL dialect (PostgreSQL, MSSQL, MYSQL) and able to use others as needed
Able to provide technical support in a production trading environment
Culture
Hudson River Trading (HRT) brings a scientific approach to trading financial products. We have built one of the world's most sophisticated computing environments for research and development. Our researchers are at the forefront of innovation in the world of algorithmic trading.
At HRT we welcome a variety of expertise: mathematics and computer science, physics and engineering, media and tech. We’re a community of self-starters who are motivated by the excitement of being at the cutting edge of automation in every part of our organization—from trading, to business operations, to recruiting and beyond. We value openness and transparency, and celebrate great ideas from HRT veterans and new hires alike. At HRT we’re friends and colleagues – whether we are sharing a meal, playing the latest board game, or writing elegant code. We embrace a culture of togetherness that extends far beyond the walls of our office.
Feel like you belong at HRT? Our goal is to find the best people and bring them together to do great work in a place where everyone is valued. HRT is proud of our diverse staff; we have offices all over the globe and benefit from our varied and unique perspectives. HRT is an equal opportunity employer; so whoever you are we’d love to get to know you.
Show more
Show less","Python, Data engineering, Data analysis, Data debugging, SQL, Linux, ETL pipelines, Financial datasets, Machine learning, Automation, Quantitative research, Data reconciliation, Data validation, Data quality checks, Data cleaning, Feature engineering, Data pipelines, Production support","python, data engineering, data analysis, data debugging, sql, linux, etl pipelines, financial datasets, machine learning, automation, quantitative research, data reconciliation, data validation, data quality checks, data cleaning, feature engineering, data pipelines, production support","automation, data cleaning, data debugging, data engineering, data quality checks, data reconciliation, data validation, dataanalytics, datapipeline, etl pipelines, feature engineering, financial datasets, linux, machine learning, production support, python, quantitative research, sql"
Data Center Engineer,First Point Group,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-center-engineer-at-first-point-group-3782054674,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"FPG seeks an experienced
Data Center Engineer
for a 6-12 month contract. The role covers Logistics, Health and safety, Capacity Management, and Project Installation in Data Centers. Essential skills include managing equipment, ensuring safety compliance, optimizing capacity, and occasional project installations.
Responsibilities:
Logistics: Manage equipment storage and disposal.
Health & Safety: Conduct site inductions, oversee equipment inspection, and manage visitor access.
Project Installation: Assist in technical tasks and ensure compliance with installation criteria.
Conduct a physical inventory audit of BCA and BCTO's DC assets and IT equipment.
Improve methods (automation) for anticipating and updating capacity reports.
Assist the CMDB team in obtaining physical data.
Power data from UPSs, PDUs, and MDUs is regularly recorded.
Keep an eye on the installation material stocks in the workshop and let us know when a new order is necessary.
Requirements:
Ability to multitask under pressure and interact effectively with customers.
Strong focus on safety, continuous improvement, and operational efficiency.
Desirable skills: Strategic capacity management and understanding of IT infrastructure configurations.
Candidates must have valid UK work rights; sponsorship unavailable. This is a great opportunity for a versatile Data Center Professional within the media broadcast industry.
Show more
Show less","Logistics, Health & Safety, Capacity Management, Project Installation, Equipment Management, Safety Compliance, Project Management, Inventory Audit, Automation, Capacity Reporting, CMDB, Data Recording, Installation Material Management, Multitasking, Customer Interaction, Safety Focus, Continuous Improvement, Operational Efficiency, Strategic Capacity Management, IT Infrastructure Configurations","logistics, health safety, capacity management, project installation, equipment management, safety compliance, project management, inventory audit, automation, capacity reporting, cmdb, data recording, installation material management, multitasking, customer interaction, safety focus, continuous improvement, operational efficiency, strategic capacity management, it infrastructure configurations","automation, capacity management, capacity reporting, cmdb, continuous improvement, customer interaction, data recording, equipment management, health safety, installation material management, inventory audit, it infrastructure configurations, logistics, multitasking, operational efficiency, project installation, project management, safety compliance, safety focus, strategic capacity management"
Senior Data Engineer,The People Network,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-the-people-network-3780396525,2023-12-17,Greater London, United Kingdom,Mid senior,Onsite,"Senior DataOps Engineer
Are you passionate about data and looking for a challenging and stimulating career in one of the most exciting and innovative Consultancies in the UK?
We are looking for an experienced DataOps Engineer to join in on a large-scale government project, this is a hybrid role where the expectation is to be in the offices once a week in central London.
The successful DataOps Engineer will have/be:
At least 8 years of hands on experience in data engineering projects
Good hands on experience of designing, implementing, debugging ETL pipeline
Expertise in Python and SQL languages
Experience of designing data pipeline using cloud native services on AWS
Extensive knowledge of AWS services like API Gateway, Lambda, Redshift, Glue, Cloudwatch, etc.
Experience of configuring & fine tuning of AWS resources using IaC like terraform
Hand-on experience of setting up CI/CD workflows in GitHub
The successful candidate can expect a salary between £70,000 - £75,000 (dependant on experience), as well as an excellent benefits package.
Apply NOW if you are an ambitious Senior DataOps Engineer who is looking for a challenging role where you can make a real impact and progress through the organization!
Show more
Show less","DataOps, ETL, Python, SQL, AWS, API Gateway, Lambda, Redshift, Glue, Cloudwatch, Terraform, GitHub, CI/CD","dataops, etl, python, sql, aws, api gateway, lambda, redshift, glue, cloudwatch, terraform, github, cicd","api gateway, aws, cicd, cloudwatch, dataops, etl, github, glue, lambda, python, redshift, sql, terraform"
Senior Data Engineer,Xiatech,"Fitzrovia, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-xiatech-3695001259,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"About Us
Here at Xiatech, we believe that a great work-life balance leads to a contented and productive team. We enable all our teams to work remotely in order that they enjoy the flexibility and freedom to work at a time and place convenient to their lifestyle. Of course, when it comes to the serious stuff of doing business, everyone at Xiatech makes themselves available and delivers to a high standard.
Using the latest in Open Source, Microservices, Big Data and Cloud technologies, Xiatech has developed Xfuze, an innovative Hyper-Integration Platform.
Xfuze (Hyper-Integration Platform - Xiatech) provides organisations with three core capabilities in a single platform:
Real-time system integration
Real-time, single view of data
Real-time, actionable insights, including predictive analytics
Job Purpose
This role is an opportunity to join an established UK-based Software as a Service organisation who believe in using the latest leading-edge technology to drive business growth.
The Senior Data Engineer is responsible for data engineering features and functionality of our Xfuze integration and Analytic Platforms, including involvement in the roadmap and prioritisation of new requirements.
They will work closely with the client business teams, the Xfuze project team, the stakeholders and their peers in the data engineering team, ensuring all requirements and views are represented in delivering the final product. This includes all elements of the platform, including the integrations, data flows, dashboards, user interface, back office tooling and third-party plug-ins.
You will be working with the very latest technologies from our partners at Amazon, Google, DataBricks, BigEye, Tableau, etc. and will be required to have prior experience in developing cutting edge, big data applications.
Key Responsibilities
Enhance and grow our Xfuze platform, our suite of analytics and data products
Integrate Xfuze into our clients’ system landscapes.
Develop complex data flows with Google BigQuery, SQL, Dataform, Cloud Functions and other technologies associated with GCP and AWS
Analyse client requirements, help them to specify, document, map the data, etc
Work with the CTO, CDO, engineering & data team and wider business to develop features and functionality and achieve success
Take part in the agile process, writing stories and working with the backlog in Jira
Provide industry insight, innovation and relevant expertise to ensure our Analytical products remains commercially competitive and efficient whilst optimising the brand and customer experience
Knowledge & Experience
Client
Customer first approach. Focused on driving deliveries to success
You’ll be required to engage directly with our clients, so a high energetic and dynamic personality is required. You’ll be happy to present and talk to the business
Excellent written and verbal communication skills at all levels, both technical and non-technical
Core technical skills
Experience of working with large scale, high volume, data-centric, applications and infrastructure
Design and deliver complex data pipelines
Experience of developing high quality SQL is essential, particularly with a good understanding of how to build views, materialisations and data flow scripts within a Data Lake / Data Warehouse environment against business requirements.
Strong SQL development background, with at least 5 years’ experience in any SQL syntax etc.
Integration, and ETL experience of Data Warehouse & Data Lake platforms either by scripting or ETL tools
Programming experience - any language, ideally Golang or Python
Demonstrable experience delivering clean well tested code
Analysis & design
Strong analytical skills and ability to undertake large data designs
Previous data modelling and development experience is essential, particularly within BigData.
A strong understanding of Entity Relationship and Dimensional Modelling.
You’ll need to have experience in data analysis and data mapping to understand the data requirements and translate these into code and data structures
Quick prototyping in order to show the value quickly
Cloud, tools and tech stack
Good command of one major cloud, preferably GCP or AWS. Certification is a plus but not mandatory
Good database experience - but not necessarily a DBA
A working experience of Google BigQuery would be desirable (alternatively Snowflake, Redshift or equivalent)
Experience of NoSql document Databases like MongoDB/DynamoDB/Google BigQuery is a plus
Experience with BI tools such as Tableau, QlikView, Power BI, Adobe analytics, Google Analytics, DataStudio, Looker and any other similar applications is desirable
Big data stack such as Spark, Hadoop, FLink or similar would be ideal but not mandatory
CI/CD and modern engineering toolset desirable. Good engineering practices
Team
Well versed into Agile, epics, stories and the methodology
Lead junior members of the team technically. Perform peer reviews, guide and coach
Can work on their own without constant supervision
The senior data engineer will be an integral part of the data engineering team and will be working mostly remotely
The position will be primarily remote based although some clients will require an onsite presence from time to time. We will always discuss the travel requirements with you prior to taking on a new client
There are many opportunities to grow within Xiatech, so if you want a hands-on role, using the latest technology within a rapidly expanding company and you have career aspirations as well, this role would be highly suitable.
Your Personality
Innovative
Energetic and Driven
Resilient
Insightful
Empathic
Creative
Analytical
Capacity to Learn
Starter/finisher
If you would like the opportunity to join an established UK-based Software as a Service organisation who believe in using the latest leading-edge technology to drive business growth, please enquire now by emailing maddie@xiatech.co.uk
If you receive an offer of employment, this will be conditional upon satisfactory completion of a right to work & identity check, a reference check and a basic criminal record check for any unspent convictions.
Xiatech is proud to be an equal opportunity employer and prohibits discrimination and harassment of any kind.
We have an awesome Talent Acquisition Specialist, and exclusive terms signed with a Recruitment Agency. We’re unable to work with other recruiters at this time so please don’t reach out regarding our roles.
Show more
Show less","Microservices, Big Data, Cloud technologies, Open Source, Data Engineering, Software as a Service, Integration, Analytics, Data Warehousing, Data Pipelines, SQL, Data Modelling, Cloud Computing, Google BigQuery, AWS, MongoDB, DynamoDB, Tableau, Power BI, DataStudio, Spark, Hadoop, FLink, Agile, Go, Python","microservices, big data, cloud technologies, open source, data engineering, software as a service, integration, analytics, data warehousing, data pipelines, sql, data modelling, cloud computing, google bigquery, aws, mongodb, dynamodb, tableau, power bi, datastudio, spark, hadoop, flink, agile, go, python","agile, analytics, aws, big data, cloud computing, cloud technologies, data engineering, data modelling, datapipeline, datastudio, datawarehouse, dynamodb, flink, go, google bigquery, hadoop, integration, microservices, mongodb, open source, powerbi, python, software as a service, spark, sql, tableau"
Senior Data Engineer,CUBE,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-cube-3754726096,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"Role:
Senior Data Engineer
Location:
United Kingdom
Recently listed as a ""RegTech Top Performer"" in Market Fintech's RegTech Supplier Performance Report, CUBE is pioneering the development of machine automated compliance.
We are a global RegTech business defining and implementing the gold standard of regulatory intelligence and change for the financial services industry. We deliver our services through a SaaS platform, powered by an innovative combination of AI and proprietary data ontology, to simplify the complex and everchanging world of compliance for our clients.
CUBE is creating the future and are a company rooted in strong values, team spirit and commitment to our customers and wider communities. We serve some of the largest financial institutions globally and are expanding our footprint very fast. As we do so, we are keen for new talent to join us and realize their full potential to grow into leadership positions within the business.
Our Products:
RegPlatform
is an Enterprise grade technology platform that streamlines regulatory change management. It provides firms with a one-stop, continuously maintained inventory of global regulations, with effortless horizon scanning, integration capabilities and workflow management. RegPlatform combines industry leading AI technology with expert validated insights to simplify the complexities of multi-jurisdictional regulatory content.
RegAssure
caters for nimble, lean financial organizations that want assured, seamless compliance without the burden of set-up costs and implementation processes. RegAssure provides fast, automated regulatory intelligence that intuitively knows your business needs and can grow with you.
Role mission:
Establish a strong data architecture in GCP for the backend of AI/ML/DS Services supporting a SaaS product.
Build cost-efficient, low latency data pipelines into and within GCP.
Represent the interests of the RegBrain AI team in the establishment of a global data strategy at CUBE.
Assist ML engineering team to build SOTA applications on structured and unstructured data, at scale, by providing fitted data solutions
Ensure seamless integration of RegBrain’s product features to analytics via CUBE’s data lake.
Continuously educate the ML team regarding best data practices
Ensure reliability, completeness and quality of datasets replicated from internal sources.
Own the data governance needs of RegBrain, educate the ML Engineers on the subject.
Improve and manage ETL/ELT pipelines to the graph database(s) supporting CUBE's Knowledge Graph.
Responsibilities:
Create, orchestrate and monitor strong ETL pipelines for core datasets in the business from on-prem to GCP.
Build monitoring solutions for data quality within GCP, with reporting to management stakeholders.
Support creation, orchestration and monitoring of data pipelines to graph database
Support the establishment of a data catalogue.
Coordinate with the Infrastructure team to ensure seamless creation of needed
solutions.
Manage data lifecycle and operational costs.
What we’re looking for:
Cloud champion. You must have extensive experience in GCP specifically. None of
the secrets of BigQuery, DataFlow, Pub/Sub, Composer etc... elude you. You can
quickly decide which solution is suited to a given use case.
You are at ease with all the tools in GCP: CloudSQLBigQuery,DataFlow/Fusion/Proc, Cloud Composer.
A specialist in their domain. You must be able to identify CUBE/RegBrain's needs, communicate planned solutions efficiently and carry them out.
> You must be able to deal with:
> data transport
> NoSql
> Caching
> Message Buses
> Data cleansing, esp. for textual data.
You not only merely think about data as bytes to be moved around, but as valuable information about the real world.
You are proactive. We will be relying on you to set the standards and raise them. Your work should enable others in the team.
An appetite for leadership. When the team grows from its success, you must be
eager to assist its composition and lead more junior data engineers.
A can-do attitude. As a mid-size company, pragmatism is key. We need to enable
short iteration cycles within the ML team while slowly establishing strong data foundations for its future growth.
You like working remotely as much as in-person
Why Us?
🌍
Globally, we are one of a kind!
CUBE is a well-established player in Regtech (we were around before Regtech was even a thing!), and our category-defining product is used by leading financial institutions around the world (including Revolut, Citi, and HSBC). We cover over 5,000 issuing bodies in 712 jurisdictions across 180 countries. Substantially more than our closest competitors.
🗽
Freedom, flexibility & progression.
We are the market leader within Regtech, and yet we are constantly evolving and you will have a significant influence over how we develop moving forwards. We will help you realise your full potential and grow into leadership positions across CUBE.
🗣️
Internationally collaborative culture
CUBE has more than 300 CUBERs across 11 locations in Europe, the Americas and APAC and we are united by our strong team-oriented culture. You will have the opportunity to work with likeminded colleagues from all over the world, gaining exposure to different cultures and business practices.
🌱
Innovative & meaningful work
We are always exploring new technologies and innovations that can help clients solve complex problems. If you’re someone seeking the opportunity to work with cutting edge technology and work on challenging projects that have a real impact then we want to hear from you!
💻
Work life balance
CUBE is a remote first business, you'll be able to design your home office and choose your own work equipment. Unable to work from home one week, or desperate for in-person interaction with colleagues? No problem—book a room in a coworking space or join one of our department monthly team meets (they’re happening all the time!).
🤝
Diversity, Equity and Inclusion:
We believe in equal opportunities and encourage applicants to apply to our open roles regardless of gender, marital status, race, nationality, ethnicity, religion,
neurodiversity
,
sexual orientation and/or age and are committed to providing a wo
rking environment where
everyone is supported to be their authentic best
selves.
At CUBE we do our best to implement inclusive hiring processes to build a culture where we value diversity. We are a values driven company who care about growth and learning, therefore recognise we still have long way to go from where we want to be in regards to DE&I.
As part of the application, we ask for personal data for our internal diversity and inclusion bench-marking, so that we can tailor our DE&I strategies and identify opportunities to improve as a business.
Show more
Show less","Google Cloud Platform (GCP), BigQuery, DataFlow, Pub/Sub, Composer, CloudSQLBigQuery, DataFlow/Fusion/Proc, Cloud Composer, NoSQL, Caching, Message Buses, Data cleansing, ETL/ELT, Data governance, Machine learning (ML), Artificial intelligence (AI), Data science (DS), Data pipelines, Data architecture, Data quality, Data integration, Data modeling, Data visualization, Data mining, Data analysis","google cloud platform gcp, bigquery, dataflow, pubsub, composer, cloudsqlbigquery, dataflowfusionproc, cloud composer, nosql, caching, message buses, data cleansing, etlelt, data governance, machine learning ml, artificial intelligence ai, data science ds, data pipelines, data architecture, data quality, data integration, data modeling, data visualization, data mining, data analysis","artificial intelligence ai, bigquery, caching, cloud composer, cloudsqlbigquery, composer, data architecture, data governance, data integration, data mining, data quality, data science ds, dataanalytics, datacleaning, dataflow, dataflowfusionproc, datamodeling, datapipeline, etlelt, google cloud platform gcp, machine learning ml, message buses, nosql, pubsub, visualization"
"Senior/Lead Data Engineer - Remote - Salary to £90,000",Jefferson Frank,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-lead-data-engineer-remote-salary-to-%C2%A390-000-at-jefferson-frank-3723941406,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"Senior/Lead Data Engineer - Databricks - Remote - Salary to £90,000
One of the leading names in AI & ML with a keen focus on advanced analytics are currently looking for an experienced Senior/Lead Data Engineer to join the team. If you have exposure to migrations or Databricks within an AWS environment then this is the role for you.
Joining at this time would be an ideal opportunity for those looking to move more into generative AI as well as being given the chance to earn certifications whilst working on quite an exciting project. Although Databricks is more suited in an Azure environment we are specifically looking for that have dealt with Databricks in an AWS environment. Past exposure to PySpark would be beneficial.
Key Experience
AWS (S3, Redshift, Snowflake, Glue, Lambda etc)
CI/CD Processes.
ETL Pipelines.
Build/Design exposure.
Matillion or Informatic experience is a bonus.
What Is On Offer
Access to certifications.
Exposure to some of the latest ML & AI practices.
Competitive Salary.
Bonus structure.
Flexible working arrangements.
Our client is currently in the midst of some exciting collaborations and joining now would see you are the forefront as the big changes begin to happen. If you are someone that is progressive in their career advancement then get in touch.
Please note sponsorship is unavailable at this time.
Call Daniel Cordy on 020 3879 8375 or email d.cordy@jeffersonfrank.com with an updated version of your CV.
Show more
Show less","AWS, S3, Redshift, Snowflake, Glue, Lambda, CI/CD Processes, ETL Pipelines, Build/Design, Matillion, Informatica, PySpark, Databricks, Azure, Machine Learning, Artificial Intelligence, Generative AI","aws, s3, redshift, snowflake, glue, lambda, cicd processes, etl pipelines, builddesign, matillion, informatica, pyspark, databricks, azure, machine learning, artificial intelligence, generative ai","artificial intelligence, aws, azure, builddesign, cicd processes, databricks, etl pipelines, generative ai, glue, informatica, lambda, machine learning, matillion, redshift, s3, snowflake, spark"
"Senior/Lead Data Engineer - Remote - Salary to £90,000",Jefferson Frank,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-lead-data-engineer-remote-salary-to-%C2%A390-000-at-jefferson-frank-3731256239,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"Senior/Lead Data Engineer - Databricks - Remote - Salary to £90,000
One of the leading names in AI & ML with a keen focus on advanced analytics are currently looking for an experienced Senior/Lead Data Engineer to join the team. If you have exposure to migrations or Databricks within an AWS environment then this is the role for you.
Joining at this time would be an ideal opportunity for those looking to move more into generative AI as well as being given the chance to earn certifications whilst working on quite an exciting project. Although Databricks is more suited in an Azure environment we are specifically looking for that have dealt with Databricks in an AWS environment. Past exposure to PySpark would be beneficial.
Key Experience
AWS (S3, Redshift, Snowflake, Glue, Lambda etc)
CI/CD Processes.
ETL Pipelines.
Build/Design exposure.
Matillion or Informatic experience is a bonus.
What Is On Offer
Access to certifications.
Exposure to some of the latest ML & AI practices.
Competitive Salary.
Bonus structure.
Flexible working arrangements.
Our client is currently in the midst of some exciting collaborations and joining now would see you are the forefront as the big changes begin to happen. If you are someone that is progressive in their career advancement then get in touch.
Please note sponsorship is unavailable at this time.
Call Daniel Cordy on 020 3879 8375 or email d.cordy@jeffersonfrank.com with an updated version of your CV.
Show more
Show less","AWS, Redshift, Snowflake, Glue, Lambda, CI/CD Processes., ETL Pipelines., Build/Design exposure., Databricks, PySpark, Matillion, Informatic, ML & AI","aws, redshift, snowflake, glue, lambda, cicd processes, etl pipelines, builddesign exposure, databricks, pyspark, matillion, informatic, ml ai","aws, builddesign exposure, cicd processes, databricks, etl pipelines, glue, informatic, lambda, matillion, ml ai, redshift, snowflake, spark"
Lead Azure Data Engineer,Laminar Projects,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-azure-data-engineer-at-laminar-projects-3776565822,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"​
We are Laminar Projects!
We are a project engineering consultancy and tech startup hybrid! We help engineering companies who work on capital projects optimise how they use data, whilst developing our own digital products.
We do things the Laminar Way, that is, we bring control and clarity to projects by implementing pragmatic data-driven solutions using a mix of latest technology, our own processes, and a big dose of common sense.
Missions (What we are aiming for)
To fully digitise the construction of human civilisation
To create a new type of organisation that enables human flourishing while delivering a huge positive impact on society.
So, at this point you might be asking what makes us different from other opportunities you might be looking at. Here are a few pointers to answer that question:
Our leaders are more like coaches and one of our main objectives is to give 5 years’ worth of learning and development for every 1 year with us.
Gain diverse experience by working on projects of all sizes. You’ll be building data pipelines that for flagship construction projects such as High-Tech manufacturing plants, High-Speed Rail, Hospitals, Nuclear, or Hyperscale data centres for tech giants.
100% of our profits go into R&D projects, including developing our own platform to digitise construction project management.
Fast and transparent progression based on our development matrix. This means that our roles and salaries are purely meritocratic, focusing on capability over experience. So, if you have the skills to hop to the next level, you will get reviewed and promoted, simple as that. And if you don’t have the skills yet, you can learn them with us.
Get to know us more here!
___________________
The Role
We are currently looking for Senior/Lead Azure Data Engineers to join our team. As a dynamic and tech-driven startup, curiosity and ingenuity are two of the main traits that we love about our applicants!
We offer you the opportunity to define, shape and expand your career alongside a cutting-edge team with a unique combination of industry domain and digital expertise which actively allow us to offer solutions to truly address our customer challenges. You will be working in one of the most exciting parts of our business: shape and deliver top notch digital solutions to our customers who struggle implementing these projects themselves.
As an Azure Data Engineer at Laminar, your responsibilities would include:
Build robust, scalable data extraction and loading infrastructure to centralise raw datasets.
Build data pipelines that transform these raw datasets into analytics assets that can be consumed by our BI developers and data scientists.
Design, build, optimise and document analytics tables in our database.
Deliver code and datasets that are robust, well-documented and version-controlled to help our team and clients understand what data is available to them and how it has been transformed.
As a candidate, you should have:
Degree educated in one of the following: Computer Science, Data Science, Engineering, Mathematics, Physics, or equivalent experience.
Azure “ninja”/ “jedi”
4+ years working with SQL: building and optimising analytics tables, building complex queries. Experience with other database technologies is a bonus.
3+ years designing and implementing robust data pipelines in Azure Data Factory, Azure Synapse Analytics, Azure Databricks.
Experience with Python and Azure Functions to glue together data infrastructure.
Strong experience in software engineering fundamentals including testing, version control, agile development and sharing knowledge via code review, mentoring and documentation.
It would also be great if you are/have:
Microsoft Certification: Azure Data Engineer Associate.
AWS experience
Experience building and leading data/ software teams.
Experience building processes at a departmental level.
Experience in implementing products with data scientists.
At Laminar we are dedicated to build a diverse, inclusive and authentic workplace, so if you don’t think you meet all the criteria but still are excited about this role we encourage you to apply anyways! Nobody checks every box and you may be just the right candidate for this or other roles.
___________________
Salary and Benefits
Full time contract, fully flexible hours. (we have offices in London and Lisbon as well, if you prefer!).
Senior Azure Data Engineer salary range will be according to your country of residence. We will discuss this during the hiring process.
Health Insurance and pension schemes would also be provided.
You will be supported to excel in whichever areas you are most interested in! With Laminar, development is unbounded.
___________________
Application
We get hundreds of applications so please ensure
your CV is no more than 2 pages
and include
a cover letter explaining why you want to join us and what you think you bring to the team
- We aren't a giant faceless corporation; we really care about the people who want to join our team so all letters will be read by one of the Directors.
You must be located and have the current right to work in the UK/EU. We can support the transfer of existing UK visas, but cannot sponsor new visas.
In terms of application, we usually start with a screening call to tell you what we are all about in a small, casual conversation. That helps you to know us a bit better and tell us a bit about yourself.
​
Show more
Show less","Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Python, Azure Functions, Agile Development, Version Control, Data Pipelines, SQL, Cloud Computing, Data Engineering, Data Analysis, Software Development, Business Intelligence, Machine Learning, Python, Azure Data Lake Storage, Azure Cosmos DB, Data Warehousing, Data Visualization, Power BI","azure data factory, azure synapse analytics, azure databricks, python, azure functions, agile development, version control, data pipelines, sql, cloud computing, data engineering, data analysis, software development, business intelligence, machine learning, python, azure data lake storage, azure cosmos db, data warehousing, data visualization, power bi","agile development, azure cosmos db, azure data factory, azure data lake storage, azure databricks, azure functions, azure synapse analytics, business intelligence, cloud computing, data engineering, dataanalytics, datapipeline, datawarehouse, machine learning, powerbi, python, software development, sql, version control, visualization"
"Lead Data Engineer (Python, Pyspark) – Remote",Noir,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-python-pyspark-%E2%80%93-remote-at-noir-3782002269,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"Lead
Data Engineer (Python, PySpark) – Remote
(Lead Data Engineer, Team Lead, Technical Lead, Senior Data Engineer, Data Engineer, Python, PySpark, SQL, Big Data, Databricks, R, Machine Learning, AI, Agile, Scrum, TDD, BDD, CI / CD, SOLID principles, Github, Azure DevOps, Jenkins, Terraform, AWS CDK, AWS CloudFormation, Azure, Lead Data Engineer, Team Lead, Technical Lead, Senior Data Engineer, Data Engineer)
Our client is a global innovator and world leader with one of the most recognisable names within technology . They are looking for a Lead Data Engineer with significant Python and PySpark experience as well as management responsibility to run an exceptional Agile engineering team and provide technical and team leadership through coaching and mentorship.
We are seeking a Lead Data Engineer and line manager capable of creating a dynamic and positive environment for your team to excel. This will include coaching your team, working with architects, creating automated tests, instilling a culture of continuous improvement and setting standards for the team. You will be responsible for building a greenfield modern data platform using cutting-edge technologies, architecting big data solutions and developing complex enterprise data ETL and ML pipelines and projections.
The successful candidate will have strong Python, PySpark and SQL experience, possess a clear understanding of databricks, as well as a passion for Data Science (R, Machine Learning and AI). Database experience with SQL and No-SQL – Aurora, MS SQL Server, MySQL is expected, as well as significant Agile and Scrum exposure along with SOLID principles. Continuous Integration tools, Infrastructure as code and strong Cloud Platform knowledge, ideally with AWS is also key.
W e are keen to hear from talented Lead Data Engineer candidates from all backgrounds.
This is a truly amazing opportunity to work for a prestigious brand that will do wonders for your career. They invest heavily in training and career development with unlimited career progression for top performers.
Location:
Remote
Salary:
£75k – £95k + Bonus + Pension + Benefits
To apply for this position please send your CV to Nathan Warner at Noir Consulting.
(Lead Data Engineer, Team Lead, Technical Lead, Senior Data Engineer, Data Engineer, Python, PySpark, SQL, Big Data, Databricks, R, Machine Learning, AI, Agile, Scrum, TDD, BDD, CI / CD, SOLID principles, Github, Azure DevOps, Jenkins, Terraform, AWS CDK, AWS CloudFormation, Azure, Lead Data Engineer, Team Lead, Technical Lead, Senior Data Engineer, Data Engineer)
NOIRUKTECHREC
NOIRUKREC
Show more
Show less","Python, PySpark, SQL, Databricks, R, Machine Learning, AI, Agile, Scrum, TDD, BDD, CI/CD, SOLID principles, GitHub, Azure DevOps, Jenkins, Terraform, AWS CDK, AWS CloudFormation, Aurora, MS SQL Server, MySQL, NoSQL, AWS","python, pyspark, sql, databricks, r, machine learning, ai, agile, scrum, tdd, bdd, cicd, solid principles, github, azure devops, jenkins, terraform, aws cdk, aws cloudformation, aurora, ms sql server, mysql, nosql, aws","agile, ai, aurora, aws, aws cdk, aws cloudformation, azure devops, bdd, cicd, databricks, github, jenkins, machine learning, ms sql server, mysql, nosql, python, r, scrum, solid principles, spark, sql, tdd, terraform"
Senior Data Engineer,SR2 | Socially Responsible Recruitment | Certified B Corporation™,"England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-sr2-socially-responsible-recruitment-certified-b-corporation%E2%84%A2-3773324294,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"Senior Data Engineer – Azure
24 month FTC
Remote - UK
Tech4Good
£60k
Work for a genuine Tech4Good company whose work has a tangible impact on the lives of over 100,000 people each year
Highly autonomous role to lead a brand new data architecture project
Shape and build the entire data ecosystem
Not all Tech4Good companies are created equal, and this not-for-profit business is one of the most purpose driven organisations I’ve ever had the pleasure of supporting. If using your skills to do something meaningful is important to you then you’re going to love this.
Let’s start with the essentials. You MUST have a strong Azure background. This is a non-negotiable as your job is essentially to design and build a brand new enterprise data warehouse. This company wide data transformation project will centralise the current siloed data assets into one visible ecosystem allowing for a much better overview of insights for current projects and databases. In turn, this will allow for highly valuable information to be used to predict the needs of their front-line services.
Naturally, expert Python skills are a given and you will need experience in architecture design, ETL development, and pipeline management and be happy to take a mentorship role for junior colleagues. You will be the SME for all things Azure including the infrastructure, governance, costing, optimisation and performance tuning.
Whilst this is a fixed term contract, it is for 2 years and you will be treated and considered as a core part of the business. You’ll have access to the full range of benefits and importantly, the outstanding learning and development opportunities on offer. The business has access to loads of external resources so if you’d like a mentor to develop a certain part of your skillset then just ask and one will be assigned. This is not something you get in most perm roles, let alone FTCs.
The role is a remote role but you MUST be based in the UK and have the right to work.
If this sounds like a bit of you then reach out to Jamie Forgan at SR2 and I’d be happy to fill you in on the extra details.
Show more
Show less","Azure, Enterprise Data Warehouse, Python, Architecture Design, ETL Development, Pipeline Management, Infrastructure, Governance, Costing, Optimization, Performance Tuning, SQL, SQL Server Integration Services (SSIS), Data Blending, Data Visualization, Machine Learning, Artificial Intelligence","azure, enterprise data warehouse, python, architecture design, etl development, pipeline management, infrastructure, governance, costing, optimization, performance tuning, sql, sql server integration services ssis, data blending, data visualization, machine learning, artificial intelligence","architecture design, artificial intelligence, azure, costing, data blending, enterprise data warehouse, etl development, governance, infrastructure, machine learning, optimization, performance tuning, pipeline management, python, sql, sql server integration services ssis, visualization"
Senior AI Data Engineer (UK REMOTE),Turnitin,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-ai-data-engineer-uk-remote-at-turnitin-3718257764,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"Company Description
100% REMOTE
MUST BE UK BASED
At Turnitin, an AI-centric leader in the educational and research sectors, we've been innovating and promoting academic integrity for over two decades. We have an established reputation for our advanced solutions, utilized by numerous academic institutions, corporations, and publishers worldwide.
Offering remote work as a default arrangement, we honor individual choices, value diversity, and respect local cultures. However, for those who prefer the office environment, we have multiple locations across the globe including Oakland, Dallas, Pittsburgh, Kyiv (Ukraine), Newcastle (UK), and Utrecht (Netherlands). Our team is diverse, but unified by our commitment to significantly impacting the realm of education.
As a Senior Data Engineer at Turnitin, you will be part of a global team of proactive, supportive, and independent professionals, striving to deliver sophisticated, well-structured AI and data systems. Collaborating closely with different teams within Turnitin, you'll integrate AI and data science across our broad suite of products, further enriching learning, teaching, and academic integrity.
Job Description
Your role as a Senior Data Engineer entails a range of responsibilities, necessitating a balanced skillset:
AI Data Engineering: Design, build, operate and deploy real-time data pipelines at scale using AI techniques and best practices. Support Turnitin's AI R&D efforts by applying advanced data warehousing, data science, and data engineering technologies. Aim for automation to enable a faster time-to-market and better reusability of new AI initiatives.
Collaboration: Work in tandem with the AI R&D teams and the Data Platform Team to collect, create, curate and maintain high-quality AI datasets. Ensure alignment of data architecture and data models across different products and platforms.
Innovation: Unearth insights from Turnitin's rich data resources through innovative research and development.
Hands-on Involvement: Engage in data engineering and data science tasks as required to support the team and the projects. Conduct and own external data collection efforts - including state of the art prompt engineering techniques - to support the construction of state of the art AI models.
Communication: Foster clear communication within the team and the organization, and ensure understanding of the company's vision and mission.
Continuous Learning: Keep abreast of new tools and development strategies, bringing innovative recommendations to leadership.
Qualifications
At least 4 years of experience in data engineering, ideally focused on enabling and accelerating AI R&D.
Strong proficiency in Python, Java, and SQL.
Proficiency with Redshift, Hadoop, Elasticsearch, and cloud platforms (AWS, Azure, GCP).
Familiarity interacting with AI frameworks including PyTorch and TensorFlow and AI libraries such as Huggingface and Scikit-Learn.
Experience with Large Language Models (LLMs) and LLM APIs.
Strong problem-solving, analytical, and communication skills, along with the ability to thrive in a fast-paced, collaborative environment.
Desired Qualifications
6+ years of experience in data engineering with a focus on AI and machine learning projects.
Experience in a technical leadership role.
Familiarity with natural language processing (NLP) techniques and tools.
Experience in the education or education technology sectors.
Experience with data visualization and data communications.
Characteristics for Success
As a Senior Data Engineer, you should possess:
A passion for creatively solving complex data problems.
The ability to work collaboratively and cross-functionally.
A continuous learning mindset, always striving to improve your skills and knowledge.
A proven track record of delivering results and ensuring a high level of quality.
Strong written and verbal communication skills.
Curiosity about the problems at hand, the field at large, and the best solutions.
Strong system-level problem-solving skills.
Additional Information
Total Rewards @ Turnitin
Turnitin maintains a Total Rewards package that is competitive within the local job market. People tend to think about their Total Rewards monetarily – solely as regular pay plus bonus or commission. This what they earn in exchange for what they do. However, Turnitin delivers more than just these components. Beyond the intrinsic rewards of making a difference in the lives of educators, administrators, learners and researchers around the world, and thriving in an organization that is free of politics and full of humble, inclusive and collaborative teammates, the extrinsic rewards at Turnitin include generous time off and health and wellness programs that offer choice and flexibility and provide a safety net for the challenges that life presents from time to time. In our Remote-First approach to collaborating, you are also able to work the way that best fits your style and situation – whether that be remote, in one of our offices/rented spaces or hybrid.
Our Mission
is to ensure the integrity of global education and meaningfully improve learning outcomes.
Our Values
underpin everything we do.
Customer Centric
- We realize our mission to ensure integrity and improve learning outcomes by  putting educators and learners at the center of everything we do.
Passion for Learning
- We seek out teammates that are constantly learning and growing and build a workplace which enables them to do so.
Integrity
- We believe integrity is the heartbeat of ExamSoft. It shapes our products, the way we treat each other, and how we work with our customers and vendors.
Action & Ownership
- We have a bias toward action and empower teammates to make decisions.
One Team
- We strive to break down silos, collaborate effectively, and celebrate each other’s successes.
Global Mindset
- We respect local cultures and embrace diversity. We think globally and act locally to maximize our impact on education.
Global Benefits
Flexible/hybrid working
Remote First Culture
Health Care Coverage*
Tuition Reimbursement*
Competitive Paid Time Off
4 Self-Care Days per year
National Holidays*
3 all-company global holidays (Juneteenth + 2 Founder’s Days)
Paid Volunteer Time*
Charitable cContribution Match*
Monthly Wellness Reimbursement/Home Office Equipment*
Access to Modern Health (mental health platform)
Parental Leave*
Retirement Plan with match/contribution*
* varies by country
Turnitin, LLC is committed to the policy that all persons have equal access to its programs, facilities and employment. We strongly encourage applications from people of color, persons with disabilities, women, and the LGBTQ+ community, regardless of age, gender, religion, marital or veterans status.
Show more
Show less","AI Data Engineering, Machine Learning, Data Warehousing, Data Science, Data Engineering, PyTorch, TensorFlow, Huggingface, ScikitLearn, Redshift, Hadoop, Elasticsearch, AWS, Azure, GCP, SQL, Python, Java, Large Language Models, LLM APIs, NLP, Natural Language Processing","ai data engineering, machine learning, data warehousing, data science, data engineering, pytorch, tensorflow, huggingface, scikitlearn, redshift, hadoop, elasticsearch, aws, azure, gcp, sql, python, java, large language models, llm apis, nlp, natural language processing","ai data engineering, aws, azure, data engineering, data science, datawarehouse, elasticsearch, gcp, hadoop, huggingface, java, large language models, llm apis, machine learning, natural language processing, nlp, python, pytorch, redshift, scikitlearn, sql, tensorflow"
Senior Data Engineer,CobbleWeb - The Online Marketplace Experts,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-cobbleweb-the-online-marketplace-experts-3626727395,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"What are we looking for?
At CobbleWeb we do not simply churn out cookie-cutter products. Our clients rely on us to turn their online marketplace ideas into sustainable businesses. That’s why we offer them a custom user-focused approach that increases their opportunities for success substantially.
Based on the Lean Startup Method (used by Google, Airbnb, Uber and Amazon) we follow a BUILD – MEASURE – LEARN process to help our clients establish product-market fit, gain competitive advantages, and grow their businesses.
The golden thread linking each step in our process is
data
. Without it, we cannot help our clients make informed decisions about their target audience, marketing channels, product features and much more.
That’s where you come in. We are looking for an experienced
Data Engineer
who will help us create and manage appropriate metrics models for our clients’ marketplace projects. That includes collaborating with our Business Analyst to identify the right metrics for each project and then collecting, managing, and converting raw data into useful information.
Our ideal candidate understands that the metrics models that we build during the Discovery phase of each project go beyond determining what users are doing; they aim to seek the fundamental reason why things exist at all. Your mission is to help our clients discover their business in a way that will constantly evolve their thinking and their products to realise their ultimate vision.
Your metrics models will help our growth hacking efforts, finding the best way to acquire, activate, retain and convert our clients’ user bases. Using the Pirates Metrics Model to measure and analyse our clients’ website or mobile apps, to help us adjust whatever is necessary to improve performance. You are comfortable building and managing data pipelines for technical metrics (track if the product is working as expected and quickly identify technical problems), as well as UX/UI metrics that help us increase audience engagement.
Current projects that you can expect to work on include Nestify, a fast-growing property management platform. We have been asked to implement performance tracking for their employees (via admin and employee dashboards) and identify new business opportunities (cities to focus on, optimal pricing, etc.)
You will also help us build CobbleWeb’s internal communication system and knowledge base known as Umy. This set of internal tools will support our globally distributed company structure.
What You Will Be Doing
Design, deliver and continuously test data pipelines that will aggregate data into reports.
Collaborate with the team to create innovative proofs-of-concept, pilot projects, minimum viable products, and business cases.
Transform data into valuable insights that inform business decisions, making use of our internal data platforms and applying appropriate analytical techniques.
Help us to understand our users and serve them better through data, conversations, and active research to hear from them directly.
Engineer reliable data pipelines for sourcing, processing, distributing, and storing data in different ways, using data platform infrastructure effectively.
Produce and automate delivery of key metrics and KPIs to the business. In some cases, this will mean simply making data available and in others it will constitute developing full reports for end users.
Monitor usage of data platforms and work with clients to deprecate reports and data sets that are not needed and create a continuous improvement model for the data.
Work with clients to understand data issues, tracing back data lineage and helping the business put appropriate data cleansing and quality processes in place.
Work with stakeholders to define and establish data quality rules, definitions and strategies in line with business strategies and goals.
Monitor and set standards for data quality.
Prioritise data issues.
Job Requirements
BSc or MS in Computer Science or related technical fields. Equivalent work experience will also be considered.
At least 4 years of experience in cloud data engineering roles with a solid understanding of cloud storage and cloud technologies.
A strong coding background in either Python or Scala.
Experience with Cloud SQL and NoSQL.
Excellent SQL skills enabling large scale data transformation and analysis.
Experience of developing relational databases based on SQL and data warehousing technologies.
Knowledge of the pros and cons of various database technologies like Relational, NoSQL, MPP, and Columnar databases
A comprehensive understanding of cloud data warehousing and data transformation (extract, transform and load) processes and supporting technologies such as Google Dataflow, Looker, Amazon Glue, EMR, Azure Data Factory, Data Lake, and other analytics tools.
Expert knowledge of Elasticsearch
Experience in manipulating data through cleansing, parsing, standardising etc, especially in relation to improving data quality and integrity
Proven ability to design Data Models and ETL pipelines that meet the business requirements in the most efficient manner.
You have designed and deployed data pipelines and ETL systems for data-at-scale
focusing on outcomes and continuous learning.
Good data modelling experience to address scale and read/write performance
Previous experience of meeting the visualisation, reporting and analytics needs of key business functions through development of presentation and data models
Experienced in defining and developing data sets, models and cubes.
Knowledge of the emerging technologies that support Business Intelligence, Analytics and Data.
You have a curious level-headed approach to problem solving, with a fine eye for detail and the ability to look at the wider business context to spot opportunities for improvement.
Passionate about data, and unlocking data for the masses
Recommended
Previous experience working with software development companies.
An understanding of the platform economy, especially online marketplaces
Note To Candidates
This is a %100 remote position available to candidates within +/- 3 hours from the UK time zone. Candidates are kindly requested to answer the below screening questions in their cover letter. Your responses will help us better understand your suitability for the role.
Please specify your current country of residence.
Country of Residence:
What is your salary expectation(per year) in £ GBP for this role?
Salary Expectation (GBP):
Show more
Show less","Data Engineering, Lean Startup Method, Data Pipelines, Metrics Models, Pirates Metrics Model, Technical Metrics, UX/UI Metrics, Growth Hacking, Cloud Data Engineering, Cloud Storage, Cloud Technologies, Python, Scala, Cloud SQL, NoSQL, SQL, SQL Databases, Data Warehousing, Relational Databases, NoSQL Databases, MPP Databases, Columnar Databases, Cloud Data Warehousing, Data Transformation, Extract Transform and Load (ETL) Processes, Google Dataflow, Looker, Amazon Glue, EMR, Azure Data Factory, Data Lake, Analytics Tools, Elasticsearch, Data Cleansing, Data Parsing, Data Standardisation, Data Quality, Data Integrity, Data Models, ETL Pipelines, Data Visualisation, Data Reporting, Data Analytics, Presentation Models, Data Sets, Data Cubes, Business Intelligence, Analytics, Data, Problem Solving, Business Context, Software Development, Platform Economy, Online Marketplaces","data engineering, lean startup method, data pipelines, metrics models, pirates metrics model, technical metrics, uxui metrics, growth hacking, cloud data engineering, cloud storage, cloud technologies, python, scala, cloud sql, nosql, sql, sql databases, data warehousing, relational databases, nosql databases, mpp databases, columnar databases, cloud data warehousing, data transformation, extract transform and load etl processes, google dataflow, looker, amazon glue, emr, azure data factory, data lake, analytics tools, elasticsearch, data cleansing, data parsing, data standardisation, data quality, data integrity, data models, etl pipelines, data visualisation, data reporting, data analytics, presentation models, data sets, data cubes, business intelligence, analytics, data, problem solving, business context, software development, platform economy, online marketplaces","amazon glue, analytics, analytics tools, azure data factory, business context, business intelligence, cloud data engineering, cloud data warehousing, cloud sql, cloud storage, cloud technologies, columnar databases, data, data cubes, data engineering, data integrity, data lake, data models, data parsing, data quality, data reporting, data sets, data standardisation, data transformation, data visualisation, dataanalytics, datacleaning, datapipeline, datawarehouse, elasticsearch, emr, etl pipelines, extract transform and load etl processes, google dataflow, growth hacking, lean startup method, looker, metrics models, mpp databases, nosql, nosql databases, online marketplaces, pirates metrics model, platform economy, presentation models, problem solving, python, relational databases, scala, software development, sql, sql databases, technical metrics, uxui metrics"
Senior AI Data Engineer (UK or Poland REMOTE) must reside in the UK or Poland,Jobs for Humanity,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-ai-data-engineer-uk-or-poland-remote-must-reside-in-the-uk-or-poland-at-jobs-for-humanity-3784210659,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"Company Description
Jobs for Humanity is partnering with TurnItIn to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: TurnItIn
Job Description
Company Description
100% REMOTE
MUST BE UK BASED
Turnitin is a leader in the educational and research sectors, using AI technology to promote academic integrity. We have been innovating in this field for over two decades and our solutions are trusted by academic institutions, corporations, and publishers worldwide.
We offer remote work options to honor individual preferences and value diversity. However, we also have office locations in Oakland, Dallas, Pittsburgh, Kyiv (Ukraine), Newcastle (UK), and Utrecht (Netherlands) for those who prefer a traditional office environment. Our team is diverse and united by our goal of making a significant impact in education.
As a Senior Data Engineer at Turnitin, you will join a global team of proactive, supportive, and independent professionals. Your role will involve designing and building AI and data systems to enrich learning, teaching, and academic integrity across our suite of products.
Job Description
Your role as a Senior Data Engineer involves:
Designing and building real-time data pipelines at scale, using AI techniques and best practices
Collaborating with AI R&D teams and the Data Platform Team to collect, curate, and maintain high-quality AI datasets
Unearthing insights from our rich data resources through innovative research and development
Engaging in data engineering and data science tasks as required to support the team and projects
Fostering clear communication within the team and the organization
Keeping up with new tools and development strategies, and making innovative recommendations to leadership
Qualifications
At least 4 years of experience in data engineering, with a focus on AI R&D
Strong proficiency in Python, Java, and SQL
Experience with Redshift, Hadoop, Elasticsearch, and cloud platforms (AWS, Azure, GCP)
Familiarity with AI frameworks including PyTorch and TensorFlow, and AI libraries such as Huggingface and Scikit-Learn
Experience with Large Language Models (LLMs) and LLM APIs
Strong problem-solving, analytical, and communication skills
Desired Qualifications
6+ years of experience in data engineering with a focus on AI and machine learning projects
Experience in a technical leadership role
Familiarity with natural language processing (NLP) techniques and tools
Experience in the education or education technology sectors
Experience with data visualization and data communications
Characteristics for Success
A passion for creatively solving complex data problems
The ability to work collaboratively and cross-functionally
A continuous learning mindset, always striving to improve your skills and knowledge
A proven track record of delivering results and ensuring a high level of quality
Strong written and verbal communication skills
Curiosity about the problems at hand, the field at large, and the best solutions
Strong system-level problem-solving skills
Additional Information
Total Rewards @ Turnitin
At Turnitin, we offer a comprehensive Total Rewards package that goes beyond just regular pay and benefits. In addition to competitive compensation, we provide a supportive and inclusive work environment, generous time off, and health and wellness programs. We embrace a Remote-First approach, allowing you to work in a way that suits your style and situation.
Mission:
Our mission is to ensure the integrity of global education and improve learning outcomes.
Values:
We uphold the following values in everything we do:
Customer Centric: We prioritize educators and learners and put them at the center of our work.
Passion for Learning: We value continuous learning and growth.
Integrity: We believe in honesty, transparency, and ethical behavior.
Action & Ownership: We take initiative and empower our team members to make decisions.
One Team: We collaborate effectively, break down silos, and celebrate each other's successes.
Global Mindset: We respect diversity, think globally, and act locally to make a positive impact on education.
Global Benefits:
Flexible/hybrid working options
Remote-First Culture
Health care coverage*
Tuition reimbursement*
Competitive paid time off
4 self-care days per year
National holidays*
3 all-company global holidays
Paid volunteer time*
Charitable contribution match*
Monthly wellness reimbursement/home office equipment*
Access to mental health platform
Parental leave*
Retirement plan with match/contribution*
Benefits may vary by country.
Turnitin, LLC is committed to equal opportunity for all applicants and employees. We encourage applications from individuals of diverse backgrounds, including people of color, persons with disabilities, women, and the LGBTQ+ community, regardless of age, gender, religion, marital or veterans status.
Show more
Show less","Data Engineering, AI R&D, Python, Java, SQL, Redshift, Hadoop, Elasticsearch, AWS, Azure, GCP, PyTorch, TensorFlow, Huggingface, ScikitLearn, Large Language Models, LLMs, Natural Language Processing (NLP), Data Visualization, Data Communications, SystemLevel ProblemSolving","data engineering, ai rd, python, java, sql, redshift, hadoop, elasticsearch, aws, azure, gcp, pytorch, tensorflow, huggingface, scikitlearn, large language models, llms, natural language processing nlp, data visualization, data communications, systemlevel problemsolving","ai rd, aws, azure, data communications, data engineering, elasticsearch, gcp, hadoop, huggingface, java, large language models, llms, natural language processing nlp, python, pytorch, redshift, scikitlearn, sql, systemlevel problemsolving, tensorflow, visualization"
Senior Database Engineer (UK Remote),Turnitin,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-database-engineer-uk-remote-at-turnitin-3693801217,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"Company Description
MUST BE UK BASED
100% REMOTE
When you join Turnitin, you'll be welcomed into a company that is a recognized innovator in the global education space. For more than 20 years, Turnitin has partnered with educational institutions to promote honesty, consistency, and fairness across all subject areas and assessment types. Over 16,000 academic institutions, publishers, and corporations use our services: Gradescope by Turnitin, iThenticate, Turnitin Feedback Studio, Turnitin Originality, Turnitin Similarity, ExamSoft, and ProctorExam.
Turnitin has offices in Australia, India, Indonesia, Japan, Korea, Mexico, the Netherlands, the Philippines, Ukraine, the United Kingdom, and the United States. Our diverse community of colleagues are all unified by a shared desire to make a difference in education. Come join us, and let's make change together.
Job Description
We are looking for an experienced Senior Data Engineer who thinks in clever ways to solve data problems of scale and load with elegant solutions.  Our team supports several PostgreSQL/SQLServer clusters that contain thousands of databases.  These data stores are up to 4+TB each with hundreds of millions of rows, as well as many on-premises and cloud data storage systems, such as Aurora, Redis, Memcached, Redshift, CockroachDB, DynamoDB, and Cassandra.  As a senior member of this team, you will work closely with our DevOps and Engineering Teams to help us maintain automation and stability in our data stores.
Key Responsibilities
Protect, tune, migrate, and administer On-premises and Cloud Data Stores.
Participate in a 24x7 on-call rotation.
Always perform in a manner that guarantees the Protection, Availability, and Performance of our Global Data Stores.
Be opinionated enough to speak up when you think we could be doing something better than we're doing it now -- and tactful and empathetic enough to communicate this in a way that brings people along instead of distancing them
Qualifications
Passion for data stores and a high sense of ownership while performing critical duties based on senior-level experience in Security, Disaster Recovery, and High Availability.
Ability to have a strong work ethic in a fast-paced environment with multiple priorities that may occasionally change.
Ability to work independently and perform under pressure.
Good interpersonal skills, friendly, and approachable.
Deep Linux experience.Strong SQL skills.
Expert in PostgreSQL/SQLServer tuning and best practices.
AWS experience, including Terraform.
Automated monitoring and alerting of On-premises and Cloud data technologies, such as Aurora, Redshift, Redis, CockroachDB, and Cassandra.
Kubernetes experience.
Working experience with configuration management tools, preferably Puppet and Terraform.
Additional Information
No agency submissions
Our Mission
is to ensure the integrity of global education and meaningfully improve learning outcomes.
Our Values
underpin everything we do.
Customer Centric -
We realize our mission to ensure integrity and improve learning outcomes by  putting educators and learners at the center of everything we do.
Passion for Learning -
We seek out teammates that are constantly learning and growing and build a workplace which enables them to do so.
I
ntegrity -
We believe integrity is the heartbeat of Turnitin. It shapes our products, the way we treat each other, and how we work with our customers and vendors.
Action & Ownership -
We have a bias toward action and empower teammates to make decisions.
One Team -
We strive to break down silos, collaborate effectively, and celebrate each other’s successes.
Global Mindset -
We respect local cultures and embrace diversity. We think globally and act locally to maximize our impact on education.
Global Benefits
Flexible/hybrid working
Remote First Culture
Health Care Coverage*
Tuition Reimbursement*
Competitive Paid Time Off
4 Self-Care Days per year
National Holidays*
2 Founder Days + Juneteenth Observed
Paid Volunteer Time*
Charitable contribution match*
Monthly Wellness Reimbursement/Home Office Equipment*
Access to Modern Health (mental health platform)
Parental Leave*
Retirement Plan with match/contribution*
*
varies by country
Seeing Beyond the Job Ad
At Turnitin, we recognize it’s unrealistic for candidates to fulfill 100% of the criteria in a job ad.  We encourage you to apply if you meet the majority of the requirements because we know that skills evolve over time. If you’re willing to learn and evolve alongside us, join our team!
Turnitin, LLC is committed to the policy that all persons have equal access to its programs, facilities and employment. All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","PostgreSQL, SQLServer, Aurora, Redis, Memcached, Redshift, CockroachDB, DynamoDB, Cassandra, DevOps, Linux, SQL, Terraform, Kubernetes, Puppet, Automated monitoring, Alerting, AWS","postgresql, sqlserver, aurora, redis, memcached, redshift, cockroachdb, dynamodb, cassandra, devops, linux, sql, terraform, kubernetes, puppet, automated monitoring, alerting, aws","alerting, aurora, automated monitoring, aws, cassandra, cockroachdb, devops, dynamodb, kubernetes, linux, memcached, postgresql, puppet, redis, redshift, sql, sqlserver, terraform"
Senior AI Data Engineer (UK REMOTE),Jobs for Humanity,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-ai-data-engineer-uk-remote-at-jobs-for-humanity-3770780929,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"Company Description
Jobs for Humanity is partnering with TurnItIn to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: TurnItIn
Job Description
Company Description
100% REMOTE
MUST BE UK BASED
At Turnitin, we have been advancing academic integrity in the educational and research sectors for over 20 years. Our innovative solutions are trusted by academic institutions, corporations, and publishers worldwide.
We offer remote work as our default arrangement, embracing individual choices, diversity, and respecting local cultures. However, we also have several office locations globally, where you can choose to work if you prefer. Our team is diverse but united in our mission to make a significant impact on education.
As a Senior Data Engineer at Turnitin, you will join a global team of proactive, supportive, and independent professionals, working together to develop sophisticated AI and data systems. You will collaborate with various teams to integrate AI and data science into our products, enhancing learning, teaching, and academic integrity.
Job Description
Your role as a Senior Data Engineer will involve a range of responsibilities:
AI Data Engineering: Design, build, and deploy real-time data pipelines using AI techniques and best practices. Support AI R&D efforts by applying advanced data warehousing, data science, and data engineering technologies. Aim for automation to speed up development and improve reusability of new AI initiatives.
Collaboration: Work with AI R&D teams and the Data Platform Team to collect, create, curate, and maintain high-quality AI datasets. Ensure consistent data architecture and models across different products and platforms.
Innovation: Use Turnitin's rich data resources to uncover insights through innovative research and development.
Hands-on Involvement: Take part in data engineering and data science tasks as needed to support the team and projects. Lead external data collection efforts, including state-of-the-art prompt engineering techniques, to build advanced AI models.
Communication: Foster clear communication within the team and the organization, ensuring alignment with the company's vision and mission.
Continuous Learning: Stay updated on new tools and development strategies, providing innovative recommendations to leadership.
Qualifications
At least 4 years of experience in data engineering, preferably focused on AI R&D.
Strong proficiency in Python, Java, and SQL.
Proficiency with Redshift, Hadoop, Elasticsearch, and cloud platforms (AWS, Azure, GCP).
Familiarity with AI frameworks such as PyTorch and TensorFlow, and AI libraries like Huggingface and Scikit-Learn.
Experience with Large Language Models (LLMs) and LLM APIs.
Strong problem-solving, analytical, and communication skills, with the ability to thrive in a fast-paced, collaborative environment.
Desired Qualifications
6+ years of experience in data engineering with a focus on AI and machine learning projects.
Experience in a technical leadership role.
Familiarity with natural language processing (NLP) techniques and tools.
Experience in the education or education technology sectors.
Experience with data visualization and data communications.
Characteristics for Success
Passion for creatively solving complex data problems.
Ability to work collaboratively and cross-functionally.
A continuous learning mindset, always seeking to improve skills and knowledge.
Proven track record of delivering results and ensuring quality.
Strong written and verbal communication skills.
Curiosity about the field and the best solutions to its challenges.
Strong system-level problem-solving skills.
Additional Information
Total Rewards @ Turnitin
Turnitin offers a Total Rewards package that is competitive in the local job market. Beyond regular pay and bonuses, we understand that rewards go beyond monetary value. In addition to making a difference in the lives of educators, administrators, learners, and researchers globally, we provide extrinsic rewards such as generous time off and various health and wellness programs. Our Remote-First approach allows you to work in a way that suits your style and situation, whether fully remote, in our offices, or a hybrid arrangement.
Our Mission
Our mission is to ensure the integrity of global education and improve learning outcomes.
Our Values
Customer Centric: Put educators and learners at the center of everything we do.
Passion for Learning: Foster a workplace that encourages constant growth and learning.
Integrity: Maintain high integrity in our products, relationships, and work.
Action & Ownership: Empower teammates to take action and make decisions.
One Team: Collaborate effectively and celebrate each other's successes.
Global Mindset: Respect diversity and embrace local cultures for maximum impact.
Global Benefits
Flexible/hybrid working options.
Remote-First Culture.
Health Care Coverage*.
Tuition Reimbursement*.
Competitive Paid Time Off.
4 Self-Care Days per year.
National Holidays*.
3 all-company global holidays (Juneteenth + 2 Founder's Days).
Paid Volunteer Time*.
Charitable Contribution Match*.
Monthly Wellness Reimbursement/Home Office Equipment*.
Access to Modern Health (mental health platform).
Parental Leave*.
Retirement Plan with match/contribution*.
Benefits may vary by country.
Turnitin, LLC is committed to equal access to our programs, facilities, and employment opportunities. We strongly encourage applications from people of color, persons with disabilities, women, and the LGBTQ+ community, regardless of age, gender, religion, marital or veteran status.
Show more
Show less","AI Data Engineering, Python, Java, SQL, Redshift, Hadoop, Elasticsearch, AWS, Azure, GCP, Tensorflow, PyTorch, Huggingface, ScikitLearn, NLP, Large Language Models, Machine Learning","ai data engineering, python, java, sql, redshift, hadoop, elasticsearch, aws, azure, gcp, tensorflow, pytorch, huggingface, scikitlearn, nlp, large language models, machine learning","ai data engineering, aws, azure, elasticsearch, gcp, hadoop, huggingface, java, large language models, machine learning, nlp, python, pytorch, redshift, scikitlearn, sql, tensorflow"
Senior AI Data Engineer (UK REMOTE),Jobs for Humanity,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-ai-data-engineer-uk-remote-at-jobs-for-humanity-3770785205,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"Company Description
Jobs for Humanity is partnering with TurnItIn to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: TurnItIn
Job Description
Company Description: - Turnitin is a leader in the educational and research sectors, focusing on promoting academic integrity for over two decades. - We offer remote work as the default option, respecting individual choices, valuing diversity, and respecting local cultures. - We have multiple offices around the world for those who prefer an office environment, including locations in the UK, USA, Ukraine, and the Netherlands. - Our team is diverse and united in our commitment to making a significant impact on education. Job Description: - As a Senior Data Engineer at Turnitin, you will be part of a global team working on sophisticated AI and data systems. - Collaborate with different teams to integrate AI and data science into our products, improving learning, teaching, and academic integrity. - Responsibilities include designing and building real-time data pipelines, collaborating with AI R&D teams, conducting research and development, and fostering clear communication. Qualifications: - Minimum 4 years of data engineering experience, preferably focused on AI R&D. - Strong proficiency in Python, Java, and SQL. - Experience with data engineering technologies like Redshift, Hadoop, and cloud platforms. - Familiarity with AI frameworks and libraries such as PyTorch, TensorFlow, Huggingface, and Scikit-Learn. - Strong problem-solving, analytical, and communication skills. Desired Qualifications: - 6+ years of data engineering experience, particularly in AI and machine learning projects. - Experience in a technical leadership role. - Familiarity with natural language processing (NLP) techniques and tools. - Experience in the education or education technology sectors. - Knowledge of data visualization and data communication. Characteristics for Success: - Passion for creatively solving complex data problems. - Ability to work collaboratively and cross-functionally. - Continuous learning mindset. - Proven track record of delivering results with high quality. - Strong written and verbal communication skills. - Curiosity and a strong drive to find the best solutions. - Strong system-level problem-solving skills. Additional Information: - Turnitin provides a total rewards package that is competitive within the local job market. - We offer generous time off, health and wellness programs, and flexibility in remote, office, or hybrid work arrangements. - Our mission is to ensure the integrity of global education and improve learning outcomes. - Our values include being customer-centric, passionate about learning, having integrity, taking action and ownership, being one team, and having a global mindset. - We offer a range of global benefits, including flexible working, healthcare coverage, tuition reimbursement, competitive paid time off, and more. - We strongly encourage applications from people of all backgrounds and identities, including people of color, persons with disabilities, women, and the LGBTQ+ community.
Show more
Show less","AI, Data Engineering, Python, Java, SQL, Redshift, Hadoop, Cloud Platforms, PyTorch, TensorFlow, Huggingface, ScikitLearn, NLP, Data Visualization, Data Communication, SystemLevel ProblemSolving","ai, data engineering, python, java, sql, redshift, hadoop, cloud platforms, pytorch, tensorflow, huggingface, scikitlearn, nlp, data visualization, data communication, systemlevel problemsolving","ai, cloud platforms, data communication, data engineering, hadoop, huggingface, java, nlp, python, pytorch, redshift, scikitlearn, sql, systemlevel problemsolving, tensorflow, visualization"
"Senior Data Engineer, 80% remote",Energy Jobline,"England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-80%25-remote-at-energy-jobline-3773892754,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"We are recruiting for a Senior Data Engineer on behalf of a prestigious Defence client.
In this role, you will be part of a key Project aimed to improve data quality and provide a more granular event level view of the future activity, to enable a more accurate and informed support solution.
This is a contract opportunity with initial duration until the end of February 2024, remote working 4 days/week and 1 day/week on site. The successful candidate must be willing to be in the office to start - i.e. for induction.
Skills
Due to the highly sensitive nature of the Project, details need to remain confidential at this stage however, essential skills and experience needed are:
Experience of Defence IT systems ex. - CHURCHILL and TAFMIS and OPUS
Experience of data analysis & manipulation
Security clearance SC - active, current
We are currently interviewing but still accepting applications.
For a confidential discussion about this or many other job opportunities in the UK, you can reach me at (extension 8186) and ask for Luminita or .
Do you have any disability or condition that could affect you in the application and interview process? Please feel free to share this information as part of your application, including any necessary adjustments you might need, so we can help make the process easier for you. We assure you this information will not have any negative effect on the interview process outcome
We are NonStop, a leading provider of staffing solutions throughout Europe and now in the US. We specialise in connecting talented professionals with exciting opportunities in various industries. For more information, to browse our available roles, or to discuss how we can assist you, please visit our website, NonStop Consulting (https://(url removed))
Show more
Show less","Data Engineering, Data Analysis, Data Manipulation, Security Clearance, CHURCHILL, TAFMIS, OPUS","data engineering, data analysis, data manipulation, security clearance, churchill, tafmis, opus","churchill, data engineering, data manipulation, dataanalytics, opus, security clearance, tafmis"
"Lead Data Engineer (Python, Pyspark) – Remote",Noir,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-python-pyspark-%E2%80%93-remote-at-noir-3781899441,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"Lead
Data Engineer (Python, PySpark) – Remote
(Lead Data Engineer, Team Lead, Technical Lead, Senior Data Engineer, Data Engineer, Python, PySpark, SQL, Big Data, Databricks, R, Machine Learning, AI, Agile, Scrum, TDD, BDD, CI / CD, SOLID principles, Github, Azure DevOps, Jenkins, Terraform, AWS CDK, AWS CloudFormation, Azure, Lead Data Engineer, Team Lead, Technical Lead, Senior Data Engineer, Data Engineer)
Our client is a global innovator and world leader with one of the most recognisable names within technology . They are looking for a Lead Data Engineer with significant Python and PySpark experience as well as management responsibility to run an exceptional Agile engineering team and provide technical and team leadership through coaching and mentorship.
We are seeking a Lead Data Engineer and line manager capable of creating a dynamic and positive environment for your team to excel. This will include coaching your team, working with architects, creating automated tests, instilling a culture of continuous improvement and setting standards for the team. You will be responsible for building a greenfield modern data platform using cutting-edge technologies, architecting big data solutions and developing complex enterprise data ETL and ML pipelines and projections.
The successful candidate will have strong Python, PySpark and SQL experience, possess a clear understanding of databricks, as well as a passion for Data Science (R, Machine Learning and AI). Database experience with SQL and No-SQL – Aurora, MS SQL Server, MySQL is expected, as well as significant Agile and Scrum exposure along with SOLID principles. Continuous Integration tools, Infrastructure as code and strong Cloud Platform knowledge, ideally with AWS is also key.
W e are keen to hear from talented Lead Data Engineer candidates from all backgrounds.
This is a truly amazing opportunity to work for a prestigious brand that will do wonders for your career. They invest heavily in training and career development with unlimited career progression for top performers.
Location:
Remote
Salary:
£75k – £95k + Bonus + Pension + Benefits
To apply for this position please send your CV to Nathan Warner at Noir Consulting.
(Lead Data Engineer, Team Lead, Technical Lead, Senior Data Engineer, Data Engineer, Python, PySpark, SQL, Big Data, Databricks, R, Machine Learning, AI, Agile, Scrum, TDD, BDD, CI / CD, SOLID principles, Github, Azure DevOps, Jenkins, Terraform, AWS CDK, AWS CloudFormation, Azure, Lead Data Engineer, Team Lead, Technical Lead, Senior Data Engineer, Data Engineer)
NOIRUKTECHREC
NOIRUKREC
Show more
Show less","Python, PySpark, SQL, Databricks, R, Machine Learning, AI, Agile, Scrum, TDD, BDD, CI / CD, SOLID principles, Github, Azure DevOps, Jenkins, Terraform, AWS CDK, AWS CloudFormation, Azure, Aurora, MS SQL Server, MySQL","python, pyspark, sql, databricks, r, machine learning, ai, agile, scrum, tdd, bdd, ci cd, solid principles, github, azure devops, jenkins, terraform, aws cdk, aws cloudformation, azure, aurora, ms sql server, mysql","agile, ai, aurora, aws cdk, aws cloudformation, azure, azure devops, bdd, ci cd, databricks, github, jenkins, machine learning, ms sql server, mysql, python, r, scrum, solid principles, spark, sql, tdd, terraform"
"Senior Data Engineer - Blockchain, Ethereum",Clearmatics,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-blockchain-ethereum-at-clearmatics-3694292557,2023-12-17,Greater London, United Kingdom,Mid senior,Remote,"About Clearmatics
Clearmatics is a blockchain protocol engineering company that builds decentralised financial market infrastructures that are more open, efficient, and resilient than those serving legacy market structures of today.
We have a bold vision for this new market infrastructure - enabling market participants to solve currently unaddressed problems of the real economy, including ""missing markets"" problems. We aim to achieve our vision through technologies that allow peer-to-peer trading and settlement within new markets, and with digital assets whose values track real-world risk factors that currently have no suitable expression in legacy markets.
No matter the nature of a risk factor, whether macro-economic or relating to real-world metrics such as climate data, our mission is to make important risk factors tradable; by anyone, anywhere, without reliance on financial intermediaries. We are building this new infrastructure in collaboration with incredible trading and blockchain communities.
Clearmatics was founded in 2015, and we have ever since been committed advocates of Open Source development, and active participants in the blockchain and cryptography research communities. We've also been an active participant and supporter of decentralized infrastructure initiatives.
We're growing our team and are looking for a
Senior
Data Engineer
to join us and become part of our journey.
Our stack is Ethereum-like, so familiarity with – and passion for – the space is key.
Some of the areas we tackle include blockchain infrastructure optimized for financial use-cases, protocols for specific applications (such as smart contract-based derivatives, synthetic assets, cryptocurrency stabilization, …), scalability and privacy through cryptography (zkSNARKs, multiparty computation, private transactions/assets).
Responsibilities
As a data engineer, you will be responsible for designing, implementing, testing and maintaining scalable data pipelines that bring together data from an array of sources.
To achieve this, you will need to consolidate, cleanse, and structure the data, as well as maintain the automated scripts, data pipelines, and databases that comprise the data platform.
This data provides researchers with insights into blockchain protocols to enable them to perform analytical work. It also drives public and internal dashboards and reports.
About You
You like to build robust, scalable, maintainable, and well-documented data pipelines.
You are passionate about blockchain and are comfortable working with large datasets.
You have exquisite attention to detail and a strong appreciation for data integrity and correctness.
Requirements
A background in Computer Science and Data Science
Experience administering relational databases (Snowflake, PostgreSQL)
Experience designing and maintaining data pipelines and ETL/ELT processes
Experience developing custom data connectors
Fluency in Python programming to intermediate level
A good understanding of the Ethereum protocol
Proficiency in SQL, GraphQL, and query optimization
Good knowledge of Unix shell scripting and commandline tools
Familiarity with cloud services (e.g. AWS EC2/S3, Google Cloud)
Excellent verbal and written English skills
BONUS POINTS
Proficiency with Python scientific computing (NumPy, Pandas, Jupyter)
Experience with managing and processing financial market timeseries data
Show more
Show less","Computer Science, Data Science, SQL, GraphQL, Python, Unix shell scripting, Cloud services (AWS EC2/S3 Google Cloud), Snowflake, PostgreSQL, NumPy, Pandas, Jupyter, Ethereum protocol, Blockchain, Data pipelines, ETL/ELT processes, Data connectors, Data integrity, Data correctness","computer science, data science, sql, graphql, python, unix shell scripting, cloud services aws ec2s3 google cloud, snowflake, postgresql, numpy, pandas, jupyter, ethereum protocol, blockchain, data pipelines, etlelt processes, data connectors, data integrity, data correctness","blockchain, cloud services aws ec2s3 google cloud, computer science, data connectors, data correctness, data integrity, data science, datapipeline, ethereum protocol, etlelt processes, graphql, jupyter, numpy, pandas, postgresql, python, snowflake, sql, unix shell scripting"
Junior Data Engineer,Pepper Mill,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-at-pepper-mill-3783941323,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"About Sparta Global
Embark on a transformative career journey with Sparta Global, where diversity, innovation, and passion for technology converge. We welcome individuals from all backgrounds, offering not just jobs, but dynamic careers in the tech industry. You'll work alongside enthusiastic professionals, receiving top-tier training and mentorship to hone your skills for success in both public and private sectors. Our commitment to designing impactful careers and coaching future leaders is evident in our over 10 prestigious awards in Learning & Development and Equality, Diversity & Inclusion. As a Top 20 Employer for Social Mobility and a proud B-Corp certified organisation, we're not just advancing careers; we're fostering a more diverse, equitable, and inclusive tech landscape. Join us in shaping the future of technology - where your growth is our mission, and your success, our pride. Apply now to be part of our award-winning team at Sparta Global.
About This Role
You'll become versatile in a wide array of tools across topics covering data focused coding, data visualisation, Cloud Services and Big Data.
You'll be designing, building, maintaining, and troubleshooting the data pipelines that enable organizations to store, process, and analyse their data, and ensuring the data is reliable through testing and debugging. Looking for efficiencies and optimising the data pipelines for scalability and performance will be a focus.
You'll be handling and working with large sets of structured and unstructured data and will be responsible for ensuring that the data is organized and available for data scientists and analysts to use.
Working with others is key, you could be working with other engineers, developers, data scientists, analysts and even stakeholders to understand their data needs.
We're not expecting you to have the proficiencies right away - that's where our award-winning Academy comes in. We are the experts in building skills and confidence in a fun and supportive environment that will not only challenge you but also develop your specialist capabilities ready to work on our clients' projects.
What we're looking for.
To be successful for this role you will demonstrate a level of ability in Python or similar. You will be passionate about technology and eager to learn programme development to an advanced level.
We're Seeking Candidates Who Can Exemplify Our Values
Empathy and Diversity: Integrity, respect, and a commitment to inclusivity.
Drive: A goal-oriented mindset with pride in exceeding targets.
Collaboration: A team-focused approach, fostering positive relationships.
Innovation: Curiosity, creativity, and openness to diverse ideas.
Flexibility: Adaptability and composure in the face of change.
As a national organisation with clients across the UK, we require flexibility and a willingness to relocate post-remote training. Deployment locations vary and cannot be guaranteed. We encourage applications from diverse backgrounds and experience levels. Eligibility to work in the UK by the start of employment is mandatory.
Why You Should Apply
Our environment is designed to nurture your talents and skills, your hard work and progress are not just appreciated - they're tangibly rewarded. We conduct performance-based reviews every six months, offering you the chance to increase your earning potential twice a year. This regular appraisal system is our way of ensuring that your efforts and achievements are consistently recognised and rewarded.
We Also Provide
20 days annual leave + bank holidays.
An extra day off for your birthday.
Pension.
Discounted gym membership.
Eye care.
Death in service cover.
Cycle to work scheme.
Season ticket loan.
Employee assistance program.
Yearly budget for personal development.
Access to alumni and community networks.
Opportunities to be brand ambassadors.
Being employed by Sparta Global is an investment in your future that pays dividends along the way. We give you breadth of experience and skills, along with increasing opportunities to develop further and earn more. No two career paths look the same at Sparta.
Our Recruitment Process
Begin your journey via our supportive recruitment process. Apply online and our team will promptly review your application, contacting successful candidates within 48 hours to initiate the next steps. If you pass our initial screening, candidates will proceed to online assessments which vary depending on the opportunity you are applying for. The final stage is a competency interview, here you'll have the opportunity to impress us with your ability to communicate effectively and exhibit behavioural competencies through relevant examples. We're looking for candidates who can demonstrate a collaborative spirit and a growth mindset.
Your dedicated Talent Team member will be with you every step of the way to support and answer any questions you have. You can also visit our YouTube channel to gain valuable insights and expert advice on virtual interviews, strategies to manage nerves, and tips on nonverbal communication.
We look forward to receiving your application - good luck!
Show more
Show less","Python, Data Science, Data Analysis, Data Visualization, Cloud Services, Data Pipelines, Machine Learning, Data Mining, Data Integration, Big Data, Scalability, Data Quality, Data Storage, Analytical Skills, Programming Languages","python, data science, data analysis, data visualization, cloud services, data pipelines, machine learning, data mining, data integration, big data, scalability, data quality, data storage, analytical skills, programming languages","analytical skills, big data, cloud services, data integration, data mining, data quality, data science, data storage, dataanalytics, datapipeline, machine learning, programming languages, python, scalability, visualization"
Junior Data Engineer,Sparta Global,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-at-sparta-global-3783939108,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"About Sparta Global
Embark on a transformative career journey with Sparta Global, where diversity, innovation, and passion for technology converge. We welcome individuals from all backgrounds, offering not just jobs, but dynamic careers in the tech industry. You'll work alongside enthusiastic professionals, receiving top-tier training and mentorship to hone your skills for success in both public and private sectors. Our commitment to designing impactful careers and coaching future leaders is evident in our over 10 prestigious awards in Learning & Development and Equality, Diversity & Inclusion. As a Top 20 Employer for Social Mobility and a proud B-Corp certified organisation, we're not just advancing careers; we're fostering a more diverse, equitable, and inclusive tech landscape. Join us in shaping the future of technology - where your growth is our mission, and your success, our pride. Apply now to be part of our award-winning team at Sparta Global.?
About This Role
You'll become versatile in a wide array of tools across topics covering data focused coding, data visualisation, Cloud Services and Big Data.
You'll be designing, building, maintaining, and troubleshooting the data pipelines that enable organizations to store, process, and analyse their data, and ensuring the data is reliable through testing and debugging. Looking for efficiencies and optimising the data pipelines for scalability and performance will be a focus.
You'll be handling and working with large sets of structured and unstructured data and will be responsible for ensuring that the data is organized and available for data scientists and analysts to use.
Working with others is key, you could be working with other engineers, developers, data scientists, analysts and even stakeholders to understand their data needs.
We're not expecting you to have the proficiencies right away - that's where our award-winning Academy comes in. We are the experts in building skills and confidence in a fun and supportive environment that will not only challenge you but also develop your specialist capabilities ready to work on our clients' projects.
What we're looking for.
To be successful for this role you will demonstrate a level of ability in Python or similar. You will be passionate about technology and eager to learn programme development to an advanced level.
We're seeking candidates who can exemplify our values:??
Empathy and Diversity: Integrity, respect, and a commitment to inclusivity.??
Drive: A goal-oriented mindset with pride in exceeding targets.??
Collaboration: A team-focused approach, fostering positive relationships.??
Innovation: Curiosity, creativity, and openness to diverse ideas.??
Flexibility: Adaptability and composure in the face of change.
As a national organisation with clients across the UK, we require flexibility and a willingness to relocate post-remote training. Deployment locations vary and cannot be guaranteed. We encourage applications from diverse backgrounds and experience levels. Eligibility to work in the UK by the start of employment is mandatory.
Why you should apply:?
Our environment is designed to nurture your talents and skills, your hard work and progress are not just appreciated - they're tangibly rewarded. We conduct performance-based reviews every six months, offering you the chance to increase your earning potential twice a year. This regular appraisal system is our way of ensuring that your efforts and achievements are consistently recognised and rewarded.?
We also provide:??
20 days annual leave + bank holidays.??
An extra day off for your birthday.??
Pension.??
Discounted gym membership.??
Eye care.??
Death in service cover.??
Cycle to work scheme.??
Season ticket loan.??
Employee assistance program.??
Yearly budget for personal development.??
Access to alumni and community networks.??
Opportunities to be brand ambassadors.??
Being employed by Sparta Global is an investment in your future that pays dividends along the way. We give you breadth of experience and skills, along with increasing opportunities to develop further and earn more. No two career paths look the same at Sparta.??
Our Recruitment Process:?
Begin your journey via our supportive recruitment process. Apply online and our team will promptly review your application, contacting successful candidates within 48 hours to initiate the next steps. If you pass our initial screening, candidates will proceed to online assessments which vary depending on the opportunity you are applying for. The final stage is a competency interview, here you'll have the opportunity to impress us with your ability to communicate effectively and exhibit behavioural competencies through relevant examples. We're looking for candidates who can demonstrate a collaborative spirit and a growth mindset.??
Your dedicated Talent Team member will be with you every step of the way to support and answer any questions you have. You can also visit our YouTube channel to gain valuable insights and expert advice on virtual interviews, strategies to manage nerves, and tips on nonverbal communication.???
We look forward to receiving your application - good luck!???
Show more
Show less","Python, Data Visualization, Cloud Services, Big Data, Data Pipelines, Data Storage, Data Processing, Data Analysis, Data Reliability, Testing and Debugging, Data Scalability, Data Performance, Structured Data, Unstructured Data, Data Organization, Data Science, Data Analytics, Data Stakeholders, Data Engineering, Collaboration, Teamfocused Approach, Innovation, Curiosity, Creativity, Adaptability, Change Management, Performancebased Reviews, Pension, Gym Membership, Eye Care, Life Insurance, Cycle to Work Scheme, Season Ticket Loan, Employee Assistance Program, Competency Interview","python, data visualization, cloud services, big data, data pipelines, data storage, data processing, data analysis, data reliability, testing and debugging, data scalability, data performance, structured data, unstructured data, data organization, data science, data analytics, data stakeholders, data engineering, collaboration, teamfocused approach, innovation, curiosity, creativity, adaptability, change management, performancebased reviews, pension, gym membership, eye care, life insurance, cycle to work scheme, season ticket loan, employee assistance program, competency interview","adaptability, big data, change management, cloud services, collaboration, competency interview, creativity, curiosity, cycle to work scheme, data engineering, data organization, data performance, data processing, data reliability, data scalability, data science, data stakeholders, data storage, dataanalytics, datapipeline, employee assistance program, eye care, gym membership, innovation, life insurance, pension, performancebased reviews, python, season ticket loan, structured data, teamfocused approach, testing and debugging, unstructured data, visualization"
Data Engineer,RedCat Digital,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-redcat-digital-3771905699,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"Working for a high-growth
global multichannel luxury retail brand
will have the opportunity to shape the future of insights and data processes and improve the digital experience for their millions of customers globally.
Key Responsibilities:
Design, develop, and maintain scalable data pipelines and ETL processes.
Collaborate with cross-functional teams to understand data requirements and deliver solutions that meet business needs.
Optimize and troubleshoot existing data systems to ensure performance and reliability.
Implement best practices in data management, security, and quality.
Stay abreast of industry trends and emerging technologies in data engineering.
What's on Offer:
· Flexible Working
· Private Healthcare
· Generous staff discount
· Learning & development budget
· Bonus
Please submit your details for more information about this opportunity.
Email: va@RedCat-Digital.com
Location: London
Start Date: ASAP
Show more
Show less","Data Engineering, ETL, Data Pipelines, Data Quality, Data Security, Data Management, Cloud Computing, AWS, Azure, Data Warehousing, Data Analytics, Data Mining, Data Visualization, Business Intelligence, Machine Learning, Artificial Intelligence","data engineering, etl, data pipelines, data quality, data security, data management, cloud computing, aws, azure, data warehousing, data analytics, data mining, data visualization, business intelligence, machine learning, artificial intelligence","artificial intelligence, aws, azure, business intelligence, cloud computing, data engineering, data management, data mining, data quality, data security, dataanalytics, datapipeline, datawarehouse, etl, machine learning, visualization"
Data Engineer,Leadenhall Search & Selection,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-leadenhall-search-selection-3765136527,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"Leadenhall Search & Selection are actively recruiting a data engineer on behalf of a fast-growing boutique multi-claimant law firm who are looking for an experienced data professional to strengthen their team. This is a great opportunity to work for a UK company who are going through an exciting period of growth. They are hiring in their London office.
The role
Design, develop, and maintain scalable data pipelines using Python, Azure Data Factory, and other relevant tools.
Collaborate with business stakeholders to understand requirements and integrate diverse data sources into a unified data lake or warehouse.
Perform data cleansing, transformation, and enrichment to ensure data quality and consistency.
Implement and maintain data security and compliance measures to protect sensitive information.
Implement automation scripts and workflows to streamline data engineering processes.
Create and maintain clear documentation for data pipelines, data schemas, and processes.
Liaise with third parties, providing relevant data to support project delivery, system maintenance and change programmes.
Essential skills
BSc in Computer Science, Information Technology or a related field.
2+ years’ experience in data engineering or similar role
People management experience with proven track record of leading a small team
Strong proficiency in Python programming for data manipulation and transformation
Knowledge of data modelling and database design principles
Familiarity with ETL (Extract, Transform, Load) processes and tools
Preferred skills
Certification in Azure Data Engineering or a related field
Familiarity with data warehousing concepts
Experience with data visualisation tools (e.g., Power BI, Tableau)
Experience with Azure cloud services, including Azure Data Factory, Azure Data Lake Storage, Azure Databricks, and Azure SQL Database.
Show more
Show less","Data Engineering, Python, Azure Data Factory, Data Pipelines, Data Integration, Data Cleansing, Data Transformation, Data Enrichment, Data Security, Data Compliance, Automation Scripts, Workflows, Data Documentation, Data Modelling, Database Design, ETL Processes, Data Warehousing, Data Visualization, Power BI, Tableau, Azure Cloud Services, Azure Data Lake Storage, Azure Databricks, Azure SQL Database","data engineering, python, azure data factory, data pipelines, data integration, data cleansing, data transformation, data enrichment, data security, data compliance, automation scripts, workflows, data documentation, data modelling, database design, etl processes, data warehousing, data visualization, power bi, tableau, azure cloud services, azure data lake storage, azure databricks, azure sql database","automation scripts, azure cloud services, azure data factory, azure data lake storage, azure databricks, azure sql database, data compliance, data documentation, data engineering, data enrichment, data integration, data modelling, data security, data transformation, database design, datacleaning, datapipeline, datawarehouse, etl, powerbi, python, tableau, visualization, workflows"
Data Engineer,Lawrence Harvey,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-lawrence-harvey-3761868756,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"Data Engineer | Advertising & Media | London, hybrid
An exciting Global Advertising agency, working with companies such as HSBC, Google, Virgin Media, Clinique and more, is looking for a Data Engineer to join their team!
As the successful candidate you will be responsible for designing, implementing and managing data solutions on Azure. You will help create data pipelines, data storage solutions, data integrations and lead the data-driven decision-making for the company!
The role is in central London, on a hybrid basis, offering up to £60,000!
Required skills:
Experience working as a Data Engineer with Azure Cloud Platform.
Hands-on experience with SQL and Azure Data Factory.
Experience in designing, developing, and maintaining data warehouse environments.
Experience in developing and maintaining ETL pipelines.
Excellent communication skills, both written and verbal required.
Nice to haves:
Background working for media or advertising companies.
Experience in Python scripting is ideal.
Exposure to Power BI, Tableau, or similar data visualisation tools.
Interested in working with some of the biggest companies in the UK, like Google, HSBC, Aldi and more?? Apply now below!
Show more
Show less","Data Engineer, Azure Cloud Platform, SQL, Azure Data Factory, Data Warehouse, ETL Pipelines, Python, Power BI, Tableau, Data Visualization","data engineer, azure cloud platform, sql, azure data factory, data warehouse, etl pipelines, python, power bi, tableau, data visualization","azure cloud platform, azure data factory, dataengineering, datawarehouse, etl pipelines, powerbi, python, sql, tableau, visualization"
Data Engineer,Atos,"England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-atos-3766625809,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"We require an ETL Consultant to support the delivery of projects undertaken by the IMA practice.
Responsibilities:
Input into or sole definition of Physical Data Models
Design and delivery of data mapping rules detailing the transformation of data from source to target
Design of ETL routines in Azure Data Factory and Databricks and standards to deliver the data management solution
Development of ETL routines to deliver the data management solution
Code reviews of colleague’s ETL routines
Involvement in pre-sales and bid efforts to define, document and explain BI data management solutions and responses to questions
Acting as design authority for small BI Data Management application projects
Team lead for teams of ETL Developers (and possibly other members of the project team) to support the above work
Client facing role working through requirements gathering to deliver the data management solution
Definition and estimation of the effort to deliver the data management solution
Mandatory skills:
Developing data transformation code using Azure Data Factory and Databricks, testing of code ( significant and demonstrable experience in the use of at least 2 leading ETL tools across multiple projects – must have Azure Data Factory and Databricks)
Demonstrable experience in leading teams of developers
In-depth knowledge of Inmon and Kimball data warehouse designs including the use of slowly changing dimensions
Demonstrate experience across a range of relational and database technology, including Oracle and SQL Server
Must be able to demonstrate good analytical skills and a logical approach to problem solving
Pragmatic and detail orientated
Excellent organizational and co-ordination skills
Excellent communication skills, both orally and in writing
Demonstrate a good understanding of data quality and data quality/profiling tools that support this
A good understanding of Master Data Management theory and application and the tools that support this
Experience of at least one BI tool
Experience of leading an ETL development team.
Show more
Show less","Azure Data Factory, Databricks, ETL routines, Data modeling, Data mapping, Data transformation, Data management, Inmon data warehouse design, Kimball data warehouse design, Slowly changing dimensions, Oracle, SQL Server, Analytical skills, Problem solving, Data quality, Data profiling, Master Data Management, BI tool, ETL development team","azure data factory, databricks, etl routines, data modeling, data mapping, data transformation, data management, inmon data warehouse design, kimball data warehouse design, slowly changing dimensions, oracle, sql server, analytical skills, problem solving, data quality, data profiling, master data management, bi tool, etl development team","analytical skills, azure data factory, bi tool, data management, data mapping, data profiling, data quality, data transformation, databricks, datamodeling, etl development team, etl routines, inmon data warehouse design, kimball data warehouse design, master data management, oracle, problem solving, slowly changing dimensions, sql server"
Data Engineer,Adecco,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-adecco-3783635951,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"Job Title: Data Engineer
Start/End Dates: 20/12/2023 - 19/06/2024
Location: UK - Remote
Minimum pay rate: £50 (Negotiable)
The Role:
Do you like working with big data? Do you want to use data to influence product decisions for products being used by over half a billion people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. In this role, you will work with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.
Data Engineer, Analytics Responsibilities
• Manage data warehouse plans for a product or a group of products.
• Interface with engineers, product managers and product analysts to understand data needs.
• Build data expertise and own data quality for allocated areas of ownership.
• Design, build and launch new data models in production.
• Design, build and launch new data extraction, transformation and loading processes in production.
• Support existing processes running in production.
• Define and manage SLA for all data sets in allocated areas of ownership.
• Work with data infrastructure to triage infra issues and drive to resolution.
Minimum Qualifications
• Extensive experience in the data warehouse space.
• Extensive experience in custom ETL design, implementation and maintenance.
• Extensive experience with programming languages (Python or Java), Python preferred.
• Extensive experience in writing efficient SQL statements.
• Experience working with either a Map Reduce or an MPP system.
• Hands on and deep experience with schema design and dimensional data modeling.
• Ability to analyze data to identify deliverables, gaps and inconsistencies.
• Excellent communication skills including the ability to identify and communicate data driven insights.
• Ability and interest in managing and communicating data warehouse plans to internal clients.
A benefit of applying for this role through Adecco is that you will not only receive professional interview preparation and guidance from us, but also our representation and support throughout the application process.
A full job description is available for those who would like a copy. Please be advised that due to the high volume of applications we receive, we are unable to respond to each application individually. This role is an immediate start, so please apply and we will contact you if you have been selected for interview.
Show more
Show less","Data warehousing, ETL design and implementation, Python, Java, SQL, MapReduce, MPP, Schema design, Dimensional data modeling, Data analysis, Data communication, Data management","data warehousing, etl design and implementation, python, java, sql, mapreduce, mpp, schema design, dimensional data modeling, data analysis, data communication, data management","data communication, data management, dataanalytics, datawarehouse, dimensional data modeling, etl design and implementation, java, mapreduce, mpp, python, schema design, sql"
Data Engineer (Sports) - Up to £70k,Stott and May,"London Area, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-sports-up-to-%C2%A370k-at-stott-and-may-3767378390,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"Role:
Data Engineer
Location:
London
Hybrid:
2 days in office a Week
Industry:
Sports
Salary:
£60,000-£70,000 + Bonus
Stott and May are currently partnered with a professional sports team on an exclusive basis as they look to grow their Data Engineering Team.
As a Data Engineer, you will build robust data pipelines, optimise data storage and retrieval, and ensure data quality. You will also work across functions and be responsible for how data is utilised.
In your first few weeks in this Data Engineer role, you can expect to:
Design, develop, and deploy scalable and efficient data pipelines using Data Lake Storage, SQL Data Warehouse and Blob/object Storage.
Work towards ensuring data quality, performance and reliability through the implementation of data validation and monitoring processes
Collaborate with data scientists, analysts, and business stakeholders to understand data requirements and translate them into technical solutions
Build data models to drive marketing use cases and automation as well as to feed visualisation tools.
To apply for this data engineer role, you will need proficiency in programming languages such as Python (or R), SQL as a minimum. You will also require the following:
Proven experience working in cloud platforms and using DBT and Github
Knowledge of Azure data services
Understanding of data modelling and creation of semantic data layers
Experience in building and managing pipelines and data integration techniques
Knowledge and experience of BI tools (Visio or Miro highly advantageous)
It is also advantageous if you have knowledge of some of the following:
Understanding of dimensional modelling
Understanding of marketing technologies
An interest in sports
Show more
Show less","Python, R, SQL, DBT, GitHub, Azure data services, Data modelling, Semantic data layers, Pipelines, Data integration techniques, BI tools, Visio, Miro, Dimensional modelling, Marketing technologies","python, r, sql, dbt, github, azure data services, data modelling, semantic data layers, pipelines, data integration techniques, bi tools, visio, miro, dimensional modelling, marketing technologies","azure data services, bi tools, data integration techniques, data modelling, dbt, dimensional modelling, github, marketing technologies, miro, pipelines, python, r, semantic data layers, sql, visio"
Data Engineer,Winston Fox,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-winston-fox-3773332537,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"This Data Analytics Team (5 members) at a UK Quantitative Hedge Fund is in high demand and growing significantly. As a member of this team, you will be running data analytics projects from start to finish, and working directly with various internal business stakeholders, from finance, investor relations, risk, quant research, and portfolio management teams. As a member of this team, you will be delivering new data products/solutions/analytics for business users and contributing to the core data platform utilising a modern technology stack.
What we are looking for
5+ years of software development/data engineering experience.
Good exposure to product/project ownership, and full project lifecycle.
Good project track record - seeing projects through from inception to completion.
Good Python development. Test-driven and well-documented solutions.
Good exposure to working and track record implementing data/ETL solutions, with a particular focus on data Transformation.
Good exposure to using DBT.
Excellent communication skills and full fluency in English.
Motivated, enthusiastic, and thrives in a fast-moving and changing environment
Assured and confident in own abilities and speaking with senior members of the organisation
Record of professional innovation and achievement
Desirable Experience
Exposure to Snowflake, Airflow, Dash, Git
Show more
Show less","Python, DBT, Snowflake, Apache Airflow, Dash, Git","python, dbt, snowflake, apache airflow, dash, git","apache airflow, dash, dbt, git, python, snowflake"
Data Engineer,NP Group,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-np-group-3752697976,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"Job title:
Data Engineer - Azure
Location:
West London
Salary Details:
£50k-60k depending on experience
The Data Engineer will support software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. You must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
DUTIES & RESPONSIBILITIES:
Understand, build and develop ETL and data integration solutions using a wide array of technologies and data sources
Explore ways to enhance data quality and reliability
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.
Work with cloud-based infrastructure (Azure) for hosting data solutions and applications
Collaborate with architects, data analysts and data scientists to help meet the business goals
SKILLS REQUIRED:
Proven experience in development and maintenance of ETL/ELT processes within a medallion architecture
Strong experience with SQL and relational databases
Good knowledge of the Azure data engineering stack - Azure Data Factory, Azure Synapse, Azure Data Lake
Analytical skills related to working with structured and unstructured datasets
Excellent written and verbal communication skills
Experience supporting and working with cross-functional teams in a dynamic environment
Show more
Show less","ETL/ELT, Data Integration, Data Quality and Reliability, Process Improvement, Azure Cloud, SQL, Relational Databases, Azure Data Factory, Azure Synapse, Azure Data Lake, Analytical Skills, Unstructured Data","etlelt, data integration, data quality and reliability, process improvement, azure cloud, sql, relational databases, azure data factory, azure synapse, azure data lake, analytical skills, unstructured data","analytical skills, azure cloud, azure data factory, azure data lake, azure synapse, data integration, data quality and reliability, etlelt, process improvement, relational databases, sql, unstructured data"
Data Engineer,GoHenry,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-gohenry-3769246612,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"Summary
GoHenry is a London and New York-based fintech company that pioneered the youth fintech and financial education category. We are on a mission to make every kid smart with money and to delight our community of 2 million parents, kids, and teens with innovative, market-leading tools to help parents raise financially healthy kids.
GoHenry offers a debit card and app for kids and teens and companion apps for the family, with in-app tools for sending money, automating allowance, managing chores, setting savings goals, giving to charity, and in-app financial education lessons where kids can watch videos, take quizzes and earn points & badges. This is all designed to help kids and teens build good money habits that will last a lifetime.
Where this role fits in
We are looking for an experienced and self-driven Data Engineer to work in our central Data team, a group of ~10 data analysts, data engineers and analytics engineers. The role will report into the Core Data Lead.
The team is currently building a brand new data platform on Google Cloud to ingest raw data from a variety of internal and external source systems, transform the data so it aligns to GoHenry’s analytic needs, and make the data available to data analysts, business users and downstream systems. You will be working with a wide variety of data such as card transactions, advertising activity, user interactions with GoHenry’s ‘Money Missions’ etc. Some time will also be spent maintaining our legacy data infrastructure and eventually assisting with decommissioning activities.
This role has responsibilities spanning both the Ingestion and Data Warehouse layers of the data platform. We follow an ‘Extract, Load, Transform (ELT)’ approach at GoHenry, and the Data Engineer will be responsible for the end-to-end ELT process. The key technologies used in this role are Datastream, Dataflow, Cloud Storage, Cloud Functions, Pub/Sub, BigQuery, Dataform, Dataplex, Data Catalog, DLP, Airflow, git, Python and SQL.
Responsibilities
Key responsibilities include:
Designing, building, testing, deploying and monitoring data pipelines, covering both the initial ingestion from source systems, as well as data transformation within the warehouse.
Designing and building data models in the data warehouse, carefully considering the analytical needs of users.
Helping data analysts to get the most out of the warehouse, advising them on query tuning techniques.
Keeping our documentation, data catalogue and data lineage tools up-to-date.
Performing regular proactive maintenance and housekeeping on the data platform.
Working with colleagues from other teams to monitor and improve data quality within the warehouse.
Responding to incidents arising from data pipeline failures.
Sharing knowledge with other team members, and playing an active role in the Data team and wider Tech team.
Other activities that may be required from time to time.
What we’re looking for
Essential:
Experience in building and maintaining data pipelines.
Data modelling experience.
An understanding of data warehousing concepts and best practices.
Experience using a data transformation tool such as Dataform or dbt.
Ability to write advanced SQL queries in a data warehouse environment.
Ability to write Python to a very good level.
Experience building a modern cloud data platform, ideally using Google Cloud.
Desirable:
Good working knowledge of BigQuery, particularly complex field types and optimisation techniques.
Experience using Airflow.
As a person, you are a self-starter who is motivated to kick start their own projects, welcomes challenges, seeks feedback from others, asks “how can I help” and then “how can we make this better”, shows perseverance on long-term goals, and generally goes above and beyond in their work.
What we’re offering
25 days annual leave, plus bank holidays
An additional day off on your birthday (or any other day in the same week)
Flexible Public Holidays
Family-friendly leave policies
Death in service – 4x your annual salary from month 1
Enhanced Maternity & Paternity Leave.
Westfield Cash Health Plan (for you and your children)
Westfield surgery plan after 3 years’ service
Westfield Rewards (receive discounts at over 600 retailers)
Mental health platform – Open up
Cycle to work scheme
Training opportunities to further develop your craft
Choose your own equipment
We’re proud to say...
We help over two million members improve their money skills every single day.
We’re one of Tech Track’s top 50 fastest growing UK companies.
We were nominated for ‘Best Personal Finance App’ at the British Bank Awards 2020.
We were voted 'Best Children's Financial Provider' at the British Bank Awards 2021.
Our kids and parents have donated over £200,000 of their own money to NSPCC via their GoHenry accounts
We won the Kids' Cards category of Finder's Customer Satisfaction Awards 2022
But we’re still growing, and that’s why we need you.
GoHenry is an equal opportunity employer, and we’re on a mission to foster a diverse & inclusive workplace. Individuals seeking employment at GoHenry are considered without regard to race, religion, national origin, age, sex, gender, gender identity, gender expression, sexual orientation, marital status, medical condition, ancestry, physical or mental disability, military or veteran status, or any other characteristic protected by applicable law.
Want to join our mission?
If GoHenry sounds like a place you’d like to be, please apply using the link below.
Show more
Show less","Data Engineering, Data Analytics, Data Warehousing, Data Pipeline, ELT, Data Modelling, Data Quality, Data Lineage, Python, SQL, BigQuery, Dataform, dbt, Airflow, Cloud Data Platform, Google Cloud, Datastream, Dataflow, Cloud Storage, Cloud Functions, Pub/Sub, Dataplex, Data Catalog, DLP, Git","data engineering, data analytics, data warehousing, data pipeline, elt, data modelling, data quality, data lineage, python, sql, bigquery, dataform, dbt, airflow, cloud data platform, google cloud, datastream, dataflow, cloud storage, cloud functions, pubsub, dataplex, data catalog, dlp, git","airflow, bigquery, cloud data platform, cloud functions, cloud storage, data catalog, data engineering, data lineage, data modelling, data pipeline, data quality, dataanalytics, dataflow, dataform, dataplex, datastream, datawarehouse, dbt, dlp, elt, git, google cloud, pubsub, python, sql"
"Data Engineer (Python, Spark, Airflow) - New Data Platform",GL Global,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-python-spark-airflow-new-data-platform-at-gl-global-3774708157,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"Data Engineer (Python, Spark, Airflow) - New Data Platform
A London based organisation require an experienced Data Engineer to join their new AI/ML Data function.
This will be an opportunity to join a newly established team which has the opportunity to work within a number of interesting projects including Data Science, Market Data and Pricing Transformation, Customer Data and Analytics.
As this is a new role, you will be meeting with key stakeholders to gather requirements, design and implement the solutions, and lead the development lifecycle. My client is heading into a period of rapid growth and we expect this data team to grow with the company.
This will be a hybrid working position which requires 2 days a week onsite in London.
Responsibilities:
Develop and maintain scalable data pipelines to power new initiative data products
Use the latest data analytics and quality tools to ensure data meets strict data standards.
Design and implement new data pipelines using best in class tools such as Airflow, DBT etc.
Build a scalable cloud infrastructure on Azure using modern technologies such as terraform
Bring your own idea to show how we can do things better.
Experience:
Strong hands-on development experience with Python
Hands-on experience with Apache Spark
Strong SQL experience (Any No-SQL and/or Snowflake experience is a plus)
Experience of working within small teams or a startup environment
Interest in working closely with Artificial Intelligence / Machine Learning
Experience with schema design and dimensional data modelling
Any experience with Azure cloud services, DBT, Airflow is a huge plus
If interested, please apply for further details & a confidential discussion.
Show more
Show less","Python, Apache Spark, Airflow, SQL, NoSQL, Snowflake, Terraform, Azure, DBT, Schema design, Dimensional data modelling, AI, Machine Learning","python, apache spark, airflow, sql, nosql, snowflake, terraform, azure, dbt, schema design, dimensional data modelling, ai, machine learning","ai, airflow, apache spark, azure, dbt, dimensional data modelling, machine learning, nosql, python, schema design, snowflake, sql, terraform"
Data Engineer,Agora Talent,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-agora-talent-3779158981,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"Agora are excited to be working with a London-based B2B SaaS startup who are looking to expand the engineering team by bringing in a business-savvy data engineer to be the third member of the data team.
The role:
This role primarily covers building out the core product by integrating new datasets into client workflows, with a focus on identifying the commonalties between different client installations to create a scalable SaaS product. Once this has achieved a certain level of product maturity, this role will also involve building their own data platform.
We they offer:
• Senior mentorship
• Competitive compensation package, incl. stock options
• Flexible hours and hybrid working, typically 3 days a week in the office
• Attractive office space
• A company passionate about making a positive impact - pledging to be a net-zero CO2
company and donate 1% of their revenues to social impact causes.
Requirements:
• A proven level of commercial experience in data engineering
• Experience at seed to series A/B stage companies or an excitement to experience first-hand
the joys and pains the come with scaling a company
• The ability to translate complex and sometimes ambiguous business requirements into clean and maintainable data pipelines
• Excellent knowledge of PySpark, Python and SQL fundamentals
• Experience in contributing to complex shared repositories.
What’s nice to have:
• Prior early-stage B2B SaaS experience involving client-facing projects
• Experience in front-end development and competency in JavaScript
• Knowledge of API development
• Familiarity with Airflow, DBT, Databricks
• Experience working with Enterprise Resource Planning (e.g. Oracle, SAP) and CRM systems.
If this role sounds of interest, please apply using the link and we will endeavour to respond if your profile is suitable.
Show more
Show less","Data engineering, PySpark, Python, SQL, Airflow, DBT, Databricks, API development, JavaScript, Enterprise Resource Planning (ERP), CRM systems","data engineering, pyspark, python, sql, airflow, dbt, databricks, api development, javascript, enterprise resource planning erp, crm systems","airflow, api development, crm systems, data engineering, databricks, dbt, enterprise resource planning erp, javascript, python, spark, sql"
"Lead Data Software Engineer, London",Credera UK,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-software-engineer-london-at-credera-uk-3787316848,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"Would you like to be part of a fast-growing technology and engineering organisation that nurtures an open, collaborative learning environment? Are you passionate about technology, people and developing your coding skills? If this describes you, we’re trying to create the best engineering consultancy in the UK and would love for you to be part of the journey.
We draw on our reputation for outstanding delivery to allow our engineers to do the right thing for our clients, and not necessarily the easy thing. Innovation is in our DNA, and we encourage our engineers and consultants to work together to rethink conventional wisdom on how problems should be solved.
Our engineers take a holistic approach that addresses technology, process, and structure to make a lasting impact. At Credera, you will work on diverse client projects, such as building modern data platforms and services using DevOps practices, including large, distributed workloads, batch and streaming data pipelines as well as high-quality monitoring. Working with architects, technology consultants and client stakeholders, you will help customers make use of the data available to them while gaining exposure to different industries, networks and latest technologies.
Requirements
As an Engineer at Credera, you will spend roughly 80% of your time on client work, with the remainder of your time spent working on internal projects as well as training and development. Consultancy to us is about customer delivery first, carried out by technically excellent people. Our aim is not to pigeonhole but rather upskill our engineers so they can learn and expand their expertise, as well as deliver on a variety of business-critical client projects.
You are perceptive, personable, culturally sensitive and demonstrate a high degree of emotional intelligence. Being able to work collaboratively in a matrix organisation is essential, as is the ability to use cloud native products to build bespoke applications from scratch.
You will have:
Experience leading a team of 4+ engineers and acting as the technical owner a project
Demonstrable experience of building and deploying microservice APIs in Java or .NET
Experience creating and/or maintaining production software delivery pipelines using common CI/CD tools (GitHub Actions, Azure DevOps, Jenkins, etc.)
Demonstrable experience of building data pipelines with Python and SQL
Experience working with one or more of the main cloud providers (AWS, Azure or Google)
Experience working with data platforms such as Data Lakes or Data Warehouses
Have a drive for self-improvement and learning, including learning new programming languages
Approach solving problems pragmatically
Experience supporting and operating production systems
Desirable skills:
Experience of big data platforms (EMR, Databricks or DataProc)
Experience structuring data for optimal performance
Understanding of Data Security and Data Governance principles
Experience of building automated data quality checks and metrics
Experience with Infrastructure as Code (Terraform, CloudFormation, ARM templates etc.)
Benefits
Along with a great company culture, Credera provides an exceptional compensation package including a competitive salary and a comprehensive benefits plan. Our consistent growth and entrepreneurial environment provide an excellent platform to embark upon an exciting career path, where your contribution really counts. You can also expect:
A highly collaborative working environment and great rates of pay (including base salary and bonus potential)
A range of flexible benefits for your well-being and lifestyle
One-to-one mentoring and hands-on experience for continuous growth and development
Personalised learning and development opportunities with a £2,000 annual training budget per employee
25 days of holiday and the ability to flex up to 30 days
2 CSR volunteering days to give back to the community
Learn More
As a global consultancy with over 3000 consultants worldwide, and part of Omnicom Precision Marketing Group, we partner with industry-leading brands including FTSE 100 companies and government departments. Our mission is to create a positive impact for clients and our people, guided by our core values of integrity, tenacity, people-first, humility, and excellence. We prioritise well-being and offer diverse networks, such as LGBT+, CredCOLOUR, CredAbility, Women in Engineering, Asian Leader Circle, and Gender Diversity. Our commitment to well-being has earned us the Excellence in Well-Being Award 2022, and our inclusive culture has earned us the ""Great Place to Work"" award for five consecutive years.
Ready to achieve your vision? We’re here to help- contact us.
Show more
Show less","Cloud, Microservices, Java, .NET, CI/CD, Python, SQL, AWS, Azure, Google, Data Lakes, Data Warehouses, EMR, Databricks, DataProc, Data Security, Data Governance, Infrastructure as Code, Terraform, CloudFormation, ARM","cloud, microservices, java, net, cicd, python, sql, aws, azure, google, data lakes, data warehouses, emr, databricks, dataproc, data security, data governance, infrastructure as code, terraform, cloudformation, arm","arm, aws, azure, cicd, cloud, cloudformation, data governance, data lakes, data security, data warehouses, databricks, dataproc, emr, google, infrastructure as code, java, microservices, net, python, sql, terraform"
Data Engineer,Orbis,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-orbis-3779828472,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"One of Orbis's large Oil & Gas clients is looking for multiple Data Engineers, this project is within their trade & shipping domain so experience with this is a huge plus, no essential.
Strong experience of Azure Databricks is needed.
Hybrid working - Ideally 3 days p/week in the London HQ, 2 days remote working.
Requirements/more details:
Experience with the Azure Databricks
Power BI experience
Oil and Gas trading domain knowledge is a plus
Proficient in SQL queries
Hands on with Agile development methodology
Strong communication and stakeholder management skills
Contract:
6 month rolling contract
3 days p/week in London - 2 days remote
£450 - £475 p/day Inside IR35
Show more
Show less","Azure Databricks, Power BI, SQL, Agile development methodology, Communication, Stakeholder management","azure databricks, power bi, sql, agile development methodology, communication, stakeholder management","agile development methodology, azure databricks, communication, powerbi, sql, stakeholder management"
Data Engineer,Workonomics,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-workonomics-3783450724,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"TL;DR
💫
Company
| AI, ML, B2B, startup
📏
Size
| <10 people
📈
Stage
| Seed
🧢
Role
| Data Engineer
✨
Tech
| Python
📍
Based
| Central London (Zone 1)
💻
Hybrid
| Flexible, no set office days
💰
Offer
| up to £115k + share options
Hi 👋
Workonomics were recently recommended to the CEO a
London
-
based
AI
-
native
startup
. We're now partnering with their 3 Co-Founders to
establish their founding engineering team
.
Their platform unravels the complexities of public and private markets, to enable companies to easily
discover new information
, and
make smart
,
strategic business decisions
.
Leveraging advanced research in vector embeddings and unsupervised learning, their technology
organises markets from web
-
scale data
, and deciphers patterns that companies can use to
foster innovation
,
creativity
,
and economic growth
.
They're now looking for their
first Data Engineer
to help:
refactor their data pipeline to follow best practices, increasing test coverage, improving modularity and observability
integrate new end-to-end data sources to flow into their ML models, and surface in their application
design an architecture to incrementally refresh and capture data changes over time, to use for predictive analytics and recommendations
find novel solutions to optimise their stack, e.g. to increase match accuracy
The ideal candidate they have in-mind will:
be passionate about building game-changing data products
love writing clean, efficient, maintainable code to stand the test of time
keep up-to-date with open source tools, and be aware of their trade-offs
be able to parlay between high-level architecture / data modelling and rigorous low-level optimisations to unlock scale
This is a founding engineer role (i.e.
you’ll be 1 of the first 2 engineers
) but you won’t be setting up the initial codebase. Their CTO has already laid solid foundations. It’s actually a relatively mature product for its stage (18m development so far). The next step is to fine-tune the code in production and further commercialise the platform.
If you’re a
product
-
minded engineer
with a
passion for data engineering
, who enjoys
using the right tool for the job
, please hit apply for more insight on the company and role.
Show more
Show less","Python, Vector Embeddings, Unsupervised Learning, Data Engineering, Data Pipelines, Machine Learning Models, Predictive Analytics, Data Modelling, Data Optimization, Code Refactoring, Test Coverage, Modular Design, Observability, Stack Optimization, Match Accuracy","python, vector embeddings, unsupervised learning, data engineering, data pipelines, machine learning models, predictive analytics, data modelling, data optimization, code refactoring, test coverage, modular design, observability, stack optimization, match accuracy","code refactoring, data engineering, data modelling, data optimization, datapipeline, machine learning models, match accuracy, modular design, observability, predictive analytics, python, stack optimization, test coverage, unsupervised learning, vector embeddings"
Data Engineer,Intec Select,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-intec-select-3778512366,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"Data Integration Engineer - £45,000 - £55,000 (FTC) – 9 Months – London (Hybrid)
Overview:
A market leading organization operating within the health and wellness sector are looking for an experienced Data Integration Engineer to join their growing data team in London on a hybrid basis.
Role & Responsibilities:
Define, maintain, and improve Data Warehousing solutions
Build and maintain data pipelines and ensure products are robust, high performance and cost effective
Work with the Data Team providing advice and guidance to size effort, value and create solutions
Lead on data migrations projects
Support Data Science and Analytics teams and build a stakeholder base across the business
Produce and present prototypes for user evaluation in preparation for full scale development and deployment
Introduce continuous improvement, automation, and using technology to reduce effort
Analyse and document processes and requirements to inform the design of solutions
Technical Requirements:
Expert-level proficiency in SQL and Excel (essential) and a strong understanding of RDBMS
Strong experience developing and maintaining ETL flows
Hands-on experience with data visualisation tools (ideally PowerBI)
Experience of working with large data sets to derive insights & business value
Strong skills within theAzure Cloud Platform tools such as Azure Data Factory, Azure Data Lake, Synapse
Python (or other programming language) desirable
Package:
£45,000 - £55,000 (FTC)
Possible extension up to 24 months.
Hybrid Working
Data Integration Engineer - £45,000 - £55,000 (FTC) – 9 Months – London (Hybrid)
Show more
Show less","Data Warehousing, Data Pipelines, SQL, Excel, ETL, Data Visualization, PowerBI, Azure Cloud Platform, Azure Data Factory, Azure Data Lake, Synapse, Python, RDBMS","data warehousing, data pipelines, sql, excel, etl, data visualization, powerbi, azure cloud platform, azure data factory, azure data lake, synapse, python, rdbms","azure cloud platform, azure data factory, azure data lake, datapipeline, datawarehouse, etl, excel, powerbi, python, rdbms, sql, synapse, visualization"
Data Engineer,hackajob,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-hackajob-3780140648,2023-12-17,Greater London, United Kingdom,Mid senior,Hybrid,"hackajob is a matching platform partnering with Zoopla helping them to hire the best talent and build the future.
In order to be considered for the role you have to fill out the form by clicking ""Apply""
Benefits:
Everyday Flex - greater flexibility over where and when you work
25 days annual leave + extra days for years of service
Day off for your birthday, house move, good deed day, and digital detox day
Cycle to work and electric car schemes
Free Calm App membership
Enhanced Paternity Leave
Fertility Treatment Financial Support
Group Income Protection and private medical insurance
Gym on-site in London – or membership in regional offices
7.5% pension contribution by the company
Discretionary annual bonus up to 10% of base salary
Talent referral bonus up to £5K
Essential Skills:
A passionate engineer with previous commercial data engineering experience
Able to support and mentor other team members
Comfortable working in a dynamic environment with a certain degree of uncertainty
Comfortable working with SQL / NoSQL
Comfortable learning new technologies, languages, and tools on the job, to ensure that the product is not left behind in a rapidly evolving ecosystem
A positive, collaborative mindset and a desire to deliver real business value to the customer
An ability to challenge us! We want people who can come in and shape the future of this business, not afraid to raise questions and help us improve.
Technical skills:
Experience using Amazon Web Services (Glue, Redshift, Lambda, Step Functions) (essential)
Terraform (essential)
Python, Spark (essential)
Kafka (desirable)
Docker / ECS / Kubernetes (desirable)
DBT (desirable)
Experience with a modern orchestration tool - EG - Airflow, Dagster, Prefect (desirable)
Experience running machine learning pipelines in production (desirable)
Show more
Show less","Commercial data engineering, Mentorship, SQL, NoSQL, Python, Spark, Terraform, AWS Glue, AWS Redshift, AWS Lambda, AWS Step Functions, Kafka, Docker, ECS, Kubernetes, DBT, Airflow, Dagster, Prefect, Machine learning pipelines","commercial data engineering, mentorship, sql, nosql, python, spark, terraform, aws glue, aws redshift, aws lambda, aws step functions, kafka, docker, ecs, kubernetes, dbt, airflow, dagster, prefect, machine learning pipelines","airflow, aws glue, aws lambda, aws redshift, aws step functions, commercial data engineering, dagster, dbt, docker, ecs, kafka, kubernetes, machine learning pipelines, mentorship, nosql, prefect, python, spark, sql, terraform"
Data Conversion Analyst (Workday),eSense Incorporated,"West Green, GA",https://www.linkedin.com/jobs/view/data-conversion-analyst-workday-at-esense-incorporated-3770686729,2023-12-17,Georgia,United States,Associate,Onsite,"Primary Duties & Responsibilities
Develop/test/execute data conversion extractions, transformations, and processes within an established
conversion framework in a timely and accurate manner.
Develop and maintain data conversion documentation.
Write and maintain automation scripts as needed.
Verify data integrity and identify data cleanliness issues.
Reconcile converted data to ensure accuracy.
Communicate effectively with management.
Develop good working relationships with departmental staff in order to troubleshoot technical and
functional issues that may arise during the implementation process.
Skills And Competencies
Solid grasp of relational database structures and strong use of SQL
Familiarity with a variety of database types and interfaces (Microsoft Access, Oracle, various text formats,
ODBC).
Solid understanding and usage of Microsoft Excel
Strong troubleshooting skills and analytical thinking to resolve issues.
Positive, proactive, take-charge attitude and attention to detail.
Strong time management skills with the ability to meet deadlines for multiple, concurrent projects.
Ability to quickly gain working knowledge of product, interpret requirements, and recommend solutions.
Excellent verbal and written communication skills.
Minimum Qualifications
Bachelor's degree in computer science or a related field from an accredited college or university AND One year of job-related experience.
Preferred Qualifications
Preference may be given to applicants who, in addition to meeting the Minimum Qualifications, possess the following:
Experience with PeopleSoft, Oracle, Taleo, Workday or other SaaS and On-Premises ERP systems.
Experience in Human Resource, Financial, or Procurement domains
Can work in a fast paced, fast-growth, high-energy environment and take care of multiple high priority
activities concurrently.
Ability to balance multiple projects and initiatives simultaneously.
Standout colleague who can collaborate and communicate optimally with all partners, i.e. developers,
technical operations, and customers.
Possess excellent verbal and written communication skills.
Passion for data accuracy and completeness.
Show more
Show less","Data conversion, Data extraction, Data transformation, Data processing, SQL, Relational databases, Microsoft Access, Oracle, ODBC, Text formats, Microsoft Excel, Troubleshooting, Analytical thinking, Communication skills, Time management, Project management, Problemsolving, PeopleSoft, Oracle, Taleo, Workday, SaaS, ERP systems, Human resources, Financial, Procurement","data conversion, data extraction, data transformation, data processing, sql, relational databases, microsoft access, oracle, odbc, text formats, microsoft excel, troubleshooting, analytical thinking, communication skills, time management, project management, problemsolving, peoplesoft, oracle, taleo, workday, saas, erp systems, human resources, financial, procurement","analytical thinking, communication skills, data conversion, data extraction, data processing, data transformation, erp systems, financial, human resources, microsoft access, microsoft excel, odbc, oracle, peoplesoft, problemsolving, procurement, project management, relational databases, saas, sql, taleo, text formats, time management, troubleshooting, workday"
Sr. Data Engineer (PASA),TechTammina LLC,"Peachtree City, GA",https://www.linkedin.com/jobs/view/sr-data-engineer-pasa-at-techtammina-llc-3745378296,2023-12-17,Georgia,United States,Mid senior,Onsite,"Role:
Sr. Data Engineer (PASA)
Location:
Hybrid: Mon & Thur onsite in either Peachtree City, GA or Boston, MA
The Team Manager is located in Boston, MA but his team sits out of Peachtree City, GA.
It would be nice if we could find someone local to either Peachtree, GA or Boston, MA, but if not, then 100% remote work is ok for EST candidates.
Rate:
Market
Duration: Long term
Manager Notes
Manager is looking for 15+ years of exp.
Manager is looking for “Data Expiration”
Data modeling is a must have
This person will grow into a Data Architect role and will be the backbone of the entire team
It would be great to have “Auto and Manufacturing”
Must have conceptual experience; not just the background
Should be able to explain clearly how they came up with the solution
The manager comes with 25yrs of Data space and will ask detailed questions during the interview process.
Senior Data Engineer is responsible for building and maintaining the data infrastructure that enables an organization to efficiently collect, process, store, and analyze data for informed decision-making.
Their role is critical in ensuring data availability, quality, and reliability throughout the data lifecycle.
Accountabilities
Data Pipeline Architecture Design: responsible for designing and architecting data pipelines that move, transform, and process data from various sources to target data storage or analytics platforms and need to ensure scalability, reliability, and efficiency in these architectures.
Data Modeling: They design and implement data models that suit the requirements of the project and structuring data to support efficient querying and analysis.
ETL (Extract, Transform, Load) Processes: Building robust ETL processes is a key responsibility.
Senior Data Engineers design and develop ETL workflows that extract data from source systems, transform it into the desired format, and load it into Azure Data Lake.
Data Integration: integrating data from different sources, which may include databases, APIs, third-party services, and more.
Integration involves handling data in various formats and ensuring data consistency and accuracy.
Data Quality and Monitoring: implement data quality checks and monitoring mechanisms to identify and rectify data quality issues.
Also create alerts and notifications for anomalies in data processing.
Performance Optimization: responsible for optimizing the performance of data pipelines and processing systems.
This includes fine-tuning queries, optimizing data storage, and managing resources effectively to ensure timely data processing.
Data Security: Ensuring data security and implement security measures to protect sensitive data and ensure proper access controls.
Collaboration: collaborate with cross-functional teams, including data analysts, business analysts and software engineers, to understand data requirements and provide the necessary guidelines to support their work.
Technology Selection: assess and select appropriate tools, technologies, and frameworks for different aspects of the data engineering process and stay up to date with industry trends and emerging technologies.
Leadership and Mentoring: As senior members of the team, should be able take on leadership roles by guiding and mentoring junior data engineers.
Participate in code reviews, best practice discussions, and training sessions.
Troubleshooting and Support: When issues arise in data pipelines or systems, Senior Data Engineer is responsible for diagnosing and resolving these issues in a timely manner to minimize disruptions.
Documentation: document data pipelines, workflows, architecture designs, and other relevant processes to ensure knowledge sharing and maintainable systems.
Scope And Competency Requirements
Problem-Solving/Know How: (complexity of problems and education/experience & other knowledge required)
A bachelor’s or master’s degree in computer science, information technology, data science, or a related field with 7+ years of experience as a data engineer, with a significant portion of that experience specifically working with Azure data services.
Proficiency in Microsoft Azure services and tools related to data engineering, such as Azure Data Factory, Azure Databricks, Azure Analytical Service, Azure Synapse Analytics, Azure Cosmos DB, etc.
Strong Experience in implementing data solutions in Azure Data Lake platform
Experience with Data Governance tools like Azure Purview, Alation.etc are preferred.
Expertise in SQL, T-SQL and experience working with various database systems, including relational databases and NoSQL databases is required.
Experience with data modeling, schema design, and performance optimization, especially creating tabular models for Self-service analytics using Power BI.
Strong skills in data integration, ETL (Extract, Transform, Load) processes, and data orchestration workflows.
Expertise in data warehousing concepts and techniques.
Programming skills in languages such as Python, PySpark, Java, Scala, or PowerShell for scripting and automation.
Experience designing, building, and optimizing end-to-end data pipelines.
Proven track record of implementing data solutions that meet performance, scalability, and reliability requirements.
Experience with data migration, data warehousing, and data lake solutions.
Experience in creating data pipelines to extract the data from SAP applications is preferred.
The ability to analyze complex data engineering challenges and develop innovative solutions.
Strong troubleshooting skills to identify and resolve issues in data pipelines and systems.
Effective communication skills to work collaboratively with cross-functional teams, including data scientists, analysts, and business stakeholders.
The ability to translate technical concepts into understandable terms for non-technical team members.
Data accuracy and quality are paramount.
A Senior Azure Data Engineer should be meticulous in ensuring that data pipelines and processes maintain high data integrity.
As a senior role, this individual might be required to lead and mentor junior members of the data engineering team.
Attributes/Skills
Functional knowledge and ability of Systems and Software.
Must be a capable team player and able to adapt to various project requirements.
Must be able to use methods appropriate to the position and assignment to solve problems and achieve accurate results.
Excellent organizational skills for multi-tasking.
Demonstrated ability to work in a cross-functional, team oriented environment.
Must have the ability to remain organized and self directed in a fast-paced environment.
Must have ability to analyze, interpret, and communicate related data, including analytical data.
Must have the ability to develop solutions for problems or systems.
Must be able to work with minimal supervision displaying strong self-motivation.
Must have the ability to learn computer applications as necessary for the position.
Must successfully complete any training or certification courses required to support or strengthen position.
Must have the ability to read, interpret, and follow verbal and written instructions, work procedures and other related materials.
Communication
Persuasive, professional communication style.
Must possess strong verbal and written communication skills including strong customer service skills.
Must communicate appropriate information accurately and clearly as needed including strong presentation skills.
Must have the ability to effectively and appropriately interface and communicate with a variety of individuals or groups on a frequent basis.
Show more
Show less","Azure Data Factory, Azure Databricks, Azure Analytical Service, Azure Synapse Analytics, Azure Cosmos DB, Azure Purview, Alation, SQL, TSQL, Python, PySpark, Java, Scala, PowerShell, Data modeling, Schema design, Performance optimization, Data integration, ETL (Extract Transform Load), Data orchestration workflows, Data warehousing, Data migration, Data lake solutions, SAP applications, Troubleshooting, Crossfunctional collaboration, Data accuracy, Data quality, Leadership, Mentoring, Systems and Software, Team player, Organizational skills, Selfdirection, Data analysis, Problemsolving, Selfmotivation, Computer applications, Training and certification, Communication, Customer service","azure data factory, azure databricks, azure analytical service, azure synapse analytics, azure cosmos db, azure purview, alation, sql, tsql, python, pyspark, java, scala, powershell, data modeling, schema design, performance optimization, data integration, etl extract transform load, data orchestration workflows, data warehousing, data migration, data lake solutions, sap applications, troubleshooting, crossfunctional collaboration, data accuracy, data quality, leadership, mentoring, systems and software, team player, organizational skills, selfdirection, data analysis, problemsolving, selfmotivation, computer applications, training and certification, communication, customer service","alation, azure analytical service, azure cosmos db, azure data factory, azure databricks, azure purview, azure synapse analytics, communication, computer applications, crossfunctional collaboration, customer service, data accuracy, data integration, data lake solutions, data migration, data orchestration workflows, data quality, dataanalytics, datamodeling, datawarehouse, etl extract transform load, java, leadership, mentoring, organizational skills, performance optimization, powershell, problemsolving, python, sap applications, scala, schema design, selfdirection, selfmotivation, spark, sql, systems and software, team player, training and certification, troubleshooting, tsql"
Associate Data Engineer,Qcells North America,"Cartersville, GA",https://www.linkedin.com/jobs/view/associate-data-engineer-at-qcells-north-america-3714584859,2023-12-17,Georgia,United States,Mid senior,Onsite,"Summary
The Associate Data Engineer will build and operate a data pipeline for the solar product manufacturing line and conduct end-to-end analysis from data gathering, data processing, analysis and visualization.
Responsibilities
Define and extract a high volume of manufacturing data from multiple sources and integrate data into a target database, application, or file using efficient programming processes
Design and develop data visualizations to convey information to engineers and technicians
Develop and implement scripts for ETL process maintenance, monitoring, and performance tuning
Collaborate with cross-functional teams to resolve data quality and operational issues
Maintain existing data pipelines and visualizations enhancement requests
Required Qualifications
Bachelor’s degree in a quantitative discipline (e.g. Computer Science, Statistics, Industrial Engineering, Management Information Systems or a related field)
3+ years of prior experience as a data engineer or similar role
Extensive experience working with various data sources (Oracle, SQL database)
Experience in web applications tools (Java, Spring Boot, etc.)
Deep understanding of Object-Oriented Programming concept
Excellent analytical, problem-solving, and written/verbal communication skills
Ability to work independently and collaboratively in a team environment
Preferred Qualifications
Master’s degree in a quantitative discipline (e.g. Computer Science, Statistics, Industrial Engineering, Management Information Systems or a related field)
5+ years of prior experience as a data engineer or a similar role
Experience in manufacturing industry such as solar PV or semiconductors
Proficiency in data visualization tools (Spotfire, Tableau, PowerBI, etc.)
Experience in using statistical packages (Numpy, Scipy, Scikit-learn) and Web framework (Django, Flask) in Python
Experience in the big data technologies (Hadoop, Spark, Kafka, etc.)
Hanwha Q CELLS provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics.
Show more
Show less","Data Pipeline, Data Visualization, ETL, Java, Spring Boot, ObjectOriented Programming, Oracle, SQL, Spotfire, Tableau, PowerBI, Numpy, Scipy, Scikitlearn, Django, Flask, Python, Hadoop, Spark, Kafka","data pipeline, data visualization, etl, java, spring boot, objectoriented programming, oracle, sql, spotfire, tableau, powerbi, numpy, scipy, scikitlearn, django, flask, python, hadoop, spark, kafka","data pipeline, django, etl, flask, hadoop, java, kafka, numpy, objectoriented programming, oracle, powerbi, python, scikitlearn, scipy, spark, spotfire, spring boot, sql, tableau, visualization"
Data Analyst,eTeam,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-analyst-at-eteam-3783146547,2023-12-17,Georgia,United States,Mid senior,Onsite,"Data Analyst
Location: Atlanta, GA- Hybrid 2-3 days/wk onsite but this is flexible
12 Months contract
Pay Rate: $51.47/hr on w2
A Data Analyst will play a key role, responsible for (1) custom analytics execution for our stakeholders across the system (incl. Bottlers and Customers); and (2) PowerBI development/engagement with our Freestyle Connected Insights capability. This is a wonderful opportunity for talent with a Data-oriented growth path to expand their breadth of TCCC data knowledge and gain exposure across the system. The ideal candidate should have a strong analytics background, a tech-savvy and entrepreneurial spirit, and be comfortable working collaboratively with other teams.
What You Do For Us:
Execute custom analytics on our proprietary data for Customers, Bottlers, NAOU and EOU account teams, and Functional stakeholders.
Develop new reports and maintain/update existing reports for our Freestyle Connected Insights (PowerBI) capabilities.
Lead Client Test & Learn program (statistical testing and business optimization) for Freestyle Marketing, Commercial, Operations, and Sales-related activations and experiments.
Qualifications & Requirements:
Strong fundamental analytics knowledge (financial analysis, testing, marketing analytics)
Experience working with big data and building data visualizations, dashboards, and datasets using BI tools (PowerBI, Alteryx)
Great teammate with the ability to work cross-functionally in resolving complex issues and take a new perspective on existing solutions
Curious data innovator with a passion for experimenting and iterating with emerging approaches and ideas
Tech Plusses: AWS, Azure, and PowerPlatform; SQL and Python; Client Test & Learn for Sites
Functional Skills
Analytics
Data Storytelling
Data Product Management
Show more
Show less","Data Analyst, PowerBI, Alteryx, AWS, Azure, PowerPlatform, SQL, Python, Client Test & Learn, Analytics, Data Storytelling, Data Product Management","data analyst, powerbi, alteryx, aws, azure, powerplatform, sql, python, client test learn, analytics, data storytelling, data product management","alteryx, analytics, aws, azure, client test learn, data product management, data storytelling, dataanalytics, powerbi, powerplatform, python, sql"
Sr. Cloud Database Engineer/ Sr. Data Engineer,Right Fit Advisors,"Georgia, United States",https://www.linkedin.com/jobs/view/sr-cloud-database-engineer-sr-data-engineer-at-right-fit-advisors-3784512193,2023-12-17,Georgia,United States,Mid senior,Onsite,"Sr Data Engineer - Cloud Specialist
Location: Atlanta Metropolitan Area
Employment Type: Hybrid (Office and Remote)
Salary: $150,000 - $180,000, with benefits and 401(k) package.
Company Overview
Our client is a leading HealthTech company based in the heart of the Atlanta Metropolitan Area. They are on an exciting journey to modernize thier data infrastructure and are seeking a seasoned Sr Data Engineer focusing on cloud technologies to spearhead this transformation.
Role Overview
The Data Engineer will be a critical player in our team, driving the transition of our data infrastructure to the cloud. As part of this role, you'll work closely with data scientists, analysts, and business stakeholders to ensure our cloud data solutions are robust, scalable, and aligned with business objectives.
Key Responsibilities
Architect, design, and implement end-to-end cloud-based data solutions.
Collaborate with cross-functional teams to define and implement best practices and standards related to cloud data technologies.
Migrate existing data processes and systems to the cloud.
Optimize and enhance their cloud data infrastructure's performance, scalability, and reliability.
Maintain documentation and provide training to internal teams on cloud data technologies and best practices.
Qualifications
8 years of professional experience in data engineering, with a significant focus on cloud platforms such as AWS, Google Cloud Platform, or Azure.
Proficiency in programming languages, such as Python, Java, or Scala.
Strong knowledge of SQL and NoSQL databases.
Experience with big data technologies like Hadoop, Spark, or similar frameworks.
Proven ability to design and implement scalable, reliable, and performance-efficient solutions in the cloud.
Exceptional communication and collaboration skills.
What's In It For You
Competitive salary within the range of $150,000 - $180,000.
Comprehensive benefits package, including health, dental, and vision insurance.
Generous 401(k) retirement plan.
The opportunity to work on impactful projects that shape the company's future.
A collaborative, dynamic, and innovation-driven work environment.
Ongoing professional and personal development opportunities.
﻿
If you're ready to be a driving force in our cloud transition and align with the qualifications listed, we want to hear from you! Apply today and be a part of our exciting journey.
Show more
Show less","AWS, Azure, Google Cloud Platform, Hadoop, Java, NoSQL, Python, Scala, Spark, SQL","aws, azure, google cloud platform, hadoop, java, nosql, python, scala, spark, sql","aws, azure, google cloud platform, hadoop, java, nosql, python, scala, spark, sql"
Senior Data Engineer,Professional Diversity Network,"Georgia, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-professional-diversity-network-3788308740,2023-12-17,Georgia,United States,Mid senior,Onsite,"Responsibilities
The Senior Data Engineer responsibilites include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of Seattle Children's Enterprise Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatement, etc.
Requirements
Required Education/Experience: - Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competency - Minimum of five (5) years technology industry or related experience, including items such as: - Build highly scalable, scaled-out architectures on large scale database platforms - Experience working in a complex data infrastructure environment - Five (5) years of experience in a data engineering role - Extensive and in depth data pipeline development experience with industry standard data integration tools - Advanced competency in SQL with ability to perform query optimization in large scale database platforms - Experience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support. - Experience with any industry standard tool for Source Control and Project Management - Experience wrting test cases and test scripts for data quality assurance - Experience creating stored procedures and functions - Experience developing dimensional data model with any industry standard tool. Required Credentials: - N/A Preferred: - Experience in Healthcare or related industry - Experience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plus - Experience productizing/automating predictive models that use R, SAS, Python, SPSS, etc. - Continuous delivery and deployment automation for analytic solutions - Familiarity with Agile framework and test driven development methodology for analytic solutions - API development - Data visualization and/or dashboard development
Min to Max Hourly Salary
$59.18 - $88.77 /hr
Min to Max Annual Salary
$123,094.00 - $184,640.00 /yr
Salary Information
This compensation range was calculated based on full-time employment (2080 hours worked per calendar year). Offers are determined by multiple factors including equity, skills, experience, and expertise, and may vary within the range provided.
The union pay ranges can be found on the Seattle Children's website here: WSNA - UFCW
Seattle Children's offers annual incentive pay based upon performance that is commensurate with the level of the position.
Disclaimer for Out of State Applicants
This compensation range is specific to Seattle, positions located outside of Seattle may be compensated differently depending on various factors
.
Benefits Information
Seattle Children's offers a generous benefit package, including medical, dental, and vision plans, 403(b), life insurance, paid time off, tuition reimbursement, and more. Click here for more information.
About Us
Hope. Care. Cure. These three simple words capture what we do at Seattle Children's -- to help every child live the healthiest and most fulfilling life possible. Are you ready to engage with a mission-driven organization that is life-changing to many, and touches the hearts of all? #HOPECARECURE
Our founding promise to the community is as valid today as it was over a century ago: we will care for all children in our region, regardless of the families' ability to pay. Together, we deliver superior patient care, advance new discoveries and treatments through pediatric research, and serve as the pediatric and adolescent, academic medical center for Washington, Alaska, Montana and Idaho - the largest region of any children's hospital in the country.
In 2022, U.S. News & World Report once again ranked Seattle Children's among the nation's best children's hospitals - for the 30th year in a row. For more than a decade, Seattle Children's has been nationally ranked in all 10 specialty areas evaluated by U.S. News & World Report. We are honored to be the top-ranked pediatric hospital in Washington and the Pacific Northwest.
As a Magnet designated institution, and classified among America's best large employers by Forbes, we recognize the importance of hiring and developing great talent to provide best-in-class care to the patients and families we serve. Our organizational DNA takes form in our core values: Compassion, Excellence, Integrity, Collaboration, Equity and Innovation. Whether it's delivering frontline care to our patients in a kind and caring manner, practicing the highest standards of quality and safety, or being relentlessly curious as we work towards eradicating childhood diseases, these values are the fabric of our culture and community. The future starts here.
Our Commitment to Diversity
Our community welcomes diverse experiences, backgrounds, and thoughts as this is what drives our spirit of inquiry and allows us to better connect with our increasingly diverse patients and families. Our organization recruits, employs, trains, compensates, and promotes without regard to race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
The people who work at Seattle Children's are members of a community that seeks to respect and celebrate all the qualities that make each of us unique. Each of us is empowered to be ourselves within this community, which cultivates and promotes equity, diversity, and inclusion at all levels.
Seattle Children's is proud to be an Equal Opportunity Workplace and Affirmative Action Employer.
PDN-9a3eeb2f-9713-4914-ab24-ccd4bbda9ba1
Show more
Show less","Data Integration, Data Pipeline Development, Data Visualization, Data Warehousing, Agile Methodology, SQL, Source Control, Project Management, Test Driven Development, API Development, Dimensional Data Modeling, R, SAS, Python, SPSS, Continuous Delivery, Deployment Automation, Netezza, Datastage, BitBucket, JIRA, Confluence","data integration, data pipeline development, data visualization, data warehousing, agile methodology, sql, source control, project management, test driven development, api development, dimensional data modeling, r, sas, python, spss, continuous delivery, deployment automation, netezza, datastage, bitbucket, jira, confluence","agile methodology, api development, bitbucket, confluence, continuous delivery, data integration, data pipeline development, datastage, datawarehouse, deployment automation, dimensional data modeling, jira, netezza, project management, python, r, sas, source control, spss, sql, test driven development, visualization"
"Sr. Engineer, Database Infrastructure - Slack",Slack,"Georgia, United States",https://www.linkedin.com/jobs/view/sr-engineer-database-infrastructure-slack-at-slack-3760633104,2023-12-17,Georgia,United States,Mid senior,Remote,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Software Engineering
Job Details
About Salesforce
We’re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good – you’ve come to the right place.
Slack's DataStores team builds and operates the database platform powering Slack. We write software to manage thousands of stateful hosts, providing several petabytes of online database capacity. We are building one of the fastest-growing database platforms in the world. Our MySQL databases run in Vitess. You can read more about our migration to Vitess at: Scaling Datastores at Slack with Vitess
Background
Slack enables people all over the world to communicate and collaborate together. Teams of all scales — from the world’s largest public companies to the smallest of startups — use Slack to get work done, so we take performance and reliability very seriously. A taste of our scale:
The average user spends over 10 hours connected and 2.5 hours active in Slack every single day
1.5 billion+ messages are sent per month, half of those outside the United States
Every day we see over 10M+ daily active users, over a billion web requests, and 10s of billions of database queries.
For millions of people, Slack is the primary communication tool they use at work all day long. They expect it to be exceptionally reliable and fast, all the time.
Infrastructure at Slack
We operate at tremendous scale with systems that process millions of events per second. Teams in our group maintain and build the lower levels of our stack, including:
Edge services
Data Stores and Caches
Real-time messaging
Asynchronous background job processing
We know we’ve done our job correctly when
none of our users think about us.
We don’t typically ship new user-facing features, but rather ensure our systems are incredibly performant, highly available, reliable, and scalable. In other words, Slack just works seamlessly.
Slack's API and web backend is built on PHP/Hack, our backend services are written in Java and Go, and we use Vitess as our storage engine. Our architecture is constantly evolving to handle millions more users. You can read about how we scaled our datastores with Vitess, how we respond to incidents, and much more on our blog.
What you will be doing:
Operate and enhance our large, highly-available database infrastructure, utilizing technologies such as MySQL and Vitess.
Develop tools to enable self-service and self-managing capabilities of our database infrastructure so that other teams can operate full-stack while rapidly building new features for our customers.
Collaborate with engineering teams on their database storage needs, and advise them throughout the development lifecycle.
Write code to capture database performance, and create tools and dashboards to provide actionable insight into that data.
Participate in our on-call rotation and collaborate with our operations team to triage and resolve production issues.
Support FedRAMP and DoD SRG activities.
What you should have:
Have been working in data storage, core infrastructure, or distributed system-owning teams with increasing responsibilities for 5+ years.
Have professional experience using Go, PHP/Hacklang, Python, Ruby, or Java.
Write code that can be easily understood by others with an eye towards clarity and maintainability. Collaborate with other teams to integrate new features of your platform or adopt self-service features.
Operated at least one distributed system, at scale and in a team environment. Some examples include: a relational database like MySQL/Postgres, or systems like Kafka, Cassandra, or ElasticSearch.
Deployed server software on Linux, and then operated it at scale. You’ve debugged its problems, and analyzed and optimized its performance.
Have experience operating cloud infrastructure, especially AWS.
Are familiar with deployment automation/configuration management tools like Chef, Ansible, Puppet, or Terraform.
Are a very strong communicator. You’re excited to explain complex technical concepts and share your knowledge with different audiences.
Are curious how things work; when they break you are eager and able to help fix them.
Infrastructure is a diverse and inclusive team that treats their colleagues exceptionally well. We are happy to help you learn what you need to know; we encourage and support each other’s growth and thus it’s not expected that you would have expertise across all of these areas.
Come join us!
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com .
Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce .
﻿Salesforce welcomes all.
For Colorado-based roles, the base salary hiring range for this position is $156,800 to $215,600.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.
Show more
Show less","PHP, Python, Ruby, MySQL, Postgres, Go, AWS, ElasticSearch, Kafka, Hadoop, Chef, Ansible, Puppet, Terraform, Linux, Java, Vitess, Slack, CRM, DevOps, MySQL, Cassandra, Hadoop, Datastores","php, python, ruby, mysql, postgres, go, aws, elasticsearch, kafka, hadoop, chef, ansible, puppet, terraform, linux, java, vitess, slack, crm, devops, mysql, cassandra, hadoop, datastores","ansible, aws, cassandra, chef, crm, datastores, devops, elasticsearch, go, hadoop, java, kafka, linux, mysql, php, postgres, puppet, python, ruby, slack, terraform, vitess"
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"Georgia, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-engineering-at-acv-auctions-3762878256,2023-12-17,Georgia,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, NoSQL, MongoDB, AWS RDS, AWS S3, AWS SQS, AWS SNS, DBT, Airflow, ETL, SSIS, C#, Python, Unit testing, Integration testing, Data modeling, Data storage, Message brokers, Protocols, Interfaces, Documentation generation, Debugging, Troubleshooting, Mentoring, English language","sql, nosql, mongodb, aws rds, aws s3, aws sqs, aws sns, dbt, airflow, etl, ssis, c, python, unit testing, integration testing, data modeling, data storage, message brokers, protocols, interfaces, documentation generation, debugging, troubleshooting, mentoring, english language","airflow, aws rds, aws s3, aws sns, aws sqs, c, data storage, datamodeling, dbt, debugging, documentation generation, english language, etl, integration testing, interfaces, mentoring, message brokers, mongodb, nosql, protocols, python, sql, ssis, troubleshooting, unit testing"
Senior Cloud Data Engineer,BDO USA,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765470292,2023-12-17,Georgia,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, SQL, Data Definition Language, Data Manipulation Language, Views, Functions, Stored Procedures, Performance Tuning, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, UiPath, Alteryx, Computer Vision, Tableau, .Net, Qlik, Azure Data Factory, RedShift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, SSIS, SSAS, SSRS","data analytics, business intelligence, artificial intelligence, application development, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, sql, data definition language, data manipulation language, views, functions, stored procedures, performance tuning, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, uipath, alteryx, computer vision, tableau, net, qlik, azure data factory, redshift, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql, ssis, ssas, ssrs","ai algorithms, alteryx, application development, artificial intelligence, automation tools, aws lake formation, azure analysis services, azure data factory, batch data ingestion, bicep, business intelligence, c, cloud data analytics, computer vision, data definition language, data lake medallion architecture, data manipulation language, data ops, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops, functions, git, java, kinesis, linux, machine learning, microsoft fabric, net, pandas, performance tuning, powerbi, purview, python, qlik, quicksight, redshift, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, stored procedures, streaming data ingestion, tableau, tabular modeling, terraform, uipath, views"
Data Engineer,Equifax,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-engineer-at-equifax-3763037072,2023-12-17,Georgia,United States,Mid senior,Hybrid,"Equifax is where you can power your possible. If you want to achieve your true potential, chart new paths, develop new skills, collaborate with bright minds, and make a meaningful impact, we want to hear from you.
Equifax is searching for a career level Data Engineer to join our world-class Entity Resolution (Keying & Linking) team within the Global Data & Analytics CoE.
The ideal candidate is a rare hybrid, an engineer with the programming abilities to scrape, combine, andmanage data from a variety of sources and a statistician who knows how to derive insights from the information within. He or she will combine the skills to create new prototypes with the creativity and thoroughness to ask and answer the deepest questions about the data. Qualified candidates will have a strong academic background in mathematics or statistics, passion for data science, data mining and machine learning.
What you’ll do
Manage data analysis to develop fact-based recommendations for in house projects.
Profile big data and develop in depth analytics on the impact of new rules for Entity Resolution
Collaborate with Technology, Security, Data Privacy, Data Governance, and IT Operation teams to support the movement, scanning and analyzing of data assets migrated to GCP
Conceptualize, analyze, and develop actionable recommendations for strategic challenges.
5+ years’ experience working with Data Schemas and Data formats such as JSON, Parquet, AVRO etc
Present results and recommendations to internal and external customers
What Experience You Need
Bachelor’s degree in Computer Science, Statistics, or other engineering fields
Minimum of 5 years professional experience (that must include 3 years experience in Data Engineer, Data Wrangler, and/or similar positions.
Minimum of 2 years experience performing analysis using Google BigQuery, BigTable, and other Google Cloud Platform technologies, Google DataFlow, Scala+ Spark or PySpark is a plus.
Minimum of 2 experience in virtualized or public cloud platforms, ideally Google Cloud Platform, Amazon Web Services or Microsoft Azure.
Excellent problem solving skills with the ability to design algorithms (this experience may include data cleaning, data mining, data clustering and pattern recognition methodologies)
Expertise in relational databases, NoSQL (or related technologies), SQL
At least one general purpose programming such as Python, Scala, is required.
Agile development including Scrum and other lean techniques is a plus.
What could set you apart
Master’s degree (preferred) in Computer Science, Data Science, Analytics or Statistics or other engineering fields
Strong knowledge of credit bureau data
Experience in Entity Resolution concepts, prototype or hand-on project.
Graph database, AI/ML tools usage to solve large scale business problems
Experience in Data Visualization and storytelling
We offer comprehensive compensation and healthcare packages, 401k matching, paid time off, and organizational growth potential through our online learning platform with guided career tracks.
Are you ready to power your possible? Apply today, and get started on a path toward an exciting new career at Equifax, where you can make a difference!
Equifax is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","Data Engineer, Data Science, Data Mining, Machine Learning, Data Schemas, Data Formats (JSON Parquet AVRO), Google BigQuery, Google BigTable, Google Cloud Platform, Google DataFlow, Scala, Spark, PySpark, Virtualized or Public Cloud Platforms, Google Cloud Platform, Amazon Web Services, Microsoft Azure, Problem Solving, Algorithm Design, Data Cleaning, Data Clustering, Pattern Recognition, Relational Databases, NoSQL, SQL, Python, Scala, Agile Development, Scrum, Lean Techniques, Credit Bureau Data, Entity Resolution Concepts, Graph Database, AI/ML Tools, Data Visualization, Storytelling","data engineer, data science, data mining, machine learning, data schemas, data formats json parquet avro, google bigquery, google bigtable, google cloud platform, google dataflow, scala, spark, pyspark, virtualized or public cloud platforms, google cloud platform, amazon web services, microsoft azure, problem solving, algorithm design, data cleaning, data clustering, pattern recognition, relational databases, nosql, sql, python, scala, agile development, scrum, lean techniques, credit bureau data, entity resolution concepts, graph database, aiml tools, data visualization, storytelling","agile development, aiml tools, algorithm design, amazon web services, credit bureau data, data cleaning, data clustering, data formats json parquet avro, data mining, data schemas, data science, dataengineering, entity resolution concepts, google bigquery, google bigtable, google cloud platform, google dataflow, graph database, lean techniques, machine learning, microsoft azure, nosql, pattern recognition, problem solving, python, relational databases, scala, scrum, spark, sql, storytelling, virtualized or public cloud platforms, visualization"
Data Engineer,Great Dane,"Savannah, GA",https://www.linkedin.com/jobs/view/data-engineer-at-great-dane-3736894726,2023-12-17,Georgia,United States,Mid senior,Hybrid,"****This position is not contract-to-contract hire*********
The Data Engineer is responsible for expanding and optimizing our current data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is experienced in both data pipeline creation and data transformation. The Data Engineer will support our software developers, system architects, data analysts and Business Analysts on all data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s current data architecture to support our next generation of products and data initiatives.
Responsibilities for Data Engineer
● Create and maintain optimal data pipeline architecture
● Assemble complex data sets that meet functional / non-functional business requirements.
● Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
● Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
● Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
● Keep data separated and secure across all platforms used.
● Work with data and business experts to strive for greater functionality in our data systems.
Qualifications for Data Engineer
● Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
● Experience building and optimizing data pipelines architectures and data sets.
● Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
● Strong analytic skills related to working with unstructured datasets.
● Build processes supporting data transformation, data structures, metadata, dependency and workload management.
● A successful history of manipulating, processing and extracting value from disconnected datasets.
● Project management and organizational skills.
● Experience supporting and working with cross-functional teams in a dynamic environment.
● We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Bachelor degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:
○ Experience with relational SQL databases, including MS SQL server, Oracle, DB2, and Maria.
○ Experience with Data Cloud platforms like SnowFlake and or Data Bricks.
○ Experience with data pipeline and workflow management tools: SQDR, Airflow, Fivetran, Airbyte, etc.
○ Experience with object-oriented/object function scripting languages: Python, Java
Physical Demands/Work Environment:
The physical demands and work environment characteristics described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. • Physical demands: While performing duties of job, employee is occasionally required to stand; walk; sit; use hands to finger, handle or feel objects; reach with hands and arms; talk and hear. Specific vision abilities required by the job include close and distance vision. • Work environment: The noise level in the work environment is usually minimal to moderate.
Great Dane is an EEO employer.
Show more
Show less","Data Pipeline Architecture, Data Transformation, SQL, Relational Databases, Data Cloud Platforms, Data Pipeline Management Tools, Objectoriented Scripting Languages, Python, Java","data pipeline architecture, data transformation, sql, relational databases, data cloud platforms, data pipeline management tools, objectoriented scripting languages, python, java","data cloud platforms, data pipeline architecture, data pipeline management tools, data transformation, java, objectoriented scripting languages, python, relational databases, sql"
"Data Engineer, Data Platform",Grammarly,"Georgia, United States",https://www.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3656897196,2023-12-17,Georgia,United States,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Python, Scala, Java, Database design, Relational queries, API design, Service design, System design, Internal tool building, Data lake processing, AWS, Open source services, Thirdparty services, Admin site building","python, scala, java, database design, relational queries, api design, service design, system design, internal tool building, data lake processing, aws, open source services, thirdparty services, admin site building","admin site building, api design, aws, data lake processing, database design, internal tool building, java, open source services, python, relational queries, scala, service design, system design, thirdparty services"
Senior Data Engineer,FanDuel,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-fanduel-3783997210,2023-12-17,Georgia,United States,Mid senior,Hybrid,"About Fanduel
FanDuel Group (“FanDuel"") is an innovative sports-tech entertainment company that is changing the way consumers engage with their favorite sports, teams, and leagues. The premier gaming destination in the United States, FanDuel consists of a portfolio of leading brands across gaming, sports betting, daily fantasy sports, advance-deposit wagering, and TV/media.
FanDuel has a presence across all 50 states with approximately 17 million customers and 28 retail locations. FanDuel is based in New York with offices in New Jersey, Georgia, California, Oregon, Canada and Scotland.
Its networks FanDuel TV and FanDuel+ are broadly distributed on linear cable television and through its relationships with leading direct-to-consumer over-the-top platforms.
FanDuel is a subsidiary of Flutter Entertainment plc, the world's largest sports betting and gaming operator with a portfolio of globally recognized brands and a constituent of the FTSE 100 index of the London Stock Exchange.
THE ROSTER…
At FanDuel, we give fans a new and innovative way to interact with their favorite games, sports and teams. We’re dedicated to building a winning team and we pride ourselves on being able to make every moment mean more, especially when it comes to your career. So, what does “winning” look like at FanDuel? It’s recognition for your hard-earned results, a culture that brings out your best work—and a roster full of talented coworkers. Make no mistake, we are here to win, but we believe in winning right. That means we’ll never compromise when it comes to looking out for our teammates. From creatives professionals to cutting edge technology innovators, FanDuel offers a wide range of career opportunities, best in class benefits, and the tools to explore and grow into your best selves. At FanDuel, our principle of “We Are One Team” runs through all our offices across the globe, and you can expect to be a part of an exciting company with many opportunities to grow and be successful.
THE POSITION
Our roster has an opening with your name on it
FanDuel is looking for an experienced Senior Data Engineer with deep understanding of large-scale data handling and processing best practices in a cloud environment to help us build scalable systems. As our data is a key component of the business used by almost every facet of the company, including product development, marketing, operations, and finance. It is vital that we deliver robust solutions that ensure reliable access to data with a focus on quality and availability.
Our competitive edge comes from making decisions based on accurate and timely data and your work will provide access to that data across the whole company. Looking ahead to the next phase of our data platform we are keen to do more with real time data processing and working with our data scientists to create machine learning pipelines.
THE GAME PLAN
Everyone on our team has a part to play
Creating and maintain optimal data pipeline architecture.
Designing and implementing data pipelines required in the data warehouse and data lake in batch or real-time using data transformation technologies.
Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
Designing and deploying data models and views with large datasets that meet functional / non-functional business requirements.
Delivering data integration solutions to downstream marketing and campaign software
Delivering quality production-ready code in an agile environment.
Delivering test plans, monitoring, debugging and technical documents as a part of development cycle.
Creating data tools for analytics and working with stakeholders across all departments to assist with data-related technical issues and supporting their data infrastructure needs.
THE STATS
What we’re looking for in our next teammate
Advanced experience writing Python scripts.
Advanced working SQL knowledge and experience working with relational databases.
Understanding and working experience with AWS or Google Cloud.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
Show proficiency understanding complex ETL processes.
Demonstrate the ability to optimize processes (ram vs io).
Knowledge of data integrity and relational rules.
Ability to quickly learn new technologies is critical.
Proficiency with agile or lean development practices.
Player Benefits
We treat our team right
From our many opportunities for professional development to our generous insurance and paid leave policies, we’re committed to making sure our employees get as much out of FanDuel as we ask them to give. Competitive compensation is just the beginning. As part of our team, you can expect:
An exciting and fun environment committed to driving real growth
Opportunities to build really cool products that fans love
Mentorship and professional development resources to help you refine your game
Be well, save well and live well - with FanDuel Total Rewards your benefits are one highlight reel after another
FanDuel is an equal opportunities employer and we believe, as one of our principal states, “We Are One Team!” We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, Veteran status, or another other characteristic protected by state, local or federal law. We believe FanDuel is strongest and best able to compete if all employees feel valued, respected, and included. We want our team to include diverse individuals because diversity of thought, diversity of perspectives, and diversity of experiences leads to better performance. Having a diverse and inclusive workforce is a core value that we believe makes FanDuel stronger and more competitive as One Team!
Show more
Show less","Python, SQL, AWS, Google Cloud, ETL processes, Agile development, Machine learning, Data pipelines, Data warehouses, Data lakes, Data models, Data integration, Analytics, Data structures, Metadata, Workload management, Data integrity, Relational databases","python, sql, aws, google cloud, etl processes, agile development, machine learning, data pipelines, data warehouses, data lakes, data models, data integration, analytics, data structures, metadata, workload management, data integrity, relational databases","agile development, analytics, aws, data integration, data integrity, data lakes, data models, data structures, data warehouses, datapipeline, etl, google cloud, machine learning, metadata, python, relational databases, sql, workload management"
Enterprise Data Engineer(Hybrid - Atlanta),Aptonet Inc,"Atlanta, GA",https://www.linkedin.com/jobs/view/enterprise-data-engineer-hybrid-atlanta-at-aptonet-inc-3768095873,2023-12-17,Georgia,United States,Mid senior,Hybrid,"Title :Enterprise Data Engineer(Hybrid - Atlanta)
Location: Atlanta, GA
Hybrid – 2days
This Data Engineer will be responsible for creating new data flows into AWS including coordinating with business and functional areas to establish and communicate our data fabric – best practices, framework, and tools. Together we will establish a new data fabric for Norfolk Southern that will help create a common view of data and provide a centralized mechanism for its aggregation, cleansing, transformation, augmentation, validation, and syndication.
Responsibilities :
Sharing project solutions and outcomes with colleagues to improve delivery on future projects
Analyzing and translating business needs into long-term solution data pipelines.
Evaluating existing data systems.
Working with the development team to create conceptual data flows.
Developing best practices for data coding to ensure consistency within the system.
Reviewing modifications of existing systems for cross-compatibility.
Implementing data strategies and developing data integration points.
Evaluating implemented data systems for variances, discrepancies, and efficiency.
Troubleshooting and optimizing data systems.
Interpreting and delivering impactful strategic plans improving data integration, data quality, and data delivery in support of business initiatives and roadmaps
Formulating and articulating architectural trade-offs across solution options before recommending an optimal solution ensuring technical requirements are met
Motivating and developing staff through teaching, empowering, and influencing technical and consulting “soft” skills
Driving innovative technology solutions through thought leadership on emerging trends
Skills :
Required:
15+ years of overall IT experience
3+ years of experience with high-velocity high-volume stream processing: Apache Spark
Experience with real-time data processing and streaming techniques using Spark structured streaming and Kafka
Deep knowledge of troubleshooting and tuning Spark applications
3+ years of experience with data ingestion from Message Queues (Tibco, IBM, etc.) and different file formats across different platforms like JSON, XML, CSV
3+ years of experience with Big Data tools/technologies like Hadoop, Spark, Spark SQL, Kafka, Sqoop, Hive, S3, or HDFS
3+ years of experience building, testing, and optimizing ‘Big Data’ data ingestion pipelines, architectures, and data sets
2+ years of experience with Python (and/or Scala) and PySpark/Scala-Spark
3+ years of experience with AWS cloud platform
3+ years of experience with database solutions like Databricks or Snowflake
2+ years of experience with NoSQL databases, including HBASE and/or Cassandra
Knowledge of Unix/Linux platform and shell scripting is a must
Strong analytical and problem-solving skills
Preferred (Not Required):
Experience with Cloudera/Hortonworks CDP, HDP and HDF platforms
Strong SQL skills with ability to write intermediate complexity queries
Strong understanding of Relational & Dimensional modeling
Experience with GIT code versioning software
Experience with REST API and Web Services
Good business analyst and requirements gathering/writing skills
Benefits (employee contribution):
Health insurance
Health savings account
Dental insurance
Vision insurance
Flexible spending accounts
Life insurance
Retirement plan
All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","Apache Spark, Spark structured streaming, Kafka, Tibco, IBM, Hadoop, Spark SQL, Sqoop, Hive, S3, HDFS, Python, PySpark, ScalaSpark, AWS, Databricks, Snowflake, HBASE, Cassandra, Unix, Linux, Shell scripting, Cloudera, Hortonworks CDP, HDP, HDF, SQL, Relational modeling, Dimensional modeling, GIT, REST API, Web Services","apache spark, spark structured streaming, kafka, tibco, ibm, hadoop, spark sql, sqoop, hive, s3, hdfs, python, pyspark, scalaspark, aws, databricks, snowflake, hbase, cassandra, unix, linux, shell scripting, cloudera, hortonworks cdp, hdp, hdf, sql, relational modeling, dimensional modeling, git, rest api, web services","apache spark, aws, cassandra, cloudera, databricks, dimensional modeling, git, hadoop, hbase, hdf, hdfs, hdp, hive, hortonworks cdp, ibm, kafka, linux, python, relational modeling, rest api, s3, scalaspark, shell scripting, snowflake, spark, spark sql, spark structured streaming, sql, sqoop, tibco, unix, web services"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Medicine Hat, Alberta, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3752012625,2023-12-17,Medicine Hat, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, SQL, R, Python, Performance Metrics, DataDriven DecisionMaking, Optimization, A/B Testing, Data Quality, Data Manipulation, Data Visualization, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL Processes","data analysis, statistical techniques, sql, r, python, performance metrics, datadriven decisionmaking, optimization, ab testing, data quality, data manipulation, data visualization, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, data manipulation, data quality, dataanalytics, datadriven decisionmaking, etl, hypothesis testing, optimization, performance metrics, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
"Job - Data Engineer  (DDR memory, data scientist with Python, Elasticsearch ) - JV",Cube Hub Inc.,"Folsom, CA",https://www.linkedin.com/jobs/view/job-data-engineer-ddr-memory-data-scientist-with-python-elasticsearch-jv-at-cube-hub-inc-3627343537,2023-12-17,El Dorado,United States,Mid senior,Onsite,"Position Title: Database Engineer (DDR memory, Data scientist Elasticsearch
)
Job Location: Folsom, California
Job Duration: 6 months
Please note: This role will be 100% Remote in any US location
There is some flexibility on the start and end time.
Will be required to join a late night call at least once a week.
The sponsor would be willing to conside familiar with
DDR memory, Data scientist with Python
experience.
He would also like for candidates to be
familiar with Elasticsearch
experience
Description
Database Engineer works on a multidisciplinary project in collaboration with software and hardware engineers in the design, development, and utilization of database enhancement tools.
Your Responsibility Includes But Will Not Be Limited To
Creating production-quality software systems of enterprise-scale, crafting data pipelines handling enterprise data loads, connecting heterogeneous systems, and providing analytical capabilities that can grow.
Develop tools, technologies, frameworks that will enable us to move faster and decide better.t.
Are comfortable designing, implementing, validating, and preparing datasets for future algorithms.
Develop code to scrub, consolidate, analyze, and visualize data
Create code that's simple, clean, and follows design patterns.
Have a solid understanding of
elastic
and other index-based
search technologies
Qualifications
You must possess the below minimum qualifications to be initially considered for this position.
Preferred qualifications are in addition to the requirements and are considered a plus factor in identifying top candidates.
Knowledge and/or experience listed below would be obtained through a combination of your school work and/or classes and/or research and/or relevant previous job and/or internship experiences .
Minimum Qualifications
Must have experience with database use and design.
Must have Data scientist
Coding proficiency in
python
Basic knowledge of platform architecture and memory
Experience working with data belonging to a statistical pipeline.
Extraction, sanitization, analysis, visualization, and automation.
Preferred Qualifications
Hands-on with elastic search or other search technologies.
Familiarity with at least one UI framework, libraries like React.
Familiar with s
cripting - shell, Bash, PowerShell
, etc, you should identify and automate boring tasks.
Know clean code rules
Familiarity with
github
Minimum Educational Requirement: Bachelor's degree or higher - no exceptions
Show more
Show less","Database, DDR memory, Data science, Python, Elasticsearch, Software system, Data pipeline, Data visualization, Elastic, Search technology, Database design, Platform architecture, Memory, Statistical pipeline, Data extraction, Data analysis, Automation, React, Scripting, Shell, Bash, PowerShell, GitHub","database, ddr memory, data science, python, elasticsearch, software system, data pipeline, data visualization, elastic, search technology, database design, platform architecture, memory, statistical pipeline, data extraction, data analysis, automation, react, scripting, shell, bash, powershell, github","automation, bash, data extraction, data pipeline, data science, dataanalytics, database, database design, ddr memory, elastic, elasticsearch, github, memory, platform architecture, powershell, python, react, scripting, search technology, shell, software system, statistical pipeline, visualization"
Sr. Data Warehouse Business Intelligence Analyst,Adventist Health,"Roseville, CA",https://www.linkedin.com/jobs/view/sr-data-warehouse-business-intelligence-analyst-at-adventist-health-3786306842,2023-12-17,El Dorado,United States,Mid senior,Onsite,"Job Description
Adventist Health is ranked #10 in Becker's list of the largest nonprofit hospital systems in the U.S. We are the largest company headquartered and sixth largest employer in Roseville, California. Our corporate headquarters have been located at a desirable location on Douglas Boulevard since 1984. To accommodate our growing services, we are creating a new campus that will not only bring our workforce of nearly 900 people together in one location, but also facilitate a deeper connection with our Roseville neighbors and community.
Job Summary
Leads the analysis, design, build, implementation and support of business intelligence applications. Performs research and analysis to support business operations. Develops recommendations to solve problems and improve performance. Works in a data warehouse environment, which includes database design, data architecture, metadata management and repository creation. Leads the development, maintenance and support of an EDW system and corresponding data marts. Troubleshoots escalated operational problems and operational issues and tunes existing EDW applications. Creates new or enhanced components of the data warehouse.
Job Requirements
Education and Work Experience:
Associate’s/Technical Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Required
Bachelor's Degree in computer science, management information systems, applied mathematics or equivalent combination of education/related experience: Preferred
Five years' experience as a business intelligence analyst or developer focusing on business intelligence solutions: Preferred
SQL and database management experience within common DBMS platforms such as Oracle, SQL Server, MySQL, etc.: Preferred
Experience analyzing query performance issues and query tuning and healthcare experience: Preferred
Essential Functions
Assesses how users are progressing with their use of analytics applications and assists with optimization training to further develop user proficiency, efficiency, and satisfaction.
Leads the development of training materials for analytics applications. Conducts end user training on analytics applications and ensures that system documentation is current and available to the customer and that customers are educated as to where to find it. Conducts presentations, presents findings and provides valuable input at the department, site or corporate level to train, present group work, and facilitate decisions.
Identifies potential process and/or department-specific risks and assists in strategies to mitigate or resolve them. Assumes responsibility to independently research, document and facilitate resolutions to issues reported by end-users or those that arise during development.
Leads a collaborative effort with the user community to gather and adequately document requirements for business intelligence applications. Oversees service level agreements with various end-user departments and enterprise business units.
Provides subject matter expertise in the technologies and trends being used in the business intelligence sector and in general by the information technology and systems field. Serves as resource for business direction on best practice standards for business intelligence.
Performs other job-related duties as assigned.
Organizational Requirements
Adventist Health is committed to the safety and wellbeing of our associates and patients. Therefore, we require that all associates receive all required vaccinations, including, but not limited to, measles, mumps, flu (based on the seasonal availability of the flu vaccine typically during October-March each year), COVID-19 vaccine (required in CA, HI and OR) etc., as a condition of employment, and annually thereafter. Medical and religious exemptions may apply.
About Us
Adventist Health is more than an award-winning health system. We provide whole-person care to our communities and champion the greater good - from the operating room to the boardroom, we are driven by our unique passion to live God's love through health, wholeness and hope. From Oregon to Oahu, we have a calling to always do more. Now is your chance to apply your passion to our mission.
Show more
Show less","SQL, Oracle, SQL Server, MySQL, Database Management, Business Intelligence, Data Warehouse, Data Architecture, Metadata Management, Data Marts, Query Tuning, Data Analysis, Report Generation, Presentation Skills, Training, Documentation, Risk Management, Problem Solving, Communication, Collaboration, Research, Best Practices, Subject Matter Expertise","sql, oracle, sql server, mysql, database management, business intelligence, data warehouse, data architecture, metadata management, data marts, query tuning, data analysis, report generation, presentation skills, training, documentation, risk management, problem solving, communication, collaboration, research, best practices, subject matter expertise","best practices, business intelligence, collaboration, communication, data architecture, data marts, dataanalytics, database management, datawarehouse, documentation, metadata management, mysql, oracle, presentation skills, problem solving, query tuning, report generation, research, risk management, sql, sql server, subject matter expertise, training"
Lead IT Data Architect,"Vir Biotechnology, Inc.","San Francisco, CA",https://www.linkedin.com/jobs/view/lead-it-data-architect-at-vir-biotechnology-inc-3723912415,2023-12-17,Novato,United States,Associate,Onsite,"Vir Biotechnology, Inc. is an immunology company focused on combining cutting-edge technologies to treat and prevent infectious diseases and other serious conditions. Vir has assembled two technology platforms that are designed to stimulate and enhance the immune system by exploiting critical observations of natural immune processes. Its current clinical development pipeline consists of product candidates targeting hepatitis B and hepatitis delta viruses and human immunodeficiency virus. Vir has several preclinical candidates in its pipeline, including those targeting influenza A and B, COVID-19, RSV/MPV and HPV.
We believe the success of our colleagues drives the success of our mission. We are committed to creating a company passionate about equality, inclusion, and respect. When everyone feels supported and encouraged to give their best, we will collectively deliver outstanding results. We are proud to be the first company to be ranked at the top of the Deloitte Fast 500 list two years in a row (2023 & 2022)!
Vir Biotechnology is looking for a Lead IT Data Architect to create a data architecture vision, align data architecture with our enterprise architecture and implement a set of standards for our data solutions. You will ensure the access, performance and security of our data systems and applications. You will report to the Senior Director, IT Infrastructure and Operations and collaborate across the Data Engineering, Data Science, Research, Clinical and TechOps groups to develop a data architecture to advance the goals of our Data Science, Bioinformatics and Machine Learning teams. You will also collaborate with other groups to across the organization to provide data marts and analytical tools to support their operations.
RESPONSIBILITIES AND LEARNING OPPORTUNITIES:
Lead the architecture, design, and deployment of enterprise data solutions.
Define data standards and policies and develop the data architecture for Vir.
Establish the Data Architecture Forum (DAF) and lead forum meetings.
Help evaluate and select AWS and data governance technologies to support building an enterprise healthcare data lake.
Design and planning, including application architecture, system landscape, installation sizing and planning, system change management, performance monitoring and tuning, troubleshooting and problem analysis and resolution.
Ensure data integrity and the data's strategic value is understood throughout the Vir.
Mentor IT operations and business teams on data architecture design and requirements.
Collaborate across the IT, Machine Learning, Bioinformatics and Data Engineering Teams.
Drive adoption of advanced AWS cloud-based features and capabilities.
Assist the development and operational teams to build infrastructure and troubleshoot production issues.
Review and enhance documentation and knowledgebase.
Translate business requirements into conceptual, logical, and physical models.
QUALIFICATIONS AND EXPERIENCE:
BS or MS in Computer Science or a science-related field.
12+ years industry experience, including experience as a Data Architect or Lead Data Engineer.
Development experience in Python or similar object-oriented languages.
Experience building enterprise data governance programs.
Experience with Managed Service Providers.
Deep experience with AWS technologies, APIs, backup, Disaster Recovery and ITOps.
Experience building data lakes using AWS or similar technologies.
Experience designing and building data processing pipelines using Nextflow, Airflow, or similar technologies.
Understanding of the Machine learning lifecycle, LLMs and their use.
DevOps exposure including Ci/CD pipelines in AWS - Code/Build/Deployment: GIT, Docker, Jenkins, Kubernetes.
Applicable certifications include AWS Certified Cloud Data Engineer, AWS Certified Application Architect.
You will work primarily from our Headquarters in San Francisco's Mission Bay district.
_AA1
The expected salary range for this position is $168,500.00 to $246,500.00. Actual pay will be determined based on experience, qualifications, geographic location, and other job-related factors.
Vir’s compensation and benefits are aligned with the current market and commensurate with the person’s experience and qualifications. All full-time employees receive a package that includes: compensation, bonus and equity as well as many other Vir benefits and perks such as: health, dental, vision, life and disability insurance benefits, non-accrual paid time off, company shut down for holidays, commuter benefits, child care reimbursement, education reimbursement, 401K match and lunch for all onsite employees!
Vir Biotechnology (“Vir”) is an equal opportunity employer. All employment decisions at Vir are based on legitimate, non-discriminatory business requirements, job duties and individual qualifications. Employment decisions are made without regard to race, color, religion, sex (including pregnancy), gender, gender identity, gender expression, sexual orientation, age, parental status, marital status, national origin, ancestry, disability, medical condition, genetic information (including family medical history), political affiliation, military service or any other legally protected characteristic.
This commitment extends to all management practices and decisions, including recruitment and hiring, compensation, appraisal systems, promotions, training and career development programs. Vir also strongly commits to providing employees with a work environment free of unlawful discrimination or harassment.
Vir Human Resources leads recruitment and employment for Vir. Unsolicited resumes sent to Vir from recruiters do not constitute any type of relationship between the recruiter and Vir and do not obligate Vir to pay fees should we hire from those resumes. We ask that external recruiters and/or agencies not contact or present candidates directly to our hiring manager or employees.
For hires based in the United States, Vir Biotechnology, Inc., participates in E-Verify.
Candidate Privacy Notice
Show more
Show less","Data Architecture, Data Governance, AWS, Python, Airflow, Nextflow, Kubernetes, Git, Docker, Jenkins, DevOps, Machine Learning, Apache Kafka, GitOps, Tableau, Microsoft Azure, BigQuery, Jenkins","data architecture, data governance, aws, python, airflow, nextflow, kubernetes, git, docker, jenkins, devops, machine learning, apache kafka, gitops, tableau, microsoft azure, bigquery, jenkins","airflow, apache kafka, aws, bigquery, data architecture, data governance, devops, docker, git, gitops, jenkins, kubernetes, machine learning, microsoft azure, nextflow, python, tableau"
Data Scientist,JBC,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-scientist-at-jbc-3578456365,2023-12-17,Novato,United States,Associate,Onsite,"Location: San Francisco, California
Type: Freelance/Contract
Job #196302
Title: Data Scientist
Company: Software Company
Location: hybrid, San Francisco
Job Type: 6+ month contract
Hourly: Up to $80/hr W2
Responsibilities:
Designing data tables and dashboards to highlight insights and opportunity areas for the product team
Adding new metrics and aggregations to our data warehouse.
Designing/ analyzing experiments to measure the impact of new product features
About you:
Experienced in building scalable data pipelines & data modeling.
Proficiency in data processing and storage technologies like AWS/S3/HDFS, Redshift, Python/Scala, SQL, Spark, Airflow
Strong Data Visualization experience.
Apply Now
The post Data Scientist appeared first on JBC Team.
Show more
Show less","Data Modeling, Data Visualization, Python, Scala, SQL, Spark, Airflow, AWS, S3, HDFS, Redshift","data modeling, data visualization, python, scala, sql, spark, airflow, aws, s3, hdfs, redshift","airflow, aws, datamodeling, hdfs, python, redshift, s3, scala, spark, sql, visualization"
Senior Data Engineer (Finance/Claims),Turo,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-finance-claims-at-turo-3774970482,2023-12-17,Novato,United States,Mid senior,Hybrid,"As the world's largest car sharing marketplace, Turo is growing fast and hiring talent in the US, Canada, the UK, France, and Australia! Our driven, down to earth team empowers you to push yourself, make a huge impact, and accelerate your career growth.
About The Team
Data is our fuel at Turo. It is ever-more abundant and valuable, but it's raw material. Harnessed by data scientists and machine learning engineers, it propels Turo on its mission to put the world’s 1.5+ billion cars to better use, delighting our customers with matching the right car for their next adventure from an exceptionally diverse selection, and at the same time helping our marketplace remain safe.
In this role, you’ll be given the chance to work with cutting-edge technologies to construct resilient, scalable systems for gathering and analyzing extensive data sets. You’ll also be responsible for creating and managing data pipelines, data structures, and reports. If you’re a detail-oriented individual who enjoys troubleshooting and delving into data-related problems, this is the role for you.
The candidate for this position is anticipated to develop a comprehensive understanding of the finance/claims domain. This will involve working closely with data analysts, as well as finance and accounting teams, to enhance our workflows by building the appropriate data pipeline infrastructure
Learn more about our Engineering team here:
https://www.beforeyouapply.com/team/engineering-at-turo
What You Will Do
On a daily basis, you will work with members of the team to update our data engineering roadmap and execute upon those initiatives
Working closely with data analysts, as well as finance and accounting teams to build new data pipelines to further improve and automate the existing workflow and processes.
Develop, deploy and maintain workflow management tools such as Airflow, Jenkins etc in cloud environments.
Using cloud technology such as AWS, Kubernetes, Docker, Redshift, EMR
Your profile
5+ years of relevant experience
Past experience building ETL processes.
Strong programmer who views their code as a craft
Experience with a workflow manager — Airflow, Luigi, Jenkins, etc.
Experience working with data tools in the public cloud (AWS, GCP, Azure)
Bonus If You Have
Past experience working with the finance/claims domain is a plus.
The SF base salary target range for this full-time position is $162,000-$199,000 + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your work location during the hiring process. Please note that the salary range listed in this posting reflects the base salary only, and does not include bonus (if applicable), equity, or benefits.
Benefits
Competitive salary, equity, benefits, and perks for all full-time employees
Employer-paid medical, dental, and vision insurance (Country specific)
Retirement employer match
$2,000 Learning & Development stipend to invest in your professional development
$1,000 USD Turo host matching and $1,500 USD vehicle reimbursement program
$100 USD Monthly Turo travel credit
Cell phone, internet and Fringe benefit stipend
Paid time off to relax and recharge
Paid holidays, volunteer time off, and parental leave
For those who are in the office full-time or hybrid we have weekly in-office lunch, office snacks, and fun activities
Annual Turbo Week (week-long, company-wide conference)
We are committed to building a diverse team. If you are from a background that's underrepresented in tech, we'd love to meet you.
Aside from an award winning work environment and the opportunity to be part of the world’s largest car sharing marketplace, we are also growing the team quickly - join us! Even if you don't meet every qualification, we are looking for people with enthusiasm for what we do and we will consider you for this and other possibilities.
Turo is an Equal Opportunity Employer and a participant in the U.S. Federal E-Verify program. Women, minorities, individuals with disabilities and protected veterans are encouraged to apply. We welcome people of different backgrounds, experiences, abilities and perspectives.
Turo will consider qualified applicants with criminal histories in a manner consistent with the San Francisco Fair Chance Ordinance, as applicable.
We welcome candidates with physical, mental, and/or neurological disabilities. If you require assistance applying for an open position, or need accommodation during the recruiting process due to a disability, please submit a request to People Operations by emailing PeopleOps@turo.com.
About Turo
Turo is the world’s largest car sharing marketplace where you can book any car you want, wherever you want it, from a vibrant community of trusted hosts across the US, Canada, France, UK and Australia. Guests choose from a totally unique selection of nearby cars, while hosts earn extra money to offset the costs of car ownership. A pioneer of the sharing economy and the travel industry, Turo is a safe, supportive community where the car you book is part of a story, not a fleet.
Discover Turo at https://turo.com, the App Store, and Google Play, and check out our blog, Field Notes.
Read more about the Turo culture according to Turo CEO, Andre Haddad.
Show more
Show less","Data Science, Machine Learning, AWS, Kubernetes, Docker, Redshift, EMR, SQL, ETL, Airflow, Luigi, Jenkins, GCP, Azure, Finance, Claims, Data Analysis","data science, machine learning, aws, kubernetes, docker, redshift, emr, sql, etl, airflow, luigi, jenkins, gcp, azure, finance, claims, data analysis","airflow, aws, azure, claims, data science, dataanalytics, docker, emr, etl, finance, gcp, jenkins, kubernetes, luigi, machine learning, redshift, sql"
Experienced Data Engineer - Data Engineering,Jobs via eFinancialCareers,"San Francisco, CA",https://www.linkedin.com/jobs/view/experienced-data-engineer-data-engineering-at-jobs-via-efinancialcareers-3755659604,2023-12-17,Novato,United States,Mid senior,Hybrid,"We believe that the way people interact with their finances will drastically improve in the next few years. We're dedicated to empowering this transformation by building the tools and experiences that thousands of developers use to create their own products. Plaid powers the tools millions of people rely on to live a healthier financial life. We work with thousands of companies like Venmo, SoFi, several of the Fortune 500, and many of the largest banks to make it easy for people to connect their financial accounts to the apps and services they want to use. Plaid's network covers 12,000 financial institutions across the US, Canada, UK and Europe. Founded in 2013, the company is headquartered in San Francisco with offices in New York, Washington D.C., London and Amsterdam.
Making data-driven decisions is key to Plaid's culture. To support that, we need to scale our data systems while maintaining correct and complete data. We provide tooling and guidance to teams across engineering, product, and business and help them explore our data quickly and safely to get the data insights they need, which ultimately helps Plaid serve our customers more effectively. In addition, Plaid will not be successful if we can't move quickly. We build the data systems and tools that enable everyone at Plaid to be data-driven, making analytics easy, obvious, and proactive across the company.
Data Engineers heavily leverage SQL and Python to build data workflows that integrate with our Golang and Typescript applications. We use tools like DBT, Airflow, Redshift, ElasticSearch, Atlan, and Retool to orchestrate data pipelines and define workflows. We work with engineers, product managers, business intelligence, data analysts, and many other teams to build Plaid's data strategy and a data-first mindset. Our engineering culture is IC-driven -- we favor bottom-up ideation and empowerment of our incredibly talented team. We are looking for engineers who are motivated by creating impact for our consumers and customers, growing together as a team, shipping the MVP, and leaving things better than we found them.
You will be in a high impact role that will drive and define data standards and data culture across Plaid. You will have the opportunity to transition into the Tech Lead role on the team which will come with higher visibility and more stakeholder collaboration. You will collaborate with and have strong and cross functional partnerships with literally all teams at Plaid from Engineering to Product to Marketing/Finance. You will be responsible for mentoring junior engineers on the team and being closely involved in their design and implementations. You will be able to provide guidance and build the technical culture on the team in the form of design reviews and PRs.
Responsibilities
Defining the long-term technical roadmap for building a data-driven culture at Plaid.
Focus on data quality and privacy.
Leading key data engineering projects that drive collaboration across the company.
Mentoring engineers, operations, and data analysts on best practices for data organization and query performance.
Advocating for adopting industry tools and practices at the right time.
Owning core SQL and python data pipelines that power our data lake and data warehouse.
Well-documented data with defined dataset quality, uptime, and usefulness.
Qualifications
6+ years of dedicated data engineering experience, solving complex data pipelines issues at scale.
You value SQL as a flexible and extensible tool, and are comfortable with modern SQL data orchestration tools like DBT, Mode, Hightouch, and Airflow.
Experience working both real-time systems like Kinesis, Kafka, Flink, and batch data pipelines in Redshift, Presto, and Data Lakes.
You appreciate the importance of schema design, and can evolve an analytics schema on top of unstructured data.
You are excited to try out new technologies. You like to produce proof-of-concepts that balance technical advancement and user experience and adoption.
You like to get deep in the weeds to manage, deploy, and improve low level data infrastructure.
You are empathetic working with stakeholders. You listen to them, ask the right questions, and collaboratively come up with the best solutions for their needs.
You are a champion for data privacy and integrity, and always act in the best interest of consumers.
$187,200 - $280,800 a year
Target base Salary for this role is $187,200- $280,800 per year. Additional compensation in the form(s) of equity and/or commission are dependent on the position offered. Plaid provides a comprehensive benefit plan, including medical, dental, vision, and 401(k). Pay is based on factors such as (but not limited to) scope and responsibilities of the position, candidate's work experience and skillset, and location. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.
Our mission at Plaid is to unlock financial freedom for everyone. To support that mission, we seek to build a diverse team of driven individuals who care deeply about making the financial ecosystem more equitable. We recognize that strong qualifications can come from both prior work experiences and lived experiences. We encourage you to apply to a role even if your experience doesn't fully match the job description. We are always looking for team members that will bring something unique to Plaid!
Plaid is proud to be an equal opportunity employer and values diversity at our company. We do not discriminate based on race, color, national origin, ethnicity, religion or religious belief, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, military or veteran status, disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state, and local laws. Plaid is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance with your application or interviews due to a disability, please let us know at accommodations@plaid.com.
Please review our Candidate Privacy Notice here .
Show more
Show less","SQL, Python, Golang, Typescript, DBT, Airflow, Redshift, ElasticSearch, Atlan, Retool, Kinesis, Kafka, Flink, Data Lakes, Presto, Mode, Hightouch","sql, python, golang, typescript, dbt, airflow, redshift, elasticsearch, atlan, retool, kinesis, kafka, flink, data lakes, presto, mode, hightouch","airflow, atlan, data lakes, dbt, elasticsearch, flink, golang, hightouch, kafka, kinesis, mode, presto, python, redshift, retool, sql, typescript"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Greenville, SC",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783191085,2023-12-17,Greenville,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Greenvil-DataResearchAn.032
Show more
Show less","Python, JavaScript, JSON, Generative AI, R, OOP, Data Science, Technical Writing, Verbal Communication, Collaboration, Time Management, Stakeholder Management, Algorithms","python, javascript, json, generative ai, r, oop, data science, technical writing, verbal communication, collaboration, time management, stakeholder management, algorithms","algorithms, collaboration, data science, generative ai, javascript, json, oop, python, r, stakeholder management, technical writing, time management, verbal communication"
Lead Data Scientist,Purpose Financial,"Greenville, SC",https://www.linkedin.com/jobs/view/lead-data-scientist-at-purpose-financial-3777317097,2023-12-17,Greenville,United States,Mid senior,Hybrid,"JOB SUMMARY
Purpose Financial is seeking an experienced
Lead Data Scientist
to join our Data Science team to help manage our lending portfolio. The ideal candidate will have a comprehensive understanding of the line of credit lifecycle business and work closely with a team of data scientists and analysts. The primary duties will include constructing and deploying stochastic and deterministic models to support the company's profitability, performance analysis, test/control strategies, and other lending decisions. Furthermore, the role will involve mentoring and guiding junior analysts. The successful candidate must have the capacity to adapt to change quickly and fulfill all assigned and requested responsibilities.
This is a hybrid opportunity in Greenville, SC! Relocation assistance may be available!
JOB RESPONSIBILITIES
Manage the development and implementation of advanced statistical and machine learning models across the customer lifecycle to include customer acquisition, onboarding (operational processes), credit underwriting, product optimization, and collections.
Provide technical leadership and guidance to the data science team, ensuring high-quality output and adherence to best practices.
Monitor consumer loan portfolio and optimize profit-based risk management decisions at all stages of the customer credit life cycle, including (but not limited to) new customer acquisition, underwriting, conversion, collections, and retention.
Independently designs experiments to enable longer term optimization of decisions, line assignment, and offer terms across our menu of financial products and customer segments.
Collaborate with Finance and Business Intelligence to build and improve monitoring of portfolio KPIs and product strategy.
Stay up to date with the latest developments in data science and machine learning and evaluate how they can be applied to enhance our lending processes.
Equivalent Education Level Required:
Master’s Degree in Quantitative Field (Engineering, Statistics, Mathematics, Economics, or other comparable quantitative field) required or Bachelor’s Degree in Quantitative Field (Engineering, Statistics, Mathematics, Economics, or another comparable quantitative field) required.
Experience Required
: Three (3) years of experience working in data-science or a related field, with a focus on credit risk management.
Qualifications:
Experience in lending and line of credit is essential.
Proficiency in R/Python, SQL, or another analytical tool is a must.
Experience in building stochastic models.
Strong knowledge of data modeling, machine learning, and statistical techniques.
Comfortable with solving complex problems, working with computers and technology, and using higher level mathematics. Ability to combine data from multiple sources and manipulate to analyze and interpret business performance.
Excellent written and verbal communications skills as would be needed to communicate in person, by phone, and through email. Ability to quickly learn all technology needed to perform the role.
Physical Requirements:
Sitting for long periods of time; standing occasionally; walking; bending; squatting; kneeling; pushing/pulling; reaching; twisting; frequent lifting of less than 10 lbs., occasional lifting of up to 20 lbs.; driving and having access during the workday to an insured and reliable transportation; typing; data entry; grasping; transferring items between hands and/or to another person or receptacle; use of office equipment to include computers; ability to travel to, be physically present at, and complete the physical requirements of the position at any assigned location.
Travel:
0-10%
Attire:
Business Casual
Other:
Must be eligible to work in the USA and able to pass a background check.
Show more
Show less","Machine learning, Data science, R, Python, SQL, Statistics, Data modeling, Stochastic models, Credit risk management, Lending, Line of credit, Portfolio management, Performance analysis, Test/control strategies, Experiment design, Optimization, Business intelligence, KPI monitoring, Product strategy, Data manipulation, Data interpretation, Communication, Problemsolving, Technology","machine learning, data science, r, python, sql, statistics, data modeling, stochastic models, credit risk management, lending, line of credit, portfolio management, performance analysis, testcontrol strategies, experiment design, optimization, business intelligence, kpi monitoring, product strategy, data manipulation, data interpretation, communication, problemsolving, technology","business intelligence, communication, credit risk management, data interpretation, data manipulation, data science, datamodeling, experiment design, kpi monitoring, lending, line of credit, machine learning, optimization, performance analysis, portfolio management, problemsolving, product strategy, python, r, sql, statistics, stochastic models, technology, testcontrol strategies"
Senior Principal Consultant – Data and Analytics,Genesys,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/senior-principal-consultant-%E2%80%93-data-and-analytics-at-genesys-3781018277,2023-12-17,State College,United States,Mid senior,Remote,"Build something new with a world-class team.
At Genesys, we allow our employees to make their mark by entrusting them to make decisions and do what they’ve been hired to do: their very best. Your potential is waiting; why are you?
About Genesys:
GENESYS® powers more than 25 billion of the world’s best customer experiences each year.
In GENESYS, Innovations team builds solutions that enable our customers to get more value from their Genesys platforms. The Innovations team’s primary job is to build new, innovative applications using AI and analytics, with the goal of helping our customers effectively and efficiently deliver experience as a service (XaaS).
Summary: We are seeking a highly skilled and experienced Contact Center Data and Analytics Consultant to join our team. The ideal candidate will have a strong background in data analysis, data visualization, data engineering, and business intelligence. This is a customer facing role and requires the ability to present to both technical and business executive audiences.
Responsibilities:
Present Genesys data and analytics technology and vision to executives and technical contributors at prospects and customers
Work hands-on with prospects and customers to demonstrate and communicate the value of Genesys data and analytics apps throughout the sales cycle, from demo to proof of concept to design and implementation
Immerse yourself in the ever-evolving industry of contact center and experience management SaaS, maintaining a deep understanding of competitive and complementary technologies and vendors and how to position our analytics apps in relation to them
Collaborate with Product Management, Engineering, and Saloes to continuously improve Genesys data and analytics apps and marketing
Requirements:
8+ years direct experience with Genesys Engage Infomart or GCXI or NICE CX Analytics; strong understanding of Contact Center and Customer Experience data and metrics
Strong proficiency in SQL, data modeling, and data engineering
Familiarity with scripting languages for data engineering
Outstanding presenting skills to both technical and executive audiences, whether impromptu on a whiteboard or using presentations and demos.
Hands-on expertise with SQL and SQL analytics
Ability to connect a customer’s specific business problems to Genesys analytics solutions
Experience with data visualization tools such as Tableau or Power BI
Preferred Qualifications:
Experience with Snowflake
Experience with Elastic (ELK stack)
Experience with data governance and data management best practices.
Experience working with remote engineering teams
Familiar with software development and project management frameworks
Experience in consulting or developing products and solutions with contact center technologies like Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya, etc.
Compensation:
This role has a market-competitive salary with an anticipated base compensation range listed below. Actual salaries will vary depending on a candidate’s experience, qualifications, skills, and location. This role might also be eligible for a commission or performance-based bonus opportunities.
$121,500.00 - $238,400.00
Benefits:
Medical, Dental, and Vision Insurance.
Telehealth coverage
Flexible work schedules and work from home opportunities
Development and career growth opportunities
Open Time Off in addition to 10 paid holidays
401(k) matching program
Adoption Assistance
Fertility treatments
More details about our company benefits can be found at the following link: https://mygenesysbenefits.com
If a Genesys employee referred you, please use the link they sent you to apply.
About Genesys:
Every year, Genesys orchestrates billions of remarkable customer experiences for organizations in more than 100 countries. Through the power of our cloud, digital and AI technologies, organizations can realize Experience as a Service™ our vision for empathetic customer experiences at scale. With Genesys, organizations have the power to deliver proactive, predictive, and hyper personalized experiences to deepen their customer connection across every marketing, sales, and service moment on any channel, while also improving employee productivity and engagement. By transforming back-office technology to a modern revenue velocity engine Genesys enables true intimacy at scale to foster customer trust and loyalty. Visit www.genesys.com.
Reasonable Accommodations:
If you require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you or someone you know may complete the Reasonable Accommodations Form for assistance. Please use the Candidate field in the dropdown menu to ensure a timely response.
This form is designed to assist job seekers who seek reasonable accommodation for the application process. Submissions entered for non-accommodation-related issues, such as following up on an application or submitting a resume, may not receive a response.
Genesys is an equal opportunity employer committed to equity in the workplace. We evaluate qualified applicants without regard to race, color, age, religion, sex, sexual orientation, gender identity or expression, marital status, domestic partner status, national origin, genetics, disability, military and veteran status, and other protected characteristics.
Please note that recruiters will never ask for sensitive personal or financial information during the application phase.
Show more
Show less","Genesys, Data Analysis, Data Visualization, Data Engineering, Business Intelligence, SQL, SQL Analytics, Data Modeling, Scripting Languages, Tableau, Power BI, Snowflake, Elastic (ELK stack), Data Governance, Data Management, Software Development, Project Management, Contact Center Technologies, Customer Experience, XaaS","genesys, data analysis, data visualization, data engineering, business intelligence, sql, sql analytics, data modeling, scripting languages, tableau, power bi, snowflake, elastic elk stack, data governance, data management, software development, project management, contact center technologies, customer experience, xaas","business intelligence, contact center technologies, customer experience, data engineering, data governance, data management, dataanalytics, datamodeling, elastic elk stack, genesys, powerbi, project management, scripting languages, snowflake, software development, sql, sql analytics, tableau, visualization, xaas"
Data Visualization Specialist,"Information Technology Strategies, Inc","Fort Belvoir, VA",https://www.linkedin.com/jobs/view/data-visualization-specialist-at-information-technology-strategies-inc-3781744200,2023-12-17,Mount Vernon,United States,Associate,Onsite,"Information Technology Strategies, LLC. is a government IT solutions provider servicing commercial and government initiative in various parts of the United States. We are currently seeking a Data Visualization Specialist to work for our company.
Summary:
Client Agency is the Defense Logistics Agency.
Must have an active U.S. Government Clearance level of SECRET OR ABOVE.
Leads initiatives utilizing data visualization tools to provide actionable insights for addressing strategic and tactical mission objectives.
Develops visualizations to manipulate complex datasets in simple, intuitive, interactive formats.
Communicates information clearly and efficiently, enabling data-driven decisions and analysis.
Creates complex queries, stored procedures to deliver dynamic data visualizations.
Stays abreast of the visualization space and recommends new tools or processes to modernize in the future.
Graduate degree
Requirements:
Must have an active U.S. Government Clearance level of SECRET OR ABOVE.
BS or BA degree
8+ years of relevant experience
Experience with complex custom developed visualizations using D3, HTML, JavaScript, etc.
Experience with COTS enterprise data visualization tools (e.g., Qlik, SAP Lumira) Experience with the Information Technology Acquisition process and its milestones.
Experience developing data visualizations against ERP transactional and analytical data models.
Vendor Certifications with one or more technologies (e.g., Qlik Sense, Qlik View, SAP Lumira).
About Us
IT-Strat is a technology consulting company that holds various contract vehicles including best in class vehicles. IT-Strat has supported multiple clients including the Department of Homeland Security (DHS), Customs and Border Protection (CBP) and Immigration and Customs Enforcement (ICE via both prime and meaningful subcontracts). Additionally, IT-Strat has prime contracts with Defense Information Systems Agency (DISA), Defense Logistics Agency (DLA) and many others. We maintain relationships with multiple large businesses.
IT-Strat was established in 2002. We are a certified Woman Owned Small Business. IT-Strat also successfully graduated as a SBA 8(A) company. It was an 8(a) company from 2008 through 2017 and currently still has 8(a) contract vehicles.
Benefits We Offer:
Four Medical/Vision options including an HSA plan
Dental and Orthodontia plan
Vision Materials plan
Paid Life, Short-Term Disability, and Long-Term Disability
401K Retirement Program with company contribution
Paid Vacation, Holidays, Sick Leave, Floating Holidays, Bereavement Leave
Semi-monthly pay cycle
Information Technology Strategies (“IT-Strat”) is an Equal Employment Opportunity employer, and it is our policy to consider applicants for employment without regard to sex, race, color, creed, religion, national origin, sexual orientation, marital status, age, disability, veteran status, alienage, ancestry, and any other factors prohibited by law. Employment selections are based on company and client requirements and the qualifications and skills of the candidate. IT-Strat is committed to actively capitalizing on the diversity of skills, talents, and perspectives of our employees.
Show more
Show less","Data Visualization, Complex Datasets, Visualization Tools, Strategic and Tactical Mission Objectives, Datadriven Decisions, Dynamic Data Visualization, Vendor Certifications, D3, HTML, JavaScript, Qlik, SAP Lumira, Information Technology Acquisition, ERP Transactional, Analytical Data Models, Business Intelligence, Qlik Sense, Qlik View","data visualization, complex datasets, visualization tools, strategic and tactical mission objectives, datadriven decisions, dynamic data visualization, vendor certifications, d3, html, javascript, qlik, sap lumira, information technology acquisition, erp transactional, analytical data models, business intelligence, qlik sense, qlik view","analytical data models, business intelligence, complex datasets, d3, datadriven decisions, dynamic data visualization, erp transactional, html, information technology acquisition, javascript, qlik, qlik sense, qlik view, sap lumira, strategic and tactical mission objectives, vendor certifications, visualization, visualization tools"
Lead Data Engineer (Hybrid),Fannie Mae,"Reston, VA",https://www.linkedin.com/jobs/view/lead-data-engineer-hybrid-at-fannie-mae-3713009288,2023-12-17,Mount Vernon,United States,Associate,Onsite,"Company Description
At Fannie Mae, futures are made. The inspiring work we do helps make a home a possibility for millions of homeowners and renters. Every day offers compelling opportunities to use tech to tackle housing’s biggest challenges and impact the future of the industry. You’ll be a part of an expert team thriving in an energizing, flexible environment. Here, you will grow your career and help create access to fair, affordable housing finance.
Job Description
As a valued colleague on our team, you will provide expert advice and guidance to the team regarding the development of data infrastructures and pipelines to capture, integrate, organize, and centralize data while testing and ensuring data is readily accessible and in a usable state, including quality assurance.
THE IMPACT YOU WILL MAKE
Responsibilities
The Lead Data Engineer role will offer you the flexibility to make each day your own, while working alongside people who care so that you can deliver on the following responsibilities:
Assess customer needs and intended use of requested data in the development of database requirements and support the planning and engineering of enterprise databases.
Maintain comprehensive knowledge of database technologies, complex coding languages, and computer system skills.
Lead the team to organize and integrate data into readily available formats while maintaining existing structures and govern their use according to business requirements.
Lead the analysis of new data sources and monitoring of performance, scalability, and security of data.
Analyze the initial analysis and deliver the user interface (UI) to the customer to enable further analysis.
Qualifications
THE EXPERIENCE YOU BRING TO THE TEAM
Minimum Required Experiences
4+ years with Big Data Hadoop cluster (HDFS, Yarn, Hive, MapReduce frameworks), Spark, AWS EMR
4+ years of recent experience with building and deploying applications in AWS (S3, Hive, Glue, AWS Batch, Dynamo DB, Redshift, AWS EMR, Cloudwatch, RDS, Lambda, SNS, SWS etc.)
4+ years of Python, SQL, SparkSQL, PySpark
Excellent problem solving skills and strong verbal & written communication skills
Ability to work independently as well as part of an agile team (Scrum / Kanban)
Desired Experiences
Bachelor degree or equivalent
Knowledge of Spark streaming technologies
Experience in working with agile development teams
Familiarity with Hadoop / Spark information architecture, Data Modeling, Machine Learning (ML)
Knowledge of Environmental, Social, and Corporate Governance (ESG)
Skills
Skilled in discovering patterns in large data sets with the use of relevant software such as Oracle Data Mining or Informatica
Skilled in documentation and database reporting for the purposes of analysis, data discovery, and decision-making with the use of relevant software such as Crystal Reports, Excel, or SSRS
Skilled in cloud technologies and cloud computing
Experience using software and computer systems' architectural principles to integrate enterprise computer applications such as xMatters, AWS Application Integration, or WebSphere
Determining causes of operating errors and taking corrective action
Experience in the process of analyzing data to identify trends or relationships to inform conclusions about the data
Skilled in creating and managing databases with the use of relevant software such as MySQL, Hadoop, or MongoDB
Business Insight including advising, designing business models, interpreting customer and market insights, forecasting, benchmarking, etc.
Programming including coding, debugging, and using relevant programming languages
Communication including communicating in writing or verbally, copywriting, planning and distributing communication, etc.
Governance and Compliance including creating policies, evaluating compliance, conducting internal investigations, developing data governance, etc.
Adept at managing project plans, resources, and people to ensure successful project completion
Working with people with different functional expertise respectfully and cooperatively to work toward a common goal
Tools
SageMaker
AWS
Python
Additional Information
The future is what you make it to be. Discover compelling opportunities at careers.fanniemae.com.
Fannie Mae is an Equal Opportunity Employer, which means we are committed to fostering a diverse and inclusive workplace. All qualified applicants will receive consideration for employment without regard to race, religion, national origin, gender, gender identity, sexual orientation, personal appearance, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation in the application process, email us at careers_mailbox@fanniemae.com.
The hiring range for this role is set forth on each of our job postings located on Fannie Mae's Career Site. Final salaries will generally vary within that range based on factors that include but are not limited to, skill set, depth of experience, certifications, and other relevant qualifications. This position is eligible to participate in a Fannie Mae incentive program (subject to the terms of the program). As part of our comprehensive benefits package, Fannie Mae offers a broad range of Health, Life, Voluntary Lifestyle, and other benefits and perks that enhance an employee’s physical, mental, emotional, and financial well-being. See more here.
Show more
Show less","Hadoop, Spark, AWS, Python, SQL, SparkSQL, PySpark, Oracle Data Mining, Informatica, Crystal Reports, SSRS, xMatters, WebSphere, MySQL, MongoDB, SageMaker","hadoop, spark, aws, python, sql, sparksql, pyspark, oracle data mining, informatica, crystal reports, ssrs, xmatters, websphere, mysql, mongodb, sagemaker","aws, crystal reports, hadoop, informatica, mongodb, mysql, oracle data mining, python, sagemaker, spark, sparksql, sql, ssrs, websphere, xmatters"
Senior Data Engineer,i360,"Arlington, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-i360-3768834445,2023-12-17,Mount Vernon,United States,Mid senior,Onsite,"Your Job
i360 is seeking a Senior Data Engineer to join its Data Engineering team. With your technical expertise, you will architect, implement, and improve end to end pipelines, processes, procedures, and automation for all database-centric processes. You will maintain our relational and NoSQL systems for performance and reliability. You are responsible for tuning and configuring our databases and data platforms, as well as building tools and scripts to monitor, troubleshoot and automate our systems. You mentor the junior and mid-level engineers on our team and help set standards, frameworks, and best practices.
O
ur Team
The Data Engineering team plays a central role in our organization, given our focus on data. We collaborate closely with both our internal departments and external clients. Our primary responsibility involves designing, building, and upkeeping data pipelines and APIs to provide data to other teams, enabling them to create their products on that data.
What You Will Do
Design, architect, and build end to end SQL and NoSQL database solutions. This includes determining business requirements, recommending technologies, architecting, developing code, mentoring team members, and overseeing QA and deployment processes.
Load and process disparate data sets using technologies including but not limited to PostgreSQL, Elasticsearch, Snowflake, Spark, Java, and Python.
Work with business users to translate requirements into system flows, data flows, data mappings etc., and develop solutions to complex business problems.
Code applications that adhere to enterprise design patterns.
Resolve technical issues through debugging, research, and investigation.
Who You Are (Basic Qualifications)
BA/BS or Master’s degree in Computer Science, Computer Engineering, Information Science or Data Science/Statistics
Hands-on experience in data engineering, business intelligence, data modeling, building ETL pipelines and multi-dimensional data warehouses.
Hands-on experience with and strong understanding of RDBMS concepts and query optimization.
Strong coding experience in developing enterprise applications using Java and Python
Understanding of DevOps/DataOps and CI/CD toolset such as git, GitLab CI, GitHub Actions.
Experience building, scaling, and maintaining high volume systems.
Experience analyzing large complex data sets to resolve data quality and performance issues.
Experience mentoring a team of data engineers and helping set standards and frameworks.
What Will Put You Ahead
Experience with Cloud system architecture and design, large scale streaming data processing
Experience with Spring framework, AOP, JPA and REST
Experience with building RESTful web services
Experience with libraries like scikit-learn, keras and functional programming concepts.
At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.
Hiring Philosophy
All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.
Who We Are
Koch Industries creates and innovates a wide spectrum of products and services that make life better. Our work spans a vast number of industries across the world, including engineered technology, refining, chemicals and polymers, pulp and paper, glass, electronics and many more. Headquartered in Wichita, Kansas, Koch employs 122,000+ employees across the globe.
At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.
Our Benefits
Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.
Equal Opportunities
Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information: http://www.kochcareers.com/doc/Everify.pdf
Show more
Show less","PostgreSQL, Elasticsearch, Snowflake, Spark, Java, Python, Spring framework, AOP, JPA, REST, scikitlearn, keras, Data engineering, Business intelligence, Data modeling, ETL pipelines, Multidimensional data warehouses, DevOps, DataOps, NoSQL, RDBMS, SQL, Git, GitLab CI, GitHub Actions, Cloud system architecture, Large scale streaming data processing, RESTful web services","postgresql, elasticsearch, snowflake, spark, java, python, spring framework, aop, jpa, rest, scikitlearn, keras, data engineering, business intelligence, data modeling, etl pipelines, multidimensional data warehouses, devops, dataops, nosql, rdbms, sql, git, gitlab ci, github actions, cloud system architecture, large scale streaming data processing, restful web services","aop, business intelligence, cloud system architecture, data engineering, datamodeling, dataops, devops, elasticsearch, etl pipelines, git, github actions, gitlab ci, java, jpa, keras, large scale streaming data processing, multidimensional data warehouses, nosql, postgresql, python, rdbms, rest, restful web services, scikitlearn, snowflake, spark, spring framework, sql"
Data Engineer SME (2021-0181),Acclaim Technical Services,"Chantilly, VA",https://www.linkedin.com/jobs/view/data-engineer-sme-2021-0181-at-acclaim-technical-services-3787775562,2023-12-17,Mount Vernon,United States,Mid senior,Onsite,"Acclaim Technical Services, founded in 2000, is a leading language and intelligence services company supporting a wide range of U.S. Federal agencies. We are an Employee Stock Ownership Plan (ESOP) company, which is uncommon within our business sector. We see this as a significant strength, and it shows: ATS is consistently ranked as a top workplace among DC area firms and continues to grow.
We are actively hiring a
Data Engineer (SME) with TS/SCI clearance and polygraph
to join our team working in Chantilly, Virginia.
Responsibilities
Create and maintain a data pipeline architecture
Assembling large, complex sets of data that meet mission requirements
Identify, design, and implement improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes
Build required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS, SQL, NiFi technologies
Build analytical tools to utilize the data pipeline, providing actionable insight into data performance including operational efficiency and customer acquisition
Working with mission stakeholders and assist them with data-related technical issues
Working with technology stakeholders to support their data infrastructure needs while assisting with data-related technical issues
Required Education & Experience
Bachelor’s degree in information systems, informatics, statistics, or computer science/software engineering
Ability to build and optimize data sets, 'big data' data pipelines and architectures
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytic skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Minimum of five years of experience using some of the below software and tools:
Big data tools like Kafka, Spark and Hadoop
Relational NoSQL and SQL databases including Cassandra and PostgreSQL
Workflow management and pipeline tools such as Airflow, Luigi and Azkaban
AWS close services including Redshift, RDS, EMR and EC2
Stream-processing systems like Spark-Streaming and Storm
Object function/object-oriented scripting languages including Scala, C++, Java and Python.
Data workflow orchestration tools including Pentahoe or Apache NiFi
Equal Employment Opportunity / Affirmative Action
ATS is committed to a program of equal employment opportunity without regard to race, color, ethnicity, national origin, ancestry, citizenship, sex, pregnancy, marital status, sexual orientation, gender identity, age, religion/creed, hairstyles and hair textures, handicap/disability, genetic information/history, military/veteran status, or any other characteristic or condition protected by federal, state or local law. It is the policy of ATS not merely to refrain from employment discrimination as required by the various federal, state, and local enactments, but to take positive affirmative action to realize for women, people of color, individuals with disabilities and protected veterans full equal employment opportunity. We support the employment and advancement in employment of individuals with disabilities and of protected veterans, and we treat qualified individuals without discrimination on the basis of their physical or mental disability or veteran status.
Powered by JazzHR
vXdvpSOgFQ
Show more
Show less","Data Pipeline Architecture, Big Data Tools (Kafka Spark Hadoop), Relational NoSQL and SQL Databases (Cassandra PostgreSQL), Workflow Management and Pipeline Tools (Airflow Luigi Azkaban), AWS Cloud Services (Redshift RDS EMR EC2), StreamProcessing Systems (SparkStreaming Storm), Object Function/ObjectOriented Scripting Languages (Scala C++ Java Python), Data Workflow Orchestration Tools (Pentahoe Apache NiFi)","data pipeline architecture, big data tools kafka spark hadoop, relational nosql and sql databases cassandra postgresql, workflow management and pipeline tools airflow luigi azkaban, aws cloud services redshift rds emr ec2, streamprocessing systems sparkstreaming storm, object functionobjectoriented scripting languages scala c java python, data workflow orchestration tools pentahoe apache nifi","aws cloud services redshift rds emr ec2, big data tools kafka spark hadoop, data pipeline architecture, data workflow orchestration tools pentahoe apache nifi, object functionobjectoriented scripting languages scala c java python, relational nosql and sql databases cassandra postgresql, streamprocessing systems sparkstreaming storm, workflow management and pipeline tools airflow luigi azkaban"
Sr Data Engineer,Talint,"Washington, DC",https://www.linkedin.com/jobs/view/sr-data-engineer-at-talint-3788756875,2023-12-17,Mount Vernon,United States,Mid senior,Onsite,"Job Description
Talint
's client, a management consulting firm provides technology services and delivers deep-tech/innovation for a variety of mission-critical fields to support U.S. National Security, is a fast growing, employee focused company. They are actively hiring talented data and engineering experts looking for a challenging yet rewarding career path in the Intelligence Community (IC).
Our client has an immediate need for a
Sr
Data Engineer
. This person will provide technical support in data science, data engineering, and systems engineering and reviewing and providing technical assessments on a new
Five-year
contract
. The right candidate will have at least three years of experience in data analysis, using data programming tools (Python and R), JavaScript, REACT, and AngularJS. Additionally, the right person will have a
Top Secret Clearance with a Full-Scope Polygraph
. This role is onsite in
McLean, VA
.
Desired Skills and Experience:
Top Secret Clearance with Full-Scope Polygraph;
5 years of experience in business intelligence reporting tools and data visualization software including Tableau;
Knowledgeable with JavaScript, REACT, and AngularJS;
Experience with data cleaning and transformation efforts in delivery of CDRLs;
Expert with extracting and aggregating structured and unstructured data;
Knowledgeable in data programming languages such as Python and R;
Familiar with SQL or similar database language;
Experience with designing and implementing data models to enable, sustain, and enhance the value of information they contain.
Why our client?
Competitive total compensation and benefits package that includes health and paid time off;
Our client fosters an open-door policy between employees and leadership to ensure direct lines of communication;
Incredible opportunities for advancement as they continue to grow;
Training and development is a key tenet of our client.
Responsibilities:
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers;
Make sure pedigree and provenance of the data is maintained such that the access to data is protected;
Clean and preprocess data to enable analytic access;
Collaborate with the engineering team, data stewards, and mission partners to aid in getting actionable value out of the data holdings architects complex, repeatable ETL processes;
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic, and others;
Develop API connectors to enable ingest of new data catalog entries from databases and files.
Company Description
At Talint, we believe in the strength and opportunity of small businesses and recognize their work environments as optimal places for people to innovate, excel and thrive. We work with candidates to position them in fulfilling roles that offer heightened visibility, latitude, enhanced responsibilities and access to leadership.
A stark contrast from other hiring agencies, we pride ourselves on personable, transparent and trustworthy relationships with job seekers. We don’t let communication lag and we approach every step of your placement experience with diligence.
As DC natives who are well connected, we know the market and understand the lifestyle that comes with the territory so you can count on sound advice from all angles – work, transit, housing and more.
At Talint, we believe in the strength and opportunity of small businesses and recognize their work environments as optimal places for people to innovate, excel and thrive. We work with candidates to position them in fulfilling roles that offer heightened visibility, latitude, enhanced responsibilities and access to leadership. A stark contrast from other hiring agencies, we pride ourselves on personable, transparent and trustworthy relationships with job seekers. We don’t let communication lag and we approach every step of your placement experience with diligence. As DC natives who are well connected, we know the market and understand the lifestyle that comes with the territory so you can count on sound advice from all angles – work, transit, housing and more.
Show more
Show less","Data engineering, Systems engineering, Data analysis, Python, R, JavaScript, React, AngularJS, Tableau, SQL, Spark, Hudi, EMR cloud services, Kubernetes containers, Oracle, MySQL, MariaDB, MongoDB, Elastic, Data pipelines, Data pedigree and provenance, Data cleaning and preprocessing, ETL processes, Data stewards, Mission partners, Data holdings architects, Actionable value, Advanced Database Administration","data engineering, systems engineering, data analysis, python, r, javascript, react, angularjs, tableau, sql, spark, hudi, emr cloud services, kubernetes containers, oracle, mysql, mariadb, mongodb, elastic, data pipelines, data pedigree and provenance, data cleaning and preprocessing, etl processes, data stewards, mission partners, data holdings architects, actionable value, advanced database administration","actionable value, advanced database administration, angularjs, data cleaning and preprocessing, data engineering, data holdings architects, data pedigree and provenance, data stewards, dataanalytics, datapipeline, elastic, emr cloud services, etl, hudi, javascript, kubernetes containers, mariadb, mission partners, mongodb, mysql, oracle, python, r, react, spark, sql, systems engineering, tableau"
Senior Data Engineer Informatica,"CTC Group, Inc","Lanham, MD",https://www.linkedin.com/jobs/view/senior-data-engineer-informatica-at-ctc-group-inc-3699649066,2023-12-17,Mount Vernon,United States,Mid senior,Hybrid,"Must be US Citizen to be considered
Position Overview:
As a Senior Informatica Data Engineer, you will take charge of designing, developing, and maintaining our ETL processes, ensuring the seamless flow of data across various systems. You will collaborate with cross-functional teams to shape data solutions that support our strategic goals, mentor junior developers, and drive data integrity and quality.
Responsibilities
Lead the design and architecture of complex ETL solutions to transform and load data from diverse sources into target data warehouses.
Support the analysis, collection, transformation and ingestion of complex datasets in support of Data Migration of mission critical systems of our clients.
Design, develop, monitor, and maintain solutions on the Informatica platform.
Integrate with broader technology architecture used across the organization.
Evaluate, develop, test and build ETL solutions on Informatica PowerCenter based on project requirements.
Analyze data issues, understand requirements, create specifications, design data workflows, provide estimates.
Support environment setup, monitoring and support change management in a multi-vendor environment.
Collaborate with cross-functional teams to deliver data solutions that align with business objectives.
Qualifications
7+ years of experience with Informatica PowerCenter, workflows, mappings, complex queries, scripting and tuning
5+ years of experience working with Data and the Cloud environment
Experience with enterprise-level design and implementation of data migration from legacy repositories, including Access DB, SQL Server, Oracle, or PostgreSQL
Experience developing technical design documentation in collaboration with functional and integration teams
Experience in bulk importing Flat files such as CSV, XML. JSON
Experience with performance tuning, deployment scripts, or reusable frameworks
Excellent problem-solving skills and attention to detail.
Effective communication skills to collaborate with technical and non-technical stakeholders.
Demonstrated ability to lead projects and mentor junior team members.
Applicants selected will be subject to a government background investigation
Show more
Show less","Informatica PowerCenter, ETL, Data Warehousing, Data Migration, Data Analysis, Data Transformation, Data Ingestion, Data Quality, Data Integration, Data Architecture, Data Workflows, Data Modeling, Data Governance, SQL Server, Oracle, PostgreSQL, Access DB, CSV, XML, JSON, Flat files, Performance Tuning, Deployment Scripts, Reusable Frameworks","informatica powercenter, etl, data warehousing, data migration, data analysis, data transformation, data ingestion, data quality, data integration, data architecture, data workflows, data modeling, data governance, sql server, oracle, postgresql, access db, csv, xml, json, flat files, performance tuning, deployment scripts, reusable frameworks","access db, csv, data architecture, data governance, data ingestion, data integration, data migration, data quality, data transformation, data workflows, dataanalytics, datamodeling, datawarehouse, deployment scripts, etl, flat files, informatica powercenter, json, oracle, performance tuning, postgresql, reusable frameworks, sql server, xml"
Data Integration Engineer,The Wills Group,"La Plata, MD",https://www.linkedin.com/jobs/view/data-integration-engineer-at-the-wills-group-3774785402,2023-12-17,Mount Vernon,United States,Mid senior,Hybrid,"Join our dynamic IT team at The Wills Group, as a Data Integration Engineer. You'll play a pivotal role in operationalizing data and analytics for our digital initiatives. Responsibilities include building and optimizing data pipelines, internal analytics, and system integrations. We're seeking self-motivated candidates who thrive in a fast-paced, small team environment and enjoy utilizing data to develop creative reporting and integrations that solve business challenges.
Key Responsibilities:
Develop and maintain data pipelines for reporting.
Create and manage system integrations using Azure Data Factory and ERP-specific tools.
Collaborate with IT Solutions team to gather requirements for reporting and integrations.
Maintain and enhance algorithms using Azure Machine Learning.
Document system integrations.
Create and maintain Power BI datasets and SQL Server Reporting Services reports.
Collaborate with IT leadership to implement self-service automation and analytics.
Act as an escalation point for support desk requests related to reporting and integrations.
Qualifications and Experience:
Bachelor's degree in computer science, Data Management, or equivalent experience.
3 years of experience in data management, integration, and optimization.
3 years of hands-on experience with relational SQL databases.
2 years of experience with object-oriented/object function scripting languages such as Python or R.
2 years of experience with Azure data management services (Azure Data Factory or Azure Synapse) required.
1 year of experience with Power BI.
Proficiency with SQL Server Reporting Services.
Working knowledge of Regular Expressions (REGEX).
Strong data analysis skills and attention to detail.
Excellent written and verbal communication skills.
US Citizenship or green card required; C2C or sponsorship opportunities not currently available
Preferred Qualifications:
Experience supporting a retail, food service, or consumer goods company in an IT role.
Availability and Travel:
Core hours: Monday – Friday, 8:00 AM - 4:30 PM ET
Hybrid schedule: 2 days per week in-office, with the remainder remote.
Occasional travel required for company events, industry events, training, and conferences.
Competencies:
Proven ability to deliver results and meet customer expectations.
Collaborative team player.
Effective adaptation to change.
Strong technological expertise.
Strong analytical skills.
Why You Should Join Wills Group
As a thriving, family-owned, $1.5 billion company headquartered in scenic La Plata, Maryland, (a 45-minute commute from Washington, DC), we take pride in our strong presence across the Mid-Atlantic region. Featuring nearly 300 retail locations of our family of brands including Dash In, Splash In ECO Car Wash, and SMO Motor Fuels, we are shaping the future of convenience retailing, fuels marketing, and commercial real estate.
Since 1926, our work-hard, play-hard mentality propels us to serve the communities that have supported us throughout the years. Keeping lives in motion is more than our mission--it's our way of life! We're dedicated to empowering individuals to embrace new possibilities and chart their own paths to success. Discover the fulfillment of working alongside passionate professionals, where your ideas are valued, and your potential is nurtured. Become part of something bigger when you join the Wills Group!
Benefits and Perks
Embark on a rewarding journey where your growth, future, and well-being take center stage! As a certified Great Place to Work™, the Wills Group understands today's professionals desire meaningful careers with a culture that's as authentic as possible. We pride ourselves in fostering an environment that supports your overall development. Look forward to joining a company that celebrates your wins whether big or small. You can count on us to provide industry-leading total rewards packages that include a range of benefits and perks that contribute to your overall well-being:
Financial Well-being – Employer 401(k) match (currently at 7%), health savings plan, and financial planning.
Physical Well-being – Comprehensive health, vision, and dental plans tailored to meet the needs of our people and their families, even their pets!
Paid Time Off – Vacation, sick, personal, community engagement, and parental leave for new parents.
Work/Life Balance – Hybrid and Flexible work environment, Employee Assistance Program, travel assistance, family life planning.
Education and Development Opportunities – 100% tuition reimbursement to support our people’s education goals, robust development programs, and certificate program assistance (up to 100% employer-paid).
Competitive Salary - Competitive pay matched to DC Metro area.
Bonus Opportunity - Up to 12.5% annually
Wills Group is an equal opportunity employer. Wills Group does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need.
Show more
Show less","Data Integration, Data Pipelines, Data Analysis, Machine Learning, SQL, Azure Data Factory, Azure Machine Learning, Power BI, SQL Server Reporting Services, Regular Expressions, Python, R, Objectoriented Programming, Data Management, System Integration, Tableau, Reporting, Relational Databases, Business Intelligence, Data Visualization, Software Development, Software Integration, Data Architecture, Cloud Computing, Big Data, Agile Development, Data Mining","data integration, data pipelines, data analysis, machine learning, sql, azure data factory, azure machine learning, power bi, sql server reporting services, regular expressions, python, r, objectoriented programming, data management, system integration, tableau, reporting, relational databases, business intelligence, data visualization, software development, software integration, data architecture, cloud computing, big data, agile development, data mining","agile development, azure data factory, azure machine learning, big data, business intelligence, cloud computing, data architecture, data integration, data management, data mining, dataanalytics, datapipeline, machine learning, objectoriented programming, powerbi, python, r, regular expressions, relational databases, reporting, software development, software integration, sql, sql server reporting services, system integration, tableau, visualization"
Senior Data Engineer,"Modern Technology Solutions, Inc. (MTSI)","Springfield, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-modern-technology-solutions-inc-mtsi-3754931303,2023-12-17,Mount Vernon,United States,Mid senior,Hybrid,"Own Your Future.
Modern Technology Solutions, Inc. (MTSI), is seeking a
Senior Data Engineer
.
Why is MTSI known as a Great Place to Work?
Interesting Work: Our co-workers support some of the most important and critical programs to our national defense and security.
Values: Our first core value is that employees come first. We challenge our co-workers to provide the highest level of support and service, and reward them with some of the best benefits in the industry.
100% Employee Ownership: we have a stake in each other's success, and the success of our customers. It's also nice to know what's going on across the company; we have company wide town-hall meetings three times a year.
Great Benefits - Most Full-Time Staff Are Eligible for:
Starting PTO accrual of 20 days PTO/year + 10 holidays/year
Flexible schedules
6% 401k match with immediate vesting
Semi-annual bonus eligibility (July and December)
Company funded Employee Stock Ownership Plan (ESOP) - a separate qualified retirement account
Up to $10,000 in annual tuition reimbursement
Other company funded benefits, like life and disability insurance
Optional zero deductible Blue Cross/Blue Shield health insurance plan
Track Record of Success: We have grown every year since our founding in 1993
Modern Technology Solutions, Inc. (MTSI) is a 100% employee-owned engineering services and solutions company that provides high-demand technical expertise in Digital Transformation, Modeling and Simulation, Rapid Capability Development, Test and Evaluation, Artificial Intelligence, Autonomy, Cybersecurity and Mission Assurance.
MTSI delivers capabilities to solve problems of global importance. Founded in 1993, MTSI today has employees at over 20 offices and field sites worldwide.
For more information about MTSI, please visit www.mtsi-va.com.
Responsibilities:
MTSI is looking for a Senior Data Engineer to be a member of our team in support of the Army Geospatial Center (AGC). Qualified candidates will support a team with development, testing, and deployment of next-generation geospatial basemaps, derived from open-source content, in support of the US Army.
Responsibilities
:
Support data processing initiatives to create tiled basemap datasets from open-source and/or Department of Defense content
Perform regular updates to foundation datasets, incorporating new data as it becomes available
Design, optimize, and execute automated data processing pipelines for large datasets.
Coordinate with AGC stakeholders for development and testing of geospatial data dissemination systems
Provide subject matter expertise for containerization of data and data processing pipelines
Provide data management support to enterprise databases and datasets
Assist with migration, testing, and operation of data dissemination platforms
Provide research into automated data processing workflows for analysis and product generation of geospatial data
Support evaluation and experimentation of Tactical Servers and Edge Nodes as geospatial map servers
Qualifications:
Required
:
Active Secret Clearance
Bachelor’s degree in in Computer Science, Data Science, Data Engineering or a related field
15 or more years related work experience in engineering, computer science, or related engineering discipline
Experience with Docker
Experience optimizing PostgreSQL for large data processing
Experience with optimizing data processing workflows to include parallelization
Familiarity with Microservices
Strong Python and Javascript development skills
Strong written and oral communication skills, with emphasis on briefing to obtain decisions and solve technical issues and test plans/reports writing
Ability to work independently as well as part of a team
Willingness to learn, solve problems and perform in a dynamic work environment
Desired
:
Familiarity with GDAL
Cloud integration and deployment experience (Azure or AWS)
Development/integration experience with MapBox
Development experience with PostGIS
Security+ Certification
Please Note: U.S. Citizenship is required for most MTSI positions.
#mtsi
#SMD22
2023-8390
Show more
Show less","Data Engineering, Geospatial Basemaps, OpenSource Content, Tiled Basemap Datasets, Automated Data Processing Pipelines, Data Dissemination Systems, Containerization, Enterprise Databases, Data Dissemination Platforms, Tactical Servers, Edge Nodes, Docker, PostgreSQL, Parallelization, Microservices, Python, Javascript, GDAL, Azure, AWS, MapBox, PostGIS","data engineering, geospatial basemaps, opensource content, tiled basemap datasets, automated data processing pipelines, data dissemination systems, containerization, enterprise databases, data dissemination platforms, tactical servers, edge nodes, docker, postgresql, parallelization, microservices, python, javascript, gdal, azure, aws, mapbox, postgis","automated data processing pipelines, aws, azure, containerization, data dissemination platforms, data dissemination systems, data engineering, docker, edge nodes, enterprise databases, gdal, geospatial basemaps, javascript, mapbox, microservices, opensource content, parallelization, postgis, postgresql, python, tactical servers, tiled basemap datasets"
Data Analyst,Insight Global,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-analyst-at-insight-global-3775442767,2023-12-17,Stockbridge,United States,Mid senior,Onsite,"Job Description:
A client of Insight Global is looking to bring on a Project Coordinator. This individual will be joining the Cross Bore Program under the Operations Support team. The Cross Bore Program is accountable for ensuring execution of the Construction Operations Cross Bore Prevention, Inspection, and Remediation program. This position is responsible for the analytical research and analysis necessary to develop and support new analytic solutions within Operations Support/Asset Protection. The job will involve working with program implementation, data management, KPI metric development, operational planning and compliance monitoring to define questions and communicate and implement new initiatives; working with databases to extract the data; and analyzing the data to answer the questions.
Responsibilities:
Gathers, extracts, manipulates, analyzes and models data using analytical and statistical methods to assist with current and future planning decisions and programs
Work with business teams to translate data into actionable solutions
Identifies new sources of data and methods to improve data collection, analysis and reporting
Identify data flow gaps, develop and automate work flows to support program execution, and create data quality monitoring procedures (including data audits and exception tracking…etc.,)
Data extraction, cleaning, analyzing, and interpreting to collaborate with stakeholders to present findings
Create data controls and develop dashboards for leadership to keep a pulse on compliance programs
Effectively create queries and scripts to analyze and visualize internal and external data sources
Exhibit strong written and verbal skills.
Effectively communicates findings/recommendations with peers and senior management regarding analysis of data.
Support departmental projects with project management skills.
Able to work independently with multiple department members and external resources to define tasks and prioritize deliverable deadlines.
Requires willingness to enhance the overall productivity of the department by performing a wide variety of support tasks.
Requirements:
Associates or Bachelor’s Degree in Computer Science, Information Systems/Technology, GIS, Statistics, Economics, Business Intelligence Analytics or a related field
5-7 years of experience working with large data sets as a Data Analyst
Proficient in Power BI or Data ETL experience
Experience running queries and creating dashboards
Advanced Excel/Access skills, Statistical Analysis, Database management, Oracle, SQL, or VBA
Strong planning and organization skills
Analytical and problem-solving skills
Ability to recognize how tasks rank in order of urgency to prioritize time and attend to critical details
Overall good business acumen and ability to communicate effectively
Ability to work independently and problem solve
Ability to establish and maintain effective working relationships and communicate with all levels in the organization
Strong decision-making skills
Plusses:
Related experience in energy industry related analytical work, compliance reporting, program management
Show more
Show less","Data Analysis, Data Manipulation, Data Modeling, Data Collection, Data Extraction, Data Cleaning, Data Interpretation, Data Visualization, Data Analytics, Data Quality Monitoring, Data Audits, Exception Tracking, Dashboard Development, Querying, Scripting, Communication, Project Management, Problem Solving, Decision Making, Power BI, Data ETL, Excel, Access, Statistical Analysis, Oracle, SQL, VBA","data analysis, data manipulation, data modeling, data collection, data extraction, data cleaning, data interpretation, data visualization, data analytics, data quality monitoring, data audits, exception tracking, dashboard development, querying, scripting, communication, project management, problem solving, decision making, power bi, data etl, excel, access, statistical analysis, oracle, sql, vba","access, communication, dashboard development, data audits, data cleaning, data collection, data etl, data extraction, data interpretation, data manipulation, data quality monitoring, dataanalytics, datamodeling, decision making, excel, exception tracking, oracle, powerbi, problem solving, project management, querying, scripting, sql, statistical analysis, vba, visualization"
Data Engineer/ Tech Lead with PBM,Noralogic Inc,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-engineer-tech-lead-with-pbm-at-noralogic-inc-3739398027,2023-12-17,Stockbridge,United States,Mid senior,Onsite,"Job Location:-
Atlanta, GA(Onsite from day-1)
Long Term Contract
Data Engineer/ Tech Lead - Spark, Scala, AWS, Lambda, SQL, Redshift, Hive , PBM
Description Client is looking for a Data Engineer /Tech Lead to work with one of the leading healthcare providers in US as part of UST's PBM Practice. The ideal candidate may possess good background on Healthcare Business
Responsibilities
As a Data Engineer/ Tech Lead, you will Write system code, end to end unit test and documentation.
Lead/mentor other developers.
Work with architects and other leads to design solutions that articulate the business context, conceptual design and component-level logical design.
Ensure development is in compliance with overall architecture vision for the platform and ensures specific components are appropriately designed and leveraged Understand the construction of platform architecture components Participate in design activities and own the development of the work assigned.
Work closely with QA and integration team to resolve issues.
Able to understand the technology roadmap and delivers cost effectiveness, business value, and competitiveness
Requirements
10+ years of experience in design/development relevant technology disciplines
Experience working with Agile and continuous integration in a commercial product environment Healthcare knowledge is a plus Technology Stack Mandatory Spark Scala AWS Lambda SQL Redshift Hive Location Any Client location
Regards,
Priti Kumari
Technical Recruiter Noralogic Inc.
109 E 17th St, Cheyenne WY 82001
+1.307-274-3112(Priti@noralogic.com) |www.noralogic.com
https://www.linkedin.com/in/priti-sinha-526949150/
USA: WY, MD, NJwww.noralogic.com
Mexico: Guadalajara, Monterrey
India: Noida UP
**WBE and MBE company**
** ISO 9001:2015**
**WY Top 50 Minority owned growing company**
Show more
Show less","Spark, Scala, AWS, Lambda, SQL, Redshift, Hive, Agile, Continuous Integration, Healthcare, Software Development, Data Engineering, Technical Leadership, System Design, Unit Testing, Documentation, Mentoring, Architecture, Quality Assurance, Integration, Technology Roadmap, Cost Effectiveness, Business Value, Competitiveness","spark, scala, aws, lambda, sql, redshift, hive, agile, continuous integration, healthcare, software development, data engineering, technical leadership, system design, unit testing, documentation, mentoring, architecture, quality assurance, integration, technology roadmap, cost effectiveness, business value, competitiveness","agile, architecture, aws, business value, competitiveness, continuous integration, cost effectiveness, data engineering, documentation, healthcare, hive, integration, lambda, mentoring, quality assurance, redshift, scala, software development, spark, sql, system design, technical leadership, technology roadmap, unit testing"
Senior Data Engineer - POS Integrations,Crunchtime,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-pos-integrations-at-crunchtime-3729914297,2023-12-17,Stockbridge,United States,Mid senior,Remote,"Global restaurant brands run their operation on the Crunchtime platform. Delivering a consistent guest experience across every location and managing food and labor costs are at the core of how Crunchtime's software is used today in over 125,000 locations across 100+ countries by the world's top restaurant and foodservice operators. Customers including Chipotle, Culver's, Domino's, Dunkin', Five Guys and P.F. Chang's rely on our top-ranked platform which now includes Zenput to manage inventory, staff scheduling, learning and development, food safety, operational tasks and audits.
About The Role
Join our product development team focused on point-of-sale (POS) integrations. Our team creates bi-directional real-time integration to every POS that our customers deploy. The ideal candidate is someone who enjoys integration, continual learning, and a fast-paced development environment. If you possess a drive for excellence, thrive in a fast-paced environment, and desire to be part of a team that is dedicated to being an industry leader, then we want to hear from you.
What You'll Do As a Senior Data Engineer
Providing plans and estimates of effort and duration of projects and tasks to project management, and tracking tasks, progress, and milestones.
Design data pipeline solutions, analyze customer data, and identify data patterns, performing data profiling to reverse engineer schema or API documentation.
Design, develop, optimize, document, and support a high-performance data pipeline, delivery network, and bi-directional data exchange between customers and partners.
Develop automated Unit tests, Integration tests, and Regression tests.
Provide escalated technical support.
Provide software features and integration platform training.
Provide software process health monitoring and timely resolution.
What We're Looking For
4+ years professional data or software development experience with SQL and Python.
Solid analytical and problem-solving skills, detail-oriented mindset
Strong interpersonal skills as this position will interface with many levels both inside and outside of our organization.
Fundamental understanding of common data structure and algorithms.
Proficiency in data communication, transformation, manipulation and process management in both structured and semi-structured data.
Proficiency in manual and automated data testing.
Proficiency in building repeatable and idempotent ETL/ELT consuming APIs.
Nice to haves
Working experience designing and building APIs.
Working experience managing Linux, AWS, performance optimization.
Working experience developing CI/CD automation.
What You'll Get
Great mission-driven team members from diverse backgrounds with a strong company culture
Competitive pay
Unlimited PTO
Paid company holidays
Yearly team off-sites
International travel opportunities
Medical, dental, and vision benefits (FSA, HSA & HRA options)
Basic & Voluntary Life Insurance
401k employer match
Wellness benefits (Headspace, OneMedical, Omada, Ginger.io, Gympass, Carrot)
Commuter benefits
Work in an open environment on solutions that are reshaping the way businesses operate
Fun team events
Ability to have a big impact
10 weeks of paid parental leave
Fitness reimbursement
Learning & development funds
Show more
Show less","SQL, Python, Data pipeline solutions, Data profiling, Data exchange, Unit tests, Integration tests, Regression tests, ETL/ELT, APIs, Linux, AWS, Performance optimization, CI/CD automation, Data structures, Algorithms","sql, python, data pipeline solutions, data profiling, data exchange, unit tests, integration tests, regression tests, etlelt, apis, linux, aws, performance optimization, cicd automation, data structures, algorithms","algorithms, apis, aws, cicd automation, data exchange, data pipeline solutions, data profiling, data structures, etlelt, integration tests, linux, performance optimization, python, regression tests, sql, unit tests"
"Senior/Staff Software Engineer, Data Pipelines",EvenUp,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-staff-software-engineer-data-pipelines-at-evenup-3766509616,2023-12-17,Stockbridge,United States,Mid senior,Remote,"EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year.
Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds
. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.
The role:
We’re looking to bring on board
Senior/Staff Software Engineers
focused on our
Data Pipelines
as we’ve experienced unprecedented growth and need to build & scale out our data pipelines and infrastructure. We’re looking for strong team members to help architect and drive forward the vision of our ideal data infrastructure at EvenUp. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
What you'll do:
Build fault tolerant data pipelines to process diverse datasets at EvenUp
Design and develop modularized services to increase the capabilities and scope of our data infrastructure
Collaborate with our DS team to Integrate ML models into our production workflows and simplify ML deployment and observability
Implement event driven, low latency systems to empower our stakeholders with accurate and reliable data
Analyze and solve key performance bottlenecks, scaling challenges, and high availability issues.
Mentor and coach junior team members
Help grow our engineering team and define a “data first” mentality across our organization.
What we are seeking:
8+ years of industry experience designing and building distributed data systems
Previous experience architecting and scaling event driven architectures
Strong understanding and practical experience with data pipeline tooling and storage systems such as Dagster, DBT, BigQuery, Elasticsearch
The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions
Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure
You enjoy owning a project from start to finish and love to drive a project across the finish line.
Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves:
Fluency in Python, SQL and GraphQL
Previous experience integrating ML models and LLMs into data services
Domain expertise in legal technology, medical records, and working with unstructured data
Benefits & Perks:
We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:
Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!)
Flexible working hours to match your style
Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months!
Choice of great medical, dental, and vision insurance plan options
Flexible paid time off
A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Data Pipelines, Distributed Data Systems, Event Driven Architectures, Data Pipeline Tooling, Storage Systems, Dagster, DBT, BigQuery, Elasticsearch, Python, SQL, GraphQL, ML Models, LLMs, Legal Technology, Medical Records, Unstructured Data","data pipelines, distributed data systems, event driven architectures, data pipeline tooling, storage systems, dagster, dbt, bigquery, elasticsearch, python, sql, graphql, ml models, llms, legal technology, medical records, unstructured data","bigquery, dagster, data pipeline tooling, datapipeline, dbt, distributed data systems, elasticsearch, event driven architectures, graphql, legal technology, llms, medical records, ml models, python, sql, storage systems, unstructured data"
Lead Data Engineer,Equifax,"Atlanta, GA",https://www.linkedin.com/jobs/view/lead-data-engineer-at-equifax-3762951868,2023-12-17,Stockbridge,United States,Mid senior,Hybrid,"Equifax is where you can power your possible. If you want to achieve your true potential, chart new paths, develop new skills, collaborate with bright minds, and make a meaningful impact, we want to hear from you.
Equifax is searching for a Data Engineering Lead to join our world-class Entity Resolution (Keying & Linking) team within the Global Data & Analytics CoE. The successful candidate will lead our transformation of Entity Resolution capabilities to the Google Cloud and establish processes and practices for linking Equifax, partner and customer data to facilitate insights about businesses and the relationships between them. They may lead a small team of data scientists and data engineers who will explore a wide range of data assets using cutting edge analytics and statistics.
This role requires understanding large, diverse data sources and extensive experience developing data matching and entity resolution rules. Experience linking data about consumers and/or businesses will be critical as well.
What you will do
Work with key stakeholders and understand their needs to develop new or improve existing global Entity Resolution processes
Work in a cross-functional, matrix organization, at times under ambiguous circumstances.
Provide leadership and guidance in the design and development of scalable solutions using large datasets within a cloud platform and cross train new Big Data and/or data analyst on the team
Devise and deploy innovations, including but not limited to AI/ML techniques, graph databases and distributed computing, for Entity Resolution
Understand the various aspects of architecture practices: business, application, data, security, infrastructure and governance
Ensure quality end-to-end deliverables of developed technology solutions
Clearly communicate insights and impacts from process, technology and rule changes
What Experience You Need
Bachelors in Data/Computer Science, Statistics, Mathematics, Economics or other related discipline
Minimum of 7 years experience in data management and analysis in the credit risk, telecommunications, financial services, marketing, fraud, or insurance analytics arena
Minimum of 2 years experience in development and usage of search, match and entity resolution, building search systems, data matching, master data management of large (multi-TB to PB) complex data sets for consumer or commercial data
Minimum 5 years experience working with data schemas and data formats such as JSON, Parquet, AVRO etc.
Clear communication of technical or analytical results and recommendations to non-technical leaders
What Could Set You Apart
Masters Degree or PhD in Data/Computer Science, Statistics, Mathematics, Economics or other related discipline
Strong knowledge of credit bureau data
Experience or advanced degree with focus on Entity Resolution concepts, prototype or hand-on project
Experience in Data Visualization and storytelling
We offer comprehensive compensation and healthcare packages, 401k matching, paid time off, and organizational growth potential through our online learning platform with guided career tracks.
Are you ready to power your possible? Apply today, and get started on a path toward an exciting new career at Equifax, where you can make a difference!
Equifax is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","Data Engineering, Entity Resolution, Google Cloud, Data Matching, AI/ML, Graph Databases, Distributed Computing, Data Schemas, Data Formats, JSON, Parquet, AVRO, Credit Bureau Data, Data Visualization, Storytelling","data engineering, entity resolution, google cloud, data matching, aiml, graph databases, distributed computing, data schemas, data formats, json, parquet, avro, credit bureau data, data visualization, storytelling","aiml, avro, credit bureau data, data engineering, data formats, data matching, data schemas, distributed computing, entity resolution, google cloud, graph databases, json, parquet, storytelling, visualization"
Sr. Data Analytics Consultant,"Bytecode IO, Inc","Vermont, United States",https://www.linkedin.com/jobs/view/sr-data-analytics-consultant-at-bytecode-io-inc-3775009687,2023-12-17,Huntington,United States,Mid senior,Remote,"Bytecode IO is a growing data consulting company that is looking for a customer-focused Data Analytics Consultant to join our team. We work on the latest technologies - Snowflake, BigQuery, Stitch, Looker - with a wide range of fast growing companies.
The ideal candidate can work with minimal direction delivering analytic solutions to unlock data value. You will engage on a variety of client projects using both new and proven technologies.
This is a remote full-time W2 position - US only based
You must prove you are authorized to work in the US - if hired
ABOUT THE ROLE
Develop data models, reports and dashboards from ideation through production
Integrate and transform data for analysis using SQL, ETL tools and API integrations
Validate data to ensure accuracy
Provide guidance to clients on optimizing their data environment
Work with clients (product managers, marketers, engineering team) to define requirements, establish priorities, offer solutions and execute development
Perform technical and business user training
Execute projects with minimal guidance
WHO YOU ARE
Required:
5+ years of SQL experience
Consulting experience leading projects
Technical expertise with data modeling and database design
Experience with business intelligence software
Excellent communication and problem-solving skills
Ability to manage multiple clients concurrently
Availability to take conference calls with clients during business hours
Interest in mentoring
Great if you have (not a must):
Experience working remotely
Experience with Looker
Familiarity with Python, Javascript, HTML
Experience with SQL-type database administration
BS in Computer Science, Computer Information Systems, Engineering, Statistics or Mathematics
BENEFITS
Flexible daily schedule to maximize work / life balance
Health Benefits
Vacation, Sick Leave, Paid public holiday
401K
Training and Certification on Looker, AWS and Google Cloud
Company Laptop
Show more
Show less","Data Analytics, Snowflake, BigQuery, Stitch, Looker, SQL, ETL tools, API integrations, Data modeling, Database design, Business intelligence software, Communication skills, Problemsolving skills, Client management, Remote work, Looker, Python, Javascript, HTML, SQLtype database administration, Computer Science, Computer Information Systems, Engineering, Statistics, Mathematics","data analytics, snowflake, bigquery, stitch, looker, sql, etl tools, api integrations, data modeling, database design, business intelligence software, communication skills, problemsolving skills, client management, remote work, looker, python, javascript, html, sqltype database administration, computer science, computer information systems, engineering, statistics, mathematics","api integrations, bigquery, business intelligence software, client management, communication skills, computer information systems, computer science, dataanalytics, database design, datamodeling, engineering, etl tools, html, javascript, looker, mathematics, problemsolving skills, python, remote work, snowflake, sql, sqltype database administration, statistics, stitch"
Data Warehouse Analyst / Analytics Engineer,Starling Bank,"Cardiff, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/data-warehouse-analyst-analytics-engineer-at-starling-bank-3767388121,2023-12-17,Cardiff, United Kingdom,Mid senior,Onsite,"Starling is the UK's first and leading digital bank on a mission to fix banking! Our vision is fast technology, fair service, and honest values. All at the tap of a phone, all the time.
We are about giving customers a new way to spend, save and manage their money while taking better care of the planet which has seen us become a multi-award winning bank that now employs over 2800 across five offices in London, Cardiff, Dublin, Southampton, and Manchester. Our journey started in 2014, and since then we have surpassed 3.5 million accounts (and four account types!) with 350,000 business customers. We are a fully licensed UK bank but at the heart, we are a tech first company, enabling our platform to deliver brilliant products.
Our technologists are at the very heart of Starling and enjoy working in a fast-paced environment that is all about building things, creating new stuff, and disruptive technology that keeps us on the cutting edge of fintech. We operate a flat structure to empower you to make decisions regardless of what your primary responsibilities may be, innovation and collaboration will be at the core of everything you do. Help is never far away in our open culture, you will find support in your team and from across the business, we are in this together!
The way to thrive and shine within Starling is to be a self-driven individual and be able to take full ownership of everything around you: From building things, designing, discovering, to sharing knowledge with your colleagues and making sure all processes are efficient and productive to deliver the best possible results for our customers. Our purpose is underpinned by five Starling values: Listen, Keep It Simple, Do The Right Thing, Own It, and Aim For Greatness.
Hybrid Working
We have a Hybrid approach to working here at Starling - our preference is that you're located within a commutable distance of one of our offices so that we're able to interact and collaborate in person. We don't like to mandate how much you visit the office and work from home, that's to be agreed upon between you and your manager.
Responsibilities:
Translate data requirements from across the organisation in to robust and reusable data models
Apply data transformation to datasets via dbt
Follow established SDLC standards / processes for rapid and reliable releasing of changes to production
Maintain consistent and clear documentation and definitions across the data warehouse and Looker
Collaborate with the wider data team to help meet the business goals
Requirements
Strong experience with SQL
Strong experience with Looker or a similar visualisation tool
Strong experience with dbt or a desire to learn
Strong communication skills
Experience with data architecture and dimensional modelling
Experience supporting and working with cross-functional teams in a dynamic environment
Interview Process
Interviewing is a two way process and we want you to have the time and opportunity to get to know us, as much as we are getting to know you! Our interviews are conversational and we want to get the best from you, so come with questions and be curious. In general you can expect the below, following a chat with one of our Talent Team:
Stage 1 - 30 mins with one of the team
Stage 2 - 60 mins technical interview with two team members
Stage 3 - 45 min final with an executive and a member of the people team
Benefits
25 days holiday (plus take your public holiday allowance whenever works best for you)
An extra day's holiday for your birthday
Annual leave is increased with length of service, and you can choose to buy or sell up to five extra days off
16 hours paid volunteering time a year
Salary sacrifice, company enhanced pension scheme
Life insurance at 4x your salary & group income protection
Private Medical Insurance with VitalityHealth including mental health support and cancer care. Partner benefits include discounts with Waitrose, Mr&Mrs Smith and Peloton
Generous family-friendly policies
Incentives refer a friend scheme
Perkbox membership giving access to retail discounts, a wellness platform for physical and mental health, and weekly free and boosted perks
Access to initiatives like Cycle to Work, Salary Sacrificed Gym partnerships and Electric Vehicle (EV) leasing
About Us:
You may be put off applying for a role because you don't tick every box. Forget that! While we can't accommodate every flexible working request, we're always open to discussion. So, if you're excited about working with us, but aren't sure if you're 100% there yet, get in touch anyway.
We're on a mission to radically reshape banking - and that starts with our brilliant team. Whatever came before, we're proud to bring together people of all backgrounds and experiences who love working together to solve problems.
Starling Bank is an equal opportunity employer, and we're proud of our ongoing efforts to foster diversity & inclusion in the workplace. Individuals seeking employment at Starling Bank are considered without regard to race, religion, national origin, age, sex, gender, gender identity, gender expression, sexual orientation, marital status, medical condition, ancestry, physical or mental disability, military or veteran status, or any other characteristic protected by applicable law.
By submitting your application, you agree that Starling Bank may collect your personal data for recruiting and related purposes. Our Privacy Notice explains what personal information we may process, where we may process your personal information, its purposes for processing your personal information, and the rights you can exercise over our use of your personal information.
Show more
Show less","SQL, Looker, dbt, Data visualization, Data architecture, Dimensional modeling, Data transformation, Software development life cycle, Data warehousing","sql, looker, dbt, data visualization, data architecture, dimensional modeling, data transformation, software development life cycle, data warehousing","data architecture, data transformation, datawarehouse, dbt, dimensional modeling, looker, software development life cycle, sql, visualization"
Senior Data Engineer,Creditsafe Group,"Cardiff, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-creditsafe-group-3756950762,2023-12-17,Cardiff, United Kingdom,Mid senior,Onsite,"We are currently looking Senior Data Engineer to join the Data Cloud team.
WHO ARE WE?
Privately owned and independently-minded, Creditsafe operates with the singular vision of powering business decisions. We do this by delivering valuable intelligence on customers, suppliers and potential buyers to corporates, public sector organisations and SMEs globally.
Our journey began in 1997 in Oslo, Norway in 1997, with a revolutionary dream to make business information accessible to all. Twenty-five years later, we’ve not only realised this dream, changed the market for the better, made data intelligence accessible to all businesses big and small but most importantly, opened up new avenues of data intelligence for businesses with machine learning, AI and connected data.
From risk management through to opportunity identification, our industry-leading solutions, power decisions for companies by turning their data into actionable insights that help them become stronger, grow faster and thrive.
WHY CREDITSAFE
Career Progression: Clearly defined career progression path with opportunities to grow as a technical leader or move into adjacent roles.
Learning & Development: Access to a plethora of learning resources and courses to perpetually enhance your skills, with weekly dedicated training and focus time.
Work-Life Balance: Committed to promoting a balanced lifestyle with flexible working hours and hybrid work options.
Innovation: Engage in exciting projects, harnessing the power of the latest technologies in cloud computing and data management.
Our Technology Stack: Python, Linux, Airflow, AWS DynamoDB, S3, Glue, Athena, Redshift, lambda, API Gateway, Terraform, CI/CD
JOB PROFILE
This opportunity is to join a team of highly technically skilled engineers who are redesigning Creditsafe’s data platform with high throughput and scalability as the primary goal. The data delivery platform is being built upon AWS Redshift and S3 cloud storage. The platform is expected to manage over a billion objects along with daily increments of more than 10 million objects while handling addition, deletion, and correction of our data and indexes in an auditable manner. Our data processing application is entirely based on Python and designed to efficiently transform incoming raw data volume into API consumable schema. The team is also building highly available and low latency APIs to enable our clients with faster data delivery.
Key Duties And Responsibilities
You will actively contribute to the codebase and participate in peer reviews.
Design and build metadata driven, event based distributed data processing platform using technologies such as Python, Airflow, Redshift, DynamoDB, AWS Glue, S3.
As an experienced Engineer, you will play a critical role in the design, development, and deployment of our business-critical system.
You will be building and scaling Creditsafe’s data pipeline primarily moving data between Redshift and S3.
Execute practices such as continuous integration and deployment using test-driven development where applicable to enable the rapid delivery of working pipelines.
Understanding company & domain data to make recommendations to improve the existing product.
The responsibilities detailed above are not exhaustive and you may be requested to take on additional responsibilities deemed as reasonable by their direct line manager.
Skills And Qualifications
You understand and can implement data engineering best practices.
You demonstrate the ability to write clean & efficient code and combine it with the cloud environment for best performance.
You have a minimum 8 years of development experience within a commercial environment creating production grade data pipelines in python.
You are looking to grow your skills through daily technical challenges and enjoy problem solving, whiteboarding in collaboration with team.
You have excellent communication skills, and ability to explain your views clearly to the team and are open to understanding the views of others.
You have a proven track record to draw from a deep and broad technical expertise to mentor engineers, complete hands-on technical work, and provide leadership on complex technology issues.
Share your ideas collaboratively via wikis, discussions boards, etc and share any decisions made, for the benefit of others.
Benefits
Competitive Salary.
Company Laptop supplied.
Bonus Scheme.
25 Days Annual Leave (plus bank holidays).
Hybrid working model.
Healthcare & Company Pension.
Cycle to work and Wellbeing Programme.
Global Company gatherings and events.
E-learning and excellent career progression opportunities.
Plus more that can be found on the benefits section on the Careers page, https://careers.creditsafe.com/gb.
Creditsafe is an equal opportunities employer that values diversity. Please contact Creditsafe if there is any support you need with your application.
Show more
Show less","Python, Linux, Airflow, AWS, DynamoDB, S3, Glue, Athena, Redshift, Lambda, API Gateway, Terraform, CI/CD, Continuous Integration, Deployment, TestDriven Development, Software Development, Data Architecture, Data Engineering, Data Analytics, Cloud Computing, Data Management, Data Science, Machine Learning, Artificial Intelligence","python, linux, airflow, aws, dynamodb, s3, glue, athena, redshift, lambda, api gateway, terraform, cicd, continuous integration, deployment, testdriven development, software development, data architecture, data engineering, data analytics, cloud computing, data management, data science, machine learning, artificial intelligence","airflow, api gateway, artificial intelligence, athena, aws, cicd, cloud computing, continuous integration, data architecture, data engineering, data management, data science, dataanalytics, deployment, dynamodb, glue, lambda, linux, machine learning, python, redshift, s3, software development, terraform, testdriven development"
Senior Data Engineer,Confused.com,"Cardiff, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-confused-com-3724242001,2023-12-17,Cardiff, United Kingdom,Mid senior,Onsite,"Description
Hybrid from our Cardiff office, 2 days a week.
About Confused.com
Confused.com is the UK’s first comparison platform for car insurance. We’ve been helping customers since 2002 by empowering them to make better decisions around insurance and financial services. Our mission is simple: take away the confusion when comparing financial products and services to help you save time and money.
We’re part of RVU - a global group of online brands (including Uswitch and Money.co.uk) that work to empower consumers. We do this by helping people compare home services, insurance and other financial products.
About The Role
We are looking for a skilled data/software engineer to join our External Integrations team. The role requires strong collaboration with internal and external development teams and key stakeholders to help design, develop and implement robust, scalable, operational solutions to meet business needs.
This position demands a well-organised, action-oriented team player with excellent time management skills to support various work streams in a fast paced environment.
Must possess strong written and verbal communication skills, with the ability to communicate effectively with technical and non-technical stakeholders.
We work in a fast-paced, agile, and continuous delivery release-driven environment. The right candidate for this position will be motivated by this challenge and will help to develop our future ways of working.
What you’ll be doing:
Creating/designing/reviewing modern, scalable data pipelines using the Azure/databricks stack.
Ensuring data products are accurate and available in a timely manner.
Optimising storage, processes and pipelines.
Monitoring and maintaining existing processes, identifying opportunities for improvements.
Preparing and providing demos, creating user stories, requirements definitions and acceptance criteria using Agile tools and techniques.
Working with product owners and stakeholders to gather requirements and translate them into technical requirements as well as providing timely updates on progress and highlighting relevant risks or opportunities.
Staying up-to-date with emerging trends and technologies in data engineering.
What you'll bring to the role:
Strong knowledge of SQL and Python programming.
Extensive experience designing, implementing and maintaining robust data pipelines and transformation processes.
Extensive experience working within a cloud environment (Preferably Azure).
Experience with big data technologies (e.g. Spark, Databricks, Delta Lake, BigQuery).
Familiarity with event driven architectures (Event Hubs, Kafka etc…).
Demonstrable understanding of how to expose data from systems (eg. through APIs), link data from multiple systems and deliver streaming services.
Experience in modern software development tools / ways of working (Agile methodologies, GitHub, CI/CD & DevOps tools, infrastructure as code, metrics/monitoring etc…).
Ability to understand detailed technical requirements and explain technical concepts to both technical and non-technical stakeholders alike.
Acts with integrity and makes sound decisions in the best interest of the business.
Solid understanding of security best practice, accessibility, compliance and GDPR regulations.
Continuously looking for opportunities to learn, build skills and share learning both internally and externally.
You don’t need to tick off everything on this list - so don’t let that hold you back from applying. There are plenty of opportunities to learn with us!
Benefits
Our commitment to you:
At Confused.com we believe that we can be the change we wish to see in the world. We hold ourselves accountable for being open and inclusive teammates and community members. We embrace our differences and are committed to creating an inclusive environment that reflects the world we live in.
We want to give you a great work environment. We want to contribute to both your personal and professional development and give you great benefits to make your time at Confused.com even more enjoyable. Some of these benefits include:
A competitive salary and bonus package
Employer matching pension up to 7.5%
A hybrid approach of in-office and remote working
Annual leave of 25 days (excluding bank holidays) which will increase with length of service (up to a maximum of 30 days) and additional leave options.
A ‘Work from Home’ budget to help contribute towards a great work environment at home
Excellent maternity, paternity, and adoption leave policy, for those key moments in your life
Employee Assistance Programme (EAP), Simply Health Scheme, and access to the Calm app.
Regular events - company-wide events with insightful external speakers as we want to make sure our colleagues continue to feel connected.
A healthy learning and training budget, as well as the chance to go to conferences around the world every year
Private medical scheme
If you would like to know more, please get in touch by emailing recruitment@confused.com
Confused.com is proud to have been officially recognised as a 2022 UK’s Best Workplace™ and as a 2022 UK’s Best Workplace™ for Wellbeing in the Large size category!
As a tech company that strives to get better every day, we use Metaview during the interview processes to record and transcribe interviews so the interviewer can focus on the conversation rather than note-taking. This also helps us to continuously improve the quality of our interviews and your experience.
This has no bearing on the assessment of you as a candidate and you can opt out at any time, just let us know before your call.
Show more
Show less","Azure, Databricks, SQL, Python, Cloud Computing, Spark, Delta Lake, BigQuery, Kafka, Agile, Git, CI/CD, DevOps, REST API, Metrics, Monitoring, GDPR, Data Pipelines, Data Engineering","azure, databricks, sql, python, cloud computing, spark, delta lake, bigquery, kafka, agile, git, cicd, devops, rest api, metrics, monitoring, gdpr, data pipelines, data engineering","agile, azure, bigquery, cicd, cloud computing, data engineering, databricks, datapipeline, delta lake, devops, gdpr, git, kafka, metrics, monitoring, python, rest api, spark, sql"
Associate/Principal Mechanical Engineer - Data Centres,Hydrock,"Cardiff, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/associate-principal-mechanical-engineer-data-centres-at-hydrock-3752871822,2023-12-17,Cardiff, United Kingdom,Mid senior,Onsite,"Job Advert
We have a fantastic opportunity for a Principal or Associate Mechanical Engineer to join our established Data Centre specialist team within our successful national MEP division.
You will have the opportunity to develop your consulting engineering expertise with a clear plan for progression, honing your skills within commercial management, client service and people leadership skills.
The MEP Team
We excel in delivering MEP engineering design services from project inception to construction completion, emphasizing early involvement, collaboration with the design team and tailored solutions. We serve a wide range of sectors including commercial, industrial, leisure, retail, manufacturing, logistics, heritage and residential, for end-user clients, investors, developers, architects, project managers or engineer-led teams. You can check out MEP’s project portfolio here.
Responsibilities Of The Role
Undertake the design of data centre specific mechanical systems from RIBA 1 through to RIBA 4c. Be fully aware of the current technical trends in the sector and be able to apply these to new projects.
Lead projects (or elements of large projects) and take responsibility for the technical delivery and financial profitability of the project.
As the lead discipline for data centre projects, you will also manage sub-disciplines and external contractors, assisted by the team’s design managers.
Be an expert in the sector and support with the training/upskilling of the Hydrock MEP team.
Assist in the marketing of the team externally and be able to generate fee income for new and existing clients.
You’ll be set up for success if you have:
Proven experience as a Principal or Associate Mechanical Engineer with a strong track record in the UK leading mechanical (ideally M&E) delivery on a variety of Data Centre building service projects.
Relevant mechanical engineering qualification at HNC, HND, Bachelors or Masters level.
Ideally a chartered engineer or working towards membership with a relevant institution.
Sound knowledge of BIM processes, Revit, IES and other design software.
Working knowledge of current building regulations with a strong understanding and passion to apply sustainable design principles.
A passion for sharing knowledge and developing junior members of the team.
Experience of winning work, managing teams, and undertaking business operations such as marketing, commercial and financial responsibilities (Associate level only).
What's Great About Hydrock 'in a Nutshell'
We are a British-owned integrated multi-disciplinary engineering consultancy of over 900 staff in 21 offices across the UK. Our driving motivation is to be a ‘Force for Good’, as it is our aim to improve the quality of people’s lives from our employees to our clients, through to the communities we work in and our planet as a whole through the work we do.
From the buildings that surround you, the roads and bridges you cross, all related to the infrastructure that we create, we aim to offer the most sustainable possibilities to shape the places, communities and society that we live in through meeting our client’s needs. Through the path we are on to delivering a green future, the result is something that everyone can be proud of.
Our welcoming and friendly culture is something we are proud of and has gained us recognition with 9 years in the Top 100 Best Companies to Work For list. Check out some of our incredible projects which have been awarded: 2022 Net Zero Award for Bay Technology Centre, Integration and Collaborative Working Award for YGG Tan-y-Lan primary school, The Deaf Academy awarded for its Universal Design at the 2022 Civic Trust Awards and Bristol’s iconic waterfront Wapping Wharf Living making a double win at the Bristol Property Awards!
To top off, here’s our 2022 wrap up video!
What We Can Offer You
Inspiring and supportive colleagues
Reward for progression and hard work
An opportunity to develop your soft skills, as well your technical skills
Competitive starting salary
Excellent health benefits 25 days of holiday (buy/sell up to five days), accrue 1 day extra every 2 years, with bonus holidays too!
An earlier finish on Friday (4pm!)
An opportunity to give back: “Day off for good cause” (on a workday)
A huge range of flexible benefits, including climate perks and an EV car leasing scheme
Our biggest event: Challenge Day!
A place to feel included
We champion diversity, equity and inclusion. As an Equal Opportunities Employer, we commit to supporting our employees and ensure we create a safe environment that nurtures you to perform at your best. Offering our people flexibility is an important factor in achieving this aim.
We consider all application individually with the required qualifications and knowledge without regard to any of the protected characteristics. We would like to provide everyone with a fair selection, assessment and employment experience so we ask with are made aware of any physical or neurodiverse condition within your application for which appropriate reasonable adjustments can be made by us.
Looking for the next steps?
Once you have completed your application through our careers site, we aim to review and respond to you as soon as your application’s been reviewed.
If shortlisted, a member of our Recruitment Team will call you for an initial pre-screen by phone (typically 30 minutes) to help us assess your motivations and interest in the position and Hydrock.
If you progress following this telephone pre-screen, you will be invited to attend a formal interview by video conference (Microsoft Teams) or in our offices.
For the latest updates and news, connect with us on our LinkedIn page!
Department
MEP
Contract type
Permanent
Hours
37.5 Hours
Salary
Competitive
Show more
Show less","Mechanical engineering, Building services, Data center design, RIBA 14c, Subdiscipline management, External contractor management, Team training, Fee income generation, BIM processes, Revit, IES, Sustainable design principles, Knowledge sharing, Business operations, Marketing, Commercial, Financial responsibilities","mechanical engineering, building services, data center design, riba 14c, subdiscipline management, external contractor management, team training, fee income generation, bim processes, revit, ies, sustainable design principles, knowledge sharing, business operations, marketing, commercial, financial responsibilities","bim processes, building services, business operations, commercial, data center design, external contractor management, fee income generation, financial responsibilities, ies, knowledge sharing, marketing, mechanical engineering, revit, riba 14c, subdiscipline management, sustainable design principles, team training"
Data Researcher,With Intelligence,"Cardiff, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/data-researcher-at-with-intelligence-3778588747,2023-12-17,Cardiff, United Kingdom,Mid senior,Onsite,"We are seeking a dynamic Data Researcher to join the Cardiff research centre. The successful candidate will be responsible for collecting and maintaining accurate data pertaining to institutional investors, through the analysis of annual reports, other specialist documents, and relationship building.
The ideal candidate will possess strong attention to detail, in addition to excellent analytic and communication skills, as well as being organised, ambitious and driven. This is an exciting opportunity for a bright and enthusiastic individual who wishes to work within a professional business environment.
Responsibilities
Using a variety of online and public sources to identify relevant sources of information
Analysing reports and filings to identify key pieces of data
Normalising data researched and entering it into the database
Forming relationships with investors to obtain proprietary data
Identifying new sources of data through the development and maintenance of key contact relationships, via both telephone and email
Be responsible for the acquisition of data and the development of this in line with business requirements and priorities.
Manage the process of quality checking, reporting and improvement ensuring the quality and performance of data
Requirements
Excellent problem-solving abilities, as well as logical and critical thinking skills.
Strong communication and presentation skills.
Proficient with spreadsheets, databases and MS Office.
Able to work on your own and as part of a team to meet project deadlines.
BS degree in Finance, Economics or related field is desirable.
Previous experience in a data analysis, data science, or related role desirable.
Benefits
24 days annual leave rising to 29 days
Enhanced parental leave
Medicash (Healthcare Cash Plan)
Wellness Days
Flexible Fridays (Opportunity to finish early)
Birthday day off
Employee assistance program
Travel loan scheme
Charity days
Breakfast provided
Fully stocked drinks fridge
Social Events throughout the year
Hybrid Working with 3 days in the office
Our Company
With Intelligence is based at One London Wall, London EC2Y 5EA. We offer amazing benefits, free breakfast daily and drinks provided all day, every day. We actively encourage social networks that oversee activities from sports, book reading to rock climbing, that you are free to join.
As part of our company, you will enjoy the benefits of an open plan office and working with a social and energetic team. With Intelligence provides exclusive editorial, research, data and events for senior executives within the asset management industry. These include hedge funds, private credit, private equity, real estate and traditional asset management, and our editorial brands are seen as market leaders in providing asset manager sales and IR execs with the actionable information they require to help them raise and retain assets. To maintain and grow our leading position in the market we need to continue to hire highly motivated, thoughtful and to ensure our subscribers are getting the exclusive intelligence they need first, and most comprehensively, through our range of services. If you are interested so far in what you have read, please apply, we look forward to hearing from you.
We are an Equal Opportunity Employer. Our policy is not to discriminate against any applicant or employee based on actual or perceived race, age, sex or gender (including pregnancy), marital status, national origin, ancestry, citizenship status, mental or physical disability, religion, creed, colour, sexual orientation, gender identity or expression (including transgender status), veteran status, genetic information, or any other characteristic protected by applicable law.
Show more
Show less","Data analysis, Data science, Data entry, Data management, Data quality control, MS Office, Databases, Spreadsheets, Communication, Teamwork, Research, Problem solving, Critical thinking, Proficient with MS Office, Data collection, Attention to detail","data analysis, data science, data entry, data management, data quality control, ms office, databases, spreadsheets, communication, teamwork, research, problem solving, critical thinking, proficient with ms office, data collection, attention to detail","attention to detail, communication, critical thinking, data collection, data entry, data management, data quality control, data science, dataanalytics, databases, ms office, problem solving, proficient with ms office, research, spreadsheets, teamwork"
Data Engineer,Yolk Recruitment Ltd,"Cardiff, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-yolk-recruitment-ltd-3765635738,2023-12-17,Cardiff, United Kingdom,Mid senior,Hybrid,"Data Engineer (Marketing Platform) – Cardiff 2 days per week – Up to £70,000 per annum
Yolk is working with a leading company transforming data into intelligent insights, you’d be joining the Marketing Platform team, focused on enhancing data products and platforms for marketing teams. This is your chance to work with cutting-edge technologies alongside a team of passionate data enthusiasts.
This is what you’ll be doing:
Collaborate with development teams and stakeholders to design, develop, and implement robust, scalable solutions.
Support marketing teams by provisioning/improving data products for forecasting and attribution.
Implement data ingestion and transformation pipelines using various technologies, with a preference for marketing platforms and APIs.
Work with product owners to gather requirements, translate them into technical specifications, and provide timely updates on progress.
The experience you’ll bring to the team:
Strong knowledge of SQL and Python programming (data manipulation context).
Extensive experience in designing, implementing, and maintaining robust data pipelines and transformation processes.
Hands-on experience in cloud-based services and big data technologies (Database, Compute, Storage, Spark, Databricks, Delta Lake, BigQuery).
Familiarity with event-driven architectures (Event Hubs, Kafka, etc.).
Experience in modern software development tools and Agile methodologies.
*Desirable Skills:
Familiarity with PPC platforms (Google Ads, Microsoft Ads) and Google Analytics.
Experience in Azure, Databricks, and/or Google Cloud Platform (GCP).
Understanding of exposing and linking data from systems, delivering streaming services.
Familiarity with server-side tagging and tag managers (e.g., Google Tag Manager).
And this is what you’ll get in return:
Salary up to £70,000 DOE
Employer matching pension up to 7.5%.
25 days of annual leave (increasing with length of service).
'Work from Home' budget.
Are you up to the challenge?
Contact Lewis Allen to find out more!
Please apply with a CV and a cover letter outlining why you’re perfect for the role. We also have a referral scheme so if you know of someone who would be great for the role please get in touch.
*Please note, whilst we do our best to contact all candidates, due to the high number of applications we receive we cannot guarantee this for every role. If you have not heard anything from us within 7 days of applying - then unfortunately you have been unsuccessful. Please keep an eye on our website for more opportunities.
Show more
Show less","SQL, Python, Data pipelines, Data transformation, Cloudbased services, Big data technologies, Database, Compute, Storage, Spark, Databricks, Delta Lake, BigQuery, Eventdriven architectures, Event Hubs, Kafka, Agile methodologies, PPC platforms, Google Ads, Microsoft Ads, Google Analytics, Azure, Google Cloud Platform (GCP), Serverside tagging, Tag managers, Google Tag Manager","sql, python, data pipelines, data transformation, cloudbased services, big data technologies, database, compute, storage, spark, databricks, delta lake, bigquery, eventdriven architectures, event hubs, kafka, agile methodologies, ppc platforms, google ads, microsoft ads, google analytics, azure, google cloud platform gcp, serverside tagging, tag managers, google tag manager","agile methodologies, azure, big data technologies, bigquery, cloudbased services, compute, data transformation, database, databricks, datapipeline, delta lake, event hubs, eventdriven architectures, google ads, google analytics, google cloud platform gcp, google tag manager, kafka, microsoft ads, ppc platforms, python, serverside tagging, spark, sql, storage, tag managers"
Data Engineer,Yolk Recruitment Ltd,"Cardiff, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-yolk-recruitment-ltd-3765642314,2023-12-17,Cardiff, United Kingdom,Mid senior,Hybrid,"Data Engineer (Integrations) – Cardiff 2 days per week – Up to £70,000 per annum
Are you a skilled data/software engineer seeking a new challenge? Yolk is working with a leading company transforming data into intelligent insights where you'll collaborate with internal and external development teams to design, develop, and implement robust, scalable solutions.
This is what you’ll be doing:
Create/design/review modern, scalable data pipelines using Azure/Databricks stack.
Ensure accurate and timely availability of data products.
Optimize storage, processes, and pipelines.
Monitor and maintain existing processes, identifying opportunities for improvements.
Work with product owners and stakeholders to gather requirements, translating them into technical
The experience you’ll bring to the team:
Strong knowledge of SQL and Python programming.
Extensive experience in designing, implementing, and maintaining robust data pipelines and transformation processes.
Cloud environment expertise, preferably in Azure.
Experience with big data technologies (Spark, Databricks, Delta Lake, BigQuery).
Familiarity with event-driven architectures (Event Hubs, Kafka, etc.).
Demonstrable understanding of exposing data from systems, linking data, and delivering streaming services.
Experience in modern software development tools and Agile methodologies.
Solid understanding of security best practices, accessibility, compliance, and GDPR regulations.
*Desirable:
Familiarity with PPC platforms (Google Ads, Microsoft Ads) and Google Analytics.
Experience in Azure, Databricks, and/or Google Cloud Platform (GCP).
And this is what you’ll get in return:
Salary up to £70,000 DOE
Employer matching pension up to 7.5%.
25 days of annual leave (increasing with length of service).
'Work from Home' budget.
Are you up to the challenge?
Contact Lewis Allen to find out more!
Please apply with a CV and a cover letter outlining why you’re perfect for the role. We also have a referral scheme so if you know of someone who would be great for the role please get in touch.
*Please note, whilst we do our best to contact all candidates, due to the high number of applications we receive we cannot guarantee this for every role. If you have not heard anything from us within 7 days of applying - then unfortunately you have been unsuccessful. Please keep an eye on our website for more opportunities.
Show more
Show less","SQL, Python, Data Pipelines, Data Transformation, Azure, Databricks, Spark, Delta Lake, BigQuery, EventDriven Architectures, Data Exposition, Data Linking, Streaming Services, Agile Methodologies, Security Best Practices, Accessibility, Compliance, GDPR, Google Analytics, PPC Platforms, Google Ads, Microsoft Ads","sql, python, data pipelines, data transformation, azure, databricks, spark, delta lake, bigquery, eventdriven architectures, data exposition, data linking, streaming services, agile methodologies, security best practices, accessibility, compliance, gdpr, google analytics, ppc platforms, google ads, microsoft ads","accessibility, agile methodologies, azure, bigquery, compliance, data exposition, data linking, data transformation, databricks, datapipeline, delta lake, eventdriven architectures, gdpr, google ads, google analytics, microsoft ads, ppc platforms, python, security best practices, spark, sql, streaming services"
Senior Data Engineer,Confused.com,"Cardiff, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-confused-com-3710631809,2023-12-17,Cardiff, United Kingdom,Mid senior,Hybrid,"Description
Hybrid - 2 days a week from our Cardiff office.
About Confused.com
Confused.com is the UK’s first comparison platform for car insurance. We’ve been helping customers since 2002 by empowering them to make better decisions around insurance and financial services. Our mission is simple: take away the confusion when comparing financial products and services to help you save time and money.
We’re part of RVU, a group of online brands (including Uswitch, Tempcover, and Money.co.uk) that empower people to make confident decisions across a range of household services.
About The Role
We are looking for an experienced Senior Engineer with strong data skills to join our Marketing Platform team. This team is focused on improving, deploying, or developing new data products and platforms for marketing teams and improving marketing efficiency. This is an exciting opportunity to work in an environment using cutting-edge technologies alongside a team of like-minded data enthusiasts.
This position demands a well-organised, action-oriented team player with excellent time management skills to support various work streams. Must possess strong written and verbal communication skills, with the ability to communicate effectively with technical and non-technical stakeholders.
We work in a fast-paced, agile, and continuous delivery release-driven environment. The right candidate for this position will be motivated by this challenge and will help to develop our future ways of working.
What you’ll be doing:
Collaborating with development teams and key stakeholders to help design, develop, and implement robust, scalable, operational solutions to meet business needs.
Supporting the marketing teams by provisioning or improving data products available to them for forecasting and attribution.
Implementing data ingestion and transformation pipelines with deep technical skills in a variety of technologies; experience with marketing platforms and APIs (Google Ads, Microsoft Ads, Google Analytics, etc.…) is preferred.
Working with product owners and stakeholders to gather requirements and translate them into technical requirements as well as providing timely updates on progress and highlighting relevant risks or opportunities.
Preparing and providing demos, creating user stories, requirements definitions, and acceptance criteria using Agile tools and techniques.
Monitoring and maintaining existing processes, identifying opportunities for improvements.
Staying up-to-date with emerging trends and technologies within the fields of engineering and marketing platforms
What you'll bring to the role:
Strong knowledge of SQL and Python programming (within the context of data manipulation)
Extensive experience designing, implementing, and maintaining robust data pipelines and transformation processes.
Ability to understand detailed technical requirements and explain technical concepts to both technical and non-technical stakeholders alike.
Hands-on experience in cloud-based services and big data technologies (e.g. Database, Compute and Storage, Spark, Databricks, Delta Lake, BigQuery)
Familiarity with event-driven architectures (Event Hubs, Kafka, etc.)
Experience in modern software development tools and ways of working (Agile methodologies, Git, CI/CD & DevOps tools, infrastructure as code, metrics/monitoring, etc.)
Solid understanding of security best practices, accessibility, compliance, and GDPR regulations.
Acts with integrity and makes sound decisions in the best interest of the business.
Continuously looking for opportunities to learn, build skills, and share learning both internally and externally.
Desired:
Familiarity with PPC platforms (such as Google Ads and Microsoft Ads) and Google Analytics data.
Experience working within Azure, Databricks, and/or Google Cloud Platform (GCP).
Demonstrable understanding of how to expose data from systems (eg. through APIs), link data from multiple systems and deliver streaming services.
Familiarity with server-side tagging and tag managers (e.g. Google Tag Manager).
You don’t need to tick off everything on this list - so don’t let that hold you back from applying. There are plenty of opportunities to learn with us!
Benefits
Our commitment to you:
At Confused.com we believe that we can be the change we wish to see in the world. We hold ourselves accountable to being open and inclusive teammates and community members. We embrace our differences and are committed to creating an inclusive environment that reflects the world we live in.
We want to give you a great work environment. We want to contribute to both your personal and professional development, and give you great benefits to make your time at Confused.com even more enjoyable. Some of these benefits include:
A competitive salary and bonus package
Employer matching pension up to 7.5%
Hybrid approach of in-office and remote working, and a “Work from Home” budget to help contribute towards a great work environment at home
Annual leave of 25 days (excluding bank holidays) which will increase with length of service (up to a maximum of 30 days) and additional leave options
Excellent maternity, paternity and adoption leave policy, for those key moments in your life
Employee Assistance Programme (EAP), Simply Health Scheme and access to the Calm app.
Regular events - company-wide events with insightful external speakers as we want to make sure our colleagues continue to feel connected
A healthy learning and training budget, as well as the chance to go to conferences around the world every year
Private medical scheme
If you would like to know more, please get in touch by emailing recruitment@confused.com
Confused.com is proud to have been officially recognised as a 2022 UK’s Best Workplace™ and as a 2022 UK’s Best Workplace™ for Wellbeing in the Large size category!
As a tech company that strives to get better every day, we use Metaview during the interview processes to record and transcribe interviews so the interviewer can focus on the conversation rather than note-taking. This also helps us to continuously improve the quality of our interviews and your experience.
This has no bearing on the assessment of you as a candidate and you can opt-out at any time, just let us know before your call.
Show more
Show less","SQL, Python, Data pipelines, Data transformation, Cloudbased services, Big data technologies, Spark, Databricks, Delta Lake, BigQuery, Eventdriven architectures, Git, CI/CD & DevOps tools, Infrastructure as code, Metrics/monitoring, Security best practices, Accessibility, Compliance, GDPR regulations, PPC platforms, Google Ads, Microsoft Ads, Google Analytics, Azure, Google Cloud Platform (GCP), APIs, Serverside tagging, Tag managers","sql, python, data pipelines, data transformation, cloudbased services, big data technologies, spark, databricks, delta lake, bigquery, eventdriven architectures, git, cicd devops tools, infrastructure as code, metricsmonitoring, security best practices, accessibility, compliance, gdpr regulations, ppc platforms, google ads, microsoft ads, google analytics, azure, google cloud platform gcp, apis, serverside tagging, tag managers","accessibility, apis, azure, big data technologies, bigquery, cicd devops tools, cloudbased services, compliance, data transformation, databricks, datapipeline, delta lake, eventdriven architectures, gdpr regulations, git, google ads, google analytics, google cloud platform gcp, infrastructure as code, metricsmonitoring, microsoft ads, ppc platforms, python, security best practices, serverside tagging, spark, sql, tag managers"
Data Engineer,Yolk Recruitment Ltd,"Cardiff, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-yolk-recruitment-ltd-3765636308,2023-12-17,Cardiff, United Kingdom,Mid senior,Hybrid,"Data Engineer (Insights) – Cardiff 2 days per week – Up to £70,000 per annum
Yolk is working with a leading company transforming data into intelligent insights, seeking a talented individual to enhance their Insights team. You’d be expected to come into the Cardiff office twice per week.
This is what you’ll be doing:
Designing scalable data pipelines using Azure/Databricks stack.
Ensuring accuracy and timely availability of data products.
Monitoring and optimizing operation costs, storage, processes, and pipelines.
Collaborating with product owners to gather requirements and translate them into technical specifications.
The experience you’ll bring to the team:
Strong expertise in SQL and Python programming.
Extensive experience in designing, implementing, and maintaining robust data pipelines.
Proficiency in working within a cloud environment, preferably Azure.
Familiarity with event-driven architectures (Event Hubs, Kafka, etc.).
Understanding of exposing and linking data from systems, delivering streaming services.
*Desirable Skills:
Knowledge of Data Factory or equivalent technology.
Familiarity with C#.
Experience with big data technologies (PySpark, Databricks, Delta Lake, BigQuery).
And this is what you’ll get in return:
Salary up to £70,000 DOE
Employer matching pension up to 7.5%.
25 days of annual leave (increasing with length of service).
'Work from Home' budget.
Are you up to the challenge?
Contact Lewis Allen to find out more!
Please apply with a CV and a cover letter outlining why you’re perfect for the role. We also have a referral scheme so if you know of someone who would be great for the role please get in touch.
*Please note, whilst we do our best to contact all candidates, due to the high number of applications we receive we cannot guarantee this for every role. If you have not heard anything from us within 7 days of applying - then unfortunately you have been unsuccessful. Please keep an eye on our website for more opportunities.
Show more
Show less","Azure, Databricks, SQL, Python, Cloud computing, Eventdriven architectures, Event Hubs, Kafka, Data Factory, C#, PySpark, Delta Lake, BigQuery","azure, databricks, sql, python, cloud computing, eventdriven architectures, event hubs, kafka, data factory, c, pyspark, delta lake, bigquery","azure, bigquery, c, cloud computing, data factory, databricks, delta lake, event hubs, eventdriven architectures, kafka, python, spark, sql"
Senior Data Platform Engineer,Upstart,United States,https://www.linkedin.com/jobs/view/senior-data-platform-engineer-at-upstart-3757491791,2023-12-17,Fitzgerald,United States,Mid senior,Remote,"About Upstart
Upstart is a leading AI lending marketplace partnering with banks and credit unions to expand access to affordable credit. By leveraging Upstart's AI marketplace, Upstart-powered banks and credit unions can have higher approval rates and lower loss rates across races, ages, and genders, while simultaneously delivering the exceptional digital-first lending experience their customers demand. More than two-thirds of Upstart loans are approved instantly and are fully automated.
Upstart is a digital-first company, which means that most Upstarters can live and work anywhere in the U.S. We also have offices in San Mateo, California; Columbus, Ohio; and Austin, Texas.
Most Upstarters join us because they connect with our mission of enabling access to effortless credit based on true risk. If you are energized by the impact you can make at Upstart, we’d love to hear from you!
The Team
The
Data Engineering team
(Part of Upstart's core platform vertical) provides developer tools, frameworks, and scalable data infrastructure as shared services across Upstart. The team's primary objective is to provide Data Analysts, Software Engineers, and ML scientists access to high-quality data and developer tools to create business metrics for respective product verticals.
You will join the subdivision focusing on Data quality, Governance, and Production Experience (Data Infrastructure).
As a
Senior Data Platform Engineer
joining this team, you will get the opportunity to contribute to 3 areas:
Expand the existing developer tools to support Upstart's various data validation & quality checks by integrating with open-source, 3rd party, and custom data quality solutions.
Build a framework to capture Data Lineage and location of critical fields and integrate with Upstart's data catalog solution that informs various data handling rules.
Improve data infrastructure uptime and observability.
We would love to hear from you if you are passionate about building data products!
Position Location -
This role is available in the following locations: Remote, San Mateo, Columbus, Austin
Time Zone Requirements -
This team operates across all US time zones.
Travel Requirements -
This team has periodic on-site collaboration sessions twice or thrice a year. Upstart will cover all travel-related expenses if you need to travel to make these meetups.
How you’ll make an impact:
Participate in planning and prioritization by collaborating with stakeholders across the various product verticals and functions (ML, Analytics, Finance) to ensure our architecture aligns with the overall business objectives.
Collaborate with data governance and security teams to implement robust data protection mechanisms, access controls, and data lineage.
Participate in code reviews and architecture discussions to exchange actionable feedback with peers.
Contribute to engineering best practices and mentor junior team members.
Help break down complex projects and requirements into sprints.
Continuously monitor and improve data platform performance, reliability, and security.
Stay up-to-date with emerging technologies and industry best practices in data engineering.
Design and ship code independently
What we’re looking for:
Minimum requirements:
A bachelor's degree in Computer Science, Data Science, Engineering, or a related field.
5+ years of experience in data engineering or related fields, with a strong focus on data quality, governance, and data infrastructure.
Proficiency in data engineering tech stack; Databricks / PostgreSQL / Python / Spark / Kafka / SQL / AWS / Airflow/ DBT / containers and orchestration (Docker, Kubernetes) and others.
Ability to approach problems with first principles thinking, embrace ambiguity, and enjoy collaborative work on complex solutions.
Preferred qualifications:
Strong foundation in algorithms and data structures and their real-world use cases.
Experience and understanding of distributed systems, data architecture design, and big data technologies (e.g., Spark, Kafka, Data Lake, Databricks, Redshift).
Experience with AWS technologies ( e.g., AWS Lambda, Redshift, RDS, S3, etc.).
Knowledge of data quality management, data governance, and data security best practices.
Good knowledge of DevOps engineering using Continuous Integration/Delivery tools like Kubernetes, Jenkins, Terraform, etc., thinking about automation, alerting, monitoring, security, and other declarative infrastructure.
Ability to explain complex concepts in easy-to-understand ways and navigate environments where problems are not well-defined (and evolve quickly).
What you'll love:
Competitive Compensation (base + bonus & equity)
Comprehensive medical, dental, and vision coverage with Health Savings Account contributions from Upstart
401(k) with 100% company match up to $4,500 and immediate vesting and after-tax savings
Employee Stock Purchase Plan (ESPP)
Life and disability insurance
Generous holiday, vacation, sick and safety leave
Supportive parental, family care, and military leave programs
Annual wellness, technology & ergonomic reimbursement programs
Social activities including team events and onsites, all-company updates, employee resource groups (ERGs), and other interest groups such as book clubs, fitness, investing, and volunteering
Catered lunches + snacks & drinks when working in offices
At Upstart, your base pay is one part of your total compensation package. The anticipated base salary for this position is expected to be within the below range. Your actual base pay will depend on your geographic location–with our “digital first” philosophy, Upstart uses compensation regions that vary depending on location. Individual pay is also determined by job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.
In addition, Upstart provides employees with target bonuses, equity compensation, and generous benefits packages (including medical, dental, vision, and 401k).
United States | Remote - Anticipated Base Salary Range
$155,800—$215,500 USD
Upstart is a proud Equal Opportunity Employer. We are dedicated to ensuring that underrepresented classes receive better access to affordable credit, and are just as committed to embracing diversity and inclusion in our hiring practices. We celebrate all cultures, backgrounds, perspectives, and experiences, and know that we can only become better together.
If you require reasonable accommodation in completing an application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please email
candidate_accommodations@upstart.com
https://www.upstart.com/candidate_privacy_policy
Show more
Show less","Data Engineering, Python, Spark, AWS, Docker, Kubernetes, SQL, PostgreSQL, Apache Airflow, Databricks, Apache Kafka, DBT, Data Lineage, Data Quality, Data Governance, Data Architecture, Big Data, DevOps, Jenkins, Terraform, Continuous Integration, Continuous Delivery","data engineering, python, spark, aws, docker, kubernetes, sql, postgresql, apache airflow, databricks, apache kafka, dbt, data lineage, data quality, data governance, data architecture, big data, devops, jenkins, terraform, continuous integration, continuous delivery","apache airflow, apache kafka, aws, big data, continuous delivery, continuous integration, data architecture, data engineering, data governance, data lineage, data quality, databricks, dbt, devops, docker, jenkins, kubernetes, postgresql, python, spark, sql, terraform"
Data Architect,Robert Half,"League City, TX",https://www.linkedin.com/jobs/view/data-architect-at-robert-half-3776683525,2023-12-17,Dickinson,United States,Mid senior,Onsite,"We are seeking a highly skilled Data Architect to join our dynamic IT team. The ideal candidate will have extensive experience in designing and implementing robust data architectures, with a strong focus on Microsoft Azure. As a Data Architect, you will play a pivotal role in shaping our data strategy, ensuring data integrity, and optimizing data solutions. The successful candidate will possess a deep understanding of database technologies and a proven track record in architecting scalable and efficient data systems.
Responsibilities:
Data Architecture Design:
Design and implement scalable, secure, and efficient data architectures in alignment with organizational goals.
Collaborate with stakeholders to understand data requirements and translate them into technical solutions.
Azure Data Platform Expertise:
Leverage Microsoft Azure services for data storage, processing, and analytics.
Design and implement data solutions using Azure SQL Database, Azure Synapse Analytics, and other Azure data services.
Database Management:
Oversee database design, optimization, and performance tuning.
Ensure data consistency, integrity, and security across various databases.
Data Integration and Migration:
Lead data integration initiatives, ensuring seamless connectivity between disparate systems.
Manage and execute data migration projects to Azure, ensuring minimal downtime and data loss.
Data Governance and Compliance:
Develop and implement data governance policies and procedures.
Ensure compliance with data privacy regulations and industry standards.
Collaboration and Communication:
Collaborate with cross-functional teams, including developers, analysts, and business stakeholders.
Communicate complex technical concepts to non-technical audiences effectively.
Documentation:
Create and maintain comprehensive documentation of data architectures, standards, and processes.
Provide training and support to internal teams on data-related best practices.
Qualifications:
Education:
Bachelor's degree in Information Technology, Computer Science, or a related field. Advanced degree preferred.
Experience:
Minimum of 5 years of experience as a Data Architect, with a strong emphasis on Azure data solutions.
Proven experience in designing and implementing data architectures in a complex environment.
Technical Skills:
Expertise in Microsoft Azure data services (Azure SQL Database, Azure Synapse Analytics, Azure Data Factory, etc.).
Strong database management skills, including experience with SQL Server and other relational databases.
Certifications:
Relevant certifications such as Microsoft Certified: Azure Data Engineer Associate or equivalent.
Soft Skills:
Excellent communication and interpersonal skills.
Strong analytical and problem-solving abilities.
Ability to work collaboratively in a team environment.
Show more
Show less","Data architecture, Data modeling, Data integration, Data migration, Data governance, Data security, Data quality, Data warehousing, Big data, Cloud computing, Microsoft Azure, Azure SQL Database, Azure Synapse Analytics, Azure Data Factory, SQL Server, Relational databases, Microsoft Certified: Azure Data Engineer Associate","data architecture, data modeling, data integration, data migration, data governance, data security, data quality, data warehousing, big data, cloud computing, microsoft azure, azure sql database, azure synapse analytics, azure data factory, sql server, relational databases, microsoft certified azure data engineer associate","azure data factory, azure sql database, azure synapse analytics, big data, cloud computing, data architecture, data governance, data integration, data migration, data quality, data security, datamodeling, datawarehouse, microsoft azure, microsoft certified azure data engineer associate, relational databases, sql server"
Senior System Engineer - (Bryan/College Station Data Center),Kelsey-Seybold Clinic,"Pearland, TX",https://www.linkedin.com/jobs/view/senior-system-engineer-bryan-college-station-data-center-at-kelsey-seybold-clinic-3754678448,2023-12-17,Dickinson,United States,Mid senior,Onsite,"Responsibilities
Job Description
Primary responsibilities driving and owning projects and initiatives set out by leadership, ability to lead and set examples to non-senior staff on how to best optimize and create processes as well as document for more effective support. Technology vendor liaison and alignment with business objectives set forth by leadership as well as responding to critical situations. Staying up to date on any technology advancements and making recommendations to the business as to changes that align with business objectives. On-Call and ticket resolution as well as responsibility coverage from non-senior positions maybe required in specific instances.
Job Title: Senior System Engineer - IT Systems
Clinic Location: Pearland Administrative Office
Department:
IT Systems
Job Type: Full Time
Salary Range: $122,247 - $151,015 (Pay is based on several factors including but not limited to education, work experience, certifications, etc.)
Qualifications
Education
Required: Technical Bachelor’s degree or equivalent experience
Preferred: Master’s Degree
Experience
Required: 7 years of System Administrator / Engineering experience
Preferred: Prior experience in the health care industry including exposure to Epic Systems technologies.
License(s)
Required: N/A
Preferred: CompTIA A+
CompTIA Server+
Microsoft Certified Professional
Microsoft Certified IT Professional
Microsoft Certified Technology Specialist
Microsoft Certified Systems Administrator
Microsoft Certified Systems Engineer
VMware Certified Professional
Special Skills
Required: Excellent verbal and written communication skills; Strong analytical and problem-solving skills;
Self motivated and excels in team and individual environments; Implementation, support, and maintenance of Windows and Linux Server Operating Systems;
Implementation, support, and maintenance of Microsoft Clustering or other HA solutions;
Strong Directory Services Administration (Active Directory, LDAP, ADAM);
Strong understanding of networking fundamentals and troubleshooting;
Strong understanding of backup / recovery and other business continuity services;
Strong understanding of alerting and monitoring;
Strong understanding of architecture, security, and performance tuning;
Strong understanding of storage technologies including SAN and NAS (Configuration, Zoning on Fiber Channel Switches, and multi-pathing);
Knowledge of build automation, patch management, automated deployments and packaging;
Knowledge of Cloud authentication 2FA
Preferred: Linux Server Administration / Engineering
Microsoft SQL Server Administration / Engineering
Microsoft IIS Server Administration / Engineering
Microsoft SQL Server Administration / Engineering
Microsoft Exchange Administration / Engineering
Microsoft System Center Administration / Engineering
M365 Admin Portal Administration
Azure IaaS, Paas, Saas Administration / Engineering
Server Virtualization (VMWare, Hyper-V) Administration / Engineering
On-Prem / Hybrid Cloud Orchestration and Automation
Other
Required: N/A
Preferred: N/A
Working Environment:
Other
About Us
Start your career journey and become a part of a community of renowned Healthcare professionals. Kelsey-Seybold Clinic is Houston’s fastest growing, multispecialty organization with more than 40 premier locations and over 65 specialties. Our clinics are comprised of more than 600 physicians and as we continue to grow, our focus is providing quality patient care by adding to our team of clinical and non-clinical professionals that work together in a convenient, coordinated, and collaborative manner. Enjoy the rewards of a successful career while maintaining a work/life balance by joining our team today and changing the way health cares.
Why Kelsey-Seybold Clinic?
Medical, Vision, and Dental
Tuition Reimbursement
Company Matching 401K
Employee Reward and Recognition Program
Paid time off for vacation, sick, and holidays
Employee Assistance Program
Continuing Medical Education allowance
Show more
Show less","Leadership, Process Optimization, Documentation, Technology Vendor Liaison, Business Objectives Alignment, Technology Advancements, IT Systems, Windows Server Operating Systems, Linux Server Operating Systems, Microsoft Clustering, HA Solutions, Directory Services Administration, Active Directory, LDAP, ADAM, Networking Fundamentals, Troubleshooting, Backup / Recovery, Business Continuity Services, Alerting, Monitoring, Architecture, Security, Performance Tuning, Storage Technologies, SAN, NAS, Configuration, Zoning, Fiber Channel Switches, Multipathing, Build Automation, Patch Management, Automated Deployments, Packaging, Cloud Authentication 2FA, Linux Server Administration / Engineering, Microsoft SQL Server Administration / Engineering, Microsoft IIS Server Administration / Engineering, Microsoft Exchange Administration / Engineering, Microsoft System Center Administration / Engineering, M365 Admin Portal Administration, Azure IaaS, PaaS, SaaS Administration / Engineering, Server Virtualization, VMWare, HyperV, OnPrem / Hybrid Cloud Orchestration, Automation","leadership, process optimization, documentation, technology vendor liaison, business objectives alignment, technology advancements, it systems, windows server operating systems, linux server operating systems, microsoft clustering, ha solutions, directory services administration, active directory, ldap, adam, networking fundamentals, troubleshooting, backup recovery, business continuity services, alerting, monitoring, architecture, security, performance tuning, storage technologies, san, nas, configuration, zoning, fiber channel switches, multipathing, build automation, patch management, automated deployments, packaging, cloud authentication 2fa, linux server administration engineering, microsoft sql server administration engineering, microsoft iis server administration engineering, microsoft exchange administration engineering, microsoft system center administration engineering, m365 admin portal administration, azure iaas, paas, saas administration engineering, server virtualization, vmware, hyperv, onprem hybrid cloud orchestration, automation","active directory, adam, alerting, architecture, automated deployments, automation, azure iaas, backup recovery, build automation, business continuity services, business objectives alignment, cloud authentication 2fa, configuration, directory services administration, documentation, fiber channel switches, ha solutions, hyperv, it systems, ldap, leadership, linux server administration engineering, linux server operating systems, m365 admin portal administration, microsoft clustering, microsoft exchange administration engineering, microsoft iis server administration engineering, microsoft sql server administration engineering, microsoft system center administration engineering, monitoring, multipathing, nas, networking fundamentals, onprem hybrid cloud orchestration, paas, packaging, patch management, performance tuning, process optimization, saas administration engineering, san, security, server virtualization, storage technologies, technology advancements, technology vendor liaison, troubleshooting, vmware, windows server operating systems, zoning"
"Senior Data Systems Developer – (Epic Certification required Tapestry, Clarity)",Kelsey-Seybold Clinic,"Pearland, TX",https://www.linkedin.com/jobs/view/senior-data-systems-developer-%E2%80%93-epic-certification-required-tapestry-clarity-at-kelsey-seybold-clinic-3756387677,2023-12-17,Dickinson,United States,Mid senior,Onsite,"Responsibilities
Job Description
Works collaboratively as a senior member of the Business Intelligence & Reporting team under direction of KSC Application Technology Leadership. Supports, develops, and maintains business intelligence (BI), data warehousing, and reporting systems. Acts as a project lead or team member on multi-team projects and uses advanced skill set to develop, test, troubleshoot, and support BI and reporting-related projects and associated solutions/applications. Acts as mentor to others learning essential and advanced BI and/or reporting-related skills.
Job Title: Senior Data Systems Developer – IT Admin
Clinic Location: Pearland Administrative Office
Department: IT Admin
Job Type: Full Time
Salary Range: $110,090 - $135,995 (Pay is based on several factors including but not limited to education, work experience, certifications, etc.)
Qualifications
Education
Required: Bachelor’s degree in a technical area or equivalent work experience
Preferred: Advanced degree preferred.
Experience
Required: Minimum of four years’ experience in data warehouse/business intelligence development.
Experience as BI/DW architect or lead in at least one BI/DW initiative / project. Senior level member of BI/DW initiatives and projects.
Preferred: Previous experience in a healthcare industry, large-scale data warehousing, and/or exposure to Epic Systems Clarity.
License(s)
Required: Valid Texas driver’s license
Preferred: N/A
Special Skills
Required: Significant experience and thorough technical knowledge in several of the following tools:
MS SQL Database Services / Transact-SQL (T-SQL)
MS SQL Analysis Services (SSAS)
MS SQL Reporting Services (SSRS)
MS SQL Server Integration Services (SSIS)
Business Objects / Crystal Reports
Other BI/DW Toolsets including Cognos, Microsoft, Oracle, Microstrategy
Thorough understanding and knowledge of the following topics:
Datawarehouse design
Reporting
OLAP / Cubes
KPIs, Scorecards, Dashboards
Ad-Hoc
Analytics
Data Mining
Metadata Management
Data Quality
Master Data Management
Preferred: Advanced certification including:
MS SQL-related Certifications
Crystal Report-Related Certifications
Vendor-Specific Certifications
Other
Required: Flexibility and the ability to adapt to change. Proven communication, presentation, analytical, problem solving, technical and writing skills. Candidates must have a positive “can do” attitude and a professional demeanor.
Preferred: N/A
Working Environment:
Office
About Us
Start your career journey and become a part of a community of renowned Healthcare professionals. Kelsey-Seybold Clinic is Houston’s fastest growing, multispecialty organization with more than 40 premier locations and over 65 specialties. Our clinics are comprised of more than 600 physicians and as we continue to grow, our focus is providing quality patient care by adding to our team of clinical and non-clinical professionals that work together in a convenient, coordinated, and collaborative manner. Enjoy the rewards of a successful career while maintaining a work/life balance by joining our team today and changing the way health cares.
Why Kelsey-Seybold Clinic?
Medical, Vision, and Dental
Tuition Reimbursement
Company Matching 401K
Employee Reward and Recognition Program
Paid time off for vacation, sick, and holidays
Employee Assistance Program
Continuing Medical Education allowance
Show more
Show less","Data Warehouse, Business Intelligence, MS SQL Database Services, TransactSQL (TSQL), MS SQL Analysis Services (SSAS), MS SQL Reporting Services (SSRS), MS SQL Server Integration Services (SSIS), Business Objects, Crystal Reports, Cognos, Microsoft, Oracle, Microstrategy, Datawarehouse design, Reporting, OLAP / Cubes, KPIs, Scorecards, Dashboards, AdHoc, Analytics, Data Mining, Metadata Management, Data Quality, Master Data Management, MS SQLrelated Certifications, Crystal ReportRelated Certifications, VendorSpecific Certifications","data warehouse, business intelligence, ms sql database services, transactsql tsql, ms sql analysis services ssas, ms sql reporting services ssrs, ms sql server integration services ssis, business objects, crystal reports, cognos, microsoft, oracle, microstrategy, datawarehouse design, reporting, olap cubes, kpis, scorecards, dashboards, adhoc, analytics, data mining, metadata management, data quality, master data management, ms sqlrelated certifications, crystal reportrelated certifications, vendorspecific certifications","adhoc, analytics, business intelligence, business objects, cognos, crystal reportrelated certifications, crystal reports, dashboard, data mining, data quality, datawarehouse, datawarehouse design, kpis, master data management, metadata management, microsoft, microstrategy, ms sql analysis services ssas, ms sql database services, ms sql reporting services ssrs, ms sql server integration services ssis, ms sqlrelated certifications, olap cubes, oracle, reporting, scorecards, transactsql tsql, vendorspecific certifications"
"Healthcare Business Intelligence Analyst, Assoc. (Remote) - Clinical Data Management",The University of Texas Medical Branch,"Galveston, TX",https://www.linkedin.com/jobs/view/healthcare-business-intelligence-analyst-assoc-remote-clinical-data-management-at-the-university-of-texas-medical-branch-3733981318,2023-12-17,Dickinson,United States,Mid senior,Remote,"Minimum Qualifications:
Bachelor’s degree in Statistics, Computer Science, Information Technology, Mathematics, Public Health, business, healthcare administration or related
field or equivalent. Four years of relevant work experience required, including extensive experience using SQL to query, manipulate, and organize data
from relational databases.
Job Summary:
The Business Intelligence Analyst, Associate is responsible for providing meaning to the data in UTMB systems and developing methods to leverage that data in UTMB applications. Responsibilities may include assisting with standardizing and cleaning data, building prediction models, modeling outcomes, and developing metrics for complex data analysis projects that can be multi-disciplinary or interdepartmental. Addresses data requirements for analytic reporting and operating areas with expertise in specific data systems and/or analytical methodologies. Works with proficiency with a variety of tools (ex: SQL, Excel, Tableau, etc.) to extract data, write queries, develop programming logic, run reports, manipulate data, and analyze datasets to meet the needs of the organization.
Job Duties:
Collects, aggregates, and analyzes data from multiple internal and external sources to drive insights into business performance. Utilizes business intelligence tools to create reports and dashboards, and produces actionable reports that show key performance indicators, identify areas of improvement into current operations, and display root cause analysis of problems.
Develops summaries of findings and recommends actions; drives business decisions by providing quantitative and qualitative data analysis and reporting of patterns, insights, and trends to decision-makers.
Assist with the development, implementation and validation of new predictive models, conduct QA of models’ outcomes and monitor performance regularly.
Uses analytics and metrics to improve processes and provide data-drive insights into patient safety, quality and experience.
Works with internal clients to develop and translate business requirements into project charters, business and technical requirements, and ultimately produce effective and insightful interactive analytics.
Assist with compiling, cleaning, manipulating, and analyzing data related to department projects. Responds to data integrity and functional or interactive issues in existing and future dashboards quickly and effectively.
Formulates validation strategies and methods to ensure accurate and reliable data.
Perform other similar and related duties as required or directed.
Knowledge/Skills/Abilities:
Critical thinking skills to help predict the needs of various business units and leaders across UTMB rather than require them to provide detailed prescriptive direction.
Knowledge and familiarity with business intelligence tools such as SQL, Microsoft Power BI, Qlik or Tableau to design and develop reporting analytics and visualizations.
Knowledge and familiarity with Microsoft Office suite software (Word, Excel, PowerPoint, Outlook, Visio, etc.).
Knowledge and familiarity with Epic Cogito Suite (Clarity, Caboodle, SlicerDicer) preferred.
Interpersonal and communication skills to effectively work and communicate with others.
Ability to quickly and demonstrate proficiency in new software tools.
Analytical reasoning and problem-solving skills.
Ability to document work and communicate progress to others.
Ability to function with moderate supervision in a fast-paced environment.
Ability to be detailed and accurate.
Salary Range:
$74,000.00 to $105,600.00, a
ctual salary commensurate with experience.
Work Schedule:
Partial Remote, on site as needed. Monday through Friday, 8am to 5pm and as needed on occasion.
Equal Employment Opportunity
UTMB Health strives to provide equal opportunity employment without regard to race, color, religion, age, national origin, sex, gender, sexual orientation, gender identity/expression, genetic information, disability, veteran status, or any other basis protected by institutional policy or by federal, state or local laws unless such distinction is required by law. As a VEVRAA Federal Contractor, UTMB Health takes affirmative action to hire and advance women, minorities, protected veterans and individuals with disabilities.
Primary Location
United States-Texas-Galveston
Work Locations
0752 - Clinical Science Bldg.
Job
Research Academic & Clinical
Organization
UTMB Health
Regular
Shift
Standard
Employee Status
Non-Manager
Job Level
Day Shift
Job Posting
Oct 5, 2023, 2:01:11 PM
Show more
Show less","Statistics, Computer Science, Information Technology, Mathematics, Public Health, Business, Healthcare Administration, SQL, Data Manipulation, Data Organization, Relational Databases, Data Analysis, Data Cleaning, Data Standardization, Prediction Models, Outcome Modeling, Complex Data Analysis, DataDriven Insights, Reporting and Dashboards, Actionable Reports, Key Performance Indicators, Root Cause Analysis, Quantitative Analysis, Qualitative Analysis, DataDriven Decision Making, Predictive Analytics, Quality Assurance, Model Performance Monitoring, Process Improvement, Patient Safety, Patient Quality, Patient Experience, Business Requirements, Project Charters, Business Analysis, Technical Analysis, Data Integrity, Data Validation, Critical Thinking, Business Intelligence Tools, Microsoft Power BI, Qlik, Tableau, Microsoft Office Suite, Epic Cogito Suite, Clarity, Caboodle, SlicerDicer, Interpersonal Skills, Communication Skills, New Software Tools, Analytical Reasoning, ProblemSolving, Documentation, Progress Communication, Moderate Supervision, Attention to Detail, Accuracy","statistics, computer science, information technology, mathematics, public health, business, healthcare administration, sql, data manipulation, data organization, relational databases, data analysis, data cleaning, data standardization, prediction models, outcome modeling, complex data analysis, datadriven insights, reporting and dashboards, actionable reports, key performance indicators, root cause analysis, quantitative analysis, qualitative analysis, datadriven decision making, predictive analytics, quality assurance, model performance monitoring, process improvement, patient safety, patient quality, patient experience, business requirements, project charters, business analysis, technical analysis, data integrity, data validation, critical thinking, business intelligence tools, microsoft power bi, qlik, tableau, microsoft office suite, epic cogito suite, clarity, caboodle, slicerdicer, interpersonal skills, communication skills, new software tools, analytical reasoning, problemsolving, documentation, progress communication, moderate supervision, attention to detail, accuracy","accuracy, actionable reports, analytical reasoning, attention to detail, business, business analysis, business intelligence tools, business requirements, caboodle, clarity, communication skills, complex data analysis, computer science, critical thinking, data cleaning, data integrity, data manipulation, data organization, data standardization, data validation, dataanalytics, datadriven decision making, datadriven insights, documentation, epic cogito suite, healthcare administration, information technology, interpersonal skills, key performance indicators, mathematics, microsoft office suite, microsoft power bi, model performance monitoring, moderate supervision, new software tools, outcome modeling, patient experience, patient quality, patient safety, prediction models, predictive analytics, problemsolving, process improvement, progress communication, project charters, public health, qlik, qualitative analysis, quality assurance, quantitative analysis, relational databases, reporting and dashboards, root cause analysis, slicerdicer, sql, statistics, tableau, technical analysis"
"Sr. Software Engineer, Data",Abbott,"Orlando, FL",https://www.linkedin.com/jobs/view/sr-software-engineer-data-at-abbott-3651424717,2023-12-17,Avondale,United States,Associate,Onsite,"Abbott is a global healthcare leader that helps people live more fully at all stages of life. Our portfolio of life-changing technologies spans the spectrum of healthcare, with leading businesses and products in diagnostics, medical devices, nutritionals and branded generic medicines. Our 115,000 colleagues serve people in more than 160 countries.
About Abbott
Abbott is a global healthcare leader, creating breakthrough science to improve people’s health. We’re always looking towards the future, anticipating changes in medical science and technology.
Working at Abbott
At Abbott, You Can Do Work That Matters, Grow, And Learn, Care For Yourself And Family, Be Your True Self And Live a Full Life. You’ll Also Have Access To
Career development with an international company where you can grow the career you dream of.
Free medical coverage for employees* via the Health Investment Plan (HIP) PPO
An excellent retirement savings plan with high employer contribution
Tuition reimbursement, the Freedom 2 Save student debt program and FreeU education benefit - an affordable and convenient path to getting a bachelor’s degree.
A company recognized as a great place to work in dozens of countries around the world and named one of the most admired companies in the world by Fortune.
A company that is recognized as one of the best big companies to work for as well as a best place to work for diversity, working mothers, female executives, and scientists.
The Opportunity
At Abbott, we believe people with diabetes should have the freedom to enjoy active lives. That’s why we’re focused on helping people with diabetes manage their health more effectively and comfortably, with life-changing products that provide accurate data to drive better-informed decisions. We’re revolutionizing the way people monitor their glucose levels with our new sensing technology.
Interested in applying your wealth of technical knowledge and experience towards an opportunity in the medical field and improving the lives of people with diabetes?
The candidate will be responsible for performing DevOps activities across multiple Cloud Service Providers in eleven global regions. Candidate will establish configuration management, automate infrastructure, implement continuous integration, and apply DevOps best practices to achieve a continuously deployable system. Candidate will support building scalable, highly available, efficient, and secure cloud infrastructure for a medical device SaaS. Candidate will work closely with Business, Software Engineers and Security Engineers to collaborate on a best-in-class healthcare platform for improving the lives of the hundreds of millions living with Diabetes.
#Software
Job locations:
Orlando, FL - United States Remote
What You’ll Work On
Apply your engineering expertise to produce meaningful and measurable differences in people’s lives on a global scale. Our software is provided free to patients and healthcare professionals in over 50 countries worldwide. https://www.freestyleprovider.abbott/us-en/outcomes.html
Thrive in a challenging engineering environment. Our Cloud Healthcare Platform consumes hundreds of millions of uploads per day and processes over one billion messages daily. Scalability, efficiency and reliability are critical.
Pioneer with a modern tech stack constantly evolving to meet performance and scalability requirements. Engage in architectural research, design reviews, and PR reviews across multiple teams.
Accelerate your technical skills and growth. Perform continuous cross-team communication and engage directly with other leaders within our partnerships with major CSPs.
Responsibilities
Provide engineering expertise to plan, analyze, design, test, and deploy secure, scalable, and highly available cloud infrastructure expressed as code.
Evaluation of new technology alternatives and implementation of new cloud-based initiatives, providing associated training as required
Employ exceptional problem-solving skills, with the ability to see and solve issues before they affect business productivity
Strive for continuous improvement and build continuous integration, continuous development, and constant deployment pipeline (CI/CD Pipeline)
Participate in all aspects of the software development life cycle for Cloud solutions, including planning, requirements, development, testing, and quality assurance collaboratively with Software Engineering
Work collaboratively with InfoSec to ensure that infrastructure is safe and secure against cybersecurity threats including adherence to Center for Internet Security (CIS) benchmarks.
Perform infrastructure cost analysis and optimization
Management of creation, release, and configuration of production systems. Optimize existing development and release processes through automation
Qualifications
Bachelor's Degree in Computer Science, Information Technology or other relevant technical fields.
At least 5-7+ years of experience building and maintaining AWS infrastructure (VPC, EC2, Security Groups, IAM, ECS, CodeDeploy, CloudFront, S3) and creating highly automated environments with Infrastructure as Code (Ansible, Terraform, CloudFormation)
Experience with container orchestration services (Kubernetes experience preferred)
Strong foundation of networking and Linux administration and experience with a variety of open-source technologies
Preferred Qualifications
Previous exposure to medical devices, biomedical or high technology industry.
Experience working in an agile environment.
Apply Now
Participants who complete a short wellness assessment qualify for FREE coverage in our HIP PPO medical plan. Free coverage applies in the next calendar year.
Learn more about our health and wellness benefits, which provide the security to help you and your family live full lives:
www.abbottbenefits.com
Follow your career aspirations to Abbott for diverse opportunities with a company that can help you build your future and live your best life. Abbott is an Equal Opportunity Employer, committed to employee diversity.
Connect with us at www.abbott.com, on Facebook at www.facebook.com/Abbott and on Twitter @AbbottNews and @AbbottGlobal.
The base pay for this position is $90,700.00 – $181,300.00. In specific locations, the pay range may vary from the range posted.
Show more
Show less","DevOps, Cloud Service Providers, Configuration management, Infrastructure automation, Continuous integration, Continuous deployment, Best practices, Scalable systems, Highly available systems, Efficient systems, Secure systems, Medical device SaaS, Software engineering, Security engineering, Healthcare platform, Diabetes management, Engineering expertise, Software development, Cloud computing, Cloud infrastructure, AWS, VPC, EC2, Security Groups, IAM, ECS, CodeDeploy, CloudFront, S3, Infrastructure as Code, Ansible, Terraform, CloudFormation, Container orchestration services, Kubernetes, Networking, Linux administration, Opensource technologies, Medical devices, Biomedical, High technology, Agile environment","devops, cloud service providers, configuration management, infrastructure automation, continuous integration, continuous deployment, best practices, scalable systems, highly available systems, efficient systems, secure systems, medical device saas, software engineering, security engineering, healthcare platform, diabetes management, engineering expertise, software development, cloud computing, cloud infrastructure, aws, vpc, ec2, security groups, iam, ecs, codedeploy, cloudfront, s3, infrastructure as code, ansible, terraform, cloudformation, container orchestration services, kubernetes, networking, linux administration, opensource technologies, medical devices, biomedical, high technology, agile environment","agile environment, ansible, aws, best practices, biomedical, cloud computing, cloud infrastructure, cloud service providers, cloudformation, cloudfront, codedeploy, configuration management, container orchestration services, continuous deployment, continuous integration, devops, diabetes management, ec2, ecs, efficient systems, engineering expertise, healthcare platform, high technology, highly available systems, iam, infrastructure as code, infrastructure automation, kubernetes, linux administration, medical device saas, medical devices, networking, opensource technologies, s3, scalable systems, secure systems, security engineering, security groups, software development, software engineering, terraform, vpc"
Forecasting and Planning Data Model Scientist,Siemens Energy,"Orlando, FL",https://www.linkedin.com/jobs/view/forecasting-and-planning-data-model-scientist-at-siemens-energy-3772670852,2023-12-17,Avondale,United States,Associate,Onsite,"A Snap Shot of your Day:
How You’ll Make An Impact
Ensure and support identification of internal and external data sources and datasets relevant for demand forecasting
Ensure relevant data is available in time through export files or via interfaces by driving internal initiatives with support from management and implementation by data engineers
Implement data pipelines (data ingestion, data cleansing, and data preparation for analysis) for the creation of datasets used for analyses and model building
Define data requirements for analysis (e.g., accuracy, consistency)
Request and initiate internal data correction, if required (e.g., data is incomplete and incorrect)
Build and maintain models (e.g., causal models to measure and quantify the impact of drivers on
demand)
Establish operationalization through automation (e.g., drive MLOps initiatives, set up model repositories)
Prepare regular and ad-hoc reports for stakeholders (model outputs and data visualizations)
Validate model outputs with subject matter experts
Send out regular reports and communicate findings to management
Manage relationships to internal and external data providers/sources
Prepare internal trainings to transfer knowledge to other data experts (i.e., data analysts and data engineers)
What You Bring
Competencies
Strong background in general data science topics (e.g., data preparation, mathematical foundations, modeling) - Plus: Prior exposure to causal AI topics (e.g., Bayesian networks) and related software (e.g., BayesiaLab)
Experience in technical implementation of data science applications and frameworks (incl. implementation of data pipelines) - Plus: Basic understanding of software development principles (e.g., modularity, testing, version control), common databases and paradigms, and data management practices
Keen interest in new technologies and ability to create prototypes - Plus: Experience in using common analytics tools for collaborative project work
Able to communicate with different stakeholders and to create appropriate data visualizations
Qualifications
Advanced Degree in STEM (Science, Technology, Engineering, Mathematics) or other field of study with strong quantitative focus (e.g., Econometrics, Quantitative Finance)
2+ years of professional experience developing data science solutions in a business context
5+ years of experience in data science related programming
Gas Services
Our Gas Services division offers Low-emission power generation through service and decarbonization. Zero or low emission power generation and all gas turbines under one roof, steam turbines and generators. Decarbonization opportunities through service offerings, modernization, and digitalization of the fleet.
Check out this video to learn more about our Gas Service business https://www.siemens-energy.com/global/en/offerings/power-generation.html
What’s it like to work at Siemens Energy?
https://bit.ly/3IfnlaR
Who is Siemens Energy?
At Siemens Energy, we are more than just an energy technology company. We meet the growing energy demand across 90+ countries while ensuring our climate is protected. With more than 92,000 dedicated employees, we not only generate electricity for over 16% of the global community, but we’re also using our technology to help protect people and the environment.
Our global team is committed to making sustainable, reliable, and affordable energy a reality by pushing the boundaries of what is possible. We uphold a 150-year legacy of innovation that encourages our search for people who will support our focus on decarbonization, new technologies, and energy transformation.
Our Commitment to Diversity
Lucky for us, we are not all the same. Through diversity we generate power. We run on inclusion and our combined creative energy is fueled by over 130 nationalities. Siemens Energy celebrates character – no matter what ethnic background, gender, age, religion, identity, or disability. We energize society, all of society, and we do not discriminate based on our differences.
Rewards
Career growth and development opportunities
Supportive work culture
Company paid Health and wellness benefits
Paid Time Off and paid holidays
401K savings plan with company match
Family building benefits
Parental leave
https://jobs.siemens-energy.com/jobs
Show more
Show less","Data Science, Causal AI, Bayesian Networks, BayesiaLab, Data Pipelines, Data Preparation, Data Cleansing, Data Analysis, Model Building, Data Requirements, Data Correction, Model Validation, MLOps, Model Repositories, Data Visualization, Data Communication, Data Transfer, Data Management, Software Development, Modularity, Testing, Version Control, Databases, Analytics Tools, Collaborative Project Work, STEM, Econometrics, Quantitative Finance, Lowemission, Decarbonization, Steam Turbines, Generators, Modernization, Digitalization","data science, causal ai, bayesian networks, bayesialab, data pipelines, data preparation, data cleansing, data analysis, model building, data requirements, data correction, model validation, mlops, model repositories, data visualization, data communication, data transfer, data management, software development, modularity, testing, version control, databases, analytics tools, collaborative project work, stem, econometrics, quantitative finance, lowemission, decarbonization, steam turbines, generators, modernization, digitalization","analytics tools, bayesialab, bayesian networks, causal ai, collaborative project work, data communication, data correction, data management, data preparation, data requirements, data science, data transfer, dataanalytics, databases, datacleaning, datapipeline, decarbonization, digitalization, econometrics, generators, lowemission, mlops, model building, model repositories, model validation, modernization, modularity, quantitative finance, software development, steam turbines, stem, testing, version control, visualization"
"Data Analyst III-locals____________orlando,FL",Steneral Consulting,"Orlando, FL",https://www.linkedin.com/jobs/view/data-analyst-iii-locals-orlando-fl-at-steneral-consulting-3755297369,2023-12-17,Avondale,United States,Associate,Hybrid,"Data Analyst III
Orlando-FL
12M
Skills:Quantexa and data mapping
Will this have the ability to be temp to perm? Yes
How many years exp are you looking for? 5+
Is financial exp a must? nice to have
Degree required/type? Will you consider experience in lieu of a college degree? Bachelor’s, but will consider 5+ years of experience over degree
Will position be a remote start or onsite? Onsite
If Hybrid-how many days are required to be in the office per week? 3x a week - Will only consider remote if a perfect fit
Will the candidate be required to provide their own equipment?
What are the top 3 technical skills that you’ll be looking for on a resume?
Data mapping/analytics / writing user stories
Good Leadership /communication skills -can present at meetings
Will be working on Quantexa platform, text cases, data mapping, data platforms, queries - Experience in data platforms such as Quantexa, Oracle, etc
Jira, SQL, Oracle
Show more
Show less","SQL, Data mapping, Oracle, Quantexa, Jira, Data analytics, Text cases, Writing user stories, Leadership skills, Communication skills","sql, data mapping, oracle, quantexa, jira, data analytics, text cases, writing user stories, leadership skills, communication skills","communication skills, data mapping, dataanalytics, jira, leadership skills, oracle, quantexa, sql, text cases, writing user stories"
"GCP Data Engineer at Orlando, FL",XCUTIVES Inc.,"Orlando, FL",https://www.linkedin.com/jobs/view/gcp-data-engineer-at-orlando-fl-at-xcutives-inc-3768244361,2023-12-17,Avondale,United States,Mid senior,Onsite,"Need
GCP Data Engineer
in
Orlando, FL
. 100% onsite job. Please send an email (aamna.anwar@xcutives.com) to know more about this position.
Job Title: GCP Data Engineer
Location: Orlando, FL
Type: Fulltime
Job Description
Strong understanding of Dataflow, BigQuery, Cloud SQL, ETL with GCP (must have), Airflow & Composer
Excellent with Java/Python and Google Cloud SDK, API Scripting
Work with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
Big Data, Good SQL scripting experience
Good communication, client-facing.
Regards,
Aamna Anwar
Senior Technical Recruiter
Phone: +1470- 891- 5812 Ext 1031
Email: aamna.anwar@xcutives.com
Xcutives Inc. |
www.xcutives.com
Show more
Show less","Google Cloud Platform (GCP), Dataflow, BigQuery, Cloud SQL, ETL, Java, Python, Google Cloud SDK, API Scripting, Airflow, Composer, SQL, Data engineering, Data analytics, Data warehousing, Big data, Cloud computing","google cloud platform gcp, dataflow, bigquery, cloud sql, etl, java, python, google cloud sdk, api scripting, airflow, composer, sql, data engineering, data analytics, data warehousing, big data, cloud computing","airflow, api scripting, big data, bigquery, cloud computing, cloud sql, composer, data engineering, dataanalytics, dataflow, datawarehouse, etl, google cloud platform gcp, google cloud sdk, java, python, sql"
Senior Data Engineer,Sparibis,"Orlando, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-sparibis-3727272649,2023-12-17,Avondale,United States,Mid senior,Onsite,"Location:
100% telework
Years’ Experience:
10+ years
Education:
Bachelor’s in IT related field
Work Authorization:
Must show that applicant is legally permitted to work in the United States.
Clearance:
Applicants must be able to meet the requirements to obtain a Public Trust security clearance. NOTE: United States Citizenship is required.
Key Skills
10+ years of IT experience focusing on enterprise data architecture and management.
Experience with Databricks required.
8+ years’ experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services
Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
Experience with AWS environment, CI/CD pipelines, and Python (Python 3) a bonus
Responsibilities
Plan, create, and maintain data architectures, ensuring alignment with business requirements.
Obtain data, formulate dataset processes, and store optimized data.
Identify problems and inefficiencies and apply solutions.
Determine tasks where manual participation can be eliminated with automation.
Identify and optimize data bottlenecks, leveraging automation where possible.
Create and manage data lifecycle policies (retention, backups/restore, etc.)
Create, maintain, and manage ETL/ELT pipelines.
Create, maintain, and manage data transformations.
Maintain/update documentation.
Create, maintain, and manage data pipeline schedules.
Monitor data pipelines.
Create, maintain, and manage data quality gates (Great Expectations) to ensure high data quality.
Support AI/ML teams with optimizing feature engineering code
Spark updates
Create, maintain, and manage Spark Structured Steaming jobs, including using the newer Delta Live Tables and/or DBT.
Research existing data in the data lake to determine best sources for data.
Create, manage, and maintain ksqlDB and Kafka Streams queries/code.
Maintain and update Python-based data processing scripts executed on AWS Lambdas
Unit tests for all the Spark, Python data processing and Lambda codes
Maintain PCIS Reporting Database data lake with optimizations and maintenance (performance tuning, etc)
Qualifications
10+ years of IT experience focusing on enterprise data architecture and management.
Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
Experience with Databricks, Structured Streaming, Delta Lake concepts, and Delta Live Tables required
Additional experience with Spark, Spark SQL, Spark DataFrames and DataSets, and PySpark
Data Lake concepts such as time travel and schema evolution and optimization
Structured Streaming and Delta Live Tables with Databricks a bonus
Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
Advanced level understanding of streaming data pipelines and how they differ from batch systems
Formalize concepts of how to handle late data, defining windows, and data freshness
Advanced understanding of ETL and ELT and ETL/ELT tools such as SSIS, Pentaho, Data Migration Service etc
Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermark, etc.
Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus
Understanding of streaming data pipelines and batch systems
Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness
Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
Indexing and partitioning strategy experience
Debug, troubleshoot, design and implement solutions to complex technical issues
Experience with large-scale, high-performance enterprise big data application deployment and solution
Understanding how to create DAGs to define workflows
Familiarity with CI/CD pipelines, containerization, and pipeline orchestration tools such as Airflow, Prefect, etc a bonus but not required
Architecture experience in AWS environment a bonus
Familiarity working with Kinesis and/or Lambda specifically with how to push and pull data, how to use AWS tools to view data in Kinesis streams, and for processing massive data at scale (UNICORN) a bonus
Experience with Docker, Jenkins, and CloudWatch
Ability to write and maintain Jenkinsfiles for supporting CI/CD pipelines
Experience working with AWS Lambdas for configuration and optimization
Experience working with DynamoDB to query and write data
Experience with S3
Knowledge of Python (Python 3 desired) for CI/CD pipelines a bonus
Familiarity with Pytest and Unittest a bonus
Experience working with JSON and defining JSON Schemas a bonus
Experience setting up and management Confluent/Kafka topics and ensuring performance using Kafka a bonus
Familiarity with Schema Registry, message formats such as Avro, ORC, etc.
Understanding how to manage ksqlDB SQL files and migrations and Kafka Streams
Ability to thrive in a team-based environment
Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management
About Sparibis
Sparibis LLC is a professional solution firm that Clients rely on to access the best talent to drive their business success.
Sparibis is an equal opportunity employer that values diversity at all levels. All individuals, regardless of personal characteristics, are encouraged to apply.
Show more
Show less","Data Architecture, Enterprise Data Management, Databricks, SQL, AWS, Python, Data Modeling, ETL, ELT, SSIS, Pentaho, Data Migration Services, Spark, Spark SQL, Spark DataFrames, DataSets, PySpark, Data Lake, Time Travel, Schema Evolution, Structured Streaming, Delta Live Tables, System Integration, Data Migration, Data Transformation, Data Warehouse, Data Mart, Lambda, Great Expectations, Data Quality, Data Validation, Indexing, Partitioning, CI/CD Pipelines, Airflow, Prefect, Containerization, Pipeline Orchestration, Docker, Jenkins, CloudWatch, Pytest, Unittest, JSON, JSON Schemas, Confluent/Kafka, Kafka, Schema Registry, Avro, ORC, ksqlDB, Kafka Streams","data architecture, enterprise data management, databricks, sql, aws, python, data modeling, etl, elt, ssis, pentaho, data migration services, spark, spark sql, spark dataframes, datasets, pyspark, data lake, time travel, schema evolution, structured streaming, delta live tables, system integration, data migration, data transformation, data warehouse, data mart, lambda, great expectations, data quality, data validation, indexing, partitioning, cicd pipelines, airflow, prefect, containerization, pipeline orchestration, docker, jenkins, cloudwatch, pytest, unittest, json, json schemas, confluentkafka, kafka, schema registry, avro, orc, ksqldb, kafka streams","airflow, avro, aws, cicd pipelines, cloudwatch, confluentkafka, containerization, data architecture, data lake, data mart, data migration, data migration services, data quality, data transformation, data validation, databricks, datamodeling, datasets, datawarehouse, delta live tables, docker, elt, enterprise data management, etl, great expectations, indexing, jenkins, json, json schemas, kafka, kafka streams, ksqldb, lambda, orc, partitioning, pentaho, pipeline orchestration, prefect, pytest, python, schema evolution, schema registry, spark, spark dataframes, spark sql, sql, ssis, structured streaming, system integration, time travel, unittest"
Healthcare Data Analyst,AssistRx,"Orlando, FL",https://www.linkedin.com/jobs/view/healthcare-data-analyst-at-assistrx-3646116559,2023-12-17,Avondale,United States,Mid senior,Onsite,"The Data Quality Analyst is responsible for acquiring, managing and analyzing data that will be presented to internal teams, clients and partners. The Data Analyst leads the onboarding of electronic data trading partners and is responsible for the success of trading partner implementations. The Data Quality Analyst is responsible for monitoring client data submissions and communicating with them in the resolution of data quality issues.
Successful applicants will have experience with Excel
Pharmacy Claims experience is a huge plus
Requirements
Developing reports and analytics using Excel
Heavy Excel
Works with partners to help guide and assist in creating a data feed according to defined specifications
Proactively communicates with trading partners and clients to expedite their onboarding
Communicates directly with customer on data needs and key deadlines
Researches and identifies data quality issues reported through the trading partner or found through new file submission and work with client to create a resolution
Fields ongoing, incoming partner requests and questions regarding data specifications
Reviews files received for compliance with data needs, including testing files and identifying the business rules that will need to be configured in mapping tool
Configures mapping tool to support translation from source data to desired format. Includes testing of tool and providing clear documentation to internal resources and customer
Analyzes and communicates trading partner performance in their ability to meet data specifications in a timely and effective manner
Act as resident expert for data requirements/specifications internally and for the client
Communicates data needs to the trading partner both initially and after we begin to receive files for the customer
Researches, coordinates, and executes the transfer of new data feeds or data corrections with clients to ensure that continuity and level of service are maintained
Remain informed and up to speed with ongoing changes and evolution of data specs
Lead client/partner web-based trainings regarding data specifications and requirements
Qualifications:
Self-starter, an individual who is not fully dependent on direction to fulfill the functions of the role
Thrives in an entrepreneurial-like environment
Experience with ETL, analytic-based, data-driven, SSIS and Microsoft BI stack
Experience with Microsoft Excel and SQL is a must
Experience with healthcare and/or pharmacy data is preferred
Previous client-facing experience is a must
Must be extremely responsive, able to work under pressure in crisis with a strong sense of urgency
Benefits
Supportive, progressive, fast-paced environment
Competitive pay structure
Matching 401(k) with immediate vesting
Medical, dental, vision, life, & short-term disability insurance
AssistRx, Inc. is proud to be an Equal Opportunity Employer. All qualified applicants will receive consideration without regard to race, religion, color, sex (including pregnancy, gender identity, and sexual orientation), parental status, national origin, age, disability, family medical history or genetic information, political affiliation, military service, or other non-merit based factors, or any other protected categories protected by federal, state, or local laws.
All offers of employment with AssistRx are conditional based on the successful completion of a pre-employment background check.
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire. Sponsorship and/or work authorization is not available for this position.
AssistRx does not accept unsolicited resumes from search firms or any other vendor services. Any unsolicited resumes will be considered property of AssistRx and no fee will be paid in the event of a hire
Show more
Show less","Data Analysis, Data Quality, Excel, Reporting, ETL, Healthcare Data, Pharmacy Data, SQL, SSIS, Microsoft BI Stack, Data Specifications, Data Mapping, Data Integration","data analysis, data quality, excel, reporting, etl, healthcare data, pharmacy data, sql, ssis, microsoft bi stack, data specifications, data mapping, data integration","data integration, data mapping, data quality, data specifications, dataanalytics, etl, excel, healthcare data, microsoft bi stack, pharmacy data, reporting, sql, ssis"
Snowflake Data Engineer - (Hybrid),Latitude Inc,"Orlando, FL",https://www.linkedin.com/jobs/view/snowflake-data-engineer-hybrid-at-latitude-inc-3787732138,2023-12-17,Avondale,United States,Mid senior,Onsite,"Are you a skilled data engineer with expertise in Snowflake, the cutting-edge cloud data platform? Join the team and play a pivotal role in architecting and optimizing our data infrastructure. As a Snowflake Data Engineer, you will be instrumental in designing, building, and maintaining our data solutions to support analytics, reporting, and business intelligence efforts.
Responsibilities:
Data Architecture: Design and implement robust data pipelines, data models, and integration strategies using Snowflake.
ETL Development: Develop and manage ETL processes to extract, transform, and load data from various sources into Snowflake.
Performance Optimization: Ensure efficient query performance by optimizing data structures, indexes, and query execution in Snowflake.
Data Quality: Implement data validation, data cleansing, and data quality checks to maintain accurate and reliable data.
Collaboration: Collaborate with data analysts, scientists, and business stakeholders to understand requirements and deliver valuable insights.
Security and Compliance: Implement security measures and data governance practices to maintain data privacy and compliance.
Monitoring and Maintenance: Monitor the health and performance of data pipelines and resolve any issues that arise.
Documentation: Maintain clear and comprehensive documentation of data pipelines, processes, and transformations.
Powered by JazzHR
2kQkA5ajeQ
Show more
Show less","Snowflake, Data Architecture, ETL Development, Data Engineering, Performance Optimization, Data Quality, Collaboration, Data Analysis, Data Science, Data Governance, Security, Compliance, Monitoring, Maintenance, Documentation","snowflake, data architecture, etl development, data engineering, performance optimization, data quality, collaboration, data analysis, data science, data governance, security, compliance, monitoring, maintenance, documentation","collaboration, compliance, data architecture, data engineering, data governance, data quality, data science, dataanalytics, documentation, etl development, maintenance, monitoring, performance optimization, security, snowflake"
Senior Data Engineer / Database Developer,Tews Company,"Orlando, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-database-developer-at-tews-company-3783721227,2023-12-17,Avondale,United States,Mid senior,Onsite,"Job Description
Sr. Data Engineer / Database Developer
100% Remote
Databricks Experience Required
Mergers & Acquisition Experience Preferred
Excellent Benefits including Day1 Medical, Dental & Vision Insurance
Our enterprise client is looking for a Sr. Data Engineer & Database Developer. We are looking for someone who has experience loading databases using modern data integration tools. The ideal candidate should be familiar with creating tables, writing stored procedures, functions and building ETL/ELT jobs to load data from disparate systems. The Sr Data Engineer will be loading data to and from a variety of sources including data marts and transactional applications with conventional ETL/ELT patterns as well as modern data streaming or messaging patterns.
Develop and maintain high-performance code based on SQL Server stack, including Stored Procedures, Views, Functions, and table design for transactional and analytic purposes.
Develop and maintain Extract Transform Load jobs (ETL/ELT) and other data integrations
Develop new data solutions and migrate existing on-premise data solutions to cloud platforms such as Azure or GCP
Create Continuous Integration and Continuous Delivery (CI/CD) pipelines to automate delivery of solutions
Provide support for data solutions, including remediation of failed ETL jobs, integrations and investigation of data quality issues.
Data modeling for transactional and analytical platforms.
Work directly with stakeholders to gather requirements and provide status updates and solution support
Ensure data solutions meet or exceed enterprise regulatory and compliance requirements
Qualifications:
Bachelor's degree in business, computer science, computer engineering, electrical engineering, system analysis or a related field of study, or equivalent experience.
5+ years experience using Microsoft SQL Server
2-3 years experience with Databricks
5+ years experience building data integrations leveraging ETL tools such as SSIS or Talend
Familiarity with data integration patterns including ETL/ELT, event messaging and APIs
Familiarity with database change capture tools including Change Tracking and Change Data Capture
Familiarity with data modeling patterns
Familiarity with Master Data Management concepts and platforms
Familiarity with data product Software Development Lifecycle (SDLC)
Excellent communications skills.
Excellent analytical and technical skills.
Excellent planning and organizational skills.
The ability to work in a collaborative team environment.
Preferred Qualifications:
Master's degree in business, computer science, computer engineering, electrical engineering, system analysis or a related field of study, or equivalent experience.
10+ years experience with Microsoft SQL stack
Experiencing with an Enterprise Architecture and Service Delivery model, such as TOGAF and ITIL
Certification in a cloud data engineering curriculum
#zip
Show more
Show less","SQL Server, Database Development, Databricks, ETL/ELT, Data Integration, Cloud Platforms, Azure, GCP, CI/CD Pipelines, Data Modeling, Master Data Management, SDLC, SSIS, Talend, Change Tracking, Change Data Capture, API","sql server, database development, databricks, etlelt, data integration, cloud platforms, azure, gcp, cicd pipelines, data modeling, master data management, sdlc, ssis, talend, change tracking, change data capture, api","api, azure, change data capture, change tracking, cicd pipelines, cloud platforms, data integration, database development, databricks, datamodeling, etlelt, gcp, master data management, sdlc, sql server, ssis, talend"
Sr. Data Engineer,Latitude Inc,Greater Orlando,https://www.linkedin.com/jobs/view/sr-data-engineer-at-latitude-inc-3787730196,2023-12-17,Avondale,United States,Mid senior,Onsite,"SENIOR DATA ENGINEER DATA OVERVIEW OF RESPONSIBILITIES
:
The Senior Data Engineer role is responsible for overseeing the data engineering activities, including but not limited to building the business’ data collection systems and processing pipelines, building and maintaining optimized and highly available data pipelines that facilitate deeper analysis and reporting by the Data Science and Analytics departments.
The Senior Data Engineer is involved in various phases of data science initiatives, starting from data acquisition, data collection, building and configuring the data connectors to funnel data from various heterogenous sub-systems to the corporate’s data warehouse, data cleansing and merging, creating and maintaining the dimensional models on the data warehouse, creating and maintaining necessary database constructs (such as views, functions, stored procedures), providing the necessary pipelines to feed the corporate’s visualization tools.
The Senior Data Engineer works closely with the data science leadership as well as other data and analytics teams in leveraging data with reporting and scientific tools, for instance, Tableau, Python, SnowFlake, among other tools and technologies. The Senior Data Engineer strives to continuously develop new and improved data engineering capabilities.
DESCRIPTION OF DUTIES
:
Maintain and build on top of our data warehouse and analytics environment, the home for almost all of the data.
Design, implement, test, deploy, and maintain stable, secure, and scalable data engineering solutions and pipelines in support of data and analytics projects, including integrating new sources of data into our central data warehouse, and moving data out to applications and affiliates.
Build reports and data visualizations, using data from the data warehouse and other sources.
Produce scalable, replicable code and engineering solutions that help automate repetitive data management tasks.
Perform one-off data manipulation/munging and analysis on a wide variety of data.
Implement and monitor best in class security measures in our data warehouse and analytics environment, with an eye towards the evolving threat landscape.
Help troubleshoot other data engineers’ SQL, Python, or R code.
Other duties as assigned.
QUALIFICATIONS
:
BS in related field or equivalent work experience (MS is preferred for this role).
Strong command of relational databases and SQL. Extract, Transform, and Load (ETL) data into a relational database.
Proficiency with Python or R, especially for data manipulation and analysis, and ability to build, maintain and deploy sequences of automated processes with these tools.
BS in related field or equivalent work experience (MS is preferred for this role).
General data manipulation skills: read in data, process and clean it, transform and recode it, merge different data sets together, reformat data between wide and long, etc.
Demonstrated ability to learn new techniques and troubleshoot code without support, ex. find answers to common programming challenges on Google. In other words, be able to learn on the job.
Demonstrated ability to write clear code that is well-documented and stored in a version control system (we use Git).
Demonstrated ability to work independently and be a self-starter.
Excellent listening, interpersonal, communication and problem solving skills.
Demonstrated ability to work effectively in teams, in both a lead and support role.
Use APIs to push and pull data from various data systems and platforms.
Experience working with cloud infrastructure services like Amazon Web Services and Google Cloud is preferred but not required.
Effective time management skills, including demonstrated ability to manage and prioritize multiple tasks and projects.
Experience with SnowFlake is required.
Experience with advanced data visualization and mapping are required.
Powered by JazzHR
bunrQBGTLk
Show more
Show less","MySQL, SQL, Python, R, ETL, Data Manipulation, Data Analysis, Data Visualization, SnowFlake, Git, Version Control System, Cloud Infrastructure Services, Data Warehousing, Data Engineering Solutions, Data Pipelines, Data Cleansing, Data Merging, Data Modeling, Dimensional Modeling, Data Security, Reporting, Data Analytics, Machine Learning","mysql, sql, python, r, etl, data manipulation, data analysis, data visualization, snowflake, git, version control system, cloud infrastructure services, data warehousing, data engineering solutions, data pipelines, data cleansing, data merging, data modeling, dimensional modeling, data security, reporting, data analytics, machine learning","cloud infrastructure services, data engineering solutions, data manipulation, data merging, data security, dataanalytics, datacleaning, datamodeling, datapipeline, datawarehouse, dimensional modeling, etl, git, machine learning, mysql, python, r, reporting, snowflake, sql, version control system, visualization"
Sr Data Engineer,The Walt Disney Company,"Lake Buena Vista, FL",https://www.linkedin.com/jobs/view/sr-data-engineer-at-the-walt-disney-company-3776831164,2023-12-17,Avondale,United States,Mid senior,Onsite,"The Disney Decision Science and Integration (DDSI)
is the Walt Disney Company’s center of excellence for developing and deploying innovative analytics that support decision making for client organizations across The Walt Disney Company (TWDC), including Disney Media & Entertainment Distribution (e.g., ABC, ESPN, FX, Studio Entertainment, Disney Theatrical Group) and Parks, Experiences & Products. To drive business value through decision-shaping analytics, DDSI leverages technology, optimization, simulation, statistics and machine learning models to explore opportunities and deploy new analytical capabilities. The Decision Science Product team leverages technology, data analytics, optimization, statistical and econometric modeling to identify opportunities to inform business decisions and drive value for our internal business partners. The team works to understand our partners’ business problems, workflows, technical systems, and data to deliver products ranging from straight-forward analytics insights to complex, fully-integrated analytics solutions.
The Decision Science Product team resides within the Disney Decision Science + Integration (DDSI) organization. DDSI provides internal consulting services for clients across The Walt Disney Company. The DDSI team sits at the intersection of business strategy, advanced analytics, and technology integration to help our partners explore opportunities for analytics, shape business decisions, and drive value. Our work includes conceptualizing new solutions, solution design, custom code engineering / application development, implementation and integration with business processes and technologies.
The successful candidate will be data-driven and business-minded, and will be passionate about solving business opportunities through data analysis and application development. In this position, you will transform data into meaningful business insights and recommended actions. The role will include working with analytically minded peers, where you are expected to contribute to and challenge the current knowledgebase. You will lead improvements of existing solutions or development of new solutions by scoping, implementing, and testing these solutions in order to address business problems. You should enjoy connecting with clients to brainstorm ways to better support business decisions, being a thought leader in analytical settings, solving high-value complex problems, and managing / improving software systems.
Responsibilities:
Act as a Company-wide consultant leveraging knowledge of existing data structures and business knowledge to solve problems.
Own the vision for analytical systems (or subsystems), be accountable for integrating these systems into existing business processes and software, and define and maintain the consolidated product roadmap for future ongoing evolution of these systems.
Be an expert in the data; including source systems, design data integration solutions, data validation, and data testing processes.
Be an expert in our applications; understand how our systems and applications receive data, how it is stored and used, and the underlying processes that leverage it.
Be a problem solver; when presented with new challenges, you are expected to research and network to find solutions.
Develop custom-code solutions that are production-ready; when a solution is needed for a specific business problem, support the full application lifecycle.
Be a hands-on data expert who writes new/custom SQL and Python to extract, translate, and load data (ETL)
Seek out answers to business problems, and look for opportunities to automate processes.
Collaborate with partners in Data Engineering, App Engineering, and Decision Science throughout the development process to ensure products are built cohesively
Proactively communicate with technical and non-technical stakeholders, seek to learn and understand, and potentially challenge existing processes in order to continuously seek improved efficiencies.
Required Qualifications:
Must have 5+ years of progressively complex experience in software or data engineering - professional experience not just academic
3+ years experience with Python
4+ years experience with SQL
2+ years experience with cloud based technologies, preferably AWS EMR, EC2, and S3
2+ years leveraging, designing and/or building relational databases (preferably Postgres or Snowflake)
2+ years of experience using Gitlab/Github
2+ years of experience using Docker
Experience working with large datasets and big data technologies, preferably cloud-based, such as Snowflake, Databricks, or similar
Experience using Apache Airflow
Proven ability to leverage best practices around data engineering application development processes
Experience with cloud-based application development and deployment
Demonstrated excellent written and verbal communication skills
Demonstrated excellent interpersonal skills, including ability to partner with others and build consensus in a cross-functional team toward a desired outcome
Proven ability to organize and prioritize work
Ability to adapt to a rapidly changing business environment and priorities
Advanced proficiency with multiple client- or internal systems and databases
Experience delivering custom-written code across multiple product environments (Development to Production)
Proficient in a variety of data science methods, scalable infrastructure setup, software development, and system architecture plus a basic understanding of system design / architecture
Preferred Qualifications:
Knowledge of the fields of revenue management, advertising technology (AdTech), and/or pricing strategies
Knowledge of Media Ad Sales (Linear and/or Digital)
Strong understanding of advanced analytical methods
Experience developing short- and long-term product roadmaps for implementation and sustainment
Technical expertise applied to rapidly changing, highly agile requirements
Proficiency in automating and improving existing processes
Knowledge of machine learning methods
Experience with Nielsen audience data or similar
Experience with or understanding of Data Management Platforms
Required Education:
Bachelor's degree in Computer Science, Engineering, Mathematics, Finance, Economics, Data Mining, Analytics, Statistics, or other quantitative field; Master's Degree preferred.
Show more
Show less","Python, SQL, Gitlab, Github, Docker, Snowflake, Apache Airflow, AWS EMR, AWS EC2, AWS S3, Postgres, Data Science, Data Engineering, Cloudbased application development and deployment, Software development, System architecture, Revenue management, Advertising technology, Pricing strategies, Media Ad Sales, Advanced analytical methods, Machine learning methods, Nielsen audience data, Data Management Platforms","python, sql, gitlab, github, docker, snowflake, apache airflow, aws emr, aws ec2, aws s3, postgres, data science, data engineering, cloudbased application development and deployment, software development, system architecture, revenue management, advertising technology, pricing strategies, media ad sales, advanced analytical methods, machine learning methods, nielsen audience data, data management platforms","advanced analytical methods, advertising technology, apache airflow, aws ec2, aws emr, aws s3, cloudbased application development and deployment, data engineering, data management platforms, data science, docker, github, gitlab, machine learning methods, media ad sales, nielsen audience data, postgres, pricing strategies, python, revenue management, snowflake, software development, sql, system architecture"
Performance Data Analyst,The Walt Disney Company,"Celebration, FL",https://www.linkedin.com/jobs/view/performance-data-analyst-at-the-walt-disney-company-3780365774,2023-12-17,Avondale,United States,Mid senior,Onsite,"About The Role & Team
Consumer Insight, Measurement & Analytics is a part of Disney Parks, Experiences and Products, and is a center of excellence leading strategy development and decision making through actionable data analytics, integration, and insights. The Digital and Travel Operations Analytics team is passionate about using standard methodologies in analytics to capture data, analyze, and optimize our digital and contact center channels to drive Guest experience and business outcomes.
We provide measurement and analytical support for Contact Centers and pre‑arrival Walt Disney World Commerce. The main responsibility will be forecasting as well as developing sophisticated models to answer business questions. We also engage in root-cause analysis, processing, quality assurance, and providing analytical support to operations and partners across the division. This includes development, administration, measurement, and analytics. Successful candidates will be self-starters, who are endlessly curious, meticulous, and passionate about data storytelling.
We encourage you to apply, even if you don't think you meet every single requirement in this posting. In DTOA, we’re looking for phenomenal analytical talent, not just those who tick 100% of the boxes.
You will report to Manager, Travel Operations Analytics and be located in Orlando, Florida.
What You Will Do
Performance Analytics and Forecasting
Conduct forecasting, ad hoc modeling, and deep-dive analysis to influence business decisions
Analyze agent performance, proactively identify areas that require additional research, and own analyses from start to finish
Build presentations and visualizations to tell compelling stories with data
Present and communicate findings, insights, and recommendations to Senior Leadership and partners
Build, maintain, audit, and continuously optimize reports and files used to process numerous programs
Manage programs critical to the business operation accurately under strict timelines
Measurement and Analytics
Leverage multiple reporting systems and databases (Tableau, Snowflake, Hadoop, Dataiku, Excel, etc.) to conduct analyses and build/maintain reports
Supervise data quality and report accuracy
Collaborate with team, leaders, and business partners to answer abstract business questions with sound quantitative analyses and reproducible results
Apply innovative analytic approaches to test hypotheses and extract actionable insights
Stay up-to-date with the latest data science trends and share knowledge and best practices with team members
Required Qualifications & Skills
2+ years of quantitative, non-internship experience in forecasting and analytics
Experience with time series forecasting methods and modeling using machine learning or deep learning techniques
Proficient in Python
Proactive curiosity to understand the “Why” behind performance, decisions, etc.
Strong attention to detail and ability to follow through on tasks and complete projects on time
Effective verbal, written, and visual skills
Desire to excel in a fast-paced, and ambiguous business environment
Preferred Qualifications
Solid understanding of statistical inference and applying statistical rigor to analyses
Knowledge of data visualization standard methodologies and experience with data visualization tools, such as Tableau or Power BI
Excellent SQL skills
Graduate degree and/or 4+ years of proven experience in analytics or data science
Education
Bachelor’s degree or equivalent
Show more
Show less","Forecasting, Analytics, Data storytelling, Python, SQL, Tableau, Snowflake, Hadoop, Dataiku, Excel, Machine learning, Deep learning, Timeseries forecasting, Statistical inference, Statistical rigor, Data visualization, Power BI","forecasting, analytics, data storytelling, python, sql, tableau, snowflake, hadoop, dataiku, excel, machine learning, deep learning, timeseries forecasting, statistical inference, statistical rigor, data visualization, power bi","analytics, data storytelling, dataiku, deep learning, excel, forecasting, hadoop, machine learning, powerbi, python, snowflake, sql, statistical inference, statistical rigor, tableau, timeseries forecasting, visualization"
Senior/Staff Data Engineer,EvenUp,"Orlando, FL",https://www.linkedin.com/jobs/view/senior-staff-data-engineer-at-evenup-3728157943,2023-12-17,Avondale,United States,Mid senior,Remote,"EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year.
Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds
. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.
Why we are hiring a Senior/Staff Data Engineer now?
We have experienced unprecedented growth and need to scale out our data warehousing, data tooling and internal analytics. We need to architect the future of our data infrastructure at EvenUp and we’re seeking engineering leaders to help drive that vision.
We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
We need to design & build data warehousing that democratizes data for our entire organization. We will invest in identifying and integrating tools and services that empower our teams to build on top of our data and analytics.
What you’ll do:
Democratize data at EvenUp. Ensure our organization can scale with consistent, standardized access to our data stores and accelerate our ability to build and experiment with data products
Architect and build out the future of data warehousing at EvenUp
Enable and empower our Data Science team to rapidly iterate on model experimentation
Design, organize and refine data storage strategies that reduce development friction for our tech organization
Collaborate with cross functional teams to solve critical data problems
Help grow our nascent Data Insights team and define a “data first” mentality across our organization
What we are seeking:
8+ years of data engineering experience
Previous experience building out data warehousing, data pipelines, and internal analytics
Strong understanding and practical experience with data tooling, BI tools, and systems such as DBT, BigQuery, Elasticsearch
The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions
Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure
You enjoy owning a project from start to finish and love to drive a project across the finish line
Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves:
Have previously built out a Data Insights team at a data-oriented startup
Have previously planned and architected data migrations at scale
Have stood up analytics tooling to enable cross-functional teams
Domain expertise in legal technology, medical records, and working with unstructured data
A successful first year may look like:
75% doing system design and contributing code, starting with shipping code within 2 weeks!
25% collaborating with stakeholders and mentoring, lunch and learns, and more
Leverage a self-starter mindset by taking a product concept and building the feature end to end (whether it’s a component of the system or a significant piece of functionality).
Collaborate with the team to scale the tech stack based on our rapidly growing user base!
Benefits & Perks:
We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:
Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!)
Flexible working hours to match your style
Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months!
Choice of great medical, dental, and vision insurance plan options
Flexible paid time off
A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Data engineering, Data warehousing, Data pipelines, Internal analytics, Data tooling, BI tools, DBT, BigQuery, Elasticsearch, Data Insights, Agile, SQL, Python, Java, Unix, Linux, AWS, GCP, Azure","data engineering, data warehousing, data pipelines, internal analytics, data tooling, bi tools, dbt, bigquery, elasticsearch, data insights, agile, sql, python, java, unix, linux, aws, gcp, azure","agile, aws, azure, bi tools, bigquery, data engineering, data insights, data tooling, datapipeline, datawarehouse, dbt, elasticsearch, gcp, internal analytics, java, linux, python, sql, unix"
Senior Data Engineer,BambooHR,"Orlando, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-bamboohr-3760295690,2023-12-17,Avondale,United States,Mid senior,Remote,"About Us
Our mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.
What You'll Do
As a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.
Your initial areas of focus will include:
Collaborate with stakeholders to make effective use of core data assets
With Spark and Pyspark libraries, load both streaming and batched data
Engineer lakehouse models to support defined data patterns and use cases
Leverage a combination of tools, engines, libraries, and code to build scalable data pipelines
Work within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environments
Documentation of data pipelines, cloud infrastructure, and standard operating procedures
Express data platform cloud infrastructure, services, and configuration as code
Automate load, scaling, and performance testing of data platform pipelines and infrastructure
Monitor, operate, and optimize data pipelines and distributed applications
Help ensure appropriate data privacy and security
Automate continuous upgrades and testing of data platform infrastructure and services
Build data pipeline unit, integration, quality, and performance tests
Participate in peer code reviews, code approvals, and pull requests
Identify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!)
Experience developing, scaling, and tuning data pipelines in Spark with PySpark
Understanding of data lake, lakehouse, and data warehouse systems, and related technologies
Knowledge and understanding of data formats, data patterns, models, and methodologies
Experience storing data objects in hadoop or hadoop like environments such as S3
Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark
Experience working with streaming technologies such as Kafka and Kinesis
Experience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or Greenplum
Ability to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets manager
Understanding of security around cloud infrastructure and data systems
Git-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!)
Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and Delta
Experience with Flink, Presto, Dremio, Databricks, or Kubernetes
Experience with expressing infrastructure as code leveraging tools like Terraform
Experience and understanding of a zero trust security framework
Experience developing CI/CD pipelines for automated testing and code deployment
Experience with QA and test automation
Exposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are:
Clear communicators with team members and stakeholders
Analytical and perceptive of patterns
Creative in coding
Detail-oriented and persistent
Productive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.
An Equal Opportunity Employer--M/F/D/V
Because our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.
For information on our Privacy Policy, click here.
Show more
Show less","Spark, PySpark, Data Lake, Lakehouse, Data Warehouse, Hadoop, S3, EMR, Kafka, Kinesis, Snowflake, Databricks, Redshift, Vertica, Greenplum, AWS, S3, RDS, IAM, Security Groups, AMIs, Cloudwatch, Cloudtrail, Secrets Manager, Hudi, Iceberg, Delta, Flink, Presto, Dremio, Kubernetes, Terraform, Zero Trust Security Framework, CI/CD Pipelines, QA, Test Automation, Tableau","spark, pyspark, data lake, lakehouse, data warehouse, hadoop, s3, emr, kafka, kinesis, snowflake, databricks, redshift, vertica, greenplum, aws, s3, rds, iam, security groups, amis, cloudwatch, cloudtrail, secrets manager, hudi, iceberg, delta, flink, presto, dremio, kubernetes, terraform, zero trust security framework, cicd pipelines, qa, test automation, tableau","amis, aws, cicd pipelines, cloudtrail, cloudwatch, data lake, databricks, datawarehouse, delta, dremio, emr, flink, greenplum, hadoop, hudi, iam, iceberg, kafka, kinesis, kubernetes, lakehouse, presto, qa, rds, redshift, s3, secrets manager, security groups, snowflake, spark, tableau, terraform, test automation, vertica, zero trust security framework"
Senior Data Engineer,StoneX Group Inc.,"Winter Park, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-stonex-group-inc-3752008138,2023-12-17,Avondale,United States,Mid senior,Hybrid,"Overview
The Data Platform Team looks to raise the level and productivity of data engineering and data science by building, scaling, and supporting our big data infrastructure with an emphasis on simple and efficient solutions on top of complex distributed data stores. As a contributing senior data engineer, you will assist in architecting, designing, and implementing components within our cloud data platform expanding our data assets while continuously improving the architecture and processes around our daily operations.
Responsibilities
Create cloud and big data technical design recommendations for developing and integrating new software and system technologies – from the physical layer through to the virtual layer – per written specifications; test, evaluate, engineer, implement and support said technologies.
Review, influence and contribute to new and evolving design, architecture, standards, and methods for operating and contributing to services within our big data ecosystem.
Add to our existing business and data models. Reviews existing designs and processes to highlight more efficient ways to complete existing workload more effectively through industry perspectives.
Drive technical innovation and efficiency in infrastructure operations through automation by assisting in improvements to continuous integration, continuous deployment and
Create cloud and big data technical design recommendations for developing and integrating new software and system technologies – from the physical layer through to the virtual layer – per written specifications; test, evaluate, engineer, implement and support those technologies
Collaborates with technical teams and utilizes system expertise to deliver technical solutions, continuously learning and evolving big data skillsets.
Monitors and evaluates overall strategic data infrastructure; tracks system efficiency and reliability; identifies and recommends efficiency improvements and mitigates operational vulnerabilities. Respond to and resolve emergent service problems. Design solutions using automation and self-repair rather than relying on alarming and human intervention
Qualifications
Pursuing a Bachelor’s degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline.
5-7 years experience developing software in a professional environment (preferably financial services but not required)
Exposure to Docker/Containers, microservices, distributed systems architecture, Kubernetes, and cloud computing preferably Azure.
Comfortable with core programming concepts and techniques (e.g. concurrency, memory management)
ETL tooling like Airflow and Databricks.
Experience in supporting API Gateways and building and consuming REST APIs along with other distribution technologies.
Familiarity with Financial Systems architecture/ecosystems, Real Time Market Data messaging and FIX Protocol a huge plus.
Foundational knowledge of data structures, algorithms, and designing for performance.
Competent in one of the following programming languages: Java, C# or Python (preferred) and willingness to learn and adopt new languages as necessary
Experience in database technology like MSSQL and one of key value and document databases like MongoDb, Redis, Dynamo Db, Casandra.
Monitoring/Observability concepts and tooling: APM, Distributed Tracing, Grafana, Splunk, Prometheus.
Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts.
Ability to work and potentially lead in an Agile methodology environment.
Show more
Show less","Cloud computing, Big data, Data engineering, Data science, Software architecture, Software design, Software implementation, Continuous integration, Continuous deployment, Automation, Docker, Containers, Microservices, Distributed systems, Kubernetes, Core programming concepts, Concurrency, Memory management, ETL tooling, Airflow, Databricks, API Gateways, REST APIs, Financial Systems architecture, Real Time Market Data messaging, FIX Protocol, Data structures, Algorithms, Java, C#, Python, MSSQL, MongoDB, Redis, DynamoDB, Cassandra, APM, Distributed Tracing, Grafana, Splunk, Prometheus","cloud computing, big data, data engineering, data science, software architecture, software design, software implementation, continuous integration, continuous deployment, automation, docker, containers, microservices, distributed systems, kubernetes, core programming concepts, concurrency, memory management, etl tooling, airflow, databricks, api gateways, rest apis, financial systems architecture, real time market data messaging, fix protocol, data structures, algorithms, java, c, python, mssql, mongodb, redis, dynamodb, cassandra, apm, distributed tracing, grafana, splunk, prometheus","airflow, algorithms, api gateways, apm, automation, big data, c, cassandra, cloud computing, concurrency, containers, continuous deployment, continuous integration, core programming concepts, data engineering, data science, data structures, databricks, distributed systems, distributed tracing, docker, dynamodb, etl tooling, financial systems architecture, fix protocol, grafana, java, kubernetes, memory management, microservices, mongodb, mssql, prometheus, python, real time market data messaging, redis, rest apis, software architecture, software design, software implementation, splunk"
Data Automation Engineer,NR Consulting,"Orlando, FL",https://www.linkedin.com/jobs/view/data-automation-engineer-at-nr-consulting-3768020194,2023-12-17,Avondale,United States,Mid senior,Hybrid,"Job Description
Proficiency in UI automation ( Selenium, Robot, Watir)
Experience in Gherkin ( BDD /TDD )
Proficiency with Python or other OO language
Agile testing
Experience building or improving test automation frameworks.
Proficiency CICD integration and pipeline development in Jenkins, Spinnaker or other similar tools
Show more
Show less","Selenium, Robot, Watir, Gherkin, BDD, TDD, Python, Object Oriented Programming, Unit Testing, Agile Testing, Jenkins, Spinnaker","selenium, robot, watir, gherkin, bdd, tdd, python, object oriented programming, unit testing, agile testing, jenkins, spinnaker","agile testing, bdd, gherkin, jenkins, object oriented programming, python, robot, selenium, spinnaker, tdd, unit testing, watir"
Senior Cloud Data Engineer,BDO USA,"Orlando, FL",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765472149,2023-12-17,Avondale,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Data Warehousing, Data Modeling, Semantic Modeling, Star Schema Construction, Cloud Data Analytics, Data Ingestion, Data Visualization, Streaming Processes, API Integration, Automation, RPA, DevOps, DataOps, MLOps, SQL, C#, Python, Java, Scala, Power BI, Azure Analysis Services, Git, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, UiPath, Alteryx, Computer Vision, .Net, Qlik, Tableau, Microsoft Fabric, dbt, Spark SQL, PySpark, Pandas, Terraform, Bicep, Purview, Delta, SSIS, SSAS, SSRS","data analytics, business intelligence, data warehousing, data modeling, semantic modeling, star schema construction, cloud data analytics, data ingestion, data visualization, streaming processes, api integration, automation, rpa, devops, dataops, mlops, sql, c, python, java, scala, power bi, azure analysis services, git, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, uipath, alteryx, computer vision, net, qlik, tableau, microsoft fabric, dbt, spark sql, pyspark, pandas, terraform, bicep, purview, delta, ssis, ssas, ssrs","ai algorithms, alteryx, api integration, automation, azure analysis services, batch data ingestion, bicep, business intelligence, c, cloud data analytics, computer vision, data ingestion, data lake medallion architecture, dataanalytics, datamodeling, dataops, datawarehouse, dbt, delta, devops, git, java, linux, machine learning, microsoft fabric, mlops, net, pandas, powerbi, purview, python, qlik, rpa, scala, semantic modeling, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, streaming data ingestion, streaming processes, tableau, terraform, uipath, visualization"
Data Engineer Consultant,"Infinite Resource Solutions, LLC","Orlando, FL",https://www.linkedin.com/jobs/view/data-engineer-consultant-at-infinite-resource-solutions-llc-3748762775,2023-12-17,Avondale,United States,Mid senior,Hybrid,"About The Job
Clients Education Analytics practice is looking for an experienced data engineer with the ability to own the data lifecycle from sourcing through consumption.
Our team members are dynamic and able to wear a lot of hats, and this person will need to leverage their breadth of experience within data analysis and engineering. This role will split time between our core product development and consulting projects, where we develop custom data solutions for both K-12 and higher-ed institutions.
You Will
Source complex data, manage and prepare for consumption into PowerBI visualizations for K-12 & Higher-Ed data products, including early warnings, live progress monitoring, graduation & school-grade predictors
Be comfortable across the data lifecycle and be able to jump into tasks related to front end prototyping and dashboard development as needed
Contribute to data architecture and strategy for our core education analytics product development.
Build for the scalability of a turnkey solution.
Engineer custom data solutions for specific consulting clients, including contributing to data architecture and management.
Work with cross-functional client stakeholders to gather, analyze, and understand the user requirements
Act as a liaison to the client stakeholders for the models, analysis or reports created; understand the impact of specific drivers and provide insights.
Continually refine models and/or reports and analyze results.
Utilize a core management consulting skill set including business analysis, project management, and process analysis.
Work with managing director(s) to successfully deliver to clients
Oversee and direct the work of 2-3 junior team members assisting with troubleshooting of data problems and client requests
You Have
2-3 years of data engineering experience, preferably with consulting experience OR in an education institution
Exposure to a variety of areas of data & analytics, including data management, statistical programming, and visualization
Experience with large enterprise level data environments and working with 1B+ record data sets
Skills with the Azure Tech Stack, including: Data Lake, Data Factory, Data Bricks or Analytics, and Azure Machine Learning
Reporting & Dashboard development experience with PowerBI SSRS Reports Migration to Paginated Reports
Understanding of management consulting functional skills including: business analysis, process improvement, solutions architecture, and project management
Extras we are looking for
Experience in Education Analytics, either in a research or professional role
Excellent written and verbal communication skills
Knowledgeable in back-testing, simulation, and statistical techniques (auto-regression, auto-correlation, and Principal Component Analysis)
Bachelors degree in a quantitative major; Graduate degree in data & analytics field preferred
Show more
Show less","Data Engineering, Data Lifecycle Management, Data Analysis, Data Visualization, Data Architecture, Scalability, Data Management, Statistical Programming, Azure Tech Stack, PowerBI, Data Lake, Data Factory, Data Bricks, Azure Machine Learning, Reporting, Dashboard Development, Business Analysis, Project Management, Process Analysis, Backtesting, Simulation, Statistical Techniques, Autoregression, Autocorrelation, Principal Component Analysis","data engineering, data lifecycle management, data analysis, data visualization, data architecture, scalability, data management, statistical programming, azure tech stack, powerbi, data lake, data factory, data bricks, azure machine learning, reporting, dashboard development, business analysis, project management, process analysis, backtesting, simulation, statistical techniques, autoregression, autocorrelation, principal component analysis","autocorrelation, autoregression, azure machine learning, azure tech stack, backtesting, business analysis, dashboard development, data architecture, data bricks, data engineering, data factory, data lake, data lifecycle management, data management, dataanalytics, powerbi, principal component analysis, process analysis, project management, reporting, scalability, simulation, statistical programming, statistical techniques, visualization"
Data Engineer 2,Daikin Comfort,"Waller, TX",https://www.linkedin.com/jobs/view/data-engineer-2-at-daikin-comfort-3781959618,2023-12-17,San Felipe,United States,Mid senior,Onsite,"The Opportunity
Daikin Comfort Technologies Manufacturing, L.P. is seeking a skilled individual for our Data Engineer 2 position at our DTTP - Waller, TX location. The Data Engineer is responsible to put in place the framework for a Modern, Simple, Accurate and Secure Data Environment that connects data across the company and sets data up as an asset to the company. Data Engineers will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Data engineers implement methods to improve data reliability and quality If you meet the qualifications listed below, then we invite you to apply for our open position by visiting our website at http//careers.daikincomfort.com and submit your resume.
About DTTP
Daikin Texas Technology Park- has a footprint of 4.23 million square feet under a single roof, and is the third largest factory in the United States. Opened in 2017 as the manufacturing, logistics, and engineering center for Daikin's American subsidiary Goodman, the plant makes heating and air conditioning products sold under the Goodman, Amana, and Daikin brands.
Why work with us?
>
Benefits are effective on day one for all full-time direct hires
>
Training programs are available to help guide team members and develop new skills
>
Growth Opportunities - there are immense opportunities to grow your career
>
You will be part of a Global Company - our family brands are backed by Daikin Industries, Ltd.
May include
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies
Work with stakeholders including the Executive, Manufacturing, Sales and Marketing teams to assist with data-related technical issues and support their data infrastructure needs
Work with data and analytics experts to strive for greater functionality in our data systems
Develop ways to improve data quality, reliability, and efficiency
Perform additional projects/duties to support ongoing business needs
Nature & Scope
Works within knowledge
Receives general objectives from the supervisor, consisting of work assignments, goals desired, and recommends sources of information that may assist in task completion
Works under the general supervision of the Director of IT receiving instructions as to the nature and scope of assignments and objectives to be achieved and receives advice on unusual or complex problems which deviate from basic policy
Knowledge & Skills
Working knowledge of programming languages and applications & database apps and toolsAbility to apply good judgement, decision making skills including strong work ethics & integrity on the job
Ability to work independently on multiple tasks and projects, with various teams including Engineering, Sales, IT, Finance, Marketing, Manufacturing, Logistics, etc.
Solid collaboration abilities; professional & diplomatic team builder
Effective organizational & time management skills including prioritization
Effective written & verbal communication skills
Demonstrated analytical, quantitative & creative problem solving skills
Experience or specialization within Data Management / Data Science preferred but not necessary
Ability to apply good judgement, decision making skills including strong work ethics & integrity on the job
Experience
3+ years
Education
Bachelor’s degree in Engineering, Data Science, Computer Science or may consider equivalent & relevant work experience with formal training and certifications
Physical Requirements/Work Environment
Must be able to perform essential responsibilities with or without reasonable accommodations
Qualified Applicants must be legally authorized for employment in the United States. Qualified applicants will not require employer sponsored work authorization now or in the future for employment in the United States.
The Company provides equal employment opportunity to all employees and applicants regardless of a person’s race, color, religion (including religious dress or grooming practices), creed, national origin (including language use restrictions), citizenship, uniform service member or veteran status, ancestry, disability, physical or mental disability (including HIV/AIDS), medical condition (including cancer and genetic characteristics), genetic information, request for protected leave, marital status, sex, pregnancy, age (over 40), sexual orientation, gender, gender identity or expression, political affiliation, or any other characteristic protected by law. The Company will comply with all federal and state regulations and statutes pertaining to individuals with disabilities.
Show more
Show less","Data Engineering, SQL, AWS, Big Data, Data Quality, Data Extraction, Data Transformation, Data Loading, IT, Programming Languages, Database Applications & Tools, Decision Making, Problem Solving, Communication Skills, Collaboration, Data Science, Data Management, Analytics","data engineering, sql, aws, big data, data quality, data extraction, data transformation, data loading, it, programming languages, database applications tools, decision making, problem solving, communication skills, collaboration, data science, data management, analytics","analytics, aws, big data, collaboration, communication skills, data engineering, data extraction, data loading, data management, data quality, data science, data transformation, database applications tools, decision making, it, problem solving, programming languages, sql"
sUAS Data Analyst,"Percheron, LLC","Katy, TX",https://www.linkedin.com/jobs/view/suas-data-analyst-at-percheron-llc-3516746700,2023-12-17,San Felipe,United States,Mid senior,Onsite,"The sUAS Data Processor/Analyst is responsible for the processing and analysis of remotely sensed data, including but not limited to LiDAR and aerial photography from all types of sensors or payload which could include terrestrial and/or static platforms. The Data Analyst is responsible for the Quality Assurance/Quality Control of post-acquisition Photogrammetry and LiDAR data in preparation for final product generation. The selected candidate will be primarily processing, validating, and manipulating low altitude LiDAR and digital aerial photography, extracting topographic and spatial information from point clouds, creating bare earth digital terrain models, preparing 3D visualizations and topographic and planimetric mapping.
Able to prepare and review aerial LiDAR point cloud datasets
Manually clean/classify/filter aerial LiDAR point cloud data
Able to prepare and review aerial Photogrammetric maps and datasets
Ensure all aerial data is free of gross errors or anomalies
Ensuring data meets required accuracy specifications and feature specifications
Feature coding – 2D / 3D feature coding of collected Photogrammetry and LiDAR datasets
Produce final deliverables in various formats
Serve as a primary resource for LiDAR data processing on topographic and utility surveys
Process and QC aircraft trajectories using POSPac MMS
Process and QC calibrated LiDAR point clouds using Optech's LiDAR Mapping Suite
Classify LiDAR data using Terrascan to meet customer requirements
Utilize Autodesk Civil3D software to prepare initial project boundaries and final map deliverables
Utilize GIS software to convert data, attribute features and produce maps
Issue all project deliverables in a timely manner in order to support administrative, procurement and construction activities
Communicate and coordinate with internal and external departments to achieve resolution of project-related activities
Provide internal daily status reports of activities
Other special duties as assigned
Minimum two years UAS related work experience (Required)
Minimum of one year of experience LiDAR data classification and extraction (Required)
Post-secondary educational in Land Surveying, Geography, GIS, Geomatics or other related discipline, or equivalent work experience (Required)
Bachelor’s degree is a plus, but not required
Knowledge of standard surveying and mapping practices (Required)
sUAS part 107 (Required)
Local candidates strongly preferred
Skilled use of Microsoft Office Software, Adobe Suite, MicroStation/Terrascan, Pix4D, AutoCAD Civil3D, ArcGIS, Google Earth, Global Mapper, and other surveying and mapping software a plus
Serve as a primary resource for LiDAR data processing on topographic surveys
Process and QC aircraft trajectories using Spatial Explorer and/or PosPAC
Process and QC calibrated LiDAR point clouds using
Classify LiDAR data using TerraSolid to meet company and customer requirements
Utilize Autodesk Civil3D/ArcMap/Global Mapper software to prepare initial project boundaries and final map deliverables
Provide internal daily status reports of activities
Ability to organize daily and weekly plans
Exceptional data management skills
Able to work in a team environment
Excellent communication both written and verbal
High level of computer literacy
Obligation to produce high quality work
Communicate and coordinate with internal and external departments to achieve resolution of project-related activities
Adapt to multiple production task – cross trainable
Able to work effectively with repetitive, computer-based tasks
Good organizational skills
Positive attitude and willing to expand their knowledge in the Geomatics field
Meets expectations for attendance and punctuality
Must be able to work independently
Valid driver's license
Willingness to embrace and learn new technologies
Show more
Show less","LiDAR, Photogrammetry, Aerial photography, Quality Assurance, Quality Control, Point cloud, Digital terrain models, 3D visualizations, Topographic mapping, Planimetric mapping, POSPac MMS, Optech's LiDAR Mapping Suite, Autodesk Civil3D, GIS software, Microsoft Office Suite, Adobe Suite, MicroStation/Terrascan, Pix4D, ArcGIS, Google Earth, Global Mapper, TerraSolid, Spatial Explorer, Crosstraining, Data management, Teamwork, Communication, Computer literacy, Problemsolving, Attendance, Punctuality, Independence, Driver's license, Willingness to learn","lidar, photogrammetry, aerial photography, quality assurance, quality control, point cloud, digital terrain models, 3d visualizations, topographic mapping, planimetric mapping, pospac mms, optechs lidar mapping suite, autodesk civil3d, gis software, microsoft office suite, adobe suite, microstationterrascan, pix4d, arcgis, google earth, global mapper, terrasolid, spatial explorer, crosstraining, data management, teamwork, communication, computer literacy, problemsolving, attendance, punctuality, independence, drivers license, willingness to learn","3d visualizations, adobe suite, aerial photography, arcgis, attendance, autodesk civil3d, communication, computer literacy, crosstraining, data management, digital terrain models, drivers license, gis software, global mapper, google earth, independence, lidar, microsoft office suite, microstationterrascan, optechs lidar mapping suite, photogrammetry, pix4d, planimetric mapping, point cloud, pospac mms, problemsolving, punctuality, quality assurance, quality control, spatial explorer, teamwork, terrasolid, topographic mapping, willingness to learn"
Database Engineer III - TTO 0185-03-33,Quevera,"Laurel, MD",https://www.linkedin.com/jobs/view/database-engineer-iii-tto-0185-03-33-at-quevera-3757579452,2023-12-17,Washington,United States,Mid senior,Hybrid,"Job Description
Quevera is seeking a Database Engineer III to join an exciting, collaborative and innovative team. A place where you are positioned for More than Just a Job. Where leadership partners with you, seek to cultivate and support career development, encouraging growth from within while striving to foster a diverse and inclusive environment that improves individual and organizational performance.
Highlights Of Working For Quevera Are
Quevera employees voted Quevera as a TOP EMPLOYER in the Baltimore /DC area for 2020 (ranked #8 out of 150 companies) and 2022 (ranked #5 out of 150 companies).
Yearly $5,000 towards education/training.
Employees are in control of their career path through our Career Pathway Program.
Family and corporate events
Excellent health care coverage (100% paid premium option) and 401K matching (up to 4%).
And many more!
Q-Culture Video
Q-Careers
Duties And Responsibilities
Collaborate with experienced developers and engineers to collectively enhance the organization's data engineering expertise.
Actively participate in agile development practices to align data engineering projects with evolving business needs.
Utilize the latest industry standards and techniques to design, develop, and implement data analytics solutions.
Employ cutting-edge technologies and machine learning to automate data analysis and enhance business intelligence capabilities.
Work on projects that aim to answer critical business questions and perform in-depth business analysis.
Identify and deploy innovative approaches to automate data processing and analysis, improving efficiency and accuracy.
Play a pivotal role in revolutionizing how the organization manages and acquires business intelligence data.
Collaborate closely with experienced developers and system engineers to create software solutions that automate and optimize existing processes, enhancing workforce effectiveness.
Required Experience
Experience closely collaborating and communicating with the customer
Experience developing and updating technical documentation
Familiarity with other languages, such as Java or JavaScript
Solid understanding of software development principles, including version control (e.g., Git), code review processes, and software development methodologies like Agile or Scrum.
A bachelor’s degree and twelve (12) years of software development/engineering. Sixteen (16) years of experience may be substituted in lieu of a degree.
Strong data engineering and analytics experience.
Excellent communication and teamwork skills.
Ability to adapt in a fast-paced, collaborative environment.
Quevera is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age or any other characteristic protected by law.
Show more
Show less","Agile, Java, JavaScript, Git, Version control, Software development methodologies, Data engineering, Data analytics, Machine learning, Business intelligence, SQL, NoSQL, Hadoop, Spark, Cassandra, MongoDB, Tableau, Power BI","agile, java, javascript, git, version control, software development methodologies, data engineering, data analytics, machine learning, business intelligence, sql, nosql, hadoop, spark, cassandra, mongodb, tableau, power bi","agile, business intelligence, cassandra, data engineering, dataanalytics, git, hadoop, java, javascript, machine learning, mongodb, nosql, powerbi, software development methodologies, spark, sql, tableau, version control"
Associate Data Engineer - ETL Developer (Hybrid) with Security Clearance,ClearanceJobs,"Fort Meade, MD",https://www.linkedin.com/jobs/view/associate-data-engineer-etl-developer-hybrid-with-security-clearance-at-clearancejobs-3783029410,2023-12-17,Washington,United States,Mid senior,Hybrid,"ASRC Federal is seeking an Associate Data Engineer (ETL Developer) to join our team in anticipation of upcoming work in the National Capital Region. The successful candidate will design, develop, and optimize ETL processes, specializing in the migration of data from legacy systems, ensuring efficient and accurate data transfer and seamless integration. The program provides data management services including extraction, transformation, loading (ETL) and migration of legacy data; data governance, and the consumption of data. Key Responsibilities:
Collaborate closely with Senior Data Engineers (ETL Developers) to leverage their expertise, seek guidance, and actively participate in knowledge-sharing sessions to contribute to the team's objectives.
Design, develop, and implement Extract, Transform, Load (ETL) processes for migrating data from legacy systems into an AWS environment, with a focus on Oracle databases.
Utilize data mapping, data mining, and data transformational analysis tools to ensure efficient and accurate data transfer.
Design, build, and sustain database management systems for the purpose of ETL, emphasizing database modeling, design, and architecture.
Implement metadata and repository creation and configuration or seamless ETL operations.
Collaborate with cross-functional teams to ensure alignment with project objectives.
Implement best practices for ETL processes within an AWS environment.
Conduct thorough analysis and testing to ensure data integrity, accuracy, and consistency throughout the ETL process.
Leverage experience in Oracle databases to ensure compataibility, performance, and security in ETL workflows.
Create and maintain detailed documentation for ETL processes, database configurations, and system requirements. ASRC Federal Advantages
Learning and Development: After 90 days of employment, regular full-time employees are eligible to participate in our professional development program including funds annually to go towards Associate's, Bachelor's or Graduate Degrees; Industry standard professional certification; A professional certificate program; Continuing education classes; and Registration fees to attend professional conferences.
Employee Resource Groups: That provide our employees the opportunity to collaborate and network with colleges with common interests, backgrounds, and experiences including Women's Impact Network (WIN), Multicultural ERG, Military Community (MILCOM), and Pride ERG for LGBTQ+ employees and allies.
Purpose Driven Careers: Certified Great Place to Work™; Certified Military Times' 'Best for Vets' and Military.com 'Top 25 Veteran Employer.'
Benefits: Comprehensive insurance packages including medical, dental, vision, life insurance, and short term/long term disability, as well as a 401K with generous company match and immediate vesting. Qualifications
Active Top Secret Clearance.
Bachelor's degree in related field and 2+ years of relevant experience.
Hands on experience with AWS services (such as AWS Glue) and Oracle databases in an ETL context.
Experience with large data sources (5-9 TB).
Knowledge of data modeling, design, and relational database architecture.
Proficient in data mapping, data mining, and data transformation tools.
Show more
Show less","ETL, Data migration, Data governance, Data management, Oracle databases, AWS environment, AWS Glue, Data mapping, Data mining, Data transformational analysis, Data integrity, Data accuracy, Data consistency, Metadata, Repository creation, System requirements, Professional development, Employee resource groups, Certified Great Place to Work™, Certified Military Times' 'Best for Vets', Military.com 'Top 25 Veteran Employer.', Top Secret Clearance, Bachelor's degree, AWS services, Relational database architecture, Data modeling, Data design","etl, data migration, data governance, data management, oracle databases, aws environment, aws glue, data mapping, data mining, data transformational analysis, data integrity, data accuracy, data consistency, metadata, repository creation, system requirements, professional development, employee resource groups, certified great place to work, certified military times best for vets, militarycom top 25 veteran employer, top secret clearance, bachelors degree, aws services, relational database architecture, data modeling, data design","aws environment, aws glue, aws services, bachelors degree, certified great place to work, certified military times best for vets, data accuracy, data consistency, data design, data governance, data integrity, data management, data mapping, data migration, data mining, data transformational analysis, datamodeling, employee resource groups, etl, metadata, militarycom top 25 veteran employer, oracle databases, professional development, relational database architecture, repository creation, system requirements, top secret clearance"
Data analyst,Stellar Professionals,"Richmond, VA",https://www.linkedin.com/jobs/view/data-analyst-at-stellar-professionals-3690575844,2023-12-17,Montpelier,United States,Mid senior,Onsite,"Applicant must have 4 years of relevant experience with the following:
Experience in data analytics
Understanding how to store, manage, & retrieve large-scale data sets from multiple systems for analysis or operations
Experience working with varied, complex data sets, including statistics, surveys, sales, and operational data
Proficiency in data analysis and visualization tools
Ability to communicate data insights to both technical and non-technical audiences
Knowledge and experience in government operations and utilities preferred
Show more
Show less","Data Analytics, Data Storage, Data Management, Data Retrieval, Data Analysis, Data Visualization, Statistics, Surveys, Sales Data, Operational Data, Government Operations, Utilities","data analytics, data storage, data management, data retrieval, data analysis, data visualization, statistics, surveys, sales data, operational data, government operations, utilities","data management, data retrieval, data storage, dataanalytics, government operations, operational data, sales data, statistics, surveys, utilities, visualization"
Data analyst,Stellar Professionals,"Richmond, VA",https://www.linkedin.com/jobs/view/data-analyst-at-stellar-professionals-3690578253,2023-12-17,Montpelier,United States,Mid senior,Onsite,"Applicant must have 4 years of relevant experience with the following:
Experience in data analytics
Understanding how to store, manage, & retrieve large-scale data sets from multiple systems for analysis and reporting
Strong analytical and problem-solving skills, with the ability to analyze complex data sets and derive meaningful insights
Ability to communicate data insights to both technical and non-technical audiences
Knowledge and experience in government operations and utilities preferred
Show more
Show less","data analytics, data storage, data management, retrieving data, problem solving, analytical skills, communication, government operations, utilities","data analytics, data storage, data management, retrieving data, problem solving, analytical skills, communication, government operations, utilities","analytical skills, communication, data management, data storage, dataanalytics, government operations, problem solving, retrieving data, utilities"
Senior Data Engineer,Professional Diversity Network,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-professional-diversity-network-3784625260,2023-12-17,Montpelier,United States,Mid senior,Onsite,"West Creek 6 (12076), United States of America, Richmond, Virginia
Senior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking
Data Engineers
who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you'll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You'll Do
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications
Bachelor's Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.
No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City's Fair Chance Act; Philadelphia's Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
PDN-99c424f6-d3bd-4fba-9a47-0b6329fa8db8
Show more
Show less","Java, Scala, Python, Open Source RDBMS, NoSQL databases, Redshift, Snowflake, Agile, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Mongo, Cassandra, UNIX/Linux, Shell scripting","java, scala, python, open source rdbms, nosql databases, redshift, snowflake, agile, mapreduce, hadoop, hive, emr, kafka, spark, gurobi, mysql, mongo, cassandra, unixlinux, shell scripting","agile, cassandra, emr, gurobi, hadoop, hive, java, kafka, mapreduce, mongo, mysql, nosql databases, open source rdbms, python, redshift, scala, shell scripting, snowflake, spark, unixlinux"
Senior Data Engineer (Python),Capital One,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-python-at-capital-one-3783101357,2023-12-17,Montpelier,United States,Mid senior,Onsite,"Center 1 (19052), United States of America, McLean, VirginiaSenior Data Engineer (Python)
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking
Data Engineers
who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
San Francisco, California (Hybrid On-Site): $171,500 - $195,800 for Senior Data Engineer
Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.
This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Scala, Java, Python, RDBMS, NoSQL, Hadoop, Hive, EMR, Spark, MySQL, Mongo, Cassandra, Redshift, Snowflake, UNIX/Linux, Agile","scala, java, python, rdbms, nosql, hadoop, hive, emr, spark, mysql, mongo, cassandra, redshift, snowflake, unixlinux, agile","agile, cassandra, emr, hadoop, hive, java, mongo, mysql, nosql, python, rdbms, redshift, scala, snowflake, spark, unixlinux"
Data analyst(Power BI),Stellar Professionals,"Richmond, VA",https://www.linkedin.com/jobs/view/data-analyst-power-bi-at-stellar-professionals-3625020578,2023-12-17,Montpelier,United States,Mid senior,Onsite,"We are looking to fill a long term Contract role as Data analyst in Richmond, VA
Mode of Interview: In person
Selected candidates must work onsite 3 days per week
Applicants Must Have The Following Required Skills
5+ years' in analytical position
3+ years' using Power BI (other leading BI applications will be considered)
3+ years' of database design and work with unstructured data in a data lake environment
Strong knowledge of data collection software and protocol
Advanced Microsoft Excel proficiency Advanced Formulas, Pivot Tables & Macros
Ability to structure business discussions and scope solutions
Excellent analytical and forecasting ability
Understand current data protection and privacy laws
Ability to think through problems and provide recommendations to a solution
Demonstrated ability to manage multiple complex projects in parallel
Proven ability to develop and manage BI solutions using Power BI
Strong knowledge of T-SQL and DAX to support reports, dashboards, and data models
Demonstrated experience using Blue Prism or a similar tool to automate business operations and data analysis and reduce manual labor
Exceptional data visualization experience
Ability to analyze and mine data from multiple systems.
Ability to understand user's needs and devise solutions accordingly.
In-depth knowledge of the data development life cycle
Excellent documentation skills.
Experience creating detailed reports and giving presentations.
Competency in Microsoft applications including Word, Excel, and Power Point
Advanced knowledge of Power BI, DAX, T-SQL, and Blue Prism.
Show more
Show less","Data Analysis, Power BI, Database Design, Data Lake, Data Collection Software, Microsoft Excel, Business Discussions, Analytical and Forecasting, Data Protection and Privacy, Problem Solving, Project Management, BI Solutions, TSQL, DAX, Blue Prism, Business Operations Automation, Data Visualization, Data Mining, User Needs Analysis, Data Development Life Cycle, Documentation, Reporting, Presentations, Microsoft Applications","data analysis, power bi, database design, data lake, data collection software, microsoft excel, business discussions, analytical and forecasting, data protection and privacy, problem solving, project management, bi solutions, tsql, dax, blue prism, business operations automation, data visualization, data mining, user needs analysis, data development life cycle, documentation, reporting, presentations, microsoft applications","analytical and forecasting, bi solutions, blue prism, business discussions, business operations automation, data collection software, data development life cycle, data lake, data mining, data protection and privacy, dataanalytics, database design, dax, documentation, microsoft applications, microsoft excel, powerbi, presentations, problem solving, project management, reporting, tsql, user needs analysis, visualization"
"Distinguished Engineer, Enterprise Data Platforms - Data Creation",Jobs for Humanity,"Richmond, VA",https://www.linkedin.com/jobs/view/distinguished-engineer-enterprise-data-platforms-data-creation-at-jobs-for-humanity-3768296404,2023-12-17,Montpelier,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with Capital One to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Capital One
Job Description
[Your Name]
[Your Address]
[City, State, ZIP]
[Email Address]
[Phone Number]
[Date]
Re: Job Opportunity - Distinguished Engineer, Enterprise Data Platforms - Data Creation
Dear Hiring Manager,
I am writing to express my interest in the position of Distinguished Engineer, Enterprise Data Platforms - Data Creation at Capital One. I believe that my skills and experience make me a strong candidate for this role.
As part of Capital One's Enterprise Data & Machine Learning group, you will have the opportunity to join my teams in building a highly scalable and well-governed data ecosystem. Our goal is to create a platform that can handle large amounts of data, spanning across thousands of AWS accounts, tens of thousands of database instances, and millions of files stored across the enterprise. Additionally, you will play a crucial role in shaping the future of the platform that enables the exchange of terabytes of data between Capital One and our partners on a daily basis.
At Capital One, we value diversity of thought and believe that it strengthens our ability to collaborate, influence, and provide innovative solutions. As a Distinguished Engineer, your contributions will have a significant impact on our trajectory. You will be responsible for devising clear roadmaps and delivering next-generation technology solutions.
Responsibilities:
Build awareness and drive adoption of modern technologies, sharing their benefits to gain buy-in from stakeholders
Encourage an inclusive environment where diverse ideas are heard and championed
Promote a culture of engineering excellence, reusing solutions and collaborating with the broader Capital One team
Effectively communicate and influence key stakeholders at all levels of the organization
Provide expertise and guidance in a specific technology, platform, or capability domain
Mentor and coach internal talent, while actively recruiting external talent to strengthen Capital One's Tech community
Basic Qualifications:
Bachelor’s Degree
At least 7 years of Software Architecture or Enterprise Architecture experience
At least 5 years of experience in software architecture and design patterns
At least 5 years of AWS cloud experience
Preferred Qualifications:
Masters' Degree
Experience with developing strategy and implementing target architectures
AWS Solution Architect - Professional or AWS Certified Data Analytics - Specialty certification
8+ years of data governance, data access, data lineage, data monitoring, and security controls experience
7+ years of experience developing in Python, Java, Scala, or Node
3+ years of experience in modern database technology evaluation and data modeling
3+ years of experience building data products and implementing enterprise-level data governance
3+ years of experience in building highly resilient distributed data systems
3+ years of experience in data engineering, including distributed data pipelines and test data engineering
3+ years of experience dealing with a large number of cross AWS account communications
3+ years of experience in Agile practices
Capital One offers a comprehensive benefits package to support your total well-being. To learn more about the available benefits, please visit the Capital One Careers website at [insert link]. Eligibility for benefits may vary based on full or part-time status, exempt or non-exempt status, and management level.
Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. We value all qualified applicants and will consider them for employment without regard to sex, race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other prohibited basis under applicable law. We promote a drug-free workplace.
If you require any accommodations during the application process, please reach out to Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. Any information you provide will be kept confidential and used only to provide the accommodations you need.
For technical support or questions about the recruiting process at Capital One, please email Careers@capitalone.com.
Thank you for considering my application. I look forward to the opportunity to contribute to Capital One's mission and work alongside your diverse and talented teams.
Sincerely,
[Your Name]
Show more
Show less","AWS, AWS Solution Architect, AWS Certified Data Analytics, Python, Java, Scala, Node, Data governance, Data access, Data lineage, Data modeling, Database technology, Data products, Data engineering, Data pipelines, Agile","aws, aws solution architect, aws certified data analytics, python, java, scala, node, data governance, data access, data lineage, data modeling, database technology, data products, data engineering, data pipelines, agile","agile, aws, aws certified data analytics, aws solution architect, data access, data engineering, data governance, data lineage, data products, database technology, datamodeling, datapipeline, java, node, python, scala"
RIC - Data Analyst 2,NextRow Digital,"Richmond, VA",https://www.linkedin.com/jobs/view/ric-data-analyst-2-at-nextrow-digital-3690520260,2023-12-17,Montpelier,United States,Mid senior,Onsite,"Data Analyst
Richmond, VA
ON-SITE - Candidates will be expected to work ON-SITE 5 days per week
Primary Duties: Data Analyst 2
Provide ad-hoc reports
Develop reporting cadence and provide performance reports at the individual and team level for all operating areas
Collect data and examine it to spot trends and glean information that can be used to make business decisions Demonstrate ability to be a trusted & respected business partner to all levels in the organization, and the ability to translate data into clear, actionable next steps for internal teams
Collaborate with Data Scientist and other team members as needed
Qualifications
2 years' experience in data analytics
Working knowledge of data engineering principles: understanding how to store, manage, & retrieve large-scale data sets from multiple systems for analysis and reporting
Strong analytical and problem-solving skills, with the ability to analyze complex data sets and derive meaningful insights A higher level of computer proficiency
Advanced proficiency in Microsoft Office, specifically Excel
Ability to communicate data insights to both technical and non-technical audiences
Knowledge and experience in government operations and utilities preferred
Required/Desired Skills
Skill Required /Desired Amount of Experience Experience in data analytics Required 2 Years Understanding how to store, manage, & retrieve large-scale data sets from multiple systems for analysis and reporting Required 2 Years Strong analytical and problem-solving skills, with the ability to analyze complex data sets and derive meaningful insights Required 2 Years Ability to communicate data insights to both technical and non-technical audiences Required 2 Years Knowledge and experience in government operations and utilities preferred Desired 2 Years
Show more
Show less","Data Analytics, Data Engineering, Microsoft Office, Excel, Communication, ProblemSolving, Data Analysis, Data Visualization, Government Operations, Utilities","data analytics, data engineering, microsoft office, excel, communication, problemsolving, data analysis, data visualization, government operations, utilities","communication, data engineering, dataanalytics, excel, government operations, microsoft office, problemsolving, utilities, visualization"
Senior Data Engineer,Capital One,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3784234102,2023-12-17,Montpelier,United States,Mid senior,Onsite,"Center 3 (19075), United States of America, McLean, VirginiaSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Python, SQL, Scala, and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, or Scala
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Agile, Apache Hadoop, Apache Hive, Apache Kafka, Apache Spark, AWS, Cassandra, Cloud Computing, DevOps, EMR, Gurobi, Linux, MapReduce, Microsoft Azure, Mongo, MySQL, NoSQL, Open Source RDBMS, Python, Redshift, Scala, Snowflake, SQL, Unix","agile, apache hadoop, apache hive, apache kafka, apache spark, aws, cassandra, cloud computing, devops, emr, gurobi, linux, mapreduce, microsoft azure, mongo, mysql, nosql, open source rdbms, python, redshift, scala, snowflake, sql, unix","agile, apache hadoop, apache hive, apache kafka, apache spark, aws, cassandra, cloud computing, devops, emr, gurobi, linux, mapreduce, microsoft azure, mongo, mysql, nosql, open source rdbms, python, redshift, scala, snowflake, sql, unix"
Senior Data Engineer,Capital One,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3774773865,2023-12-17,Montpelier,United States,Mid senior,Onsite,"Center 1 (19052), United States of America, McLean, VirginiaSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking a
Senior Data Engineer
who is passionate about marrying data with emerging technologies. As a Capital One Senior Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One's Enterprise Data Team.
What You’ll Do
Proactively seeks out opportunities to address customer needs and influences stakeholders so that we are building the best solutions for the most important problems
Support the design and development of scalable data architectures and systems that extract, store, and process large amounts of data
Build and optimize data pipelines for efficient data ingestion, transformation, and loading from various sources while ensuring data quality and integrity
Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts and/or Product Owners to understand their requirements and provide efficient solutions for data exploration, analysis, and modeling
Implement testing, validation and pipeline observability to ensure data pipelines are meeting customer SLAs
Use cutting edge technologies to develop modern data pipelines supporting Machine Learning and Artificial Intelligence
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python or SQL
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka or Spark)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Data Engineering, Data Architecture, Data Pipelines, Machine Learning, Artificial Intelligence, Python, SQL, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Redshift, Snowflake, UNIX/Linux, Agile","data engineering, data architecture, data pipelines, machine learning, artificial intelligence, python, sql, aws, microsoft azure, google cloud, mapreduce, hadoop, hive, emr, kafka, spark, redshift, snowflake, unixlinux, agile","agile, artificial intelligence, aws, data architecture, data engineering, datapipeline, emr, google cloud, hadoop, hive, kafka, machine learning, mapreduce, microsoft azure, python, redshift, snowflake, spark, sql, unixlinux"
Senior Data Engineer,Capital One,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3780759049,2023-12-17,Montpelier,United States,Mid senior,Onsite,"Center 1 (19052), United States of America, McLean, VirginiaSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking
Data Engineers
who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Scala, Python, Open Source RDBMS, NoSQL databases, Redshift, Snowflake, Cloudbased data warehousing services, Agile, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Realtime data, Streaming applications, Mongo, Cassandra, Linux, Shell scripting","java, scala, python, open source rdbms, nosql databases, redshift, snowflake, cloudbased data warehousing services, agile, mapreduce, hadoop, hive, emr, kafka, spark, gurobi, mysql, realtime data, streaming applications, mongo, cassandra, linux, shell scripting","agile, cassandra, cloudbased data warehousing services, emr, gurobi, hadoop, hive, java, kafka, linux, mapreduce, mongo, mysql, nosql databases, open source rdbms, python, realtime data, redshift, scala, shell scripting, snowflake, spark, streaming applications"
Senior Data Engineer,Capital One,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3783173789,2023-12-17,Montpelier,United States,Mid senior,Onsite,"West Creek 6 (12076), United States of America, Richmond, VirginiaSenior Data Engineer
Senior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, PySpark, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Agile, AWS, Cassandra, Cloud, EMR, Gurobi, Hadoop, Hive, Java, Kafka, Linux, MapReduce, Microsoft Azure, Mongo, MySQL, Open Source RDBMS, Python, PySpark, Redshift, Scala, Snowflake, Spark, SQL, UNIX","agile, aws, cassandra, cloud, emr, gurobi, hadoop, hive, java, kafka, linux, mapreduce, microsoft azure, mongo, mysql, open source rdbms, python, pyspark, redshift, scala, snowflake, spark, sql, unix","agile, aws, cassandra, cloud, emr, gurobi, hadoop, hive, java, kafka, linux, mapreduce, microsoft azure, mongo, mysql, open source rdbms, python, redshift, scala, snowflake, spark, sql, unix"
Senior Data Engineer,Capital One,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3774778278,2023-12-17,Montpelier,United States,Mid senior,Onsite,"West Creek 6 (12076), United States of America, Richmond, VirginiaSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking
Data Engineers
who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Scala, Python, RDBMS, NoSQL, Redshift, Snowflake, Cloud data warehousing, Machine learning, Microservices, Distributed systems, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, UNIX/Linux, Shell scripting, Agile","java, scala, python, rdbms, nosql, redshift, snowflake, cloud data warehousing, machine learning, microservices, distributed systems, mapreduce, hadoop, hive, emr, kafka, spark, gurobi, mysql, unixlinux, shell scripting, agile","agile, cloud data warehousing, distributed systems, emr, gurobi, hadoop, hive, java, kafka, machine learning, mapreduce, microservices, mysql, nosql, python, rdbms, redshift, scala, shell scripting, snowflake, spark, unixlinux"
Senior Data Engineer,Capital One,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3774775698,2023-12-17,Montpelier,United States,Mid senior,Onsite,"West Creek 6 (12076), United States of America, Richmond, VirginiaSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Scala, Python, NoSQL, Redshift, Snowflake, Data warehousing, Agile engineering practices, Open Source RDBMS, Cloud based data warehousing services, Machine learning, Distributed microservices, Full stack systems, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, Mongo, Cassandra, UNIX/Linux, SQL","java, scala, python, nosql, redshift, snowflake, data warehousing, agile engineering practices, open source rdbms, cloud based data warehousing services, machine learning, distributed microservices, full stack systems, aws, microsoft azure, google cloud, mapreduce, hadoop, hive, emr, kafka, spark, gurobi, mysql, mongo, cassandra, unixlinux, sql","agile engineering practices, aws, cassandra, cloud based data warehousing services, datawarehouse, distributed microservices, emr, full stack systems, google cloud, gurobi, hadoop, hive, java, kafka, machine learning, mapreduce, microsoft azure, mongo, mysql, nosql, open source rdbms, python, redshift, scala, snowflake, spark, sql, unixlinux"
Senior Data Engineer,Capital One,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3774780031,2023-12-17,Montpelier,United States,Mid senior,Onsite,"Plano 1 (31061), United States of America, Plano, TexasSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Senior Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Scala, Python, Open Source RDBMS, NoSQL databases, Redshift, Snowflake, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, NoSQL, Mongo, Cassandra, UNIX, Linux, Agile","java, scala, python, open source rdbms, nosql databases, redshift, snowflake, aws, microsoft azure, google cloud, mapreduce, hadoop, hive, emr, kafka, spark, gurobi, mysql, nosql, mongo, cassandra, unix, linux, agile","agile, aws, cassandra, emr, google cloud, gurobi, hadoop, hive, java, kafka, linux, mapreduce, microsoft azure, mongo, mysql, nosql, nosql databases, open source rdbms, python, redshift, scala, snowflake, spark, unix"
Senior Data Engineer,Capital One,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3780759048,2023-12-17,Montpelier,United States,Mid senior,Onsite,"Plano 1 (31061), United States of America, Plano, TexasSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
New York City (Hybrid On-Site): $161,900 - $184,800 for Senior Data EngineerSan Francisco, California (Hybrid On-Site): $171,500 - $195,800 for Senior Data Engineer
Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.
This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Scala, Python, Open Source RDBMS, NoSQL databases, Redshift, Snowflake, Cloud based data warehousing services, AWS, Microsoft Azure, Google Cloud, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, UNIX, Linux, Agile engineering practices","java, scala, python, open source rdbms, nosql databases, redshift, snowflake, cloud based data warehousing services, aws, microsoft azure, google cloud, mapreduce, hadoop, hive, emr, kafka, spark, gurobi, mysql, unix, linux, agile engineering practices","agile engineering practices, aws, cloud based data warehousing services, emr, google cloud, gurobi, hadoop, hive, java, kafka, linux, mapreduce, microsoft azure, mysql, nosql databases, open source rdbms, python, redshift, scala, snowflake, spark, unix"
Senior Data Engineer,Capital One,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3774780019,2023-12-17,Montpelier,United States,Mid senior,Onsite,"Center 2 (19050), United States of America, McLean, VirginiaSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking
Data Engineers
who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Scala, Python, RDBMS, NoSQL, Redshift, Snowflake, Cloud data warehousing, Machine learning, Distributed microservices, Agile, Unit testing, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, UNIX, Linux, Shell scripting","java, scala, python, rdbms, nosql, redshift, snowflake, cloud data warehousing, machine learning, distributed microservices, agile, unit testing, hadoop, hive, emr, kafka, spark, gurobi, mysql, unix, linux, shell scripting","agile, cloud data warehousing, distributed microservices, emr, gurobi, hadoop, hive, java, kafka, linux, machine learning, mysql, nosql, python, rdbms, redshift, scala, shell scripting, snowflake, spark, unit testing, unix"
Senior Data Engineer,Capital One,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3785725463,2023-12-17,Montpelier,United States,Mid senior,Onsite,"West Creek 2 (12072), United States of America, Richmond, VirginiaSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
You'll be part of an Agile team dedicated to breaking the norm and pushing the limits of continuous improvement and innovation. You will participate in detailed technical design, development and implementation of applications using existing and emerging technology platforms. Working within an Agile environment, you will provide input into architectural design decisions, develop code to meet story acceptance criteria, and ensure that the applications we build are always available to our customers. You'll have the opportunity to mentor other engineers and develop your technical knowledge and skills to keep your mind and our business on the cutting edge of technology. At Capital One, we have seas of big data and rivers of fast data.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with experience in integrating data with CRM tools like salesforce CRM
Developing and deploying distributed computing Big Data applications using Open Source frameworks like Apache Spark, Flink, Nifi, Storm and Kafka on AWS Cloud
Work with other enterprise teams to integrate applications with API’s & other internal CapitalOne Tools.
Utilizing programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and snowflake
Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test Driven Development to enable the rapid delivery of working code utilizing tools like Jenkins, Maven, Nexus, Chef, Terraform, Ruby, Git and Docker
Performing unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
2+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
2+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Apache Spark, Flink, Nifi, Storm, Kafka, AWS Cloud, API, Java, Scala, Python, RDBMS, NoSQL, Redshift, Snowflake, Jenkins, Maven, Nexus, Chef, Terraform, Ruby, Git, Docker, Hadoop, Hive, EMR, Gurobi, MySQL, MongoDB, Cassandra, Unix/Linux, Agile, DevOps, Continuous Integration, Continuous Deployment, Test Automation, Build Automation, Test Driven Development","apache spark, flink, nifi, storm, kafka, aws cloud, api, java, scala, python, rdbms, nosql, redshift, snowflake, jenkins, maven, nexus, chef, terraform, ruby, git, docker, hadoop, hive, emr, gurobi, mysql, mongodb, cassandra, unixlinux, agile, devops, continuous integration, continuous deployment, test automation, build automation, test driven development","agile, apache spark, api, aws cloud, build automation, cassandra, chef, continuous deployment, continuous integration, devops, docker, emr, flink, git, gurobi, hadoop, hive, java, jenkins, kafka, maven, mongodb, mysql, nexus, nifi, nosql, python, rdbms, redshift, ruby, scala, snowflake, storm, terraform, test automation, test driven development, unixlinux"
HealthCare Data Analyst,Sentara Health,"Richmond, VA",https://www.linkedin.com/jobs/view/healthcare-data-analyst-at-sentara-health-3779250625,2023-12-17,Montpelier,United States,Mid senior,Onsite,"City/State
Richmond, VA
Overview
Work Shift
First (Days) (United States of America)
Be a part of an excellent healthcare organization that cares about our People, Quality, Patient Safety, Service, and Integrity. Join a team that has a mission to improve health every day and a vision to be the healthcare choice of the communities that we serve!
Sentara Health Plan is hiring for a HEALTHCARE DATA ANALYST
This is a Full-Time position, fully remote, with day shift hours and great benefits!
Work Location:
Remote opportunities available in the following states: Alabama, Delaware, Florida, Georgia, Idaho, Indiana, Kansas, Louisiana, Maine, Maryland, Minnesota, Nebraska, Nevada, New Hampshire, North Carolina, North Dakota, Ohio, Oklahoma, Pennsylvania, South Carolina, South Dakota, Tennessee, Texas, Utah, Virginia, Washington (state), West Virginia, Wisconsin, Wyoming
Job Responsibilities
Medical expense and trends
Quality including HEDIS reporting, interventions, and STAR ratings
Clinical gaps and adherence
Network Management contracting and pay-for-performance/value-based measures
State and Federal regulatory reporting
Employer groups reporting and client ad-hoc reporting related to clinical and financial services
Population health reporting & analysis
Risk, member/patient stratification, member/patient segmentation, quality, provider scoring, and cost savings
Ambulatory, acute & post-acute quality, clinical performance, and operations reporting & analysis
Quality & performance benchmarking
Research data & analytic support
Job Requirements
Required to have Bachelor's Level Degree
Required to have a minimum of 1 year of relevant experience
Required to have SQL and TableauAs an integrated health system, Sentara delivers analytics across a wide variety of domains, including payer functions, ambulatory, acute & post-acute care and support functions. The subject matter expertise required will vary based on the specific opening, but include the following:
Preferred Qualifications
Preferred to have experience in Healthcare
Benefits:
Sentara offers an attractive array of full-time benefits to include Medical, Dental, Vision, Paid Time Off, Sick, Tuition Reimbursement, a 401k/403B, 401a, Performance Plus Bonus, Career Advancement Opportunities, Work Perks, and more.
For information about our employee benefits, please visit: Benefits – Sentara www.sentaracareers.com
Sentara Health Plans
is the health insurance division of Sentara Healthcare doing business as Optima Health and Virginia Premier.
Sentara Health Plans
provides health insurance coverage through a full suite of commercial products including consumer-driven, employee-owned and employer-sponsored plans, individual and family health plans, employee assistance plans and plans serving Medicare and Medicaid enrollees.
For applicants within Washington State, the following hiring range will be applied: $73,819.20 to $87,447.36 annually.
Join our team, where we are committed to quality healthcare, improving health every day, and provide the opportunity for training, development, growth!
To apply, please go to www.sentaracareers.com and use the following as your Keyword Search:
JR-38941
#Indeed
#Dice
#Monster
Talroo–IT
Indeed
Monster
Talroo
Talroo–Health Plan
Keywords: Data Analysis, SQL, Data Analysis
Job Summary
The Healthcare Data Analyst provides data analysis support to the customer by assisting with the development of reports and/or dashboards to monitor program and operational performance. Promotes self-service analytics for customer adoption, understanding and use of data. Supports design of programmatic analyses and reporting capabilities in addition to business requirement definition for new analyses and performs ad-hoc analysis as directed. This position supports data quality and data stewardship functions to maintain data accuracy and identify new metrics. As a subject matter expert, the Healthcare Data Analyst uses analytic skills to offer data-driven conclusions and recommendations to business partners.
As an integrated health system, Sentara delivers analytics across a wide variety of domains, including payer functions, ambulatory, acute & post-acute care and support functions. The subject matter expertise required will vary based on the specific opening, but include the following:
Medical expense and trends
Quality including HEDIS reporting, interventions, and STAR ratings
Clinical gaps and adherence
Network Management contracting and pay-for-performance/value-based measures
State and Federal regulatory reporting
Employer groups reporting and client ad-hoc reporting related to clinical and financial services
Population health reporting & analysis
Risk, member/patient stratification, member/patient segmentation, quality, provider scoring, and cost savings
Ambulatory, acute & post-acute quality, clinical performance, and operations reporting & analysis
Quality & performance benchmarking
Research data & analytic support
Works directly with internal customers and external provider partners.
Knowledge of SQL and Tableau
Excellent Oral and Written Communication Skills
Problem Solving
Consultative Engagement Skills
Qualifications:
BLD - Bachelor's Level Degree (Required), MLD - Master's Level Degree
Data Analysis, Financial Analysis, Population Management, Statistics
Skills
Active Learning, Active Listening, Communication, Complex Problem Solving, Coordination, Critical Thinking, Reading Comprehension, Service Orientation, Time Management, Troubleshooting, Writing
Sentara Healthcare prides itself on the diversity and inclusiveness of its close to an almost 30,000-member workforce. Diversity, inclusion, and belonging is a guiding principle of the organization to ensure its workforce reflects the communities it serves.
Per Clinical Laboratory Improvement Amendments (CLIA), some clinical environments require proof of education; these regulations are posted at ecfr.gov for further information. In an effort to expedite this verification requirement, we encourage you to upload your diploma or transcript at time of application.
In support of our mission “to improve health every day,” this is a tobacco-free environment.
Show more
Show less","Data Analysis, Tableau, SQL, Clinical Data, Healthcare, HEDIS, STAR Ratings, Quality Measures, Population Health, Risk Stratification, Clinical Performance, Financial Analysis, Employer Groups, Client Reporting, Research Support, Communication, Complex Problem Solving, Critical Thinking, Time Management","data analysis, tableau, sql, clinical data, healthcare, hedis, star ratings, quality measures, population health, risk stratification, clinical performance, financial analysis, employer groups, client reporting, research support, communication, complex problem solving, critical thinking, time management","client reporting, clinical data, clinical performance, communication, complex problem solving, critical thinking, dataanalytics, employer groups, financial analysis, healthcare, hedis, population health, quality measures, research support, risk stratification, sql, star ratings, tableau, time management"
Senior Data Engineer,Capital One,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-capital-one-3774775718,2023-12-17,Montpelier,United States,Mid senior,Onsite,"West Creek 4 (12074), United States of America, Richmond, VirginiaSenior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
Team Info:
In Finance Tech, we are on a mission to create a Finance Ecosystem of interoperable capabilities using a common design pattern and experience adhering to the core principles, with enterprise scale in mind. We are looking for a Senior Data Engineer to help drive delivery of exciting new capabilities in the data engineering space across several teams to support our mission.
What You’ll Do:
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
Bachelor’s Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications:
5+ years of experience in application development including Python, SQL, Scala, or Java
2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of experience with NoSQL implementation (Mongo, Cassandra)
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Scala, Python, RDBMS, NoSQL, Redshift, Snowflake, Agile, Unit testing, UNIX/Linux, Hadoop, MapReduce, Hive, EMR, Kafka, Spark, Gurobi, MySQL, MongoDB, Cassandra, AWS, Azure, Google Cloud","java, scala, python, rdbms, nosql, redshift, snowflake, agile, unit testing, unixlinux, hadoop, mapreduce, hive, emr, kafka, spark, gurobi, mysql, mongodb, cassandra, aws, azure, google cloud","agile, aws, azure, cassandra, emr, google cloud, gurobi, hadoop, hive, java, kafka, mapreduce, mongodb, mysql, nosql, python, rdbms, redshift, scala, snowflake, spark, unit testing, unixlinux"
Data Scientist,IVY TECH SOLUTIONS INC,"Richmond, VA",https://www.linkedin.com/jobs/view/data-scientist-at-ivy-tech-solutions-inc-3787780095,2023-12-17,Montpelier,United States,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
REMOTE
Data Scientist
6MONTHS Contract to hire
Please send the resume to
or 847- 350-1008
Only W2 or 1099
We are looking for an entry-level Data Scientist for our Practice who will work with clients for problem definition, data exploration and solution identification and design for critical business problems of our customers. The ideal candidate must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must be comfortable working with a wide range of customers from various industries.
must have :
AWS, Python ,machine learning
Required Qualifications:
Comfortable with developing end-to-end advanced analytics projects using statistical analysis and/or machine learning
Expert with programming for data analysis using Python.
Knowledge of Cloud technologies such as AWS SageMaker.
Knowledge of any relational database such as SQL/My SQL.
Ability to integrate data from disparate sources into an efficient and intuitive rational database structure.
Bachelor's or Master's degree in computer science, data science, statistics, mathematics, business administration and/or related field or the foreign equivalent or relevant work experience.
Other Qualifications:
Excellent analytical, troubleshooting and problem-solving skills, with proven ability to quickly understand a problem, determine the significance of the issue and identify a solution.
Focus on delivering results, and work ethic based on a strong desire to exceed expectations.
Demonstrated understanding of product database structure and design principles.
Strong interpersonal skills, including written and oral communication skills.
Ability to work within a team environment, networking and collaborating cross-functionally.
Ability to remain flexible in a fast-paced and rapidly changing environment.
Commitment to continuous improvement, and demonstrated the ability to innovate.
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
PH.
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
8TrA97VmtN
Show more
Show less","AWS, Python, Machine learning, Data analysis, Statistical analysis, Relational database, SQL, MySQL, Cloud technologies, Data mining, Algorithms, Simulations, Problemsolving, Troubleshooting, Analytical skills, Team environment, Communication skills, Innovation, Continuous improvement","aws, python, machine learning, data analysis, statistical analysis, relational database, sql, mysql, cloud technologies, data mining, algorithms, simulations, problemsolving, troubleshooting, analytical skills, team environment, communication skills, innovation, continuous improvement","algorithms, analytical skills, aws, cloud technologies, communication skills, continuous improvement, data mining, dataanalytics, innovation, machine learning, mysql, problemsolving, python, relational database, simulations, sql, statistical analysis, team environment, troubleshooting"
Senior Risk Adjustment Data Analyst - Remote,Sentara Health,"Richmond, VA",https://www.linkedin.com/jobs/view/senior-risk-adjustment-data-analyst-remote-at-sentara-health-3758772511,2023-12-17,Montpelier,United States,Mid senior,Remote,"City/State
Richmond, VA
Overview
Work Shift
First (Days) (United States of America)
Sentara Healthcare
is seeking to hire a qualified individual to join our team as a Senior Risk Adjustment Data Analyst.
Position Status
: Full-time, Day Shift
Position Location:
This position is 100% remote.
Candidates must have a current residence in one of the follow states or being willing to relocate: Alabama, Delaware, Florida, Georgia, Idaho, Indiana, Kansas, Louisiana, Maine, Maryland, Minnesota, Nebraska, Nevada, New Hampshire, North Dakota, Ohio, Oklahoma, Pennsylvania, South Carolina, South Dakota, Tennessee, Texas, Utah, Washington State, West Virginia, Wisconsin, Wyoming
Standard Working Hours
: 8:00AM to 5:00PM (ET).
Minimum Requirements:
Advanced Knowledge of SQL required.
3+ years in data analysis, predictive modeling, medical economics or related experience required.
Bachelor's Level Degree in Mathematics, Statistics, Finance, Economics, Computer Science, or related field required.
Understanding of Prospective and Retrospective Risk Adjustment Interventions preferred.
Diversity and Inclusion at Sentara
Our vision is that everyone brings the strengths that come with diversity to work with them every day. When we are achieving our vision, we have team members that feel they belong and can be their authentic selves, and our workforce is reflective of the communities we serve.
We are realizing this vision through our Diversity and Inclusion strategy, which has three pillars: A diverse and talented workforce, an inclusive and supportive workplace, and outreach and engagement with our community. We have made remarkable strides in these areas over the past several years and, as our world continues to evolve, we know our work is never done.
Our strategies focus on both
structural inclusion
, which looks at our organizational structures, processes, and practices; as well as
behavioral inclusion
, which evaluates our mindsets, skillsets, and relationships. Together, these strategies are moving our organization forward in an environment that fosters a culture of mutual respect and belonging for all.
Please visit the link below to learn more about Sentara’s commitment to diversity and inclusion:
https://www.sentara.com/aboutus/mission-vision-and-values/diversity.aspx
Sentara Overview
For more than a decade, Modern Healthcare magazine has ranked Sentara Healthcare as one of the nation's top integrated healthcare systems. That's because we are dedicated to growth, innovation, and patient safety at more than 300 sites of care in Virginia and northeastern North Carolina, including 12 acute care hospitals.
Sentara Benefits
As the third-largest employer in Virginia, Sentara Healthcare was named by Forbes Magazine as one of America's best large employers. We offer a variety of amenities to our employees, including, but not limited to:
Medical, Dental, and Vision Insurance
Paid Annual Leave, Sick Leave
Flexible Spending Accounts
Retirement funds with matching contribution
Supplemental insurance policies, including legal, Life Insurance and AD&D among others
Work Perks program including discounted movie and theme park tickets among other great deals
Opportunities for further advancement within our organization
Sentara employees strive to make our communities healthier places to live. We're setting the standard for medical excellence within a vibrant, creative, and highly productive workplace. For information about our employee benefits, please visit: Benefits - Sentara (sentaracareers.com)
Join our team! We are committed to quality healthcare, improving health every day, and provide the opportunity for training, development, and growth!
Note:
Sentara Healthcare offers employees comprehensive health care and retirement benefits designed with you and your family's well-being in mind. Our benefits packages are designed to change with you by meeting your needs now and anticipating what comes next. You have a variety of options for medical, dental and vision insurance, life insurance, disability, and voluntary benefits as well as Paid Time Off in the form of sick time, vacation time and paid parental leave. Team Members have the opportunity to earn an annual flat amount Bonus payment if established system and employee eligibility criteria is met.
For applicants within Washington State, the following hiring range will be applied: $87,447.36 - $135,096.00.
Job Summary
The Senior Risk Adjustment Data Analyst provides data analysis supports to Sentara Health Plans’ Revenue Management team, affiliated internal departments, and external vendor partners. The Senor RA Analyst supports in designing and implementing processes and solutions utilizing a wide variety of data sets to resolve business challenges in relation to optimizing Risk Adjustment premium revenue. The senior analyst uses a combinations of data/text mining, analysis, reporting, predictive and risk modeling to support informed business decisions. This requires complete understanding of claims, pharmacy, lab, network utilization, and cost containment data. Additionally, familiarity with external data sources including Medicare, Medicaid, and Health Exchange data files is required.
The Senior RA Data Analyst is required to show independent thinking. The senior analyst assists in identifying business challenges, support development of processes to address the challenges, and metrics to track and monitor business processes. This requires a high level of understanding of Centers for Medicare and Medicaid (CMS), Health and Human Services (HHS), or Virginia Department of Medical Assistance (VA-DMAS) Risk Adjustment programs. The Senior RA Data Analyst must develop Subject Matter Expertise in CMS, HHS, and VA-DMAS Risk Adjustment programs. The subject matter expertise required will vary based on the specific program the Senior Analyst supports but a general understanding of Risk Adjustment programs is expected, including:
Understanding of Prospective and Retrospective Risk Adjustment Interventions
Mastery of at least one of CMS, HHS, DMAS Risk Adjustment models
General understanding of the other two Risk Adjustment models
Mastery of risk classification and diagnosis grouping to disease categories
Lead in development of Performance reports in support of provider engagement
Implement logic to identify members for Risk Adjustment interventions
Lead in implementation of vendor extracts to support Risk Adjustment interventions
Understanding of Database principles and development of relational tables and views
Works with internal, external and enterprise individuals as needed to research, develop, and document new standard reports or processes
Advanced Knowledge of SQL and knowledge of Tableau
Excellent Oral and Written Communication Skills
Problem Solving
Consultative Engagement Skills
Ability to break out complex technical processes and explain to non-technical audiences
Required Education
Bachelor's Degree in Mathematics, Statistics, Finance, Economics, Computer Science, or related
Required Experience
3 - 5 years in data analysis, predictive modeling, Medical Economics or related
Qualifications:
BLD - Bachelor's Level Degree (Required)
Data Analysis
Skills
Sentara Healthcare prides itself on the diversity and inclusiveness of its close to an almost 30,000-member workforce. Diversity, inclusion, and belonging is a guiding principle of the organization to ensure its workforce reflects the communities it serves.
Per Clinical Laboratory Improvement Amendments (CLIA), some clinical environments require proof of education; these regulations are posted at ecfr.gov for further information. In an effort to expedite this verification requirement, we encourage you to upload your diploma or transcript at time of application.
In support of our mission “to improve health every day,” this is a tobacco-free environment.
Show more
Show less","SQL, Data analysis, Predictive modeling, Medical economics, Tableau, Database principles, Relational tables, Views, Risk classification, Diagnosis grouping, Disease categories, CMS, HHS, VADMAS, Prospective Risk Adjustment Interventions, Retrospective Risk Adjustment Interventions, Performance reports, Vendor extracts, Problem solving, Consultative Engagement Skills, Oral communication, Written communication","sql, data analysis, predictive modeling, medical economics, tableau, database principles, relational tables, views, risk classification, diagnosis grouping, disease categories, cms, hhs, vadmas, prospective risk adjustment interventions, retrospective risk adjustment interventions, performance reports, vendor extracts, problem solving, consultative engagement skills, oral communication, written communication","cms, consultative engagement skills, dataanalytics, database principles, diagnosis grouping, disease categories, hhs, medical economics, oral communication, performance reports, predictive modeling, problem solving, prospective risk adjustment interventions, relational tables, retrospective risk adjustment interventions, risk classification, sql, tableau, vadmas, vendor extracts, views, written communication"
Cloud Big Data Engineer Lead,Elevance Health,"Richmond, VA",https://www.linkedin.com/jobs/view/cloud-big-data-engineer-lead-at-elevance-health-3780064917,2023-12-17,Montpelier,United States,Mid senior,Hybrid,"Description
Cloud Big Data Engineer Lead
Location:
This position will work a hybrid model (remote and office). Ideal candidates will live in the state of Georgia or within 50 miles of one of our Pulse Point locations.
Preferred Location: Atlanta, GA.
The
Cloud Big Data Engineer Lead
is responsible for full delivery of end to end system development and maintenance on medium Enterprise wide technology platforms. A proud member of the Elevance Health family of companies, CarelonRx (formerly IngenioRx) leverages the power of new technologies and a strong, clinical first lens, to deliver member centered, lasting pharmacy care.
How You Will Make An Impact
Maintains active relationships with customers to determine business requirements and leads requirements gathering meetings.
Leads requirements gathering meetings and reviews designs with the business.
Leads efforts with Web and marketing team to increase the presence of web products.
May implement improvements in stability, performance, and scalability across major business-critical systems.
May implement process to reduce barriers and roadblocks in projects, services, and processes in order to operate more efficiently.
Owns the change request process and coordinates with other teams as necessary.
Develops and owns list of final enhancements.
Develops and defines application scope and objectives and supervises the preparation of technical and/or functional specifications from with programs will be written.
Performs technical design reviews and code reviews.
Ensures unit test is completed and meets the test plan requirements, system testing is completed and system is implemented according to plan.
Responsible for delivery of application technology solutions and data information planning effort.
Coordinates and manages on-call support and owns the system monitoring process.
Owns the technical development environment and works on the Enterprise team.
Leads multiple or large projects and facilitates large group JAD sessions for requirements, modeling in several disciplines.
Leads vendor evaluation and analysis.
Minimum Qualifications
Requires an BA/BS degree in Information Technology, Computer Science or related field of study and a minimum of 7 years related experience; multi platform, multi-dimensional experience, and expert level experience with business and technical applications; or any combination of education and experience, which would provide an equivalent background.
Preferred Skills, Capabilities And Experiences
Experience mentoring others and provide troubleshooting support strongly preferred.
Multi database and/or multi language strongly preferred.
Please be advised that Elevance Health only accepts resumes for compensation from agencies that have a signed agreement with Elevance Health. Any unsolicited resumes, including those submitted to hiring managers, are deemed to be the property of Elevance Health.
Who We Are
Elevance Health is a health company dedicated to improving lives and communities – and making healthcare simpler. We are a Fortune 25 company with a longstanding history in the healthcare industry, looking for leaders at all levels of the organization who are passionate about making an impact on our members and the communities we serve.
How We Work
At Elevance Health, we are creating a culture that is designed to advance our strategy but will also lead to personal and professional growth for our associates. Our values and behaviors are the root of our culture. They are how we achieve our strategy, power our business outcomes and drive our shared success - for our consumers, our associates, our communities and our business.
We offer a range of market-competitive total rewards that include merit increases, paid holidays, Paid Time Off, and incentive bonus programs (unless covered by a collective bargaining agreement), medical, dental, vision, short and long term disability benefits, 401(k) +match, stock purchase plan, life insurance, wellness programs and financial education resources, to name a few.
Elevance Health operates in a Hybrid Workforce Strategy. Unless specified as primarily virtual by the hiring manager, associates are required to work at an Elevance Health location at least once per week, and potentially several times per week. Specific requirements and expectations for time onsite will be discussed as part of the hiring process. Candidates must reside within 50 miles or 1-hour commute each way of a relevant Elevance Health location.
The health of our associates and communities is a top priority for Elevance Health. We require all new candidates in certain patient/member-facing roles to become vaccinated against COVID-19. If you are not vaccinated, your offer will be rescinded unless you provide an acceptable explanation. Elevance Health will also follow all relevant federal, state and local laws.
Elevance Health is an Equal Employment Opportunity employer and all qualified applicants will receive consideration for employment without regard to age, citizenship status, color, creed, disability, ethnicity, genetic information, gender (including gender identity and gender expression), marital status, national origin, race, religion, sex, sexual orientation, veteran status or any other status or condition protected by applicable federal, state, or local laws. Applicants who require accommodation to participate in the job application process may contact elevancehealthjobssupport@elevancehealth.com for assistance.
Show more
Show less","Cloud Computing, Big Data, Data Analytics, Data Engineering, Data Integration, Data Warehousing, Data Mining, Machine Learning, Data Visualization, Python, Java, SQL, NoSQL, Hadoop, Spark, Hive, Pig, Tableau, Power BI, Data Governance, Data Security, Data Privacy, Agile, DevOps, Cloud Platforms, AWS, Azure, GCP, Google Cloud Platform, Microsoft Azure, Amazon Web Services","cloud computing, big data, data analytics, data engineering, data integration, data warehousing, data mining, machine learning, data visualization, python, java, sql, nosql, hadoop, spark, hive, pig, tableau, power bi, data governance, data security, data privacy, agile, devops, cloud platforms, aws, azure, gcp, google cloud platform, microsoft azure, amazon web services","agile, amazon web services, aws, azure, big data, cloud computing, cloud platforms, data engineering, data governance, data integration, data mining, data privacy, data security, dataanalytics, datawarehouse, devops, gcp, google cloud platform, hadoop, hive, java, machine learning, microsoft azure, nosql, pig, powerbi, python, spark, sql, tableau, visualization"
Lead Azure Data engineer,Accord Technologies Inc,"Glen Allen, VA",https://www.linkedin.com/jobs/view/lead-azure-data-engineer-at-accord-technologies-inc-3678127827,2023-12-17,Montpelier,United States,Mid senior,Hybrid,"Title: Lead Azure Data engineer
Location: Glen Allen, VA
Duration: 12 months contract
Hire type : Hybrid (3 days min onsite mandatory)
Nbr of positions: 1
Job requirement:
9+ years of experience in leading the design and development of data and analytics projects in a global company.
Experience in working for projects across cross functional teams, building sustainable processes and coordinating release schedules.
7+ years of MS SQL experience with data modelling
3+ years of experience in working with Microsoft Azure and strong knowledge about ADLS, Blob Storage, Data Factory, SQL Server and Warehouses. Experience in Cloud based EDW platforms (Snowflake, Synapse, Redshift, Big Query etc)
Building and launching new data models that provide intuitive analytics for the analysts and customers
Mandatory Skill Sets
Azure lead Data engineer.
Databricks (s.Python, sql)
Azure Data Factory
Synapse Analytics
Parque and Delta table
Show more
Show less","Azure, Databricks, Python, SQL, Azure Data Factory, Synapse Analytics, Parquet, Delta table, Data Warehousing, Data Modelling, Data Analytics, Cloud Computing, Microsoft SQL Server","azure, databricks, python, sql, azure data factory, synapse analytics, parquet, delta table, data warehousing, data modelling, data analytics, cloud computing, microsoft sql server","azure, azure data factory, cloud computing, data modelling, dataanalytics, databricks, datawarehouse, delta table, microsoft sql server, parquet, python, sql, synapse analytics"
Principal Data Engineer,hackajob,"Richmond, VA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-hackajob-3776978114,2023-12-17,Montpelier,United States,Mid senior,Hybrid,"hackajob has partnered with a disruptive automotive business that puts innovation at the heart of its new digital products and works with an unrivaled amount of automotive consumer data.
Role
: Principal Data Engineer, MDM
Location
: Richmond, VA
Work model:
hybrid
Qualifications and Requirements
At least eight years of hands on ETL development experience
At least four years of hands-on experience developing in Azure Data Factory, Databricks Python and Spark
At least 2 years leading a technical team, providing leadership, design documentation and diagrams
At least 2 years of hands-on experience working with configuration of Azure subscriptions, resource groups, resources and provisioning
At least 2 years leading the end-to-end design and development of ETL integrations to be consumed by the enterprise, including monitoring and production support
Experience with data driven initiatives, with background in MDM or other data management and governance platforms
Proven ability to learn and fully understand new technology, becoming the SME for the platforms and services your team interacts with
Solid understanding of DevOps capabilities such as automated testing, continuous integration, and continuous delivery
Experience working with a Master Data Management platform tool (pref. Reltio)
Proven track record of exhibiting strong critical thinking by analyzing facts in order to understand a business request or requirement thoroughly.
Proven ability to mentor and develop others. Experience positively influencing team norms, culture, and technical vision.
Experience with agile methodologies
Very strong communication both written and verbal
If you're interested in finding out more about this fantastic opportunity please get your application in and we can arrange a call.
hackajob is a recruitment platform that will match you with relevant roles based on your preferences and in order to be matched with the roles you need to create an account with us.
*This role requires you to be based in the US*
Show more
Show less","ETL, Data Factory, Databricks Python, Spark, Azure subscriptions, Azure resource groups, DevOps, Master Data Management, Reltio, Agile methodologies, Communication","etl, data factory, databricks python, spark, azure subscriptions, azure resource groups, devops, master data management, reltio, agile methodologies, communication","agile methodologies, azure resource groups, azure subscriptions, communication, data factory, databricks python, devops, etl, master data management, reltio, spark"
Business/Data Analyst,Steneral Consulting,"Richmond, VA",https://www.linkedin.com/jobs/view/business-data-analyst-at-steneral-consulting-3676929099,2023-12-17,Montpelier,United States,Mid senior,Hybrid,"Job Description
Title: Business/Data Analyst
Location: Richmond, VA
Duration: 6+ Months
End Client: Virginia Department of Transportation
Position Details
Title- Business/Data Analyst
Location- Hybrid; 3 days per week will be onsite Richmond, Virginia (
**local candidates ONLY due to onsite requirement
Please provide address proof and answer to the questions in ""
ITEMS REQUIRED AT THE TIME OF SUBMITTAL""
at the time of submission.
prior state experience a STRONG plus, esp. in the Department of Transportation or similar domains. Min 10 years of Business Analyst/Data Analyst exp. REQUIRED. Candidates with Certified Business Analysis Professional (CBAP) or Certified Scrum Product Owner (CSPO) Certification or Certified Data Management Professional (CDMP) or Certified Data Professional (CDP) certifications will be preferred
JD-
Responsibilities
Work as a key member of Data Management Program for data projects.
Support teams working in Agile (Sprint) within a Data Product Line delivery mechanism.
Analyzing problems; documenting business processes and data lifecycle; developing data requirements, user stories and acceptance criteria; and testing.
Handle multiple projects, and remain flexible and productive, despite changing priorities and processes.
Work with the Project team members and business stakeholders to understand business processes and pain points
Develop expertise in the customers' datasets and data lifecycle
Profile source data; review source data and compare content and structure to dataset requirements; identify conflicts and determine recommendations for resolution Conduct entity resolution to identify matching and merging and semantic conflicts
Elicit, record, and manage metadata
Diagram current processes and proposed modifications using process flows, context diagrams and data flow diagrams
Discover and document requirements and user stories with a focus on improving both business and technical processing
Decompose requirements into Epics and Features and create clear and concise user stories that are easy to understand and implement by technical staff
Utilize progressive elaboration; map stories to data models and architectures to be used by internal staff to facilitate master data management
Identify and group related user stories into themes, document dependencies and associated business processes
Assist Product Owner in maintaining the product backlog
Create conceptual prototypes and mock-ups
Collaborate with staff, vendors, consultants, and contractors as they are engaged on tasks to formulate, detail and test potential and implemented solutions
Perform Quality Analyst functions such as defining test objectives, test plans and test cases, and executing test cases
Coordinate and Facilitate User Acceptance Testing with Business and ensure Project Managers/Scrum Masters are informed of the progress
Knowledge And Skills
Experience organizing and maintaining Product and Sprint backlogs
Experience with reporting systems operational data stores, data warehouses, data lakes, data marts
Required Skills
10 Years - Delivering business and systems analysis artifacts such as Business Requirements Documentation (BRD), Requirements Traceability Matrix (RTM)
5 Years - Experience with enterprise data management, data analysis, and data lifecycle
5 Years - Agile Business Analyst; strong understanding of Scrum concepts and methodology
Experience translating business and product strategy requirements into application requirements and user stories
Proficient with defining acceptance criteria and managing acceptance process with development
Experience creating wireframes and prototypes (using applications such as Balsamiq)
Experience with large, multi-module systems
Expertise with Microsoft Office products (Word, Excel, Access, Outlook, Visio, PowerPoint, Project Server)
Experience with Application Lifecycle management (ALM)
Exceptional written and oral communications skills and have the proven ability to work well with a diverse set of peers and customers
Desired Skills
Certified Business Analysis Professional (CBAP) or Certified Scrum Product Owner (CSPO) Certification
Certified Data Management Professional (CDMP) or Certified Data Professional (CDP)
Experience using Team Foundation Server for agile software development and work item tracking
Experience with automated testing tools such as TestComplete
Skills
Skills Matrix
Years Used
Last Used
10 Years - Delivering business and systems analysis artifacts such as Business Requirements Documentation (BRD), Requirements Traceability Matrix (RTM)
5 Years - Experience with enterprise data management, data analysis, and data lifecycle
5 Years - Agile Business Analyst; strong understanding of Scrum concepts and methodology
Experience translating business and product strategy requirements into application requirements and user stories
Proficient with defining acceptance criteria and managing acceptance process with development
Experience creating wireframes and prototypes (using applications such as Balsamiq)
Experience with large, multi-module systems
Expertise with Microsoft Office products (Word, Excel, Access, Outlook, Visio, PowerPoint, Project Server)
Experience with Application Lifecycle management (ALM)
Exceptional written and oral communications skills and have the proven ability to work well with a diverse set of peers and customers
Certified Business Analysis Professional (CBAP) or Certified Scrum Product Owner (CSPO) Certification
Certified Data Management Professional (CDMP) or Certified Data Professional (CDP)
Experience using Team Foundation Server for agile software development and work item tracking
Experience with automated testing tools such as TestComplete
Show more
Show less","Agile, Microsoft Office, Scrum, Balsamiq, Team Foundation Server, Data Warehouse, Data Lake, Business Requirements Documentation (BRD), Requirements Traceability Matrix (RTM), Application Lifecycle management (ALM), Data Analysis, Data Management, Prototyping, User Stories, Acceptance Criteria, Wireframing, Large MultiModule Systems, Data Integration, Data Quality, Data Governance, Data Architecture, Data Modeling, Metadata Management, Data Migration, Data Security, Data Warehousing, Data Mining, Business Intelligence, Reporting","agile, microsoft office, scrum, balsamiq, team foundation server, data warehouse, data lake, business requirements documentation brd, requirements traceability matrix rtm, application lifecycle management alm, data analysis, data management, prototyping, user stories, acceptance criteria, wireframing, large multimodule systems, data integration, data quality, data governance, data architecture, data modeling, metadata management, data migration, data security, data warehousing, data mining, business intelligence, reporting","acceptance criteria, agile, application lifecycle management alm, balsamiq, business intelligence, business requirements documentation brd, data architecture, data governance, data integration, data lake, data management, data migration, data mining, data quality, data security, dataanalytics, datamodeling, datawarehouse, large multimodule systems, metadata management, microsoft office, prototyping, reporting, requirements traceability matrix rtm, scrum, team foundation server, user stories, wireframing"
Business/Data Analyst,TekIntegral,"Richmond, VA",https://www.linkedin.com/jobs/view/business-data-analyst-at-tekintegral-3677436196,2023-12-17,Montpelier,United States,Mid senior,Hybrid,"Job Title
Business/Data Analyst
Employment
Contract
Location
Richmond, Virginia
Notes
10 years experience Business Analyst and should have strong knowledge of BRD (Business Requirements Documents) and RTM (Requirements Traceability Matrix)
Job Description
Business/Data Analyst
Primary Location: Richmond, Virginia
Local Candidates Strongly Preferred.
V-Soft Consulting is currently hiring for a
Business/Data Analyst
for our premier client in
Richmond, Virginia
.
What You'll Need
Technical Requirements and Certifications
Preferred
CBAP and/or CSPO Certification
CDMP and/or CDP Certification
Education And Experience
Experience organizing and maintaining Product and Sprint backlogs.
Proficient with defining acceptance criteria and managing acceptance process.
Experience with enterprise data management.
Experience with reporting systems operational data stores, data warehouses, data lakes, data marts.
Delivering business and systems analysis artifacts such as Business Requirements Documentation (BRD), Requirements Traceability Matrix (RTM): 10 Years.
Experience with enterprise data management, data analysis, and data lifecycle: 5 Years.
Agile Business Analyst; strong understanding of Scrum concepts and methodology: 5 Years.
Experience translating business and product strategy requirements into application requirements and user stories.
Proficient with defining acceptance criteria and managing acceptance process with development.
Experience creating wireframes and prototypes (using applications such as Balsamiq).
Experience with large, multi-module systems.
Expertise with Microsoft Office products (Word, Excel, Access, Outlook, Visio, PowerPoint, Project Server).
Experience with Application Lifecycle management (ALM).
Exceptional written and oral communications skills and have the proven ability to work well with a diverse set of peers and customers.
Desired
Experience using Team Foundation Server for agile software development and work item tracking.
Experience with automated testing tools such as TestComplete.
What You'll Do
Job Responsibilities:
Create conceptual prototypes and mock-ups.
Develop expertise in the customers' datasets and data lifecycle.
Profile source data; review source data and compare content and structure to dataset requirements; identify conflicts and determine recommendations for resolution.
Elicit, record, and manage metadata.
Diagram current processes and proposed modifications using process flows, context diagrams and data flow diagrams.
Utilize progressive elaboration; map stories to data models and architectures to be used by internal staff to facilitate master data management.
Conduct entity resolution to identify matching and merging and semantic conflicts.
Discover and document requirements and user stories with a focus on improving both business and technical processing.
Decompose requirements into Epics and Features and create clear and concise user stories that are easy to understand and implement by technical staff.
Assist Product Owner in maintaining the product backlog.
Utilize progressive elaboration; map stories to data models and architectures to be used by internal staff to facilitate master data management.
Collaborate with staff, vendors, consultants, and contractors as they are engaged on tasks to formulate, detail and test potential and implemented solutions.
Perform Quality Analyst functions such as defining test objectives, test plans and test cases, and executing test cases.
Coordinate and Facilitate User Acceptance Testing with Business and ensure Project Managers/Scrum Masters are informed of the progress.
Primary Skills
MS Office and BRD and RTM
Show more
Show less","Business Analyst, Data Analyst, CBAP, CSPO, CDMP, CDP, Scrum, Balsamiq, Microsoft Office, Team Foundation Server, TestComplete, BRD, RTM, Product Backlog, Sprint Backlog, Acceptance Criteria, Enterprise Data Management, Data Warehouses, Data Lakes, Data Marts, Agile, User Stories, Wireframes, Prototypes, Conceptual Prototypes, Mockups, Metadata, Process Flows, Context Diagrams, Data Flow Diagrams, Progressive Elaboration, Entity Resolution, Epics, Features, Product Owner, Quality Analyst, Test Objectives, Test Plans, Test Cases, User Acceptance Testing","business analyst, data analyst, cbap, cspo, cdmp, cdp, scrum, balsamiq, microsoft office, team foundation server, testcomplete, brd, rtm, product backlog, sprint backlog, acceptance criteria, enterprise data management, data warehouses, data lakes, data marts, agile, user stories, wireframes, prototypes, conceptual prototypes, mockups, metadata, process flows, context diagrams, data flow diagrams, progressive elaboration, entity resolution, epics, features, product owner, quality analyst, test objectives, test plans, test cases, user acceptance testing","acceptance criteria, agile, balsamiq, brd, business analyst, cbap, cdmp, cdp, conceptual prototypes, context diagrams, cspo, data flow diagrams, data lakes, data marts, data warehouses, dataanalytics, enterprise data management, entity resolution, epics, features, metadata, microsoft office, mockups, process flows, product backlog, product owner, progressive elaboration, prototypes, quality analyst, rtm, scrum, sprint backlog, team foundation server, test cases, test objectives, test plans, testcomplete, user acceptance testing, user stories, wireframes"
Data Governance Analyst- Hybrid Role,"ARK Solutions, Inc.","Richmond, VA",https://www.linkedin.com/jobs/view/data-governance-analyst-hybrid-role-at-ark-solutions-inc-3782210372,2023-12-17,Montpelier,United States,Mid senior,Hybrid,"Data Governance Analyst
Hybrid Role!!
About The Role
KEY RESPONSIBILITIES:
The Data Governance Analyst will work with the team to strategically define, implement and operate Data Governance at client dept. This individual will manage and monitor data quality, data definition, and data lineage requirements in order to ensure that the company’s data is fit for purpose.
R
EQUIRED YEARS OF EXPERIENCE: 10+ in the following:
Other Responsibilities Are Listed Below.
Analyzes data to provide insights for decision-making.
Supports the implementation of data governance processes.
Monitors and ensures data quality and integrity.
Develops and implements data quality standards and processes.
Collaborates with other IT teams to ensure data integration and consistency.
Maintain repository of Data Governance policies, procedures and standards with appropriate version control
Work with Data Governance Manager during onboarding new applications under governance and provide support during operationalization
Provide support to Data Governance Stewards during data profiling activities
Assist in data reconciliation and validation checks across various systems
Come up with initial proof of concepts for data that needs to be merged from multiple systems and define the business benefits of moving towards a consolidated source
Assist in documenting document data lineage/data flows
Assist in capturing business glossary/data dictionary and metadata lineage information
Work with IT teams to enforce ILM processes and procedure
This individual is also responsible for assisting in data reconciliation and validation checks across various systems.
Skills Matrix:
Skill Required / Desired Amount of Experience Data Analysis Required 10 Years Experience with Data Lineage / Cataloging Required 10 Years Data Policy /Procedure creation Required 10 Years Proficiency with SQL Required 10 Years Datawarehouse Experience Required 10 Years Requirements Elicitation Required 10 Years Experience with Databases(Relational databases, SQL Server, Oracle , Snowflake etc) Required 7 Years
Show more
Show less","Data Governance, Data Quality, Data Lineage, Data Analysis, Data Integration, Data Consistency, Data Standards, Data Profiling, Data Reconciliation, Data Validation, Data Dictionary, Metadata Lineage, Business Glossary, ILM Process, Data Policy Creation, Data Warehouse, SQL, Oracle, Snowflake, Relational Databases","data governance, data quality, data lineage, data analysis, data integration, data consistency, data standards, data profiling, data reconciliation, data validation, data dictionary, metadata lineage, business glossary, ilm process, data policy creation, data warehouse, sql, oracle, snowflake, relational databases","business glossary, data consistency, data dictionary, data governance, data integration, data lineage, data policy creation, data profiling, data quality, data reconciliation, data standards, data validation, dataanalytics, datawarehouse, ilm process, metadata lineage, oracle, relational databases, snowflake, sql"
Data Governance Analyst,"Vector Consulting, Inc","Richmond, VA",https://www.linkedin.com/jobs/view/data-governance-analyst-at-vector-consulting-inc-3784802234,2023-12-17,Montpelier,United States,Mid senior,Hybrid,"Our government client is looking for an experienced
Data Governance Analyst
on a hybrid 9+ months renewable contract role in
Richmond, VA.
Position – Data Governance Analyst
About the Role:
The Data Governance Analyst will work with the team to strategically define, implement, and operate Data Governance at the agency. This individual will manage and monitor data quality, data definition, and data lineage requirements in order to ensure that the company’s data is fit for purpose.
Responsibilities:
• Analyzes data to provide insights for decision-making.
• Supports the implementation of data governance processes.
• Monitors and ensures data quality and integrity.
• Develops and implements data quality standards and processes.
• Collaborates with other IT teams to ensure data integration and consistency.
• Maintain repository of Data Governance policies, procedures, and standards with appropriate version control
• Work with Data Governance Manager during onboarding new applications under governance and provide support during operationalization
• Provide support to Data Governance Stewards during data profiling activities
• Assist in data reconciliation and validation checks across various systems
• Come up with initial proof of concepts for data that needs to be merged from multiple systems and define the business benefits of moving towards a consolidated source
• Assist in documenting document data lineage/data flows
• Assist in capturing business glossary/data dictionary and metadata lineage information
• Work with IT teams to enforce ILM processes and procedure
• This individual is also responsible for assisting in data reconciliation and validation checks across various systems.
Required/Desired Skills and Experience:
10 years of Data Analysis Required.
10 years of Experience with Data Lineage / Cataloging Required.
10 years of Data Policy /Procedure creation Required.
10 years of Proficiency with SQL Required.
10 years of Datawarehouse Experience Required.
10 years of Requirements Elicitation Required
7 years of Experience with Databases (Relational databases, SQL Server, Oracle , Snowflake etc) Required.
About Vector:
Vector Consulting, Inc., (Headquartered in Atlanta) is an IT Talent Acquisition Solutions firm committed to delivering results. Since our founding in 1990, we have been partnering with our customers, understanding their business, and developing solutions with a commitment to quality, reliability and value. Our continuing growth has been and continues to be built around successful relationships that are based on our organization's operating philosophy and commitment to ** People, Partnerships, Purpose and Performance - THE VECTOR WAY
www.vectorconsulting.com
“Celebrating 30 years of service.”
Show more
Show less","Data Governance, Data Analysis, Data Lineage, Data Policy, SQL, Data Warehouse, Requirements Elicitation, Databases, Relational Databases, SQL Server, Oracle, Snowflake, Data Quality, Data Integrity, Data Reconciliation, Data Validation, Proof of Concept, Data Consolidation, Data Lineage, Business Glossary, Data Dictionary, Metadata, ILM, Data Cataloging","data governance, data analysis, data lineage, data policy, sql, data warehouse, requirements elicitation, databases, relational databases, sql server, oracle, snowflake, data quality, data integrity, data reconciliation, data validation, proof of concept, data consolidation, data lineage, business glossary, data dictionary, metadata, ilm, data cataloging","business glossary, data cataloging, data consolidation, data dictionary, data governance, data integrity, data lineage, data policy, data quality, data reconciliation, data validation, dataanalytics, databases, datawarehouse, ilm, metadata, oracle, proof of concept, relational databases, requirements elicitation, snowflake, sql, sql server"
Senior Cyber Security Data Protection Engineer,Sentara Health,Greater Richmond Region,https://www.linkedin.com/jobs/view/senior-cyber-security-data-protection-engineer-at-sentara-health-3765497494,2023-12-17,Montpelier,United States,Mid senior,Hybrid,"City/State
Norfolk, VA
Overview
Work Shift
First (Days) (United States of America)
Sentara Healthcare is seeking to hire a qualified individual to join our team as a Senior Security Data Protection Engineer.
Position Status
: Full-time, Day Shift
Position Location:
This position is 100% remote.
Standard Working Hours
: 8:00AM to 5:00PM (ET).
Minimum Requirements:
Strong working knowledge of engineering, implementing, operating, and supporting a Data Loss Prevention Platform.
Understanding of end-to-end implications of DLP implementation from business requirements to implementation challenges to constituent education to continual service improvement and support.
Experience with data protection and cloud tooling (DLP, Data Security, Encryption and CASB). A working knowledge of Cloud Security and supporting Technologies (e.g., AWS, Azure, GCP, SaaS, PaaS, DBaaS), particularly Data Loss Prevention within Microsoft O365, Azure Information Protection & AWS.
Proven experience of information protection and data classification technologies, concepts, and techniques to classify unstructured and structured data both on-premises and in the cloud.
Understanding of vulnerability assessment tools, methodologies, and frameworks.
Ability to analyze and interpret vulnerability scan results and data protection requirements, identify risks, and recommend appropriate actions.
Diversity and Inclusion at Sentara
Our vision is that everyone brings the strengths that come with diversity to work with them every day. When we are achieving our vision, we have team members that feel they belong and can be their authentic selves, and our workforce is reflective of the communities we serve.
We are realizing this vision through our Diversity and Inclusion strategy, which has three pillars: A diverse and talented workforce, an inclusive and supportive workplace, and outreach and engagement with our community. We have made remarkable strides in these areas over the past several years and, as our world continues to evolve, we know our work is never done.
Our strategies focus on both
structural inclusion
, which looks at our organizational structures, processes, and practices; as well as
behavioral inclusion
, which evaluates our mindsets, skillsets, and relationships. Together, these strategies are moving our organization forward in an environment that fosters a culture of mutual respect and belonging for all.
Please visit the link below to learn more about Sentara’s commitment to diversity and inclusion:
https://www.sentara.com/aboutus/mission-vision-and-values/diversity.aspx
Sentara Overview
For more than a decade, Modern Healthcare magazine has ranked Sentara Healthcare as one of the nation's top integrated healthcare systems. That's because we are dedicated to growth, innovation, and patient safety at more than 300 sites of care in Virginia and northeastern North Carolina, including 12 acute care hospitals.
Sentara Benefits
As the third-largest employer in Virginia, Sentara Healthcare was named by Forbes Magazine as one of America's best large employers. We offer a variety of amenities to our employees, including, but not limited to:
Medical, Dental, and Vision Insurance
Paid Annual Leave, Sick Leave
Flexible Spending Accounts
Retirement funds with matching contribution
Supplemental insurance policies, including legal, Life Insurance and AD&D among others
Work Perks program including discounted movie and theme park tickets among other great deals
Opportunities for further advancement within our organization
Sentara employees strive to make our communities healthier places to live. We're setting the standard for medical excellence within a vibrant, creative, and highly productive workplace. For information about our employee benefits, please visit: Benefits - Sentara (sentaracareers.com)
Join our team! We are committed to quality healthcare, improving health every day, and provide the opportunity for training, development, and growth!
Please Note
:
The Covid Vaccination(s) and yearly Flu Vaccination are required for employment.
Note:
Sentara Healthcare offers employees comprehensive health care and retirement benefits designed with you and your family's well-being in mind. Our benefits packages are designed to change with you by meeting your needs now and anticipating what comes next. You have a variety of options for medical, dental and vision insurance, life insurance, disability, and voluntary benefits as well as Paid Time Off in the form of sick time, vacation time and paid parental leave. Team Members have the opportunity to earn an annual flat amount Bonus payment if established system and employee eligibility criteria is met.
For applicants within Washington State, the following hiring range will be applied: $81,572 to $135,954 annually.
#DICE
Job Summary
As the Senior Cyber Security Data Protection Engineer, you will be responsible for designing, deploying, and maintaining technology to support the Sentara Health’s Data Security Strategy. As a Senior member of the team, you will lead efforts in managing vulnerabilities and protecting critical data assets. Your responsibilities will include conducting vulnerability assessments, implementing data protection controls, and collaborating with cross-functional teams to mitigate risks. In addition to conducting vulnerability assessments and implementing data protection controls, you will be responsible for providing research and recommendation around solutions to improve the program for more efficient remediation of vulnerabilities.
A Senior Professional applies advanced knowledge of job areas typically obtained through advanced education and work experience. Responsibilities typically include:
Managing projects/processes, and working independently with limited supervision.
Coaching and reviewing the work of lower-level professionals.
Problems faced are difficult and sometimes complex. Experience in lieu of Bachelor’s Degree
5+ years of relevant experience with a degree
7+ years of relevant experience without a degree
Qualifications:
Skills
Sentara Healthcare prides itself on the diversity and inclusiveness of its close to an almost 30,000-member workforce. Diversity, inclusion, and belonging is a guiding principle of the organization to ensure its workforce reflects the communities it serves.
Per Clinical Laboratory Improvement Amendments (CLIA), some clinical environments require proof of education; these regulations are posted at ecfr.gov for further information. In an effort to expedite this verification requirement, we encourage you to upload your diploma or transcript at time of application.
In support of our mission “to improve health every day,” this is a tobacco-free environment.
Show more
Show less","Data Loss Prevention (DLP), Encryption, CASB, Cloud Security, AWS, Azure, GCP, SaaS, PaaS, DBaaS, Microsoft O365, Azure Information Protection, Information Protection, Data Classification, Vulnerability Assessment, Risk Management, Security Strategy, Vulnerability Management, Data Protection Controls, CrossFunctional Collaboration, Project Management, Team Leadership, Coaching, Mentoring, Problem Solving","data loss prevention dlp, encryption, casb, cloud security, aws, azure, gcp, saas, paas, dbaas, microsoft o365, azure information protection, information protection, data classification, vulnerability assessment, risk management, security strategy, vulnerability management, data protection controls, crossfunctional collaboration, project management, team leadership, coaching, mentoring, problem solving","aws, azure, azure information protection, casb, cloud security, coaching, crossfunctional collaboration, data classification, data loss prevention dlp, data protection controls, dbaas, encryption, gcp, information protection, mentoring, microsoft o365, paas, problem solving, project management, risk management, saas, security strategy, team leadership, vulnerability assessment, vulnerability management"
Business Analyst – Data warehouse Project,Stellar Professionals,"Richmond, VA",https://www.linkedin.com/jobs/view/business-analyst-%E2%80%93-data-warehouse-project-at-stellar-professionals-3652269791,2023-12-17,Montpelier,United States,Mid senior,Hybrid,"Business Analyst Data warehouse Project
Applicant must have 8 years of relevant experience with the following:
Gathering business requirements from stakeholders
Gathering, compiling, evaluating, and interpreting complex information
Capturing key elements during meetings and brainstorming sessions, generate group member direction, and lead group to define and achieve goals
Presenting work results and recommendations in a written report or a verbal presentation
Ability to communicate clearly with stakeholders at all levels of the organization
Expert in gathering and analyzing requirements related to data management, reports, and analytics
Expert level skill in MS Excel, Vision, and PowerPoint
Experience with health information systems, health data and clinical and billing codes
Experience with analyzing SQL Server stored procedures to extract business rules and data quality checks
Show more
Show less","Business analysis, Data warehouse project management, Requirements gathering, Data management, Reporting, Analytics, MS Excel, Microsoft Vision, PowerPoint, Health information systems, Health data, Clinical codes, Billing codes, SQL Server stored procedures, Data quality checks","business analysis, data warehouse project management, requirements gathering, data management, reporting, analytics, ms excel, microsoft vision, powerpoint, health information systems, health data, clinical codes, billing codes, sql server stored procedures, data quality checks","analytics, billing codes, business analysis, clinical codes, data management, data quality checks, data warehouse project management, health data, health information systems, microsoft vision, ms excel, powerpoint, reporting, requirements gathering, sql server stored procedures"
Entry Level Business/Data Analyst,Asta Crs Inc,"Richmond, VA",https://www.linkedin.com/jobs/view/entry-level-business-data-analyst-at-asta-crs-inc-3788064778,2023-12-17,Montpelier,United States,Mid senior,Hybrid,"Job Description
We are seeking Junior Business & Data Analyst's with 6 months to 1 year of experience to become an integral part of our team! You will analyze data to understand business and market trends in order to increase company revenue and efficiency.
Multiple job openings, Nationwide
. Must be willing to relocate .Candidates must be located in USA.
Responsibilities:
Use data to understand business patterns and trends
Analyze internal and external data through quantitative research
Communicate findings to company through standard and ad hoc reports
Promote best practices in data analysis and reporting
Collaborate with cross-functional teams
Qualifications:
Bachelor's degree
Knowledge of SQL,statistical tools and business reporting
Strong problem solving and critical thinking skills
Strong attention to detail & Good communication
Ability to prioritize and multitask
""Asta CRS, Inc. is an Equal Opportunity Employer M/F/V/D."" ASTA CRS is proud to state that we are enrolled with the USCIS for the E-Verification Program .
Company Description
ASTA Corporate Resource Solutions Inc is one of the Fastest Growing IT Companies in Northern America and the DC Metro Area with its headquarters in Ashburn, Virginia. ASTA CRS is an Information Technology Provider delivering superior quality software development, consulting, and staffing solutions to our client partners.
ASTA CRS services are uniquely positioned to support clients in achieving profound efficiencies and relentlessly delivering results. ASTA CRS is a long-time and trusted resource for its clients and partners.
Asta CRS, Inc. is an Equal Opportunity Employer M/F/V/D. ASTA CRS is proud to state that we are enrolled with the USCIS for the E-Verification Program.
ASTA Corporate Resource Solutions Inc is one of the Fastest Growing IT Companies in Northern America and the DC Metro Area with its headquarters in Ashburn, Virginia. ASTA CRS is an Information Technology Provider delivering superior quality software development, consulting, and staffing solutions to our client partners. ASTA CRS services are uniquely positioned to support clients in achieving profound efficiencies and relentlessly delivering results. ASTA CRS is a long-time and trusted resource for its clients and partners. Asta CRS, Inc. is an Equal Opportunity Employer M/F/V/D. ASTA CRS is proud to state that we are enrolled with the USCIS for the E-Verification Program.
Show more
Show less","Data Analysis, SQL, Statistics, Business Reporting, Problem Solving, Critical Thinking, Attention to Detail, Communication, Prioritization, Multitasking","data analysis, sql, statistics, business reporting, problem solving, critical thinking, attention to detail, communication, prioritization, multitasking","attention to detail, business reporting, communication, critical thinking, dataanalytics, multitasking, prioritization, problem solving, sql, statistics"
Junior Data Engineer Python SQL,Client Server,"Tyne & Wear, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-python-sql-at-client-server-3783917157,2023-12-17,Tyneside, United Kingdom,Associate,Hybrid,"Junior Data Engineer (Python SQL AWS) Sunderland / WFH to £68k
Are you a bright, ambitious Data Engineer who loves technology?
You could be progressing your career within a relaxed, supportive team environment at a tech driven online gaming / low-cost gambling SaaS tech company that provide a range of white labelled gaming platforms for household names with millions of concurrent players.
You will join a small team of Data Engineers responsible for implementing methods to improve data reliability and quality, combining raw information from a variety of different sources to create consistent and machine readable formats as well as developing the data infrastructure that enables data extraction and transformation for predictive or prescriptive modelling.
You'll have a broad scope of responsibilities including data ingestion, data transformation, data storage, ETL, data modelling, data quality and governance, data integration, performance tuning, scalability, resilience, security, tooling and technology.
As a Junior Data Engineer you'll be mentored and supported, there's continual learning, training and self-development opportunities.
WFH Policy:
You'll join colleagues in the Sunderland office three days a week, where you'll join fellow technologists in a relaxed environment in awesome custom built offices in Sunderland with a range of facilities and perks including free meals at the onsite restaurant as well as membership at onsite gym; there's flexibility to work from home two days a week.
Requirements:
You have a good understanding of data models, data mining and segmentation techniques
You have coding skills with Python (or C#) and SQL
You have experience with SQL databases (e.g. Redshift, PostgreSQL)
You are familiar with data tools such as Airflow, DBT, AWS Kinesis
You have strong analysis and problem solving skills
You're collaborative with excellent communication skills, keen to learn and progress
What's in it for you:
Competitive salary to £40k depending on experience and knowledge
Continual training, learning and career development opportunities
Bonus
Pension
Private medical care
And a range of other perks and benefits
Apply now
to find out more about this Junior Data Engineer (Python SQL AWS) opportunity.
At Client Server we believe in a diverse workplace that allows people to play to their strengths and continually learn. We're an equal opportunities employer whose people come from all walks of life and will never discriminate based on race, colour, religion, sex, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The clients we work with share our values.
Show more
Show less","Python, SQL, AWS, Apache Airflow, DBT, AWS Kinesis, Redshift, PostgreSQL, Data models, Data mining, Data segmentation, Data ingestion, Data transformation, Data storage, ETL, Data modelling, Data quality, Data governance, Data integration, Data analysis, Problem solving, Communication, Collaboration","python, sql, aws, apache airflow, dbt, aws kinesis, redshift, postgresql, data models, data mining, data segmentation, data ingestion, data transformation, data storage, etl, data modelling, data quality, data governance, data integration, data analysis, problem solving, communication, collaboration","apache airflow, aws, aws kinesis, collaboration, communication, data governance, data ingestion, data integration, data mining, data modelling, data models, data quality, data segmentation, data storage, data transformation, dataanalytics, dbt, etl, postgresql, problem solving, python, redshift, sql"
Data Engineer - Arcadis Gen,Arcadis,"Newcastle upon Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-arcadis-gen-at-arcadis-3770624403,2023-12-17,Tyneside, United Kingdom,Mid senior,Onsite,"Role: Data Engineer
Location: UK, Remote
As an Arcadian, you already help us deliver world leading sustainable design, engineering, and consultancy solutions for natural and built assets. You are part of our global business comprising 36,000 people, in over 70 countries, dedicated to improving quality of life.
Everyone has an important role to play. With the power of many curious minds, together we can continue to solve the world’s most complex challenges and deliver more impact together.
Role description:
The Data Engineer is responsible for migrating the data systems, delivery of the technical leadership in data migration, support of data migration strategies, tools, design and build. As well as formulating various methods to transform raw data into useful data systems. Overall, you’ll strive for efficiency by aligning data systems with business goals. This includes data ETL and modelling pipelines, hosting environments, automated testing and deployment, information storage, and retrieval and support enabling infrastructure across multiple client projects.
The Data Engineer will execute migration projects and may be a coach to other Junior Engineers within the Service Engineering Group (SEG). They will work together with Data architects, Data Consultants, and QA Consultants to enable technical solutions throughout the project lifecycle, including assessments, tool evaluations, demonstrations, requirements gathering, results presentation.
Role accountabilities:
Support and own the end-to-end migration journey on cloud-based products.
Technical delivery of ETL data migration with framework agreed.
Execute data migration runs.
Perform data migration using Snowflake, Dbt and Airflow or similar toolset.
Analyse and organize raw data; build data systems and pipelines.
Assess the compatibility with your cloud destination.
Ensure that the design fully meets the client’s needs.
Manage outputs and quality throughout delivery of a project ensuring all work packages align to the project scope.
Qualifications & Experience:
The ideal candidate will have a background in computer science and/or data engineering, ideally within a data migration consultancy or physical asset owning company context. They will have good leadership, communication, and organisational skills. They will have a strong understanding of how to deploy and scale machine learning and advance analytics techniques in a modern cloud environment. A keen interest in the latest technologies is important, to help ensure that the product (as well as Gen in general) stays on the cutting edge.
More importantly than these technical skills, however, we are looking for someone to be a data solution leader, pushing forward the boundaries of what we do, and the standards expected of the wider Gen data architecture. The successful candidate will need to be resilient and maintain a positive attitude when faced with challenges, be an excellent problem-solver, and be happy to be out of their comfort zone. They will need to have excellent communication skills, not just at a technical level, but also with more business-focused colleagues. They will not shy away from a challenge and will stand up and be counted. They will never be satisfied, and always look to find the next improvement in what Gen does, and how we do it.
We are looking for someone who possesses:
Must haves:
Experience in cloud-based data migration tools and validations
Experience using AWS and/or Azure
Experience using the following tools: Snowflake & DBT
Excellent understanding of SQL
Familiarity with DevOps processes/best practices
Part of assessment, tool options and data migration enablement
Experience of delivery of client facing projects, ideally in a data migration consultancy
Knowledge of programming languages (e.g., Java and Python)
You have experience in the use of automated testing, CI/CD, the use and creation of APIs and of virtual package environments to ensure repeatability of code.
Demonstrable skills in database management
Desirable:
Experience using Airflow
Experience using Azure Data Factory
Continue your career journey as an Arcadian.
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You do meaningful work, and no matter where your next role in Arcadis takes you, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark; on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We believe that by working together diverse people with different experiences develop the most innovative ideas. Equality, diversity and inclusion is at the heart of how we improve quality of life and we work closely with our people across six ED&I Workstreams: Age, Disability, Faith, Gender, LGBT+ and Race. A diverse and skilled workforce is essential to our success.
Show more
Show less","Data Engineering, Data Migration, Data Systems, ETL Pipelines, Data Analysis, Data Modelling, Snowflake, Dbt, Airflow, SQL, DevOps, Java, Python, Automated Testing, CI/CD, APIs, Database Management, AWS, Azure, Azure Data Factory","data engineering, data migration, data systems, etl pipelines, data analysis, data modelling, snowflake, dbt, airflow, sql, devops, java, python, automated testing, cicd, apis, database management, aws, azure, azure data factory","airflow, apis, automated testing, aws, azure, azure data factory, cicd, data engineering, data migration, data modelling, data systems, dataanalytics, database management, dbt, devops, etl pipelines, java, python, snowflake, sql"
Data Cabling Engineer- Contract,Digital Waffle,"Warwick, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-contract-at-digital-waffle-3765286570,2023-12-17,Tyneside, United Kingdom,Mid senior,Onsite,"Position: Data Cabling Engineer (Temp/Contract/Freelance)
Department: Information Technology / Network Infrastructure
Location: Warwick
Rate: 180-200 p/d
Contract: 6 Months
Job Summary: We are seeking a skilled and dedicated Data Cabling Engineer. The successful candidate will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables within metal trunking and containment to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labelling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labelling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high-quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices.
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations.
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labelling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle.
If you are a motivated Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Joe on joe@digitalwaffle.co.uk
Show more
Show less","Cat6, Cat6a, Copper cabling, Cabling installation, Cable pathways, Cable connectors, Cable termination, Cable labelling, Cable testing, Cable troubleshooting, Network expansion, Safety protocols, TIA/EIA standards, ISO/IEC standards, RJ45 connectors, Punchdown tool, Cable tester, Cable certifier, Tone generator, Cable labels, Cable markers, Label printer, Tape measure, Ruler, Level, Cable ties, Velcro straps, Cable clips, Cable mounts, Power drill, Screwdrivers, Safety glasses, Work gloves, Tool bag, Notepad, Pen, Mobile device, Personal Protective Equipment (PPE), Steeltoed boots, Hard hat, Cable fish tape, Cable lubricant, Cable toner","cat6, cat6a, copper cabling, cabling installation, cable pathways, cable connectors, cable termination, cable labelling, cable testing, cable troubleshooting, network expansion, safety protocols, tiaeia standards, isoiec standards, rj45 connectors, punchdown tool, cable tester, cable certifier, tone generator, cable labels, cable markers, label printer, tape measure, ruler, level, cable ties, velcro straps, cable clips, cable mounts, power drill, screwdrivers, safety glasses, work gloves, tool bag, notepad, pen, mobile device, personal protective equipment ppe, steeltoed boots, hard hat, cable fish tape, cable lubricant, cable toner","cable certifier, cable clips, cable connectors, cable fish tape, cable labelling, cable labels, cable lubricant, cable markers, cable mounts, cable pathways, cable termination, cable tester, cable testing, cable ties, cable toner, cable troubleshooting, cabling installation, cat6, cat6a, copper cabling, hard hat, isoiec standards, label printer, level, mobile device, network expansion, notepad, pen, personal protective equipment ppe, power drill, punchdown tool, rj45 connectors, ruler, safety glasses, safety protocols, screwdrivers, steeltoed boots, tape measure, tiaeia standards, tone generator, tool bag, velcro straps, work gloves"
Senior Data Analyst - Hybrid On-Site - Newcastle,BBC,"Newcastle upon Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-hybrid-on-site-newcastle-at-bbc-3781604314,2023-12-17,Tyneside, United Kingdom,Mid senior,Onsite,"Package
Band: D
Contract type: Permanent
Location: Hybrid working (part home/part office). Office base in Newcastle
Closing Date: 08/01/2024
We’re happy to discuss flexible working. Please indicate your choice under the flexible working question in the application. There is no obligation to raise this at the application stage but if you wish to do so, you are welcome to. Flexible working will be part of the discussion at offer stage
Excellent career progression – the BBC offers great opportunities for employees to seek new challenges and work in different areas of the organisation.
Unrivalled training and development opportunities – our in-house Academy hosts a wide range of internal and external courses and certification.
Benefits- We offer a competitive salary package, a flexible 35-hour working week for work-life balance and 26 days (1 of which is a corporation day) with the option to buy an extra 5 days, a defined pension scheme and discounted dental, health care, gym and much more.
Introduction
The BBC has been serving audiences online for a quarter of a century. Across key products including iPlayer, Sounds, Bitesize, BBC News and BBC Sport, we entertain, educate and inform audiences in their millions every day.
Behind the scenes, we are making the shift from broadcasting at our audiences to a service shaped by them and designed around their wants and needs. We are creating personalised products and services that bring the right content to the right people at the right times — a personalised BBC. It will be our greatest leap since iPlayer, and that’s why it is right at the top of our agenda.
Data is fundamental to our future and in Product Analytics we help shape that future. We work across all key BBC products meaning that our portfolio is as diverse as we are, giving us the opportunity to work with one of the broadest and most exciting datasets in the UK. We harness this data to understand the experiences and needs of our audiences, providing the data-driven insights that help Product Group create richer, more personalised experiences that our audiences love, keeping people coming back to the BBC time after time.
As a Senior Product Analyst, you will take ownership of delivering actionable insights within your area, working closely with teams across the business to support key strategic projects and day-to-day decision making. This is a chance to really make an impact. Through a combination of discovery work, in-depth analysis and reporting you will shape the future of our digital products right across the business, deepening our understanding of our audiences and offering essential insight on how to maximise the value we bring them.
Main Responsibilities
Working Within The Product Analytics Team You Will
Build a clear understanding of our audiences’ experience and needs to guide product design and development
Work closely with product managers, engineers and business analysts, championing data and taking the lead in understanding problem areas and working collaboratively on solutions
Develop and mentor members of the wider team
Support decision making across the Product Group by gaining a broad understanding of our Product portfolio and working flexibly to support business goals
Build and maintain dashboards and reports to facilitate data-led decision making
Support experimentation by working with product and engineering teams to set up and run experiments, and analyse and draw insight from the results
Proactively look for opportunities to optimise audience experience and drive engagement
Work to improve and shape our data analysis capabilities through automating data pipelines and working to understand and resolve any data issues
Advocate for the value of data in driving effective decision making
Are you the right candidate?
Essential
Significant experience in an analytical role, preferably in digital products
Proactive self-starter, focused on working strategically and for maximum value
Advanced SQL skills and experience working with very large and complex datasets
A continual focus on learning and development and a proven ability to mentor and develop others
Excellent time management skills and the ability to prioritise effectively
A focus on collaboration and thrives working as part of a cross-functional team
Excellent data visualisation skills and experience working with data visualisation tools such as Tableau
Working knowledge of how digital products use experimentation and experience analysing results of A/B tests
Desirable
Knowledge of R or Python
Experience working with on-demand audio media products
Familiarity with agile or other rapid application development methods
Experience with Data Science & Machine Learning
Understanding of data pipelines and/or data modelling
Experience with Optimizely integrations and tooling
Show more
Show less","SQL, Data visualization, Data analysis, Data pipelines, Data modeling, A/B testing, Experimentation, Tableau, R, Python, Agile, Rapid application development, Machine learning, Optimizely","sql, data visualization, data analysis, data pipelines, data modeling, ab testing, experimentation, tableau, r, python, agile, rapid application development, machine learning, optimizely","ab testing, agile, dataanalytics, datamodeling, datapipeline, experimentation, machine learning, optimizely, python, r, rapid application development, sql, tableau, visualization"
Data Analyst (Temp),The Recruitment Crowd (TRC Group),"Shildon, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-temp-at-the-recruitment-crowd-trc-group-3779276423,2023-12-17,Tyneside, United Kingdom,Mid senior,Onsite,"TRC are looking for a
temporary
Data Analyst to work within our clients IT department. Purpose of role is data cleansing from old ERP system to new ERP (Training provided and SOPs)
Job Description
High level attention to detail is a must
Gather and analyze data
Follow Standard operating procedures
Not afraid to ask questions to ensure full understanding
Managing workload and prioritizing
Computer literate
Monday to Thursday - 08:30 – 17:00
Friday - 08:30 - 16:30
Benefits
Company pension
Free parking
On-site parking
Experience
Administrative experience: 1 year (required)
About TRCGroup
We break the mould when it comes to recruitment agencies. With a straight-forward approach, we know how to get results for both our candidates and clients. As a multi-vertical recruitment agency, we source candidates for both temporary and permanent contracts across a range of disciplines.When you choose to work with The Recruitment Crowd, you’re choosing a team of devoted recruitment consultants who will only find the perfect fit for you.So if you’re looking for a recruitment agency in Leeds that provides a reliable service and trusted advice, The Recruitment Crowd have you covered. As far as recruitment agencies go, we’re a bit different! We’re friendly and talented and know our stuff!A job shouldn’t be a drag and that’s why we listen to individual candidate needs and match them perfectly to our clients. We invest in every candidate, addressing individual goals to ensure everyone comes out on top. That means reliable, professional, no bull recruitment.
Show more
Show less","Data Analysis, Data Cleansing, ERP Systems, Standard Operating Procedures, Workload Management, Prioritization, Computer Literacy","data analysis, data cleansing, erp systems, standard operating procedures, workload management, prioritization, computer literacy","computer literacy, dataanalytics, datacleaning, erp systems, prioritization, standard operating procedures, workload management"
Senior Database Engineer (POLAND Remote),Jobs for Humanity,"Newcastle upon Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-database-engineer-poland-remote-at-jobs-for-humanity-3773355765,2023-12-17,Tyneside, United Kingdom,Mid senior,Remote,"Company Description
Jobs for Humanity is partnering with TurnItIn to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: TurnItIn
Job Description
Company Description
- Must be located in Poland. - 100% remote work opportunity. - Turnitin is a recognized innovator in the global education space. - We have partnered with educational institutions for over 20 years to promote honesty, consistency, and fairness. - Our services are used by over 16,000 academic institutions, publishers, and corporations worldwide. - We have offices in different countries and a diverse community of colleagues dedicated to making a difference in education.
Job Description
- We are seeking an experienced Senior Data Engineer with a problem-solving mindset. - Your role will involve working with PostgreSQL/SQLServer clusters containing numerous databases. - These data stores can be large and include on-premises and cloud storage systems like Aurora, Redis, Memcached, Redshift, CockroachDB, DynamoDB, and Cassandra. - As a senior member of the team, you will collaborate with the DevOps and Engineering teams to ensure the automation and stability of our data stores. - Key responsibilities include protecting and administering data stores, participating in on-call rotations, and ensuring the availability and performance of our global data stores. - We value your input and encourage you to communicate ways to improve our processes constructively.
Qualifications
- We are looking for someone passionate about data stores and with senior-level experience in security, disaster recovery, and high availability. - You should thrive in a fast-paced environment and handle multiple priorities. - Good interpersonal skills are important, as we value an approachable and friendly demeanor. - Deep Linux experience and strong SQL skills are required. - Expertise in PostgreSQL/SQLServer tuning and best practices is necessary. - Experience with AWS, including Terraform, and knowledge of automated monitoring and alerting systems for on-premises and cloud data technologies like Aurora, Redshift, Redis, CockroachDB, and Cassandra, is desired. - Familiarity with Kubernetes and configuration management tools such as Puppet and Terraform is advantageous.
Additional Information
- No agency submissions. - Our mission is to ensure the integrity of global education and improve learning outcomes. - Our values guide everything we do, including customer-centricity, passion for learning, integrity, action and ownership, one team mindset, and a global outlook. - We offer global benefits including flexible/remote working, healthcare coverage, tuition reimbursement, competitive paid time off, self-care days, national holidays, charity contribution match, wellness reimbursement, access to mental health platforms, parental leave, and retirement plans. - We believe that skills evolve over time and encourage candidates to apply even if they don't meet all requirements. - Turnitin is committed to equal opportunities, providing equal access to programs, facilities, and employment regardless of age, race, color, religion, sex, sexual orientation, gender identity, disability, national origin, or veteran status. Thank you for considering joining our team at Turnitin. If you have any questions or would like to apply, please feel free to reach out using the contact information below.
Show more
Show less","PostgreSQL, SQLServer, Aurora, Redis, Memcached, Redshift, CockroachDB, DynamoDB, Cassandra, DevOps, Linux, SQL, AWS, Terraform, Kubernetes, Puppet","postgresql, sqlserver, aurora, redis, memcached, redshift, cockroachdb, dynamodb, cassandra, devops, linux, sql, aws, terraform, kubernetes, puppet","aurora, aws, cassandra, cockroachdb, devops, dynamodb, kubernetes, linux, memcached, postgresql, puppet, redis, redshift, sql, sqlserver, terraform"
Data Engineer,BJSS,"Newcastle upon Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-3253818861,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
About the Role
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","Python, AWS, Azure, GCP, CD/CI, SQL, NoSQL, Data Science, Machine Learning, Data Analytics, Cloud Computing, Agile, DataOps, Data Pipelines, Data Warehousing, Data Engineering, Software Engineering, Data Quality, Data Integration, Data Governance, Data Architecture, ETL, ELT, Data Lakes, Big Data, Hadoop","python, aws, azure, gcp, cdci, sql, nosql, data science, machine learning, data analytics, cloud computing, agile, dataops, data pipelines, data warehousing, data engineering, software engineering, data quality, data integration, data governance, data architecture, etl, elt, data lakes, big data, hadoop","agile, aws, azure, big data, cdci, cloud computing, data architecture, data engineering, data governance, data integration, data lakes, data quality, data science, dataanalytics, dataops, datapipeline, datawarehouse, elt, etl, gcp, hadoop, machine learning, nosql, python, software engineering, sql"
AWS Data Engineer,In Technology Group,"Newcastle-upon-Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/aws-data-engineer-at-in-technology-group-3775366022,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"Data Engineer - £60,000 - Newcastle (Hybrid Options)
My client is looking to bring in a new Data Engineer to play a central role in our efforts in building and maintaining robust data pipelines within the AWS ecosystem.
This role will be based from my clients Newcastle office and hybrid working models/flexible working can be discussed!
Responsibilities:
Data Ingestion:
Build and optimize data ingestion pipelines using AWS services such as Lambda, Kinesis, and Glue.
Data Transformation:
Develop, maintain, and optimize transformation pipelines within our Delta Lake on Databricks.
Data Modelling
: Design and implement efficient and reliable data models to store and retrieve data.
Collaboration:
Work closely with data scientists, analysts, and stakeholders to understand data requirements and deliver optimal solutions.
Data Quality:
Ensure the reliability, integrity, and timeliness of data by implementing best practices and quality checks.
Requirements:
AWS (Lambda, Kinesis, & Glue)
Databricks & Delta Lake (essential)
Python
SQL
Scala
Previous ETL experience
Data Modelling
A certification in AWS is highly desirable!
Benefits
22 days holiday rising to 25
Staff discounts & Friends and Family discounts
Cycle to work scheme and Tech Scheme
Breakfast and drinks provided
Charity day per annum supported
Summer and Christmas Parties
Street food days
Access to Perkbox portal
If the above role is of interest and you’ve got the required experience, apply with your most up to date CV for immediate consideration and interview!
Alternatively, if you would like to find out more about the position, reach out to me on 0113 526 6892 or at ethan.chesterfield@intechnologygroup.com
Show more
Show less","AWS, Lambda, Kinesis, Glue, Delta Lake, Databricks, Python, SQL, Scala, ETL, Data Modelling","aws, lambda, kinesis, glue, delta lake, databricks, python, sql, scala, etl, data modelling","aws, data modelling, databricks, delta lake, etl, glue, kinesis, lambda, python, scala, sql"
Senior Data Engineer,WOLF,"Cramlington, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-wolf-3767613169,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"About You.
Are you an experienced Data Engineer who wants to design and build Data pipelines within a scaling-up digital entertainment app that’s at the forefront of the new ""online"" way of socialising?
Are you looking to work somewhere that you're part of a team who take pride in working together?
Does working within a team who are straightforward, constantly challenging themselves and always learning with a genuine love for what they’re creating appeal?
We’re looking for someone who is energetic and proactive who is fascinated by modern data stacks and enjoys digging into the schemas and file formats.
Who are we?
WOLF is a fast-growing community-orientated platform that enables communities in our core MENA market to create entertaining audio content and programmes – so users can participate, enjoy and build friendships. WOLF is a space for freedom and creativity in your online life—whether that’s participating in live shows, chatting, or discovering talent – all wrapped up in a fun community.
The role
We’re looking for someone to help us create data pipelines and shape the data for our fascinating and unique chat and entertainment app for iOS, Android and the web.
The ideal candidate will have a background in software engineering, experience in an analytics field, working with multiple data sets to help generate data-driven decisions.
Ideally you would be familiar with the modern data stack, data warehouse solutions (BigQuery) and the world of mobile and web apps.
You will also be responsible for one data engineer and will manage them day to day and support their development.
Responsibilities
● Implement new technology to leverage data across the business
● Partner with BI team, marketing/community teams, and finance team to solve problems, and design data processes
● Design, manage, and support the data infrastructure, alongside implementing principles and processes to ensure that the platform remains highly automated, self-servicing and able to scale as we continue to grow.
You’ll work across the following four key areas:
1. ETL/ELT
● Connecting various data sources to our data warehouse solution in Google BigQuery
● Developing and implementing procedures for secure and effective data management.
● Implementing data definitions, data mappings and providing support across ongoing integration activities
2. Data warehousing
● Creating data transformation pipelines in Dataform and generating reports tables for analytics
● Writing SQL unit tests for data transformation scripts
● Running regular quality checks on data
3. Business intelligence
● Working with our BI team to deliver data engineering support for existing BI reports
● Working collaboratively with software engineering teams to understand and define application and platform data requirements.
4. Data infrastructure
● Managing our data ingestion pipelines with infrastructure as code
● Managing data transformation pipelines with SQL and Git
● Providing recommendations on data architecture and process improvements on an ongoing basis
The successful candidate will need to be highly technical, follow a hands-on approach, and have extensive experience managing and analysing data.
Requirements
● Experience with the Cloud, ideally AWS with a desire to learn Google Cloud
● Experience with data warehouse solutions (BigQuery)
● Previous experience of working with products that have regular releases through automated CI / CD processes
● An Agile background with experience in the use of tools such as Jira, Confluence etc
● Solid experience working with ELT/ETL tools
● Have an in-depth understanding of database structures and principles.
● Experience troubleshooting application and data platform related incidents through to resolution
● Experience working with and developing continuous integration (CI) and continuous development (CD) environments
Required Technical Knowledge
● Good working knowledge of AWS services (ECS, EC2, Kinesis, etc.)
● Familiar with serverless, event-driven technology such as AWS Lambda
● Hands-on experience working with databases, i.e. AWS MySQL, DynamoDB.
● Experience working with BigQuery data warehouse.
● Experience with Node.JS is a must.
● Experience of scripting languages such as Python is highly desirable.
● Understanding of Cloud Infrastructure as a Code (AWS Cloudformation, Terraform).
● The ability and appetite to learn and use a wide variety of open-source technologies and tools
About WOLF
WOLF is the unique Audio Entertainment Communities platform that enables people to create entertaining audio content and programmes – so users can participate, enjoy and build friendships.
We bring together a lively and interactive community of producers, show hosts, radio presenters, singers, poets, comedians, musicians, DJs, and fans. We have stunning stages across thousands of different groups hosting live and interactive audio shows and festival events, as well as after-show gatherings and socialising. Entertainment is how we stand out, built on a foundation of friendship, community, and supporting emerging talent.
WOLF is a place where everyone can be their true self.
At WOLF, everyone is appreciated equally for being themselves. It’s even built into our Brand Values! We’re creating a work environment that is truly representative of our diverse society, where every colleague feels respected and is able to give their best when at work. And when we have happy employees, we can then deliver the best experience to our users.
Benefits
● Working alongside a brilliant team - with a culture of excellence and innovation.
● 33 days holiday - with increases for long term service.
● Flexible/hybrid working - minimum 2 days in the office (if office based).
● Early finish Fridays.
● Flexibility over Christmas and New Year.
● Perkbox Subscription - over 1,000 perks and discounts.
● Contributory Pension Scheme.
● Octopus EV Scheme - tax-efficient electric car leasing via salary sacrifice.
● Cycle to Work Scheme.
WOLF is committed to encouraging equality, diversity and inclusion among our workforce, and eliminating unlawful discrimination where it may exist. The aim is for our workforce to be truly representative of all sections of society and for each colleague to feel respected and able to give their best when at work.
Show more
Show less","SQL, AWS Lambda, AWS ECS, AWS EC2, AWS Kinesis, AWS CloudFormation, Terraform, Cloud Infrastructure as a Code, NoSQL, MySQL, DynamoDB, Node.JS, Python, Dataform, Jenkins, Jira, Confluence, BigQuery, Google Cloud, AWS, ELT/ETL, Data Warehousing, Data Infrastructure, Data Science, Data Analytics, BI (Business Intelligence), Data Processing, Data Integration, Data Security, Data Governance, Data Management, Data Analysis, DataDriven Decision Making, Team Collaboration, Agile Development, CI/CD (Continuous Integration/Continuous Delivery)","sql, aws lambda, aws ecs, aws ec2, aws kinesis, aws cloudformation, terraform, cloud infrastructure as a code, nosql, mysql, dynamodb, nodejs, python, dataform, jenkins, jira, confluence, bigquery, google cloud, aws, eltetl, data warehousing, data infrastructure, data science, data analytics, bi business intelligence, data processing, data integration, data security, data governance, data management, data analysis, datadriven decision making, team collaboration, agile development, cicd continuous integrationcontinuous delivery","agile development, aws, aws cloudformation, aws ec2, aws ecs, aws kinesis, aws lambda, bi business intelligence, bigquery, cicd continuous integrationcontinuous delivery, cloud infrastructure as a code, confluence, data governance, data infrastructure, data integration, data management, data processing, data science, data security, dataanalytics, datadriven decision making, dataform, datawarehouse, dynamodb, eltetl, google cloud, jenkins, jira, mysql, nodejs, nosql, python, sql, team collaboration, terraform"
"Lead Azure Data Engineer - Washington/Hybrid - £90,000",Energy Jobline,"Durham, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-azure-data-engineer-washington-hybrid-%C2%A390-000-at-energy-jobline-3773345183,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"I am working with a well-established high street retailer based in the North East who are looking for a Lead Data Engineer to join their team. In this role you will be hands on leading on the development of the organisations Azure data platform whilst also managing the close-knit data engineering team.
You will join the organisation at a significant point in their data journey and a time of significant investment in its data function. They are looking to revolutionise the way they use data, and you will have the opportunity to implement best practice across the department. You will be responsible for designing and implementing the technical roadmap as the business look to develop their Azure Data Platform
This is a salaried role paying up to £90,000 per year depending on experience. This is a hybrid role based out of the organisation's office in Washington. This role is also eligible for the organisation's benefits package which includes generous holidays allowance, competitive pensions scheme and other benefits.
To be successful in this role you will have the following experience.
Previous experience within a Lead or Senior Data Engineer role
A strong understanding of data architecture concepts
Experience working with Azure technologies such as Data Lakes, Data Factory, Databricks, Synapse Analytics
Strong ETL experience
Experience as a team lead or mentoring other members of the team
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now!
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment
Show more
Show less","Azure, Data Lakes, Data Factory, Databricks, Synapse Analytics, ETL, Data architecture, Data engineering","azure, data lakes, data factory, databricks, synapse analytics, etl, data architecture, data engineering","azure, data architecture, data engineering, data factory, data lakes, databricks, etl, synapse analytics"
Data Engineer Python SQL AWS,Client Server,"Tyne & Wear, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-python-sql-aws-at-client-server-3783915484,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"Data Engineer (Python SQL AWS) Sunderland / WFH to £68k
Are you a data technologist? You could be progressing your career within a relaxed, supportive team environment at a tech driven online gaming / low-cost gambling SaaS tech company that provide a range of white labelled gaming platforms for household names with millions of concurrent players.
You will join a small team of Data Engineers responsible for implementing methods to improve data reliability and quality, combining raw information from a variety of different sources to create consistent and machine readable formats as well as developing the data infrastructure that enables data extraction and transformation for predictive or prescriptive modelling.
You'll have a broad scope of responsibilities including data ingestion, data transformation, data storage, ETL, data modelling, data quality and governance, data integration, performance tuning, scalability, resilience, security, tooling and technology. You'll take a senior role and be able to contribute to continual process and technology improvements.
WFH Policy:
There's a hybrid work from home policy with 2-3 days a week; when you're in the office you'll be collaborating with fellow technologists in a relaxed environment in awesome custom built offices in Sunderland with a range of facilities and perks including free meals at the onsite restaurant as well as membership at onsite gym.
Requirements:
You are a Data Engineer with strong experience of data models, data mining and segmentation techniques
You have coding skills with Python (or C#) and SQL
You have experience with SQL databases (e.g. Redshift, PostgreSQL)
You have experience with data tooling (e.g. Airflow, DBT, AWS Kinesis)
You have strong analysis and problem solving skills
You're collaborative with excellent communication skills
What's in it for you:
Competitive salary to £68k depending on experience and knowledge
Continual training, learning and career development opportunities
Bonus
Pension
Private medical care
And a range of other perks and benefits
Apply now
to find out more about this Data Engineer (Python SQL AWS) opportunity.
At Client Server we believe in a diverse workplace that allows people to play to their strengths and continually learn. We're an equal opportunities employer whose people come from all walks of life and will never discriminate based on race, colour, religion, sex, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The clients we work with share our values.
Show more
Show less","Data Engineering, Python, SQL, AWS, Redshift, PostgreSQL, Airflow, DBT, AWS Kinesis, Data Modeling, Data Mining, Data Segmentation, Data Extraction, Data Transformation, ETL, Data Quality, Data Governance, Data Integration, Performance Tuning, Scalability, Resilience, Security, Tooling, Technology, Data Infrastructure, Predictive Modeling, Prescriptive Modeling, Data Ingestion, Data Storage","data engineering, python, sql, aws, redshift, postgresql, airflow, dbt, aws kinesis, data modeling, data mining, data segmentation, data extraction, data transformation, etl, data quality, data governance, data integration, performance tuning, scalability, resilience, security, tooling, technology, data infrastructure, predictive modeling, prescriptive modeling, data ingestion, data storage","airflow, aws, aws kinesis, data engineering, data extraction, data governance, data infrastructure, data ingestion, data integration, data mining, data quality, data segmentation, data storage, data transformation, datamodeling, dbt, etl, performance tuning, postgresql, predictive modeling, prescriptive modeling, python, redshift, resilience, scalability, security, sql, technology, tooling"
Lead Data Engineer - Azure,Nigel Frank International,"Newcastle upon Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-azure-at-nigel-frank-international-3776381989,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"A well-established retailer are looking for a Lead Data Engineer to lead on the development of a brand-new data platform on Azure, whilst managing a small Data Engineering team. This is a hybrid role, and will require you to come into their office in Washington (Tyne & Wear) 2-3 days per week.
The role will involve line managing a small and growing Data Engineering team, managing efficient ways of working between functional and technical teams, providing guidance on best-practices and encouraging a high-performing a collaborative culture.
Given they are relatively early-on in their data journey, this is a really unique opportunity to make a real impact on their technical roadmap, and help introduce them to cutting-edge Azure technologies.
This role would be well-suited to an experienced Data Engineer or Developer who is looking to step up into a Management role, or a current manager who is ready for a fresh challenge.
Requirements:
Significant experience in Data Engineer or Business Intelligence development roles
Strong SQL development experience
Strong ETL experience
An understanding of data architecture concepts such as Data Warehouses, Data Lakes, Data Marts etc.
Experience with Azure data technologies such as Azure Data Factory, Data Lake, Synapse etc.
Any experience with Dynamics 365 would be beneficial but not essential
Either informal mentoring or formal leadership experience
Excellent communication and stakeholder management skills
Benefits:
Salary up to £90,000 depending on experience
34 days holiday, inclusive of bank holidays
Company pension scheme
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","Data Engineering, Data Platform, Azure, Data Warehouses, Data Lakes, Data Marts, ETL, SQL, Azure Data Factory, Azure Data Lake, Azure Synapse, Dynamics 365, Power BI","data engineering, data platform, azure, data warehouses, data lakes, data marts, etl, sql, azure data factory, azure data lake, azure synapse, dynamics 365, power bi","azure, azure data factory, azure data lake, azure synapse, data engineering, data lakes, data marts, data platform, data warehouses, dynamics 365, etl, powerbi, sql"
"Lead Azure Data Engineer - Washington/Hybrid - GBP90,000",Nigel Frank International,"Newcastle upon Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-azure-data-engineer-washington-hybrid-gbp90-000-at-nigel-frank-international-3769862480,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"Lead Azure Data Engineer - Washington/Hybrid - £90,000
I am working with a well-established high street retailer based in the North East who are looking for a Lead Data Engineer to join their team. In this role you will be hands on leading on the development of the organisations Azure data platform whilst also managing the close-knit data engineering team.
You will join the organisation at a significant point in their data journey and a time of significant investment in its data function. They are looking to revolutionise the way they use data, and you will have the opportunity to implement best practice across the department. You will be responsible for designing and implementing the technical roadmap as the business look to develop their Azure Data Platform
This is a salaried role paying up to £90,000 per year depending on experience. This is a hybrid role based out of the organisation's office in Washington. This role is also eligible for the organisation's benefits package which includes generous holidays allowance, competitive pensions scheme and other benefits.
To be successful in this role you will have the following experience.
Previous experience within a Lead or Senior Data Engineer role
A strong understanding of data architecture concepts
Experience working with Azure technologies such as Data Lakes, Data Factory, Databricks, Synapse Analytics
Strong ETL experience
Experience as a team lead or mentoring other members of the team
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now!
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure, Data Lakes, Data Factory, Databricks, Synapse Analytics, ETL, Data Engineering, Data architecture","azure, data lakes, data factory, databricks, synapse analytics, etl, data engineering, data architecture","azure, data architecture, data engineering, data factory, data lakes, databricks, etl, synapse analytics"
Lead Data Engineer - Azure,Nigel Frank International,"Newcastle upon Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-azure-at-nigel-frank-international-3763118419,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"A well-established retailer based in the North East of England are looking for a Lead Data Engineer to lead on the development of a brand-new data platform on Azure, whilst managing a small Data Engineering team.
Given they are relatively early-on in their data journey, this is a really unique opportunity to make a real impact on their technical roadmap, and help introduce them to cutting-edge technologies.
You will manage efficient ways of working between functional and technical teams, providing guidance on best-practices and encouraging a high-performing a collaborative culture.
This role would be well-suited to an experienced Data Engineer or Developer who is looking to step up into a Management role, or a current manager who is ready for a fresh challenge.
Requirements:
Significant experience in Data Engineer or Business Intelligence development roles
Strong SQL development experience
Strong ETL experience
An understanding of data architecture concepts such as Data Warehouses, Data Lakes, Data Marts etc.
Experience with Azure data technologies such as Azure Data Factory, Data Lake, Synapse etc.
Any experience with Dynamics 365 would be beneficial but not essential
Either informal mentoring or formal leadership experience
Excellent communication and stakeholder management skills
Benefits:
Salary up to £90,000 depending on experience
34 days holiday, inclusive of bank holidays
Company pension scheme
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","Data Engineering, Data Science, Data Architecture, ETL, Data Lakes, Data Marts, Data Warehouses, SQL, Azure Data Factory, Azure Data Lake, Azure Synapse, Power BI, Dynamics 365, Stakeholder Management, Data Platform, Communication Skills","data engineering, data science, data architecture, etl, data lakes, data marts, data warehouses, sql, azure data factory, azure data lake, azure synapse, power bi, dynamics 365, stakeholder management, data platform, communication skills","azure data factory, azure data lake, azure synapse, communication skills, data architecture, data engineering, data lakes, data marts, data platform, data science, data warehouses, dynamics 365, etl, powerbi, sql, stakeholder management"
Lead Data Engineer - Azure,Nigel Frank International,"Washington, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-azure-at-nigel-frank-international-3776383837,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"A well-established retailer are looking for a Lead Data Engineer to lead on the development of a brand-new data platform on Azure, whilst managing a small Data Engineering team. This is a hybrid role, and will require you to come into their office in Washington (Tyne & Wear) 2-3 days per week.
The role will involve line managing a small and growing Data Engineering team, managing efficient ways of working between functional and technical teams, providing guidance on best-practices and encouraging a high-performing a collaborative culture.
Given they are relatively early-on in their data journey, this is a really unique opportunity to make a real impact on their technical roadmap, and help introduce them to cutting-edge Azure technologies.
This role would be well-suited to an experienced Data Engineer or Developer who is looking to step up into a Management role, or a current manager who is ready for a fresh challenge.
Requirements:
Significant experience in Data Engineer or Business Intelligence development roles
Strong SQL development experience
Strong ETL experience
An understanding of data architecture concepts such as Data Warehouses, Data Lakes, Data Marts etc.
Experience with Azure data technologies such as Azure Data Factory, Data Lake, Synapse etc.
Any experience with Dynamics 365 would be beneficial but not essential
Either informal mentoring or formal leadership experience
Excellent communication and stakeholder management skills
Benefits:
Salary up to £90,000 depending on experience
34 days holiday, inclusive of bank holidays
Company pension scheme
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","Azure, SQL, ETL, Data Warehouses, Data Lakes, Data Marts, Azure Data Factory, Azure Data Lake, Azure Synapse, Data Engineering, Data Architecture, Dynamics 365, Data Platform, Communication, Stakeholder Management","azure, sql, etl, data warehouses, data lakes, data marts, azure data factory, azure data lake, azure synapse, data engineering, data architecture, dynamics 365, data platform, communication, stakeholder management","azure, azure data factory, azure data lake, azure synapse, communication, data architecture, data engineering, data lakes, data marts, data platform, data warehouses, dynamics 365, etl, sql, stakeholder management"
"Lead Azure Data Engineer - Washington/Hybrid - GBP90,000",Nigel Frank International,"Washington, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-azure-data-engineer-washington-hybrid-gbp90-000-at-nigel-frank-international-3769862407,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"Lead Azure Data Engineer - Washington/Hybrid - £90,000
I am working with a well-established high street retailer based in the North East who are looking for a Lead Data Engineer to join their team. In this role you will be hands on leading on the development of the organisations Azure data platform whilst also managing the close-knit data engineering team.
You will join the organisation at a significant point in their data journey and a time of significant investment in its data function. They are looking to revolutionise the way they use data, and you will have the opportunity to implement best practice across the department. You will be responsible for designing and implementing the technical roadmap as the business look to develop their Azure Data Platform
This is a salaried role paying up to £90,000 per year depending on experience. This is a hybrid role based out of the organisation's office in Washington. This role is also eligible for the organisation's benefits package which includes generous holidays allowance, competitive pensions scheme and other benefits.
To be successful in this role you will have the following experience.
Previous experience within a Lead or Senior Data Engineer role
A strong understanding of data architecture concepts
Experience working with Azure technologies such as Data Lakes, Data Factory, Databricks, Synapse Analytics
Strong ETL experience
Experience as a team lead or mentoring other members of the team
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now!
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure Data Platform, Data Engineering, Data Architecture, ETL, Data Lakes, Data Factory, Databricks, Synapse Analytics, Team Leading, Mentoring","azure data platform, data engineering, data architecture, etl, data lakes, data factory, databricks, synapse analytics, team leading, mentoring","azure data platform, data architecture, data engineering, data factory, data lakes, databricks, etl, mentoring, synapse analytics, team leading"
Lead Data Engineer - Azure,Nigel Frank International,"Sunderland, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-azure-at-nigel-frank-international-3776387527,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"A well-established retailer are looking for a Lead Data Engineer to lead on the development of a brand-new data platform on Azure, whilst managing a small Data Engineering team. This is a hybrid role, and will require you to come into their office in Washington (Tyne & Wear) 2-3 days per week.
The role will involve line managing a small and growing Data Engineering team, managing efficient ways of working between functional and technical teams, providing guidance on best-practices and encouraging a high-performing a collaborative culture.
Given they are relatively early-on in their data journey, this is a really unique opportunity to make a real impact on their technical roadmap, and help introduce them to cutting-edge Azure technologies.
This role would be well-suited to an experienced Data Engineer or Developer who is looking to step up into a Management role, or a current manager who is ready for a fresh challenge.
Requirements:
Significant experience in Data Engineer or Business Intelligence development roles
Strong SQL development experience
Strong ETL experience
An understanding of data architecture concepts such as Data Warehouses, Data Lakes, Data Marts etc.
Experience with Azure data technologies such as Azure Data Factory, Data Lake, Synapse etc.
Any experience with Dynamics 365 would be beneficial but not essential
Either informal mentoring or formal leadership experience
Excellent communication and stakeholder management skills
Benefits:
Salary up to £90,000 depending on experience
34 days holiday, inclusive of bank holidays
Company pension scheme
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","Data Engineer, Data Engineering, SQL, ETL, Data Warehouse, Data Lake, Data Mart, Azure Data Factory, Azure Data Lake, Azure Synapse, Dynamics 365, Mentoring, Leadership, Communication, Stakeholder Management","data engineer, data engineering, sql, etl, data warehouse, data lake, data mart, azure data factory, azure data lake, azure synapse, dynamics 365, mentoring, leadership, communication, stakeholder management","azure data factory, azure data lake, azure synapse, communication, data engineering, data lake, data mart, dataengineering, datawarehouse, dynamics 365, etl, leadership, mentoring, sql, stakeholder management"
Lead Azure Data Engineer,Nigel Frank International,"Washington, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-azure-data-engineer-at-nigel-frank-international-3761603750,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"A well-established retailer based in the North East of England are looking for a Lead Data Engineer to lead on the development of a brand-new data platform on Azure, whilst managing a small Data Engineering team.
Given they are relatively early-on in their data journey, this is a really unique opportunity to make a real impact on their technical roadmap, and help introduce them to cutting-edge technologies.
You will manage efficient ways of working between functional and technical teams, providing guidance on best-practices and encouraging a high-performing a collaborative culture.
This role would be well-suited to an experienced Data Engineer or Developer who is looking to step up into a Management role, or a current manager who is ready for a fresh challenge.
Requirements:
Significant experience in Data Engineer or Business Intelligence development roles
Strong SQL development experience
Strong ETL experience
An understanding of data architecture concepts such as Data Warehouses, Data Lakes, Data Marts etc.
Experience with Azure data technologies such as Azure Data Factory, Data Lake, Synapse etc.
Any experience with Dynamics 365 would be beneficial but not essential
Either informal mentoring or formal leadership experience
Excellent communication and stakeholder management skills
Benefits:
Salary up to £90,000 depending on experience
34 days holiday, inclusive of bank holidays
Company pension scheme
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","Data Engineering, Data Architecture, Data Warehouses, Data Lakes, Data Marts, Azure Data Factory, Data Lake, Synapse, SQL, ETL, Dynamics 365, Mentoring, Communication, Stakeholder Management","data engineering, data architecture, data warehouses, data lakes, data marts, azure data factory, data lake, synapse, sql, etl, dynamics 365, mentoring, communication, stakeholder management","azure data factory, communication, data architecture, data engineering, data lake, data lakes, data marts, data warehouses, dynamics 365, etl, mentoring, sql, stakeholder management, synapse"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Newcastle upon Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3733888255,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader in the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on Databricks. Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too.
You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration too. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks.
There is great room for progression within this role, and support and training will be provided as well.
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Data Engineering, Databricks, SQL, Python, PySpark, Azure Data Factory, Pipelines, Azure Data Factory, Communication","data engineering, databricks, sql, python, pyspark, azure data factory, pipelines, azure data factory, communication","azure data factory, communication, data engineering, databricks, pipelines, python, spark, sql"
Lead Azure Data Engineer,Nigel Frank International,"Newcastle upon Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-azure-data-engineer-at-nigel-frank-international-3761605520,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"A well-established retailer based in the North East of England are looking for a Lead Data Engineer to lead on the development of a brand-new data platform on Azure, whilst managing a small Data Engineering team.
Given they are relatively early-on in their data journey, this is a really unique opportunity to make a real impact on their technical roadmap, and help introduce them to cutting-edge technologies.
You will manage efficient ways of working between functional and technical teams, providing guidance on best-practices and encouraging a high-performing a collaborative culture.
This role would be well-suited to an experienced Data Engineer or Developer who is looking to step up into a Management role, or a current manager who is ready for a fresh challenge.
Requirements:
Significant experience in Data Engineer or Business Intelligence development roles
Strong SQL development experience
Strong ETL experience
An understanding of data architecture concepts such as Data Warehouses, Data Lakes, Data Marts etc.
Experience with Azure data technologies such as Azure Data Factory, Data Lake, Synapse etc.
Any experience with Dynamics 365 would be beneficial but not essential
Either informal mentoring or formal leadership experience
Excellent communication and stakeholder management skills
Benefits:
Salary up to £90,000 depending on experience
34 days holiday, inclusive of bank holidays
Company pension scheme
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","Data Engineer, Data Engineering, Data Warehouse, Data Lake, Data Mart, SQL, ETL, Azure Data Factory, Azure Data Lake, Azure Synapse, Dynamics 365, Mentoring, Leadership, Communication, Stakeholder Management","data engineer, data engineering, data warehouse, data lake, data mart, sql, etl, azure data factory, azure data lake, azure synapse, dynamics 365, mentoring, leadership, communication, stakeholder management","azure data factory, azure data lake, azure synapse, communication, data engineering, data lake, data mart, dataengineering, datawarehouse, dynamics 365, etl, leadership, mentoring, sql, stakeholder management"
Data Analyst - Power BI - SQL - Newcastle - Office Working,Nigel Frank International,"Newcastle upon Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-power-bi-sql-newcastle-office-working-at-nigel-frank-international-3737168310,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"I'm currently working with a Nationally renowned organisation who are looking to add to their Analytics division within their business. Established in the late 90's, they began as a local business who were looking to make a difference in the lives of as many local people who needed their support. They fast grew into one of the industries leaders and their continued success means that they are looking to continue to grow the business.
As the business grows, they fully understand that in order to remain at the top of their game, they need to utilise the latest and greatest technologies to help scale the business.
As part of this role, You will be working closely with stakeholders within the business to provide insights that will ultimately impact the strategy of the overall business. You will be immersed into the Azure world and getting exposure to a range of technologies that will allow you to present impactful data that can help to continue to shape the business.
This is a salaried position that is paying up to £40k with a good benefits package.
My client are looking for applications where candidates are happy to work in the office full time. A big part of their success has come through the collaborative nature of the business where they pride themselves on the constant knowledge sharing & inclusive nature of the business.
They are also all about personal development and career progression, so successful candidates will be having regular reviews so the business can understand their career aspirations but also constantly provide them with the platform to always develop their skillset.
I'm looking for experience in...
Strong SQL server experience
Strong background in working within a Analyst capacity
Strong stakeholder management experience
Experience of reporting with Power BI, SSRS, Tableau & similar reporting tools
This is a unrivalled opportunity for Data Analysts looking to join an organisation who appreciate the power of Data as well as the importance of continuous development.
We have limited interview slots as they are interviewing now so if you're interested in hearing more, get in touch ASAP!
To discuss in more detail, please send your CV to t.shahid@nigelfrank.com or alternatively, call Taz Shahid on 0191 338 7551
Key Skills: Data Analyst, Analyst, BI Developer, Developer, Power BI, Panintelligence, SQL, SQL Server, Business Intelligence, T-SQL, SSIS, SSRS, Store Procedures,Visualisation, Newcastle, Tyne & Wear
Show more
Show less","SQL, SQL Server, Data Analyst, Analyst, BI Developer, Developer, Power BI, Tableau, SSRS, SSIS, Store Procedures, Visualisation, Business Intelligence, TSQL","sql, sql server, data analyst, analyst, bi developer, developer, power bi, tableau, ssrs, ssis, store procedures, visualisation, business intelligence, tsql","analyst, bi developer, business intelligence, dataanalytics, developer, powerbi, sql, sql server, ssis, ssrs, store procedures, tableau, tsql, visualisation"
Lead Azure Data Engineer,Nigel Frank International,"Sunderland, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-azure-data-engineer-at-nigel-frank-international-3761605521,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"A well-established retailer based in the North East of England are looking for a Lead Data Engineer to lead on the development of a brand-new data platform on Azure, whilst managing a small Data Engineering team.
Given they are relatively early-on in their data journey, this is a really unique opportunity to make a real impact on their technical roadmap, and help introduce them to cutting-edge technologies.
You will manage efficient ways of working between functional and technical teams, providing guidance on best-practices and encouraging a high-performing a collaborative culture.
This role would be well-suited to an experienced Data Engineer or Developer who is looking to step up into a Management role, or a current manager who is ready for a fresh challenge.
Requirements:
Significant experience in Data Engineer or Business Intelligence development roles
Strong SQL development experience
Strong ETL experience
An understanding of data architecture concepts such as Data Warehouses, Data Lakes, Data Marts etc.
Experience with Azure data technologies such as Azure Data Factory, Data Lake, Synapse etc.
Any experience with Dynamics 365 would be beneficial but not essential
Either informal mentoring or formal leadership experience
Excellent communication and stakeholder management skills
Benefits:
Salary up to £90,000 depending on experience
34 days holiday, inclusive of bank holidays
Company pension scheme
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","Azure, Azure Data Factory, Azure Data Lake, Azure Synapse, BI (Business Intelligence), Cloud Computing, Data Architecture, Data Engineering, Data Lakes, Data Marts, Data Warehouse, Dynamics 365, ETL (Extract Transform Load), Leadership, Mentoring, Power BI, Python, SQL","azure, azure data factory, azure data lake, azure synapse, bi business intelligence, cloud computing, data architecture, data engineering, data lakes, data marts, data warehouse, dynamics 365, etl extract transform load, leadership, mentoring, power bi, python, sql","azure, azure data factory, azure data lake, azure synapse, bi business intelligence, cloud computing, data architecture, data engineering, data lakes, data marts, datawarehouse, dynamics 365, etl extract transform load, leadership, mentoring, powerbi, python, sql"
Lead Azure Data Engineer,Nigel Frank International,"Durham, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-azure-data-engineer-at-nigel-frank-international-3761602794,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"A well-established retailer based in the North East of England are looking for a Lead Data Engineer to lead on the development of a brand-new data platform on Azure, whilst managing a small Data Engineering team.
Given they are relatively early-on in their data journey, this is a really unique opportunity to make a real impact on their technical roadmap, and help introduce them to cutting-edge technologies.
You will manage efficient ways of working between functional and technical teams, providing guidance on best-practices and encouraging a high-performing a collaborative culture.
This role would be well-suited to an experienced Data Engineer or Developer who is looking to step up into a Management role, or a current manager who is ready for a fresh challenge.
Requirements:
Significant experience in Data Engineer or Business Intelligence development roles
Strong SQL development experience
Strong ETL experience
An understanding of data architecture concepts such as Data Warehouses, Data Lakes, Data Marts etc.
Experience with Azure data technologies such as Azure Data Factory, Data Lake, Synapse etc.
Any experience with Dynamics 365 would be beneficial but not essential
Either informal mentoring or formal leadership experience
Excellent communication and stakeholder management skills
Benefits:
Salary up to £90,000 depending on experience
34 days holiday, inclusive of bank holidays
Company pension scheme
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","Data Engineering, Data Platform, Azure, Data Warehouses, Data Lakes, Data Marts, Data Factory, ETL, SQL, Dynamics 365, Synapse, Leadership","data engineering, data platform, azure, data warehouses, data lakes, data marts, data factory, etl, sql, dynamics 365, synapse, leadership","azure, data engineering, data factory, data lakes, data marts, data platform, data warehouses, dynamics 365, etl, leadership, sql, synapse"
Principal Data Developer,Sage,"Newcastle upon Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/principal-data-developer-at-sage-3520105712,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"Job Description
Job Description
.About our Principal Data Developer role
As the Principal Data Developer you will be the technical lead across 4 functional delivery teams within our IT Data Team, mentoring our Data Developers and providing technical expertise as we continue to build and evolve our centralised Data Platform. We’re looking for someone with hands-on experience with data engineering and leading programs of work in designing and developing data platforms, particularly if that experience includes Snowflake and DBT technologies. In addition to the technical capabilities, you’ll need to be able to communicate equally effectively with technical and non-technical audiences, working closely with stakeholders and delivery teams to really understand business outcomes and turn them into technical requirements for the delivery teams.
Key Responsibilities
Summary of your day-to-day?
Key Responsibilities:
Experience in a technical leadership role; mentoring cross-functional delivery teams in the design and build of reliable, scalable data platform solutions.
Champion best practices like writing clean and reusable code, define good design principles and lead the delivery team in the adoption of standards and best practices.
Lead the peer reviews of colleague’s code, providing constructive feedback and developing technical capabilities in others.
Create and maintain relevant documentation to describe designs, coding/configuration, team processes etc.
Collaborate across the Principal Developer network, working together to set the standard for development practices within IT.
Motivate and inspire others to innovate, be an Evangelist for data technologies, ensuring the development teams use the right tools and technologies to deliver robust and efficient solutions.
Develop through self-learning and training to maintain a deep expertise that sets a standard across the team.
Requirements:
5+ years’ experience implementing complex data solutions
Strong technical background with experience in data engineering and cloud data platforms (particularly Snowflake & DBT).
Experienced mentor / coach; Ability to lead people on a journey and support less experienced colleagues in their career progression ambitions.
Dig deeper about who we are:
Who is Sage: https://www.sage.com/en-us/company/about-sage/
Life at Sage:  https://www.sage.com/en-us/company/careers/
Our Values & Behaviors:  https://www.youtube.com/watch?app=desktop&v=vt5JXf-Gwno&feature=youtu.be
How we make a difference:  https://www.sage.com/en-us/company/sage-foundation/
Sage Business Cloud - SaaS for Every Business:  https://www.sage.com/en-us/products/
Your benefits
Benefits video – https://www.youtube.com/watch?v=TCMtTYUUiuU
Comprehensive health, dental and vision coverage
Work away scheme for up to 10 weeks a year
On-going training and professional development
Paid 5 days yearly to volunteer through our Sage Foundation
Flexible work patterns and hybrid working
Show more
Show less","Data Engineering, Snowflake, DBT, Cloud data platforms, Mentoring, Code reviews, Documentation, Agile, Technical leadership, Communication","data engineering, snowflake, dbt, cloud data platforms, mentoring, code reviews, documentation, agile, technical leadership, communication","agile, cloud data platforms, code reviews, communication, data engineering, dbt, documentation, mentoring, snowflake, technical leadership"
Senior Data Engineer ETL | MySQL | PostgreSQL| Python | AWS,Energy Jobline,"Newcastle upon Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-etl-mysql-postgresql-python-aws-at-energy-jobline-3774719897,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"Job Title: Senior Data Engineer, Location: Newcastle, Salary: £45,000 - £60,000, ETL | SQL | T-SQL | Postgre SQL | AWS | Azure | Hadoop | Spark
We are looking for a Senior Data Engineer to join our client's Consultancy team in Newcastle. As our Senior Data Engineer, you will be responsible for designing, developing, and implementing data solutions to meet our clients' needs and ensure the highest quality of service is provided.
Responsibilities
Design, develop, and implement data solutions to meet our clients' needs
Identify and assess data-related opportunities for improvement
Work with a range of stakeholders to identify and implement data-driven solutions
Monitor data quality and performance
Troubleshoot and resolve data issues
Provide technical support to the team
Develop and maintain data architectures
Technical Skills
5+ years of experience in a Data Engineer role
Expertise in databases, data architectures, data modelling, and data warehousing
Proficiency in SQL and NoSQL
Experience with Data Visualisation tools (Tableau, Power BI, etc.)
Knowledge of Big Data technologies (Hadoop, Spark, etc.)
Knowledge of ETL tools (Informatica, Talend, etc.)
Ability to work independently and in a team
Excellent problem-solving skills
If you are a talented Senior Data Engineer and have the skillset and experience listed above, please apply for immediate consideration and interview. To receive further details about the role, please don't hesitate to get in touch.
Keywords: Senior Data Engineer, Data Engineer, Consultancy, Newcastle, SQL, NoSQL, Big Data, Data Architecture, Data Modelling, Data Visualisation, ETL
Job Title: Senior Data Engineer, Location: Newcastle, Salary: £45,000 - £60,000, ETL | SQL | T-SQL | Postgre SQL | AWS | Azure | Hadoop | Spark
For more information about Shift F5 and the opportunities we have to offer follow us on Twitter @F5_Jobs
Shift F5 Ltd is acting as an Employment Agency in relation to this vacancy
Show more
Show less","SQL, NoSQL, Hadoop, Spark, Informatica, Talend, Tableau, Power BI, AWS, Azure, Data architecture, Data modelling, Data visualization, ETL, Data warehousing","sql, nosql, hadoop, spark, informatica, talend, tableau, power bi, aws, azure, data architecture, data modelling, data visualization, etl, data warehousing","aws, azure, data architecture, data modelling, datawarehouse, etl, hadoop, informatica, nosql, powerbi, spark, sql, tableau, talend, visualization"
Data Engineer,Apexon,"Sunderland, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-apexon-3774189071,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"Apexon is a pure-play digital engineering services firm focused on helping companies accelerate their digital initiatives from strategy and planning through execution. We leverage deep technical expertise, Agile methodologies and data-driven intelligence to modernize systems of engagement and simplify human/tech interaction. We deliver custom solutions that meet customers’ technology needs wherever they are in their digital lifecycle. Backed by Goldman Sachs and Everstone Capital, Apexon works with both large enterprises and emerging innovators -- putting digital to work to enable new products and business models, engage with customers in new ways, and create sustainable competitive differentiation.
We are currently looking for a Lead Data Engineer to manage teams on client projects.
Your Profile:
Experience of managing small teams whilst also being hands on
Strong database fundamentals including SQL/TSQL, performance and schema design.
Experience building a Data Warehouse/Data Lake and data applications using Azure.
Technologies: Azure Data Factory, Azure Synapse, Azure Data Lake, Data Warehousing
ETL Pipelines and integrations with data sources, both in batch and streaming modes
Proficiency in working with APIs and integrating them into data pipelines
Knowledge of a scripting language (e.g. Python)
Data cleansing and curation e.g. Synapse notebooks, Databricks
Rudimentary understanding of Data Modelling
Visualization (PowerBI)
Good knowledge of data governance , data cataloguing, MDM
It would be great if you have:
Azure DP-900 certification (or above)
Azure AZ-900 certification (or above)
Integration ETL pipelines with Oracle databases
Integration with EPM tooling
Knowledge of organizational financial transaction data
We’re committed to providing our people with a great environment to work in. You can expect ongoing skills-based development, career progression as well as health & well-being benefits and support. You’ll work within a friendly and supportive team, working on a variety of projects and the chance to obtain relevant certifications along the way!
Show more
Show less","Data Engineering, Agile Methodologies, SQL/TSQL, Performance and Schema Design, Data Warehousing, Data Lake, Azure Data Factory, Azure Synapse, Azure Data Lake, ETL Pipelines, Data Sources, APIs, Scripting Languages, Python, Data Cleansing, Synapse Notebooks, Databricks, Data Modeling, Visualization, PowerBI, Data Governance, Data Cataloging, MDM, Azure DP900, Azure AZ900, Oracle Databases, EPM Tooling, Organizational Financial Transaction Data","data engineering, agile methodologies, sqltsql, performance and schema design, data warehousing, data lake, azure data factory, azure synapse, azure data lake, etl pipelines, data sources, apis, scripting languages, python, data cleansing, synapse notebooks, databricks, data modeling, visualization, powerbi, data governance, data cataloging, mdm, azure dp900, azure az900, oracle databases, epm tooling, organizational financial transaction data","agile methodologies, apis, azure az900, azure data factory, azure data lake, azure dp900, azure synapse, data cataloging, data engineering, data governance, data lake, data sources, databricks, datacleaning, datamodeling, datawarehouse, epm tooling, etl pipelines, mdm, oracle databases, organizational financial transaction data, performance and schema design, powerbi, python, scripting languages, sqltsql, synapse notebooks, visualization"
Data Engineer,Sentinel,"Newcastle-upon-Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-sentinel-3767949676,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"Data Engineer – Azure, Data Factory, Databricks, Python
Permanent
Hybrid working - 2-3 days a week in Newcastle office
£50,000 to £60,000 plus benefits
Do you have strong data engineering experience with Azure Data Services? Are you ready to work with the latest modern tech?
Sentinel are working with a food retailer who are looking for strong Azure Data Engineers to join their team and help to further the data platforms capabilities. This is an exciting time to join the company as they harness the latest techonlogy. The role would suit a mid-level Azure Data Engineer who has strong skills with Data Factory and Databricks
Skills & Experience:
We are looking for someone that has a strong mix of the following skills and experience:
· Strong experience utilising the Microsoft Azure stack including Logic Apps/Function Apps
· Core skills in coding with SQL and Python
· Strong Data Factory skills for ETL pipelines
· Experience with Databricks
Responsibilities:
As the Data Engineer you’ll be responsible for:
· Building complex data solutions
· Optimising data pipelines
· Supporting and collaborating with other teams
Data Engineering Consultant / Data Consultant / Data Warehouse Developer / ETL Developer / Data Engineer / Analytics Engineer /Azure Data Engineer / Data specialist
Your application with Sentinel
Sentinel is an award-winning technology recruitment and consulting company with offices in the UK, USA, Czech Republic, and Switzerland. We work with global brands, ambitious start-ups, and leading recruitment outsourcers, ensuring access to exceptional talent through permanent, contingent labour, and statement of work services.
Get in touch today for new opportunities, or guidance on sourcing strategies, salary and rate benchmarking.
We aspire to provide great service to all applicants. If we think you’re right for the role, we’ll be in touch as soon as possible. If not, we’ll keep you updated with other opportunities in your field.
To apply for this role or more information, please apply with your up to date CV below or email
ali.crowley@sentinelit.com
Show more
Show less","Azure, Data Factory, Databricks, Python, SQL, Logic Apps, Function Apps, ETL pipelines","azure, data factory, databricks, python, sql, logic apps, function apps, etl pipelines","azure, data factory, databricks, etl pipelines, function apps, logic apps, python, sql"
AWS Data Engineer,In Technology Group,"Newcastle-upon-Tyne, England, United Kingdom",https://uk.linkedin.com/jobs/view/aws-data-engineer-at-in-technology-group-3768705449,2023-12-17,Tyneside, United Kingdom,Mid senior,Hybrid,"Data Engineer - £60,000 - Newcastle (Hybrid Options)
My client is looking to bring in a new Data Engineer to play a central role in our efforts in building and maintaining robust data pipelines within the AWS ecosystem.
This role will be based from my clients Newcastle office and hybrid working models/flexible working can be discussed!
Responsibilities:
Data Ingestion:
Build and optimize data ingestion pipelines using AWS services such as Lambda, Kinesis, and Glue.
Data Transformation:
Develop, maintain, and optimize transformation pipelines within our Delta Lake on Databricks.
Data Modelling
: Design and implement efficient and reliable data models to store and retrieve data.
Collaboration:
Work closely with data scientists, analysts, and stakeholders to understand data requirements and deliver optimal solutions.
Data Quality:
Ensure the reliability, integrity, and timeliness of data by implementing best practices and quality checks.
Requirements:
AWS (Lambda, Kinesis, & Glue)
Databricks & Delta Lake (essential)
Python
SQL
Scala
Previous ETL experience
Data Modelling
A certification in AWS is highly desirable!
Benefits
22 days holiday rising to 25
Staff discounts & Friends and Family discounts
Cycle to work scheme and Tech Scheme
Breakfast and drinks provided
Charity day per annum supported
Summer and Christmas Parties
Street food days
Access to Perkbox portal
If the above role is of interest and you’ve got the required experience, apply with your most up to date CV for immediate consideration and interview!
Alternatively, if you would like to find out more about the position, reach out to me on 0113 526 6892 or at ethan.chesterfield@intechnologygroup.com
Show more
Show less","AWS, Lambda, Kinesis, Glue, Databricks, Delta Lake, Python, SQL, Scala, ETL, Data Modelling, AWS Certification","aws, lambda, kinesis, glue, databricks, delta lake, python, sql, scala, etl, data modelling, aws certification","aws, aws certification, data modelling, databricks, delta lake, etl, glue, kinesis, lambda, python, scala, sql"
"Manager of Construction, MLZ Data Center Construction",Amazon Web Services (AWS),"Jackson, MS",https://www.linkedin.com/jobs/view/manager-of-construction-mlz-data-center-construction-at-amazon-web-services-aws-3755082846,2023-12-17,Jackson,United States,Mid senior,Onsite,"Description
As our Data Center Manager of Construction (CM MGR), you will be a part of a diverse, upbeat, and creative team tasked with solving fascinating problems constructing Amazon Data Centers. Our data centers are industry-leading examples of energy efficient and cost-effective designs.
You Will
Own project scope, quality, schedule, and budget for construction of new builds or general capital projects.
Own construction project management and oversight of construction related activities.
Work alongside partner teams such as Operations, Networking, Controls, Security, and Commissioning to build Data Centers that directly support our Customers.
Be a leader in your specific discipline (construction management, building services, architectural, electrical or mechanical engineering).
Represent Amazon’s owner’s representative on construction sites daily, as, interacting with construction trades.
At Amazon, we are all Owners and leverage unique opportunities presented to us by owning everything from the design review to construction bidding, execution, and final hand-off to our customers. We develop innovative data centers for our Customers.
Key job responsibilities
Be Amazon’s construction representative on multiple data centers building and capital improvement construction project sites from construction start through hand-off to operations.
Direct interface with construction general contractors during the construction bidding, award, execution, punch-list, and closeout phases.
Create construction project Scope of Work (SOW) and Request for Proposals (RFP).
Conduct negotiations with general contractors and evaluate bids and proposals with detail and accuracy.
Manage and drive cost, schedule, and quality while managing construction contractors and vendors.
Perform construction project management activities, including management of documents, submittals, RFIs, change orders, invoices, quality, scope, and schedule.
Drive construction teams to troubleshoot and perform root-cause failure analysis on equipment failures.
Support commissioning and integrated system testing and oversight.
Support operations of installed facilities, including review of procedures, best practices, and maintenance initiatives.
Support capital request creation.
Analyze and report construction progress and financials.
Record and report key construction metrics to team members and management.
Contribute to construction initiatives aimed at improving construction efficiency and increasing data center resiliency.
Provide constructability reviews for civil, structural, electrical and mechanical designs for new or optimization of existing data centers.
Be a leader within the group, and within internal and external data center support teams.
About The Team
Portage, Indiana will NOT be the permanent reporting location. This is a temporary location until the final location in the Indiana region is finalized.
We are open to hiring candidates to work out of one of the following locations:
Jackson, MS, USA
Basic Qualifications
Bachelor’s degree in Mechanical Engineering, Electrical Engineering, Construction Management, or an equivalent engineering science plus 10 years of relevant construction experience, OR 14 years of relevant construction experience in lieu of a degree.
5+ years of experience directly managing, mentoring, leading, and coaching teams of construction professionals.
5+ years of experience hiring, promoting, disciplining, and developing teams of construction professionals.
6+ years of experience in construction management of large, complex projects involving large-scale mechanical, electrical, and plumbing (MEP) plants.
6+ years of general contractor and vendor management experience (request for proposals, bidding, change orders, quality control, and RFI and submittal tracking) associated with construction and project execution.
5+ years of experience managing construction of complex electrical and mechanical engineering systems, including large scale power distribution or generation, and cooling systems.
Preferred Qualifications
MS in Construction Management or Engineering (Mechanical, Electrical, Civil, Structural)
8+ Yrs. Directly managing, mentoring, leading, and coaching teams of construction professionals
10+ Yrs. Exp. Project Management and Vendor Management
5+ Yrs. Exp. In Data Center system-level architecture and electrical engineering principals
5+ Yrs. Exp. In Data Center system-level architecture and mechanical engineering principals
Experience designing data centers or critical MEP infrastructure
Certified as a Professional Engineer (PE), LEED, or Certified Construction Manager (CCM)
Prior AWS/Amazon experience
5+ Yr. Military Service
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.
Company
- Amazon Data Services, Inc.
Job ID: A2494963
Show more
Show less","Construction Management, Mechanical Engineering, Electrical Engineering, Data Center Design, MEP (Mechanical Electrical and Plumbing), Project Management, Vendor Management, Budgeting, Scheduling, Quality Control, RFP (Request for Proposals), Bidding, Change Orders, RFI (Request for Information), Submittal Tracking, Construction Bidding, Construction Execution, Construction Closeout, Constructability Reviews, LEED (Leadership in Energy and Environmental Design), CCM (Certified Construction Manager), AWS (Amazon Web Services)","construction management, mechanical engineering, electrical engineering, data center design, mep mechanical electrical and plumbing, project management, vendor management, budgeting, scheduling, quality control, rfp request for proposals, bidding, change orders, rfi request for information, submittal tracking, construction bidding, construction execution, construction closeout, constructability reviews, leed leadership in energy and environmental design, ccm certified construction manager, aws amazon web services","aws amazon web services, bidding, budgeting, ccm certified construction manager, change orders, constructability reviews, construction bidding, construction closeout, construction execution, construction management, data center design, electrical engineering, leed leadership in energy and environmental design, mechanical engineering, mep mechanical electrical and plumbing, project management, quality control, rfi request for information, rfp request for proposals, scheduling, submittal tracking, vendor management"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Toledo, OH",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783190194,2023-12-17,Monroe,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Toledo-DataScientist.014
Show more
Show less","Python, JavaScript, JSON, R, OOP, AI, Data science, Generative AI, English communication, Proactive communication, Priority management, Stakeholder management, Technology, Algorithms, Coaching, Data analytics","python, javascript, json, r, oop, ai, data science, generative ai, english communication, proactive communication, priority management, stakeholder management, technology, algorithms, coaching, data analytics","ai, algorithms, coaching, data science, dataanalytics, english communication, generative ai, javascript, json, oop, priority management, proactive communication, python, r, stakeholder management, technology"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Amherstburg, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3752012626,2023-12-17,Monroe,United States,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Visualization, Data Management, ETL, Data Cleaning, Data Manipulation","data analysis, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data visualization, data management, etl, data cleaning, data manipulation","ab testing, data cleaning, data management, data manipulation, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Quality Management Systems Data Analyst Co-Op,Amcor,"Manchester, MI",https://www.linkedin.com/jobs/view/quality-management-systems-data-analyst-co-op-at-amcor-3760170966,2023-12-17,Ogden,United States,Mid senior,Onsite,"We are global, we are impacting the lives of millions every day, we are making a difference!
At Amcor we are inspired to change the packaging industry and are taking on the aspirational challenge to make all our products recyclable or reusable by 2025. Through our products and global footprint, we are in a unique position to truly make a difference in the packaging space. Our products impact the lives of millions of people across the globe every day from food, beverage, pharmaceutical, medical, home- and personal-care, and other essential products. Amcor is a thriving S&P 500 organisation listed both on the NYSE (AMCR) and ASX (AMC) with US$13 billion in sales. We have a proud history dating back to the 1860’s and come a long way from a single paper mill based in Melbourne to becoming the global leading packaging company. Today, Amcor is an international organisation empowering its 50,000 employees in over 40 countries and across 250 sites.
Will you be next to join our journey towards a more sustainable future?
At Amcor we are always looking for talented and passionate individuals who are motivated to make a difference. Working at Amcor means you will have a unique opportunity to be a part of an organisation that is committed to providing sustainable packaging solutions. To find out more about our commitment to sustainability and about Amcor, visit www.amcor.com.
Job Description
About the Opportunity
:
Amcor Rigid Packaging is seeking a Quality Management Systems Data Analyst intern/co-op that will commence as early as November 2023, and will give the selected student; a challenging project-based work experience, the opportunity to learn about Amcor Rigid Packaging business, the packaging industry, and receive mentorship from business leaders. This role will report to North American Vice President of Quality Management, and have significant exposure internal processes, metrics, and analytical tools. The role will generate management level results reports that are used regularly by ARPNA business and leadership teams.
Internship Assignments
Assume mentored responsibility for routine monthly reporting of quality-related metrics
Use Amcor Data Analysis and Dashboarding tools based upon PowerBI to present results
Receive mentoring and coaching from experienced quality management professionals nearing retirement age on how to retrieve, analyze, report, and interpret results.
Offer insights to leadership on the metrics and associated trends.
Basic Qualifications
Quality / Data Science / Packaging Technology / Business Major – Junior/Senior
3.0 GPA or higher, cumulatively (will consider other if there is appropriate work experience)
Legally authorized to work in the United States on a permanent basis without company sponsorship now or in the future
Our Expectations
We Expect Our People To Be Guided By The Amcor Way And Demonstrate Our Values Every Day To Enable The Business To Win. We Are Winning When
Our people are engaged and developing as part of a high-performing Amcor team
Our customers grow and prosper from Amcor’s quality, service, and innovation
Our investors benefit from Amcor’s consistent growth and superior returns
The environment is better off because of Amcor’s leadership and products
Equal Opportunity Employer/Minorities/Females/Disabled/Veterans/Sexual Orientation/Gender Identity
Amcor is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.
If you would like more information about your EEO rights as an applicant under the law, please click on the links ""EEO is the and ""EEO is the Law"" Poster Supplement. If, because of a medical condition or disability, you need a reasonable accommodation for any part of the employment process, please call 224-313-7000 and let us know the nature of your request and your contact information.
E-Verify
We verify the identity and employment authorization of individuals hired for employment in the United States.
Show more
Show less","Data Analysis, Dashboarding, PowerBI, Statistics, Data Science, Quality Management, Microsoft Excel","data analysis, dashboarding, powerbi, statistics, data science, quality management, microsoft excel","dashboard, data science, dataanalytics, microsoft excel, powerbi, quality management, statistics"
Data and BI Analyst,Challenger Motor Freight Inc,"Cambridge, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-and-bi-analyst-at-challenger-motor-freight-inc-3638850988,2023-12-17,Brantford, Canada,Mid senior,Onsite,"Envision:
Working for a company dedicated to personal career growth and opportunity in moving the organization forward. Challenger Motor Freight Inc. is a Platinum Club Member in Canada’s 50 Best Managed Companies. Our success is directly attributed to our dedicated and talented team of professionals who work hard together with a common purpose – to keep us on the leading edge in safety, technology, and analysis.
It’s not by chance that Challenger is a leading North American freight transportation company. In 40 years, Challenger has grown from one person with a vision and a truck to an international transportation and supply chain management company.
We win as a team when we work as a team, and succeed when our employees succeed. We want people who are resilient, team-oriented, and driven because we are laser focused on meeting commitments to our People, Customer, and Profit. If you’re looking to work for a dynamic, fast-paced, progressive organization then apply with us.
We offer the following in our search for engaged employees looking to become part of a successful team:
A continuous learning environment that develops your individual career goals
A continuous improvement environment where all ideas are explored
Engaged coaches and mentors who will provide guidance but also allow autonomy
Team atmosphere
Competitive and comprehensive total rewards package including company paid group benefits and company sponsored retirement savings plan
Support of professional memberships and certifications
Standard office hours; Monday to Friday from approximately 8:00am to 5:00pm
We are committed to protecting the health, safety and wellbeing of our employees, and have implemented Company-wide best practices which includes daily health questionnaires for our employees, mandatory facemasks in communal locations, virtual meetings, social distancing guidelines and additional cleaning procedures.
The Opportunity:
The Data and BI Analyst is responsible for providing operational data support, data analysis and visual presentation for a variety of corporate initiatives, leveraging our SQL environment and BI Reporting platforms. This role requires a candidate with excellent analytical skills, an ability to summarize data in a simple manner and a desire to work closely in cross-functional teams. The individual will have a strong understanding of business concepts with working knowledge in key technical tools
The chosen candidate will be a critical thinker with the ability to work independently to complete a wide range of analysis while also being able to work in a team to collaborate and support on a wide variety of profit-related initiatives.
Key Accountabilities:
Operational Analytics and Data Recommendations – 45%
Support operational projects; provide analysis, data-driven recommendations and enable end-user with insightful BI reporting dashboard
Liaise with Operations team members.
Leverages various Development Applications (including but not limited to SQL, Alteryx, Knime, etc.) to conduct deep data analysis on network performance; reports out periodically.
Understands the business problems we are seeking to create solutions for.
Identify KPI’s required and add them to dashboards such as PowerBI / Tableau.
Provide support for on-demand data requests from user groups.
Participate in team meetings to gather requirements and develop BI solutions suited to business needs.
SQL Server Database Development – 45%
Champion the automation processes to create workflows with SSIS in order to manage the data warehouse
Develop, test, and maintain SQL Server stored procedures, functions, tables, views, and triggers relating to vendor supported applications and custom written .NET applications.
Create, maintain, and deploy reports in SSRS, PowerBI and vendor reporting solution.
Implement improvement modifications to complex and verbose stored procedures.
Use knowledge of SSRS and report design to create and deploy reports.
Provide data management support to .NET development team.
Maintain documentation on system and process flow modifications.
Process Improvement & Implementation – 5%
Apply Lean Six Sigma process improvement methodologies to process improvement initiatives.
Prepare process recommendation documents including presentations, process models, supporting metrics, business requirements and related impact analysis.
Assist in the implementation of new processes with necessary stakeholders. This may include but is not limited to documenting all new processes, creating measurable goals for success, training end users, acting as point of contact for troubleshooting and ongoing process management.
Continuous innovation and skill development – 5%
Focuses on continuous skill development and learning in following areas:
SQL
SSIS
SSRS
Python Coding
R Script
Machine Learning
Data Management
Process Mining
Process Standardization
Six Sigma Practices
Low Code Development Application (Knime, Alteryx, others)
Change management
Project management
Performs other duties as required
What You Need To Be Successful In This Role:
Diploma/Degree in Computer Science, Mathematics, Statistics, Engineering or a related field required.
2 years of experience developing and supporting in SQL Server environment.
Experience with SSIS Package management is considered an asset.
Proficiency in system analysis and design.
Experience maintaining SQL queries, stored procedures, functions, and triggers.
Proficient in Microsoft SQL 2008+, with a solid understanding of SQL permissions to maintain high level of data security.
Strong attention to detail.
Able to collaborate with a variety of stakeholders and work effectively with multiple departments, while also working independently at times to solve problems with ambiguous solutions.
Ability to apply critical thinking and problem solving.
Time management skills: ability to prioritize and comfortable handling competing priorities.
Resilient and adaptable to change.
Highly developed written and verbal communication skills, with the ability to prepare and deliver presentations.
How To Apply:
If you are looking to join a premier transportation company, and become an integral part of results oriented team who constantly challenge themselves to Go The Distance for our customers and for each other, the role of Data and BI Analyst may be right for you.
No phone calls, please. We thank all applicants; however, only those selected for an interview will be contacted. Challenger Motor Freight Inc. is an equal opportunity employer. We welcome diversity in the workplace and encourage applications from all qualified candidates including women, members of visible minorities, persons with disabilities, and aboriginal peoples. By submitting your resume, you consent Challenger Motor Freight Inc. to share this information within its divisions in order to identify other employment opportunities that you may be suitable for.
Show more
Show less","Data Analysis, Business Intelligence, SQL, Alteryx, Knime, PowerBI, Tableau, SSIS, SSRS, Python Coding, R Script, Machine Learning, Data Management, Process Mining, Process Standardization, Six Sigma Practices, Low Code Development Application, Change Management, Project Management, SQL Server, Microsoft SQL 2008+, SQL Permissions, System Analysis, Verbal Communication, Written Communication, Problem Solving, Time Management, Critical Thinking, Collaboration","data analysis, business intelligence, sql, alteryx, knime, powerbi, tableau, ssis, ssrs, python coding, r script, machine learning, data management, process mining, process standardization, six sigma practices, low code development application, change management, project management, sql server, microsoft sql 2008, sql permissions, system analysis, verbal communication, written communication, problem solving, time management, critical thinking, collaboration","alteryx, business intelligence, change management, collaboration, critical thinking, data management, dataanalytics, knime, low code development application, machine learning, microsoft sql 2008, powerbi, problem solving, process mining, process standardization, project management, python coding, r script, six sigma practices, sql, sql permissions, sql server, ssis, ssrs, system analysis, tableau, time management, verbal communication, written communication"
Data Analyst Part Time,Voxmediallc,"Haldimand County, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-voxmediallc-3757207289,2023-12-17,Brantford, Canada,Mid senior,Onsite,"Summary:
Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data analysis, Statistical techniques, SQL, R, Python, Data visualization, Tableau, Power BI, Data management, ETL, Hypothesis testing, A/B testing","data analysis, statistical techniques, sql, r, python, data visualization, tableau, power bi, data management, etl, hypothesis testing, ab testing","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk,Carrollbalistreri,"Kitchener, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-carrollbalistreri-3742887096,2023-12-17,Brantford, Canada,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data analysis, Data entry, Statistical techniques, Data visualization, Data reporting, Datadriven decision making, Business intelligence, A/B testing, Data quality management, Statistical modeling, Hypothesis testing, ETL processes, SQL, R, Python, Tableau, Power BI","data analysis, data entry, statistical techniques, data visualization, data reporting, datadriven decision making, business intelligence, ab testing, data quality management, statistical modeling, hypothesis testing, etl processes, sql, r, python, tableau, power bi","ab testing, business intelligence, data entry, data quality management, data reporting, dataanalytics, datadriven decision making, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk,Carrollbalistreri,"Kitchener, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-carrollbalistreri-3742887095,2023-12-17,Brantford, Canada,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerkwill be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data Analysis, Data Mining, Statistics, SQL, R, Python, Tableau, Power BI, A/B Testing, Statistical Modeling, Hypothesis Testing, Data Visualization, Data Management, ETL","data analysis, data mining, statistics, sql, r, python, tableau, power bi, ab testing, statistical modeling, hypothesis testing, data visualization, data management, etl","ab testing, data management, data mining, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistics, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk/Part Time,Toyandsons,"Hamilton, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-toyandsons-3756469710,2023-12-17,Brantford, Canada,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk/Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Data Visualization, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL Processes","data analysis, data visualization, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl processes","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Senior Data Engineer,Gore Mutual Insurance,"Cambridge, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-gore-mutual-insurance-3749624035,2023-12-17,Brantford, Canada,Mid senior,Hybrid,"We’re now at the boldest phase of our Next Horizon journey
At Gore Mutual, we’ve completely transformed our business in under three years. By investing in top talent and leading technology, we’ve redefined what it means to be a modern mutual that does good.
Our path forward brings a sharper focus on our business’ performance that’s powered by innovation and an agile, high-performing culture – we’re built for success.
We’re well on our way to becoming a purpose-driven, digitally led national insurer. Come join us.
Are you a talented and motivated individual with a passion for creating cutting-edge data solutions? Do you thrive on challenges and have a desire to constantly improve the quality and performance of your work? If you are, this is an opportunity to be part of our team and help build an enterprise data platform from the ground up.
As a Senior Data Engineer at Gore Mutual Insurance, you will play a pivotal role in designing, building, and maintaining our data infrastructure. Your work will facilitate data accessibility, accuracy, and accountability, enabling data-driven decision-making across our organization. You will collaborate closely with cross-functional teams to ensure our data processes are robust and scalable.
What will you be doing in this role?
Take ownership of data engineering projects.
Be resourceful and adept at navigating unfamiliar challenges.
Pay meticulous attention to detail.
Communicate clearly and effectively with cross-functional teams.
Understand and mitigate security risks and vulnerabilities.
Apply performance optimization techniques where needed.
Collaborate efficiently with IT teams.
Strike a balance between rapid project completion and maintaining high-quality standards.
Embrace test-driven development practices.
Identify issues or opportunities proactively, raising them and taking appropriate actions.
Contribute to defining data engineering patterns and design while constructively challenging solutions.
Collect and analyze requirements, then develop data processes and applications that meet them.
Design, develop, and maintain robust, scalable, and efficient data pipelines for data ingestion, processing, and transformation.
Continuously monitor and optimize data pipelines and systems for peak performance and reliability.
Implement data quality checks and governance policies to maintain data accuracy, completeness, and security.
Maintain comprehensive documentation of the data platform.
Provide guidance and mentorship to junior data engineers, fostering their professional growth.
What will you need to succeed in this role?
Bachelor’s or Master’s degree in Computer Science, Data Engineering, Software Engineering or a related field.
Minimum 4 years of proven experience as a Data Engineer.
Proficiency in Python, including a strong grasp of Object Oriented and Functional programming paradigms.
Excellent SQL skills and expertise in database management and tuning.
Strong pySpark and distributed systems knowledge
Experience working with and analyzing structured, semi-structured, and unstructured datasets.
Familiarity with software design patterns.
Proficiency in data modeling, ETL processes, data lakes, and data warehousing concepts.
Ability to communicate complex technical concepts clearly and effectively.
Proficiency in Azure Data infrastructure components.
Ability to work independently and as part of a team in a fast-paced, dynamic environment.
Strong problem-solving and critical-thinking abilities.
Strong communication and collaboration skills.
Proactive and results-oriented mindset.
Demonstrated experience working in Agile development teams (Scrum or Kanban).
Understand the Software Development Life Cycle (SDLC).
Exceptional technical writing abilities.
Nice to Have
A graduate degree in a technical field with specialization in Analytics, Data Science, or a related subject.
Azure certifications (Microsoft Certified: Azure Data Engineer Associate).
Experience with Data Lakehouse architecture
Experience with building custom data tools.
Databricks certifications (Databricks Certified /Data Engineer Professional certification).
Experience with reference or master data management.
#IndHP
Gore Mutual Insurance is committed to providing accommodations for people with disabilities during all phases of the recruiting process, including the application process. If you require accommodation because of a disability, we will work with you to meet your needs. If you are selected for an interview and require accommodation, please advise the HR representative who will consult with you to determine an appropriate accommodation.
Show more
Show less","Python, Object Oriented Programming, Functional Programming, SQL, Database Management, pySpark, Distributed Systems, Structured Data, SemiStructured Data, Unstructured Data, Software Design Patterns, Data Modeling, ETL Processes, Data Lakes, Data Warehousing, Azure Data Infrastructure, Agile Development, Scrum, Kanban, Software Development Life Cycle (SDLC), Analytics, Data Science, Azure Certifications, Data Lakehouse Architecture, Custom Data Tools, Databricks Certifications, Reference Data Management, Master Data Management","python, object oriented programming, functional programming, sql, database management, pyspark, distributed systems, structured data, semistructured data, unstructured data, software design patterns, data modeling, etl processes, data lakes, data warehousing, azure data infrastructure, agile development, scrum, kanban, software development life cycle sdlc, analytics, data science, azure certifications, data lakehouse architecture, custom data tools, databricks certifications, reference data management, master data management","agile development, analytics, azure certifications, azure data infrastructure, custom data tools, data lakehouse architecture, data lakes, data science, database management, databricks certifications, datamodeling, datawarehouse, distributed systems, etl, functional programming, kanban, master data management, object oriented programming, python, reference data management, scrum, semistructured data, software design patterns, software development life cycle sdlc, spark, sql, structured data, unstructured data"
Senior Data Engineer (Hybrid),Siemens,"Kitchener, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-hybrid-at-siemens-3714345400,2023-12-17,Brantford, Canada,Mid senior,Hybrid,"Siemens Battery Accelerator
is looking to recruit a
Senior Data Engineer
to contribute to the creation and optimization of multi-petabyte scale real-time data solutions, working with control, sensor, and image data collected from manufacturing floors at battery gigafactories.
This is a
hybrid
role based out of our newly established office in Kitchener, ON.
What you will do:
Design and implement scalable, event-driven, containerized data pipelines for real-time ingestion, transformation, and analysis.
Optimize for extreme throughput and volume using distributed computing and storage frameworks and cloud native technologies.
Develop middleware and APIs for robust data communication between services.
Implement DataOps tools, including data-specific CI/CD pipelines and processes.
Maintain data security aligned with industry standards and regulatory frameworks.
Mentor junior data engineers for continuous skill and process improvement.
Minimum qualifications:
Undergraduate degree in Computer Engineering, Software Engineering, or a related field.
5+ years in Data or Software Engineering.
Extensive experience designing and developing real-time data processing solutions using various stream processing tools and technologies.
Deep practical and theoretical understanding of distributed systems and high-availability architectures.
Strong grasp of data modelling across multiple storage paradigms, including NoSQL systems.
Hands-on experience with Docker and Kubernetes.
Proficiency in setting up and maintaining CI/CD pipelines and processes.
Expertise in Python, Golang, or C/C++.
Preferred qualifications:
Graduate degree in a related field.
Familiarity with WebSocket, gRPC, and MQTT protocols.
Experience working with international teams.
Proficiency with Golang.
About Siemens Battery Accelerator:
We are a global network of teams comprised of researchers, engineers, and technologists. With a passion for sustainability, we work collectively to revolutionize all aspects of battery development, including design, manufacturing, and recycling, to enable the rapid transition of our grid and mobility systems to renewable energies. Our team in Canada is responsible for building state-of-the-art IIoT platforms and global-scale data systems that serve our partnering gigafactories worldwide, as well as our internal teams of experts.
Shape the future with us:
We encourage the sharing of innovative ideas, and champion the people behind them.
We seek individuals driven by both skill and ingenuity, capable of navigating an ever-evolving landscape of challenges, customer demands, and interdisciplinary inquiries. Fueled by a culture of curiosity, we are committed to revolutionizing the future of work. Join our collective endeavor to impact lives globally through cutting-edge technology and transformative ideas.
About Siemens Canada:
For over 110 years, Siemens Canada has stood for engineering excellence, innovation, quality, and reliability. We are a technology leader, providing comprehensive solutions for Smart Infrastructure and Digital Industries. We set the benchmark in transforming the world around us by innovating in the way we electrify, automate, and digitize. Ingenuity propels us forward, and the fruits of our collaboration define what truly matters to us.
Making a difference together, we raised $385,000 towards charitable contributions, supporting over 38 non-profit organizations and driving sustainability in our local communities. Siemens Canada has 2,500 employees from coast-to-coast and 24 office and production facilities across the country. Join our team of approximately 293,000 talented professionals in more than 190 countries/regions and help us tackle the most exciting challenges to build a successful future together!
Why you will love working at Siemens:
Enjoy a Flexible Work Environment with Career Advancement Opportunities: Embrace a healthy work-life balance with flexible hours, telecommuting, and digital workspaces, while also advancing your career through local and global mentorship programs.
Solve Significant and Impactful Problems in Technology Innovation: Be a part of exciting, innovative projects aimed at addressing the world’s most significant issues, all within an engaging, challenging, and rapidly evolving, cutting-edge technological environment.
Comprehensive Rewards & Benefits Package: Enjoy our competitive total rewards package which includes profit sharing, flexible vacation policies with the option to buy and sell your vacation depending on your lifestyle, and opportunities to be compensated for your innovative ideas. Additional perks and discounts are also part of the package.
Partake in Social Initiatives in a Diverse and Inclusive Environment: Contribute to our social responsibility initiatives focused on improving access to education, technology, and sustainable communities. Make a positive impact and thrive in our diverse and inclusive work environment.
Siemens is proud to be an eight-time award winner of Canada’s Top 100 Employers, Canada’s Greenest Employers 2022, Canada’s Top Employers for Young People 2023, and Greater Toronto's Top Employers 2022.
Siemens is committed to creating a diverse environment and is proud to be an equal opportunity employer. Upon request, Siemens Canada will provide reasonable accommodation for disabilities to support participation of candidates in all aspects of the recruitment process. All qualified applicants will receive consideration for employment.
By submitting personal information to Siemens Canada Limited or its affiliates, service providers and agents, you consent to our collection, use and disclosure of such information for the purposes described in our Privacy Notice available at www.siemens.ca.
Siemens s’engage à créer un environnement diversifié et est fière d’être un employeur souscrivant au principe de l’égalité d’accès à l’emploi. Sur demande, Siemens Canada prendra des mesures d’accommodement raisonnables pour les personnes handicapées, dans le but de soutenir la participation des candidats dans tous les aspects du processus de recrutement. Tous les candidats qualifiés seront pris en considération pour ce poste.
En transmettant des renseignements personnels à Siemens Canada limitée ou à ses sociétés affiliées, à ses fournisseurs de services ou à ses agents, vous nous autorisez à recueillir, à utiliser et à divulguer ces renseignements aux fins prévues dans notre Déclaration de protection de la confidentialité, que vous pouvez consulter au www.siemens.ca.
Show more
Show less","Data engineering, Realtime data solutions, Containerized data pipelines, Extreme throughput and volume, Distributed computing and storage frameworks, Cloud native technologies, Middleware and APIs, DataOps tools, Dataspecific CI/CD pipelines, Data security, Junior Engineer training, Hybrid role, Docker, Kubernetes, CI/CD pipelines and processes, NoSQL systems, WebSocket, gRPC, MQTT, Python, Golang, C/C++","data engineering, realtime data solutions, containerized data pipelines, extreme throughput and volume, distributed computing and storage frameworks, cloud native technologies, middleware and apis, dataops tools, dataspecific cicd pipelines, data security, junior engineer training, hybrid role, docker, kubernetes, cicd pipelines and processes, nosql systems, websocket, grpc, mqtt, python, golang, cc","cc, cicd pipelines and processes, cloud native technologies, containerized data pipelines, data engineering, data security, dataops tools, dataspecific cicd pipelines, distributed computing and storage frameworks, docker, extreme throughput and volume, golang, grpc, hybrid role, junior engineer training, kubernetes, middleware and apis, mqtt, nosql systems, python, realtime data solutions, websocket"
Plant Master Data Analyst,Schneider Electric,"Fairfield, OH",https://www.linkedin.com/jobs/view/plant-master-data-analyst-at-schneider-electric-3779503726,2023-12-17,Covington,United States,Associate,Onsite,"Schneider Electric has an outstanding opportunity for a motivated individual to assume the role of Plant Master Data Analyst (PMDA) at our Fairfield plant. The Plant Master Data Analyst will act as the subject matter expert on all topics related to Master Data, Material, Product, Bill of Materials, Routings, Logistics Data, etc. PMDA will support business projects and governance, and quality operations. Coordinate the Cleansing, creation, and maintenance of master data in the ERP System and support/admin Cycle Count Program.
What will you do?
Responsible for the maintenance and/or creation of for the material master data
Prepare and publish monthly data quality reports in collaboration with plant Data Coordinators, to understand the root cause and take corrective actions to avoid inconsistencies in data
Evaluate business Support new modules, releases, and projects deployment by ensuring data creation or modifications in his plant.
impact and lost opportunity due to poor data quality
Ensure compliance and security of data assets
Responsible for the Cleansing of the material master data in the plant
Plant Master Data Analyst (PMDA) will be responsible for the material master, creating parts, activating the part when ready, obsolete, and changing parts according to lifecycle and projects
Support new modules, releases, and projects deployment by ensuring data creation or modifications in his plant.
Coordinate with the plant Data Coordinators/Key Users for all concerned domains
Administer One Shuttle tool (used to coordinate the creation, update, and deletion of data)
Responsible for the Inventory Accuracy and Cycle count program in site (when applicable)
Audit every quarter the SAP access users
Work with human resources and plant functional areas to ensure ongoing coverage of master data responsibilities in the event of absence or vacancy
What qualifications will make you successful?
Bachelor’s Degree preferably in Engineering, Business or related field
Experience in ERP system, SAP preferred
3+ years of PMDA experience and/or experience in materials management, manufacturing, quality or equivalent.
2+ years of hands-on experience working with databases and writing SQL queries.
1+ years of experience with data management and data quality.
2+ years effectively communicating complex topics and knowledge transfer with non-technical users.
Self-starter with the ability to work independently with little or no supervision.
Excellent problem-solving abilities
Strong analytical, numerical, math and statistics skills
Schneider Electric offers a robust benefits package to support our employees such as flexible work arrangements, paid family leave, 401(k)+ match, and more. Click here to find out more about working with us: http://se.com/us/careers .
We seek out and reward people for putting the customer first, being disruptive to the status quo, embracing different perspectives, continuously learning, and acting like owners. We're recognized around the world for welcoming people as they are. We create an inclusive culture where all forms of diversity are seen as a real value for the company. See what our people have to say about working for Schneider Electric. https://youtu.be/C7sogZ_oQYg
Who will you report to?
Upstream Planning Leader
Let us learn about you! Apply today.
Why us?
Schneider Electric is leading the digital transformation of energy management and automation. Our technologies enable the world to use energy in a safe, efficient and sustainable manner. We strive to promote a global economy that is both ecologically viable and highly productive.
€25.7bn global revenue
137 000+ employees in 100+ countries
45% of revenue from IoT
5% of revenue devoted for R&D
You must submit an online application to be considered for any position with us. This position will be posted until filled
It is the policy of Schneider Electric to provide equal employment and advancement opportunities in the areas of recruiting, hiring, training, transferring, and promoting all qualified individuals regardless of race, religion, color, gender, disability, national origin, ancestry, age, military status, sexual orientation, marital status, or any other legally protected characteristic or conduct. Concerning agencies: Schneider Electric does not accept unsolicited resumes and will not be responsible for fees related to such.
Show more
Show less","Data Management, Master Data, SAP, ERP Systems, Data Quality, SQL, Data Analysis, Data Cleansing, Cycle Count Program, Supply Chain, Logistics, Inventory, Material Management, Manufacturing, Quality, Engineering, Statistics, Mathematics, Communication, ProblemSolving, NonTechnical Users, Work Independently, Upstream Planning, Agile Development, Scalable Solutions, Root Cause Analysis, Corrective Actions, Automation, Digital Transformation, Energy Management, IoT, R&D","data management, master data, sap, erp systems, data quality, sql, data analysis, data cleansing, cycle count program, supply chain, logistics, inventory, material management, manufacturing, quality, engineering, statistics, mathematics, communication, problemsolving, nontechnical users, work independently, upstream planning, agile development, scalable solutions, root cause analysis, corrective actions, automation, digital transformation, energy management, iot, rd","agile development, automation, communication, corrective actions, cycle count program, data management, data quality, dataanalytics, datacleaning, digital transformation, energy management, engineering, erp systems, inventory, iot, logistics, manufacturing, master data, material management, mathematics, nontechnical users, problemsolving, quality, rd, root cause analysis, sap, scalable solutions, sql, statistics, supply chain, upstream planning, work independently"
Clinical Data Analyst - Biostatistics Team,Medpace,"Cincinnati, OH",https://www.linkedin.com/jobs/view/clinical-data-analyst-biostatistics-team-at-medpace-3747761090,2023-12-17,Covington,United States,Mid senior,Onsite,"Our corporate activities are growing rapidly, and we are currently seeking a full-time, office-based Clinical Data Analyst to join our Biostatistics team. This position will work on a team to accomplish tasks and projects that are instrumental to the company’s success. If you want an exciting career where you use your previous expertise and can develop and grow your career even further, then this is the opportunity for you.
Responsibilities
Develop data visualization specifications that meet client needs
Using Spotfire; Develop data visualization dashboards and reports
Communicate and present data visualization dashboards and reports to internal and external clients
Qualifications
Bachelor's degree in Life Sciences, Data Sciences, Math, Statistics, or related field
Experience with data visualization software such as Spotfire, Power BI, or Tableau is helpful but not required.
TRAVEL:
None
Medpace Overview
Medpace is a full-service clinical contract research organization (CRO). We provide Phase I-IV clinical development services to the biotechnology, pharmaceutical and medical device industries. Our mission is to accelerate the global development of safe and effective medical therapeutics through its scientific and disciplined approach. We leverage local regulatory and therapeutic expertise across all major areas including oncology, cardiology, metabolic disease, endocrinology, central nervous system, anti-viral and anti-infective. Headquartered in Cincinnati, Ohio, employing more than 5,000 people across 40+ countries.
Why Medpace?
People. Purpose. Passion. Make a Difference Tomorrow. Join Us Today.
The work we’ve done over the past 30+ years has positively impacted the lives of countless patients and families who face hundreds of diseases across all key therapeutic areas. The work we do today will improve the lives of people living with illness and disease in the future.
Cincinnati Perks
Cincinnati Campus Overview
Hybrid work-from-home options (dependent upon position and level)
Competitive PTO packages, starting at 20+ days
Flexible work hours
Discounted tuition for UC online programs
Company-sponsored employee appreciation events
Employee health and wellness initiatives
Community involvement with local nonprofit organizations
Competitive compensation and benefits package
Structured career paths with opportunities for professional growth
Partnership and discount with onsite childcare
Discounts on local sports games, local fitness gyms and attractions
Official Sponsor of FC Cincinnati
Modern, ecofriendly campus with an on-site fitness center, bar, and restaurants
Awards
Recognized by Forbes as one of America's Best Mid-size Companies in 2021, 2022 and 2023
Continually recognized with CRO Leadership Awards from Life Science Leader magazine based on expertise, quality, capabilities, reliability, and compatibility
What To Expect Next
A Medpace team member will review your qualifications and, if interested, you will be contacted with details for next steps.
EO/AA Employer M/F/Disability/Vets
Show more
Show less","Data Visualization, Spotfire, Power BI, Tableau, Life Sciences, Data Sciences, Math, Statistics","data visualization, spotfire, power bi, tableau, life sciences, data sciences, math, statistics","data sciences, life sciences, math, powerbi, spotfire, statistics, tableau, visualization"
Onsite Work - Need Sr Test Engineer (Data Validation and Conversion) in Cincinnati Ohio,Steneral Consulting,"Cincinnati, OH",https://www.linkedin.com/jobs/view/onsite-work-need-sr-test-engineer-data-validation-and-conversion-in-cincinnati-ohio-at-steneral-consulting-3759946505,2023-12-17,Covington,United States,Mid senior,Onsite,"Sr Test Engineer (Data Validation and Conversion)
Cincinnati, Ohio- Onsite – Local candidates only
Latest Notes From Manager On This Role:
This role will Help with the adoption of Oracle ERP for back office tasks.
Focused on conversion data validation from our Client applications to Finance DB (Our single source of truth for financial info) to Oracle ERP.
We need someone who is comfortable with queries and has some experience with data validation / integration testing, investigating data issues and where they originate from.
Strong SQL and DB knowledge required.
Still a blend of manual and automation.
Lots of APIs, swagger, postman, rest sharp.
Technical role – work w/ database.
Facilitate conversations w/ other scrum teams. Data and Scrum. 1 year
Has to work with multiple products, integration testing end to end.
Regression testing
Seeking a good Culture fit, communication and team work, leadership, work independently, independent thinker are the skill set they are looking for
Overall Environment:
7 or 8 back office products to support – must show they can pivot quickly and juggle supporting multiple apps at once.
Some of those apps are desktop which poses difficulties for wholistic E2E testing.
Work comes in fast, a lot of pivoting
Issues include desktop testing as they can’t do full end to end testing
Description:
Senior Software QA Engineer (Data Validation & Conversion)
Role is focused on conversion data validation from our Client’s applications to Finance DB (Our single source of truth for financial info) to Oracle ERP.
Help adoption of Oracle ERP for back-office tasks. Seeking Strong SQL and DB knowledge Will be working with a blend of manual and automation. Lots of APIs, swagger, postman, rest sharp.
Must be comfortable with queries and has some experience with data validation / integration testing, investigating data issues and where they originate from.
This is a technical role – work w/ database. API’s. Swagger, postman. More automation but some manual. Selenium. Facilitate conversations w/ other scrum teams.
Will be working with multiple products, integration testing end to end. Regression testing.
Experience In Finance / Accounting Preferred
Should be independent thinkers, pushing quality left wherever possible.
Overall Environment:
7 or 8 back-office products to support – must be able to pivot quickly and juggle supporting multiple apps at once.
Some of those apps are desktop which poses difficulties for wholistic E2E testing.
Currently 1:7 dev:qa ratio. All our teams are scrum teams composed of 5-7 developers and 1-2 test engineers.
All back office supporting team members are onsite.
Work comes in fast, a lot of pivoting
Issues: include desktop testing as they can’t do full end to end testing
About The Role:
As a Senior Software QA Engineer for Client, you will be the Quality subject-matter-expert on a dedicated Scrum Team and will own all aspects of test planning and execution. You will participate in scrum ceremonies, including product design reviews and backlog requirements, while providing input on testability of requirements and acceptance criteria. Our Quality Assurance Engineers are also responsible for driving adoption of QA best practices and leading Shift Left approach for quality deliverables within their aligned team.
What You’ll Be Doing:
Responsible for implementation and maintenance of automated test frameworks and scenarios
Work with other QA Engineers and Developers to ensure that all aspects of the applications have proper testing coverage and overall system reliability, performance, and quality
Deliver effective and efficient automated testing solutions by meeting key product milestones and overall deliverables on-time while adhering to development and quality standards
Develop and manage best practices with respect to test data required for automated test scenarios
Perform testing using a variety of methodologies, including manual, exploratory, and automated
Create, execute, and maintain automated test scripts for both APIs and functional tests using Selenium with C#
Detect, track and document software defects and inconsistencies
Review, analyze, and supplement requirement specifications, and ensure maximum test coverage
Coordinate with offshore testing team to provide feedback and coaching on deliverables
What You Need:
Bachelor's degree or equivalent amount of relevant work experience
Experience with Finance and accounting domain preferred
Experience writing SQL, Java, or C# to develop automation frameworks to perform UI, API and backend database testing
Expertise in creating complex SQL queries to verify results of testing
Demonstrated experience working on a scrum team
Experience in automated testing including Selenium, Postman, or migration testing tools
Seeking good Culture fit, strong communication and teamwork, leadership, work independently, independent thinker are the skill set.
Show more
Show less","Oracle ERP, SQL, Data Validation, Integration Testing, API Testing, Swagger, Postman, Rest Sharp, Selenium, Java, C#, Test Automation, Test Planning, Test Execution, Test Coverage, Test Data Management, Manual Testing, Exploratory Testing, Automated Testing, Test Scripting, Defect Tracking, Scrum","oracle erp, sql, data validation, integration testing, api testing, swagger, postman, rest sharp, selenium, java, c, test automation, test planning, test execution, test coverage, test data management, manual testing, exploratory testing, automated testing, test scripting, defect tracking, scrum","api testing, automated testing, c, data validation, defect tracking, exploratory testing, integration testing, java, manual testing, oracle erp, postman, rest sharp, scrum, selenium, sql, swagger, test automation, test coverage, test data management, test execution, test planning, test scripting"
EDI Data Analyst,"Adroit Resources, Inc.","Cincinnati, OH",https://www.linkedin.com/jobs/view/edi-data-analyst-at-adroit-resources-inc-3695086476,2023-12-17,Covington,United States,Mid senior,Onsite,"Job Description
This is a new role, creating a process that allows vendors to track their products from arrival to the store on the trailers to the warehouse's back room, then to the shelves.
Working knowledge of Grocery Retail, Shipping and Receiving, and Direct Store Delivery,
Working knowledge of a product item database
Working knowledge or familiarity of Business Objects Reporting.
Preform technical support and provide documented results on testing accuracy
Ability to learn systems and policies (Direct Store Delivery (DSD), Inventory, etc.)
Looking at raw data and being able to analyze it
Key Responsibilities
Experience with Grocery and Retail EDI in support of internal and external customers
Experience with implementation, testing, and support of EDI
Excellent problem solving and organizational skills, with strong ability to influence
Excellent communication skills written and verbal, to all levels internal and external associates, customers and department level leadership.
Working knowledge of Microsoft Word, Excel and PowerPoint
Will be working with business objects reports
Location:
Cincinnati, OH, 45202
Pay Range
$25 -$32/hr on W2
About The Company
Adroit Resources Inc., (A Nisum Company) is a certified, minority-owned, dynamic Information Technology services company based out of California's Silicon Valley. Our offerings range from IT staff augmentation to technology consulting. We work with large enterprise clients via their MSP Programs, with mid-sized companies and fast-growing startups to ramp up their hiring efforts to hire the best talent in the shortest time. Adroit works nationwide on IT and Non-IT positions in a number of verticals. We place qualified, experienced and vetted talent that is in high demand today.
Adroit Resources Inc., is an Equal Opportunity Employer and we are proud of our ongoing efforts to foster diversity and inclusion in the workplace.
Show more
Show less","Grocery Retail, Shipping and Receiving, Direct Store Delivery, Inventory Management, EDI, Business Objects Reporting, Data Analysis, Microsoft Office Suite, Problem Solving, Organizational Skills, Communication Skills, Data Analysis","grocery retail, shipping and receiving, direct store delivery, inventory management, edi, business objects reporting, data analysis, microsoft office suite, problem solving, organizational skills, communication skills, data analysis","business objects reporting, communication skills, dataanalytics, direct store delivery, edi, grocery retail, inventory management, microsoft office suite, organizational skills, problem solving, shipping and receiving"
Data Analyst Level 2,Apex Systems,"Blue Ash, OH",https://www.linkedin.com/jobs/view/data-analyst-level-2-at-apex-systems-3743989994,2023-12-17,Covington,United States,Mid senior,Onsite,"Job#: 1372872
Job Description:
The Data Analyst will work in a cross-functional technical community of data engineers, data scientists, product managers, agile delivery, software engineers and business partners. Qualified candidates must have a passion for enabling others to use the data appropriately to solve business problems. Data Analysts are responsible for reviewing data for quality, looking for patterns and trends as well as creation and automation of jobs that make it easier to find data problems, suggesting resolutions that improve the quality of the data as needed. Data analysts need to be able to use tools and access data from Cloud (Azure/GCP), external interfaces and on prem environments to access and analyze data.
EEO Employer
Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at [email protected] or 844-463-6178.
Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.
Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.
4400 Cox Road
Suite 200
Glen Allen, Virginia 23060
Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at [email protected] (Do not submit resumes or solicit consultants to this email address). UnitedHealthcare creates and publishes the Transparency in Coverage Machine-Readable Files on behalf of Apex Systems.
Show more
Show less","Data Analysis, Data Engineering, Software Engineering, Data Science, Product Management, Business Analysis, Agile Methodology, Data Quality, Data Integration, Data Visualization, Cloud Computing, Azure, GCP, SQL, Hadoop, Python, R, Machine Learning, Statistics, Data Governance, Data Warehousing, Data Mining, Data Modeling, Data Security, Big Data, Business Intelligence","data analysis, data engineering, software engineering, data science, product management, business analysis, agile methodology, data quality, data integration, data visualization, cloud computing, azure, gcp, sql, hadoop, python, r, machine learning, statistics, data governance, data warehousing, data mining, data modeling, data security, big data, business intelligence","agile methodology, azure, big data, business analysis, business intelligence, cloud computing, data engineering, data governance, data integration, data mining, data quality, data science, data security, dataanalytics, datamodeling, datawarehouse, gcp, hadoop, machine learning, product management, python, r, software engineering, sql, statistics, visualization"
"Senior Staff Software Engineer, Data Foundations","Sprout Social, Inc.",Canada,https://ca.linkedin.com/jobs/view/senior-staff-software-engineer-data-foundations-at-sprout-social-inc-3775931375,2023-12-17,Sarnia-Clearwater, Canada,Mid senior,Remote,"We know experience is built in a number of ways. Even if your background doesn’t match the exact requirements, we encourage you to apply and share any relevant skills in a cover letter. Sprout welcomes all candidates to apply, including those who identify as BIPOC, women and underrepresented genders in tech, LGBTQIA+, parents, veterans, persons with disabilities and people of all faiths.
Senior Staff Software Engineer, Data Foundations
Description
Sprout Social is looking to hire a Senior Staff Software Engineer to join our Data Foundations team. This team is a new organization within Sprout Engineering whose mission is to build out the next generation of data management infrastructure, classification, and standards for the company. As a cross-functional team of platform, data, and analytics engineers, we work with stakeholders from across the company, including product managers, software engineers, data scientists, and our Revenue and Marketing organizations.
Why join Sprout’s Engineering team?
With collaborative cross-functional teams that span mobile, front-end, back-end, QA, and site reliability engineering—the Sprout Engineering team is a place to sharpen your craft and solve hard problems with the smartest people in the industry. You’ll get to work on a “tech-giant” scale with smaller, supportive teams where every engineer has the chance to make an impact on our company, and our customers. The best part? In our industry, you often have to switch jobs or even companies to learn a new part of a tech stack or business. But at Sprout, our product is a suite, so you just need to move teams. You’re able to diversify your skills, which not only benefits your team—but also your career.
What You’ll Do
We are at the beginning of a multi-year journey to transform the way Sprout manages data, and the effects will have a profound impact on our product and internal operations. Some things you’ll be doing:
You will be a founding member of the Data Foundations leadership team and be responsible for leading the technical vision for the team.
As the team grows, you will partner closely with the Director, Engineering, and other leaders to chart the strategic growth of our platform
Lead the ongoing technical design of our platform in close collaboration with other technical and product leaders within and outside of Data Foundations
Balance ambition and pragmatism to achieve the best business outcomes
Be a standard bearer for engineering best practices.
Connect the team’s activities with company-wide engineering best practices and contribute to the overall evolution of those standards
Work with cross-functional stakeholders from across the organization to develop a multiyear platform roadmap, then implement this plan and work with teams across the organization to make it a reality
Be an engine of optimism, positive change, and continuous improvement within the team
What You’ll Bring
We’re looking for a creative, collaborative, highly motivated and pragmatic engineer with a proven track record of leading the delivery of complex projects that span multiple teams and have a bottom-line impact. If this sounds like you and you want to be on a team that has a huge impact across our organization, we’d love to talk with you!
The minimum qualifications for this role include:
9+ years experience developing and supporting software in a production environment
7+ years experience in distributed systems development experience
Experience with backend development with languages such as Java, C++, Go and Rust
Demonstrated experience designing and delivering self-serve platforms
Preferred qualifications for this role include:
Familiarity with modern data platform architectures and technologies including Data Mesh
Experience with the modern data stack including distributed query engines, data catalogs, execution and orchestration tiers, streaming architectures, and data warehouses
Familiarity with scaled data architectures and data lake technologies such as DeltaLake
Experience with terabyte-to-petabyte-scaled data stores using both relational and NoSQL technologies
How You’ll Grow
Within 1 month, you’ll plant your roots, including:
Complete Sprout’s New Hire training program alongside other new Sprout team members.
Get acclimated to the team's current Mission, Goals, and Objectives along with future product roadmaps.
Onboard to the team’s production systems and join oncall rotations
Familiarize with the major aspects and data flows across our systems
Connect with key stakeholders outside across Engineering and business teams
Within 3 months, you’ll start hitting your stride by:
Be fully onboarded to our systems, having committed code on several small starter projects
Develop an understanding of the team’s business and technical goals
Start to develop opinions about and contribute to the technical and product roadmap for the team
Enable an ongoing technical dialogue with the team’s engineers to discuss specific initiatives, issues, and direction
Start to articulate best practice areas and plans for the team
Write design documents, coordinate dependencies, and act as the domain owner for new projects.
Within 6 months, you’ll be making a clear impact through:
Integrate and use monitoring and alerting tools to know about problems before our users.
Create and manage concurrent, distributed systems.
Build your engineering skills by attending in-house presentations, workshops, and training sessions.
Lead technical design meetings with your teammates to walk through new feature ideas.
Identify technical debt and performance bottlenecks within our systems, come up with a plan to improve the code, and get it pushed to production.
Work and communicate effectively with other groups across the organization to ensure big-picture alignment and encourage cross-team collaboration.
Form a career growth plan with your manager and work towards it.
Partner with the Infrastructure team to improve your team’s ability to deliver reliable, highly available services.
Within 12 months, you’ll make this role your own by:
Be the go-to expert of your teams’ systems at the company.
Own cross-organizational projects, demonstrating project management skills, consensus building, and strong leadership.
Actively mitigate risk of failed delivery and missed deadlines through courageous, transparent communication with colleagues and stakeholders throughout a project life cycle.
Lead technical architecture meetings.
Identify technical debt and performance bottlenecks within our systems, come up with a plan to improve the code, and get it pushed to production.
Mentor junior engineers, helping them level up technically.
Build connections with members from other teams through active networking and community building.
Have opportunities to contribute to in-house technical presentations and workshops that share your expertise with large groups of Sprout engineers.
Surprise us! Use your unique ideas and abilities to change your team in beneficial ways that we haven’t even considered yet.
Of course, what is outlined above is the ideal timeline, but things may shift based on business needs and other projects and tasks could be added at the discretion of your manager.
Our Benefits Program
We’re proud to regularly be recognized for our team, product and culture. Our benefits program includes:
Insurance and benefit options that are built for both individuals and families
Progressive policies to support work/life balance, like our flexible paid time off and parental leave program
High-quality and well-maintained equipment—your computer will never prevent you from doing your best
Wellness initiatives to ensure both health and mental well-being of our team
Ongoing education and development opportunities via our Grow@Sprout program, employee-led diversity, equity and inclusion initiatives and mentorship programs for aspiring leaders
Growing corporate social responsibility program that is driven by the involvement and passion of our team members
Candidates for this remote work opportunity must be based in either British Columbia or Ontario. If you are based in another location within Canada, we aren’t able to hire in your location at this time; however, if you’d like to stay in touch with us in case that changes in the future, please apply and we’ll save your application for possible future consideration.
When you apply for employment with Sprout Social, we will process your job applicant data, including your employment and education history, transcript, writing samples, and references as necessary to consider your job application for open positions. Your personal data will be shared with Greenhouse Software, Inc., and Crosschq, Inc., cloud services providers located in the United States of America and engaged by Sprout Social to help manage its recruitment and hiring process on Controller’s behalf. Accordingly, if you are located outside of the United States, by clicking “Submit Application” on this site, you consent to the transfer of your personal data to the United States. For more information about our privacy practices please visit our Privacy Policy. California residents have additional rights and should review the Additional Disclosures for California Residents section in our Privacy Policy.
Apply now
Back to all jobs
About Sprout
Sprout Social is a global leader in social media management and analytics software. Sprout’s intuitive platform offers comprehensive social media management solutions, including publishing and engagement functionality, customer care, influencer marketing, advocacy, and AI-powered business intelligence to more than 30,000 brands. Founded in 2010, Sprout has a hybrid team located across the globe. Sprout Social has been recognized as a Glassdoor Best Places to Work, PEOPLE Companies that Care, Great Place to Work Best Workplace for Parents and more.
Sprout Social powers open communication between individuals, brands and communities through elegant, sophisticated software. We are relentless about solving hard problems for our customers and committed to both customer and team success. Our team’s shared belief in Sprout’s mission promotes a culture of openness, empowerment and fun.
Show more
Show less","Software Engineering, Data Management, Data Infrastructure, Data Classification, Data Standards, Platform Engineering, Data Engineering, Analytics Engineering, Java, C++, Go, Rust, Data Mesh, Distributed Query Engines, Data Catalogs, Execution and Orchestration Tiers, Streaming Architectures, Data Warehouses, DeltaLake, Data Lake Technologies, Relational Databases, NoSQL Databases, TerabytetoPetabyteScaled Data, Distributed Systems, InHouse Presentations, Workshops, Training Sessions, Technical Design Meetings, Concurrent Systems, Monitoring Tools & Alerting, Technical Debt Management, Performance Bottlenecks, CrossOrganizational Collaboration, Project Management, Consensus Building, Leadership, Risk Mitigation, Technical Architecture Meetings, Mentorship, Networking, Community Building, Technical Presentations","software engineering, data management, data infrastructure, data classification, data standards, platform engineering, data engineering, analytics engineering, java, c, go, rust, data mesh, distributed query engines, data catalogs, execution and orchestration tiers, streaming architectures, data warehouses, deltalake, data lake technologies, relational databases, nosql databases, terabytetopetabytescaled data, distributed systems, inhouse presentations, workshops, training sessions, technical design meetings, concurrent systems, monitoring tools alerting, technical debt management, performance bottlenecks, crossorganizational collaboration, project management, consensus building, leadership, risk mitigation, technical architecture meetings, mentorship, networking, community building, technical presentations","analytics engineering, c, community building, concurrent systems, consensus building, crossorganizational collaboration, data catalogs, data classification, data engineering, data infrastructure, data lake technologies, data management, data mesh, data standards, data warehouses, deltalake, distributed query engines, distributed systems, execution and orchestration tiers, go, inhouse presentations, java, leadership, mentorship, monitoring tools alerting, networking, nosql databases, performance bottlenecks, platform engineering, project management, relational databases, risk mitigation, rust, software engineering, streaming architectures, technical architecture meetings, technical debt management, technical design meetings, technical presentations, terabytetopetabytescaled data, training sessions, workshops"
Senior Big Data Engineer - Java/Scala - Remote,EPAM Systems,Canada,https://ca.linkedin.com/jobs/view/senior-big-data-engineer-java-scala-remote-at-epam-systems-3783543968,2023-12-17,Sarnia-Clearwater, Canada,Mid senior,Remote,"Description
We are hiring a
Senior Big Data Engineer
who will support a critical digital transformation project for one of EPAM’s top clients. This is a high-impact role, with the opportunity to advance your skills and grow within a global organization. If you'd like to learn more about this position and project, apply now! Connect with a recruiter today!
Req.#564243983
Responsibilities
Evaluates different technologies and suggest which technology suits the client's needs
Completes POC and evolves it into a production-ready solution
Selects and integrates any Big Data tools and frameworks required to provide requested capabilities
Implements ETL process
Monitors performance and advises any necessary infrastructure changes
Defines data retention policies
Elaborates on the technical design of new features
Requirements
4+ years of experience with programming and scripting in Java EE OR Scala
Experience with Spark, SQL, and Bash
Experience dealing with large volumes of data from various sources, both structured and unstructured
Ability to triage and talk through performance/scaling issues of dealing with data at scale
Good understanding of how data will be read (file formats, partitioning, bucketing)
Extensive experience writing testable jobs using the Spark (or equivalent) framework
Web services and API standards: REST, OAuth, and JSON
Software architectures (micro-services, event-driven, peer-to-peer)
Application Security
Asynchronous Pub-Sub and Point-to-Point Messaging Systems
Advantage, if you have experience working in ETL and Hadoop Ecosystem: HBase, Solr, Spark Streaming, Kudu, Spring Boot, Spring Context, Spring Data Rest, General Cloudera experience
Streaming within the Hadoop ecosystem is a plus
Benefits
Extended Healthcare with Prescription Drugs, Dental and Vision Insurance, and Healthcare Spending Account (Company Paid)
Maternity/Parental/Adoption Leave Top-up
Life and AD&D Insurance (Company Paid)
Employee Assistance Program (Company Paid)
Unlimited access to LinkedIn learning solutions
Long-Term Disability
Registered Retirement Savings Plan (RRSP) with company match
Paid Time Off
Critical Illness Insurance
Employee Discounts
Employee Stock Purchase Program
About EPAM
EPAM is a leading global provider of digital platform engineering and development services. We are committed to having a positive impact on our customers, our employees, and our communities. We embrace a dynamic and inclusive culture. Here you will collaborate with multi-national teams, contribute to a myriad of innovative projects that deliver the most creative and cutting-edge solutions, and have an opportunity to continuously learn and grow. No matter where you are located, you will join a dedicated, creative, and diverse community that will help you discover your fullest potential
Apply
EPAM Systems is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, gender, gender identity, sexual orientation, age, status as a protected veteran, status as a qualified individual with disability, or any other protected characteristic under law.
Background investigations are required for all new hires as a condition of employment, after the job offer is made. Employment will not begin until EPAM Systems receives and approves the results of the background check.
Show more
Show less","Scala, Java EE, Spark, SQL, Bash, ETL, REST, OAuth, JSON, Microservices, Eventdriven Architecture, PeertoPeer, Application Security, PubSub Messaging, PointtoPoint Messaging, Hadoop Ecosystem, HBase, Solr, Spark Streaming, Kudu, Spring Boot, Spring Context, Spring Data Rest, Apache Cloudera, LinkedIn Learning, Registered Retirement Savings Plan (RRSP)","scala, java ee, spark, sql, bash, etl, rest, oauth, json, microservices, eventdriven architecture, peertopeer, application security, pubsub messaging, pointtopoint messaging, hadoop ecosystem, hbase, solr, spark streaming, kudu, spring boot, spring context, spring data rest, apache cloudera, linkedin learning, registered retirement savings plan rrsp","apache cloudera, application security, bash, etl, eventdriven architecture, hadoop ecosystem, hbase, java ee, json, kudu, linkedin learning, microservices, oauth, peertopeer, pointtopoint messaging, pubsub messaging, registered retirement savings plan rrsp, rest, scala, solr, spark, spark streaming, spring boot, spring context, spring data rest, sql"
Staff Data Science Engineer,SecurityScorecard,Canada,https://ca.linkedin.com/jobs/view/staff-data-science-engineer-at-securityscorecard-3780256386,2023-12-17,Sarnia-Clearwater, Canada,Mid senior,Remote,"About SecurityScorecard:
SecurityScorecard is the global leader in cybersecurity ratings, with over 12 million companies continuously rated, operating in 64 countries. Founded in 2013 by security and risk experts Dr. Alex Yampolskiy and Sam Kassoumeh and funded by world-class investors, SecurityScorecard’s patented rating technology is used by over 25,000 organizations for self-monitoring, third-party risk management, board reporting, and cyber insurance underwriting; making all organizations more resilient by allowing them to easily find and fix cybersecurity risks across their digital footprint.
Headquartered in New York City, our culture has been recognized by Inc Magazine as a ""Best Workplace,” by Crain’s NY as a ""Best Places to Work in NYC,"" and as one of the 10 hottest SaaS startups in New York for two years in a row. Most recently, SecurityScorecard was named to Fast Company’s annual list of the World’s Most Innovative Companies for 2023 and to the Achievers 50 Most Engaged Workplaces in 2023 award recognizing “forward-thinking employers for their unwavering commitment to employee engagement.” SecurityScorecard is proud to be funded by world-class investors including Silver Lake Waterman, Moody’s, Sequoia Capital, GV and Riverwood Capital.
About the Role:
As a Staff Data Science Engineer at SecurityScorecard, you’ll contribute to an AI research and delivery team dedicated to building highly scalable, low-latency, real-time streaming event-driven data mesh infrastructure with Machine Learning REST APIs and batch processing for event-driven data lake architecture. This infrastructure supports internal operations, data science research, and consumer-facing systems and APIs. This position serves as a key facilitator for data science operations, driving the creation of machine learning feature stores and implementing data cleansing, normalization, flattening, and enrichment processes. These processes aggregate extensive and wide data sets for 360 security insights, advanced AI model research, customer-facing systems, and internal reporting. If you're a problem solver, effective communicator, and enthusiastic about driving advancements in AI and ML in the security space, we want you on our team.
Responsibilities:
Build, architect, and maintain data infrastructure to empower and accelerate AI research and delivery of ML model REST APIs while maintaining cost efficiency.
Build, automate, and maintain batch and streaming data pipelines with cleansing, normalization, regularization and enrichment processes to automate the preparation of data for AI models, from diverse raw inputs to aggregated feature stores.
Become an expert in all security data sources and associated lineage and other nuances within the data to collaborate with research scientists.
Create well-formed schemas and SQL migration processes as required for RDBMS (Postgres), KVS (ScyllaDB), streaming (Kafka) and vector data stores.
Write high quality code with unit tests as well as data tests and perform code reviews.
Build and maintain robust, scalable, and low latency ML Model REST APIs.
Build high quality data cleansing and aggregation processes with associated testing and monitoring to prevent corrupt data from flowing in our ML models.
Influence and coach a distributed team of engineers, ensuring alignment and clarity on goals and timelines.
Required Qualifications:
5+ years experience in data engineering or software engineering.
Proficient in building and scaling highly available consumer-facing applications.
Skilled in designing enterprise real-time event-driven data lake architecture on distributed file systems like Hadoop, AWS, GCP, etc.
Expertise in creating streaming data-mesh infrastructure using Kafka.
Experience developing efficient, scalable, and reliable batch data lake ETL processes using Spark.
Familiarity with serialization/deserialization formats for real-time and batch data processing (e.g., Parquet, Protobuf, Avro).
Knowledge of architecting and scaling systems based on various data stores (RDBMs, KVS, in-memory data stores, etc.).
Skilled in software application development with modern languages (Python, Scala, Java, etc.), and best practices, standards and conventions.
Experience with Git version control, CI/CD pipelines, and Agile project management.
Proficiency in creating Docker containers, shell scripts, and familiarity with data orchestration tools (e.g., Airflow, Dagster, Dolt, and Great Expectations).
Autonomous work style with strong communication and interpersonal skills.
Strong technical estimating skills and analytical abilities.
Detail-oriented and capable of managing multiple assignments/projects simultaneously.
Ownership mentality, adaptability to change, and a customer-focused approach.
Strong written and verbal communication skills.
Preferred Qualifications:
You have a bachelors or greater in computer science, STEM or related field.
Skilled with tools such as Ray, Airflow, Argo, MLFlow, and vector databases.
Experience in the security industry.
Benefits:
Specific to each country, we offer a competitive salary, stock options, Health benefits, and unlimited PTO, parental leave, tuition reimbursements, and much more!
SecurityScorecard is committed to Equal Employment Opportunity and embraces diversity. We believe that our team is strengthened through hiring and retaining employees with diverse backgrounds, skill sets, ideas, and perspectives. We make hiring decisions based on merit and do not discriminate based on race, color, religion, national origin, sex or gender (including pregnancy) gender identity or expression (including transgender status), sexual orientation, age, marital, veteran, disability status or any other protected category in accordance with applicable law.
We also consider qualified applicants regardless of criminal histories, in accordance with applicable law. We are committed to providing reasonable accommodations for qualified individuals with disabilities in our job application procedures. If you need assistance or accommodation due to a disability, please contact talentacquisitionoperations@securityscorecard.io.
Any information you submit to SecurityScorecard as part of your application will be processed in accordance with the Company’s privacy policy and applicable law.
SecurityScorecard does not accept unsolicited resumes from employment agencies. Please note that we do not provide immigration sponsorship for this position.
Show more
Show less","Python, Scala, Java, Kafka, Hadoop, Spark, Git, Docker, Airflow, Dagster, Dolt, Great Expectations, Ray, Argo, MLFlow, SQL, RDBMS, KVS, Serialization, Deserialization, Parquet, Protobuf, Avro, Vector databases, Machine Learning, Data Engineering, Software Engineering, Data mesh infrastructure, Eventdriven data lake architecture","python, scala, java, kafka, hadoop, spark, git, docker, airflow, dagster, dolt, great expectations, ray, argo, mlflow, sql, rdbms, kvs, serialization, deserialization, parquet, protobuf, avro, vector databases, machine learning, data engineering, software engineering, data mesh infrastructure, eventdriven data lake architecture","airflow, argo, avro, dagster, data engineering, data mesh infrastructure, deserialization, docker, dolt, eventdriven data lake architecture, git, great expectations, hadoop, java, kafka, kvs, machine learning, mlflow, parquet, protobuf, python, ray, rdbms, scala, serialization, software engineering, spark, sql, vector databases"
Future Opportunity- Data Engineering Consultant,Avanade,"Newark, NJ",https://www.linkedin.com/jobs/view/future-opportunity-data-engineering-consultant-at-avanade-3781778845,2023-12-17,Bound Brook,United States,Associate,Onsite,"Job Description
RECRUITING FOR THE FUTURE
This posting is for a future job opportunity.
To support the needs of our clients, Avanade is actively recruiting and interviewing for the role outlined below, as we continue to build on another year of growth in our business. Now, with more than 60,000 employees around the globe, we are positioning our organization to effectively prepare for the future. What does this mean for you? We encourage you to apply and interview for this role without the need to decide now. This allows you to connect with leaders and hiring managers at a pace that works for you and when roles become available, you will be the first know.
Data Engineering Consultant
Our talented Data and AI Practice is made up of globally recognized experts- and there's room for more analytical and ambitious data professionals. If you're passionate about helping clients make better data-driven decisions to tackle their most complex business issues. let's talk. Take your skills to a new level and launch a career where you can can truly do what matters.
Come join us
As a Data Engineer within our Data and AI practice, you’ll design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.
Together we do what matters.
What You'll Do
Use your knowledge to plan and deliver data warehouses and storage
Take part in crafting and running bespoke data services for individual projects
Stay up to date with business best practice in using and retrieving data
Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining
Customize storage and extraction, metadata, and information repositories
Build and use effective metrics and monitoring processes
Help to develop business intelligence tools
Craft and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
Skills And Experiences
Strong knowledge of Python, Spark, and T-SQL
Microsoft Fabric/Synapse, Purview and Azure Databricks, PowerBI
Database, storage, collection and aggregation models, techniques, and technologies – and how to apply them in business
Ability to use technology to aim business problems using one or more Microsoft Analytics services for building data pipelines, data streams, and system integration
About You
Characteristics that can spell success for this role:
Advanced knowledge in entity and relationship extraction from unstructured data
Understanding of database table indexing, with emphasis on clustered column store tables
Proficient in data handling, understanding, analysis and interpretation of results
Experience with SQL technologies: Databricks (Spark), OR Azure Synapse
Data security: Must be able to demonstrate an understanding of data security at rest and in transit
Scale: Deliver performance at scale, and accurately anticipate system enhancement needs
Data integrity: Data manipulation, error identification and handling, and modeling
Enjoy your career
Some of the best things about working at Avanade
People-first culture that supports innovation and encourages people to move forward
Turn your ideas to human impact by cultivating a team that will help clients unlock what’s next
Opportunities to innovate in the Microsoft platform while charting your own career path
On-the-job growth through hackathons, innovation contests, and other expressions of our passion for innovating with purpose
Real-time access to technical and skilled resources globally
Find out more about some of our benefits here .
A great place to work
As you bring your skills and abilities to Avanade, you’ll get distinctive experiences, limitless learning, and ambitious growth in return. As we continue to build our diverse and inclusive culture, we become even more innovative and creative, helping us better serve our clients and our communities. You’ll join a community of smart, supportive collaborators to lift, mentor, and guide you, but to also lean on your expertise . You get a company purpose-built for business-critical, leading-edge technology solutions, committed to improving the way humans work, interact, and live. It’s all here, so take a closer look!
We work hard to provide an inclusive, diverse culture with a deep sense of belonging for all our employees. Visit our Inclusion & Diversity page.
Create a future for our people that focuses on
Expanding your thinking
Experimenting courageously
Learning and pivoting
Inspire greatness in our people by
Empowering every voice
Encouraging boldness
Celebrating progress
Accelerate the impact of our people by
Amazing the client
Prioritizing what matters
Acting as one
Learn more
To learn more about the types of projects our Data & AI team works on check out these case studies:
thyssenkrupp Materials Services uses data to help strike a delicate operational balance
What matters to SSE Renewables is innovating for a sustainable future
Hachette UK enables employees with machine learning-driven contract searches
Marston Holdings identifies significant cloud savings
Interested in knowing what’s going on inside Avanade? Check out our blogs:
Avanade Insights – exchange ideas that drive tomorrow’s innovation
Inside Avanade – explore what life is like working at Avanade
Compensation for roles at Avanade varies depending on a wide array of factors including but not limited to the specific office location, role, skill set and level of experience. As required by local law, Avanade provides a reasonable range of compensation for roles that may be hired in New York as set forth below and information on benefits offered is here.
Range of Starting Pay for role:
New York: $121,500 – $ 143,000
About Avanade
Avanade is the leading provider of innovative digital, cloud and advisory services, industry solutions and design-led experiences across the Microsoft ecosystem. Every day, our 59,000 professionals in 26 countries make a genuine human impact for our clients, their employees and their customers.
We have been recognized as Microsoft’s Global SI Partner of the Year more than any other company. With the most Microsoft certifications (60,000+) and 18 (out of 18) Gold-level Microsoft competencies, we are uniquely positioned to help businesses grow and solve their toughest challenges.
We are a people first company, committed to providing an inclusive workplace where employees feel comfortable being their authentic selves. As a responsible business, we are building a sustainable world and helping people from underrepresented communities fulfil their potential.
Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com .
Show more
Show less","Python, Spark, TSQL, Data warehouses, Data storage, Data services, Business intelligence, Entity and relationship extraction, Database indexing, Data handling, Data analysis, Data interpretation, SQL technologies, Data security, Data manipulation, Error identification, Data modeling, Azure Synapse, Microsoft Fabric, Microsoft Purview, Azure Databricks, PowerBI","python, spark, tsql, data warehouses, data storage, data services, business intelligence, entity and relationship extraction, database indexing, data handling, data analysis, data interpretation, sql technologies, data security, data manipulation, error identification, data modeling, azure synapse, microsoft fabric, microsoft purview, azure databricks, powerbi","azure databricks, azure synapse, business intelligence, data handling, data interpretation, data manipulation, data security, data services, data storage, data warehouses, dataanalytics, database indexing, datamodeling, entity and relationship extraction, error identification, microsoft fabric, microsoft purview, powerbi, python, spark, sql technologies, tsql"
Customs Compliance Data Analyst-1,Open Systems Inc.,"Newark, NJ",https://www.linkedin.com/jobs/view/customs-compliance-data-analyst-1-at-open-systems-inc-3755511713,2023-12-17,Bound Brook,United States,Associate,Onsite,"Customs Compliance Data Analyst-1
3 Months Contract (possible extension or conversion)
Newark NJ 07102 (100% Remote)
Job Description
Top 3 Skils for the ideal candidate: o Experience in Customs Compliance o Understanding of customs import entries o Attention to detail
Level of experience required:
Minimum of 2 years experience as an entry writer
Requires understanding 7501 form, its contents and how invoice information is allocated across a 7501 entry
Requires ability to gather data elements such invoice number, invoice date, part#, description, country of origin, etc. Expected output:
Panasonic would train on systems to be accessed and the process. Expected output 40-50 line items /day (assuming 7-8 hours per day)
Additional Job Description
The type of work would be:
Request entry packets from brokers
Save, maintain and markup entry packets and upload them to outside counsel’s website
Analyze entry packets and validate the correct part#, quantity, country of origin, shipper, commercial invoice number and values and other information and update outside counsels spreadsheet
Provide stats and discuss weekly output with project team
Supporting the weekly meetings with reports on the status of the ongoing sampling.
Who We Are
Open Systems Inc. (OSI) was founded in 1994 to provide information technology solutions and staffing services to large and mid-size companies across the U.S. Our corporate office is located at 6495 Shiloh Road, Ste 310 Alpharetta, GA 30005. We provide a full range of staffing services including contract, contract-to-hire, and direct hire solutions. Our technical recruiting experts are experienced in technical screening, candidate sourcing, and behavioral interviewing techniques. They focus on providing candidates who match your technical requirements and fit seamlessly into your company culture.
Contact Open Systems, Inc. anytime by website, phone or email. We look forward to hearing from you!!
Show more
Show less","Customs Compliance, Entry Writer, 7501 Form, Data Gathering, Invoice Analysis, Country of Origin, Data Validation, Reporting, Statistical Analysis, Spreadsheet Software, Project Management, Communication","customs compliance, entry writer, 7501 form, data gathering, invoice analysis, country of origin, data validation, reporting, statistical analysis, spreadsheet software, project management, communication","7501 form, communication, country of origin, customs compliance, data gathering, data validation, entry writer, invoice analysis, project management, reporting, spreadsheet software, statistical analysis"
Lead Data Scientist - Remote,Get It Recruit - Information Technology,"Holmdel, NJ",https://www.linkedin.com/jobs/view/lead-data-scientist-remote-at-get-it-recruit-information-technology-3782861369,2023-12-17,Bound Brook,United States,Mid senior,Remote,"Our client is embarking on a transformation journey, aiming to become a forward-thinking company dedicated to enhancing the well-being of its customers and their families. With the recent addition of a Chief Data & Analytics Officer (CDAO) to lead the Enterprise Data and Analytic Office (EDAO), we are excited to offer an exceptional opportunity to join us in contributing to the ongoing evolution of our organization.
The EDAO team plays a pivotal role in fostering a data-driven culture across the company, enabling the successful achievement of our strategic goals. Our primary focus revolves around extracting business value from data and analytics. Key responsibilities include overseeing the data lifecycle, developing actionable insights, and delivering data products. Our team comprises data analysts, data product owners, data engineers, data scientists, and data business leaders, all playing a crucial role in driving revenue growth, managing risks, and enhancing customer experiences.
In response to the ever-changing landscape of technology, societal shifts, and evolving consumer needs, our client has established a Data Science Lab (DSL) to reimagine insurance and embrace data-driven decision-making. The DSL is committed to accelerating innovation by enabling rapid testing of new technologies and translating groundbreaking research into practical, enterprise-wide solutions.
We are currently seeking an experienced individual contributor with a strong background in LLM (Large Language Models) and Generative AI. As a part of our team, you will be responsible for developing advanced data science solutions that harness machine learning and artificial intelligence to drive innovation across various business lines and products. Your role will involve collaborating with senior executives on high-impact projects, delivering AI/ML solutions that will be market-tested and deployed to significantly impact risk management and overall financial performance. Successful candidates should bring expertise in insurance and financial services, a passion for implementing cutting-edge ML and AI insights, and the ability to design and implement data science capabilities that drive growth, competitive advantage, and customer satisfaction.
Your Responsibilities Include
Develop Deep Learning/Large Language Model/Generative AI Capabilities
Uncover insights from unstructured data sources like insurance contracts, medical records, sale notes, and customer servicing logs.
Create AI/ML solutions to enhance underwriting risk assessment, claims auto adjudication, and customer servicing.
Conduct large-scale experiments, from unsupervised pre-training to fine-tuning, retrieval augmentation, and prompt engineering.
Scale LLM models during both development and production.
Design and develop high-quality prompts and templates to guide LLM behavior and responses.
Evaluate LLM models based on statistical tests, business metrics, and regulatory considerations.
Develop Enterprise Test and Learn Capabilities
Explore experimentation practices and causal inferencing/ML techniques to optimize our business processes.
Develop and execute advanced data-driven experiments to enhance business performance.
Create test hypotheses, design experiments, select KPIs, and collect and analyze data.
Support and Contribute to the Data Science Lab (DSL)
Assist in use case development, data exploration, project/sample design, data processing, analysis, and report creation.
Handle data wrangling, data matching, ETL processes, and data source identification.
Utilize advanced statistical and AI/ML techniques to build predictive models and conduct analyses.
Ensure data quality checks throughout model/solution development and production.
Package and deploy models/solutions in collaboration with Data Engineers and MLOps.
Contribute to the Overall Data Science Organization
Collaborate with cross-functional teams, including Data Science, Data Engineering, and Business groups.
Contribute to standardization of Data Science tools, processes, and best practices.
You Are
Passionate about cutting-edge technology and eager to apply new AI/ML algorithms and approaches.
Analytically driven, intellectually curious, and experienced in developing data and analytic solutions to solve complex business problems.
Enthusiastic about collaborating with a diverse team, including data engineers, business analysts, software developers, and business leaders.
You Have
A PhD with 2+ years of experience or a Master's degree with 4+ years of experience in Statistics, Computer Science, Engineering, Applied mathematics, or a related field.
3+ years of hands-on ML modeling/development experience.
Strong foundations in probability, statistics, and causal inferencing techniques.
Extensive experience in deep learning models, including Large Language Models (LLM) and Natural Language Processing (NLP).
Proficiency in GPU, distributed computing, and parallelism in ML solutions.
Strong programming skills in Python, including PyTorch and/or Tensorflow.
A solid background in algorithms and a range of ML models.
Excellent communication skills and the ability to collaborate across functions at both the leadership and hands-on levels.
Exceptional analytical and problem-solving abilities with great attention to detail.
Proven leadership in providing technical guidance and mentoring to data scientists, as well as strong management skills to ensure enterprise success.
This position offers a hybrid remote work arrangement. Join us in our journey to shape the future of insurance through data-driven innovation.
Employment Type: Full-Time
Show more
Show less","Python, PyTorch, Tensorflow, Data Analysis, Data Science, Machine Learning, Artificial Intelligence, Deep Learning, Large Language Model, Generative AI, NLP, Statistics, Probability, Causal Inferencing, Test Hypotheses, Design Experiments, Data Wrangling, Data Quality Checks, Data Standardization, Best Practices, GPU, Distributed Computing, Parallelism, Algorithms","python, pytorch, tensorflow, data analysis, data science, machine learning, artificial intelligence, deep learning, large language model, generative ai, nlp, statistics, probability, causal inferencing, test hypotheses, design experiments, data wrangling, data quality checks, data standardization, best practices, gpu, distributed computing, parallelism, algorithms","algorithms, artificial intelligence, best practices, causal inferencing, data quality checks, data science, data standardization, data wrangling, dataanalytics, deep learning, design experiments, distributed computing, generative ai, gpu, large language model, machine learning, nlp, parallelism, probability, python, pytorch, statistics, tensorflow, test hypotheses"
Senior Software Engineer - Data Strategy (NYC-Hybrid),Rad Hires,"Staten Island, NY",https://www.linkedin.com/jobs/view/senior-software-engineer-data-strategy-nyc-hybrid-at-rad-hires-3747288128,2023-12-17,Bound Brook,United States,Mid senior,Hybrid,"Role
The Senior Software Engineer position in the Data Strategy team presents a chance to create and execute data products for the world's biggest and most reputable reinsurance brokerage. Data Strategy has a “start-up style” mandate (within a $2 billion company) to enhance the acquisition, storage, analysis, fidelity, and monetization of client, internal, and third-party data across the organization. This innovation spans our petabyte-scale insured assets, including property, business, marine, and aviation entities, and their associated risks, such as hurricanes, wildfires, cyber-attacks, and wars, in a financial and economic context.
As a member of the Data Strategy group, the Senior Software Engineer will work with fellow data and web engineers, data scientists, product managers, business analysts, and stakeholders from other internal groups to design and improve data-centric projects with the dual mandate of (1) increasing the efficiency of the data collection and analysis process across the company and (2) driving the monetization of data via newly designed and existing products for the company’s reinsurance clients. The Senior Software Engineer will be the head facilitator on multiple innovative initiatives and will have ownership over the design, development, and delivery of projects requiring direct reporting to senior-level management in both business and technical groups.
Leadership Responsibilities
Work with a product manager as technical lead of a team of ~5 engineers, data scientists, and analysts to design, scope, and oversee work in an Agile environment.
Manage junior data and web engineers, focusing on productivity, quality, and professional development.
Partner with the head of Data Strategy and other senior engineers to create and evangelize best-in-class engineering competency and tooling within the organization.
Enforce strong development standards across the team through code reviews, automated testing, and monitoring.
Establish strong relationships with internal clients as an engineering representative for data strategy.
Contribute to the overall Data Strategy vision and execution via quarterly planning and executive committee reporting.
Partner regularly improving engineering recruiting process for the required skillsets and resourcing demands.
Learn the complex business of reinsurance to coach data technologists and execute the team's initiatives more effectively.
Software Engineer Responsibilities
Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting, and new data products.
Innovate new ways to leverage large and small datasets to drive revenue via the development of new products with the Data Strategy team, as well as the enhancement of existing products.
Architect engineering solutions using the latest cloud technologies in a process that spans hypothesis-validating prototypes to large-scale production data products, ensuring internal security and regulatory compliance.
Design solutions that account for unstructured data and document management system(s), including ingesting, tracking, parsing, analyzing, and summarizing documents at scale.
Perform exploratory and goal-oriented data analyses to understand and validate the requirements of data products and help create product roadmaps.
Develop, implement, and deploy front-ends and APIs, which may involve business intelligence dashboards, data pipelines, machine learning algorithms, and file ingestion mechanisms.
Work closely with data scientists, data engineers, web engineers, PMs, and other stakeholders to design & develop products.
Keep current on the latest trends and innovations in data technology and how these trends apply to the company's business and data strategy.
Required Qualifications
5-8+ years of relevant experience in data-focused software engineering
Master’s Degree or Ph.D. in data science, computer science, or related quantitative field such as applied mathematics, statistics, engineering or operations research, or equivalent experience
Experience in Python and familiarity with OOP and functional programming principles
Strong knowledge of SQL and familiarity with the high-level properties of modern data stores.
Strong understanding of the contemporary SDLC, including dev/QC/prod environments, unit/integration/UA testing, CI/CD, etc.
Experience building and maintaining CI/CD pipelines with tools such as Azure DevOps, GitLab, Travis, Jenkins, etc.
At least two and ideally all of the following sets of experience:
Data Engineering
2+ years’ experience with data engineering
Extensive experience with (py)Spark, Python, JSON, and SQL
Experience integrating data from semi-structured and unstructured sources
Knowledge of various industry-leading SQL and NoSQL database systems
Backend Web
2+ years of backend/full-stack web engineering
Experience working with Python-based server-side web frameworks like FastAPI or Django
Experience with complex backends involving multiple data stores, asynchronous worker queues, pub-sub messaging, and the like
Knowledge of cloud-based web deployments (AWS/Azure/GCP, Kubernetes, auto-scaling, etc.)
Experience with one or more major frontend frameworks (React strongly preferred)
Data Science/Analytics
2+ years of data analysis, AI, or data science work
Experience with data cleaning, enrichment, and reporting to business users
Experience selecting, training, validating, and deploying machine-learning models
Experience with or strong interest in learning about LLMs in a productized context
Experience working in an Agile environment to facilitate the quick and effective fulfillment of group goals
Good interpersonal and communication skills for establishing and maintaining sound internal relationships, working well as part of a team, and for presentations and discussions
Strong analytical skills and intellectual curiosity (interest in the meaning and usefulness of the data), as demonstrated through academic experience or work assignments
Excellent English verbal and writing skills for complex communications with company colleagues in all departments and levels of the organization, including communicating technical concepts to a non-technical audience
Good ability to prioritize workload according to volume, urgency, etc., and to deliver on required projects in a timely fashion
Preferred Qualifications
Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks
Experience with web scraping and crowdsourcing technologies
Experience with Databricks and optimizing Spark clusters
Experience architecting web ecosystems from the ground up, including monolith vs. microservice decisions, caching technologies, security integrations, etc.
Experience working with data visualization dashboarding tools (PowerBI, Tableau)
Insurance domain knowledge or strong interest in developing it
Experience with the MS Azure cloud environment
Show more
Show less","Python, SQL, NoSQL, Data Engineering, Backend Web, Data Science/Analytics, OOP, Functional Programming, SDLC, CI/CD, Azure DevOps, GitLab, Travis, Jenkins, Spark, JSON, FastAPI, Django, React, LLMs, Agile, Databricks, PowerBI, Tableau, Microservices, Caching Technologies, Security Integrations","python, sql, nosql, data engineering, backend web, data scienceanalytics, oop, functional programming, sdlc, cicd, azure devops, gitlab, travis, jenkins, spark, json, fastapi, django, react, llms, agile, databricks, powerbi, tableau, microservices, caching technologies, security integrations","agile, azure devops, backend web, caching technologies, cicd, data engineering, data scienceanalytics, databricks, django, fastapi, functional programming, gitlab, jenkins, json, llms, microservices, nosql, oop, powerbi, python, react, sdlc, security integrations, spark, sql, tableau, travis"
Sr. Data Engineer,Dice,"Raleigh, NC",https://www.linkedin.com/jobs/view/sr-data-engineer-at-dice-3788782905,2023-12-17,Raleigh,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Zachary Piper Solutions, LLC, is seeking the following. Apply via Dice today!
Piper Companies
is seeking a
Sr. Data Engineer
to join a flourishing healthcare application company based out of Raleigh, NC. The
Sr. Data Engineer
will be responsible for the engineering, analytics and automation of financial data.
Responsibilities of the
Sr. Data Engineer
includes:
Provide data manipulation and engineering for financial, insurance or mortgage industry systems and data - primarily utilizing SAS for data analyses
Utlize tools for automation such SQL and Linux Shell Scripting
Deliver back-end data solutions in an AWS cloud environment
Report analyses utilizing Tableau
Qualifications for the
Infrastructure Lead
includes :
4+ years of SAS software
2+ years of experience with Linux Shell scripting
Experience witj electronic/data file transfers and security concepts
Working knowledge of Linux operating systems, networking and storage technologies
Bachelor's degree is highly preferred
Compensation for the
Sr. Data Engineer
includes :
Salary Range: $60-$65/hr
Benefits: PTO, Paid Holidays, Healthcare, Dental, Vision, 401k, other
Show more
Show less","Data Engineering, Analytics, Automation, SAS, SQL, Linux Shell Scripting, AWS Cloud, Tableau, Electronic/Data File Transfers, Security Concepts, Linux Operating Systems, Networking Technologies, Storage Technologies","data engineering, analytics, automation, sas, sql, linux shell scripting, aws cloud, tableau, electronicdata file transfers, security concepts, linux operating systems, networking technologies, storage technologies","analytics, automation, aws cloud, data engineering, electronicdata file transfers, linux operating systems, linux shell scripting, networking technologies, sas, security concepts, sql, storage technologies, tableau"
"SENIOR SYSTEMS ENGINEER – ETL, Data Bricks, Kafka, Informatica",General Dynamics Information Technology,"Raleigh, NC",https://www.linkedin.com/jobs/view/senior-systems-engineer-%E2%80%93-etl-data-bricks-kafka-informatica-at-general-dynamics-information-technology-3784153741,2023-12-17,Raleigh,United States,Mid senior,Onsite,"Job Description:
Type of Requisition:
Regular
Clearance Level Must Currently Possess:
None
Clearance Level Must Be Able To Obtain:
None
Suitability:
Public Trust/Other Required:
Job Family:
Systems Engineering
Skills:
Job Qualifications:
Informatica PowerCenter, MongoDB, SUSE Linux
Certifications:
Experience:
10 + years of related experience
US Citizenship Required:
No
Job Description:
SENIOR SYSTEMS ENGINEER – ETL, Data Bricks, Kafka, Informatica
Seize your opportunity and make a personal impact as at GDIT. Own your opportunity to work alongside federal civilian agencies. Make an impact by providing services that help the government ensure the well-being of U.S. citizens.
At GDIT, people are our differentiator. As Senior Systems Engineer, you will help ensure today is safe and tomorrow is smarter. Our work depends on a Systems Engineer joining our team to support our client and their mission.
HOW A SENIOR SYSTEMS ENGINEER WILL MAKE AN IMPACT
Experience with Informatica EDC and ETL tools
Performing extraction, transformation, and loading (ETL) of data from multiple sources into appropriate landing tables.
Profiling incoming data to identify various patterns, inconsistencies, column characteristics, distinct values and perform data distribution.
Cleansing and manipulating data into acceptable values (trimming, normalization, various data manipulation functions, etc.), and using address validation of incoming records (using Address Doctor) to identify exceptions.
Loading the cleansed data into staging tables for EDC consumption.
Creating Data Quality Scorecards on various attributes to ‘grade’ the source data to the Golden Record (completeness, accuracy, duplication, consistency, conformity, integrity), and Data Analyst tool allows data stewards to choose components, create and refresh scorecards.
Scheduling the above listed tasks for automation purposes.
REQUIRED SKILLS & ABILITIES:
Experience with EXTRATION, TRANSFORMATION and LOADING (ETL) experience with data from multiple sources
Must have strong experience with DATA BRICKS
Must have experience in utilizing KAFKA queues
Strong with Informatica ETL coding, Shell Scripting, PL SQL
Experience with SUSE Linux and RED HAT
Exposure in using INFORMATICA or other ETL products
Must have experience in various cloud platforms (AZURE, GOOGLE CLOUD PLATFORM, INFORMATICS CLOUD) and data stores
Must have experience in creating APIs
WHAT YOU’LL NEED TO SUCCEED
Strong background in supporting Informatica Data Quality and Informatica cloud
Experience with SUSE Linux and Informatica product(s)
Strong database experience in any or all: Oracle, SQL Server, Teradata
Strong in Informatica ETL coding, shell scripting and have Linux/RedHat experience
Ability to research tool & platform related problems and document
SECURITY CLEARANCE LEVEL:
Ability to achieve a Public Trust clearance - the security clearance for this program requires the selected candidate to have resided in the US for the past five years. The selected candidate cannot have left the country for longer than 90 consecutive days and no more than 180 cumulative days.
LOCATION:
Raleigh, NC or Remote, East Coast Time
GDIT IS YOUR PLACE:
401K with company match
Comprehensive health and wellness packages
Internal mobility team dedicated to helping you own your career
Professional growth opportunities including paid education and certifications
Cutting-edge technology you can learn from
Rest and recharge with paid vacation and holidays
SECURITY CLEARANCE LEVEL:
Ability to achieve a Public Trust clearance - the security clearance for this program requires the selected candidate to have resided in the US for the past five years. The selected candidate cannot have left the country for longer than 90 consecutive days and no more than 180 cumulative days.
LOCATION:
Raleigh, NC or Remote, East Coast Time
GDIT IS YOUR PLACE:
401K with company match
Comprehensive health and wellness packages
Internal mobility team dedicated to helping you own your career
Professional growth opportunities including paid education and certifications
Cutting-edge technology you can learn from
Rest and recharge with paid vacation and holidays
The likely salary range for this position is $112,000 - $168,000. This is not, however, a guarantee of compensation or salary. Rather, salary will be set based on experience, geographic location and possibly contractual requirements and could fall outside of this range.
Scheduled Weekly Hours:
40
Travel Required:
None
Telecommuting Options:
Remote
Work Location:
USA NC Raleigh
Additional Work Locations:
Any Location / Remote
Total Rewards At GDIT:
Our benefits package for all US-based employees includes a variety of medical plan options, some with Health Savings Accounts, dental plan options, a vision plan, and a 401(k) plan offering the ability to contribute both pre and post-tax dollars up to the IRS annual limits and receive a company match. To encourage work/life balance, GDIT offers employees full flex work weeks where possible and a variety of paid time off plans, including vacation, sick and personal time, holidays, paid parental, military, bereavement and jury duty leave. GDIT typically provides new employees with 15 days of paid leave per calendar year to be used for vacations, personal business, and illness and an additional 10 paid holidays per year. Paid leave and paid holidays are prorated based on the employee’s date of hire. The GDIT Paid Family Leave program provides a total of up to 160 hours of paid leave in a rolling 12 month period for eligible employees. To ensure our employees are able to protect their income, other offerings such as short and long-term disability benefits, life, accidental death and dismemberment, personal accident, critical illness and business travel and accident insurance are provided or available. We regularly review our Total Rewards package to ensure our offerings are competitive and reflect what our employees have told us they value most.
We are GDIT. A global technology and professional services company that delivers consulting, technology and mission services to every major agency across the U.S. government, defense and intelligence community. Our 30,000 experts extract the power of technology to create immediate value and deliver solutions at the edge of innovation. We operate across 30 countries worldwide, offering leading capabilities in digital modernization, AI/ML, Cloud, Cyber and application development. Together with our clients, we strive to create a safer, smarter world by harnessing the power of deep expertise and advanced technology.
We connect people with the most impactful client missions, creating an unparalleled work experience that allows them to see their impact every day. We create opportunities for our people to lead and learn simultaneously. From securing our nation’s most sensitive systems, to enabling digital transformation and cloud adoption, our people are the ones who make change real.
GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
Show more
Show less","Informatica PowerCenter, MongoDB, SUSE Linux, Data Bricks, Kafka, Informatica EDC, ETL, SQL, Shell Scripting, PL SQL, RED HAT, Azure, Google Cloud Platform, Informatics Cloud, Informatica Data Quality, Oracle, SQL Server, Teradata, APIs","informatica powercenter, mongodb, suse linux, data bricks, kafka, informatica edc, etl, sql, shell scripting, pl sql, red hat, azure, google cloud platform, informatics cloud, informatica data quality, oracle, sql server, teradata, apis","apis, azure, data bricks, etl, google cloud platform, informatica data quality, informatica edc, informatica powercenter, informatics cloud, kafka, mongodb, oracle, pl sql, red hat, shell scripting, sql, sql server, suse linux, teradata"
"Senior Data Engineer - Snowflake, AWS, Relational Data Modeling",Adroit Software Inc.,"Durham, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-snowflake-aws-relational-data-modeling-at-adroit-software-inc-3667474894,2023-12-17,Raleigh,United States,Mid senior,Onsite,"For a financial client we need Senior Data Engineer - Snowflake, AWS, Relational Data Modeling. This position is based in Durham, NC. We are Primarily looking for W2 Candidates and not looking for Third Party Candidates.
The Expertise And Skills You Bring
3+ years of good understanding and experience of Data Warehouse environment withmodelingg like Dimensional, Data Vault and Reporting and Analytics platforms like Snowflake, with writing JSON script, and SnowSQL with tables and views creation
3+ years of solutioning experience with below AWS Cloud environments
Database Migration Service (DMS) and Change Data Capture (CDC)
S3 storage
Simple Queue Service (SQS) and Lamda
Relational Data Store (RDS)
5+ years in solutioning of Oracle development (PL/SQL) and/or Streams based development
5+ years of solutioning and development with ETL technologies
5+ years of relational data modeling
NoSQL experience (DynamoDB) will be a plus
Solid experience developing in an Agile team setting (Kanban and/or SCRUM)
Experience working in a DevOps and CI/CD environment
AWS certification a strong plus
A Bachelor's or Master's degree in Computer Science, Information Technology, or equivalent experience
Ability to think out of box and design end-to-end solutions
Passion and intellectually curious to learn new technologies and business areas
Ability to deal with ambiguity and work in a fast-paced environment
Excellent verbal and written communication skills
Excellent collaboration skills to work with multiple teams in the organization
Show more
Show less","Snowflake, AWS, Relational Data Modeling, JSON, SnowSQL, Database Migration Service (DMS), Change Data Capture (CDC), S3 storage, Simple Queue Service (SQS), Lambda, Relational Data Store (RDS), Oracle, PL/SQL, ETL, NoSQL, DynamoDB, Agile, Kanban, SCRUM, DevOps, CI/CD, Computer Science, Information Technology","snowflake, aws, relational data modeling, json, snowsql, database migration service dms, change data capture cdc, s3 storage, simple queue service sqs, lambda, relational data store rds, oracle, plsql, etl, nosql, dynamodb, agile, kanban, scrum, devops, cicd, computer science, information technology","agile, aws, change data capture cdc, cicd, computer science, database migration service dms, devops, dynamodb, etl, information technology, json, kanban, lambda, nosql, oracle, plsql, relational data modeling, relational data store rds, s3 storage, scrum, simple queue service sqs, snowflake, snowsql"
Senior Data Engineer - Vice President,Russell Tobin,"Cary, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-vice-president-at-russell-tobin-3751623348,2023-12-17,Raleigh,United States,Mid senior,Onsite,"What are we looking for in our Senior Data Engineer - Vice President?
Job title: Sr Data Engineer
Location: Cary NC
Role: Fulltime Permanent (W2)
Salary: 180k + Benefits
Details:
The manager is looking for a strong Data Engineer. Who will be coding 70% and leading 30%.
Who has good experience in writing Python Code.
Candidates will build pipelines.
Should know how to use cases works.
Banking experience is nice to have.
Nice to have: GCP and knowledge of ML (Machine Learning).
Responsibilites:
Build the Infrastructure required for optimal ETL and ELT tools from a variety of data sources using batch processing
Consult on improving data reliability, efficiency, and quality
Ensure architecture supports business requirements
Collaborate with stakeholders to understand requirements, evaluate, and refine stories
Design, implement, and test solutions, providing support through the production process
Build reliable pipe-lines that is easy to support in production and design and develop high-quality and easily maintainable code
Skills:
Degree in Computer Sciences, Math or engineering
Experience building enterprise applications using Python and/or Scala and operating systems such as Unix
Proficiency in Structure Query Language (SQL)
Experience with Apache Ecosystem, including Spark, Hive, and Hadoop
Knowledge of Continuous Integration (CI) and Continuous Deployment (CD) tools, as well as Kubernetes, Apache Airflow and monitoring tools
Rate/Salary: 150k + Benefits
Show more
Show less","Data Engineering, Python, SQL, Apache Ecosystem (Spark Hive Hadoop), Continuous Integration (CI), Continuous Deployment (CD), Kubernetes, Apache Airflow, Monitoring tools, ETL tools, ELT tools, Batch processing, Machine Learning (ML), Banking, Unix operating system, Engineering, Computer Sciences, Mathematics","data engineering, python, sql, apache ecosystem spark hive hadoop, continuous integration ci, continuous deployment cd, kubernetes, apache airflow, monitoring tools, etl tools, elt tools, batch processing, machine learning ml, banking, unix operating system, engineering, computer sciences, mathematics","apache airflow, apache ecosystem spark hive hadoop, banking, batch processing, computer sciences, continuous deployment cd, continuous integration ci, data engineering, elt tools, engineering, etl tools, kubernetes, machine learning ml, mathematics, monitoring tools, python, sql, unix operating system"
Big Data Developer,Lorven Technologies Inc.,"Jersey City, NJ",https://www.linkedin.com/jobs/view/big-data-developer-at-lorven-technologies-inc-3646689812,2023-12-17,New Jersey,United States,Mid senior,Onsite,"Job Description
Provide strong hands-on in architecting/implementing data frameworks in big data space.
Utilize in-depth knowledge of technical and business domain concepts and procedures within own area and basic knowledge of other areas to resolve issues.
Manage a team of professionals to accomplish established goals and conduct personnel duties for team (e.g. performance evaluations, hiring and disciplinary actions)
Enforce industry standard SDLC best practices for effectively managing the codebase.
Partner with other software engineers, dev-ops, and production support staff to deliver robust software solutions
Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.
Skills
Big data development with extensive hands-on experience with Spark programming
Strong knowledge of big data tools like Hive, Impala, Spark, etc.
Strong Knowledge in Java programming languages
Experience in designing solutions based on big data / spark methodology of development
Experience with NoSQL, Web services, Big Data security/entitlements
Should be familiar with algorithms and design patterns
Familiarity with Linux environment including scripting skills
SDLC/Dev Ops - Git/BitBucket, CI/CD pipeline frameworks such as Jenkins, SonarQube, JIRA, Any secure coding toolkits
Knowledge & Experience
Proven engineering experience building robust, scalable and maintainable applications in the Capital Markets Technology industry. Ideally 12+ years of application development experience.
8+ years in a technical leadership role, with experience leading global technology teams.
5+ years of hands-on experience in Big Data development using Spark, Hive on batch and real time processing
Experience in performing optimization on Big Data platform
Experience with Market Risk is a significant advantage.
Competencies
Excellent oral and written English
Ability to collaborate effectively in a large global team
Ability to influence and negotiate with senior leaders
Ability to work well under pressure
Education
Graduate in Computer Science.
Master's degree preferred
Show more
Show less","Data frameworks, Spark, Hive, Impala, NoSQL, Web services, Big data security/entitlements, Algorithms, Design patterns, Linux environment, Scripting, SDLC, Dev Ops, Git, BitBucket, CI/CD, Jenkins, SonarQube, JIRA, Secure coding toolkits, Java programming, Capital Markets Technology, Market Risk, Optimization, Big Data platform","data frameworks, spark, hive, impala, nosql, web services, big data securityentitlements, algorithms, design patterns, linux environment, scripting, sdlc, dev ops, git, bitbucket, cicd, jenkins, sonarqube, jira, secure coding toolkits, java programming, capital markets technology, market risk, optimization, big data platform","algorithms, big data platform, big data securityentitlements, bitbucket, capital markets technology, cicd, data frameworks, design patterns, dev ops, git, hive, impala, java programming, jenkins, jira, linux environment, market risk, nosql, optimization, scripting, sdlc, secure coding toolkits, sonarqube, spark, web services"
Senior Data Engineer,VIR Consultant LLC,"Hoboken, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-vir-consultant-llc-3667466682,2023-12-17,New Jersey,United States,Mid senior,Onsite,"Must Have
Support DW related to Finance
Taking chunks moving to clouds.
Integrating use cases into a single place even though from different stack (dev reporting reqs in PowerBI, Tableau but move to 1 spot)
Create and maintain highly scalable data pipelines across Azure Data Lake Storage, and Azure Synapse using Data Factory, Databricks and Apache Spark/Scala
Experience With Scala, Python, Or R
Responsible for managing a growing cloud-based data ecosystem and reliability of our corporate data lake and analytics data mart
Job Description
The Senior Data Engineer is part of a Corporate Analytics team responsible for supporting data and analytics solutions for MMC Corporate Functions.
This individual will collaborate with analytics team to design and implement MMC corporate data strategy, ensuring reliable data infrastructure and creating data solutions for variety of business use cases.
Responsibilities
Drive innovation within Data Engineering by playing a key role in technology decisions for the future of our data science and analytics
Critical team member in the design and development for highly complex and critical data projects
Leverage research and previous experience to ensure we're up-to-date and continuously exploring
We are passionate about trying new things and we follow through with 10% time and Tech hack-a-thons
Excellent Professional development and Career growth opportunities
What Can You Expect
Drive innovation within Data Engineering by playing a key role in technology decisions for the future of our data science and analytics
Critical team member in the design and development for highly complex and critical data projects
Leverage research and previous experience to ensure we're up-to-date and continuously exploring
We are passionate about trying new things and we follow through with 10% time and Tech hack-a-thons
Excellent Professional development and Career growth opportunities
Qualifications
7+ years of experience in tier 1 or 2 IT services companies or Tier 1 captives
7 years of experience implementing analytics data Solutions leveraging Azure Data Factory, Databricks, Logic Apps, ML Studio, Data Lake, Synapse, Informatica Power Center, and Oracle.
Working experience with Scala, Python, or R
Bachelor's degree or equivalent experience in Computer Science, Information Systems, or related disciplines.
Excellent communication and presentation skills
Domain knowledge in one of more of corporate functions such as HR, Finance, Real Estate is preferred
Excellent Problem-solving skills with innovative and proactive approach
Ability to recommend and implement best practices and processes
Show more
Show less","Azure Data Lake Storage, Apache Spark, Scala, Python, R, Data Factory, Databricks, Azure Synapse, Tableau, PowerBI, Informatica Power Center, Oracle","azure data lake storage, apache spark, scala, python, r, data factory, databricks, azure synapse, tableau, powerbi, informatica power center, oracle","apache spark, azure data lake storage, azure synapse, data factory, databricks, informatica power center, oracle, powerbi, python, r, scala, tableau"
AVP Data Engineer,PRI Technology,"Whitehouse Station, NJ",https://www.linkedin.com/jobs/view/avp-data-engineer-at-pri-technology-3773746713,2023-12-17,New Jersey,United States,Mid senior,Onsite,"This is an onsite role in either Whitehouse Station, NJ or Philadelphia, PA
WHAT YOU'LL BE WORKING ON
Expanding our new claim data warehouse responsible for producing actuarial loss triangles
Implementing BI processes so end users can quickly and easily derive value from billion row datasets
Creating of new ETL process to accurately stitch data together
Helping to design the future cloud architecture and migration of the warehouse
Investigating, diagnosing, and actioning on business questions and requests
Direct handling of production support and maintenance of existing financial feeds
Delivering solutions on time and within budget, meeting both internal change management standards and external SOX controls
QUALIFICATIONS
Microsoft SQL Server guru with strong data warehousing fundamentals
Fluency in other databases, coding languages (eg R, Python, .NET), and BI solutions (eg Shiny, Cognos, PowerBI)
Proficient in DevOps tools (GIT, Jenkins, Octopus)
Adept in Agile project management ideologies
Extensive technical analysis and investigative capabilities
Strong communication skills, comfortable interfacing with senior business users
Financial/actuarial data, application, or practical experience a plus
Show more
Show less","SQL Server, Data Warehousing, R, Python, .NET, Shiny, Cognos, PowerBI, DevOps, GIT, Jenkins, Octopus, Agile, Financial Analysis, Investigative Analysis, Communication Skills","sql server, data warehousing, r, python, net, shiny, cognos, powerbi, devops, git, jenkins, octopus, agile, financial analysis, investigative analysis, communication skills","agile, cognos, communication skills, datawarehouse, devops, financial analysis, git, investigative analysis, jenkins, net, octopus, powerbi, python, r, shiny, sql server"
Data Center Engineer,Aditi Consulting,"Bedminster, NJ",https://www.linkedin.com/jobs/view/data-center-engineer-at-aditi-consulting-3779387702,2023-12-17,New Jersey,United States,Mid senior,Onsite,"LOGISTICS:
Work Location: Bedminster, NJ (Onsite Position) **Candidates must report to the Bedminster office location 5 days a week from the day of joining.
JOB DESCRIPTION:
We are looking for a Datacenter Engineer who can perform multifold responsibilities of SAN Storage and HPE C7000 Chassis/DL360 Rack Server Administration.
The candidate will be responsible for delivering infrastructure deployment services to the physical installation of hardware and the logical configuration of operating systems and software products either as the sole person and/or as part of a team. Hands-on experience with provisioning, maintaining, deploying SAN Storage, HPE C700 Chassis & DL360 Rack Servers in production environments required. Candidate must be local to New Jersey and Must have 8+ years of experience in VMware and HPE C7000 Chassis & Rack Servers/SAN Storage Administration
MUST HAVE SKILLS
HPE C7000/DL360 Rack Servers Administration, HPE MSA 2040/2050/2060 Storage Administration, Brocade Switches Administration, Cisco Switches Administration, VMware, Linux, Server, Networking,
RESPONSIBILITIES:
Storage Administration Responsibilities:
Administer and maintain the storage infrastructure, including storage area networks (SAN).
General storage administration tasks, storage software and hardware support, storage device configuration, support of daily backups, monitoring of storage devices, storage performance tuning and troubleshooting.
Require prior experience in SAN Fabric Administration including zoning, LUN security, and HBA configuration and troubleshooting.
Configuring all (SAN) Storage Attached Network and (NAS) Network Attached Storage infrastructure including the Fiber Channel network. Ensures that all hosts are properly configured to use the storage infrastructure. Ensure host connectivity and configuration to Hybrid and Native Cloud Storage solutions.
Monitor storage performance, identify bottlenecks, and implement optimization strategies to ensure optimal throughput and reliability.
Management of HPE MSA storage including storage provisioning, replication, copy data management, and monitoring/alerting.
Must have expert knowledge of fiber channel and TCP/IP protocols, SAN management software, automation, and resource management.
For SAN and NAS focused support, individuals must have expert knowledge and experience in the Cisco and Brocade switches.
Troubleshoot and resolve storage issues, collaborating with cross-functional teams and vendors, as necessary.
Conduct storage capacity planning and forecasting to accommodate future growth and changing business requirements.
Create and maintain documentation related to storage configurations, procedures, and troubleshooting guides.
Ensure appropriate storage mediums are controlled and accounted for in the inventory and released to off-site processes and to on-site storage areas.
Set-up a SAN fabric using Brocade switches deploying storage arrays on the fabric to construct a viable disaster recovery infrastructure.
ADMINISTRATION OF HPE C7000 CHASSIS/HPE DL360/380 RACK SERVERS:
Implementation and Administration of HP C7000 chassis with virtual connect using VCM/VCEM and OBA.
Install, configure, and manage server hardware and software, including operating systems (like Red Hat Enterprise Linux, CentOS, Ubuntu, and Windows), applications, and patches to support and maintain effective operations.
Directly work with VMware and HPE(c7000) to support, troubleshoot, repair, and maintain server issues and security for the enterprise.- Converted Physical servers to Virtual servers using VMware vCenter Converter standalone tool and migrated those servers.
Technical documentation on the diagnostics procedure and process documentation on troubleshooting, configuration.
Vital contributor to VMware infrastructure designs a new facility, using vSphere, vCenter, and VMware ESXi to provide a mission-critical production infrastructure.
Vsphere 7.0, VMware ESX and ESXi, VCenter server 7.0, orchestrator, VMware Converter enterprise, VMware Update Manager, Power CLI, VMware Workstation8.0/10.0.
Experienced with planning, installing, configuring, and upgrading support for vCenter Server and VMware ESXi on HP C7000 and HPE DL 360/380 Proliant Physical servers.
Experienced with SAN and NAS storage including Fibre Channel switches.
Able to perform physical installation and cabling of rack-mounted servers and switches.
Show more
Show less","SAN Storage, HPE C7000 Chassis, DL360 Rack Servers, VMware, Linux, Server, Networking, Brocade Switches, Cisco Switches, Fiber Channel, TCP/IP, Red Hat Enterprise Linux, CentOS, Ubuntu, Windows, vCenter Converter, vSphere, vCenter, VMware ESXi, Orchestrator, VMware Update Manager, Power CLI, VMware Workstation","san storage, hpe c7000 chassis, dl360 rack servers, vmware, linux, server, networking, brocade switches, cisco switches, fiber channel, tcpip, red hat enterprise linux, centos, ubuntu, windows, vcenter converter, vsphere, vcenter, vmware esxi, orchestrator, vmware update manager, power cli, vmware workstation","brocade switches, centos, cisco switches, dl360 rack servers, fiber channel, hpe c7000 chassis, linux, networking, orchestrator, power cli, red hat enterprise linux, san storage, server, tcpip, ubuntu, vcenter, vcenter converter, vmware, vmware esxi, vmware update manager, vmware workstation, vsphere, windows"
Staff Data Engineer,Recruiting from Scratch,"Trenton, NJ",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744399022,2023-12-17,New Jersey,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data governance, Security, Scalability, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Deployment, Stream processing, Kafka, Storm, Spark Streaming, Dimensional data modeling, Schema design, Data warehouses, ETL, Data pipelines, Data classification, Data retention, Legal compliance","data governance, security, scalability, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, stream processing, kafka, storm, spark streaming, dimensional data modeling, schema design, data warehouses, etl, data pipelines, data classification, data retention, legal compliance","airflow, automated testing, continuous integration, data classification, data governance, data retention, data warehouses, datapipeline, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, scalability, schema design, security, snowflake, spark, spark streaming, sql, storm, stream processing, tdd"
Senior Big Data Engineer- F2F Interview,Diverse Lynx,"Trenton, NJ",https://www.linkedin.com/jobs/view/senior-big-data-engineer-f2f-interview-at-diverse-lynx-3764421801,2023-12-17,New Jersey,United States,Mid senior,Onsite,"Note: Client is looking for such consultant who can go for F2F interview.
Role : Senior Big Data Engineer
Location : NJ (hybrid)
Experience: 8&plus; Year
Duration: 12&plus; months
Job Description
We are seeking a highly skilled and experienced Senior Data Engineer to join our dynamic team. As a Senior Data Engineer, you will play a crucial role in designing, implementing, and maintaining data pipelines and infrastructure for our big data projects. Your expertise in Java, Python, Spark cluster management, data science, big data, REST API development, and knowledge of Databricks and Delta Lake will be essential in driving the success of our data initiatives.
Qualifications
Master's degree in Computer Science, Data Science, or a related field.
Minimum of 3 years of professional experience in data engineering, working with Java, Python, Spark, and big data technologies.
Strong programming skills in Java and Python, with expertise in building scalable and maintainable code.
Proven experience in Spark cluster management, optimization, and performance tuning.
Solid understanding of data science concepts and experience working with data scientists and analysts.
Proficiency in SQL and experience with relational databases (e.g., Snowflake, Delta Tables).
Experience in designing and developing REST APIs using frameworks such as Flask or Spring.
Familiarity with cloud-based data platforms (e.g.Azure)
Experience with data warehousing concepts and tools (e.g., Snowflake, BigQuery) is a plus.
Strong problem-solving and analytical skills, with the ability to tackle complex data engineering challenges.
Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.
Responsibilities
Design, develop, and implement scalable data pipelines and ETL processes using Java, Python, and Spark.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and design efficient solutions.
Manage and optimize Spark clusters to ensure high performance and reliability.
Perform data exploration, data cleaning, and data transformation tasks to prepare data for analysis and modeling.
Develop and maintain data models and schemas to support data integration and analysis.
Implement data quality and validation checks to ensure accuracy and consistency of data.
Utilize REST API development skills to create and integrate data services and endpoints for seamless data access and consumption.
Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues.
Stay updated with the latest technologies and trends in big data, data engineering, data science, and REST API development, and provide recommendations for process improvements.
Mentor and guide junior team members, providing technical leadership and sharing best practices.
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Java, Python, Spark, REST API, Databricks, Delta Lake, Machine learning, Data science, SQL, Snowflake, ETL, Big data, Data engineering, Data warehousing, Data pipeline, Data analysis, Data modeling, Data quality, Problemsolving, Analytical skills, Communication skills, Teamwork","java, python, spark, rest api, databricks, delta lake, machine learning, data science, sql, snowflake, etl, big data, data engineering, data warehousing, data pipeline, data analysis, data modeling, data quality, problemsolving, analytical skills, communication skills, teamwork","analytical skills, big data, communication skills, data engineering, data pipeline, data quality, data science, dataanalytics, databricks, datamodeling, datawarehouse, delta lake, etl, java, machine learning, problemsolving, python, rest api, snowflake, spark, sql, teamwork"
Senior Cloud Data Engineer,TekIntegral,"Jersey City, NJ",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-tekintegral-3683860649,2023-12-17,New Jersey,United States,Mid senior,Onsite,"Title: Senior Cloud Data Engineer: = 2 OPENINGS***
Location: Edison, NJ, OR NY,NY (Hybrid)
Duration: 12-24+ months
Start Date: ASAP
IT Experience: 11+ = target
Visa: ANY
Mode of interview: Phone / zoom
Industry: Financials
he ideal candidate has past experience engineering solutions for large scale distributed data intense applications on public cloud platforms.
Responsibilities
Design and Development
Work with Cloud Architect to identify data components and process flows
Design and Develop data ingestion processes into Hadoop/AWS Platform
Collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Identify, analyze, and interpret trends or patterns in complex data sets
Innovate new ways of managing, transforming and validating data
Establish and enforce guidelines to ensure consistency, quality and completeness of data assets
Apply quality assurance best practices to all work products
Qualifications
BS/BA degree or equivalent experience
5+ Experience in a Big Data technologies (Spark, Impala, Hive, Redshift, Kafka, etc.)
3+ years of Experience with Python is preferred
Experience performing data analysis (NOT DATA SCIENCE) on AWS platforms is preferred
Experience in implementing complex ETL transformations on big data platform like NoSQL databases (Mongo, DynamoDB, Cassandra)
Familiarity with relational database environment (Oracle, SQL Server, etc.) leveraging databases, tables/views, stored procedures, agent jobs, etc.
Strong development discipline and adherence to best practices and standards
Demonstrated independent problem solving skills and ability to develop solutions to complex analytical/data-driven problems
Experience with data management process on AWS is a huge Plus
Experience of working in a development teams using agile techniques
Show more
Show less","Cloud Data engineering, AWS, Hadoop, Spark, Impala, Hive, Redshift, Kafka, Python, ETL, NoSQL, MongoDB, DynamoDB, Cassandra, SQL Server, Oracle, Agile","cloud data engineering, aws, hadoop, spark, impala, hive, redshift, kafka, python, etl, nosql, mongodb, dynamodb, cassandra, sql server, oracle, agile","agile, aws, cassandra, cloud data engineering, dynamodb, etl, hadoop, hive, impala, kafka, mongodb, nosql, oracle, python, redshift, spark, sql server"
Python API Developer Data Engineer #: 23-04526,HireTalent - Diversity Staffing & Recruiting Firm,"Newark, NJ",https://www.linkedin.com/jobs/view/python-api-developer-data-engineer-%23-23-04526-at-hiretalent-diversity-staffing-recruiting-firm-3764247356,2023-12-17,New Jersey,United States,Mid senior,Onsite,"Our Technology Solutions Group is a dynamic, fast-paced environment, with exciting changes on the horizon under new senior leadership. We are looking for you to build out scalable applications to support our Compliance, Finance, Data Governance, Operational Risk, and Human Resources teams. As a UI/UX Developer, you will be responsible for designing, developing Python based data access layers and rest APIs. for our software applications. You will work closely with cross-functional teams to understand requirements and deliver high-quality, responsive, scalable and flexible data access layer, rest api and other python based applications. We want you to see this challenge as a unique and valuable opportunity, so if this sounds interesting, then PGIM could be the place for you.
Your Impact
API development using RESTful or GraphQL standards.
Develop Data Interfaces using Python, Data Frames, Pandas etc.
Integration with back-end technologies and frameworks, such as Node.js, .NET, or Java.
Build Data Access Layer and metadata driven data query tool.
Write clean, reusable, and well-documented code that follows best practices and coding standards.
Collaborate with back-end developers to integrate UI components with server-side systems and APIs. Ensure efficient data retrieval and synchronization, handle data validation and error handling, and optimize API performance.
Align with the Data Engineering Lead, Product Owner and Scrum Master in assessing business needs and transforming them into scalable applications.
Work in an iterative/Agile environment as a good team player.
Ability to manage multiple tasks and projects simultaneously.
Research emerging technologies and develop POC’s
Work with the latest CI/CD DevOps deployment model
Required Skills
Your Required Skills:
Bachelor's degree in Computer Science, Software Engineering, or a related field.
5&plus; years hands on experience in Python development using NumPy, Pandas, Data frame etc.
Proven experience of API development using RESTful or GraphQL standards.
Experience with back-end technologies and frameworks, such as Node.js, .NET, or Java, and integrating UI components with server-side systems.
Experience with BitBucket, Jenkins, Gradle, Git. Cloud experience working with AWS S3/EC2/SQS
Direct experience supporting front office end-users and sound understanding of capital markets within Fixed Income.
Experience in and front-end frameworks such as Angular, React.
Knowledge of Jira, Confluence, SAFe development methodology & DevOps.
Familiarity with security concepts authentication, authorization and SSL
Excellent analytical and problem-solving skills with the ability to think quickly and offer alternatives both independently and within teams.
Proven ability to work quickly in a dynamic environment.
Experience providing ongoing support for a wide-range of technology solutions
Experience developing solutions in BI tools such as Tableau or Power BI is a plus
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender, identity, national origin, disability, or protected veteran status.
Show more
Show less","Python, DataFrames, Pandas, NumPy, RESTful, GraphQL, Node.js, .NET, Java, BitBucket, Jenkins, Gradle, Git, AWS S3, EC2, SQS, Angular, React, Jira, Confluence, SAFe, DevOps, Authentication, Authorization, SSL, Tableau, PowerBI","python, dataframes, pandas, numpy, restful, graphql, nodejs, net, java, bitbucket, jenkins, gradle, git, aws s3, ec2, sqs, angular, react, jira, confluence, safe, devops, authentication, authorization, ssl, tableau, powerbi","angular, authentication, authorization, aws s3, bitbucket, confluence, dataframes, devops, ec2, git, gradle, graphql, java, jenkins, jira, net, nodejs, numpy, pandas, powerbi, python, react, restful, safe, sqs, ssl, tableau"
Senior Data Platform Engineer,DriveWealth,"Jersey City, NJ",https://www.linkedin.com/jobs/view/senior-data-platform-engineer-at-drivewealth-3767072848,2023-12-17,New Jersey,United States,Mid senior,Onsite,"DriveWealth, the pioneer of fractional equities trading and embedded investing, is a visionary technology company that empowers more than 100 partners around the world to engage their customers by placing the markets in the palm of their hand. We believe the future is fractional, transactional and mobile. Every mobile device should be a gateway to accessing investing and savings products, services, advice, and assistance for global citizens of all ages, wealth stages, and levels of financial expertise. DriveWealth’s unparalleled consultative support and cloud-based, industrial strength technology platform allow partners to seamlessly offer branded investing experiences to drive customer acquisition, loyalty, retention, and revenue growth. DriveWealth’s commitment to continuous evolution and innovation makes it the partner of choice for powering the future of investing.
Completing a Series D raise in 2021, DriveWealth’s mission is to democratize investing globally by working with partners to invent new ways to use its API-based technology to provide emerging investors with cutting-edge embedded experiences, offer first-time access to U.S. markets, and the ability to begin investing with as little as $1. DriveWealth is committed to empowering consumers around the world to become owners by delivering the most modern brokerage infrastructure, unparalleled industry expertise, and a culture of continued evolution.
About The Team
The Data & Analytics team at DriveWealth creates data products that provide access to relevant data and insights to help customers succeed. We are looking for a Senior Data Engineer to join a growing data engineering function within the data & analytics team. Data Engineering focuses on creating, maintaining, and monitoring data pipelines, including ingestion and broader data services, products, and processes used by internal partners and external customers.
The ideal candidate thrives working closely with analytics engineers, product managers, and internal stakeholders to interpret and clarify requirements, architect and build efficient data processing services, and ideate end-to-end data products to help customers achieve their goals. The candidate should be excited about joining a team in shaping the direction of data and analytics at DriveWealth. This means balancing competing priorities and laying the groundwork for scale while building top-tier data products.
What You’ll Do
Build and maintain data pipelines, including replication, batch, and streaming processes.
Ensuring consistent, accurate output is delivered quickly through efficient coding with testing and verification.
Work with data product managers and business partners to ensure requirements can translate into data engineering solutions, anticipating challenges to provide suggestions proactively
Work to establish an open and scalable data culture through documentation, constructive code reviews, and clear explanations of priorities and pain points.
Provide thought leadership, best practices on architecture and tooling, and standards for designing and implementing scalable data solutions
Provide mentorship and guidance to data engineers.
Work with the broader data & analytics team to ideate, build, and maintain “self-service” data products that enable data and analytics usage (tables, reports, dashboards, and Python notebooks), stepping in to help deliver and provide guidance wherever necessary and possible.
What You’ll Need
7+ years of experience solving data problems with business partners
Experience working within larger data team: CI/CD process leveraging GitHub and AGILE development where possible
Working knowledge of DBT, Airflow, or other orchestration and SQL code management tools
Fluent in Python, SQL, PySpark, Spark SQL, Databricks, AWS
Experience with Terraform (AWS, Databricks), Shell Scripting, GIT, and other Command Line Tools
Experience with Agile Software Development, Event Drive Architecture, Customer Facing Data Solutions and information sharing
BS in Computer Science or equivalent program
Nice to Have
Familiarity with Snowflake
FinTech or stock market experience
Familiarity with GDPR OR FINRA OR other regulations
Applicants must be authorized to work for any employer in the U.S. DriveWealth is unable to sponsor or take over sponsorship of an employment Visa at this time.
Compensation
Compensation package offerings are based on candidate experience and technical qualifications, as it relates to the role. These are identified and determined throughout your interviewing experience.
Please note
: at this time, we are not able to hire in all states.
Remote (Most US States) Pay Range
$185,000—$200,000 USD
US Benefits
Insurance – Medical, Dental, Vision, Life, LTD & STD. HSA and FSA options.
Unlimited PTO
401k plan
Most role have flexible working hours and work from home
Continuing education and conferences reimbursement
Fitness/Wellbeing reimbursement
Lunch program, snacks and beverages available in the office
Singapore Benefits
Insurance – Medical, Dental, Vision, Group Life, LTD & STD
Unlimited PTO
Flexible working hours and work from home
Continuing education and conferences reimbursement
Fitness/Wellbeing reimbursement
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
TO ALL AGENCIES:
Please, no phone calls or emails to any employee of DriveWealth outside of the Talent organization. DriveWealth’s policy is to only accept resumes from agencies via Greenhouse (ATS). Agencies must have a valid services agreement executed and must have been assigned by the Talent team to a specific requisition. Any resume submitted outside of this process will be deemed the sole property of DriveWealth. In the event a candidate submitted outside of this policy is hired, no fee or payment will be paid.
Show more
Show less","Data Engineering, Data Pipelines, Data Products, Data Analysis, Python, SQL, PySpark, Spark SQL, Databricks, AWS, Terraform, Shell Scripting, GIT, Agile Software Development, Event Drive Architecture, Customer Facing Data Solutions, Information Sharing, DBT, Airflow, Snowflake, FinTech, Stock Market, GDPR, FINRA, Regulations","data engineering, data pipelines, data products, data analysis, python, sql, pyspark, spark sql, databricks, aws, terraform, shell scripting, git, agile software development, event drive architecture, customer facing data solutions, information sharing, dbt, airflow, snowflake, fintech, stock market, gdpr, finra, regulations","agile software development, airflow, aws, customer facing data solutions, data engineering, data products, dataanalytics, databricks, datapipeline, dbt, event drive architecture, finra, fintech, gdpr, git, information sharing, python, regulations, shell scripting, snowflake, spark, spark sql, sql, stock market, terraform"
Staff Healthcare Data Engineer - remote,Vytalize Health,"Hoboken, NJ",https://www.linkedin.com/jobs/view/staff-healthcare-data-engineer-remote-at-vytalize-health-3764804037,2023-12-17,New Jersey,United States,Mid senior,Remote,"About Our Company
Vytalize Health is a leading value-based care platform. It helps independent physicians and practices stay ahead in a rapidly changing healthcare system by strengthening relationships with their patients through data-driven, holistic, and personalized care. Vytalize provides an all-in-one solution, including value-based incentives, smart technology, and a virtual clinic that enables independent practices to succeed in value-based care arrangements. Vytalize's care delivery model transforms the healthcare experience for more than 250,000+ Medicare beneficiaries across 36 states by helping them manage their chronic conditions in collaboration with their doctors.
About Our Growth
Vytalize Health has grown its patient base over 100% year-over-year and is now partnered with over 1,000 providers across 36-states. Our all-in-one, vertically integrated solution for value-based care delivery is responsible for $2 billion in medical spending. We are expanding into new markets while increasing the concentration of practices in existing ones.
Visit www.vytalizehealth.com for more information.
Why you will love working here
We are an employee first, mission driven company that cares deeply about solving challenges in the healthcare space. We are open, collaborative and want to enhance how physicians interact with, and treat their patients. Our rapid growth means that we value working together as a team. You will be recognized and appreciated for your curiosity, tenacity and ability to challenge the status quo; approaching problems with an optimistic attitude. We are a diverse team of physicians, technologists, MBAs, nurses, and operators. You will be making a massive impact on people's lives and ultimately feel like you are doing your best work here at Vytalize.
We are building out a world-class engineering team to empower our business. As a Staff Data Engineer, you will be responsible for overseeing the data architecture implementation and integration processes, with a primary focus on seamlessly integrating structure/semi-structure/unstructured data from various data sources and driving integration initiatives with our internal/external systems. You will collaborate with cross-functional teams, leveraging your expertise to ensure the efficient flow of data, optimize data pipelines, and contribute to the overall success of our healthcare data ecosystem. This is a highly impactful role that delivers huge value to the company resulting in potential annual saving of tens or hundreds of millions of dollars.
Responsibilities:
Data Architecture and Integration
Lead efforts to enhance, extend, and strengthen our existing data architectures to support the current and future new business use cases, including ingesting new data sources (Payer claims, ADT, Clinical, Labs data, etc.) to our central data management system.
Develop and optimize data pipelines for the extraction, loading, and transformation (ELT) of healthcare data and ensure delivery to internal/external systems with high quality.
Payer Claims / Clinical Data Integration
Collaborate with stakeholders to understand business requirements related to Payer claims data while supporting existing needs for Centers for Medicare & Medicaid Services (CMS) data
Implement solutions for integrating and harmonizing other forms of data including clinical data (Labs, ADT ect.), ensuring accuracy, completeness, and compliance with regulatory standards.
EMR Systems Integration
Support key initiatives to integrate our internal tech platform with various Electronic Medical Record (EMR) systems.
Work closely with EMR vendors and internal teams to establish seamless data exchange, inflow/outflow and interoperability.
Data Quality/Governance/Security
Implement robust data quality assurance processes to validate and ensure the accuracy of integrated healthcare data.
Collaborate with data governance and IT teams to establish and enforce data quality and security policies and standards for full business compliance
Execution and Delivery
Act as a ""player-coach"" for other data engineers and be willing to dive into the implementation and get things done as needed.
Be a strong partner to engineering leadership to achieve repeatable successes in project delivery while uphold high standards on engineering excellence and team building
Collaboration and Communication
Collaborate with cross-functional teams, including our Lead Clinical Data Architect, product managers, analytics, and IT professionals, to understand data needs and deliver effective solutions.
Communicate technical concepts and solutions to non-technical stakeholders, fostering a culture of data-driven decision-making.
Qualifications
Master's or PhD degree in Computer Science, Information Systems, or a related field.
15+ years of engineering experience.
Proven expert experience in data engineering, having built complex data systems end to end while being accountable for the outcome.
Strong domain knowledge working in the healthcare industry, expertise dealing with healthcare data is a must have. Prior experience in value-based Care space is a big bonus.
Strong proficiency in SQL, Python, and other relevant programming languages.
Experience with big data technologies (e.g., Hadoop, Spark) and cloud platforms (e.g., AWS, Azure, GCP).
Familiarity with healthcare data standards, regulations, and interoperability standards such as (FHIR, HIE)
Excellent communication and problem-solving skills and the ability to work in a dynamic and collaborative environment.
Cultural value principles: Proactivity, Tenacity, Reliability, Excellence, Clarity, Optimism, Transparency, Kindness. If these resonate well with you, you should ping us.
Perks/Benefits
Competitive base compensation
Annual bonus potential
Health benefits effective on start date; 100% coverage for base plan, up to 90% coverage on all other plans for individuals and families
Health & Wellness Program; up to $300 per quarter for your overall wellbeing
401K plan effective on the first of the month after your start date; 100% of up to 4% of your annual salary
Company paid STD/LTD
Unlimited (or generous) paid ""Vytal Time"", and 5 paid sick days after your first 90 days.
Technology setup
Ability to help build a market leader in value-based healthcare at a rapidly growing organization.
We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.
Please note at no time during our screening, interview, or selection process do we ask for additional personal information (beyond your resume) or account/financial information. We will also never ask for you to purchase anything; nor will we ever interview you via text message. Any communication received from a Vytalize Health recruiter during your screening, interviewing, or selection process will come from an email ending in @vytalizehealth.com
Show more
Show less","Data Architecture, Data Integration, Data Pipelines, Data Quality, Data Governance, Data Security, SQL, Python, Hadoop, Spark, AWS, Azure, GCP, FHIR, HIE, EMR, CMS","data architecture, data integration, data pipelines, data quality, data governance, data security, sql, python, hadoop, spark, aws, azure, gcp, fhir, hie, emr, cms","aws, azure, cms, data architecture, data governance, data integration, data quality, data security, datapipeline, emr, fhir, gcp, hadoop, hie, python, spark, sql"
Sr Data Engineer (Apache Flink/AWS/Big Data) -,Verdant Infotech Solutions,"Jersey City, NJ",https://www.linkedin.com/jobs/view/sr-data-engineer-apache-flink-aws-big-data-at-verdant-infotech-solutions-3785507895,2023-12-17,New Jersey,United States,Mid senior,Hybrid,"Title :- Sr Data Engineer (Apache Flink/AWS/Big Data) -
Type :- Onsite at Jersey City, NJ
Work - W2
Rate :- $80/ hr on W2
Looks for Data Engineers with Apache Flink, AWS , Big Data
Should have at least 9 years of experience minium
Show more
Show less","Apache Flink, AWS, Big Data","apache flink, aws, big data","apache flink, aws, big data"
Lead Data Engineer-US,Zortech Solutions,"Jersey City, NJ",https://www.linkedin.com/jobs/view/lead-data-engineer-us-at-zortech-solutions-3667472992,2023-12-17,New Jersey,United States,Mid senior,Hybrid,"Role: Cloud Engineer
Location: Remote/US
Duration: 6+ Months
Job Description
Primary Job Function:
The cloud engineering function provides in-depth technical expertise of cloud services, including cloud architecture, provisioning, and technical support of cloud platforms. Strong understanding of identifying optimal cloud-based solutions in support of business needs and deploying and maintaining cloud platforms in accordance with best practices and company policies. Responsible for designing, securing, and deploying cloud platforms primarily through infrastructure as code and automation. Regularly peer review small to mid-sized Infrastructure as Code scripts.
Provides technical support on cloud platform issues. Architectural design knowledge of cloud platforms, best practices, and industry trends. Create small to mid-sized cloud solution architectural designs incorporating industry best practices. May assist senior engineer or team lead in larger scale more complex cloud designs.
Requirements
Minimum of five years' experience in cloud computing, specifically in the design, building, deploying and system administration of cloud services. Experience with CI/CD processes. Experience working in an Agile environment. Experience coding infrastructure as code. Requires excellent analytical, communication and consultative skills, as well as sound judgment and the ability to work effectively with business stakeholders and other IT management and staff. Experience establishing guidelines, procedures and standards for platform automation is a plus. Strong OS systems knowledge is a plus. Cloud Platform certifications strongly preferred.
Responsibilities
Daily interaction with divisional clients and peer resources in other technical support organizations (e.g. hardware, storage, etc.). Monthly interaction with vendors.
Level 3+ infrastructure support for cloud platforms.
Determine cloud platform design project requirements and the architectural requirements in support of business needs.
Independently conceives and develops general approaches to a task/technology design project.
Exercises latitude in approach to problem and solution.
Adheres to established policies.
Show more
Show less","Cloud Architecture, Cloud Provisioning, Cloud Platforms, Infrastructure as Code, Automation, CI/CD, Agile, Analytical Skills, Communication Skills, Consultative Skills, Sound Judgment, OS Systems Knowledge, Cloud Platform Certifications, Level 3+ Infrastructure Support, Project Requirements Gathering, Architectural Design, General Approaches to Design Projects, ProblemSolving, Established Policies","cloud architecture, cloud provisioning, cloud platforms, infrastructure as code, automation, cicd, agile, analytical skills, communication skills, consultative skills, sound judgment, os systems knowledge, cloud platform certifications, level 3 infrastructure support, project requirements gathering, architectural design, general approaches to design projects, problemsolving, established policies","agile, analytical skills, architectural design, automation, cicd, cloud architecture, cloud platform certifications, cloud platforms, cloud provisioning, communication skills, consultative skills, established policies, general approaches to design projects, infrastructure as code, level 3 infrastructure support, os systems knowledge, problemsolving, project requirements gathering, sound judgment"
Data Engineer,Hermitage Infotech,"Cupertino, CA",https://www.linkedin.com/jobs/view/data-engineer-at-hermitage-infotech-3660228048,2023-12-17,Union City,United States,Mid senior,Onsite,"Hi,
Given below is the urgent req for my client.. If you are comfortable with it, available and looking for a project please send me your profile immediately in word document along with your expected hourly salary on CTC/1099 or yearly salary on W2. Please mention your work authorization and your availability to start the project.
Position: Data Engineer
Duration: Long Term
Location: CA
From Day 1 onsite
Main Skillsets
Experience with Advance SQL, Snowflake, Tableau, Python is a must. ETL experience is also preferred. Should be able to work in a fast paced environment. Perform effectively under dynamic conditions such as directional changes, tight deadlines.
Years of experience - 3 - 7 years of experience.
Regards
Varma
732-338-7524
Varma
Hermitage Infotech, LLC
P:732-593-8453 Extension 202
F:732-289-6103
varma@hermitageinfotech.com
www.hermitageinfotech.com
Show more
Show less","Advance SQL, Snowflake, Tableau, Python, ETL, Data Engineering","advance sql, snowflake, tableau, python, etl, data engineering","advance sql, data engineering, etl, python, snowflake, tableau"
Cloud Data Engineer,Seven Hills Group Technologies Inc.,San Francisco Bay Area,https://www.linkedin.com/jobs/view/cloud-data-engineer-at-seven-hills-group-technologies-inc-3687376542,2023-12-17,Union City,United States,Mid senior,Onsite,"Title: Cloud Data Engineer
Client: Palo Alto Networks
Location: Bay Area, CA (Hybrid, not fully remote)
Only independent contract for USC/GC
Expert in data engineering and GCP data technologies.
Work with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.
Work with Agile and DevOps techniques and implementation approaches in the delivery
Job Description
10+ Year experience in BI and Analytics
Hands on and deep experience (at least 2 years) working with Google Data Products (e.g., BigQuery, Dataflow, Dataproc,] etc.).
Experience in Spark (Scala/Python/Java) and Kafka, Airflow
Data Engineering and Lifecycle (including non-functional requirements and operations) management.
E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.
Experience with SQL and NoSQL modern data stores.
Show more
Show less","Google Data Products, BigQuery, Dataflow, Dataproc, Spark, Scala, Python, Java, Kafka, Airflow, Data Engineering, Lifecycle Management, NonFunctional Requirements, Operations Management, Solution Design, Prototyping, Usability Testing, Data Visualization, SQL, NoSQL","google data products, bigquery, dataflow, dataproc, spark, scala, python, java, kafka, airflow, data engineering, lifecycle management, nonfunctional requirements, operations management, solution design, prototyping, usability testing, data visualization, sql, nosql","airflow, bigquery, data engineering, dataflow, dataproc, google data products, java, kafka, lifecycle management, nonfunctional requirements, nosql, operations management, prototyping, python, scala, solution design, spark, sql, usability testing, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Santa Clara, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759708642,2023-12-17,Union City,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning (ML), Artificial Intelligence (AI), Data Pipelines, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Statistical Analysis, Data Visualization, Pandas, R, Airflow, KubeFlow, Natural Language Processing (NLP), Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, SQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Management Tools, Data Classification, Data Retention","data engineering, machine learning ml, artificial intelligence ai, data pipelines, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, airflow, kubeflow, natural language processing nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, sql, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, data retention","airflow, applied machine learning, artificial intelligence ai, aws, azure, bash, data classification, data cleaning, data engineering, data management tools, data mining, data normalization, data retention, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning ml, natural language processing nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Senior Data Analyst,Russell Tobin,"Cupertino, CA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-russell-tobin-3784211913,2023-12-17,Union City,United States,Mid senior,Hybrid,"Job Title: Store Experience Product Launch Global Data Analyst
Duration: 10 Month W2 Contract
Location: Cupertino, CA 95014 (On-site Tues, Wed, & Thurs)
Pay: $65-$71/hour
Job Description:
Duties
:
Own the global Channel store merchandising landscape in preparation of a product launch: Perform data validation and scrubbing, working with inputs from regional teams
Forecast store demos and security elements for product launches: Develop models based on current merchandising to forecast demand at SKU-level based on global strategy, against a fluctuating baseline
Contribute to marketing strategy: Be resourceful using sales and merchandising datasets, along with tools to research and develop insights that contribute to strategy decisions
Tell stories with data every day: Propose and defend your ideas with strong research and compelling analyses to internal cross-functional team and sales/marketing leadership stakeholders
NPI Project management: Lead projects across internal planning, merchandising, supply/demand and country/regional teams to ensure forecast accuracy and supply availability within a defined schedule
A mix of team & independent work: Manage your deliverables and deadlines independently, but connect everything to the bigger picture with your colleagues; Communicate the status and results of your independent projects to cross-functional forums
Hard Skills Required
:
Very strong Excel and data management
Data visualizations (Tableau/Power BI), analysis and reporting across several levels of the organization
MacOS and iOS
Experience with supply/demand planning, inventory management or retail merchandising an asset
Soft Skills
:
Detail-oriented
Comfortable working in an extremely agile, collaborative, fast-paced environment with a lot of ambiguity
Comfortable working with highly sensitive information
Resourceful
Deadline-driven
Strong problem solving
Ability to prioritize multiple, conflicting deliverables
Ideal candidates would have a passion for data storytelling, problem solving, and an interest in collaborating with others to shape the in-store experience for a major tech brand
Education
: Bachelor’s degree or data analysis certifications preferred. MBA or master’s in data science a plus.
Show more
Show less","Excel, Data Management, Data Visualization, Tableau, Power BI, Analysis, Reporting, MacOS, iOS, Supply/Demand Planning, Inventory Management, Retail Merchandising, Data Storytelling, Problem Solving, Collaboration, Prioritization","excel, data management, data visualization, tableau, power bi, analysis, reporting, macos, ios, supplydemand planning, inventory management, retail merchandising, data storytelling, problem solving, collaboration, prioritization","analysis, collaboration, data management, data storytelling, excel, inventory management, ios, macos, powerbi, prioritization, problem solving, reporting, retail merchandising, supplydemand planning, tableau, visualization"
Data Migration Lead,Wise Skulls,"Michigamme, MI",https://www.linkedin.com/jobs/view/data-migration-lead-at-wise-skulls-3708854630,2023-12-17,Ishpeming,United States,Mid senior,Onsite,"Job Title: Data Migration Lead
Job Location: Michigan - MI, Remote within USA
Project Duration: 6+ months ( can be extended )
System Implementation Partner: Infosys
End Client : To be disclosed
Jd
8+ years' experience in Data Migration solution design/ Data Science with D365 F&O
Experienced Business Intelligence Data Science Data Migration Solutions Architect with expertise on Planning, Designing and execution of Advanced Analytics, Business Intelligence, D365 Finance & Operations and Azure technologies on Cloud.
Designed and Implemented Data Lakes, Ingestion pipelines both with Batch and real time streaming and analytics on Azure Services.
Knowledge of D365 F&O and Azure SQL
Show more
Show less","Data Migration, Data Science, D365 F&O, Business Intelligence, Data Migration Solutions Architect, Advanced Analytics, Azure technologies, Cloud, Data Lakes, Ingestion pipelines, Batch, Real time streaming, D365 F&O, Azure SQL","data migration, data science, d365 fo, business intelligence, data migration solutions architect, advanced analytics, azure technologies, cloud, data lakes, ingestion pipelines, batch, real time streaming, d365 fo, azure sql","advanced analytics, azure sql, azure technologies, batch, business intelligence, cloud, d365 fo, data lakes, data migration, data migration solutions architect, data science, ingestion pipelines, real time streaming"
"Data Engineer, SQL SSIS Developer",EXL,"Jersey City, NJ",https://www.linkedin.com/jobs/view/data-engineer-sql-ssis-developer-at-exl-3769541207,2023-12-17,Bayonne,United States,Mid senior,Onsite,"Jersey City, NJ, USA Req #15564
Thursday, November 30, 2023
Company Overview And Culture
EXL (NASDAQ: EXLS) is a global analytics and digital solutions company that partners with clients to improve business outcomes and unlock growth. Bringing together deep domain expertise with robust data, powerful analytics, cloud, and AI, we create agile, scalable solutions and execute complex operations for the world’s leading corporations in industries including insurance, healthcare, banking and financial services, media, and retail, among others. Focused on creating value from data for driving faster decision-making and transforming operating models, EXL was founded on the core values of innovation, collaboration, excellence, integrity and respect. Headquartered in New York, our team is over 40,000 strong, with more than 50 offices spanning six continents. For information, visit www.exlservice.com .
For the past 20 years, EXL has worked as a strategic partner and won awards in its approach to helping its clients solve business challenges such as digital transformation, improving customer experience, streamlining business operations, taking products to market faster, improving corporate finance, building models to become compliant more quickly with new regulations, turning volumes of data into business opportunities, creating new channels for growth and better adapting to change. The business operates within four business units: Insurance, Health, Analytics, and Emerging businesses.
Decision Analytics
EXL (NASDAQ:EXLS) is a leading operations management and analytics company that helps businesses enhance growth and profitability in the face of relentless competition and continuous disruption. Using our proprietary, award-winning Business EXLerator Framework™, which integrates analytics, automation, benchmarking, BPO, consulting, industry best practices and technology platforms, we look deeper to help companies improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. EXL serves the insurance, healthcare, banking and financial services, utilities, travel, transportation and logistics industries. Headquartered in New York, New York, EXL has more than 24,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), Latin America, Australia and South Africa.
EXL Analytics provides data-driven, action-oriented solutions to business problems through statistical data mining, cutting edge analytics techniques and a consultative approach. Leveraging proprietary methodology and best-of-breed technology, EXL Analytics takes an industry-specific approach to transform our clients’ decision making and embed analytics more deeply into their business processes. Our global footprint of nearly 2,000 data scientists and analysts assist client organizations with complex risk minimization methods, advanced marketing, pricing and CRM strategies, internal cost analysis, and cost and resource optimization within the organization. EXL Analytics serves the insurance, healthcare, banking, capital markets, utilities, retail and e-commerce, travel, transportation and logistics industries.
Please visit www.exlservice.com for more information about EXL Analytics.
Key Responsibilities
Constructing ETL / ELT packages for data transformation and integration between various business applications or to the data warehouse
Configuring unit tests that will validate and assure quality of business data processes, ensure testing plans are documented and executed effectively
Provide data engineering support to assigned project and addressing tasks or bugs related to data issues
Work independently and with teams on complex data engineering problems to directly support integration
Create, modify and test various SQL queries, stored procedures and views for obtaining data from diverse source systems and platforms
Design, develop, and maintain SQL Server Analysis Services (SSAS) cubes for multidimensional data analysis and reporting.
Implement data modeling best practices to ensure data accuracy, consistency, and performance. Create and maintain data models and schemas.
Monitor and optimize SQL queries and database performance. Identify and resolve bottlenecks and inefficiencies.
Construct ETL / ELT packages for data transformation and integration between various business applications or to the data warehouse
Work with data warehouse architecture and maintain data warehousing solutions, including data integration and data cleansing processes.
Maintain comprehensive documentation of database designs, ETL processes, and data models
Configure unit tests that will validate and assure quality of business data processes, ensure testing plans are documented and executed effectively.
Collaborate with cross-functional teams, including business analysts and application developers, to understand data requirements and provide data solutions.
Implement data security measures and best practices to protect sensitive data.
Provide data engineering support to assigned project and addressing tasks or bugs related to data issues
Work independently and with teams on complex data engineering problems to directly support integration
Soft Skills
Strong work ethic and desire to produce quality results
Takes initiative & is a self-starter
Continuous Improvement mindset and approach to work product
Ability to take complex subjects and simplify it to less technical individuals
Provides clear documentation of processes, workflows, recommendations, etc.
High level of critical thinking capabilities
Organized and has the ability to manage work effectively, escalating issues as appropriate
Displays ownership of their work (quality, timeliness)
Consistently and proactively communicates (verbally/written) to stakeholders (progress/roadblocks/etc.)
Ability to work independently and manage multiple tasks.
Should have good problem solving skills
Candidate Profile
Bachelor’s/Master's degree in economics, mathematics, computer science/engineering, operations research or related analytics areas; candidates with BA/BS degrees in the same fields from the top tier academic institutions are also welcome to apply
4-12 years’ experience, preferably in insurance analytics
4+ years of experience as a SQL Developer.
Proficiency in SQL programming.
Strong experience with SQL Server Integration Services (SSIS) and SQL Server Analysis Services (SSAS).
Familiarity with Azure services is a plus
Strong record of achievement, solid analytical ability, and an entrepreneurial hands-on approach to work
Outstanding written and verbal communication skills
Able to work in fast pace continuously evolving environment and ready to take up uphill challenges
Is able to understand cross cultural differences and can work with clients across the globe
What We Offer
EXL Analytics offers an exciting, fast paced and innovative environment, which brings together a group of sharp and entrepreneurial professionals who are eager to influence business decisions. From your very first day, you get an opportunity to work closely with highly experienced, world class analytics consultants.
You can expect to learn many aspects of businesses that our clients engage in. You will also learn effective teamwork and time-management skills - key aspects for personal and professional growth
Analytics requires different skill sets at different levels within the organization. At EXL Analytics, we invest heavily in training you in all aspects of analytics as well as in leading analytical tools and techniques.
We provide guidance/ coaching to every employee through our mentoring program wherein every junior level employee is assigned a senior level professional as advisors.
Sky is the limit for our team members. The unique experiences gathered at EXL Analytics sets the stage for further growth and development in our company and beyond.
EEO/Minorities/Females/Vets/Disabilities
To view our total rewards offered click here —>
https://www.exlservice.com/us-careers-and-benefits
Base Salary Range Disclaimer:
The base salary range represents the low and high end of the EXL base salary range for this position. Actual salaries will vary depending on factors including but not limited to: location and experience.
The base salary range listed is just one component of EXL's total compensation package for employees.
Other rewards may include bonuses, as well as a Paid Time Off policy, and many region specific benefits.
Please also note that the data shared through the job application will be stored and processed by EXL in accordance with the EXL Privacy Policy.
Application & Interview Impersonation Warning
– Purposely impersonating another individual when applying and / or participating in an interview in order to obtain employment with EXL Service Holdings, Inc. (the “Company”) for yourself or for the other individual is a crime. We have implemented measures to deter and to uncover such unlawful conduct. If the Company identifies such fraudulent conduct, it will result in, as applicable, the application being rejected, an offer (if made) being rescinded, or termination of employment as well as possible legal action against the impersonator(s).
EXL may use artificial intelligence to create insights on how your candidate information matches the requirements of the job for which you applied. While AI may be used in the recruiting process, all final decisions in the recruiting and hiring process will be taken by the recruiting and hiring teams after considering a candidate’s full profile. As a candidate, you can choose to opt out of this artificial intelligence screening process. Your decision to opt out will not negatively impact your opportunity for employment with EXL.
Other Details
Pay Type Salary
Min Hiring Rate $90,000.00
Max Hiring Rate $100,000.00
Apply Now
Show more
Show less","SSAS, SSIS, Azure, ETL, ELT, SQL, Data Warehousing, Data modeling, Data mining, Analytics, Statistics, Machine Learning, Data Security, Economics, Mathematics, Computer Science, Operations Research","ssas, ssis, azure, etl, elt, sql, data warehousing, data modeling, data mining, analytics, statistics, machine learning, data security, economics, mathematics, computer science, operations research","analytics, azure, computer science, data mining, data security, datamodeling, datawarehouse, economics, elt, etl, machine learning, mathematics, operations research, sql, ssas, ssis, statistics"
Junior Data BI Analyst (US),Patterned Learning Career,"New York, NY",https://www.linkedin.com/jobs/view/junior-data-bi-analyst-us-at-patterned-learning-career-3788843239,2023-12-17,Bayonne,United States,Mid senior,Remote,"This is a remote position.
Junior Data BI Analyst (US) - Remote Job, 1+ Year Experience
Annual Income:
$55K - $60K
About us:
Patterned Learning is a platform that aims to help developers code faster and more efficiently. It offers features such as collaborative coding, real-time multiplayer editing, and the ability to build, test, and deploy directly from the browser. The platform also provides tightly integrated code generation, editing, and output capabilities.
Description
Bachelor's Degree required with an emphasis in business, economics, math, engineering, or analytics preferred (or relevant work experience). Advanced degree or certification preferred.
1+ years, with at least 1 year of analytic experience
Proficiency in interacting with various database and file storage systems (Examples: Oracle, Hadoop, NoSQL). Understanding of join types.
Experience with data manipulation languages, such as SQL, required
Knowledge of statistical concepts and analytic techniques, including descriptive statistics, forecasting, economic modeling, exploratory analysis, and variance analysis required.
Experience using Microsoft Excel, including the use of pivot tables, formulas, macros, VBA, charts/graphs
Experience with visualization tools such as Tableau, PowerBi, or QlikView.
Strong analytical, critical, and systems thinking required.
Skills
Good understanding of Agile framework (SCRUM)
Familiarity with Salesforce is a plus.
Why Patterned Learning LLC?
Patterned Learning can provide intelligent suggestions, automate repetitive tasks, and assist developers in writing code more effectively. This can help reduce coding errors, improve productivity, and accelerate the development process.
The pattern recognition is particularly relevant in the context of coding. Neural networks, especially deep learning models, are commonly employed for pattern detection and classification tasks. These models simulate human decision-making and can identify patterns in data, making them well-suited for tasks like code analysis and generation.
Show more
Show less","SQL, Oracle, Hadoop, NoSQL, Agile, SCRUM, Microsoft Excel, Pivot tables, Formulas, Macros, VBA, Charts, Graphs, Tableau, PowerBi, QlikView, Artificial Intelligence, Machine Learning, Neural Networks, Deep Learning, Pattern Recognition, Classification","sql, oracle, hadoop, nosql, agile, scrum, microsoft excel, pivot tables, formulas, macros, vba, charts, graphs, tableau, powerbi, qlikview, artificial intelligence, machine learning, neural networks, deep learning, pattern recognition, classification","agile, artificial intelligence, charts, classification, deep learning, formulas, graphs, hadoop, machine learning, macros, microsoft excel, neural networks, nosql, oracle, pattern recognition, pivot tables, powerbi, qlikview, scrum, sql, tableau, vba"
Data & Integration Analyst,"Maxor National Pharmacy Services, LLC","Plano, TX",https://www.linkedin.com/jobs/view/data-integration-analyst-at-maxor-national-pharmacy-services-llc-3787392018,2023-12-17,Dallas,United States,Mid senior,Remote,"Overview
The
Data & Integration Analyst
is responsible for implementing, supporting, and maintaining inbound/outbound data file exchanges. Enable and integrate clients and 3 rd party vendors onto Maxor’s PBM platform. Assist in documenting and creating efficient/automated solutions to promote operational capability while also improving client data integrity. Reporting to the Sr. Director of Eligibility and Accumulators the Data & Integration Analyst will meet all service standards to ensure client satisfaction.
Position Location
This is a remote-based position.
Our Company
We're Maxor and we're building a different kind of pharmacy company. We're transforming the pharmacy industry to create healthier lives through purposeful engagement across Pharmacy Benefit Management, Pharmacy Management, Specialty Pharmacy, 340B, Rebate and Formulary Management, and Pharmacies. We put people first and are committed to providing outstanding service across all aspects of our business. We believe there's a better way to deliver pharmacy and healthcare services to people across the country, and we'd love for you to help us do it.
Our Locations
The Maxor workforce brings robust experience, diverse perspectives and passion from over 1,000 employees working all over the US in pharmacies, hospitals, home offices, or corporate offices.
Responsibilities
Support new and existing clients through implementation/conversion/integration processes working directly with clients, account managers and implementation teams though full data-life-cycle development.
Serve as a Subject Matter Expert (SME) on integration points with Maxor’s PBM platform, application, file integration functions/standards and back end data models.
Execute advanced/dynamic SQL scripts to generate flat files or reports to support backend file processing, reconciliation analysis, and internal/external reporting.
Review, analyze, and implement complex file mappings and data conversions to support standard and custom file integrations between Maxor and our clients.
Plan, document, implement new or improved automated/ETL solutions
Conduct system testing of new features, integrations and backend features
With a focus on quality and timeliness, maintain ownership for assigned goals
Ensure quality, service level, productivity and customer standards are consistently met.
Handle routine work stream assignments along with, but not limited to, member and file research, assisting with file processing/data analysis, advise/consult with our internal teams and clients
Comply with Maxor’s Ethical Business Conduct policy and Maxor’s Compliance Program.
Complete required training, as assigned, within the established timeframes.
Must be able to cope with the mental and emotional stress of the position.
Promote teamwork and best practices
Maintain regular attendance in accordance with established policies.
Perform other job-related duties as assigned.
Qualifications
Education:
Bachelor’s Degree in business, computer science, operations, engineering or related field required.
Experience:
Experience in data cleansing/mapping/formatting, ETL frameworks, and/or database relational models. Previous experience with health care enrollment/eligibility/accumulator fundamentals and Pharmacy Benefits Managers (PBMs) data/file
Exchange/integration Experience Preferred.
Knowledge, Skills, and Abilities:
5+ years of experience in data exchange, integration, ETL frameworks, data visualization and/or relational database models/design.
5+ years of experience as Data/Integration/EDI Analyst with relevant PBM/Heath Benefit/Health IT experience.
Strong business acumen with healthcare and pharmacy benefit management industry knowledge preferred.
A desire to problem solve, think critically, analyze and interpret data.
Willingness to solution across business and technical domains.
Ability to identify assumptions, be inquisitive and reach conclusions.
Ability to communicate, document, share and acquire knowledge.
Ability to adapt, continuously improve, enjoy building new tools and process.
Proven ability to develop effective working relationships with internal and external stakeholders.
Motivated by goal oriented delivery, ability to understand overall business objectives and drive towards results.
WHY CHOOSE A CAREER AT MAXOR?
Did you know that patients see their pharmacist an average of 12 times a year? Pharmacy is at the heart of healthcare. Come join Maxor and make a direct impact on patients’ lives. Improve your own wellbeing with our robust benefits and flexible work environment. At Maxor, you have a career with limitless possibilities and the charge to make a difference. A company of 1,000 diverse people and almost 100 years of pharmacy experience, we offer the stability of a Fortune 500 company with the energy and innovation of a startup. We provide services and technology that fuel the entire pharmacy ecosystem, but we are more than pharmacy services. We enable pharmacy care .
WE OFFER
: A diverse, progressive culture that supports a “dress for your day” attire and a collaborative, team oriented environment. Our industry leading compensation and health benefits include:
Comprehensive mental health and wellbeing resources
Nationwide Blue Cross Blue Shield PPO with employee friendly plan design, including $850 individual annual medical deductible, $25 office visit copays, Low biweekly premiums
Company paid basic life/AD&D, Short-term and Long-term disability insurance
Rx, dental, vision, short-term disability, and FSA
Employer-matched 401k Plan
Industry leading PTO plan
And MORE!
Apply today at :
https://careers-maxor.icims.com/
Maxor is an EOE, including disability/vets
Show more
Show less","SQL, ETL frameworks, Database relational models, Data visualization, Data exchange, Data integration, Data cleansing, Data mapping, Data formatting, Business acumen, Healthcare industry knowledge, Pharmacy benefit management industry knowledge, Problemsolving skills, Critical thinking skills, Data analysis skills, Communication skills, Documentation skills, Knowledge sharing skills, Adaptability, Continuous improvement skills, Relationshipbuilding skills, Goaloriented delivery skills, Blue Cross Blue Shield PPO, 401k Plan, PTO plan","sql, etl frameworks, database relational models, data visualization, data exchange, data integration, data cleansing, data mapping, data formatting, business acumen, healthcare industry knowledge, pharmacy benefit management industry knowledge, problemsolving skills, critical thinking skills, data analysis skills, communication skills, documentation skills, knowledge sharing skills, adaptability, continuous improvement skills, relationshipbuilding skills, goaloriented delivery skills, blue cross blue shield ppo, 401k plan, pto plan","401k plan, adaptability, blue cross blue shield ppo, business acumen, communication skills, continuous improvement skills, critical thinking skills, data analysis skills, data exchange, data formatting, data integration, data mapping, database relational models, datacleaning, documentation skills, etl frameworks, goaloriented delivery skills, healthcare industry knowledge, knowledge sharing skills, pharmacy benefit management industry knowledge, problemsolving skills, pto plan, relationshipbuilding skills, sql, visualization"
Experimentation Data Engineer - 22300,Wimmer Solutions,"SeaTac, WA",https://www.linkedin.com/jobs/view/experimentation-data-engineer-22300-at-wimmer-solutions-3762993109,2023-12-17,Puyallup,United States,Mid senior,Onsite,"EXPERIMENTATION DATA ENGINEER
JOB ID: 22300
At Wimmer Solutions, we believe care creates community. We work smart; we have built a reputation for results-oriented, innovative, business and technology solutions that help companies execute on their strategic initiatives. We have fun; we love our work. We are positive, kind, and hungry to learn. We give big; we aim to make a real impact on the causes that affect the communities we serve and build strong relationships with the dedicated volunteers and nonprofit organizations working to address them.
We are all about people and community. Since 2002, we have offered technology staffing and managed services for the greater Seattle area and throughout the United States. We focus on getting to know our clients and candidates to create lasting partnerships and ensure success.
Wimmer Solutions is looking for a
Data Engineer
in SeaTac, WA to join our client's digital experimentation team.
What You Get To Do
The Data Engineer supports the Experimentation Team in designing and conducting experiments across all digital platforms in the organization. This individual is responsible for extracting, transforming, optimizing, and maintaining data pipelines and systems to support experimentation and data analysis.
Key Duties
Assemble large, complex sets of data to meet non-functional and functional business requirements (Big Query, Adobe, Oracle, Azure).
Monitor and report on key metrics related to experiments, highlighting trends, anomalies, and areas for improvement.
Ability to build and optimize data sets, big data pipelines and architectures.
Develop and execute Extract, Transform, Load (ETL or ELT) processes to cleanse, enhance, and structure raw data for actionable insights and analysis.
Apply cutting-edge experimentation methods to conduct tests across all digital platforms.
Collaborate with cross-functional teams, including Data Scientists, Analysts, and Engineers to discern data requirements and provide high-quality data solutions for experiments.
Identify, design, and implement internal process improvements. This includes redesigning infrastructure for scalability, optimizing data delivery, and automating manual processes.
Support web and full stack experimentation efforts, leveraging tools such as Tealium and Optimizely.
WHAT YOU BRING
Must Have
3 years of experience in AB testing, data engineering, web analytics, or related area.
Experience with database systems (SQL and NoSQL) and data modeling.
Experience with Python programming language.
Detail-oriented and highly organized, with the ability to multi-task in a fast-paced environment.
Excellent communication skills (both written and verbal) and strong interpersonal skills with the ability to drive outcomes in the best interest of the company.
Working knowledge of experimentation design, implementation, and tools (AB, MVT, Multi-Armed Bandit; JS, CSS, HTML; Optimizely).
Bachelor's degree with a focus in computer science, mathematics, statistics or related discipline, or an additional 3 years of relevant training/experience in lieu of this degree.
Nice To Have:
Graduate degree with a focus in computer science, mathematics, statistics, or related discipline.
Proficiency in data analysis tools and programming languages (e.g., Python, R, SQL).
Working knowledge of software development (e.g., C#).
Must be able to work for a US based company without requiring visa sponsorship.
Compensation And Benefits
Hourly contract rate range of $40.00 to $44.00 based on experience and qualifications, as well as geographical market and business considerations.
Your well-being is important to Wimmer Solutions. All regular, full-time employees working a minimum of 30 hours per week are eligible to participate in the benefits plan. Outside of offering excellent medical, dental, and vision benefits, we also offer the following:
Paid time off and holidays
401k & company match
Flexible Health Care, Dependent Care, and Commuter Spending Accounts
Employee Assistance Program
Life & Accidental Death and Dismemberment Insurance
Short and Long-Term Disability
Payroll advance program, Charitable donation match, Athletic event sponsorship,
Referral reward program, and more…
MORE ABOUT WIMMER SOLUTIONS
Wimmer Solutions is proud to be an equal-opportunity employer. All applicants will be considered for employment regardless of race, color, religion or belief, age, gender identity, sexual orientation, national origin, parental status, veteran or disability status. Wimmer Solutions is committed to achieving a diverse employee network through all aspects of the hiring process and we welcome all applicants.
If you are passionate about what you do and want to join a diverse team dedicated to diversity, equity, and inclusion in the workplace, we would love to hear from you. Get the job you have always wanted. You will join a broad team of professionals who are energized about their careers as well as their community. For more career opportunities or to refer a friend, please visit http://wimmersolutions.com/careers and talk to a recruiter today.
Show more
Show less","Experimentation, Data Engineering, Data Analysis, Big Query, Adobe, Oracle, Azure, SQL, NoSQL, Python, AB Testing, Tealium, Optimizely, MVT, MultiArmed Bandit, HTML, CSS, JS, C#","experimentation, data engineering, data analysis, big query, adobe, oracle, azure, sql, nosql, python, ab testing, tealium, optimizely, mvt, multiarmed bandit, html, css, js, c","ab testing, adobe, azure, big query, c, css, data engineering, dataanalytics, experimentation, html, js, multiarmed bandit, mvt, nosql, optimizely, oracle, python, sql, tealium"
Senior Data Analyst,Turnberry Solutions,"Tacoma, WA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-turnberry-solutions-3783990441,2023-12-17,Puyallup,United States,Mid senior,Remote,"Duration:
6+ months
Location:
100% Remote
Overview:
Every Turnberry consultant belongs to a practice, an internal group of consultants and leaders with shared experience and expertise. Each of these practices aligns to one of the eight core services Turnberry offers to clients. As a Senior Data Analyst, you will join Turnberry's Data& Insights practice and service. Turnberry's Data& Insights service helps clients who seek visibility into company data, lack a single source of truth for accurate data, desire advanced analytics solutions, have data governance needs to ensure data quality, or have specific data challenges that may require an assessment, proof of concept, or custom solution not available in a typical out-of-the-box analytics methodology.
Responsibilities
﻿Collaborate with stakeholders to identify data requirements and translate business needs into effective data solutions
Collaborate cross-functionally with teams such as IT, business analysts, and data engineers to align data initiatives with business objectives
Develop and enforce data governance policies and procedures to maintain data quality, integrity, and security throughout the data lifecycle
Partner with business data owners to define data quality metrics, establish automated processes to monitor and report metrics
Establish metadata management strategies to maintain a robust catalog of data assets, definitions, and lineage
Communicate complex technical concepts and analysis results effectively to both technical and non-technical stakeholders
Design, implement, and optimize data solutions leveraging cloud-based platforms (e.g., AWS, Azure, GCP) to enable scalable and efficient data processing
Utilize cloud-native tools and technologies for data storage, processing, and analysis, ensuring security and performance standards
Develop and maintain data models to ensure consistency and standardization across the enterprise
Qualifications
10+ years of experience in designing and implementing enterprise data solutions
5+ years of experience in a data analysis role, with significant exposure to enterprise-level data management and integration projects
Strong knowledge of data modeling principles and best practices
Familiarity with data governance frameworks and metadata management tools
Ability to perform complex data analysis, derive actionable insights, and present findings effectively
Excellent communication and interpersonal skills, capable of conveying technical concepts to diverse audiences
Bachelor’s degree in Computer Science, Information Systems, Statistics, or a related field or equivalent experience
Preferred Qualifications
Proficiency in cloud-based data platforms (AWS, Azure, GCP)
The salary range for this role is $80,000 to $160,000 or the hourly equivalent. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, Turnberry Solutions offers benefits such as a comprehensive healthcare package (medical, dental, vision), disability and group term life insurance, health and flexible spending accounts, a utilization bonus, 401(k) with match, flexible time off for salaried employees, parental leave for salaried employees, and flexible work arrangements (all benefits are subject to eligibility requirements). No matter where or when you begin a career with Turnberry, you'll find a far-reaching choice of benefits and incentives.
At Turnberry, inclusion is one of our core values. We are fully invested in and focused on hiring and growing a diverse team of high performers. We're committed to creating a positive and connected work environment for all. We believe that uniqueness in ideas, experiences, and backgrounds make us a better Turnberry: Turnberry is an Equal Employment Opportunity/Affirmative Action employer, and recruits, employs, trains, compensates, and promotes regardless of age, ancestry, family medical or genetic information, gender identity and expression, marital, military, or veteran status; national and ethnic origin; physical or mental disability; political affiliation; pregnancy; race; religion; sex; sexual orientation; and any other protected characteristics.
Show more
Show less","Data Analysis, Data Engineering, Data Governance, Data Modeling, Data Quality, Data Security, Data Visualization, Cloud Computing, AWS, Azure, GCP, Data Storage, Data Processing, Data Integration, Metadata Management, Business Intelligence, Communication, Interpersonal Skills, Problem Solving, Critical Thinking, Teamwork, Attention to Detail, Proficient in Office Suite, SQL, Python, R, Tableau, Power BI","data analysis, data engineering, data governance, data modeling, data quality, data security, data visualization, cloud computing, aws, azure, gcp, data storage, data processing, data integration, metadata management, business intelligence, communication, interpersonal skills, problem solving, critical thinking, teamwork, attention to detail, proficient in office suite, sql, python, r, tableau, power bi","attention to detail, aws, azure, business intelligence, cloud computing, communication, critical thinking, data engineering, data governance, data integration, data processing, data quality, data security, data storage, dataanalytics, datamodeling, gcp, interpersonal skills, metadata management, powerbi, problem solving, proficient in office suite, python, r, sql, tableau, teamwork, visualization"
Sr. Data Center Engineer - Secret Clearance,Kavaliro,"Tukwila, WA",https://www.linkedin.com/jobs/view/sr-data-center-engineer-secret-clearance-at-kavaliro-3781036407,2023-12-17,Puyallup,United States,Mid senior,Hybrid,"Job Description
Our Client is seeking for a Sr. Data Center Engineer to support a Defense and Security Program. The Mission Systems Team is responsible for design, procurement and integration of the full Mission Systems capability onboard the E-7A
The E-7 is the worlds most advanced, capable and reliable Airborne Early Warning and Control (AEW&C) platform, having proven itself in operations around the world. The aircraft is designed to track multiple airborne and maritime targets simultaneously. It can provide situational awareness and direct other assets such as fighter jets and warships. The E-7A program is an important program for our client with significant growth potential.
Top Requirements/Skills
Bachelor, Master or PhD Degree in STEM.
Experience with Electrical and/or Harware Design. This person is responsible for building a Server room in an airplane.
Active Secret Clearance highly preferred, otherwise, ability to obtain a Secret Clearance.
Preferred Qualifications
Active Secret Clearance
1+ years of experience with DO-160G Standards
Experience with Mission Computer Hardware
Experience with FAA Certification
Primary Responsibilities
Develops and documents complex electronic and electrical system requirements.
Designs hardware, software and interface specifications.
Tests and validates to ensure system designs meet operational and functional requirements.
Assists in monitoring supplier performance to ensure system integration and compliance with requirements.
Solves problems concerning fielded hardware and software over the entire product lifecycle.
Researches specific technology advances for potential application to company business needs.
Additional Information
Bachelor's and 9+ years' experience, Master's with 7+ years experience or PhD with 4+ years' experience. Bachelor, Master or Doctorate of Science degree from an accredited course of study, in engineering, computer science, mathematics, physics or chemistry.
Basic Qualifications (Required Skills And Experience)
Bachelor, Master or Doctorate of Science degree from an accredited course of
study, in engineering, computer science, mathematics, physics or chemistry. ABET is the preferred, although not required, accreditation standard.
7+ or more years’ of experience with electrical and/or hardware design
Preferred Qualifications (Desired Skills And Experience)
Experience with Mission Computing Hardware
Experience with FAA certification
Work Hours:
Core Hours are between 9 AM to 1 PM, can start early and finish early, or start late and finish late. 9x80 schedule can be requested, but needs to be coordinated with the team after stating on the assignment.
Kavaliro provides Equal Employment Opportunities to all employees and applicants. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. Kavaliro is committed to the full inclusion of all qualified individuals. In keeping with our commitment, Kavaliro will take the steps to assure that people with disabilities are provided reasonable accommodations. Accordingly, if reasonable accommodation is required to fully participate in the job application or interview process, to perform the essential functions of the position, and/or to receive all other benefits and privileges of employment, please respond to this posting to connect with a company representative.
Job Requirements
Hybrid
Show more
Show less","Electrical Design, Hardware Design, Server Room Design, DO160G Standards, Mission Computer Hardware, FAA Certification, Electronic System Requirements, Hardware Specifications, Software Specifications, Interface Specifications, Testing and Validation, Supplier Performance Monitoring, System Integration, Problem Solving, Technology Research","electrical design, hardware design, server room design, do160g standards, mission computer hardware, faa certification, electronic system requirements, hardware specifications, software specifications, interface specifications, testing and validation, supplier performance monitoring, system integration, problem solving, technology research","do160g standards, electrical design, electronic system requirements, faa certification, hardware design, hardware specifications, interface specifications, mission computer hardware, problem solving, server room design, software specifications, supplier performance monitoring, system integration, technology research, testing and validation"
Statistician/Data Scientist Fellowship,Oak Ridge Institute for Science and Education,"Frankfort, KY",https://www.linkedin.com/jobs/view/statistician-data-scientist-fellowship-at-oak-ridge-institute-for-science-and-education-3733200004,2023-12-17,Lexington,United States,Mid senior,Onsite,"Organization
U.S. Department of Defense (DOD)
Reference Code
USAISR-2023-0014
How To Apply
Click on Apply at the bottom of the opportunity to start your application.
Description
The U.S. Army Institute of Surgical Research is offering a postgraduate Data Scientist fellowship at Sam Houston, Texas.
What will I be
doing
?
As an Oak Ridge Institute for Science and Education (ORISE) participant, you will join a community of scientists and researchers in an effort to collaborate with ISR scientists and clinicians on a variety of human and animal research projects that include, but are not limited to,
prospective and retrospective studies of ISR burn patients
randomized controlled clinical trials of ISR burn patients
animal studies of potential therapeutics to be used for orthopedic and sensory trauma, expeditionary critical care and tactical combat casualty care, hemorrhage and damage control resuscitation, and burn injuries
Why should I apply?
Under the guidance of a mentor, you will gain hands-on experience to complement your education and support your academic and professional goals. Along the way, you will engage in activities and research in several areas. These include, but are not limited to,
Developing relevant and testable trauma-related research questions and hypotheses
Designing both animal and human research studies
Analyzing research data using a variety of statistical software programs including JMP and SigmaPlot
Applying advanced statistical theories, techniques, and methods to human and animal research data
Learning basic epidemiology concepts
Interpreting clinical research findings in the context of causal inference and biological pathways
Learning how to prepare animal and human research study protocols and about the regulatory approval process required to conduct such studies
Where will I be located?
San Antonio, Texas
What is the anticipated start date?
The ISR is ready to make appointments immediately. Exact start dates will be determined at the time of selection and in coordination with the selected candidate. Applications are reviewed on an ongoing basis and internships or fellowships will be filled as qualified candidates are identified.
What is the appointment length?
Appointments are initially for one year with the option to extend the appointment for up to four additional years, contingent upon project needs and funding availability.
What are the benefits?
You will receive a stipend to be determined by USAISR. Stipends are typically based on a participant’s academic standing, discipline, experience, and research facility location. Other benefits may include the following:
Participants are eligible to purchase health insurance through ORISE
Relocation Allowance
Training and Travel Allowance
About USAISR
The U.S. Army Institute of Surgical Research is one of six research laboratories within the U.S. Army Medical Research and Materiel Command of the U.S. Army Medicine Command. The Institute is the Army's lead research laboratory for improving the care of combat casualties. The mission of the Institute is to ""Optimize Combat Casualty Care"".
About ORISE
This program, administered by Oak Ridge Associated Universities (ORAU) through its contract with the U.S. Department of Energy (DOE) to manage the Oak Ridge Institute for Science and Education (ORISE), was established through an interagency agreement between DOE and DoD. Participants do not enter into an employee/employer relationship with ORISE, ORAU, DoD or any other office or agency. Instead, you will be affiliated with ORISE for the administration of the appointment through the ORISE appointment letter and Terms of Appointment. Proof of health insurance is required for participation in this program. Health insurance can be obtained through ORISE. For more information, visit the ORISE Research Participation Program at the U.S. Department of Defense .
Qualifications
The qualified candidate will have a Bachelor's, Master's or Doctoral degree in any of the disciplines specified in the opportunity listing. The degree must have been received within five years of the appointment start date.
Application Requirements
A complete application consists of:
Zintellect Profile
Educational and Employment History
Essay Questions (goals, experiences, and skills relevant to the opportunity)
Resume (PDF)
Transcripts/Academic Records - Please upload a copy of a transcript for your current or most recent degree program that meets the disciplinary qualifications of the opportunity. Click here for detailed information about acceptable transcripts .
One recommendation. Your application will be considered incomplete and will not be reviewed until one recommendation is submitted. We encourage you to contact your recommender(s) as soon as you start your application to ensure they are able to complete the recommendation form and to let them know to expect a message from Zintellect. Recommenders will be asked to rate your scientific capabilities, personal characteristics, and describe how they know you. You can always log back in to your Zintellect account and check the status of your application.
If you have questions, send an email to ARMY@MRMC@orise.orau.gov. Please list the reference code of this opportunity [USAISR-2023-0014] in the subject line of the email. Please understand that ORISE does not review applications or select applicants; selections are made by the sponsoring agency identified on this opportunity. All application materials should be submitted via the “Apply” button at the bottom of this opportunity listing. Please do not send application materials to the email address above.
Connect with ORISE...on the GO!
Download the new ORISE GO mobile app in the Apple App Store or Google Play Store to help you stay engaged, connected, and informed during your ORISE experience and beyond!
Eligibility Requirements
Citizenship: U.S. Citizen Only
Degree: Bachelor's Degree, Master's Degree, or Doctoral Degree received within the last 60 month(s).
Discipline(s):
Mathematics and Statistics ( 11 )
Show more
Show less","Data Science, Statistics, Statistical Software, Research, Clinical Trials, Epidemiology, Causal Inference, Biological Pathways, Hypothesis Development, Study Design, JMP, SigmaPlot","data science, statistics, statistical software, research, clinical trials, epidemiology, causal inference, biological pathways, hypothesis development, study design, jmp, sigmaplot","biological pathways, causal inference, clinical trials, data science, epidemiology, hypothesis development, jmp, research, sigmaplot, statistical software, statistics, study design"
Data Analyst,MarshBerry,"Beachwood, OH",https://www.linkedin.com/jobs/view/data-analyst-at-marshberry-3770157477,2023-12-17,Cleveland,United States,Associate,Hybrid,"MarshBerry is growing! We are seeking a
Data Analyst
to join our team. We have a people first, fast paced, collaborative culture with plenty of opportunity for growth. MarshBerry has been successful in achieving growth objectives because the trust our clients place in our talented team of professionals, and all MarshBerry colleagues play a critical role in directly or indirectly cultivating those trusted relationships. MarshBerry provides an environment where employees can learn, improve and realize their career goals. We offer competitive benefits, hybrid work schedules, new challenges, and learning experiences.
Job Details
Position Summary:
MarshBerry is currently seeking
a Data Analyst
for our
Beachwood, OH
office. The
Data Analyst
will be responsible for administration of MarshBerry’s proprietary database, assisting with product development, and research projects. The Data Analyst enters client data, produces reports, analyzes reports, and reviews results with clients.  They will collaborate with the team on client projects and enhancements to performance benchmarking reports.
Responsibilities:
Serve as project manager for assigned clients and be responsible for data collection, formatting and report generation on a quarterly or annual basis.
Extract data and generate reports for special requests and projects.
Establish and maintain positive client relationships.
Assist with market research, surveys, client presentations, and various data analytics projects.
Assist with product development and enhancements to MarshBerry products and services.
Answer client inquiries regarding Intellectual Capital products and services.
Develop and update procedures on a continual basis.
Qualify leads through reactive methods to introduce our services and opportunities.
Provide guidance to Analysts and Consultants on formatting of accounts, calculations of ratios, etc.
Train new team members and interns in Intellectual Capital processes and knowledge.
Complete projects in desired timeframe utilizing prioritization skills to keep multiple projects progressing on target.
Cultivate and maintain effective relationships with potential internal/external clients, partners and stakeholders that can directly or indirectly lead to revenue generation for all MarshBerry services.
Special projects and other tasks as assigned.
Selection Criteria
Education & Experience:
Bachelor’s Degree in Business, Accounting, Finance, Economics or Mathematics preferred or equivalent combination of education and experience.
At least 1 - 3 years of prior relevant experience including experience collecting and analyzing data.
Understanding of data modeling in the financial services and insurance industry preferred.
Familiar with FinTech software preferred.
High level of computer and software competency; proficient with database administration and infrastructure. Proficient in Microsoft Office Word, Excel, Access and PowerPoint.
Other:
Analytical, proactive problem solving skills; techniques to identify and resolve issues in a timely manner, gathers and analyzes information skillfully.
Confidentiality; maintains confidentiality of proprietary information and that of clients.
Strong communication skills; both written and verbal with demonstrated creativity with regard to work. Excellent spelling, grammar as well as good phone skills.
Exceptional organizational skills; using systematic methods to perform work and creativity to recommend or create new work methods or procedures.
Ability to multi task; able to complete projects and responsibilities with extreme attention to detail according to required timelines and deadlines, along with capacity to work under pressure to create accurate results, demonstrating accuracy and thoroughness and monitors works to ensure quality.
Working at MarshBerry
Who We Are:
MarshBerry practices The Collaborative Way which encourages employees to adhere to these five principles: listening generously, speaking straight, being for each other, acknowledgement & appreciation, and honoring commitments. We are committed to fostering an environment of Diversity, Equity, and Inclusion. We strive to educate our current and potential employees in these areas, while continuing to promote a welcoming and inclusive environment for all.
What We Do:
MarshBerry provides consulting services in the financial services industry primarily to independent insurance agents, brokers and carriers, as well as wealth and retirement plan advisors. Our services include but are not limited to financial, operational, sales management, merger and acquisition advisory, peer-to-peer exchange and information services. We are recognized in the financial services industry for providing innovative and customized solutions to our clients, with whom we build trusted advisor relationships. At MarshBerry, our mission is to help our clients learn, improve and realize their value. Our clients credit us for providing the vision, tools, and discipline to help them reach their strategic goals.
It’s one thing to be recognized as experts in our field, it is another to be noteworthy based on the sentiment and feedback of our team. MarshBerry has recently been awarded the following:
Crain’s Best Employers in Ohio
The Nation’s Best and Brightest in Wellness
North Coast 99
Top Work Places - The Plain Dealer
Weatherhead 100
West Michigan’s Best and Brightest Companies to Work for
To learn more about MarshBerry, visit www.marshberry.com.
We appreciate your interest in MarshBerry. As an equal opportunity employer, your application will be considered with regard to all laws which prohibit discrimination because of race, color, sex, religion, national origin, age, disability, military status, and genetic information, and requirements to take affirmative action in the hiring of minorities, including women, veterans, and those individuals with disabilities.
Show more
Show less","Data modeling, Data analytics, Data visualization, Report generation, Microsoft Excel, Microsoft Access, Microsoft PowerPoint, Oracle SQL, SAS, Business intelligence, Data mining, Statistical analysis, Project management, Client relations, Research, Financial analysis, Business analysis, FinTech, Data management, Database administration","data modeling, data analytics, data visualization, report generation, microsoft excel, microsoft access, microsoft powerpoint, oracle sql, sas, business intelligence, data mining, statistical analysis, project management, client relations, research, financial analysis, business analysis, fintech, data management, database administration","business analysis, business intelligence, client relations, data management, data mining, dataanalytics, database administration, datamodeling, financial analysis, fintech, microsoft access, microsoft excel, microsoft powerpoint, oracle sql, project management, report generation, research, sas, statistical analysis, visualization"
Sales Data Analyst,Equity Trust Company,"Westlake, OH",https://www.linkedin.com/jobs/view/sales-data-analyst-at-equity-trust-company-3772464948,2023-12-17,Cleveland,United States,Associate,Hybrid,"*This position is based out of our Westlake, OH office but eligible for a hybrid remote schedule.
JOB OVERVIEW
The Sales Data Analyst is responsible for the development and administration of sales analytics data, converting raw data into meaningful insights through interactive dashboards and easy-to-understand reports. Also provides day-to-day oversight of sales technologies and optimizes performance while providing data management and analytics for the retail and institutional sales divisions.
RESPONSIBILITIES & DUTIES
Support internal executives, sales team, and other key stake holders with detailed visibility into all aspects of business performance
Leverage Microsoft Power Bl and other tools to analyze sales data and interpret results to drive strategic business decisions
Performs all administrative and superuser functions for the CRM system and other digital tools including adding and removing users, monitoring data feeds, updating data values, adjusting workflows, creating new reports, dashboards, etc.
Create dashboards and interactive visual reports using Salesforce and Power BI
Design, develop, test, and deploy Power BI scripts and perform detailed analytics
Ensures sales dashboards are accurate and easily understood
Acts as the central point of contact for all change requests and system alterations, and works with Sales, Marketing and IT leadership to assess, prioritize and implement as needed
Develops and employs data governance processes to ensure cleanliness and quality of the client and prospect database and its appropriate use, including dashboards and ongoing audits of systems and processes
Develops and executes test plans of new CRM releases/features and communicates and trains existing users on new features
Partners with Business Intelligence to implement the reporting needs of the sales and marketing groups
Identifies and recommends areas within the sales process that can be improved through systems enhancements and automation
Partners with the business and technical teams to design and deploy CRM and related solutions
Collaborates with Sales Leaders to develop systems to prioritize customer and prospect lists, teams and associated workflows
Maintains transparent communication by sharing appropriate organizational information through department meetings and one-on-one meetings, email and regular interpersonal communication
Complete ad hoc reports to provide data to inform data drive decisions and opportunities for improvement.
Collaborate with marketing and sales leadership to develop growth objectives
Acts as trusted resource and support for the Sales Team
Healthy skepticism and curiosity - seeks feedback with an open mind and strives for deeper understanding through data.
Perform other duties as assigned
QUALIFICATIONS
Minimum 5 years’ sales operations and sales analysis experience
Minimum experience of 2 - 3 years working with BI tools, or any data-specific roles
Bachelor’s degree in Sales, Business, Marketing or related field; equivalent work experience may be considered in lieu of a degree
PROFESSIONAL CERTIFICATIONS
None required
TECHNICAL SKILLS
To be successful in this role, you should have experience with and an understanding of the following:
Advanced level proficiency in Microsoft Office Suite of Tools
Comfort with tools that create and house our data: Salesforce for complex dashboards & reports, custom report types, calculated fields, workflows. PowerBI business intelligence software. Marketo and other marketing automation tools and processes.
Exceptional quantitative and MS Excel skills
CULTURAL COMPETENCIES
In addition to our core company competencies of Cultivates Innovation, Nimble Learning, Action Oriented, Collaborates, and Being Resilient, a successful candidate in this role should exhibit the following behavioral competencies:
Tech Savvy
Communicates Effectively
Optimizes Work Processes
Nimble Learning
PHYSICAL DEMANDS/WORK ENVIRONMENT
This job operates in a professional office environment and routinely uses standard office equipment. While performing the duties of this job, the associate is regularly required to speak and hear. The associate is frequently required to sit for extended periods of time, as well as stand, walk, use hands and fingers, and reach with hands and arms. This job requires the ability to lift files, open filing cabinets, and bend or stand on a stool as necessary.
DISCLAIMER/ASSOCIATE ACKNOWLEDGEMENT
The above statements describe the general nature and level of work only. They are not an exhaustive list of all required responsibilities, duties, and skills. Other duties may be added, or this description amended at any time.
Equity Trust Company is an equal opportunity at will employer and does not discriminate against any employee or applicant for employment because of age, race, religion, color, disability, sex, sexual orientation or national origin.
Show more
Show less","Microsoft Power BI, Microsoft Office Suite, Salesforce, Marketo, MS Excel, Data governance, Data analytics, Business intelligence, BI tools, Software development, System administration, CRM systems, ETL, Data visualization, Dashboard creation, Report creation, Data analysis, Data interpretation, Business decision making, Data management, Digital tools, Sales","microsoft power bi, microsoft office suite, salesforce, marketo, ms excel, data governance, data analytics, business intelligence, bi tools, software development, system administration, crm systems, etl, data visualization, dashboard creation, report creation, data analysis, data interpretation, business decision making, data management, digital tools, sales","bi tools, business decision making, business intelligence, crm systems, dashboard creation, data governance, data interpretation, data management, dataanalytics, digital tools, etl, marketo, microsoft office suite, microsoft power bi, ms excel, report creation, sales, salesforce, software development, system administration, visualization"
Integrations Data Engineer III,TekIntegral,"Cleveland, OH",https://www.linkedin.com/jobs/view/integrations-data-engineer-iii-at-tekintegral-3667474615,2023-12-17,Cleveland,United States,Mid senior,Onsite,"Hi Everyone,
Hope you are doing great
I have very urgent need Integrations Data Engineer role please let me know if you are interested.
Job Title: Integrations Data Engineer III
Location: Cleveland, OH
Duration: 6 Months
Note:-
LinkedIn profile mandate
This is a Mon-Fri 8 hours/day job that requires off-hours production deployment/issues handling. The developer will work on-site at Cleveland and should be able to relocate to Cleveland within a month.
This is a temp to permanent position after 6 months. The conversion salary range is approx $80k-100k based on their prior work experience and skill set.
The worker should take care of car parking on their own.
Candidates selected will have a first interview with the department manager through Teams. If selected for a second interview they will meet with other members of the department through Teams.
The Integrations Data Engineer III is responsible for the design, development, testing, and support of enterprise ETL Feeds in support of various modern architecture projects. In-depth hands-on knowledge of modern Integrations Platform solutions, file and database ETL, XML transformation, web services, data security, integrity, validity, and reliability are required at the enterprise level. The Integrations Data Engineer will provide technical consulting to peers, management, business analysts, technical associates, and will collaborate with the architecture team and larger enterprise Integrations Community of Practice.
Mandatory Skills
5+ years IT experience.
5+ years' experience with the entire software development lifecycle i.e., requirements, design, implementation, integration testing, deployment and post-production support.
3+ years' experience building file-sourced ETL feeds on Talend Open Data Studio.
3+ years developing ETL between the following formats: CSV, XML, JSON, Database.
2+ years developing or administering solutions in an enterprise application platform such as Java EE.
Advanced understanding of XML, XSD, XSLT, WSDL, and WSDL versioning best practices.
Advanced understanding of Enterprise RDBMS, ER Diagrams, and developing SQL programs.
Experience using SOAP/RESTful APIs in Talend ETL.
Strong ability in designing reusable and highly performant solutions.
Desired Skills
Experience with Terraform -Experience with Dell Boomi Integration Platform-as-a-Service.
Experience developing, securing, and optimizing data feeds using public cloud services such as Amazon Web Services, Microsoft Azure, and Google Cloud Platform.
Strong understanding of Service Oriented Architecture concepts.
Strong understanding of Microservices concepts.
Fundamental understanding of API Gateway / API Manager concepts.
Knowledge of authentication utilizing LDAP compliant technologies and authorization through SAML tokens.
Experience in creating/maintaining deployments using ANT or MAVEN scripts.
Experience in working in an Agile development environment.
Experience in using Jenkins and Git
Mohit Rajput
Technical Recruiter
TekIntegral Inc.
500 N Central Expwy #500G
Plano, TX USA 75074
mrajput@tekintegral.com
www.tekintegral.com
Show more
Show less","Talend Open Data Studio, CSV, XML, JSON, Java EE, SOAP, RESTful APIs, Terraform, Dell Boomi Integration PlatformasaService, Amazon Web Services, Microsoft Azure, Google Cloud Platform, Service Oriented Architecture, Microservices, API Gateway, API Manager, LDAP, SAML, ANT, MAVEN, Agile, Jenkins, Git","talend open data studio, csv, xml, json, java ee, soap, restful apis, terraform, dell boomi integration platformasaservice, amazon web services, microsoft azure, google cloud platform, service oriented architecture, microservices, api gateway, api manager, ldap, saml, ant, maven, agile, jenkins, git","agile, amazon web services, ant, api gateway, api manager, csv, dell boomi integration platformasaservice, git, google cloud platform, java ee, jenkins, json, ldap, maven, microservices, microsoft azure, restful apis, saml, service oriented architecture, soap, talend open data studio, terraform, xml"
Staff Data Engineer,Recruiting from Scratch,"Cleveland, OH",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744397167,2023-12-17,Cleveland,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Apache Airflow, Kubernetes, Helm, Scala, PySpark, Spark, SQL, Snowflake, Kafka, Storm, Spark Streaming, TDD, Pair programming, Kafka, Dimensional data modeling, ETL, Data governance, Data classification, Data retention, Snowplow, BigQuery, Dataform","python, apache airflow, kubernetes, helm, scala, pyspark, spark, sql, snowflake, kafka, storm, spark streaming, tdd, pair programming, kafka, dimensional data modeling, etl, data governance, data classification, data retention, snowplow, bigquery, dataform","apache airflow, bigquery, data classification, data governance, data retention, dataform, dimensional data modeling, etl, helm, kafka, kubernetes, pair programming, python, scala, snowflake, snowplow, spark, spark streaming, sql, storm, tdd"
Entry Level Data Analyst/Management Consultant - Nationwide (US Based Candidates Only),Arcadis,"Cleveland, OH",https://www.linkedin.com/jobs/view/entry-level-data-analyst-management-consultant-nationwide-us-based-candidates-only-at-arcadis-3701468616,2023-12-17,Cleveland,United States,Mid senior,Onsite,"Arcadis is the world's leading company delivering sustainable design, engineering, and consultancy solutions for natural and built assets.
We are more than 36,000 people, in over 70 countries, dedicated to improving quality of life. Everyone has an important role to play. With the power of many curious minds, together we can solve the world’s most complex challenges and deliver more impact together.
Role description:
Note: See below regarding the nature of this position being a prospecting position.
Arcadis is currently seeking Analysts and Junior Management Consultants to join our world-class Business Advisory practice nationwide.
We are looking for candidates who want to apply technical know-how, combined with business principles, to the water, wastewater, and stormwater industry. We want dedicated, creative, and energetic candidates interested in tackling challenges and developing sustainable solutions to address water issues like renewal and replacement of aging infrastructure, funding of capital improvements, water supply, workforce retention and development, and emergency preparedness. Collaborating with our experienced consulting professionals, you will support and contribute to project outcomes; interact, and work with clients, and develop your technical capabilities.
We are a People First company, industry thought leaders, and drivers and allies of utility innovation.
Our passion: to Improve Quality of Life.
Our approach: to delight our clients by developing successful long-term partnerships and supporting them to address existing and emerging challenges.
Arcadis provides multiple onboarding and development programs created for young professionals that support professional growth and help drive creativeness, innovation, and greater integration within our local, National and global teams.
Role accountabilities:
What will you do?
Assess, develop, and support a variety of management consultant projects including performing data analytics, financial analysis, operational and organizational assessments, condition assessments, vulnerability, and mitigation assessments, as well as planning and development for utilities, municipalities, and cities’ (primarily water/wastewater/stormwater utilities).
Utilize strong analytical skills and ability to apply logic to solve problems.
Support teams in tasks ranging from general fieldwork to technical office-based analysis.
Assist in technical writing which may include preparation of technical reports, business development support, presentations, and other audiovisual materials.
Work independently and as part of a team, with the flexibility to accommodate collaboration with team members across the U.S. and internationally.
Manage multiple concurrent projects with multiple deadlines, ensuring completion per project budgets and timelines.
What skills will you need?
Reliable, client-focused, and capable of working independently under the supervision of project managers.
Exceptional analytical and problem-solving skills, strong attention to detail, organization skills, and work ethic.
Self-motivated and team-oriented, with the ability to work successfully both independently and within a team.
Ability to balance and address new challenges as they arise and an eagerness to take ownership of tasks.
Knowledge of engineering concepts, theories, and practices related to water/wastewater/stormwater.
Drive to succeed and grow a career in the utility industry
Qualifications & Experience:
Required Qualifications:
Masters of Science degree in Civil or Environmental Engineering, or closely related STEM discipline; or business analytics/MBA, MS in data science or related business discipline.
For those with engineering degrees, ability to obtain the EIT within six months of start date
Preferred Qualifications:
Previous relevant consulting or utility experience, either internship or full-time.
Experience applying programming languages and analytics to problem-solving is a plus
SharePoint, Building Information Modeling (BIM), Power BI, Excel, PowerPoint, Visio, Change Management skills, and/or Augmented Reality experience
This is a general job posting and not tied to a specific current open position. Please make sure you create a search agent to be alerted of specific opportunities of interest. Candidates who submit their resume to this posting may be considered for all future openings as they arise.
Why Arcadis?
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills-based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You’ll do meaningful work, and no matter what role, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark, on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Join Arcadis. Create a Legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We are an equal opportunity and affirmative action employer. Women, minorities, people with disabilities and veterans are strongly encouraged to apply. We are dedicated to a policy of non-discrimination in employment on any basis including race, creed, color, religion, national origin, sex, age, disability, marital status, sexual orientation, gender identity, citizenship status, disability, veteran status, or any other basis prohibited by law.
Arcadis offers benefits for full time and part time positions. These benefits include medical, dental, and vision, EAP, 401K, STD, LTD, AD&D, life insurance, paid parental leave, reward & recognition program and optional benefits including wellbeing benefits, adoption assistance and tuition reimbursement. We offer seven paid holidays and potentially up to two floating holidays per calendar year depending on start date, and 15 days PTO that accrue per year. The salary range for this position is $52000 - 89700 / year.
#ANACollege
Show more
Show less","Data Analytics, Financial Analysis, Operational Assessment, Organizational Assessment, Condition Assessment, Vulnerability Assessment, Mitigation Assessment, Planning, Development, Engineering, Water, Wastewater, Stormwater, Utilities, Municipalities, Cities, Analytics, Programming Languages, SharePoint, Building Information Modeling (BIM), Power BI, Excel, PowerPoint, Visio, Change Management, Augmented Reality","data analytics, financial analysis, operational assessment, organizational assessment, condition assessment, vulnerability assessment, mitigation assessment, planning, development, engineering, water, wastewater, stormwater, utilities, municipalities, cities, analytics, programming languages, sharepoint, building information modeling bim, power bi, excel, powerpoint, visio, change management, augmented reality","analytics, augmented reality, building information modeling bim, change management, cities, condition assessment, dataanalytics, development, engineering, excel, financial analysis, mitigation assessment, municipalities, operational assessment, organizational assessment, planning, powerbi, powerpoint, programming languages, sharepoint, stormwater, utilities, visio, vulnerability assessment, wastewater, water"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Cleveland, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744397381,2023-12-17,Cleveland,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, ETL, Data Warehouses, Data Compliance, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, dimensional data modeling, schema design, etl, data warehouses, data compliance, data retention","airflow, business intelligence, continuous integration, data compliance, data engineering, data retention, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Cleveland, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748832233,2023-12-17,Cleveland,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, TDD, Continuous Delivery, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Data Warehousing, ETL, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design","data engineering, tdd, continuous delivery, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, data warehousing, etl, kafka, storm, sparkstreaming, dimensional data modeling, schema design","airflow, continuous delivery, data engineering, datawarehouse, dimensional data modeling, docker, etl, helm, kafka, kubernetes, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Data Scientist,Axuall,"Cleveland, OH",https://www.linkedin.com/jobs/view/data-scientist-at-axuall-3727962558,2023-12-17,Cleveland,United States,Mid senior,Remote,"About Axuall
Built with leading healthcare systems, Axuall is a workforce intelligence company built on top of a national real-time practitioner data network. It enables healthcare systems, staffing firms, telehealth, and health plans to dramatically reduce onboarding and enrollment time while also providing unique, powerful data insights for network planning, analytics, and reporting. Its network streamlines the secure sharing of digitally verified credentials between clinicians, authorized verifiers, and organizations that require this information quickly and continuously to meet patient demand, ensure clinical coverage, and maximize revenue capture.
In 2023, Axuall closed its series B round of financing, bringing its total capital raised to more than $41 million. Axuall's investors and the organizations they represent comprise over two dozen of the nation's leading healthcare organizations that recognize the imperative to improve clinical workforce efficiency amidst significant economic and staffing challenges. Axuall intends to scale its team rapidly in 2023/2024 as it expands its customer base and develops new products that leverage its national network.
About The Role
Reporting directly to the Chief Product Officer, the role will collaborate closely with the product manager responsible for Axuall's data insights solutions to develop new offers for the market. This will mean not only having a deep understanding of Axuall's data set, it will also mean supporting the acquisition of external data sets, and working with clients to deliver and productize key insights. The ideal candidate will be highly detail oriented, a highly creative self-starter, diligently organized, and have a passion for data and analytics.
What You'll Do
Work with executives and product leaders to identify product opportunities to deliver to Axuall's hospital and health system customers
Engage customers to understand their questions and work with Axuall's data to deliver answers
Structure large data sets, including Axuall's clinician data and work with sourced claims data to find patterns and insights
Act as in-house expert to Axuall's provider data in support of product and internal initiatives
Drive strategic data acquisition and further Axuall's goal to be the most insightful clinician data company
What You Bring
Candidates for the Data Scientist role must demonstrate extensive hands-on experience and knowledge of industry best practices in the functions listed above. Specific requirements include:
4+ years experience in a data scientist role, or working with big data sets to deliver insights to internal users and customers
Solid understanding of healthcare specific data sources, including claims data, clinician data or patient health records is a plus
Bachelor's Degree with emphasis in: Information Technology, Mathematics, Management Information System (MIS), Statistics, or Business Administration
Attention to detail and a high degree of accuracy
Strong analytical skills and knowledge of statistical methodologies and data analysis techniques
Use data visualization tools such as Tableau to communicating insights effectively
Knowledge of big data technologies like Hadoop, Spark, and distributed computing frameworks
Understanding and experience with the application of machine learning algorithms and AI for predictive analytics and pattern recognition.
Ability to communicate well both verbally and in writing with all levels of the organization
Strong Teamwork and Collaboration skills to effectively collaborate with data engineers, product managers, business analysts, and other team members.
What You'll Get From Us
We offer a wide range of benefits for our team including comprehensive health insurance, unlimited PTO/vacation days, remote work flexibility, 401(k) with employer match, fitness stipend, travel-to-work stipend, continuous education support, and paid family leave.
Axuall is proud to be an equal-opportunity employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or veteran status.
Know Your Rights
The Family and Medical Leave Act (FMLA)
Workplace Discrimination and Harassment (Formally EEO)
Employee Polygraph Protection Act (EPPA)
Show more
Show less","Tableau, Hadoop, Spark, Machine learning, Big data, Predictive analytics, Pattern recognition, Data visualization, Data analysis, Data engineering, Product management, Business analysis, Healthcare, Statistics, Mathematics, Information technology","tableau, hadoop, spark, machine learning, big data, predictive analytics, pattern recognition, data visualization, data analysis, data engineering, product management, business analysis, healthcare, statistics, mathematics, information technology","big data, business analysis, data engineering, dataanalytics, hadoop, healthcare, information technology, machine learning, mathematics, pattern recognition, predictive analytics, product management, spark, statistics, tableau, visualization"
Data Warehouse Analyst,Robert Half,"Elyria, OH",https://www.linkedin.com/jobs/view/data-warehouse-analyst-at-robert-half-3783983197,2023-12-17,Cleveland,United States,Mid senior,Remote,"Description
Robert Half is seeking a Data Warehouse Engineer to aid its client in upgrading their reporting suite to the newest industry standards, and to expand and improve their existing data warehouse environment.
What You'll Do
Apply skills in database design and data modeling to identify large and complex data sets.
Design, build, and manage custom data pipelines in support of data initiatives.
Design, build, and manage new data repositories.
Transform data backwards from local data sources for integration with third-parties.
Perform transformations to move data to both cloud-based and on-prem databases.
Apply your skills to generate insights and help solve business challenges.
Requirements
What you'll need:
Broad familiarity with both SQL and NoSQL databases.
Particularly robust experience with the Microsoft tech stack, namely MS SQL and Azure.
Extensive experience with Extract, Transform, and Load (ETL) processes.
Specific knowledge of ETL tools such as SSIS and Azure Data Factory.
Thorough knowledge of varied methods of building data pipelines.
Proven experience in building new databases and data warehouses.
Comfort and expertise in performing large-scale data migrations, both on-prem and to the cloud.
Understanding of API development and integrations a plus.
What Would Set You Apart
Agile experience a plus.
Experience working both individually and in small group settings.
Familiarity with data modeling tools.
Exposure to DevOps and Containerization tools like Jenkins, Kubernetes, Docker, and Azure DevOps a bonus.
Comfort with unstructured data formats like XML and JSON useful.
Understanding of BI visualization tools a plus.
Previous exposure to other cloud-based data repositories such as AWS and GCP a plus.
Technology Doesn't Change the World, People Do.®
Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.
Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.
All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available including medical, vision, dental, life and disability insurance. Employees hired for our FTEP Program are also eligible to enroll in our company’s 401(k) or deferred compensation plan (if eligible). FTEP employees also earn paid time off for vacation, personal needs, and sick time and paid holidays. The amount of Choice Time Off (CTO) received varies based on years of service and is pro-rated based on the hours worked per week. A new FTEP employee earns up to 13 days of CTO and up to 10 paid holidays per calendar year. Learn more at
© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to
Show more
Show less","SQL, NoSQL, Microsoft tech stack, MS SQL, Azure, ETL, SSIS, Azure Data Factory, Data pipelines, Data migration, API development, Agile, Data modeling tools, DevOps, Containerization, Jenkins, Kubernetes, Docker, Azure DevOps, XML, JSON, BI visualization tools, AWS, GCP","sql, nosql, microsoft tech stack, ms sql, azure, etl, ssis, azure data factory, data pipelines, data migration, api development, agile, data modeling tools, devops, containerization, jenkins, kubernetes, docker, azure devops, xml, json, bi visualization tools, aws, gcp","agile, api development, aws, azure, azure data factory, azure devops, bi visualization tools, containerization, data migration, data modeling tools, datapipeline, devops, docker, etl, gcp, jenkins, json, kubernetes, microsoft tech stack, ms sql, nosql, sql, ssis, xml"
"Senior Database Engineer, MS SQL Server",STERIS,"Mentor, OH",https://www.linkedin.com/jobs/view/senior-database-engineer-ms-sql-server-at-steris-3604546496,2023-12-17,Cleveland,United States,Mid senior,Hybrid,"At STERIS, we help our Customers create a healthier and safer world by providing innovative healthcare and life science product and service solutions around the globe.
Position Summary
Join STERIS' global Information Technology team as it provides world class services and support to this growing, market leading company. The Microsoft SQL Server, Senior Database Administrator will be responsible for providing production database operations, ensuring performance and scalability, high availability and disaster recovery, compliance and security of database servers in addition to providing technical leadership within DBA team.
You will develop best practices and repeatable procedures for deploying databases, day-to-day operational activities including break/fix, space management, backup and restore, performance monitoring and tuning. You will also recommend physical database design, architecture, and testing of database technologies. identify data sources, construct decomposition diagrams, provide data flow diagrams and document processes.
This position is located onsite in Mentor, Ohio with the opportunity for a hybrid work schedule. Qualified candidates must relocate or be located within driving distance of Mentor, Ohio.
Duties
Manages MS SQL Server databases through multiple product lifecycle environments, from development to mission critical production systems.
Applies data modeling techniques to ensure development and implementation support efforts meet integration and performance expectations.
Provides high availability solutions for multiple database configurations.
Uses extensive experience in disaster recovery solutions and best practices to create, maintain, and verify backups using native and 3rd party solutions. Discusses RPO and RTO strategies with business leaders.
Performs advanced performance monitoring and troubleshooting. Creates execution plans, uses database optimizers, and has experience using database-specific monitoring technologies.
Applies a cybersecurity mindset and best practices to secure Database servers. Interacts/guides audit and compliance teams. Uses data security/encryption mechanisms (e.g., TDE, data masking/redacting/subsetting). Hardens database servers based upon security hardening requirements. Assists with security audits from internal and external providers.
Identifies and resolves capacity issues. Regularly monitors/reviews performance metrics to identify trends and future risks in the environment before they happen and works with leadership to get the work prioritized.
Independently analyzes, solves, and corrects issues in real time, providing problem resolution end-to-end.
Refines and automates regular processes, tracks issues, and documents changes.
Assists developers with complex query tuning and schema refinement.
Provides 24x7 support for critical production systems. Participates in an On-Call rotation as needed
Performs scheduled maintenance and support release deployment activities after hours.
Shares domain and technical expertise, providing technical mentorship and cross training to other peers and team members.
Installs, configures, patches, and upgrades database servers. Manages on-prem and cloud based (Azure-preferred) database servers and services in PaaS and IaaS cloud solutions.
Required Experience
Bachelor’s degree in Computer Science, Information Technology or other related degree along with 5+ years of MS SQL Server Database Administration experience.
In lieu of degree 13+ years of MS SQL Database Administration experience.
Technical knowledge in database development methodologies, design and implementation and basic database administration skills/experience (procedures for changon, R, Hadoop, Spark, Kafka, TensorFlow, PyTorch, Data visualization tools, Big data tools, ETL tools, Oracle, MySQL, PostgreSQL, Java, Scala","aws, kinesis, glue, data pipeline, amazon s3, amazon rds, redshift, ml ops, sagemaker, classification, regression, clustering, nlp, cnn, amazon quicksight, tableau, powerbi, sql, python, r, hadoop, spark, kafka, tensorflow, pytorch, data visualization tools, big data tools, etl tools, oracle, mysql, postgresql, java, scala","amazon quicksight, amazon rds, amazon s3, aws, big data tools, classification, clustering, cnn, data pipeline, data visualization tools, etl tools, glue, hadoop, java, kafka, kinesis, ml ops, mysql, nlp, oracle, postgresql, powerbi, python, pytorch, r, redshift, regression, sagemaker, scala, spark, sql, tableau, tensorflow",
Data Analyst,ASK Consulting,"Sandy, UT",https://www.linkedin.com/jobs/view/data-analyst-at-ask-consulting-3778717822,2023-12-17,Utah,United States,Mid senior,Onsite,"""All candidates must be directly contracted by ASK Consulting on their payroll and cannot be subcontracted. We are unable to provide sponsorship at this moment"".
Job Title: Data Analyst
Location: Sandy, UT
Duration: 12 Month Contract
Pay Range is $30 to $33.33/hr. on W2
Job Description:
Customer Request Handling / Problem Resolution / Data Analysis – as assigned
Perform different levels of problem resolution & data analysis for all aspects of retirement operations support including: contributions, distributions, processing conversions, recharacterizations, and state withholding reconciliation
Reconcile activities to ensure transactions are recorded with the proper books and entry coding
Assist the management team in developing metrics that demonstrate the progress / completion of retirement inquiry resolution
Basic Requirements:
3-5 years of experience with retirement account operations, IRS rules, and/or tax reporting obligations
Analytical thinker, with excellent written and verbal communication
Detail oriented with strong organizational skills
Ability to manage multiple projects, prioritize tasks, and work against multiple deadlines and objectives
Adaptable to dynamic work environments and priorities
Demonstrate ownership and sense of urgency in resolving inquiries and issues
Proven effectiveness in fast-paced, demanding, client-driven environment.
Ability to perform and deliver within tight deadlines
A challenging and inquiring mind
About ASK:
ASK Consulting is an award-winning technology and professional services recruiting firm servicing Fortune 500 organizations nationally. With 5 nationwide offices, two global delivery centers, and employees in 42 states-ASK Consulting connects people with amazing opportunities
ASK Consulting is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all associates.
Show more
Show less","Data Analysis, Problem Resolution, Retirement Account Operations, IRS Rules, Tax Reporting Obligations, Analytical Thinking, Written Communication, Verbal Communication, Detail Orientation, Organizational Skills, Project Management, Prioritization, Deadline Management, Adaptability, Ownership, Urgency, FastPaced Environment, ClientDriven Environment, Time Management, Challenging Mindset, Inquiring Mind","data analysis, problem resolution, retirement account operations, irs rules, tax reporting obligations, analytical thinking, written communication, verbal communication, detail orientation, organizational skills, project management, prioritization, deadline management, adaptability, ownership, urgency, fastpaced environment, clientdriven environment, time management, challenging mindset, inquiring mind","adaptability, analytical thinking, challenging mindset, clientdriven environment, dataanalytics, deadline management, detail orientation, fastpaced environment, inquiring mind, irs rules, organizational skills, ownership, prioritization, problem resolution, project management, retirement account operations, tax reporting obligations, time management, urgency, verbal communication, written communication"
Product Data Analyst,Cricut,"South Jordan, UT",https://www.linkedin.com/jobs/view/product-data-analyst-at-cricut-3785570265,2023-12-17,Utah,United States,Mid senior,Onsite,"Company Description
Cricut® makes smart cutting machines that work with an easy-to-use app, an ever-growing collection of materials, and crafting essentials to help you design and personalize almost anything — custom cards, unique apparel, everyday items, and so much more.
Overview
We believe everyone is born creative. We’re a diverse tapestry of thinkers, dreamers, givers, DIYers, handi-workers, artisans, and forever and always architects of things.
At Cricut, we place the power of handmade into the hands of all. We give you beautiful, easy-to-master tools so you can make something unique, remarkable, perfect. We surround you with ideas, community, inspiration, and encouragement to take your creativity further than you ever imagined. And as a community, we celebrate the exhilarating act of making every single day.
So, make that handcrafted card that feels like a hug. Design a shirt for fun, for family, or for a full-blown business. Craft with a passion or for a purpose. Make something big and bold, itsy-bitsy, amazingly ambitious, or just plain silly. Whatever you make, just make your heart out. Because here’s the remarkable truth: When we all make together, we make all things possible.
Let’s make.
Job Description
The Cricut Analytics Team is looking to hire an outstanding Data Analyst with strong technical analytical skills to operationalize data intelligence in multiple business units. This includes analyzing input data sources, doing statistical analysis and building data science models to drive data driven decision making. This person will collaborate very closely and cross-functionally with Product Marketing, Ecommerce, Product Management, Operations, Finance, and other teams, as well as the Analytics team, to ensure success.
Data team
We are looking for highly talented team members who are passionate about data, have the rigor needed to solve billion-dollar problems, and possess an innate entrepreneurial spirit to explore the uncharted. This team combines the engineering backbone of a best-in-class big data platform with the analytic expertise of advanced mining and predictive modeling. If you want to work among the very best talent in the industry, working on the most innovative products in the world, Cricut is the place to be.
As a Product Analyst, You’ll
30% - Actionable Insights: Develop strategies and provide direction for the application of our BI/Dashboarding tool, and build robust KPI's and drill-down capabilities to enable discovery of actionable insights.
20% - Build Data Science Products: Collaborate with data science, data engineering and business team to build innovative data solutions using Machine learning techniques.
20% - Ad-hoc analysis & reporting: Create analysis/reports to do diagnostic analysis and provide recommendation to external teams on questions related to their business unit’s datasets.
10% - Data Owners: The person in this role will be the data champion for a specific business function, and the single point-of-contact for internal & external teams on questions related to their business unit’s datasets.
10% - Domain Expert: It is essential that this role has the capability to understand business function and its nuances, and define proper KPIs to measure health & success of the business.
10% - Measurement & A/B testing: Build relevant KPIs to monitor the effectiveness of Data Science models. Create a multivariate testing and machine learning culture to unlock value and drive continuous improvement.
Qualifications
Background:
Bachelor degree in a quantitative field (e.g., Mathematics, Operations Research, Statistics, Business Analytics, Economics)
Graduate degree in Mathematics, Information Systems, Statistics, Business Analytics or related STEM fields preferred
3 or more years of professional experience in customer analytics, marketing analytics, operational analytics or related field in a quantitative role
Direct experience with using customer data in e-commerce, consumer goods, technology would be preferred
Multiple years of experience manipulating and analyzing large data sets, monitoring and predicting trends and identifying opportunities through the interpretation of financial, behavioral, demographic and operational data
Possess excellent interpersonal communication and problem-solving skills, with the ability to collaborate with senior business leaders and provide strategic recommendations
Demonstrated ability working with multiple teams to complete critical achievements under pressure with tight deadlines
Ability to interpret/translate business requests into clear and actionable technical requirements for meaningful analysis
Skills
Succinct, proactive communication; storytelling using data
Comfortable to work with people from a broad range of backgrounds from highly technical to marketing to sales/business
Very strong analytical skills
Strong in SQL Skills (PostgreSQL, Redshift, MySQL, etc)
Experience building reports & dashboard in BI tools like Domo, Tableau, Looker, Grow, PowerBI, etc
Understanding of machine learning algorithms
Experience modeling with R or Python
Mindset
Self-starter mentality with the ability to deliver results
Focused on results and their implications (rather than methods/tools/process)
Openness to work with different cultures and working styles
Additional Information
What to Do Next:
Please attach your resume including links to your portfolio where applicable. You MUST submit a portflio as well. You can be sure that Cricut® is an employer who values individuality, equality and diversity, so tell us what you’re all about. If you are a Maker or a DIY enthusiast, whether you think you are a good one or not, we would love to hear about it when you send us your information!
At Cricut®, we celebrate inclusion and diversity. Cricut is an equal opportunity employer and makes employment decisions based on merit. Cricut prohibits discrimination based on race, color, religion, sex, sexual identity, gender identity, marital status, veteran status, nationality, citizenship, age, disability, medical condition, pregnancy, or any other unlawful consideration. All your information will be kept confidential according to EEO guidelines. Cricut participates in E-Verify.
Show more
Show less","Data Analytics, Business Intelligence, Data Mining, Predictive Modeling, Machine Learning, SQL, PostgreSQL, Redshift, MySQL, Domo, Tableau, Looker, Grow, PowerBI, R, Python","data analytics, business intelligence, data mining, predictive modeling, machine learning, sql, postgresql, redshift, mysql, domo, tableau, looker, grow, powerbi, r, python","business intelligence, data mining, dataanalytics, domo, grow, looker, machine learning, mysql, postgresql, powerbi, predictive modeling, python, r, redshift, sql, tableau"
Senior Data Engineer,BambooHR,"Lindon, UT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-bamboohr-3760292962,2023-12-17,Utah,United States,Mid senior,Remote,"About Us
Our mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.
What You'll Do
As a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.
Your initial areas of focus will include:
Collaborate with stakeholders to make effective use of core data assets
With Spark and Pyspark libraries, load both streaming and batched data
Engineer lakehouse models to support defined data patterns and use cases
Leverage a combination of tools, engines, libraries, and code to build scalable data pipelines
Work within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environments
Documentation of data pipelines, cloud infrastructure, and standard operating procedures
Express data platform cloud infrastructure, services, and configuration as code
Automate load, scaling, and performance testing of data platform pipelines and infrastructure
Monitor, operate, and optimize data pipelines and distributed applications
Help ensure appropriate data privacy and security
Automate continuous upgrades and testing of data platform infrastructure and services
Build data pipeline unit, integration, quality, and performance tests
Participate in peer code reviews, code approvals, and pull requests
Identify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!)
Experience developing, scaling, and tuning data pipelines in Spark with PySpark
Understanding of data lake, lakehouse, and data warehouse systems, and related technologies
Knowledge and understanding of data formats, data patterns, models, and methodologies
Experience storing data objects in hadoop or hadoop like environments such as S3
Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark
Experience working with streaming technologies such as Kafka and Kinesis
Experience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or Greenplum
Ability to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets manager
Understanding of security around cloud infrastructure and data systems
Git-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!)
Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and Delta
Experience with Flink, Presto, Dremio, Databricks, or Kubernetes
Experience with expressing infrastructure as code leveraging tools like Terraform
Experience and understanding of a zero trust security framework
Experience developing CI/CD pipelines for automated testing and code deployment
Experience with QA and test automation
Exposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are:
Clear communicators with team members and stakeholders
Analytical and perceptive of patterns
Creative in coding
Detail-oriented and persistent
Productive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.
Schedule
9AM-5PM MST, Monday-Friday
Work Environment
Remote
What You'll Love About Us
Great Company Culture. We've been recognized by multiple organizations like Inc, Salt Lake Tribune, Glassdoor, & Comparably for our great workplace culture
Make an Impact. We care about your individuality by giving you freedom to grow and create within the company, regardless of your position
Rest and Relaxation. 4 weeks paid time off, 11 paid holidays, and we pay you to go on vacation (ask us about this)!
Health Benefits. Medical with HSA and FSA options, dental, and vision
Prepare for the Future. 401(k) with a generous company match, access to a personal financial planner, and both legal and life insurance
Financial Peace University. We pay for a one year subscription and you walk away with financial savvy and a bonus
Give back. Get paid to give your time to the community: ask us about this!
Educational Benefits. Whether you are a previous student, or currently enrolled in higher education, we can help cover some of those expenses
Flexible Work Models. In-office, work-from-home, or hybrid, depending on position and location
An Equal Opportunity Employer--M/F/D/V
Because our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.
For information on our Privacy Policy, click here.
Show more
Show less","Data Engineering, Data Warehousing, Data Lakes, Data Pipelines, Data Security, Cloud Computing, AWS, Spark, PySpark, Hadoop, EMR, Kafka, Kinesis, Snowflake, Databricks, Redshift, Vertica, Greenplum, S3, RDS, IAM, Security Groups, AMIs, Cloudwatch, Cloudtrail, Secrets Manager, Terraform, Tableau, Git, CI/CD, QA, Test Automation","data engineering, data warehousing, data lakes, data pipelines, data security, cloud computing, aws, spark, pyspark, hadoop, emr, kafka, kinesis, snowflake, databricks, redshift, vertica, greenplum, s3, rds, iam, security groups, amis, cloudwatch, cloudtrail, secrets manager, terraform, tableau, git, cicd, qa, test automation","amis, aws, cicd, cloud computing, cloudtrail, cloudwatch, data engineering, data lakes, data security, databricks, datapipeline, datawarehouse, emr, git, greenplum, hadoop, iam, kafka, kinesis, qa, rds, redshift, s3, secrets manager, security groups, snowflake, spark, tableau, terraform, test automation, vertica"
"Sr. Engineer, Database Infrastructure - Slack",Slack,"Utah, United States",https://www.linkedin.com/jobs/view/sr-engineer-database-infrastructure-slack-at-slack-3760629722,2023-12-17,Utah,United States,Mid senior,Remote,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Software Engineering
Job Details
About Salesforce
We’re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good – you’ve come to the right place.
Slack's DataStores team builds and operates the database platform powering Slack. We write software to manage thousands of stateful hosts, providing several petabytes of online database capacity. We are building one of the fastest-growing database platforms in the world. Our MySQL databases run in Vitess. You can read more about our migration to Vitess at: Scaling Datastores at Slack with Vitess
Background
Slack enables people all over the world to communicate and collaborate together. Teams of all scales — from the world’s largest public companies to the smallest of startups — use Slack to get work done, so we take performance and reliability very seriously. A taste of our scale:
The average user spends over 10 hours connected and 2.5 hours active in Slack every single day
1.5 billion+ messages are sent per month, half of those outside the United States
Every day we see over 10M+ daily active users, over a billion web requests, and 10s of billions of database queries.
For millions of people, Slack is the primary communication tool they use at work all day long. They expect it to be exceptionally reliable and fast, all the time.
Infrastructure at Slack
We operate at tremendous scale with systems that process millions of events per second. Teams in our group maintain and build the lower levels of our stack, including:
Edge services
Data Stores and Caches
Real-time messaging
Asynchronous background job processing
We know we’ve done our job correctly when
none of our users think about us.
We don’t typically ship new user-facing features, but rather ensure our systems are incredibly performant, highly available, reliable, and scalable. In other words, Slack just works seamlessly.
Slack's API and web backend is built on PHP/Hack, our backend services are written in Java and Go, and we use Vitess as our storage engine. Our architecture is constantly evolving to handle millions more users. You can read about how we scaled our datastores with Vitess, how we respond to incidents, and much more on our blog.
What you will be doing:
Operate and enhance our large, highly-available database infrastructure, utilizing technologies such as MySQL and Vitess.
Develop tools to enable self-service and self-managing capabilities of our database infrastructure so that other teams can operate full-stack while rapidly building new features for our customers.
Collaborate with engineering teams on their database storage needs, and advise them throughout the development lifecycle.
Write code to capture database performance, and create tools and dashboards to provide actionable insight into that data.
Participate in our on-call rotation and collaborate with our operations team to triage and resolve production issues.
Support FedRAMP and DoD SRG activities.
What you should have:
Have been working in data storage, core infrastructure, or distributed system-owning teams with increasing responsibilities for 5+ years.
Have professional experience using Go, PHP/Hacklang, Python, Ruby, or Java.
Write code that can be easily understood by others with an eye towards clarity and maintainability. Collaborate with other teams to integrate new features of your platform or adopt self-service features.
Operated at least one distributed system, at scale and in a team environment. Some examples include: a relational database like MySQL/Postgres, or systems like Kafka, Cassandra, or ElasticSearch.
Deployed server software on Linux, and then operated it at scale. You’ve debugged its problems, and analyzed and optimized its performance.
Have experience operating cloud infrastructure, especially AWS.
Are familiar with deployment automation/configuration management tools like Chef, Ansible, Puppet, or Terraform.
Are a very strong communicator. You’re excited to explain complex technical concepts and share your knowledge with different audiences.
Are curious how things work; when they break you are eager and able to help fix them.
Infrastructure is a diverse and inclusive team that treats their colleagues exceptionally well. We are happy to help you learn what you need to know; we encourage and support each other’s growth and thus it’s not expected that you would have expertise across all of these areas.
Come join us!
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com .
Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce .
﻿Salesforce welcomes all.
For Colorado-based roles, the base salary hiring range for this position is $156,800 to $215,600.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.
Show more
Show less","AWS, Terraform, MySQL, ElasticSearch, Kafka, Cassandra, Puppet, Go, Java, PHP, Hack, Ruby, Python, Linux, Chef, Ansible, Vitess","aws, terraform, mysql, elasticsearch, kafka, cassandra, puppet, go, java, php, hack, ruby, python, linux, chef, ansible, vitess","ansible, aws, cassandra, chef, elasticsearch, go, hack, java, kafka, linux, mysql, php, puppet, python, ruby, terraform, vitess"
Staff Cybersecurity Data Platform Engineer,Adobe,"Utah, United States",https://www.linkedin.com/jobs/view/staff-cybersecurity-data-platform-engineer-at-adobe-3767918973,2023-12-17,Utah,United States,Mid senior,Remote,"Our Company
Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.
We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!
As a Staff Cybersecurity Data Platform Engineer at Adobe, you will be joining a team responsible for enhancing the organization's security data platform. Our focus is on contributing to the organization by emphasizing real-world security and embracing automation, as well as optimizing data flow and consumption of the data to multiple teams within Security.
The ideal candidate will help develop security-focused data platform needs and the scaling of a data lake spanning petabytes of data. The job involves system architecture, design, hands-on development, optimization, setting expectations and SLAs with a focus on reliability, availability, and performance.
They must be self-directed and comfortable supporting the data needs of multiple security teams. The right candidate will be excited by the prospect of optimizing our company’s data platform to support our next generation of security products and data initiatives.
Key Responsibilities:
Collaborate with partners, cybersecurity engineers, and operations teams across the security organization to drive the development of enterprise-scale security solutions.
Work with enterprise architects team to ensure alignment with strategic objectives, and leverage design principles while providing input into program direction.
Translate business needs into technical requirements, capabilities, and formulation of solutions, while identifying risks, dependencies, financial impacts, and the risk profile in the technical solution.
Take a hands-on approach to driving proof-of-concept, design, and implementation activities from an architectural perspective.
Develop and maintain solution architecture documents and other artifacts to guide the planning, design, and implementation of the proposed solution.
Required Skills to be Successful:
7 years of experience in designing and building solutions in Databricks, across multiple clouds, using orchestration, data ingestion, medallion architectures, unity catalog, autoloader jobs, and Delta Lakehouse concepts.
10 years of experience working as either: Software Engineer/Data Engineer: query tuning, performance tuning, troubleshooting, and implementing/debugging Spark/PySpark and/or other big data solutions.
10 years of proficiency in developing or architecting modern distributed cloud architectures using AWS tools and technology.
Design end-to-end robust, scalable, real-time data streaming (Kafka / Flink) and data platform architecture that will support the analytical and reporting needs of the entire organization.
Offer technical expertise by collaborating with analysts and business users to translate diverse and intricate functional specifications into technical designs.
Build data models and improve standard schemas across different data sources and normalize data.
Significant experience in security, encompassing threat management, incident response, and enterprise security.
Strong understanding of Security Operations Center (SOC) operations and security management workflows within large organizations.
Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $146,400 -- $275,000 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.
At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).
In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.
Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.
Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.
Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.
Show more
Show less","Databricks, AWS, Kafka, Flink, Spark, PySpark, Unity Catalog, Autoloader Jobs, Delta Lakehouse, Data Modeling, Threat Management, Incident Response, Enterprise Security, Security Operations Center (SOC), Data Lake, Cloud Architectures","databricks, aws, kafka, flink, spark, pyspark, unity catalog, autoloader jobs, delta lakehouse, data modeling, threat management, incident response, enterprise security, security operations center soc, data lake, cloud architectures","autoloader jobs, aws, cloud architectures, data lake, databricks, datamodeling, delta lakehouse, enterprise security, flink, incident response, kafka, security operations center soc, spark, threat management, unity catalog"
Master Data Management (MDM) Production Support Engineer (Remote),Zions Bancorporation,"Midvale, UT",https://www.linkedin.com/jobs/view/master-data-management-mdm-production-support-engineer-remote-at-zions-bancorporation-3762797593,2023-12-17,Utah,United States,Mid senior,Remote,"Zions Bancorporation’s Enterprise Technology and Operations (ETO) team is transforming what it means to work for a financial institution. With a commitment to technology and innovation, we have been providing our community, clients and colleagues the best experience possible for over 150 years. Help us transform our workforce of the future, today.
Zions Bancorporation is currently seeking an experienced
Master Data Management (MDM) Production Support Engineer
. The successful candidate for this position will be responsible for diagnosing and resolving production issues with our MDM implementation, and contributing to the analysis, design, development and implementation of fixes and enhancements to increase the reliability and maturity of our system as we adopt more strategic technologies. Below are other duties of the
MDM Production Support Engineer.
Work on a cross-functional agile SCRUM team to deliver and support master data management capabilities to the organization.
Take ownership of production issues and see them through to resolution within specified SLA timeframes, engaging and leading additional subject matter experts as needed.
Conduct root cause analysis with appropriate subject matter experts to identify and address root causes to production issues.
Provide after-hours on-call production support, as needed.
Contribute to production support knowledgebase and other related documentation.
Support best practices and enterprise standards to satisfy compliance, reduce risk, and deliver a positive experience to internal users and end customers.
Participate in design and code reviews.
Work with the business partners to identify and ensure that all service level agreements are met.
Perform ongoing monitoring of the environment and applications for capacity planning, performance tuning and improvement opportunities.
Work with team members and the Development Manager on process improvement, team initiatives, and the continual growth of the CDI Production Support Team.
May assist training other engineers.
Other duties as assigned.
Qualifications
6+ years of experience supporting, designing and developing applications using IBM Master Data Management (MDM) Advanced Edition (Physical), with a sound understanding of master data management concepts.
Advanced analytical, organizational and problem-solving skills, including experience with root cause analysis methods (5 Whys, Cause Mapping / Fishbone Diagrams, etc.).
Sound understanding of data modeling, data quality and data profiling.
Experience with architecture, design and development of data integration solutions.
Working knowledge of Data-as-a-Service (DaaS) and API management concepts and how to use these with different types of integration technologies. Experience with REST APIs, JSON, SOAP Web Services, XML, XSD, WSDL, Python, and Kafka.
Familiar with Unix/AIX, ANSI SQL, PL/SQL, DB2 SQL and Shell Scripting. A combination of education and experience may meet requirements.
Ability and desire to learn new technologies quickly.
Ability to work independently and collaborate with others at all levels of technical understanding.
Requires a Bachelor's degree in Computer Science, Computer Engineering, Information Systems or related field.
Location:
This position can be located 100% remote within the United States or will be a hybrid work from home schedule with a minimum of three days per week in the office if you are within 50 miles of the new Zions Technology Center in Midvale, UT.
Pay Range:
$110,000-$150,000
Benefits:
Medical, Dental and Vision Insurance - START DAY ONE!
Life and Disability Insurance, Paid Parental Leave and Adoption Assistance
Health Savings (HSA), Flexible Spending (FSA) and dependent care accounts
Paid Training, Paid Time Off (PTO) and 11 Paid Federal Holidays
401(k) plan with company match, Profit Sharing, competitive compensation in line with work experience
Mental health benefits including coaching and therapy sessions
Tuition Reimbursement for qualifying employees
Employee Ambassador preferred banking products
This position may be eligible for a discretionary bonus
Apply now if you have a passion for impactful outcomes, enjoy working collaboratively with co-workers, and want to make a difference for the clients and communities we serve.
Show more
Show less","IBM Master Data Management (MDM) Advanced Edition, Physical, Master data management, Root cause analysis, Data modeling, Data quality, Data profiling, Data integration, DataasaService (DaaS), API management, REST APIs, JSON, SOAP Web Services, XML, XSD, WSDL, Python, Kafka, Unix/AIX, ANSI SQL, PL/SQL, DB2 SQL, Shell Scripting, Computer Science, Computer Engineering, Information Systems","ibm master data management mdm advanced edition, physical, master data management, root cause analysis, data modeling, data quality, data profiling, data integration, dataasaservice daas, api management, rest apis, json, soap web services, xml, xsd, wsdl, python, kafka, unixaix, ansi sql, plsql, db2 sql, shell scripting, computer science, computer engineering, information systems","ansi sql, api management, computer engineering, computer science, data integration, data profiling, data quality, dataasaservice daas, datamodeling, db2 sql, ibm master data management mdm advanced edition, information systems, json, kafka, master data management, physical, plsql, python, rest apis, root cause analysis, shell scripting, soap web services, unixaix, wsdl, xml, xsd"
Master Data Management (MDM) Production Support Engineer (Remote),Zions Bancorporation,"Midvale, UT",https://www.linkedin.com/jobs/view/master-data-management-mdm-production-support-engineer-remote-at-zions-bancorporation-3762799077,2023-12-17,Utah,United States,Mid senior,Remote,"Zions Bancorporation’s Enterprise Technology and Operations (ETO) team is transforming what it means to work for a financial institution. With a commitment to technology and innovation, we have been providing our community, clients and colleagues the best experience possible for over 150 years. Help us transform our workforce of the future, today.
Zions Bancorporation is currently seeking an experienced
Master Data Management (MDM) Production Support Engineer
. The successful candidate for this position will be responsible for diagnosing and resolving production issues with our MDM implementation, and contributing to the analysis, design, development and implementation of fixes and enhancements to increase the reliability and maturity of our system as we adopt more strategic technologies. Below are other duties of the
MDM Production Support Engineer.
Work on a cross-functional agile SCRUM team to deliver and support master data management capabilities to the organization.
Take ownership of production issues and see them through to resolution within specified SLA timeframes, engaging and leading additional subject matter experts as needed.
Conduct root cause analysis with appropriate subject matter experts to identify and address root causes to production issues.
Provide after-hours on-call production support, as needed.
Contribute to production support knowledgebase and other related documentation.
Support best practices and enterprise standards to satisfy compliance, reduce risk, and deliver a positive experience to internal users and end customers.
Participate in design and code reviews.
Work with the business partners to identify and ensure that all service level agreements are met.
Perform ongoing monitoring of the environment and applications for capacity planning, performance tuning and improvement opportunities.
Work with team members and the Development Manager on process improvement, team initiatives, and the continual growth of the CDI Production Support Team.
May assist training other engineers.
Other duties as assigned.
Qualifications
6+ years of experience supporting, designing and developing applications using IBM Master Data Management (MDM) Advanced Edition (Physical), with a sound understanding of master data management concepts.
Advanced analytical, organizational and problem-solving skills, including experience with root cause analysis methods (5 Whys, Cause Mapping / Fishbone Diagrams, etc.).
Sound understanding of data modeling, data quality and data profiling.
Experience with architecture, design and development of data integration solutions.
Working knowledge of Data-as-a-Service (DaaS) and API management concepts and how to use these with different types of integration technologies. Experience with REST APIs, JSON, SOAP Web Services, XML, XSD, WSDL, Python, and Kafka.
Familiar with Unix/AIX, ANSI SQL, PL/SQL, DB2 SQL and Shell Scripting. A combination of education and experience may meet requirements.
Ability and desire to learn new technologies quickly.
Ability to work independently and collaborate with others at all levels of technical understanding.
Requires a Bachelor's degree in Computer Science, Computer Engineering, Information Systems or related field.
Location:
This position can be located 100% remote within the United States or will be a hybrid work from home schedule with a minimum of three days per week in the office if you are within 50 miles of the new Zions Technology Center in Midvale, UT.
Pay Range:
$110,000-$150,000
Benefits:
Medical, Dental and Vision Insurance - START DAY ONE!
Life and Disability Insurance, Paid Parental Leave and Adoption Assistance
Health Savings (HSA), Flexible Spending (FSA) and dependent care accounts
Paid Training, Paid Time Off (PTO) and 11 Paid Federal Holidays
401(k) plan with company match, Profit Sharing, competitive compensation in line with work experience
Mental health benefits including coaching and therapy sessions
Tuition Reimbursement for qualifying employees
Employee Ambassador preferred banking products
This position may be eligible for a discretionary bonus
Apply now if you have a passion for impactful outcomes, enjoy working collaboratively with co-workers, and want to make a difference for the clients and communities we serve.
Show more
Show less","Master Data Management (MDM), Data Integration, Data Modeling, Data Profiling, Data Quality, IBM Master Data Management (MDM) Advanced Edition (Physical), Unix/AIX, ANSI SQL, PL/SQL, DB2 SQL, Shell Scripting, REST APIs, JSON, SOAP Web Services, XML, XSD, WSDL, Python, Kafka, DataasaService (DaaS), API management","master data management mdm, data integration, data modeling, data profiling, data quality, ibm master data management mdm advanced edition physical, unixaix, ansi sql, plsql, db2 sql, shell scripting, rest apis, json, soap web services, xml, xsd, wsdl, python, kafka, dataasaservice daas, api management","ansi sql, api management, data integration, data profiling, data quality, dataasaservice daas, datamodeling, db2 sql, ibm master data management mdm advanced edition physical, json, kafka, master data management mdm, plsql, python, rest apis, shell scripting, soap web services, unixaix, wsdl, xml, xsd"
"DATA - SQL Data Engineer - 2 - Salt Lake City, UT",Addison Group,"Draper, UT",https://www.linkedin.com/jobs/view/data-sql-data-engineer-2-salt-lake-city-ut-at-addison-group-3598749520,2023-12-17,Utah,United States,Mid senior,Hybrid,"Title: Data Engineer - SQL, Azure, PowerBI
Location: Draper, UT - Onsite 3 days weekly / Remote 2 days
Salary: $90-110K
No sponsorship offered - Relocation available
The Data Engineer will ensure that data pipelines and data objects are high-performing, efficient, organized, and reliable.
Responsibilities
Build and maintain secure, compliant data processing pipelines, tables, views, procedures and datasets
Integrate, transform, and consolidate data from various structured and unstructured data systems into structures that are suitable for building analytics solutions and/or performing analysis
Ensure that data pipelines tables, views, procedures, and datasets are high-performing, efficient, organized, and reliable
Design, develop, operate and tune data & analytics services (various)
Monitor the data pipelines and data stores responding to issues in a timely manner
Monitor, troubleshoot and resolve issues in production environments
Help business users and analysts understand what data are available and how to interpret them.
Coordinate with BI team to ensure the data solution integrates with solution.
Attend regular meetings with and participating in BI data warehouse design and development activities and adapting solution to be compatible.
Collaborate with the COE data warehouse team, third-party consultants and third-party data/service providers during the development and maintenance of the data warehouse.
Requirements
Bachelor’s degree in Computer Science or 1-5 years of directly-related work experience
1-4+ years in designing, developing, operating and maintaining various data pipelines and data stores following best practices for the given environment
1-4+ years of strong SQL scripting knowledge
1-4+ years working with data processing languages, such as SQL, Scala or Python (or Similar)
Power BI or other similar reporting tool(s)
Knowledge of NoSQL databases
Experience with parallel processing and data architecture patterns
Experience with relational and dimensional data modelling
Good team player
Nice To Have
Cloud-based Data Solutions like Azure SQL Database/Azure Synapse Analytics/Snowflake
Experience with ETL tools like Azure Data Factory, DBT, SSIS (or similar)
Show more
Show less","SQL, Azure, Power BI, ETL, Data Factory, DBT, SSIS, Azure SQL Database, Azure Synapse Analytics, Snowflake, Scala, Python, NoSQL, Parallel processing, Data architecture, Relational data modelling, Dimensional data modelling","sql, azure, power bi, etl, data factory, dbt, ssis, azure sql database, azure synapse analytics, snowflake, scala, python, nosql, parallel processing, data architecture, relational data modelling, dimensional data modelling","azure, azure sql database, azure synapse analytics, data architecture, data factory, dbt, dimensional data modelling, etl, nosql, parallel processing, powerbi, python, relational data modelling, scala, snowflake, sql, ssis"
Senior Database Engineer,Mastercard,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/senior-database-engineer-at-mastercard-3785145072,2023-12-17,Utah,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Senior Database Engineer
The database engineering team is looking for a Senior Database Engineer to help set direction for MasterCard’s use and best practices for our application database solutions. You will work with application teams to implement sound RDMS data solutions that follow industry best practices and methods.
You will also be responsible for migrating existing databases to AWS. You will design, automate, and manage databases as a service leveraging deep experience in Open-Source database technologies like MySQL, Aurora, and other Cloud Native database technologies.
Role
Optimize and maintain legacy systems in MySQL & MongoDB
Implement differing approaches to data management & administration in AWS
Partner with dev teams to create DevSecOps constructed databases in AWS
Develop plans to migrate legacy on-premises databases into AWS data stores
Engineer various database services across multiple AWS services and regions
Be THE escalation point for solving database usage issues and malfunction
All About You
Deep experience with database engineering
Experience sharding, clustering, HA, scaling, monitoring, deploying, tuning, etc.
Experience being the escalation point for any and all DBE/DBA issues
Sr. experience in cloud databases in production, development, and QA environments
Work Conditions
Rotational pager for Production Support
Tech Stack
AWS, Aurora MySQL, MongoDB
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Show more
Show less","MySQL, Aurora, MongoDB, AWS, DevSecOps, Cloud Databases, Rotational Pager, Sharding, Clustering, HA, Scaling, Monitoring, Deploying, Tuning","mysql, aurora, mongodb, aws, devsecops, cloud databases, rotational pager, sharding, clustering, ha, scaling, monitoring, deploying, tuning","aurora, aws, cloud databases, clustering, deploying, devsecops, ha, mongodb, monitoring, mysql, rotational pager, scaling, sharding, tuning"
Senior Cloud Data Engineer,BDO USA,"Salt Lake City, UT",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765467848,2023-12-17,Utah,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Python, Java, C#, Scala, SQL, Data Warehousing, Data Modeling, R, Power BI, Azure, AWS, Cloud Analytics, ETL, Machine Learning, Data Visualization, Data Analytics, DevOps, DataOps, MLOps, Data Governance, Data Quality, Data Integration, Data Engineering, Data Science, Big Data, Data Lake, Data Pipelines","python, java, c, scala, sql, data warehousing, data modeling, r, power bi, azure, aws, cloud analytics, etl, machine learning, data visualization, data analytics, devops, dataops, mlops, data governance, data quality, data integration, data engineering, data science, big data, data lake, data pipelines","aws, azure, big data, c, cloud analytics, data engineering, data governance, data integration, data lake, data quality, data science, dataanalytics, datamodeling, dataops, datapipeline, datawarehouse, devops, etl, java, machine learning, mlops, powerbi, python, r, scala, sql, visualization"
"Staff Software Engineer, Investment Data",MX,"Lehi, UT",https://www.linkedin.com/jobs/view/staff-software-engineer-investment-data-at-mx-3726022962,2023-12-17,Utah,United States,Mid senior,Hybrid,"Life at MX
We are driven by our moral imperative to advance mankind - and it all starts with our people, product and purpose. We always carry a deep sense of drive and passion with us. If you thrive in a challenging work environment, surrounded by incredible team members who will help you grow, MX is the right place for you.
Come build with us and be part of an award-winning company that’s helping create meaningful and lasting change in the financial industry.
Our software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with their finances. Our products need to handle data at a massive scale. You’ll work alongside the best and the brightest engineering talent in the industry. We have opportunities in a wide range of areas including development, design, search, platform, test, quality, big data, front end and back end. As a core participant of your team, you’ll estimate engineering efforts, design your changes, implement and test your changes, push to live, and triage production issues. You need to be dynamic, collaborative, and curious as we build new experiences, improve existing products, and develop distributed systems powering the world to be financially strong.
We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.
With your technical expertise you will manage project priorities, deadlines, and deliverables. You will design, develop, test, deploy, maintain, and enhance software solutions.
Job Duties
Write product or system development code.
Participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies.
Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Enforce clean code and excellent coding practices by conducting thoughtful code reviews.
Help us build and maintain a world-class technology system so we can achieve our mission of making the world financially strong.
Collaborate closely with Product Managers to meet and exceed customer needs in the simplest possible ways.
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Actively participate in system architecture discussions and technical design reviews to ensure scalability, reliability, and security.
You will lead by example, and elevate the design, implementation, quality, and strong engineering practices across the team.
Drive projects and initiatives to implement high quality systems and products.
Coach and support engineers on the team, with a strong focus on feedback and growth.
Facilitate alignment and clarity across teams on goals, outcomes, and timelines
Influence and coach a team of engineers.
Manage project priorities, deadlines, and deliverables.
Basic Requirements
Bachelor’s Degree or equivalent experience.
12+ years of experience with software development in one or more programming languages with data structures or algorithms.
8+ years of experience testing, maintaining, or launching software products, and 1 year of experience with software design and architecture.
Advanced Requirements (preferred But Not Required)
Master’s Degree or PhD in Computer Science or related technical field.
Work Environment
At MX, we utilize a hybrid work model, which allows us to attract top talent, provide work-life balance, and increase productivity through collaboration. Our team members enjoy a balance of remote work and monthly in-person collaboration meetings. Travel expectations are about 15%, and the company covers travel expenses for remote employees. Local employees are encouraged to utilize in-office time on a weekly basis. Both local and remote employees can take advantage of our incredible office space with onside perks like company-paid meals, onsite massage therapist, golf simulator, and meditation room to name a few.
Compensation
The expected earnings for this role could be comprised of a base salary and other forms of cash compensation, such as bonus or commissions as applicable. The base salary is $150,000 to $180,000.
This pay range is just one component of MX's total rewards package. MX takes a number of factors into account when determining individual starting pay, including job and level they are hired into, location, skillset, peer compensation.
#REMOTE
MX is proudly committed to recruiting and retaining a diverse and inclusive workforce. As an Equal Opportunity Employer, we never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, military or veteran status, status as an individual with a disability, or other applicable legally protected characteristics. We particularly welcome applications from veterans and military spouses. All your information will be kept confidential according to EEO guidelines. You may request reasonable accommodations by sending an email to hr@mx.com.
Show more
Show less","Software development, Data structures, Algorithms, Testing, Maintenance, Software architecture, Design reviews, Coding standards, Code reviews, System architecture, Technical design reviews, Product management, Documentation, User feedback, System issues, Debugging, Data analysis, Troubleshooting, Project management, Deadlines, Deliverables, Coaching, Mentoring, Leadership, Teamwork, Collaboration, Communication, Problem solving, Critical thinking, Analytical thinking, Attention to detail, Time management, Stress management, Adaptability, Flexibility, Creativity, Innovation, Initiative, Motivation, Drive, Passion, Work ethic","software development, data structures, algorithms, testing, maintenance, software architecture, design reviews, coding standards, code reviews, system architecture, technical design reviews, product management, documentation, user feedback, system issues, debugging, data analysis, troubleshooting, project management, deadlines, deliverables, coaching, mentoring, leadership, teamwork, collaboration, communication, problem solving, critical thinking, analytical thinking, attention to detail, time management, stress management, adaptability, flexibility, creativity, innovation, initiative, motivation, drive, passion, work ethic","adaptability, algorithms, analytical thinking, attention to detail, coaching, code reviews, coding standards, collaboration, communication, creativity, critical thinking, data structures, dataanalytics, deadlines, debugging, deliverables, design reviews, documentation, drive, flexibility, initiative, innovation, leadership, maintenance, mentoring, motivation, passion, problem solving, product management, project management, software architecture, software development, stress management, system architecture, system issues, teamwork, technical design reviews, testing, time management, troubleshooting, user feedback, work ethic"
Senior Technical AWS FinOps Data Engineer,NetDocuments,"Lehi, UT",https://www.linkedin.com/jobs/view/senior-technical-aws-finops-data-engineer-at-netdocuments-3787357925,2023-12-17,Utah,United States,Mid senior,Hybrid,"Description
NetDocuments is the world’s #1 trusted cloud-based content management and productivity platform that helps legal professionals do their best work. We strive to win together through passionate hard work, exploring new things and recognizing every interaction matters.
NetDocuments provides rewarding career growth in an inclusive, diverse environment where employees are encouraged to openly contribute creative ideas and innovation, backed by supportive peers and leadership working together to achieve our goals as a unified team.
At Our Core, We Are Dedicated To Empowering Our Employees To Drive Successful Business Outcomes And Better User Experiences For Our Customers And Partners. Our Customer-centric Approach And Employee Enablement Has Allowed Us To Enjoy Many Accolades, Including Being Named Among The 2022 List Of Inc. Magazine’s 5000 Fastest-Growing Private Companies In America. Other Recent Awards Include
2023 National Top Workplaces
Two-time winner (2021, 2022) Top Workplace in the US by the Salt Lake Tribune
Two-time winner (2021, 2022) Utah’s Best Companies to Work for by Utah Business magazine
2022 Employee Appreciation and Employee Well Being by the Salt Lake Tribune
2022 Top Workplace in the US by the Salt Lake Tribune for the Technology Industry
2022 Top Workplace in the US by the Salt Lake Tribune for Compensation & Benefits
2022 Top Workplace in the US by the Salt Lake Tribune for Work-Life Flexibility
2021 Top Workplace in the US by the Salt Lake Tribune for Remote Work
2021 Top Workplace in the US by the Salt Lake Tribune for Top Managers
2021 Top Workplace in the US by the Salt Lake Tribune for Compensation
2021 Coolest Tech Companies to Work for by Dev Mountain
NetDocuments is a hybrid, remote-friendly workplace. Come join our team and work inspired each day!
About The Opportunity
NetDocuments is seeking a highly skilled and detail-oriented Sr Technical AWS FinOps Specialist with a strong technical background to join our team at NetDocuments. The ideal candidate will play a crucial role in managing financial operations, optimizing costs, and ensuring financial efficiency within our AWS cloud environment.
What Your Contributions Will Be
Collaborate with cross-functional teams to analyze and optimize AWS cloud spending using FinOps frameworks, and experience with one or more commercial FinOps tools such as AWS Cost Explorer, Aptio, CloudHealth, Turbonomic, Cloudability, CloudCheckr, or RightScale.
Implement best practices for AWS cloud cost management, including tagging strategies, reserved instances, and spot instances, and provide recommendations for cost savings.
Develop and enforce resource tagging strategies to ensure accurate categorization and tracking of cloud resources.
Utilize scripting languages (e.g., Python) to automate cost analysis and reporting processes specific to AWS.
Monitor and analyze AWS financial data to identify trends and opportunities for improvement.
Work closely with finance and procurement teams to ensure accurate tracking and forecasting of AWS-related expenses and integrate financial data into AWS cloud cost analysis.
Develop and maintain sophisticated financial models using tools like Excel, and specialized FinOps platforms, with a focus on AWS services.
Demonstrate proficiency in a commercial FinOps tool such as Apptio, CloudHealth, Turbonomic, Cloudability, CloudCheckr, and/or RightScale for comprehensive financial management.
Provide insights and recommendations to leadership based on in-depth financial analysis of AWS cloud costs.
Stay updated on industry trends and best practices in AWS cloud cost management and FinOps.
Other duties as assigned
What You Will Bring To The Team
Relationship Builder
Organizational Skills
Communication Proficiency
Collaboration Skills
Problem Analysis
Business Orientation
What You Will Need For Success
Bachelor's degree in Finance, Accounting, Computer Science, or a related field is a plus; all study concentrations are acceptable.
Minimum of 1 year of experience in financial operations or a similar role, with a strong technical focus on AWS with at least $1 million in annual cloud spend.
Strong understanding of AWS cloud computing platforms and their associated cost structures.
Proficient in scripting languages such as Python or willingness to learn.
Demonstrated proficiency in a commercial FinOps tool, ideally Apptio, CloudHealth, Turbonomic, Cloudability, CloudCheckr, and/or RightScale.
Excellent communication and collaboration skills along strong attention to details.
Ideally You Will Have
AWS Certified Cost Management certification and/or Certified Cloud Practitioner (CCP) is a plus.
Experience with AWS-specific FinOps practices and frameworks.
Familiarity with DevOps and CI/CD processes in an AWS environment.
What You Will Receive
90% healthcare premiums company covered
HSA company contribution
401K match at 4% with no vesting period
Potential for twice a year merit increases
Flexible time off typically 3 to 4 weeks a year, not including the 9 paid holidays
Authenticity and accountability from leadership
Connection, access, and mentorship from exceptional leaders
Growing company with opportunities for advancement
NetDocuments is an Equal Opportunity Employer and prohibits discrimination and harassment of any kind. All employment decisions are based on business needs, job requirements, individual qualifications, without regard to race, color, religion, sex, (including pregnancy), national origin, age, physical and mental disability, marital status, sexual orientation, gender identity and/or expression, military and veteran status, or any other status protected by laws or regulations in the locations where we operate. NetDocuments believes diversity and inclusion among our employees is critical to our success, and we are committed to providing a work environment free of discrimination and harassment.
Show more
Show less","Cloud computing, AWS, FinOps, Cost management, Financial analysis, AWS Cost Explorer, Aptio, CloudHealth, Turbonomic, Cloudability, CloudCheckr, RightScale, Tagging strategies, Reserved instances, Spot instances, Python, Excel, Apptio, Communication, Collaboration, Problem analysis, Business orientation, Finance, Accounting, Computer science, AWS Certified Cost Management, Certified Cloud Practitioner, DevOps, CI/CD","cloud computing, aws, finops, cost management, financial analysis, aws cost explorer, aptio, cloudhealth, turbonomic, cloudability, cloudcheckr, rightscale, tagging strategies, reserved instances, spot instances, python, excel, apptio, communication, collaboration, problem analysis, business orientation, finance, accounting, computer science, aws certified cost management, certified cloud practitioner, devops, cicd","accounting, apptio, aptio, aws, aws certified cost management, aws cost explorer, business orientation, certified cloud practitioner, cicd, cloud computing, cloudability, cloudcheckr, cloudhealth, collaboration, communication, computer science, cost management, devops, excel, finance, financial analysis, finops, problem analysis, python, reserved instances, rightscale, spot instances, tagging strategies, turbonomic"
Finance Data and Process Architecture analyst,Arnex Solutions LLC,"Phoenix, AZ",https://www.linkedin.com/jobs/view/finance-data-and-process-architecture-analyst-at-arnex-solutions-llc-3736019750,2023-12-17,Arizona,United States,Associate,Onsite,"Title: Analyst, Finance Data and Process Architecture
Location : Phoenix, Arizona
Duration: 6+ months
Job Description
This position is part of a team that manages the data and business architecture for pan finance solutions within the Finance, Technology & Transformation group. The primary responsibility for this role is to lead the end-to-end data and process architecture and delivery for projects for multiple line of businesses within Finance organization at client. This role will primarily be responsible for determining the proper sources and processes for acquiring, enriching, and provisioning data required by Finance processes in the following major areas: Financial Reporting, Operational Reporting, Regulatory Reporting, Management Reporting, and Advanced Analytics processes.
Qualifications
Bachelor’s degree in Finance, Information Management, Data Management or similar field preferred
Previous work experience in various Finance disciplines such as Accounting, Financial Reporting, Regulatory Reporting, Management Reporting, Treasury, Tax, Banking and others., Data Management, and other data related projects
Strong knowledge of relational database concepts and data management concepts; knowledge of SQL or other query languages.
Basic understanding of Agile methodology and delivery concepts
Candidate must have strong analytical and problem-solving skills as well as strong relationship skills
Previous experience working closely with Technologies to deploy systems to re-engineer processes and solve complex business problems.
Strong written and verbal communication skills
Demonstrated organizational skills with the ability to meet critical deadlines and manage multiple projects simultaneously.
Self-motivated, proactive, and dedicated to the delivery of high-quality service to consistently exceeds customers’ expectations
Show more
Show less","Data Architect, Data Management, Relational Database, Data Management Concepts, SQL, Query Languages, Agile, Advanced Analytics, Financial Reporting, Operational Reporting, Regulatory Reporting, Management Reporting, Accounting, Treasury, Tax, Banking","data architect, data management, relational database, data management concepts, sql, query languages, agile, advanced analytics, financial reporting, operational reporting, regulatory reporting, management reporting, accounting, treasury, tax, banking","accounting, advanced analytics, agile, banking, data architect, data management, data management concepts, financial reporting, management reporting, operational reporting, query languages, regulatory reporting, relational database, sql, tax, treasury"
IT Presales Consultant - Data Center,Comarch,"Phoenix, AZ",https://www.linkedin.com/jobs/view/it-presales-consultant-data-center-at-comarch-3693930720,2023-12-17,Arizona,United States,Associate,Hybrid,"Is your career path related to Data Center, Cloud Solutions and Services? Would you like to have real impact and shape the next horizon of IT infrastructure innovation together with the industry leaders? Find out more about our large scale Comarch
ICT
&
Data Center
Services and our newest Data Center designed to meet the growing demand for high-quality, secure, and reliable private cloud and colocation services in the US. Join us in Arizona!
CANDIDATE PROFILE:
Bachelor’s Degree in Computer Science, Telecommunications or related field
Min.2 years of experience working as Data Center Technical Consultant, Sales Engineer, Solutions Architect or a similar Technical Presales position
Experience with
Cloud Solutions
and
Services
:
Managed Hosting (x86/x64, RISC/
IBM Power
)
Public Cloud (
AWS
,
Azure
,
IBM Cloud
)
Hybrid Cloud
Private Cloud
Technical background and understanding of the IT infrastructure technologies with the focus on IT Systems, Hosting Solutions and Cloud Services
General knowledge of the IT infrastructure market and products (servers, storage and backup technologies, virtualization, database technologies etc.)
Customer relationship sustaining skills, problem problem-solving ability
Results-oriented personality
Strong presentation and analytical skills
YOUR RESPONSIBILITIES:
Identify customer’s needs and requirements to determine solution from Comarch ICT portfolio
Create business offers of Data Center and Cloud Services, IT infrastructure managed services and operations support
Respond to RFPs, RFIs and other forms of requests with commercial proposals of IT infrastructure services
Visioning proposed IT approach and creating business cases for the prospective clients
Prepare and conduct presentations of the offered solutions to the executives
Take an active role in negotiations, providing technical support to the sales team
Cooperate with IT vendors and external service providers to design offered solutions
Attends industry events such as trade shows, conferences
Monitor market trends in the United States
FOR YOU:
Competitive salary
100% paid health insurance
401(k) match
Flexible Spending Account (health, dependent, commuter)
Wellness Reimbursement Program
Exciting work in a rapidly growing department working with leading world brands
Opportunity to learn from leading specialists to develop professional IT career
Hybrid Work Model
ACCOMMODATION STATEMENT:
Comarch Inc is an equal opportunity/affirmative action employer. We consider applicants without regard to race, color, religion, creed, gender, national origin, age, disability, genetic information, marital or veteran status, or any other category protected by federal, state, or local law. Comarch Inc provides reasonable accommodations to hires with disabilities on case-by-case basis.
Show more
Show less","Data Center, Cloud Solutions, Services, Managed Hosting, Public Cloud, Hybrid Cloud, Private Cloud, IT infrastructure technologies, IT Systems, Hosting Solutions, Cloud Services, Servers, Storage, Backup technologies, Virtualization, Database technologies, Customer relationship, Problemsolving, Presentation skills, Analytical skills, RFPs, RFIs, IT infrastructure services, Business cases, Negotiations, IT vendors, External service providers, Conferences, Market trends, Flexible Spending Account, Wellness Reimbursement Program","data center, cloud solutions, services, managed hosting, public cloud, hybrid cloud, private cloud, it infrastructure technologies, it systems, hosting solutions, cloud services, servers, storage, backup technologies, virtualization, database technologies, customer relationship, problemsolving, presentation skills, analytical skills, rfps, rfis, it infrastructure services, business cases, negotiations, it vendors, external service providers, conferences, market trends, flexible spending account, wellness reimbursement program","analytical skills, backup technologies, business cases, cloud services, cloud solutions, conferences, customer relationship, data center, database technologies, external service providers, flexible spending account, hosting solutions, hybrid cloud, it infrastructure services, it infrastructure technologies, it systems, it vendors, managed hosting, market trends, negotiations, presentation skills, private cloud, problemsolving, public cloud, rfis, rfps, servers, services, storage, virtualization, wellness reimbursement program"
"Big Data Developer  Phoenix, AZ ( Onsite Daya 1 )","Conch Technologies, Inc","Phoenix, AZ",https://www.linkedin.com/jobs/view/big-data-developer-phoenix-az-onsite-daya-1-at-conch-technologies-inc-3766009866,2023-12-17,Arizona,United States,Mid senior,Onsite,"HI,
Greetings from Conch Technologies Inc
Job Description
Position: BigData Engineer ( 7 position open )
Location: Phoenix, AZ ( Onsite Daya 1 )
Duration: 12+ Months Contract
Big Data engineers
Minimum Qualifications:
Bachelor's degree in Engineering or Computer Science or equivalent OR Master's in Computer Applications or equivalent.
10+ years of software development experience and leading teams of engineers and scrum teams 5+ years of hands-on experience of working with Map-Reduce, Hive, Spark (core, SQL, and PySpark) Solid Data warehousing concepts
Knowledge of the Financial reporting ecosystem will be a plus .5+ years of experience within Data Engineering/ Data Warehousing using Big Data technologies will be an add-on Expert on Distributed ecosystems Hands-on experience with programming using Core Java or Python/Scala.
Expert on Hadoop and Spark Architecture and its working principle Hands-on experience on writing and understanding complex SQL(Hive/Py Spark-dataframes), optimizing joins while processing huge amounts of data.
Experience In UNIX Shell Scripting.
Ability to design and develop optimized Data pipelines for batch and real time data processing Should have experience in analysis, design, development, testing, and implementation of system applications Demonstrated ability to develop and document technical and functional specifications and analyze software and system processing flows.
--
With Regards,
Pavani P
IT Recruiter
Desk: 901-313-3067 * 404
Email: pavani@conchtech.com
Web: www.conchtech.com
Show more
Show less","Big Data, Hadoop, Spark, MapReduce, Hive, PySpark, SQL, Java, Python, Scala, Data Warehousing, UNIX Shell Scripting, Data pipelines, Data Engineering","big data, hadoop, spark, mapreduce, hive, pyspark, sql, java, python, scala, data warehousing, unix shell scripting, data pipelines, data engineering","big data, data engineering, datapipeline, datawarehouse, hadoop, hive, java, mapreduce, python, scala, spark, sql, unix shell scripting"
Data Engineer,Tata Consultancy Services,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-engineer-at-tata-consultancy-services-3777095093,2023-12-17,Arizona,United States,Mid senior,Onsite,"Role:
Data Engineer
Duration:
Full Time
Location:
Phoenix, AZ
Roles & Responsibilities
Produces and develops detailed MDM design specifications, develops custom mapping functions and customization to the data tier of MDM Hub.
Experience on
Informatica MDM hub components
: Landing tables, base objects, staging tables, lookup tables, relationships, mappings, cleanse function, trust and matching rules, hierarchy management, E360 (provisioning) etc.
Review MDM environment for efficacy, performance improvement (including APIs), and support overall MDM data governance model.
Build customized APIs for data ingestion into MDM.
Troubleshoots issues related to MDM and identifies opportunities for improving data management processes.
Provides strategic guidance on MDM best practices.
Experience in working with IICS/IDQ ETL Cloud technologies.
Unix and shell scripting hands-on experience
Strong SQL and DB knowledge (MySQL, Oracle and SQL Server )
Good understanding of AWS services
Thanks & Regards,
Disha Ranjan
Talent Acquisition Group- North America
Tata Consultancy Services
Malito: r.disha@tcs.com
Website: https://www.tcs.com
Show more
Show less","Data Engineering, MDM Design, Informatica MDM Hub, Data Tier, Mapping Functions, Data Ingestion, APIs, Data Governance, Data Management, ETL, Unix Shell Scripting, SQL, MySQL, Oracle, SQL Server, AWS","data engineering, mdm design, informatica mdm hub, data tier, mapping functions, data ingestion, apis, data governance, data management, etl, unix shell scripting, sql, mysql, oracle, sql server, aws","apis, aws, data engineering, data governance, data ingestion, data management, data tier, etl, informatica mdm hub, mapping functions, mdm design, mysql, oracle, sql, sql server, unix shell scripting"
"Big Data Developer  Phoenix, AZ ( Onsite Daya 1 )","Conch Technologies, Inc","Phoenix, AZ",https://www.linkedin.com/jobs/view/big-data-developer-phoenix-az-onsite-daya-1-at-conch-technologies-inc-3779152878,2023-12-17,Arizona,United States,Mid senior,Onsite,"HI,
Hope you're doing great,
Greetings from Conch Technologies Inc
Job Description
Position: BigData Engineer ( 7 positions open ) 10 years exp is plus
Location: Phoenix, AZ ( Onsite Daya 1 )
Duration: 12+ Months Contract
Big Data engineers
Minimum Qualifications:
Bachelor's degree in Engineering or Computer Science or equivalent OR Master's in Computer Applications or equivalent.
10+ years of software development experience and leading teams of engineers and scrum teams 5+ years of hands-on experience of working with Map-Reduce, Hive, Spark (core, SQL, and PySpark) Solid Data warehousing concepts
Preferred Qualifications: Knowledge of cloud platforms like
GCP/AWS, building Microservices and scalable solutions, will be an advantage 1 + years of experience in designing and building solutions using Kafka streams or queues Experience with GitHub/Bitbucket and leveraging CI/CD pipelines .
Experience with NoSQL i.e., HBase, Couchbase, MongoDB is good to have Excellent technical and analytical aptitude Good communication skills .Excellent Project management skills. Results driven
--
With Regards,
Nagesh G
Mobile:
408-381-5645
Desk:
901-313-3066
Email: nagesh@conchtech.com
Web:
www.conchtech.com
Show more
Show less","Big Data, MapReduce, Hadoop, Hive, Spark, SQL, PySpark, Data warehousing, Cloud platforms, GCP, AWS, Microservices, Kafka, NoSQL, HBase, Couchbase, MongoDB, GitHub, Bitbucket, CI/CD pipelines","big data, mapreduce, hadoop, hive, spark, sql, pyspark, data warehousing, cloud platforms, gcp, aws, microservices, kafka, nosql, hbase, couchbase, mongodb, github, bitbucket, cicd pipelines","aws, big data, bitbucket, cicd pipelines, cloud platforms, couchbase, datawarehouse, gcp, github, hadoop, hbase, hive, kafka, mapreduce, microservices, mongodb, nosql, spark, sql"
100% ONSITE: Bigdata Developer,"Conch Technologies, Inc","Phoenix, AZ",https://www.linkedin.com/jobs/view/100%25-onsite-bigdata-developer-at-conch-technologies-inc-3775715063,2023-12-17,Arizona,United States,Mid senior,Onsite,"HI,
Hope you're doing great,
Greetings from Conch Technologies Inc
Job Description
Position: BigData Engineer ( 7 position open )
Location: Phoenix, AZ ( Onsite Daya 1 )
Duration: 12+ Months Contract
Big Data engineers
Minimum Qualifications:
Bachelor's degree in Engineering or Computer Science or equivalent OR Master's in Computer Applications or equivalent.
10+ years of software development experience and leading teams of engineers and scrum teams 5+ years of hands-on experience of working with Map-Reduce, Hive, Spark (core, SQL, and PySpark) Solid Data warehousing concepts
Knowledge of the Financial reporting ecosystem will be a plus .5+ years of experience within Data Engineering/ Data Warehousing using Big Data technologies will be an add-on Expert on Distributed ecosystems Hands-on experience with programming using Core Java or Python/Scala.
Expert on Hadoop and Spark Architecture and its working principle Hands-on experience on writing and understanding complex SQL(Hive/Py Spark-dataframes), optimizing joins while processing huge amounts of data.
Experience In UNIX Shell Scripting.
Ability to design and develop optimized Data pipelines for batch and real time data processing Should have experience in analysis, design, development, testing, and implementation of system applications Demonstrated ability to develop and document technical and functional specifications and analyze software and system processing flows.
Preferred Qualifications: Knowledge of cloud platforms like
GCP/AWS, building Microservices and scalable solutions, will be an advantage 1 + years of experience in designing and building solutions using Kafka streams or queues Experience with GitHub/Bitbucket and leveraging CI/CD pipelines .Experience with NoSQL i.e., HBase, Couchbase, MongoDB is good to have Excellent technical and analytical aptitude Good communication skills .Excellent Project management skills. Results driven
Thanks and Regards,
Naveen
US IT Recruiter
Conch Technologies Inc,
6750 Poplar Ave # 711, Memphis, TN.
Direct:
901-317-3454
Email:
naveeng@conchtech.com
Show more
Show less","Big Data Engineering, Hadoop, Spark, Hive, SQL, Python, Java, Scala, Core Java, Data Warehousing, Spark Architecture, MapReduce, PySpark, UNIX Shell Scripting, NoSQL, HBase, Couchbase, MongoDB, Kafka, Microservices, GCP, AWS, CI/CD, GitHub, Bitbucket","big data engineering, hadoop, spark, hive, sql, python, java, scala, core java, data warehousing, spark architecture, mapreduce, pyspark, unix shell scripting, nosql, hbase, couchbase, mongodb, kafka, microservices, gcp, aws, cicd, github, bitbucket","aws, big data engineering, bitbucket, cicd, core java, couchbase, datawarehouse, gcp, github, hadoop, hbase, hive, java, kafka, mapreduce, microservices, mongodb, nosql, python, scala, spark, spark architecture, sql, unix shell scripting"
"ONSITE DAY 1:  Bigdata Developer, Phoenix, AZ","Conch Technologies, Inc","Phoenix, AZ",https://www.linkedin.com/jobs/view/onsite-day-1-bigdata-developer-phoenix-az-at-conch-technologies-inc-3766668092,2023-12-17,Arizona,United States,Mid senior,Onsite,"Hope you're doing great,
Greetings from Conch Technologies Inc
Job Description
Position: BigData Engineer ( 7 position open )
Location: Phoenix, AZ ( Onsite Daya 1 )
Duration: 12+ Months Contract
Big Data engineers
Minimum Qualifications:
Bachelor's degree in Engineering or Computer Science or equivalent OR Master's in Computer Applications or equivalent.
10+ years of software development experience and leading teams of engineers and scrum teams 5+ years of hands-on experience of working with Map-Reduce, Hive, Spark (core, SQL, and PySpark) Solid Data warehousing concepts
Knowledge of the Financial reporting ecosystem will be a plus .5+ years of experience within Data Engineering/ Data Warehousing using Big Data technologies will be an add-on Expert on Distributed ecosystems Hands-on experience with programming using Core Java or Python/Scala.
Expert on Hadoop and Spark Architecture and its working principle Hands-on experience on writing and understanding complex SQL(Hive/Py Spark-dataframes), optimizing joins while processing huge amounts of data.
Experience In UNIX Shell Scripting.
Ability to design and develop optimized Data pipelines for batch and real time data processing Should have experience in analysis, design, development, testing, and implementation of system applications Demonstrated ability to develop and document technical and functional specifications and analyze software and system processing flows.
Preferred Qualifications: Knowledge of cloud platforms like
GCP/AWS, building Microservices and scalable solutions, will be an advantage 1 + years of experience in designing and building solutions using Kafka streams or queues Experience with GitHub/Bitbucket and leveraging CI/CD pipelines .Experience with NoSQL i.e., HBase, Couchbase, MongoDB is good to have Excellent technical and analytical aptitude Good communication skills .Excellent Project management skills. Results driven
--
Thanks & Regards
Nayak Teketi
Phone : 901-317-3453
Email:nayak@conchtech.com
Web: www.conchtech.com
Show more
Show less","MapReduce, Hive, PySpark, Hadoop, UNIX Shell Scripting, Java, Python, Scala, SQL, Distributed Systems, Data Warehousing, Data Pipelining, Batch Processing, Realtime Data Processing, System Applications, GCP, AWS, Microservices, Kafka, CI/CD Pipelines, NoSQL, HBase, Couchbase, MongoDB","mapreduce, hive, pyspark, hadoop, unix shell scripting, java, python, scala, sql, distributed systems, data warehousing, data pipelining, batch processing, realtime data processing, system applications, gcp, aws, microservices, kafka, cicd pipelines, nosql, hbase, couchbase, mongodb","aws, batch processing, cicd pipelines, couchbase, datapipeline, datawarehouse, distributed systems, gcp, hadoop, hbase, hive, java, kafka, mapreduce, microservices, mongodb, nosql, python, realtime data processing, scala, spark, sql, system applications, unix shell scripting"
Hiring for Lead Data Engineer,Persistent Systems,"Scottsdale, AZ",https://www.linkedin.com/jobs/view/hiring-for-lead-data-engineer-at-persistent-systems-3784567936,2023-12-17,Arizona,United States,Mid senior,Onsite,"About Persistent
We are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 14 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.
Our disruptor’s mindset, commitment to client success, and agility to thrive in the dynamic environment have enabled us to sustain our growth momentum by reporting $282.9M revenue in Q1FY24, delivering 17.1% Y-o-Y growth. In addition, our total employee count reached 23,130 people this quarter, located in 21 countries across the globe. We’re also pleased to share that Persistent has been named the fastest-growing Indian IT Services brand by Brand Finance. Acknowledging our vertical industry expertise, we were placed as a Leader in Everest Group’s Payments IT Services PEAK Matrix® Assessment 2023. We were also recognized as a Leader in the ISG Provider Lens™ Digital Engineering Services Quadrants U.S. 2023 and the Salesforce Ecosystem Partners 2023 ISG Provider Lens™ Study. Throughout this market-leading growth, we’ve maintained strong employee satisfaction - over 94% of our employees approve of the CEO, and 89% would recommend working at Persistent to a friend.
About Position:
Role: Lead Data Engineer
Location: Scottsdale AZ
Experience: 10+
Job Type: FTE
What You'll Do:
Must have skill set:
AWS, AWS Redshift and infrastructure, AWS Data Lake Formation and Glue components, data security, SQL, and Python
Highly analytical and data oriented.
Experience in SQL, NoSql Database
Data masking of on prem PII data.
Develop API calls with using secure data transfer.
Take standard output data to lower environments for pre prod testing!
Enable secured channels for data models and data science activities.
Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15 mins
You have experience with development tools and agile methodologies
Expertise You'll Bring:
Highly analytical and data oriented.
Experience in SQL, NoSql Database
Data masking of on prem PII data.
Develop API calls with using secure data transfer.
Take standard output data to lower environments for pre prod testing!
Enable secured channels for data models and data science activities.
Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15 mins
You have experience with development tools and agile methodologies.
Benefits:
Competitive salary and benefits package
Culture focused on talent development with quarterly promotion cycles and company-sponsored higher education and certifications.
Opportunity to work with cutting-edge technologies.
Employee engagement initiatives such as project parties, flexible work hours, and Long Service awards
Annual health check-ups
Insurance coverage: group term life, personal accident, and Mediclaim hospitalization for self, spouse, two children, and parents
Our company fosters a values-driven and people-centric work environment that enables our employees to:
· Accelerate growth, both professionally and personally
· Impact the world in powerful, positive ways, using the latest technologies
· Enjoy collaborative innovation, with diversity and work-life wellbeing at the core
· Unlock global opportunities to work and learn with the industry’s best
Let’s unleash your full potential at Persistent - persistent.com/careers
Show more
Show less","AWS, AWS Redshift, AWS Data Lake Formation, AWS Glue, Data security, SQL, Python, NoSQL Database, Data masking, Agile methodologies, API development, Data transfer, Data models, Data science","aws, aws redshift, aws data lake formation, aws glue, data security, sql, python, nosql database, data masking, agile methodologies, api development, data transfer, data models, data science","agile methodologies, api development, aws, aws data lake formation, aws glue, aws redshift, data masking, data models, data science, data security, data transfer, nosql database, python, sql"
Hiring for Lead Data Engineer,Persistent Systems,"Scottsdale, AZ",https://www.linkedin.com/jobs/view/hiring-for-lead-data-engineer-at-persistent-systems-3780714488,2023-12-17,Arizona,United States,Mid senior,Onsite,"About Persistent
We are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 14 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.
Our disruptor’s mindset, commitment to client success, and agility to thrive in the dynamic environment have enabled us to sustain our growth momentum by reporting $282.9M revenue in Q1FY24, delivering 17.1% Y-o-Y growth. In addition, our total employee count reached 23,130 people this quarter, located in 21 countries across the globe. We’re also pleased to share that Persistent has been named the fastest-growing Indian IT Services brand by Brand Finance. Acknowledging our vertical industry expertise, we were placed as a Leader in Everest Group’s Payments IT Services PEAK Matrix® Assessment 2023. We were also recognized as a Leader in the ISG Provider Lens™ Digital Engineering Services Quadrants U.S. 2023 and the Salesforce Ecosystem Partners 2023 ISG Provider Lens™ Study. Throughout this market-leading growth, we’ve maintained strong employee satisfaction - over 94% of our employees approve of the CEO, and 89% would recommend working at Persistent to a friend.
About Position:
Role: Lead Data Engineer & Data Engineer
Location: Scottsdale AZ
Experience: 10+
Job Type: FTE
What You'll Do:
You have hands-on experience in
Java/Scala/Python, Spark, S3, Glue, Redshift
. This is critical
Highly analytical and data oriented.
Experience in SQL, NoSql Database
Data masking of on prem PII data.
Develop API calls with using secure data transfer.
Take standard output data to lower environments for pre prod testing!
Enable secured channels for data models and data science activities.
Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15 mins
You have experience with development tools and agile methodologies
Expertise You'll Bring:
You have hands-on experience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical
Highly analytical and data oriented.
Experience in SQL, NoSql Database
Data masking of on prem PII data.
Develop API calls with using secure data transfer.
Take standard output data to lower environments for pre prod testing!
Enable secured channels for data models and data science activities.
Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15 mins
You have experience with development tools and agile methodologies.
Benefits:
Competitive salary and benefits package
Culture focused on talent development with quarterly promotion cycles and company-sponsored higher education and certifications.
Opportunity to work with cutting-edge technologies.
Employee engagement initiatives such as project parties, flexible work hours, and Long Service awards
Annual health check-ups
Insurance coverage: group term life, personal accident, and Mediclaim hospitalization for self, spouse, two children, and parents
Our company fosters a values-driven and people-centric work environment that enables our employees to:
· Accelerate growth, both professionally and personally
· Impact the world in powerful, positive ways, using the latest technologies
· Enjoy collaborative innovation, with diversity and work-life wellbeing at the core
· Unlock global opportunities to work and learn with the industry’s best
Let’s unleash your full potential at Persistent - persistent.com/careers
Show more
Show less","Java, Scala, Python, Spark, S3, Glue, Redshift, SQL, NoSQL, Agile, API, Data masking, Data transfer, Data science, Data modeling, Development tools","java, scala, python, spark, s3, glue, redshift, sql, nosql, agile, api, data masking, data transfer, data science, data modeling, development tools","agile, api, data masking, data science, data transfer, datamodeling, development tools, glue, java, nosql, python, redshift, s3, scala, spark, sql"
Lead Data Engineer-DE - US,Zortech Solutions,"Scottsdale, AZ",https://www.linkedin.com/jobs/view/lead-data-engineer-de-us-at-zortech-solutions-3784576951,2023-12-17,Arizona,United States,Mid senior,Onsite,"Role: Lead Data Engineer-DE
Location: Scottsdale AZ (day 1 onsite)
Duration: Fulltime
Job Description
Must have skill set: Java, Scala , Python, Spark, S3, Glue, Redshift
You have 6-8 years of relevant software development experience.
You have hands-on experience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical
Highly analytical and data oriented.
Experience in SQL, NoSql Database
Data masking of on prem PII data.
Develop API calls with using secure data transfer.
Take standard output data to lower environments for pre prod testing!
Enable secured channels for data models and data science activities.
Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15 mins
You have experience with development tools and agile methodologies.
Show more
Show less","Java, Scala, Python, Spark, S3, Glue, Redshift, SQL, NoSQL Database, Data Masking, PII Data, API Calls, Secure Data Transfer, Data Warehousing, Data Models, Data Science, Big Data, Agile Methodologies, Development Tools","java, scala, python, spark, s3, glue, redshift, sql, nosql database, data masking, pii data, api calls, secure data transfer, data warehousing, data models, data science, big data, agile methodologies, development tools","agile methodologies, api calls, big data, data masking, data models, data science, datawarehouse, development tools, glue, java, nosql database, pii data, python, redshift, s3, scala, secure data transfer, spark, sql"
GCP Data Engineer,Atos,"Phoenix, AZ",https://www.linkedin.com/jobs/view/gcp-data-engineer-at-atos-3771275055,2023-12-17,Arizona,United States,Mid senior,Onsite,"Role: GCP Data Engineer
Location: Pheonix, AZ - Onsite (Fulltime)
Job description
6-9 Years of hands on Exp with GCP - Data flow, Data proc, Apache Beam skills
Writing complex SQL queries to pull data from multiple tables
Experience in building ELT/ETL processes using DataProc (Python or Java and Pyspark) & Dataflow Apache Beam using Java (for streaming).
Experience working with Google BigQuery.
Building ETL orchestration using Google Composer or Airflow .
Strong understanding of Data Ingestion, Data processing, Orchestration, Parallelization, Transformation and ETL fundamentals.
Sound knowledge of data analysis using any SQL tools.
Knowledge on Big Data technologies.
Designs develop, automates, and support complex applications to extract, transform, and load data.
Should have knowledge of error handling and Performance tuning for Data pipelines.
Excellent communication skills are required.
Show more
Show less","GCP, Dataflow, Data Processing, Apache Beam, SQL, ETL, ELT, DataProc, Pyspark, Java, Google BigQuery, Google Composer, Airflow, Data Ingestion, Data Transformation, Orchestration, Parallelization, SQL tools, Big Data, Error Handling, Performance Tuning","gcp, dataflow, data processing, apache beam, sql, etl, elt, dataproc, pyspark, java, google bigquery, google composer, airflow, data ingestion, data transformation, orchestration, parallelization, sql tools, big data, error handling, performance tuning","airflow, apache beam, big data, data ingestion, data processing, data transformation, dataflow, dataproc, elt, error handling, etl, gcp, google bigquery, google composer, java, orchestration, parallelization, performance tuning, spark, sql, sql tools"
Data Engineer (On-Site),Mathis LLC,"Tempe, AZ",https://www.linkedin.com/jobs/view/data-engineer-on-site-at-mathis-llc-3782823935,2023-12-17,Arizona,United States,Mid senior,Onsite,"Job Type
Full-time
Description
About us
KeyGlee is a fast-growing investment real estate sales company with an emphasis on culture and values. We prioritize collaboration, innovation, and doing good for people. Our team is comprised of diverse, talented individuals who share our vision and values.
**This position is in-office only at our Tempe, AZ location**
Job Description
We are seeking a highly skilled Data Engineer to join our collaborative engineering team. As a Data Engineer at KeyGlee, you will be responsible for setting up our datalake/warehouse in AWS and porting it to Google Big Query. You will work closely with our team to design, develop, and maintain our data infrastructure, and contribute to our mission of doing good for people.
Job Requirements
4+ years of experience in relevant projects
At least 5 years of engineering experience overall
Strong experience with AWS and Google Big Query
3+ years Hands-on coding experience with Python or other scripting languages
2+ years experience utilizing SQL
Experience with taking Large Bronze layer data and converting to Gold layer data
Proficient in Python programming language
Experience working with large datasets and data warehousing
Strong understanding of data governance and data quality principles
Excellent problem-solving skills and ability to work independently
Strong communication and collaboration skills
Preferred Qualifications
Experience with cloud-based data platforms and technologies: AWS (Glue, S3, DataLakes, Dynamo DAX) | GCP (BigQuery, Firebase, Cloud Storage)
Knowledge of SQL and database design
Familiarity with data visualization tools - Google Looker Studio
Experience with Agile software development methodologies
Strong understanding of data security and compliance principles
Experience in PySpark
Benefits
Up to 140K based on experience and benefits package: Dental, Vision, Medical, 401K, PTO accrual, high percentage sponsorship for some benefits
Opportunities for professional growth and development
A collaborative and energetic work environment
Flexible work hours and remote work options (Currently 1 day/week, subject to change)
A chance to be part of a company that is doing good for people
Leadership Development training for qualified individuals
If you are passionate about data engineering, collaboration, and doing good for people, we encourage you to apply for this exciting opportunity. Please submit your resume and a brief cover letter explaining why you'd be a great fit for our team. We look forward to hearing from you!
KeyGlee supports a diverse workforce and is an Equal Opportunity Employer who does not discriminate against individuals on the basis of race, gender, color, religion, national origin, age, sexual orientation, gender identity, disability, veteran status or other classification protected by law. Drug Free Workplace. Females and minorities are encouraged to apply.
Show more
Show less","Data Engineering, AWS, Google Big Query, Python, SQL, Data Warehousing, Data Governance, Agile Development, CloudBased Data Platforms, Data Visualization, Data Security, PySpark","data engineering, aws, google big query, python, sql, data warehousing, data governance, agile development, cloudbased data platforms, data visualization, data security, pyspark","agile development, aws, cloudbased data platforms, data engineering, data governance, data security, datawarehouse, google big query, python, spark, sql, visualization"
Sr. Big Data Engineer,IntraEdge,"Phoenix, AZ",https://www.linkedin.com/jobs/view/sr-big-data-engineer-at-intraedge-3720396859,2023-12-17,Arizona,United States,Mid senior,Onsite,"Willing to relocate to Phoenix, AZ for an onsite, Hybrid role.
Big Data: Sr. (BigData & AI/ML) AI, SQL, Hive, PySpark, Big Data, Hadoop, Spark, UNIX)
7-10+ years of software development experience and leading teams of engineers and scrum teams
4+ years of experience in applying Statistics along with end to end ML engineering (design, development & implementation of end-to-end AI/ML models)
Hands-on experience on writing and understanding complex SQL(Hive/PySpark-dataframes), optimizing joins while processing huge amount of data
Good understanding of various AI/ML models including Classification, Clustering, Regression in detecting Product anomalies and building early warning systems
Expertise with data structures, data modeling, and software architecture
Good to have Qualifications:
Expert on Hadoop and Spark Architecture and its working principle
Experience in UNIX shell scripting
Ability to design and develop optimized Data pipelines for batch and real time data processing
Should have experience in analysis, design, development, testing, and implementation of system applications
Demonstrated ability to develop and document technical and functional specifications and analyze software and system processing flows
Aptitude for learning and applying programming concepts.
Ability to effectively communicate with internal and external business partners. Preferred Additional:
Experience in cloud platforms like GCP/AWS, building Microservices and scalable solutions is highly desired
Knowledge of Financial reporting ecosystem will be a plus
2+ years of experience in designing and building solutions using Kafka streams or queues
Experience with GitHub and leveraging CI/CD pipelines
Experience with NoSQL i.e., HBase, Couchbase, MongoDB
Show more
Show less","AI/ML, SQL, Hive, PySpark, Big Data, Hadoop, Spark, UNIX, Software Development, Team Leading, Statistics, Machine Learning Engineering, Data Structures, Software Architecture, Data Modeling, Data Pipelines, System Applications, Technical Specifications, Functional Specifications, Programming Concepts, Communication, GCP, AWS, Microservices, Cloud Platforms, Financial Reporting, Kafka, CI/CD Pipelines, NoSQL, HBase, Couchbase, MongoDB","aiml, sql, hive, pyspark, big data, hadoop, spark, unix, software development, team leading, statistics, machine learning engineering, data structures, software architecture, data modeling, data pipelines, system applications, technical specifications, functional specifications, programming concepts, communication, gcp, aws, microservices, cloud platforms, financial reporting, kafka, cicd pipelines, nosql, hbase, couchbase, mongodb","aiml, aws, big data, cicd pipelines, cloud platforms, communication, couchbase, data structures, datamodeling, datapipeline, financial reporting, functional specifications, gcp, hadoop, hbase, hive, kafka, machine learning engineering, microservices, mongodb, nosql, programming concepts, software architecture, software development, spark, sql, statistics, system applications, team leading, technical specifications, unix"
"Data Engineer Scala, 100% ONSITE in Phoenix, AZ","Conch Technologies, Inc","Phoenix, AZ",https://www.linkedin.com/jobs/view/data-engineer-scala-100%25-onsite-in-phoenix-az-at-conch-technologies-inc-3779822579,2023-12-17,Arizona,United States,Mid senior,Onsite,"HI Team,
Greetings from Conch Technologies Inc
Position: Scala Developer
Location: HYBRID ( Phoenix, AZ )
Duration: 12+ Months Contract
Visa: OPT, L2 EAD, H4, GC and citizenship.
Interview Information
30-minute screening
1-hour technical interview
Top Skills Details
4+ years of experience as a Scala Developer
Experience with batching large data sets
Spark programmer
External Communities Job Description
Looking for a senior software engineer with strong analytical skills to help build and enhance big data applications. This individual\'s day to day work will involve using scala, sql, big data.
Additional Skills & Qualifications
Looking for a senior software engineer with strong analytical skills to help build and enhance American Express\' big data applications. This individual\'s day to day work will involve using scala, sql, big data.
With Regards,
Nagesh G
Mobile:
408-381-5645
Desk:
901-313-3066
Email: nagesh@conchtech.com
Web:
www.conchtech.com
Show more
Show less","Scala, Spark, SQL, Big Data","scala, spark, sql, big data","big data, scala, spark, sql"
"Data Engineer at Phoenix, AZ OR Charlotte, NC",XCUTIVES Inc.,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-engineer-at-phoenix-az-or-charlotte-nc-at-xcutives-inc-3777994827,2023-12-17,Arizona,United States,Mid senior,Onsite,"Need
Data Engineer
in
Phoenix, AZ OR Charlotte, NC
. Please send an email (aamna.anwar@xcutives.com) to know more about this position.
Job Title: Data Engineer
Location: Phoenix, AZ OR Charlotte, NC
Type: Fulltime
Job Description
Experience working site analytics and tagging tools like Google Analytics/GA4 & Google Tag Manager or similar other tools in the industry
Hands-on experience in implementing click tags, conversion tags/pixel & campaign tracking URLs, URL query parameter, UTM parameter tracking methods, and troubleshooting site analytics reporting issues
Experience on Scripting (Java scripting, HTML, Python, types) and writing macros
Knowledge of CRM, Web Analytics, Tag Management (server-side vs client side), Google SearchAds360 type Marketing Automation tools; & any data visualization tools
Must have experience working with digital marketing data: Google Analytics/GA4, media, CM360 /ad server click and conversion tracking log data
Writing code and unit tests, automation, code reviews and testing
Deploying to production and dev ops
Work with product managers to prioritize features for ongoing sprints and manage a list of technical requirements based on industry trends, new technologies, known defects, and issues.
Manage your own time and work well both independently and as part of a team.
Quickly generate and update proof of concepts for testing and team feedback Embrace emerging standards while promoting best practices.
Experience in the banking domain is a plus.
Regards,
Aamna Anwar
Senior Technical Recruiter
Phone: +1470- 891- 5812 Ext 1031
Email: aamna.anwar@xcutives.com
Xcutives Inc. |
www.xcutives.com
Show more
Show less","Google Analytics, Google Analytics GA4, Google Tag Manager, HTML, Python, Java scripting, CRM, Web Analytics, Tag Management, Google SearchAds360, Marketing Automation, Data Visualization, Dev Ops, Unit Testing, Code Reviews, Proof of Concepts","google analytics, google analytics ga4, google tag manager, html, python, java scripting, crm, web analytics, tag management, google searchads360, marketing automation, data visualization, dev ops, unit testing, code reviews, proof of concepts","code reviews, crm, dev ops, google analytics, google analytics ga4, google searchads360, google tag manager, html, java scripting, marketing automation, proof of concepts, python, tag management, unit testing, visualization, web analytics"
BIg Data Engineer,BVS,"Phoenix, AZ",https://www.linkedin.com/jobs/view/big-data-engineer-at-bvs-3752017277,2023-12-17,Arizona,United States,Mid senior,Onsite,"HI ,
This is Lokesh from BVIS. I hope you are doing well
I have an urgent requirement with my client located in AZ
Iam attaching the JD below for your reference please check and let me know if you have any questions
Position: Big Data Engineer
Location: Phoenix, AZ (Only open for Phoenix or west coast candidates who wants to relocate)
Overall Experience: 11+ years minimum
No CPT, OPT, STEM EAD
Big Data - Engineer ( Hive , Spark, SQL, Pyspark, UNIX)
Software development experience and leading teams of engineers and scrum teams
3+ years of hands-on experience of working with Map-Reduce, Hive, Spark (core, SQL and PySpark)
Hands-on experience on writing and understanding complex SQL(Hive/PySpark-dataframes), optimizing joins while processing huge amount of data
Experience in UNIX shell scripting
Additional Good To Have Requirements
Solid Datawarehousing concepts
Knowledge of Financial reporting ecosystem will be a plus
Experience with Data Visualization tools like Tableau, SiSense, Looker
Expert on Distributed ecosystem
Hands-on experience with programming using Python/Scala
Expert on Hadoop and Spark Architecture and its working principle
Ability to design and develop optimized Data pipelines for batch and real time data processing
Should have experience in analysis, design, development, testing, and implementation of system applications
Demonstrated ability to develop and document technical and functional specifications and analyze software and system processing flows
Aptitude for learning and applying programming concepts.
Ability to effectively communicate with internal and external business partners. Preferred Qualifications:
Knowledge of cloud platforms like GCP/AWS, building Microservices and scalable solutions, will be preferred
2+ years of experience in designing and building solutions using Kafka streams or queues
Experience with GitHub and leveraging CI/CD pipelines
Experience with NoSQL i.e., HBase, Couchbase, MongoDB
Show more
Show less","Big Data, Hive, Spark, SQL, Pyspark, UNIX, MapReduce, Datawarehousing, Data Visualization, Tableau, SiSense, Looker, Distributed Systems, Python, Scala, Hadoop, Kafka, Microservices, GitHub, NoSQL, HBase, Couchbase, MongoDB","big data, hive, spark, sql, pyspark, unix, mapreduce, datawarehousing, data visualization, tableau, sisense, looker, distributed systems, python, scala, hadoop, kafka, microservices, github, nosql, hbase, couchbase, mongodb","big data, couchbase, datawarehousing, distributed systems, github, hadoop, hbase, hive, kafka, looker, mapreduce, microservices, mongodb, nosql, python, scala, sisense, spark, sql, tableau, unix, visualization"
Senior Data Analysis Engineer,Nikola Corporation,"Phoenix, AZ",https://www.linkedin.com/jobs/view/senior-data-analysis-engineer-at-nikola-corporation-3783192675,2023-12-17,Arizona,United States,Mid senior,Onsite,"Overview
We are seeking a Senior Data Analysis Engineer that will be responsible for BEV/FCEV database build/monitor and visualizations in dashboards to track engineering metrics related to the quality of propulsion systems in a continuously automated fashion. You will be championing the digital transformation journey of creation of a brand-new platform for advanced analytics & data services. You will have full autonomy to strategize your roadmap, architect state of the art data pipelines, and deliver innovative solutions. You will work closely with Nikola's Propulsion Engineer and Product Manager to provide data analysis tools/results for propulsion system and vehicle level.
Responsibilities
Strong experience building, managing, and optimizing big data ‘time-series’ data pipelines
Advanced working knowledge of SQL and Python/Scala, working with very large time-series data sets in a distributed computing framework
Interact with engineering subject experts to understand the analysis logic and translate the request into well-designed SQL/Python queries
Gather necessary requirements needed for designing metrics and building dashboards
Build scripts using Python/Spark/SQL to query data from large Postgres Database and/or parquet files stored in AWS S3, to present time aligned, cleaned and transformed data needed for analysis by engineering users
Write well-designed and testable Python code as Airflow DAGs or AWS Glue to execute the queries in an orchestrated fashion using a data pipeline
Create PowerBI and/or Grafana dashboards to provide insight, track metrics, drive action, and support engineering work in a data driven manner
Qualifications
Bachelor's degree in engineering
5-10 years of experience
Working knowledge with cloud-based time-series stream processing systems
Familiarity of electrified propulsion systems, physical vehicle test data, CAN-based data loggers and supporting analysis of the test data
Familiarity with CAN/LIN based SAE standard communication protocols
Candidates must have current U.S. work authorization or be TN eligible from Mexico or Canada. This position is not eligible for CPT or OPT.
EEO Statement
Nikola Corporation™ is committed to a policy of equal employment opportunity. We recruit, employ, train, compensate, and promote without regard to race, color, age, sex, ancestry, marital status, religion, national origin, physical or mental disability, sexual orientation, gender identity, medical condition, pregnancy, veteran status, genetic information or any other classification protected by state or federal law.
Show more
Show less","Data Analysis, Data Engineering, Big Data, SQL, Python, Scala, Airflow, AWS Glue, PowerBI, Grafana, AWS S3, Spark, Postgres, CAN, LIN, SAE, Propulsion Systems, Vehicle Testing, Data Pipelines, Time Series, Cloud Computing","data analysis, data engineering, big data, sql, python, scala, airflow, aws glue, powerbi, grafana, aws s3, spark, postgres, can, lin, sae, propulsion systems, vehicle testing, data pipelines, time series, cloud computing","airflow, aws glue, aws s3, big data, can, cloud computing, data engineering, dataanalytics, datapipeline, grafana, lin, postgres, powerbi, propulsion systems, python, sae, scala, spark, sql, time series, vehicle testing"
Data Engineer Sr.,TriWest Healthcare Alliance,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-engineer-sr-at-triwest-healthcare-alliance-3781021819,2023-12-17,Arizona,United States,Mid senior,Onsite,"Profile
We offer remote work opportunities for those residing in the following states ONLY:
AK, AR, AZ, *CO, FL, HI, IA, ID, IL, KS, LA, MD, MN, MO, MT, NE,NV, NM, NC, ND, OK, OR, SC, SD, TX, UT, VA/DC, *WA, WI & WY only
~Veterans, Reservists, Guardsmen and military family members are strongly encouraged to apply~
Job Summary
The Senior Data Engineer will be responsible for design and development of ETL. The role will cover full systems development life cycle (SDLC) phases including requirements gathering, data analysis, system design, development, implementation, and post-implementation support. During the design phase, the role will work closely with Data Architects/Models. The developer will be responsible the ETL movement of data from originating source feeds (files, x12, etc) to target systems (SQL Server, Snowflake) using Data Bricks.
Education & Experience
Required:
Bachelor's Degree in Business Administration, Computer Science, Mathematics, Engineering, or related field with programming and database systems coursework or equivalent database development experience
5+ years of experience in a Data Engineering role
2+ years of experience with Data Bricks preferably within Azure Cloud
3+ years of experience with Python
3+ years of experience with Azure
Experience in gathering business requirements from cross functional teams
Experience with SDLC processes and DevOps
Preferred
Experience with Data Governance
Experience with SQL Server and Snowflake in Azure Cloud
Experience in ETL with both structured and semi-structured data
Experience with x12 data file formats
Understanding of ANSI and T-SQL
Experience with data analysis in the healthcare industry
Experience in a team or project lead role, with a focus on mentoring
Key Responsibilities
Collaborate with users, business analysts, developers, database administrators, and project managers on reporting needs.
Collaborate with data architect to create conceptual, logical, and physical data models for reporting databases.
Implement changes and provide post-implementation user support and system support.
Identify and advocates beneficial change opportunities.
Acts as a technical resource for application and data users, data administrators and others.
Ensures compliance with TriWest HIPAA, privacy and government security policies.
Performs other duties as assigned.
Regular and reliable attendance is required.
Company Overview
Taking Care of Our Nation’s Heroes.
It’s Who We Are. It’s What We Do.
Do you have a passion for serving those who served?
Join the TriWest Healthcare Alliance Team! We’re On a Mission to Serve®!
Our job is to make sure that America’s heroes get connected to health care in the community.
At TriWest Healthcare Alliance, we’ve proudly been on that important mission since 1996.
Benefits
We’re more than just a health care company. We’re passionate about serving others! We believe in rewarding loyal, hard-working people who are willing to learn as they grow. TriWest Healthcare Alliance values teamwork. Join our team, fulfill your responsibilities, and you may also be considered for frequent pay raises, overtime opportunities to earn even more, recognition and reward programs, and much more. Of course, we also offer a comprehensive and progressive compensation and benefits package that includes:
Medical, Dental and Vision Coverage
Generous paid time off
401(k) Retirement Savings Plan (with matching)
Short-term and long-term disability, basic life, and accidental death and dismemberment insurance
Tuition reimbursement
Paid volunteer time
Annual base salary for Colorado and Washington State residents: $138,000 - $161,000 depending on experience
Equal Employment Opportunity
TriWest Healthcare Alliance is an equal employment opportunity employer. We are proud to have an inclusive work environment and know that a diverse team is a strength that will drive our success. To that end, TriWest strives to create an inclusive environment that cultivates and supports diversity at every organizational level, including hiring and retaining a diverse workforce, and we highly encourages candidates from all backgrounds to apply. Applicants are considered for positions without discrimination on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or any other consideration made unlawful by applicable federal, state, or local laws.
Show more
Show less","Data Engineering, ETL, SDLC, Data Analysis, System Design, Development, Implementation, Postimplementation Support, Data Architecture, Data Modeling, Python, Azure, SQL Server, Snowflake, Data Governance, ANSI, TSQL, Data Analysis, Healthcare, HIPAA, Privacy, Government Security Policies, DevOps","data engineering, etl, sdlc, data analysis, system design, development, implementation, postimplementation support, data architecture, data modeling, python, azure, sql server, snowflake, data governance, ansi, tsql, data analysis, healthcare, hipaa, privacy, government security policies, devops","ansi, azure, data architecture, data engineering, data governance, dataanalytics, datamodeling, development, devops, etl, government security policies, healthcare, hipaa, implementation, postimplementation support, privacy, python, sdlc, snowflake, sql server, system design, tsql"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"Phoenix, AZ",https://www.linkedin.com/jobs/view/expression-of-interest-data-engineer-at-fingerprint-for-success-f4s-3787778167,2023-12-17,Arizona,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
FOotvE3M8m
Show more
Show less","JazzHR, Fingerprint For Success (F4S)","jazzhr, fingerprint for success f4s","fingerprint for success f4s, jazzhr"
Senior Data Engineer,BambooHR,"Phoenix, AZ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-bamboohr-3760295698,2023-12-17,Arizona,United States,Mid senior,Remote,"About Us
Our mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.
What You'll Do
As a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.
Your initial areas of focus will include:
Collaborate with stakeholders to make effective use of core data assets
With Spark and Pyspark libraries, load both streaming and batched data
Engineer lakehouse models to support defined data patterns and use cases
Leverage a combination of tools, engines, libraries, and code to build scalable data pipelines
Work within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environments
Documentation of data pipelines, cloud infrastructure, and standard operating procedures
Express data platform cloud infrastructure, services, and configuration as code
Automate load, scaling, and performance testing of data platform pipelines and infrastructure
Monitor, operate, and optimize data pipelines and distributed applications
Help ensure appropriate data privacy and security
Automate continuous upgrades and testing of data platform infrastructure and services
Build data pipeline unit, integration, quality, and performance tests
Participate in peer code reviews, code approvals, and pull requests
Identify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!)
Experience developing, scaling, and tuning data pipelines in Spark with PySpark
Understanding of data lake, lakehouse, and data warehouse systems, and related technologies
Knowledge and understanding of data formats, data patterns, models, and methodologies
Experience storing data objects in hadoop or hadoop like environments such as S3
Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark
Experience working with streaming technologies such as Kafka and Kinesis
Experience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or Greenplum
Ability to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets manager
Understanding of security around cloud infrastructure and data systems
Git-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!)
Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and Delta
Experience with Flink, Presto, Dremio, Databricks, or Kubernetes
Experience with expressing infrastructure as code leveraging tools like Terraform
Experience and understanding of a zero trust security framework
Experience developing CI/CD pipelines for automated testing and code deployment
Experience with QA and test automation
Exposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are:
Clear communicators with team members and stakeholders
Analytical and perceptive of patterns
Creative in coding
Detail-oriented and persistent
Productive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.
An Equal Opportunity Employer--M/F/D/V
Because our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.
For information on our Privacy Policy, click here.
Show more
Show less","Data Engineering, Data Systems, Data Pipelines, Data Lakes, Lakehouses, Spark, Pyspark, Data Lake, Lakehouse, Data Warehouse, Data Formats, Data Patterns, Data Models, Hadoop, S3, EMR, Kafka, Kinesis, Snowflake, Databricks, Redshift, Vertica, Greenplum, AWS, S3, RDS, IAM, Security Groups, AMIs, Cloudwatch, Cloudtrail, Secrets Manager, Git, Terraform, Zero Trust Security, CI/CD Pipelines, Automated Testing, Code Deployment, QA, Test Automation, Tableau","data engineering, data systems, data pipelines, data lakes, lakehouses, spark, pyspark, data lake, lakehouse, data warehouse, data formats, data patterns, data models, hadoop, s3, emr, kafka, kinesis, snowflake, databricks, redshift, vertica, greenplum, aws, s3, rds, iam, security groups, amis, cloudwatch, cloudtrail, secrets manager, git, terraform, zero trust security, cicd pipelines, automated testing, code deployment, qa, test automation, tableau","amis, automated testing, aws, cicd pipelines, cloudtrail, cloudwatch, code deployment, data engineering, data formats, data lake, data lakes, data models, data patterns, data systems, databricks, datapipeline, datawarehouse, emr, git, greenplum, hadoop, iam, kafka, kinesis, lakehouse, lakehouses, qa, rds, redshift, s3, secrets manager, security groups, snowflake, spark, tableau, terraform, test automation, vertica, zero trust security"
"Sr. Engineer, Database Infrastructure - Slack",Slack,Greater Phoenix Area,https://www.linkedin.com/jobs/view/sr-engineer-database-infrastructure-slack-at-slack-3760632557,2023-12-17,Arizona,United States,Mid senior,Remote,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Software Engineering
Job Details
About Salesforce
We’re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good – you’ve come to the right place.
Slack's DataStores team builds and operates the database platform powering Slack. We write software to manage thousands of stateful hosts, providing several petabytes of online database capacity. We are building one of the fastest-growing database platforms in the world. Our MySQL databases run in Vitess. You can read more about our migration to Vitess at: Scaling Datastores at Slack with Vitess
Background
Slack enables people all over the world to communicate and collaborate together. Teams of all scales — from the world’s largest public companies to the smallest of startups — use Slack to get work done, so we take performance and reliability very seriously. A taste of our scale:
The average user spends over 10 hours connected and 2.5 hours active in Slack every single day
1.5 billion+ messages are sent per month, half of those outside the United States
Every day we see over 10M+ daily active users, over a billion web requests, and 10s of billions of database queries.
For millions of people, Slack is the primary communication tool they use at work all day long. They expect it to be exceptionally reliable and fast, all the time.
Infrastructure at Slack
We operate at tremendous scale with systems that process millions of events per second. Teams in our group maintain and build the lower levels of our stack, including:
Edge services
Data Stores and Caches
Real-time messaging
Asynchronous background job processing
We know we’ve done our job correctly when
none of our users think about us.
We don’t typically ship new user-facing features, but rather ensure our systems are incredibly performant, highly available, reliable, and scalable. In other words, Slack just works seamlessly.
Slack's API and web backend is built on PHP/Hack, our backend services are written in Java and Go, and we use Vitess as our storage engine. Our architecture is constantly evolving to handle millions more users. You can read about how we scaled our datastores with Vitess, how we respond to incidents, and much more on our blog.
What you will be doing:
Operate and enhance our large, highly-available database infrastructure, utilizing technologies such as MySQL and Vitess.
Develop tools to enable self-service and self-managing capabilities of our database infrastructure so that other teams can operate full-stack while rapidly building new features for our customers.
Collaborate with engineering teams on their database storage needs, and advise them throughout the development lifecycle.
Write code to capture database performance, and create tools and dashboards to provide actionable insight into that data.
Participate in our on-call rotation and collaborate with our operations team to triage and resolve production issues.
Support FedRAMP and DoD SRG activities.
What you should have:
Have been working in data storage, core infrastructure, or distributed system-owning teams with increasing responsibilities for 5+ years.
Have professional experience using Go, PHP/Hacklang, Python, Ruby, or Java.
Write code that can be easily understood by others with an eye towards clarity and maintainability. Collaborate with other teams to integrate new features of your platform or adopt self-service features.
Operated at least one distributed system, at scale and in a team environment. Some examples include: a relational database like MySQL/Postgres, or systems like Kafka, Cassandra, or ElasticSearch.
Deployed server software on Linux, and then operated it at scale. You’ve debugged its problems, and analyzed and optimized its performance.
Have experience operating cloud infrastructure, especially AWS.
Are familiar with deployment automation/configuration management tools like Chef, Ansible, Puppet, or Terraform.
Are a very strong communicator. You’re excited to explain complex technical concepts and share your knowledge with different audiences.
Are curious how things work; when they break you are eager and able to help fix them.
Infrastructure is a diverse and inclusive team that treats their colleagues exceptionally well. We are happy to help you learn what you need to know; we encourage and support each other’s growth and thus it’s not expected that you would have expertise across all of these areas.
Come join us!
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com .
Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce .
﻿Salesforce welcomes all.
For Colorado-based roles, the base salary hiring range for this position is $156,800 to $215,600.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.
Show more
Show less","MySQL, Vitess, PHP, Java, Go, Python, Ruby, Chef, Ansible, Puppet, Terraform, Linux, AWS, Cassandra, ElasticSearch, Kafka, Relational database system, Cloud infrastructure, Deployment automation, Configuration management","mysql, vitess, php, java, go, python, ruby, chef, ansible, puppet, terraform, linux, aws, cassandra, elasticsearch, kafka, relational database system, cloud infrastructure, deployment automation, configuration management","ansible, aws, cassandra, chef, cloud infrastructure, configuration management, deployment automation, elasticsearch, go, java, kafka, linux, mysql, php, puppet, python, relational database system, ruby, terraform, vitess"
Remote Data Governance Engineers,IVY TECH SOLUTIONS INC,"Phoenix, AZ",https://www.linkedin.com/jobs/view/remote-data-governance-engineers-at-ivy-tech-solutions-inc-3787775180,2023-12-17,Arizona,United States,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
Remote
Data Governance Engineers (2 pos.)
Duration: 6 month contract-to-hire
Only w2
Please send the resume to
or 847- 350-1008
Essential Responsibilities
Provide technical analysis, design, code, and automation in support of program and project work you're doing using data governance technologies (data catalogs, data dictionary, data quality, business glossary etc.).
Participate in design sessions, implementation plans, and resolve technical issues as needed.
Responsible for platform configuration and extension based on Enterprise Architecture Guidelines (performance, extensible, API enabled etc.).
Coordinates, schedules, installs, and tests hardware and software changes.
Develops testing and implementation plans.
Works with other engineers to plan installations and upgrades and ensure subsequent maintenance in accordance with established IT policies and procedures.
Monitor’s system to achieve optimum performance levels.
Collaborate with business teams by providing technical input to Data Governance policies, standards and processes related to data, access, and security (privacy & protection) of sensitive data.
Work closely with application/platform/governance teams to create onboarding and intake documentation and definitions as repeatable processes.
Helps in ensuring that Data Governance policies and procedures are adhered to for onboarding data subject areas with clear data classification and data ownership.
Helps in ensuring controls are in place for access and security for IT managed and self-service environments.
Supports the SLAs for Service/Incident tickets where applicable.
Participating in repeatable processes and systems to support Audit and GRC adherence requests.
Participate in the creation and delivery of the Data Governance roadmap.
Participate in evaluating and recommending 3rd party technologies that advance the Data Governance roadmap.
Identify areas of continuous improvement and automation.
Deliver software in an Agile (Scrum/Kanban etc.) framework setting.
What Would Make Us Excited About You
3+ years in IT working in multiple system environments
Understanding of enterprise data governance technologies and protocols
Ability to implement platform upgrades and migrations
Knowledge of basic ITIL framework and processes
Bachelor’s degree in related area (Computer Science, Information Systems, or related technical discipline) or an equivalent combination of education and experienc
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
PH.
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
dkYLWSoMUt
Show more
Show less","Data governance technologies, Data catalogs, Data dictionary, Data quality, Business glossary, Enterprise architecture guidelines, Hardware and software installation, Testing and implementation plans, IT policies and procedures, Data governance policies and standards, Data classification, Data ownership, Access and security controls, IT managed and selfservice environments, SLAs for service/incident tickets, Audit and GRC adherence requests, Data governance roadmap, 3rd party technologies evaluation, Continuous improvement and automation, Agile development framework, ITIL framework and processes, Bachelor's degree in related area","data governance technologies, data catalogs, data dictionary, data quality, business glossary, enterprise architecture guidelines, hardware and software installation, testing and implementation plans, it policies and procedures, data governance policies and standards, data classification, data ownership, access and security controls, it managed and selfservice environments, slas for serviceincident tickets, audit and grc adherence requests, data governance roadmap, 3rd party technologies evaluation, continuous improvement and automation, agile development framework, itil framework and processes, bachelors degree in related area","3rd party technologies evaluation, access and security controls, agile development framework, audit and grc adherence requests, bachelors degree in related area, business glossary, continuous improvement and automation, data catalogs, data classification, data dictionary, data governance policies and standards, data governance roadmap, data governance technologies, data ownership, data quality, enterprise architecture guidelines, hardware and software installation, it managed and selfservice environments, it policies and procedures, itil framework and processes, slas for serviceincident tickets, testing and implementation plans"
Data Engineer - Scala(U.S. remote),Railroad19,"Goodyear, AZ",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782855509,2023-12-17,Arizona,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, EMR, S3, Restful APIs, Relational databases, Nonrelational databases","scala, spark, aws, emr, s3, restful apis, relational databases, nonrelational databases","aws, emr, nonrelational databases, relational databases, restful apis, s3, scala, spark"
Data Engineer - Scala(U.S. remote),Railroad19,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782292619,2023-12-17,Arizona,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, EMR, S3, Restful API, Relational databases, Nonrelational databases","scala, spark, aws, emr, s3, restful api, relational databases, nonrelational databases","aws, emr, nonrelational databases, relational databases, restful api, s3, scala, spark"
Data Engineer - Scala(U.S. remote),Railroad19,"Glendale, AZ",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782854609,2023-12-17,Arizona,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Restful APIs, Spark 2.4, AWS, EMR, S3, Relational and nonrelational databases, Apache Spark, Computer science, Computer engineering","scala 212, restful apis, spark 24, aws, emr, s3, relational and nonrelational databases, apache spark, computer science, computer engineering","apache spark, aws, computer engineering, computer science, emr, relational and nonrelational databases, restful apis, s3, scala 212, spark 24"
Data Engineer - Scala(U.S. remote),Railroad19,"Chandler, AZ",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782858086,2023-12-17,Arizona,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, EMR, S3, Restful APIs, Relational databases, Nonrelational databases","scala, spark, aws, emr, s3, restful apis, relational databases, nonrelational databases","aws, emr, nonrelational databases, relational databases, restful apis, s3, scala, spark"
Data Engineer - Scala(U.S. remote),Railroad19,"Peoria, AZ",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782857216,2023-12-17,Arizona,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, EMR, S3, Restful API, Relational databases, Nonrelational databases","scala, spark, aws, emr, s3, restful api, relational databases, nonrelational databases","aws, emr, nonrelational databases, relational databases, restful api, s3, scala, spark"
Data Engineer - Scala(U.S. remote),Railroad19,"Gilbert, AZ",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782855497,2023-12-17,Arizona,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, EMR, S3, Restful APIs, Relational databases, Nonrelational databases, Problem solving, Communication","scala, spark, aws, emr, s3, restful apis, relational databases, nonrelational databases, problem solving, communication","aws, communication, emr, nonrelational databases, problem solving, relational databases, restful apis, s3, scala, spark"
Data Engineer - Scala(U.S. remote),Railroad19,"Surprise, AZ",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782857217,2023-12-17,Arizona,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, AWS, EMR, S3, Restful APIs, Relational databases, Nonrelational databases","scala 212, spark 24, aws, emr, s3, restful apis, relational databases, nonrelational databases","aws, emr, nonrelational databases, relational databases, restful apis, s3, scala 212, spark 24"
Data Engineer - Scala(U.S. remote),Railroad19,"Mesa, AZ",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782857145,2023-12-17,Arizona,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, AWS, S3, EMR, Restful APIs, SQL, NoSQL, Software design, Software development, Troubleshooting, Communication, Problemsolving","scala 212, spark 24, aws, s3, emr, restful apis, sql, nosql, software design, software development, troubleshooting, communication, problemsolving","aws, communication, emr, nosql, problemsolving, restful apis, s3, scala 212, software design, software development, spark 24, sql, troubleshooting"
Data Engineer - Scala(U.S. remote),Railroad19,"Tucson, AZ",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782855418,2023-12-17,Arizona,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Clean Code, Restful APIs, Spark 2.4, AWS, EMR, S3, Relational Databases, NonRelational Databases, Spark Development, Oral Communication, Written Communication, Analytical Skills, Problem Solving, SelfDirected Work, Computer Science, Computer Engineering","scala 212, clean code, restful apis, spark 24, aws, emr, s3, relational databases, nonrelational databases, spark development, oral communication, written communication, analytical skills, problem solving, selfdirected work, computer science, computer engineering","analytical skills, aws, clean code, computer engineering, computer science, emr, nonrelational databases, oral communication, problem solving, relational databases, restful apis, s3, scala 212, selfdirected work, spark 24, spark development, written communication"
Staff Cybersecurity Data Platform Engineer,Adobe,"Arizona, United States",https://www.linkedin.com/jobs/view/staff-cybersecurity-data-platform-engineer-at-adobe-3767919831,2023-12-17,Arizona,United States,Mid senior,Remote,"Our Company
Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.
We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!
As a Staff Cybersecurity Data Platform Engineer at Adobe, you will be joining a team responsible for enhancing the organization's security data platform. Our focus is on contributing to the organization by emphasizing real-world security and embracing automation, as well as optimizing data flow and consumption of the data to multiple teams within Security.
The ideal candidate will help develop security-focused data platform needs and the scaling of a data lake spanning petabytes of data. The job involves system architecture, design, hands-on development, optimization, setting expectations and SLAs with a focus on reliability, availability, and performance.
They must be self-directed and comfortable supporting the data needs of multiple security teams. The right candidate will be excited by the prospect of optimizing our company’s data platform to support our next generation of security products and data initiatives.
Key Responsibilities:
Collaborate with partners, cybersecurity engineers, and operations teams across the security organization to drive the development of enterprise-scale security solutions.
Work with enterprise architects team to ensure alignment with strategic objectives, and leverage design principles while providing input into program direction.
Translate business needs into technical requirements, capabilities, and formulation of solutions, while identifying risks, dependencies, financial impacts, and the risk profile in the technical solution.
Take a hands-on approach to driving proof-of-concept, design, and implementation activities from an architectural perspective.
Develop and maintain solution architecture documents and other artifacts to guide the planning, design, and implementation of the proposed solution.
Required Skills to be Successful:
7 years of experience in designing and building solutions in Databricks, across multiple clouds, using orchestration, data ingestion, medallion architectures, unity catalog, autoloader jobs, and Delta Lakehouse concepts.
10 years of experience working as either: Software Engineer/Data Engineer: query tuning, performance tuning, troubleshooting, and implementing/debugging Spark/PySpark and/or other big data solutions.
10 years of proficiency in developing or architecting modern distributed cloud architectures using AWS tools and technology.
Design end-to-end robust, scalable, real-time data streaming (Kafka / Flink) and data platform architecture that will support the analytical and reporting needs of the entire organization.
Offer technical expertise by collaborating with analysts and business users to translate diverse and intricate functional specifications into technical designs.
Build data models and improve standard schemas across different data sources and normalize data.
Significant experience in security, encompassing threat management, incident response, and enterprise security.
Strong understanding of Security Operations Center (SOC) operations and security management workflows within large organizations.
Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $146,400 -- $275,000 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.
At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).
In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.
Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.
Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.
Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.
Show more
Show less","Databricks, AWS, Apache Kafka, Apache Flink, Spark, PySpark, Delta Lakehouse, Data Engineering, Software Engineering, Solution Architecture, Data Modeling, Threat Management, Incident Response, Enterprise Security, Security Operations Center (SOC), System Architecture, Design, Implementation, Optimization, Performance Tuning, Troubleshooting, Cloud Architectures","databricks, aws, apache kafka, apache flink, spark, pyspark, delta lakehouse, data engineering, software engineering, solution architecture, data modeling, threat management, incident response, enterprise security, security operations center soc, system architecture, design, implementation, optimization, performance tuning, troubleshooting, cloud architectures","apache flink, apache kafka, aws, cloud architectures, data engineering, databricks, datamodeling, delta lakehouse, design, enterprise security, implementation, incident response, optimization, performance tuning, security operations center soc, software engineering, solution architecture, spark, system architecture, threat management, troubleshooting"
Sr. Data Engineer,MATRIX Resources,"Chandler, AZ",https://www.linkedin.com/jobs/view/sr-data-engineer-at-matrix-resources-3779395750,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Hello
Looking for Senior Data Engineers with 6+ years of experience
Hybrid Roles: Preferred Locations: Chandler, AZ
OR
Charlotte, NC
OR
Des Moines IA
Duration: 12+ months with possibility of longer term extensions
Pay Rate: $60 - $65/hr on W2
If interested, please respond with your resume to grace.johnson@motionrecruitment.com
No C2C or H1B or 1099 or TN Visa or F1 (CPT or OPT)
This role demands a seasoned Data Engineer with substantial expertise in both model development and the foundational aspects of data engineering.
Key Responsibilities:
Primary focus is on data engineering - aggregating and integrating diverse datasets, establishing robust processes supporting model execution, and facilitating the deployment of model outputs to relevant destinations.
Automation development, thorough testing, and ensuring comprehensive documentation outputs.
Managing and refining processes integral to executing the model while emphasizing developmental aspects.
Technical Proficiency: REQUIRED:
Data Modeling
SAS SQL, Python, Teradata experience - Required
Environment: Teradata Vantage
Experience working within the Teradata Vantage environment.
Show more
Show less","SAS SQL, Python, Data Modeling, Teradata Vantage","sas sql, python, data modeling, teradata vantage","datamodeling, python, sas sql, teradata vantage"
Data Engineer,Motion Recruitment,"Arizona, United States",https://www.linkedin.com/jobs/view/data-engineer-at-motion-recruitment-3739105864,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Data Engineer We are working with a client in Scottsdale, AZ to hire a data engineer. You will be building data pipelines and doing data manipulations using BigQuery, DBT and MySQL. Raw data will be coming in from various sources and it is your job to bring the data together onto a single source.
This role requires 3 days on-site in South Scottsdale. Not offering relocation assistance. Required Skills & Experience
2-3 of professional data engineering experience
Proficiency using Python
MySQL skills
Desired Skills & Experience
Experience working with DBT and FiveTran
What You Will Be Doing Tech Breakdown
100% Data Engineering
Daily Responsibilities
100% Hands on
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Natalie Daub
Show more
Show less","Data Engineering, Data Pipelines, Data Manipulations, BigQuery, DBT, MySQL, Python, FiveTran","data engineering, data pipelines, data manipulations, bigquery, dbt, mysql, python, fivetran","bigquery, data engineering, data manipulations, datapipeline, dbt, fivetran, mysql, python"
Bigdata Engineer,NR Consulting,"Phoenix, AZ",https://www.linkedin.com/jobs/view/bigdata-engineer-at-nr-consulting-3772269622,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Job Title
: Bigdata Engineer
Work Location:
Phoenix, AZ (Local)
Position Type:
Contract
Job Description:
Big data engineers that have 8+ year's experience in Spark and Hive.
Experts in SQL.
Show more
Show less","Big Data, Spark, Hive, SQL","big data, spark, hive, sql","big data, hive, spark, sql"
Data Engineer,Motion Recruitment,"Arizona, United States",https://www.linkedin.com/jobs/view/data-engineer-at-motion-recruitment-3767074278,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Data Engineer We are working with a client in the Financial Services industry to hire a Data Engineer. They are looking for someone to build and maintain their ETL pipelines. You will work closely with the product and engineering leads to ensure that the data is meeting the standards and requirements. You will also be analyzing the data for the stakeholders and decision makers.
This role will be hybrid in North Scottsdale. It is required that you go on-site 2-3 days a week. They are not offering relocation assistance. Required Skills & Experience
Proficient in Python and SQL
Experience with MySQL and PostgreSQL
Desired Skills & Experience
Experience using Databricks
What You Will Be Doing Tech Breakdown
100% Data Engineering
Daily Responsibilities
100% Hands on
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Natalie Daub
Show more
Show less","Python, SQL, MySQL, PostgreSQL, Databricks, ETL, Data Engineering","python, sql, mysql, postgresql, databricks, etl, data engineering","data engineering, databricks, etl, mysql, postgresql, python, sql"
"Data engineer with Java, Kafka, K SQL",Keylent Inc,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-engineer-with-java-kafka-k-sql-at-keylent-inc-3768063774,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Position:
Data engineer with Java, Kafka, K SQL
Location:
Phoenix AZ
Duration:
12 months plus
Shift Day 9AM TO 7PM EST
Candidate should have Java, Kafka and K-SQL DB experience and some experience with streaming compute; Good to have experience in ring buffer kind of framework.
Job summary Technical Lead with Experience on Databases Expertise in Enterprise level Business, Logical and Physical Data Model. Experience in distributed database Design, Development and Support. Hands on Experience as Cassandra Architect Experience in implementing enterprise level data management and analytical solutions i.e. ODS, EDW, etc. Experience in Datastax Cassandra Distribution Knowledge and experience developing large-scale system software. Participated in the entire product life cycle: design, implementation, testing, deployment and maintenance. Excellent Analytical and Problem Solving Skills Experienced and knowledgeable with core business functions i.e. Banking Financial Strong oral and written communication
Required Skills - Kafka Nice to have skills - PL/SQL, SQL, Cassandra
Roles & Responsibilities
Expertise in Enterprise level Business, Logical and Physical Data Model. Experience in distributed database Design, Development and Support. Hands on Experience as Cassandra Architect Experience in implementing enterprise level data management and analytical solutions i.e. ODS, EDW, etc. Experience in Datastax Cassandra Distribution Knowledge and experience developing large-scale system software. Participated in the entire product life cycle: design, implementation, testing, deployment and maintenance. Excellent Analytical and Problem Solving Skills Experienced and knowledgeable with core business functions i.e. Banking Financial Strong oral and written communication
Show more
Show less","Java, Kafka, K SQL, Streaming compute, Ring buffer, Cassandra, Datastax Cassandra Distribution, ODS, EDW, PL/SQL, SQL","java, kafka, k sql, streaming compute, ring buffer, cassandra, datastax cassandra distribution, ods, edw, plsql, sql","cassandra, datastax cassandra distribution, edw, java, k sql, kafka, ods, plsql, ring buffer, sql, streaming compute"
Data Engineer / FinTech,Motion Recruitment,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-engineer-fintech-at-motion-recruitment-3762366253,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Big Data Engineer
A client of ours in the financial space is looking to hire a Big Data Engineer to join their team. You will be responsible for developing and designing software applications as well as modifying existing applications to meet business requirements. This will be a hybrid role with the expectation to go onsite 1-2 days a week in North Phoenix.
Required Skills & Experience
5+ years of software development experience
3+ years of experience with Map-Reduce, Hive, Spark
Hands-on experience writing and understanding complex SQL
Experience in UNIX shell-scripting
Bachelor’s degree in Engineering or Computer Science or equivalent
What You Will Be Doing
Tech Breakdown
100% Data Engineering
Daily Responsibilities
100% Hands On
Applicants must be currently authorized to work in the US on a full-time basis now and in the future. This role cannot be done on a C2C basis.
Posted By:
Julie Bennett
Show more
Show less","Software Development, Data Engineering, MapReduce, Hive, Spark, SQL, UNIX, Shell Scripting, Engineering, Computer Science","software development, data engineering, mapreduce, hive, spark, sql, unix, shell scripting, engineering, computer science","computer science, data engineering, engineering, hive, mapreduce, shell scripting, software development, spark, sql, unix"
Big Data Engineer,Motion Recruitment,"Arizona, United States",https://www.linkedin.com/jobs/view/big-data-engineer-at-motion-recruitment-3748994684,2023-12-17,Arizona,United States,Mid senior,Hybrid,"A client of ours in the financial space is looking to hire a Big Data Engineer to join their team. You will be responsible for developing and designing software applications as well as modifying existing applications to meet business requirements. This will be a hybrid role with the expectation to go onsite 1-2 days a week in North Phoenix.
Required Skills & Experience
5+ years of software development experience
3+ years of experience with Map-Reduce, Hive, Spark
Hands-on experience writing and understanding complex SQL
Experience in UNIX shell-scripting
Bachelor’s degree in Engineering or Computer Science or equivalent
What You Will Be Doing
Tech Breakdown
100% Data Engineering
Daily Responsibilities
100% Hands On
Applicants must be currently authorized to work in the US on a full-time basis now and in the future. This role cannot be done on a C2C basis.
Posted By:
Natalie Daub
Show more
Show less","Big Data Engineering, Data Engineering, Software Development, MapReduce, Hive, Spark, SQL, UNIX Shell Scripting, Engineering, Computer Science","big data engineering, data engineering, software development, mapreduce, hive, spark, sql, unix shell scripting, engineering, computer science","big data engineering, computer science, data engineering, engineering, hive, mapreduce, software development, spark, sql, unix shell scripting"
Senior Data Engineer,Master Electronics,"Phoenix, AZ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-master-electronics-3784137805,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Are you passionate about harnessing the power of data to drive meaningful insights and create innovative solutions? Are you a seasoned expert in data engineering, capable of architecting and building scalable data pipelines that transform raw data into actionable intelligence? If so, we have an exciting opportunity for you! Our Data Science Team at Master Electronics is looking to add a
Senior Data Engineer
at our Phoenix, AZ headquarters.
Architect, build, scale, and optimize data pipelines from various databases, internal systems, and third-party tools into data lakes enabling seamless data flow
Develop and maintain ETL workflows, ensuring data quality, consistency, and data lineage throughout the pipeline
Drive innovation and integration of new technologies into organization-wide projects and activities in the big data space
Identify areas of improvement and recommend solutions to enhance the reliability, scalability, and efficiency of data services
Define and support internal SLAs for core datasets
Adopt and maintain engineering best practices in data security and retention
Document data engineering processes, workflows, and solutions for knowledge sharing and future references
Provide technical guidance and mentorship to junior data engineers, fostering a collaborative and innovative work environment
Proven experience as a Data Engineer, with a focus on designing and building data pipelines using Databricks and Spark
Experience with big data technologies such as Hadoop, Hive, or Presto and cloud platforms such as AWS, Azure, or Google Cloud
Extensive hands-on experience with code version control (e.g., Git)
Familiarity with workflow and data management solutions such as Fivetran, Airflow, Glue, etc
Proficiency in Python, SQL, and experience working with various databases and data formats
Strong communication skills to convey complex technical concepts to both technical and non-technical stakeholders
Bachelor’s/Master’s degree in Computer Science, Software Engineering, Information Systems, Engineering, or equivalent
5+ years in data engineering or data architecting, while building highly scalable and secure solutions
Pay Range
$100,000 - $120,000
Master Electronics has a fast-paced and entrepreneurial environment, which requires a professional, flexible self-starter attitude.
Headquartered in Phoenix, AZ, Master Electronics is a leading global authorized distributor of electronic components. For more than half a century, our family-owned company has remained focused on strong relationships, responsive service and added value. This is how Master Electronics has grown to serve hundreds of thousands of customers in partnership with hundreds of world-class suppliers.
Master Electronics, a leading global authorized distributor of electronic components, is committed to providing equal employment opportunities for all applicants and employees. The Company does not unlawfully discriminate on the basis of race, color, creed, pregnancy, religion, sex, national origin, age, disability, veteran, marital, or any other protected status. The Company also makes reasonable accommodations for disabled employees. Finally, Master Electronics prohibits the harassment of any individual based on their protected status. This policy applies to all areas of employment, including recruitment, hiring, training, promotion, compensation, benefits, transfer, and social and recreational programs.”
Show more
Show less","Data engineering, Data pipelines, Databricks, Spark, Hadoop, Hive, Presto, AWS, Azure, Google Cloud, Git, Fivetran, Airflow, Glue, Python, SQL, Databases, Data formats, Computer Science, Software Engineering, Information Systems, Engineering","data engineering, data pipelines, databricks, spark, hadoop, hive, presto, aws, azure, google cloud, git, fivetran, airflow, glue, python, sql, databases, data formats, computer science, software engineering, information systems, engineering","airflow, aws, azure, computer science, data engineering, data formats, databases, databricks, datapipeline, engineering, fivetran, git, glue, google cloud, hadoop, hive, information systems, presto, python, software engineering, spark, sql"
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"Arizona, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-engineering-at-acv-auctions-3762877415,2023-12-17,Arizona,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, ETL, Data modeling, Data warehousing, Data architecture, NoSQL, Cloud services, Python, C#, Data engineering, Hadoop, Spark, DevOps, AWS, Azure, Snowflake, DBT, Airflow, SSIS","sql, etl, data modeling, data warehousing, data architecture, nosql, cloud services, python, c, data engineering, hadoop, spark, devops, aws, azure, snowflake, dbt, airflow, ssis","airflow, aws, azure, c, cloud services, data architecture, data engineering, datamodeling, datawarehouse, dbt, devops, etl, hadoop, nosql, python, snowflake, spark, sql, ssis"
Data Engineer IV - Max Digital (Data Operations),ACV Auctions,"Arizona, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-operations-at-acv-auctions-3762878253,2023-12-17,Arizona,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack (SSIS, C#, Web APIs) outages during our operational support window.
Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required.
Leverage DMVs and monitoring tools to ensure system performance; work with data operations and engineering to improve as required.
Ensure existing HADR (availability groups) solution is functional and meets requirements.
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively. This may include being part of the emergency after-hours on-call rotation.
Mentor junior data engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert in SQL Query optimization
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (e.g., MongoDB)
Experience developing Windows services in C#
Experience writing unit and integration testing
Expert SQL and data-layer development experience; OLTP schema design.
Experience using and integrating with cloud services, specifically: AWS RDS, S3, SQS, SNS.
Nice to Have
Experience with Airflow
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL optimization, NoSQL databases, Cloud services (AWS), ETL tools (SSIS Airflow), Python, C#, Data integration, Software engineering","sql optimization, nosql databases, cloud services aws, etl tools ssis airflow, python, c, data integration, software engineering","c, cloud services aws, data integration, etl tools ssis airflow, nosql databases, python, software engineering, sql optimization"
Principle Data Engineer,Clairvoyant,"Phoenix, AZ",https://www.linkedin.com/jobs/view/principle-data-engineer-at-clairvoyant-3753045320,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Company Overview And Culture
EXL (NASDAQ: EXLS) is a global analytics and digital solutions company that partners with clients to improve business outcomes and unlock growth. Bringing together deep domain expertise with robust data, powerful analytics, cloud, and AI, we create agile, scalable solutions and execute complex operations for the world’s leading corporations in industries including insurance, healthcare, banking and financial services, media, and retail, among others. Focused on creating value from data for driving faster decision-making and transforming operating models, EXL was founded on the core values of innovation, collaboration, excellence, integrity and respect. Headquartered in New York, our team is over 40,000 strong, with more than 50 offices spanning six continents. For information, visit www.exlservice.com .
For the past 20 years, EXL has worked as a strategic partner and won awards in its approach to helping its clients solve business challenges such as digital transformation, improving customer experience, streamlining business operations, taking products to market faster, improving corporate finance, building models to become compliant more quickly with new regulations, turning volumes of data into business opportunities, creating new channels for growth and better adapting to change. The business operates within four business units: Insurance, Health, Analytics, and Emerging businesses.
Required Skills
Python, Pyspark/Spark, SQL, Data Lake (ON Prem or cloud) AWS, Snowflake.
Job Summary
The Data Engineer’s role is to play a lead role in developing a high-performance data platform, integrating data from a variety of internal and external sources, in order to support data and analytics activities. This is a technical role that involves defining changes to the warehouse data model and building scalable and efficient processes to populate or modify warehouse data. The successful candidate will have hands-on data processing and data modeling experience in cloud and on-prem environments.
Responsibilities
Be a technical lead in the development of high-volume platforms to drive decisions and have a tangible beneficial impact on our clients and on business results.
Design and implement efficient data pipelines (ETLs) in order to integrate data from a variety of sources into Data Warehouse.
Design and implement data model changes that align with warehouse standards.
Design and implement backfill or other warehouse data management processes.
Develop and execute testing strategies to ensure high quality warehouse data.
Provide documentation, training, and consulting for data warehouse users.
Perform requirement and data analysis in order to support warehouse project definition.
Provide input and feedback to support continuous improvement in team processes.
Experience in working both, On-Prem and Cloud (AWS preferred).
Responsible for leading the team onsite and off shore with technical leadership and guidance.
Qualifications
5+ years in a Data Engineering role
7+ years hands on experience with SQL:
Ability to write/ interpret SQL and Complex Joins/ Queries
Execution plan and SQL optimization (Oracle SQL Profiler)
o 3+ years coding experience (Python and/or PySpark).
o 3+ years hands on experience with big data and cloud technologies (Snowflake, EMR, Redshift, or similar technologies) is highly preferred
Schema design and architecture on snowflake
Architecture and design experience with AWS cloud
AWS services expertise: S3, RDS, EC2, ETL services (Glue, Kinesis)
Consumption layer design experience for reporting and dashboard
Expert level understanding and experience of ETL fundamentals and building efficient data pipelines.
3+ years at least - Enterprise GitHub – branch, release, DevOps, CI/CD pipeline.
Team player, Strong communication and collaboration skills.
Experience with Agile methodologies.
Master’s Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field.
EEO/Minorities/Females/Vets/Disabilities
To view our total rewards offered click here —>
https://www.exlservice.com/us-careers-and-benefits
Base Salary Range Disclaimer:
The base salary range represents the low and high end of the EXL base salary range for this position. Actual salaries will vary depending on factors including but not limited to: location and experience.
The base salary range listed is just one component of EXL's total compensation package for employees.
Other rewards may include bonuses, as well as a Paid Time Off policy, and many region specific benefits.
Please also note that the data shared through the job application will be stored and processed by EXL in accordance with the EXL Privacy Policy.
Application & Interview Impersonation Warning
– Purposely impersonating another individual when applying and / or participating in an interview in order to obtain employment with EXL Service Holdings, Inc. (the “Company”) for yourself or for the other individual is a crime. We have implemented measures to deter and to uncover such unlawful conduct. If the Company identifies such fraudulent conduct, it will result in, as applicable, the application being rejected, an offer (if made) being rescinded, or termination of employment as well as possible legal action against the impersonator(s).
EXL may use artificial intelligence to create insights on how your candidate information matches the requirements of the job for which you applied. While AI may be used in the recruiting process, all final decisions in the recruiting and hiring process will be taken by the recruiting and hiring teams after considering a candidate’s full profile. As a candidate, you can choose to opt out of this artificial intelligence screening process. Your decision to opt out will not negatively impact your opportunity for employment with EXL.
Show more
Show less","Python, PySpark, Spark, SQL, Data Lake, AWS, Snowflake, Oracle SQL Profiler, DevOps, CI/CD, GitHub, S3, RDS, EC2, Glue, Kinesis, EMR, Redshift","python, pyspark, spark, sql, data lake, aws, snowflake, oracle sql profiler, devops, cicd, github, s3, rds, ec2, glue, kinesis, emr, redshift","aws, cicd, data lake, devops, ec2, emr, github, glue, kinesis, oracle sql profiler, python, rds, redshift, s3, snowflake, spark, sql"
Senior Cloud Data Engineer,BDO USA,"Phoenix, AZ",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765469441,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Machine Learning, AI Algorithms, SQL, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, Azure, AWS, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, UiPath, Alteryx, Computer Vision AI, Professionalism, Autonomy, Communication, Organizational Skills, MultiTasking, Teamwork, DeadlineDriven Environment, People Interaction, Delegation, Relationship Building, Team Environment, Professional Development","data analytics, business intelligence, machine learning, ai algorithms, sql, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, azure, aws, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, uipath, alteryx, computer vision ai, professionalism, autonomy, communication, organizational skills, multitasking, teamwork, deadlinedriven environment, people interaction, delegation, relationship building, team environment, professional development","ai algorithms, alteryx, autonomy, aws, azure, azure analysis services, batch data ingestion, business intelligence, c, cloud data analytics, communication, computer vision ai, data lake medallion architecture, dataanalytics, datamodeling, datawarehouse, deadlinedriven environment, delegation, devops, git, java, linux, machine learning, microsoft fabric, multitasking, organizational skills, people interaction, powerbi, professional development, professionalism, python, relationship building, scala, semantic model definition, sql, star schema construction, streaming data ingestion, tabular modeling, team environment, teamwork, uipath"
Sr Streaming Data Engineer,Discount Tire,"Scottsdale, AZ",https://www.linkedin.com/jobs/view/sr-streaming-data-engineer-at-discount-tire-3731267961,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Overview
Here at Discount Tire, we celebrate the spirit of our people with extraordinary pride and enthusiasm. Our business has been growing for more than 60 years and now is the best time in our history to join us. We recognize that to remain the industry leader we must continue to grow and evolve our business in a rapidly changing industry. We are achieving this, not only by opening new stores, but by transforming our technological landscape and making data a central component of our strategy. The Business Analytics team, one of the fastest growing teams in the company, is leading this change. We are responsible for driving the insights, recommendations, and developing the decision support tools that influence the strategic direction of the company.
Under minimal supervision, the Analytics Senior Streaming Data Engineer provides hands-on technical expertise in designing, building, and implementing analytics solutions that support scalable data solutions in a real-time environment. This role is essential to drive the company's data initiatives, ensuring the successful design and execution of high-performing and robust streaming data systems.
Essential Duties and Responsibilities:
Drives Discount Tire’s real time analytics to support operational insights and decision-making leveraging established engineering disciplines.
Collaborate with source data owners, data engineers, data scientists, and other stakeholders to implement high-velocity data pipelines.
Work with other product team(s) to define data dictionary, data lineage and relationship metrics.
Contributes to the strategic direction and decision making related to Data Warehousing, Big Data and Data analytics in close collaboration with Technical Leadership.
Designs and develops high performance distributed data warehouse, distributed analytic systems and cloud architecture.
Develops, launches and maintains efficient and fault tolerant, batch and streaming, data pipelines (ETL/ELT) to populate databases and object stores from multiple disparate data sources.
Develops complex data calculations through data integration tools and scripting languages.
Designs and implements data quality metrics, standards, guidelines; automates data quality checks / routines as part of data processing frameworks; validates flow of information.
Determines Data Warehousing and Big Data infrastructure needs, including but not limited to, automation of system builds, security requirements, performance requirements and logging/monitoring in collaboration with DevOps engineers.
Troubleshoots complex data and performance related issues; implements adjustments, documents root cause and corrective measure; transfers knowledge to operations support team.
Documents standards, best practices and technical specifications. Facilitates in design and code review sessions and provides feedback and mentorship to peers.
Provides technical assistance to junior data engineers.
Collaborates with broader analytics team and internal IT partners.
Assists employees, vendors or other customers by answering questions related to Data Warehousing and Big Data processes, procedures and services.
Participates in the development of complex cross application architectures in collaboration with cross functional teams.
Implement advanced analytics and data science models and automates complex analysis at scale.
Stays current on the latest industry technologies, trends and strategies.
Completes work in a timely and accurate manner while providing exceptional customer service.
Provides Tier 3 support and create run books to mentor Tier 1 and Tier 2 support staff.
Other duties as assigned.
Qualifications:
This position requires a minimum of five years of progressive streaming data development and integration experience.
Proven understanding of logical and physical data modeling is imperative.
Ability to translate a logical data model into a relational or non-relational solution is necessary.
Understanding of multiple relational (RDMS) and non-relational (NoSQL) data platforms is needed.
Expert level SQL experience is required. Advanced scripting knowledge with SQL, Python, Java or R is necessary.
Advanced experience in SQL tuning, indexing, distribution, partitioning, data access patterns and scaling strategies is needed.
Hands-on / in-depth experience in the following tools, technologies & Concepts such as: SQL (advanced level), Apache Kafka, Apache Flink, Kinesis Data Analytics or related streaming technologies.
Expert experience with data integrations and data processing for business intelligence and analytics workloads is required.
Advanced experience with AWS S3 or other distributed object stores, MPP Data Warehousing, (e.g. AWS Redshift), Elastic MapReduce a plus.
Hands-on experience in database development using views, SQL scripts and transformations is needed.
Proficient with Microsoft office, including skills with Word and Excel, is necessary.
Experience working with large complex data sets.
Understanding of Software Development Life Cycle (SDLC) methodologies such as Agile and Waterfall is needed.
Proven analytical problem solving and decision making skills is critical.
Proven ability to communicate across all levels of the organization is necessary; must be able to clearly articulate technical ideas to a non-technical audience both verbally and in writing.
Ability to work independently and in a team is vital.
Customer service skills including the ability to manage and respond to different customer situations while maintaining a positive and friendly attitude is essential.
The ability to multi-task, and manage multiple projects to meet various deadlines simultaneously is required.
The ability to work efficiently and accurately under pressure, meet deadlines and present a professional demeanor is essential.
In addition troubleshooting and organizational skills with a can-do attitude and the ability to adjust to changing requirements are essential.
Educational Requirements:
This position requires a Bachelors Degree in Computer Science, Computer Information Systems or related or equivalent experience.
Data or cloud related certifications are a plus.
Discount Tire provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local law.
Show more
Show less","Apache Kafka, Apache Flink, Kinesis Data Analytics, SQL, Python, Java, R, AWS S3, MPP Data Warehousing, AWS Redshift, Elastic MapReduce, Agile, Waterfall","apache kafka, apache flink, kinesis data analytics, sql, python, java, r, aws s3, mpp data warehousing, aws redshift, elastic mapreduce, agile, waterfall","agile, apache flink, apache kafka, aws redshift, aws s3, elastic mapreduce, java, kinesis data analytics, mpp data warehousing, python, r, sql, waterfall"
Load Research Data Engineer,Salt River Project,Greater Phoenix Area,https://www.linkedin.com/jobs/view/load-research-data-engineer-at-salt-river-project-3773566584,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Join us in building a better future for Arizona!
SRP is one of the largest public power and water utilities in the U.S. providing electricity to approximately one million customers in the greater metropolitan Phoenix area. Since its founding in 1903, SRP has fostered a culture of stewardship and customer service consistently ranking as an industry leader in customer service according to J.D. Power and named one of Arizona's best employers by Forbes. SRP continues to adapt to its changing business environment by seeking innovative ways to reimagine utility service and the provision of critical resources essential to the life and economy of Arizona.
Why Work at SRP
SRP's success is rooted in our employees' happiness, health, and safety. That's why we offer a comprehensive benefits package to meet the needs of our employees and enhance their well-being. In addition to competitive pay and performance incentives, eligible employees can take advantage of the following benefits:
Pension Plan (at no cost to the employee)
401(k) plan with employer matching
Available your first day: Medical, vision, dental, and life insurance
Over 200+ hours of PTO (includes vacation days, holidays, floating holidays, and sick leave)
Parental leave (up to 4 weeks) and adoption assistance
Wellness programs (including access to a recreation and fitness facility)
Short and long-term disability plans
Tuition assistance for both undergraduate and graduate programs
10 Employee Resource Groups for career development, community service, and networking
Summary
Load Research is one of the primary departments at SRP that interfaces with the advanced metering infrastructure data stream. The high frequency data provided by the advanced meters is used in planning customer programs, predicting customer demand and understanding customer preferences and trends in electricity usage. Current and future metering infrastructure require the high-level data management and analysis capabilities that are offered by Load Research. These needs will continue to grow in the future as advanced distribution/load management concepts are put into practice.
This position serves as the primary database, server, and data process architect for the business unit. Additionally provides IT systems administration, software, and information custodian support.
What You'll Do:
Maintain and develop databases for customer meter data along with the tools SRP employees use to access and manipulate these data.
Developing, deploying and monitoring recurring database jobs using Windows job scheduler, Batch scripting, SQL queries, SAS scripts, and similar tools
Develop subject matter expertise related to SRP’s existing database infrastructure, which includes SQL servers, SAS servers, cloud computing servers, Hadoop, Snowflake, and other similar technologies.
Innovate, plan, and execute process improvements for database structure and data process workflows, including robotic process automation.
Customer usage reports for internal and external clients.
Support Forecasting and Load Research with database and user administration, including administration of Itron’s MV-90 software product.
Manage large data sets in an organized manner with effective quality control and supporting documentation.
Interfaces with other internal groups on process improvements and resolving issues, including root cause analysis.
Occasional training of staff and quality control of outputs.
Exhibits high-level data security, an understanding of sensitive data protocols and service as an information custodian for the department.
What it Takes to Succeed:
Promotion to Level 2 requires a minimum of two years experience at Level 1 and demonstrated capability to perform advanced and more difficult work as determined by the supervisor.
Promotion to Senior Level requires a minimum of 3 years experience at Level 2, is fully competent in all aspects of functional area of assignment and as such would be recognized as a specialist in area of assignment and may have periodic or occasional lead responsibilities.
Professional Qualifications:
T-SQL expertise is strongly preferred.
Experience using Python, Excel, Visual Basic, Batch scripting, Windows job scheduler, SAS statistical software, MS SQL Server database design and performance tuning, and working knowledge with Oracle and DB2 is preferred.
Knowledge and experience in the electric utility industry is preferred, but not required.
A degree in a related field is preferred, but not required.
Education
Completion of a Bachelor's Degree from an accredited institution that prepares the employee for the assignment.
Hybrid Workplace
SRP currently offers a hybrid workplace, which allows employees whose jobs can be performed remotely, and who have sufficient technical capability, to telework up to three days per week. Although teleworking is available, all employees must live and work in Arizona. We are taking steps to protect the health and well-being of all team members, and by following a number of health and safety protocols, to reduce the risk of the coronavirus (COVID-19).
Equal Opportunity Employer Statement
Salt River Project (SRP) recognizes diversity and inclusion as key drivers of innovation and growth and seeks to attract a diverse employee base that reflects our community. We are committed to equal employment opportunity regardless of race, color, religion, sex (including pregnancy), gender identity, sexual orientation, national origin, age, disability, genetic information, military status or any other protected status under applicable federal, state or local law. Ultimately, SRP aspires to fully apply the power of diversity and inclusion to build a more equitable and sustainable future for our customers, employees and community.
Drug/Alcohol Policy Statement
In order to promote the safety and well-being of our employees, customers and the communities we serve, SRP is committed to maintaining a drug/alcohol free work environment. Although marijuana may now be legal in Arizona, except as otherwise specified under Arizona law, SRP considers it to be an illegal drug for the purpose of our drug/alcohol policy because marijuana remains illegal at the federal level. Any candidate found to be impaired during the hiring process or who has the presence of an illegal drug or unauthorized substance in their system during the pre-employment drug/alcohol test may be disqualified from further consideration in the hiring process.
Work Authorization
All candidates must be legally authorized to work in the United States.
Currently, SRP does not sponsor H1B visas.
Show more
Show less","TSQL, Python, Excel, Visual Basic, Batch scripting, Windows job scheduler, SAS, Hadoop, Snowflake, SQL, Oracle, DB2, Database administration, SAS statistical software, MS SQL Server, Data management, Data analysis, Robotics process automation, Data security, Data manipulation, Data process workflow, Database design, Database performance tuning, Data quality control, Troubleshooting, Root cause analysis, Training, Itron MV90","tsql, python, excel, visual basic, batch scripting, windows job scheduler, sas, hadoop, snowflake, sql, oracle, db2, database administration, sas statistical software, ms sql server, data management, data analysis, robotics process automation, data security, data manipulation, data process workflow, database design, database performance tuning, data quality control, troubleshooting, root cause analysis, training, itron mv90","batch scripting, data management, data manipulation, data process workflow, data quality control, data security, dataanalytics, database administration, database design, database performance tuning, db2, excel, hadoop, itron mv90, ms sql server, oracle, python, robotics process automation, root cause analysis, sas, sas statistical software, snowflake, sql, training, troubleshooting, tsql, visual basic, windows job scheduler"
"Lead ETL Data Analyst :: Charlotte , NC :: Contract","Anveta, Inc",Greater Phoenix Area,https://www.linkedin.com/jobs/view/lead-etl-data-analyst-charlotte-nc-contract-at-anveta-inc-3689420697,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Position Title:
Lead ETL Data Analyst
Total Openings:
2
Start Date:
2-3 weeks from the day of offer
Interview process:
1 round 1 hour zoom interview
APPROVED LOCATIONS: in order of Preference
Charlotte Research Drive , MN and AZ
3 Days Hybrid and Wednesday in office is mandatory
Mission
Consolidation of HR data for leadership review.
Must have
9+ years of exp. This role is technical and requires high level of expertise with ETL Data Analysis.
Prior ETL development exp is highly preferred.
Advance SQL Skills Will Be Required.
Informatica
Data warehousing experience a MUST.
Exp with Data integration
Source to Target Mapping
Creating Data Models ( Strong exp needed )
Adv Excel
Preferable but not Mandatory
Prior HR exp
Exp with BI tools
Thanks & Regards
Raj Kumar Pandey
IT Recruiter
Email: raj.kumar@anveta.com | URL: http://www.anveta.com
Address: 1333 Corporate Drive, Suite #108 Irving, TX 75038, USA
ANVETA, Inc
Show more
Show less","ETL, Data Analysis, SQL, Informatica, Data Warehousing, Data Integration, Source to Target Mapping, Data Modeling, Excel, BI Tools","etl, data analysis, sql, informatica, data warehousing, data integration, source to target mapping, data modeling, excel, bi tools","bi tools, data integration, dataanalytics, datamodeling, datawarehouse, etl, excel, informatica, source to target mapping, sql"
"Staff Engineer - Data Analytics/Cloud, Paze",Early Warning®,"Scottsdale, AZ",https://www.linkedin.com/jobs/view/staff-engineer-data-analytics-cloud-paze-at-early-warning%C2%AE-3734190399,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Come build the next-gen fintech at Early Warning, network operator of Zelle®, where we’re relentlessly focused on empowering prosperity in all its forms.
From fast money movement for over 100 million people who can access Zelle® directly through their banking app to new account opening and beyond – we make a difference in the lives of consumers and businesses every day and enable them to live their best financial lives. And we’re only getting started.
With new state-of the-art offices in Scottsdale, AZ (Headquarters) and Chicago, IL – plus a growing presence in San Francisco – we’re entering our next big chapter.
People matter to us, so we think our best work is done when we’re together, in-person. From informal interactions, to sharing ideas, to mentoring and beyond. We also believe in workplace flexibility and empowering teams to determine the rhythm of work to create an inclusive culture. Through the power of innovative collaboration, we offer hybrid and (when necessary) virtual (“remote”) workplace models.
We focus all hiring efforts in a few states and within driving distance of an office location to enable in-person collaboration when and where possible. Priority hiring locations are Scottsdale, AZ, Illinois, NY tri-state metro area (New Jersey, New York and Connecticut) and San Francisco, CA. On exception we will hire in secondary locations such as District of Columbia, Florida, Georgia, Maryland, Nevada, North Carolina, South Carolina, Texas and Virginia. We are not actively recruiting in Colorado, Rhode Island or Washington.
Join us and make your mark on what’s next in fintech.
Applicants must be authorized to work for any employer in the United States. We are unable to sponsor an employment Visa for this position.
Overall Purpose
This position is a key role in the development, test, and deployment of complex solutions.
Essential Functions
Build data strategy for broad or complex requirements with insightful and forward-looking approaches that go beyond the direct team and solve large open-ended problems.
Participate in the strategic development of methods, techniques, and evaluation criteria for projects and programs.
Drive all aspects of technical and data architecture, design, prototyping and implementation in support of both product needs as well as overall technology data strategy.
Provide leadership and technical expertise in support of building a technical plan and backlog of stories, and then follow through on execution of design and build process through to production delivery.
Guide a broad functional area and lead efforts through the functional team members along with the team’s overall planning.
Represent engineering in cross-functional team sessions and able to present sound and thoughtful arguments to persuade others. Adapts to the situation and can draw from a range of strategies to influence people in a way that results in agreement or behavior change.
Collaborate and partner with product managers, designers, and other engineering groups to conceptualize and build new features and create product descriptions.
Actively own features or systems and define their long-term health, while also improving the health of surrounding systems.
Assist Support and Operations teams in identifying and quickly resolving production issues.
Develop and implement tests for ensuring the quality, performance, and scalability of our application.
Actively seek out ways to improve engineering and data standards, tooling, and processes.
Supporting the company’s commitment to risk management and protecting the integrity and confidentiality of systems and data.
Minimum Qualifications
Education and/or experience typically obtained through a Bachelor’s degree in computer science or related technical field.
Ten or more years of relevant related experience
Seven or more years of experience in the development of complex data platform, distributed systems, SaaS, cloud solutions, micro services.
Six or more years of experience in the development of Data Warehouse, Big Data – structured & unstructured platforms, real-time & batch processing, data standards.
Demonstrated experience in delivering business-critical systems to the market.
Ability to influence and work in a collaborative team environment.
Experience designing/developing scalable systems.
Extensive experience implementing Data Warehouse (Star / Snow flake schemas) using SQL Server or equivalent, Big Data – HDFS, Elastic Search, ETL process development using IBM Infosphere or equivalent, Reusable Frameworks
Experience with event-driven architecture and messaging frameworks (Pub/Sub, Kafka, RabbitMQ, etc).
Working experience with cloud infrastructure (Google Cloud Platform, AWS, Azure, etc).
Knowledge of mature engineering practices (CI/CD, testing, secure coding, etc).
Knowledge of Software Development Lifecycle (SDLC) best practices, software development methodologies (Agile, Scrum, LEAN etc) and DevOps practices.
Background and drug screen.
Preferred Qualifications
Large Scale Cloud Data Lake/ Warehouse transformation
AWS Cloud experience in Streaming, Batch Data Management
Redshift or other large warehouse Management experience
Advanced SQL and Analytics
BI tools like tableau, Quicksight
Data Governance and Quality Controls
Computer language experience (Python, PySpark, and R)
Monitoring and Alerting systems experience (AppDynamics, Splunk etc.)
AWS certification in Solution Architecture/ Analytics
FinTech experience
Kubernetes experience
Physical Requirements
Working conditions consist of a normal office environment. Work is primarily sedentary and requires extensive use of a computer and involves sitting for periods of approximately four hours. Work may require occasional standing, walking, kneeling and reaching. Must be able to lift 10 pounds occasionally and/or negligible amount of force frequently. Requires visual acuity and dexterity to view, prepare, and manipulate documents and office equipment including personal computers. Requires the ability to communicate with internal and/or external customers.
Employee must be able to perform essential functions and physical requirements of position with or without reasonable accommodation.
The above job description is not intended to be an all-inclusive list of duties and standards of the position.
The Pay Scale For This Position In
Phoenix, AZ/ Chicago, IL in USD per year is: $145,000 - $155,000.
New York, NY/ San Francisco, CA in USD per year is: $150,000 - $180,000.
This pay scale is subject to change and is not necessarily reflective of actual compensation that may be earned, nor a promise of any specific pay for any specific candidate, which is always dependent on legitimate factors considered at the time of job offer. Early Warning Services takes into consideration a variety of factors when determining a competitive salary offer, including, but not limited to, the job scope, market rates and geographic location of a position, candidate’s education, experience, training, and specialized skills or certification(s) in relation to the job requirements and compared with internal equity (peers). The business actively supports and reviews wage equity to ensure that pay decisions are not based on gender, race, national origin, or any other protected classes.
Additionally, candidates are eligible for a discretionary bonus, and benefits.
Some of the Ways We Prioritize Your Health and Happiness
Healthcare Coverage – Competitive medical (PPO/HDHP), dental, and vision plans as well as company contributions to your Health Savings Account (HSA) or pre-tax savings through flexible spending accounts (FSA) for commuting, health & dependent care expenses.
401(k) Retirement Plan – Featuring a 100% Company Safe Harbor Match on your first 6% deferral immediately upon eligibility.
Paid Time Off – Unlimited Time Off for Exempt (salaried) employees, as well as generous PTO for Non-Exempt (hourly) employees, plus 11 paid company holidays and a paid volunteer day.
12 weeks of Paid Parental Leave
Maven Family Planning – provides support through your Parenting journey including egg freezing, fertility, adoption, surrogacy, pregnancy, postpartum, early pediatrics, and returning to work.
And SO much more! We continue to enhance our program, so be sure to check our Benefits page here for the latest. Our team can share more during the interview process!
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
Early Warning Services, LLC (“Early Warning”) considers for employment, hires, retains and promotes qualified candidates on the basis of ability, potential, and valid qualifications without regard to race, religious creed, religion, color, sex, sexual orientation, genetic information, gender, gender identity, gender expression, age, national origin, ancestry, citizenship, protected veteran or disability status or any factor prohibited by law, and as such affirms in policy and practice to support and promote equal employment opportunity and affirmative action, in accordance with all applicable federal, state, and municipal laws. The company also prohibits discrimination on other bases such as medical condition, marital status or any other factor that is irrelevant to the performance of our employees.
Show more
Show less","Data Strategy, Agile, DevOps, SQL, HDFS, Elastic Search, IBM Infosphere, Kafka, Google Cloud Platform, AWS, Azure, ETL, Tableau, Quicksight, Python, PySpark, R, Kubernetes","data strategy, agile, devops, sql, hdfs, elastic search, ibm infosphere, kafka, google cloud platform, aws, azure, etl, tableau, quicksight, python, pyspark, r, kubernetes","agile, aws, azure, data strategy, devops, elastic search, etl, google cloud platform, hdfs, ibm infosphere, kafka, kubernetes, python, quicksight, r, spark, sql, tableau"
Database Systems Engineer,Universal Avionics,"Tucson, AZ",https://www.linkedin.com/jobs/view/database-systems-engineer-at-universal-avionics-3715416677,2023-12-17,Arizona,United States,Mid senior,Hybrid,"Not many people can say they work in an industry that makes air travel safer, efficient, and more reliable. At Universal Avionics - we can! Here at UA, you will be part of a company that's leading the future of aviation to ensure easier management and safety & reliability of all phases of flight.
As a Database Systems Engineer, you will be a member of an integrated team responsible for life-cycle development of aeronautical databases to be installed in onboard avionic systems and ensuring databases are processed and delivered on time.
Minimum Qualifications
Bachelor’s degree (B.S.) in science/engineering, or a math discipline, or instrument rated pilot, or prior work experience with aeronautical data or geospatial data.
Proficient with a personal computer, word processing, and spreadsheet programs and ability to type.
Ability to write professional correspondence and communicate information in English, both written and verbal.
Good overall output level within established schedules
Good precision, clarity, and thoroughness in work
Ability to logically and systematically detect and troubleshoot defects.
Desired Education and/or Experience
Experience developing aeronautical, terrain, or obstacle databases compliant with RTCA DO-200A/B.
Knowledge of ARINC 424, ARINC 816, RTCA DO-201A, RTCA-272C, RCTA DO-276B, RTCA DO-291B, or AC 20-153A.
Knowledge of or experience with airport and runway design and terminology.
Knowledge of or experience with Geographic Information Systems (GIS), geospatial data, aeronautical data, or avionics.
Experience with one or more high-level programming languages, including C or C++
Essential Duties and Responsibilities
Capture and allocate requirements from customer and industry documents to ensure database content meets the quality requirement for its intended use by the customer.
Develop and revise content for the data quality requirements documents and the aeronautical database tool qualification test procedures.
Develop and revise content for the aeronautical database compliance matrix report(s).
Develop and revise content for the aeronautical database compliance plan, and the aeronautical database processing procedures.
Resolve documentation review comments and findings.
Perform aeronautical database tool qualification testing and take action to resolve test discrepancies.
Perform and support aeronautical database processing and resolve processing defects.
Analyze and update database content to maintain compatibility with onboard avionic systems.
For aeronautical database products, administer required regulatory notifications, support internal compliance audits, and perform periodic and event-driven reviews.
Address product support and field service technical questions.
Participate in product team meetings.
Report milestone progress to senior systems engineers, managers, and program management.
Provide inputs to the problem report authors and investigate simple problem reports.
Take ownership of the products to be worked, tasks, quality, and project goals & milestones.
Proactive collaboration with cross-functional groups within the development teams, work alignment towards project goals, effective communication, and promoting a positive work environment.
When necessary, support RTCA/DO-200 tool, requirements test plan, procedures, testing, verification reports and results.
What motivates our team:
Excellent benefits, including medical, vision, dental, PTO, 401k, and much more, as well as a robust health & wellness program.
Strong life-work balance
Great company culture
Groundbreaking work opportunities
Impactful innovation in the avionics industry
Passionate & dedicated team members
A work environment that fosters training, professional development, and career advancement
Why you’ll like working here:
Universal Avionics is a mid-sized company with all the functions and benefits of a much larger organization, so you get the best of both worlds.
You’ll make an impact! You’ll be part of a company that is making huge strides in the safety and reliability of air travel.
You’ll work with very motivated and skilled people who are excellent at what they do and are great mentors.
You’ll bring your own unique skills set and passion to create a mutually beneficial working and learning environment.
At UASC, we care about our employees, both personally and professionally.
About Universal Avionics:
Universal Avionics and its parent company, Elbit Systems, Ltd., represent decades of leadership and innovation as developers and suppliers of Head-Down Displays (HDD), Head-Up Displays (HUD), Head Wearable Displays (HWD), and Combined Vision Systems (CVS), which includes both our Enhanced Vision System (EVS) and Synthetic Vision System (SVS). Universal Avionics is also a leading manufacturer of innovative commercial avionics systems, offering retrofit and forward-fit solutions for the largest diversification of aircraft types in the industry.
To learn more about Universal Avionics, visit www.UniversalAvionics.com
Show more
Show less","Aeronautical databases, ARINC 424, ARINC 816, RTCA DO200A/B, RTCA DO201A, RTCA272C, RCTA DO276B, RTCA DO291B, AC 20153A, Geographic Information Systems (GIS), C, C++","aeronautical databases, arinc 424, arinc 816, rtca do200ab, rtca do201a, rtca272c, rcta do276b, rtca do291b, ac 20153a, geographic information systems gis, c, c","ac 20153a, aeronautical databases, arinc 424, arinc 816, c, geographic information systems gis, rcta do276b, rtca do200ab, rtca do201a, rtca do291b, rtca272c"
CO. Seeks Data Analyst (promotable to Financial Analyst),LHH,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/co-seeks-data-analyst-promotable-to-financial-analyst-at-lhh-3788841458,2023-12-17,Cranford,United States,Associate,Hybrid,"Salary” $70,000 - $80,000
PLEASE NOTE: 2 DAYS IN OFFICE (NYC)
· A $300 million company with multiple entities is seeking a Financial Analyst with strong data analytics. The candidate MUST possess a BA in either Accounting or Finance and a minimum of 1 years of relevant experience working with large pools of data. The candidate will have the opportunity to work with various business intelligence applications and customize and standardize reporting. The responsibilities will be broken out 50/50. 50% of the responsibilities will include reviewing the operations of various lines of businesses and developing metrics to measure efficiency and potential opportunities for improvements. On the financial level, the candidate will assist in the development of budgets at a cross functional level (multiple departments), perform variance analysis, develop and analyze metrics and work with large pools of data. The manager takes a lot of pride in mentoring and developing staff. There is tremendous opportunity within the department as well as transferring to other departments. For immediate consideration, please email your resume in a Word document to ken.casaccio@lhh.com
Show more
Show less","Financial Analysis, Data Analytics, BA in Accounting or Finance, Business Intelligence Applications, Variance Analysis, Budgeting, Crossfunctional Collaboration, Data Management","financial analysis, data analytics, ba in accounting or finance, business intelligence applications, variance analysis, budgeting, crossfunctional collaboration, data management","ba in accounting or finance, budgeting, business intelligence applications, crossfunctional collaboration, data management, dataanalytics, financial analysis, variance analysis"
Mid Level Data Steward (1014383),The Judge Group,"Creve Coeur, MO",https://www.linkedin.com/jobs/view/mid-level-data-steward-1014383-at-the-judge-group-3751628194,2023-12-17,Saint Louis,United States,Mid senior,Hybrid,"Location:
Creve Coeur, MO
Salary:
Depends on Experience
Description:
Our client is currently seeking a
Mid-Level Data Steward- Saint Louis, MO
Job Description-
The job involves analyzing and documenting the current SQL scripts flow and assisting in daily support.
We are looking for a Expert SQL candidate
Education And Work Experience Requirements
7+ years with RDBMS, TD (Teradata) and GCP big Query is preferred.
Responsibilities
Analyze/reverse engineer existing SQL code from stored procedures, scripts, and document flow to create mapping documents
Understand and map source data fields from custodial and market data source.
Gather and document requirements for future system enhancement working with both the business and core systems teams.
Send status reports on the activities planned vs completed
Prepare business scenarios and UAT test cases
Support of Development, SIT and UAT teams on requirement clarification.
Work closely with the engineering team to identify, troubleshoot, and resolve issues.
Contact:
sshriyam@judge.com
This job and many more are available through The Judge Group. Find us on the web at www.judge.com
Show more
Show less","SQL, Teradata, Google Big Query, RDBMS","sql, teradata, google big query, rdbms","google big query, rdbms, sql, teradata"
Data Engineer,ShortList Recruitment Limited,"Chester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-shortlist-recruitment-limited-3776623412,2023-12-17,Warrington, United Kingdom,Associate,Hybrid,"Data Engineer
£45,000 DOE
Chester
ShortList Recruitment are working with a leading client based in Chester who are recruiting for a Data Engineer to join their growing team.
The successful candidate will ensure the data warehouse is maintained appropriately, scheduled jobs run without error and are delivered within SLA.
The role will primarily support the business with SAS, being used as the primary tool for analytics, data mining, data warehousing and production of MI/BI.
Previous commercial experience in a data engineering/development role would be desirable, but is not essential, as our client will happily considered highly-motivated, talented graduates will a relevant degree to software engineering or data analytics.
Nice to have sills for the Junior Data Engineer role:
Degree educated in a data-related or software engineering related subject
2 years of commercial experience in a similar role
SAS understanding
SQL
Understanding of ETL
Python
Attention to detail
Experience/ interest in mentoring juniors.
In return, the Junior Data Engineer can expected a salary up to £45,000 DOE + benefits.
The role will be based in the Chester offices, but will be on a hybrid working model with the flexibility to work from home 2-3 days per week.
The role is commutable from Birkenhead, Deeside, Liverpool, Runcorn, Warrington and Widnes. To apply, click here!
Show more
Show less","Data Engineering, SAS, SQL, ETL, Python, Data Warehousing, Analytics, Data Mining, Mentoring, Datarelated degree, Software Engineering degree","data engineering, sas, sql, etl, python, data warehousing, analytics, data mining, mentoring, datarelated degree, software engineering degree","analytics, data engineering, data mining, datarelated degree, datawarehouse, etl, mentoring, python, sas, software engineering degree, sql"
Data Insight Analyst,K3 Capital Group,"Bolton, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-insight-analyst-at-k3-capital-group-3778733688,2023-12-17,Warrington, United Kingdom,Associate,Hybrid,"Due to business growth and planned expansion, we have the immediate need to hire an experienced Data Insight Analyst. The successful candidate will be tasked with providing insights that will guide decision-making around driving data for sales, marketing, and other key areas of the business. You will be comfortable handling large data sets - identifying and highlighting data integrity issues, being aware of specific trends in data and defining solutions to address data lead issues.
A key part of our growth strategy is to enrich our database in the most effective way possible, meaning data acquisition and strategy is always at the forefront of what we do. This role is perfect for someone who loves all things data, but also enjoys dealing with Suppliers, negotiating costs, T&C's and building relationships with both internal colleagues and external providers to ensure we are benefiting from the best, the most accurate and insightful data on the market, to enable future growth and success across our Group.
The Role:
Negotiating contracts with new Data Suppliers and to be constantly reviewing the quality and integrity of data supplied by existing Suppliers, reviewing and updating if necessary, ensuring we are constantly striving for the best data to support our business growth.
Designing, developing, and delivering data and analytical solutions to increase overall business sales.
Analysing data, deriving insights, and making meaningful suggestions to improve the database.
Help to segment and breakdown our data for more profiled and targeted outreach.
Implement solutions and optimise current automated data cleansing processes.
Key Skills:
Data Analysis and visualisation
Knowledge of Power BI
SQL / T-SQL skills
Good knowledge of data segmentation
Strategic minded, with the ability to successful liaise with Suppliers, negotiate terms etc
If you have previous experience of B2B data that would be a bonus
Knowledge of data protection and GDPR - though training is provided
What You'll Need To Succeed
You will be a highly motivated and ambitious individual with an inquisitive nature. You will be someone who always thinks outside the box, and to be fully immersed in the business. This is a key role in our success and as such has a high profile within the Head Office, where you will be given direct access to key decision makers across our Group and external suppliers, so someone who has a confident, charismatic character would be very welcome. We want your fantastic technical skills, married with brilliant communication. This is a really exciting role, where you will be able to make a marked impact.
Show more
Show less","Data Analysis, Data Visualization, Power BI, SQL / TSQL, Data Segmentation, Data Protection, GDPR, Data Cleansing, Data Acquisition, Negotiation, Data Suppliers, Data Integrity, Data Solutions","data analysis, data visualization, power bi, sql tsql, data segmentation, data protection, gdpr, data cleansing, data acquisition, negotiation, data suppliers, data integrity, data solutions","data acquisition, data integrity, data protection, data segmentation, data solutions, data suppliers, dataanalytics, datacleaning, gdpr, negotiation, powerbi, sql tsql, visualization"
"Senior Business Data Analyst, Revenue Cycle *Remote*",Providence Health & Services,"Liberty Lake, WA",https://www.linkedin.com/jobs/view/senior-business-data-analyst-revenue-cycle-remote-at-providence-health-services-3756143881,2023-12-17,Coeur d’Alene,United States,Mid senior,Remote,"Description
Providence St. Joseph Health is calling a Senior Business Data Analyst, Revenue Cycle to work remotely within our footprint states: AK, CA, MT, OR, TX and/or WA.
This position will be a contributor to the RCS Analytics Operations team which supports efforts of the Data&Insights team. The role (and team) supports the RCS Analytics team, leadership, and its stakeholders/customers. This position is seen as an accountable Business Analyst Sr for analytics-related activities. This role also supports various administrative and customer facing activities including, but not limited to intake/triage management, artifact management, policies&procedures, effort/time estimation, product training, Data Governance&Evangelism, inventory review with customers/leadership, resource onboarding, analytics ecosystem management, etc. The role may also serve in cross functional capacity on projects and initiatives.
Providence caregivers are not simply valued – they’re invaluable. Join our team at Revenue Cycle Business Services and thrive in our culture of patient-focused, whole-person care built on understanding, commitment, and mutual respect. Your voice matters here, because we know that to inspire and retain the best people, we must empower them.
Required qualifications:
Bachelor's Degree Computer science, mathematics, statistics, information technology, health informatics, public health, or related subject. -Or-
Bachelor's Degree Data Science, Analytics, Finance, Engineering, Mathematics, Computer Science, Business Administration, Health Administration, Public Health, Public Administration, or related discipline
2 – 3 years Revenue Cycle Experience
5 years Strong SQL programming skills. Some combination of experience with: Snowflake, SQL Server, Oracle SQL or similar platforms.
5 years Progressive experience in practicing analytics as a data analyst, report analyst, business analyst, BI developer, etc.
2 year's Experience with two or more provider-based Revenue Cycle vert
3 years Proven experience creating, managing, and validating large data sets.
Demonstrated PC skills, i.e., proficient with MS Office products including Word, Excel, Teams, SharePoint, Power Point and Outlook
Preferred qualifications:
Master's Degree Computer science, mathematics, statistics, information technology, health informatics, public health, or related subject.
Experience with EPIC: reporting data out of Epic, reviewing Revenue Cycle related account detail.
Report writing skills in Tableau or Power BI
Salary Range by location:
NorCal (Napa, Sonoma)
Min: $42.53 Max: $68.67
Southern California, NorCal (Humboldt) Alaska (Kodiak, Seward, Valdez)
Min: $37.91, Max: $61.20
WA Puget Sound Oregon (Portland) Alaska (Anchorage)
Min: $36.37, Max: $58.72
Oregon (Hood River, Medford, Seaside)
Min: $32.90, Max: $54.74
Eastern Washington (Richland, Spokane, Walla Walla)
Min: $32.36, Max: $52.25
Montana
Min:$29.28, Max: $47.27
Texas
Min:$27.74, Max: $44.78
Why Join Providence?
Our best-in-class benefits are uniquely designed to support you and your family in staying well, growing professionally, and achieving financial security. We take care of you, so you can focus on delivering our Mission of caring for everyone, especially the most vulnerable in our communities.
About Providence
At Providence, our strength lies in Our Promise of “Know me, care for me, ease my way.” Working at our family of organizations means that regardless of your role, we’ll walk alongside you in your career, supporting you so you can support others. We provide best-in-class benefits and we foster an inclusive workplace where diversity is valued, and everyone is essential, heard and respected. Together, our 120,000 caregivers (all employees) serve in over 50 hospitals, over 1,000 clinics and a full range of health and social services across Alaska, California, Montana, New Mexico, Oregon, Texas and Washington. As a comprehensive health care organization, we are serving more people, advancing best practices and continuing our more than 100-year tradition of serving the poor and vulnerable.
The amounts listed are the base pay range; additional compensation may be available for this role, such as shift differentials, standby/on-call, overtime, premiums, extra shift incentives, or bonus opportunities.
Check out our benefits page for more information about our Benefits and Rewards.
About The Team
Providence Shared Services is a service line within Providence that provides a variety of functional and system support services for our family of organizations across Alaska, California, Montana, New Mexico, Oregon, Texas and Washington. We are focused on supporting our Mission by delivering a robust foundation of services and sharing of specialized expertise.
We are committed to the principle that every workforce member has the right to work in surroundings that are free from all forms of unlawful discrimination and harassment.
We are committed to cultural diversity and equal employment for all individuals. It is our policy to recruit, hire, promote, compensate, transfer, train, retain, terminate, and make all other employment-related decisions without regard to race, color, religious creed (including religious dress and grooming practices), national origin (including certain language use restrictions), ancestry, disability (mental and physical including HIV and AIDS), medical condition (including cancer and genetic characteristics), genetic information, marital status, age, sex (which includes pregnancy, childbirth, breastfeeding and related medical conditions), gender, gender identity, gender expression, sexual orientation, genetic information, and military and veteran status or any other applicable legally protected status. We will also provide reasonable accommodation to known physical or mental limitations of an otherwise qualified caregiver or applicant for employment, unless the accommodation would impose undue hardship on the operation of our business.
We are a community where all people, regardless of differences, are welcome, secure, and valued. We value respect, appreciation, collaboration, diversity, and a shared commitment to serving our communities. We expect that all workforce members in our community will act in ways which reflect a commitment to and accountability for, racial and social justice and equality in the workplace. As such, we will maintain a workplace free of discrimination and harassment based on any applicable legally protected status. We also expect that all workforce members will maintain a positive workplace free from any unacceptable conduct which creates an intimidating, hostile, or offensive work environment.
Requsition ID:
224357
Company:
Providence Jobs
Job Category:
Strategy&Planning
Job Function:
Administration
Job Schedule:
Full time
Job Shift:
Career Track:
Department:
4001 SS RC RPTNG ANALYTICS
Address:
WA Liberty Lake 24001 E Mission Ave
Work Location:
Liberty Lake-Liberty Lake
Pay Range:
$See posting - $See posting
The amounts listed are the base pay range; additional compensation may be available for this role, such as shift differentials, standby/on-call, overtime, premiums, extra shift incentives, or bonus opportunities.
Check out our benefits page for more information about our Benefits and Rewards.
Providence is proud to be an Equal Opportunity Employer. Providence does not discriminate on the basis of race, color, gender, disability, veteran, military status, religion, age, creed, national origin, sexual identity or expression, sexual orientation, marital status, genetic information, or any other basis prohibited by local, state, or federal law.
Show more
Show less","SQL, Snowflake, SQL Server, Oracle SQL, Data Analytics, Business Analytics, BI Development, Tableau, Power BI, MS Office, MS Word, MS Excel, MS Teams, SharePoint, MS PowerPoint, MS Outlook","sql, snowflake, sql server, oracle sql, data analytics, business analytics, bi development, tableau, power bi, ms office, ms word, ms excel, ms teams, sharepoint, ms powerpoint, ms outlook","bi development, business analytics, dataanalytics, ms excel, ms office, ms outlook, ms powerpoint, ms teams, ms word, oracle sql, powerbi, sharepoint, snowflake, sql, sql server, tableau"
Data Analyst,Honor Community Health,"Pontiac, MI",https://www.linkedin.com/jobs/view/data-analyst-at-honor-community-health-3787744248,2023-12-17,Macomb,United States,Mid senior,Onsite,"Honor Community Health is a 501c3 Federally Qualified Health Center co-located within 20 locations. Our mission is to provide for the health and wellness needs of the underserved of Oakland County through the provision of comprehensive, integrated primary, behavioral health, and dental care. We serve all populations regardless of their ability to pay. Our team is passionate about serving the people of Oakland County.
NOTE: All employees are required to receive the COVID-19 Vaccine
Position Description
This position will spend 100% of their time developing and implementing data collection methods and systems in conjunction with the Project Director and Project Evaluator. This role includes training on data collection, reporting on trends, communication, and information dissemination regarding the needed areas of improvement, and fulfilling the data requirements of the CCBHC grant.
Essential Functions
Assist in the design, implementation, and maintenance of standard internal and external (public) dashboards For the CCBHC project.
Respond to queries and data analysis requests by internal and external stakeholders with regards to the CCBHC project.
Design reports to identify patterns and trends.
Prepare reports that effectively communicate trends and patterns to the executive team.
Identify opportunities and support the development of automated solutions to enhance the quality of organizational data.
Assist with creating data quality metrics for identifying data related issues.
Identify problematic areas and conduct research to determine the best course of action to correct the data, identify, analyze and interpret trends and patterns in complex datasets.
Assist with the development of survey instruments and assessments for primary data collection.
Provide support for Health Informatics system management including resolving reported internal issues and coordinating vendor support.
Performs other duties as assigned. During a public health emergency, the employee may be required to perform duties similar to but not limited to those in the job description.
What are we looking for?
Minimum of an associate degree in health-related field, public health, or equivalent experience (bachelors’ or master’s degree preferred).
Relevant experience in public health and health care data analysis.
Relevant experience conducting program evaluation.
Experience with relational databases and knowledge of query tools.
Ability to present population health and health care data in a manner that supports decision-making.
Proficiency in Access. Advanced Microsoft Excel skills.
Proficiency with Electronic Health systems including NextGen.
Excellent verbal and written communication.
A flexible and positive attitude
Ability to work in a fast-paced environment
Creating an excellent patient experience
Patient focused mindset
What do we Offer?
Competitive Wages
401k with company match
Medical, Dental and Vision insurance
Employer Paid Life Insurance
Employer Paid Short-Term and Long-Term Disability Insurance
4-6 Weeks Paid Time Off for Full-Time Employees
Paid Time off for Part-Time Employees
Paid Diversity Day and Holidays
Paid Bereavement Leave
Parental Leave
LifeMart Discount Program for all employees
Continuing Medical Education (CME) Allowance for Clinicians
Free Malpractice Insurance
License Reimbursement
Free Employee Assistance Program
Public Service Loan Forgiveness to all Full-Time Employees
National Health Service Corp (NHSC) for licensed health care providers and offers up to $50,000 toward student loans
Why work for Honor?
Honor is a patient directed and community driven Federally Qualified Health Center.
We ensure that patients come first by providing enabling services to assist patients with specific barriers to care such as transportation, translation, food security, and other social services.
Our staff is passionate about providing all members of the community with access to quality health care regardless of their situation.
Please note the selected candidate will be required to submit to a criminal record check and reference check.
Our Commitment to Diversity, Equity and Inclusion
The diversity of our people and patients is one of our greatest strengths, and inclusive workplace enables us to embrace that diversity to deliver the best services to our employee and patients. Honor Community Health is an Equal Opportunity Employer. This company does not and will not discriminate in employment and personnel practices on the basis of race, sex, age, handicap, religion, national origin or any other basis prohibited by applicable law. Hiring, transferring and promotion practices are performed without regard to the above listed items.
Powered by JazzHR
a9xplcLGOl
Show more
Show less","Databases, Access, Advanced Microsoft Excel, NextGen, Electronic Health systems, Health Informatics system, Survey instruments, Data quality metrics, Healthrelated field, Public health, Health care data analysis, Program evaluation, Relational databases, Query tools, Population health, Patient focused mindset","databases, access, advanced microsoft excel, nextgen, electronic health systems, health informatics system, survey instruments, data quality metrics, healthrelated field, public health, health care data analysis, program evaluation, relational databases, query tools, population health, patient focused mindset","access, advanced microsoft excel, data quality metrics, databases, electronic health systems, health care data analysis, health informatics system, healthrelated field, nextgen, patient focused mindset, population health, program evaluation, public health, query tools, relational databases, survey instruments"
Data Scientist,Saransh Inc,"Auburn Hills, MI",https://www.linkedin.com/jobs/view/data-scientist-at-saransh-inc-3766095677,2023-12-17,Macomb,United States,Mid senior,Onsite,"Bachelor’s degree in computer science engineering or data Analytics
10+ years of experience in Data Science and Analytical Domains
Understanding of machine-learning and operations research
Proficiency in Query languages SQL, Hive and scripting languages
Knowledge of R; familiarity with Scala, Java is an asset
Experience using business intelligence tools (e.g. Tableau) and data frameworks (e.g. Hadoop, Google Cloud Platform, SAP)
Working knowledge of containers. For example, experience with Docker or podman
Comfortable with git
Proven ability deploying automated data processing pipelines Strong command lines skills in Linux environments
Experience with Kubernetes, GCP, AWS, and cloud-native technologies generally A high tolerance for OpenShift, Confluence, Jira, and other enterprise tools
Analytical mind and business acumen
Proficiency in Data wrangling
Strong math skills (e.g. statistics, algebra)
Problem-solving aptitude
Excellent communication and presentation skills
Show more
Show less","Machine learning, Operations research, SQL, Hive, Scripting languages, R, Scala, Java, Tableau, Hadoop, Google Cloud Platform, SAP, Containers, Docker, Podman, Git, Linux command line, Kubernetes, GCP, AWS, Cloudnative technologies, OpenShift, Confluence, Jira, Data wrangling, Statistics, Algebra, Problemsolving","machine learning, operations research, sql, hive, scripting languages, r, scala, java, tableau, hadoop, google cloud platform, sap, containers, docker, podman, git, linux command line, kubernetes, gcp, aws, cloudnative technologies, openshift, confluence, jira, data wrangling, statistics, algebra, problemsolving","algebra, aws, cloudnative technologies, confluence, containers, data wrangling, docker, gcp, git, google cloud platform, hadoop, hive, java, jira, kubernetes, linux command line, machine learning, openshift, operations research, podman, problemsolving, r, sap, scala, scripting languages, sql, statistics, tableau"
"Senior Data Analyst,",Global Network Technologies,"Auburn Hills, MI",https://www.linkedin.com/jobs/view/senior-data-analyst-at-global-network-technologies-3727341739,2023-12-17,Macomb,United States,Mid senior,Onsite,"Title: Senior Data Analyst, Data & Systems
Job Location - Hybrid remote , Auburn Hills, MI, 48326
Auburn Hills/Detroit office 2-3 times per week ( 5% travel)
Job Type - C2H
No Sonsorship
Job Description -
Role Responsibilities:
Pair with system SMEs to gain input/knowledge on key systems data
Develop functional requirements for system spilt
Understand and evaluate IT proposals to perform functional/logical or physical split.
Define and perform testing scenarios for split systems: functionalities Escalate to SME in case of need
Work closely with the ""GSM Data & Systems split Project Manger"", systems' SME, Corporate GSM Leads on both sides.
Skills
-
knowledge of below systems / technologies is considered a plus:
Ivalua
SharePoint based Applications
Alteryx
SAP P2P Modules
Data integration through API and flat files
Analysis of data and format data
data analytics
data science
Minimum Qualifications
Education
:-
Bachelor's degree in computer science in computer science, information systems, statistics or related field.
Experience
5+ years of experience in data analytics or data science roles or sytems maangment roles
Background experience on GSM processes/topics/tools
Experience in spin-off of systems in Multinational Companies is an absolute plus
Advanced understanding of data management and visualization practices.
familiarity in working in a multinational/ multicultural/multi-timezone environment
Show more
Show less","Data Analytics, Data Science, Ivalua, SharePoint, Alteryx, SAP, Data Integration, API, Data Analysis, Data Visualization, Data Management, GSM Processes, Multinational Experience","data analytics, data science, ivalua, sharepoint, alteryx, sap, data integration, api, data analysis, data visualization, data management, gsm processes, multinational experience","alteryx, api, data integration, data management, data science, dataanalytics, gsm processes, ivalua, multinational experience, sap, sharepoint, visualization"
"BMH - Data Scientist - Southfield, MI",Barton Malow,"Southfield, MI",https://www.linkedin.com/jobs/view/bmh-data-scientist-southfield-mi-at-barton-malow-3752888814,2023-12-17,Macomb,United States,Mid senior,Onsite,"Company:
Barton Malow Holdings
Job Location:
Southfield, MI
Position:
Data Scientist
REQ ID:
10769
Position Summary
Barton Malow is seeking an experienced Data Scientist with a strong data science background to join the Data and Automation team. This position collaborates closely with a cross-functional team, including IT, Application Developers, and Data Engineers to develop and deliver AI-powered products. The position’s primary responsibilities will include partnering between data engineering and business stakeholders to develop custom data models and algorithms to drive business solutions.
Key Job Responsibilities
Apply your passion for problem-solving through exploring data.
Partner and educate business partners and leaders on the field of data science.
Develop machine learning models through all phases of development, from design through training, evaluation, validation, and implementation.
Use portfolio of data applications including storytelling with data using Power BI.
Collaborate with the team working with Databricks and continue to learn more as necessary.
Engage in solving architectural challenges related to data lakes, lake houses, warehouses, change data capture, messaging, event streaming and real-time data pipelines.
Leverage your familiarity with databases and their querying languages to effectively manage and manipulate data within a variety of data storage systems.
Required Knowledge, Education, Experience, Skills, And Abilities
5+ years of relevant business experience working with data engineering and data science teams.
Bachelor’s degree in Statistics, Economics, Data Science, or Data Science. Preferred Master's coursework in an academic field related to Data Science.
Demonstrated ability to apply a scientist's/engineer's approach to solving problems.
Familiarity with DevOps/DataOps practices.
Prior experience with statistical programming languages and developing data models in Python/R/SQL.
Proficient in Power BI for data visualization and communication of data stories.
Proficient in deployment of models through platforms such as AWS SageMaker, Azure ML, Datarobot.
Proficient in popular business collaboration tools such as Microsoft Office, Google Apps, Apple Pages, Slack, Teams, etc.
Work Environment/Job Conditions
This position may require up to 30% travel.
This may increase/decrease depending on the team member's location and the needs of the team.
Show more
Show less","Data Science, Problem Solving, Machine Learning, Data Engineering, SQL, Python, R, Power BI, Databricks, Data Lakes, Warehouses, DevOps/DataOps, Storytelling, Data Visualization, AWS SageMaker, Azure ML, Datarobot, Data Storage Systems","data science, problem solving, machine learning, data engineering, sql, python, r, power bi, databricks, data lakes, warehouses, devopsdataops, storytelling, data visualization, aws sagemaker, azure ml, datarobot, data storage systems","aws sagemaker, azure ml, data engineering, data lakes, data science, data storage systems, databricks, datarobot, devopsdataops, machine learning, powerbi, problem solving, python, r, sql, storytelling, visualization, warehouses"
Principal Risk Data and Analytics Analyst,Emergent Holdings,"Southfield, MI",https://www.linkedin.com/jobs/view/principal-risk-data-and-analytics-analyst-at-emergent-holdings-3705260225,2023-12-17,Macomb,United States,Mid senior,Hybrid,"Summary
Job Description
The Principal Analyst is responsible for planning, organizing, directing, implementation, and leading department assignments with minimal direction from leadership. This position operates within broad objectives to ensure optimum utilization relative to Risk Adjustment for Prospective and Retrospective programs. This role researches, compiles, and analyzes appropriate and relevant data to identify opportunities to recommend and drive executionof process efficiencies in assigned market teams to achieve targeted levels of improvement in key operational performance metrics.
Responsibilities/Tasks
Responsible for the development and delivery of multiple concurrent projects/programs that encompass business process design and improvement specific to Risk Adjustment programs and performance.
Evaluate project performance and identify value drivers for Risk Adjustment initiatives and programs.
Develop processes to calculate and automate KPI's specific to Risk Adjustment as well as effectively communicate KPI's to internal stakeholders
Assists in monitoring initiatives, outcomes, and objectives to ensure goal attainment within defined parameters with minimal guidance from leadership.
Designs, develops, tests, and delivers solutions comprising of components, reports, and data stores per requested deliverable directions with minimal guidance from leadership.
Provides expertise in analytical methodology, including data analysis, used to facilitate data driven decision making, including the collection and monitoring of metrics used to assess, prioritize, and select improvement projects.
Initiates and leads problem solving efforts working closely with internal and external stakeholders.
Possesses and maintains an extensive comprehensive knowledge of Medicare Advantage Risk Adjustment business, products, programs to support overall strategy for accurate patient assessment relative to chronic conditions for overall model of care.
Manages and monitors multiple projects simultaneously by establishing project plans and objectives to ensure goal attainment within defined parameters and timelines.
Develops lines of communication to discuss/review results of analysis to management via reports/presentations and assists management in implementing programs that provide solutions.
Independently develops and plans reports, papers, and/or other materials in a clear and concise manner.
Acts as a liaison between corporate business areas and participates in group or market discussions.
This position description identifies the responsibilities and tasks typically associated with the performance of the position. Other relevant essential functions may be required.
Education
EMPLOYMENT QUALIFICATIONS:
Bachelor’s degree in Business Administration, Economics, Health Care, Information Systems, Statistics, or a related field. Master’s degree in a related field is preferred. Relevant combination of education and experience may be considered in lieu of degree.
Experience
Seven years experience in a related field, typically in two subject areas (e.g. process improvement, project planning, health care economics, health care policy, statistical modeling, business decisions, analysis, Medicare risk adjustment or business management).
Skills/Knowledge/Abilities (ska) Required
Excellent analytical, planning, problem solving, verbal, and written skills to communicate complex ideas.
Excellent organization skills and ability to effectively multi-task in a dynamic and fast paced environment.
Ability to lead and facilitate cross functional teams and team meetings.
Working knowledge of project management disciplines and methods.
Excellent execution in a fast-paced environment with tight deadlines.
Excellent knowledge and use of existing software packages such as Tableau, Access, PowerPoint, Excel, Word, etc.
Excellent knowledge of data languages such as SQL, .net, T-Sql, SAS, HTML, java, VBA, etc.
Excellent analytical, organizational, planning, and problem-solving skills.
Excellent ability to speak effectively before groups of customers or employees of organization.
Excellent ability to define problems, collect data, establish facts, and draw valid conclusions.
Excellent ability to develop project management, meeting process, and presentation skills.
Excellent ability to work independently, within a team environment, and communicate effectively with employees and clients at all levels.
Working Conditions
Work is performed in an office setting with no unusual hazards.
The qualifications listed above are intended to represent the minimum education, experience, skills, knowledge and ability levels associated with performing the duties and responsibilities contained in this job description.
Pay Range
“Actual compensation decision relies on the consideration of internal equity, candidate’s skills and professional experience, geographic location, market and other potential factors. It is not standard practice for an offer to be at or near the top of the range, and therefore a reasonable estimate for this role is between $88,600 and $148,500.”
The qualifications listed above are intended to represent the minimum education, experience, skills, knowledge and ability levels associated with performing the duties and responsibilities contained in this job description. We are an Equal Opportunity Employer. Diversity is valued and we will not tolerate discrimination or harassment in any form. Candidates for the position stated above are hired on an ""at will"" basis. Nothing herein is intended to create a contract.
Organization
COBX Co
Show more
Show less","Business Administration, Economics, Health Care, Information Systems, Statistics, Tableau, Access, PowerPoint, Excel, Word, SQL, .net, TSql, SAS, HTML, Java, VBA, Project management, Data analysis, Problem solving, Verbal skills, Written skills, Multitasking, Team leadership, Team facilitation, Cross functional teams, Analytical skills, Planning skills, Organizational skills, Problemsolving skills, Public speaking skills, Presentation skills, Teamwork, Client communication","business administration, economics, health care, information systems, statistics, tableau, access, powerpoint, excel, word, sql, net, tsql, sas, html, java, vba, project management, data analysis, problem solving, verbal skills, written skills, multitasking, team leadership, team facilitation, cross functional teams, analytical skills, planning skills, organizational skills, problemsolving skills, public speaking skills, presentation skills, teamwork, client communication","access, analytical skills, business administration, client communication, cross functional teams, dataanalytics, economics, excel, health care, html, information systems, java, multitasking, net, organizational skills, planning skills, powerpoint, presentation skills, problem solving, problemsolving skills, project management, public speaking skills, sas, sql, statistics, tableau, team facilitation, team leadership, teamwork, tsql, vba, verbal skills, word, written skills"
Principal Data Engineer,Akili,"Larkspur, CA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-akili-3622733819,2023-12-17,Petaluma,United States,Mid senior,Onsite,"The Role
Digital Therapeutics is creating innovative user journeys spanning patients, caregivers, and health care providers, creating unique opportunities for data-fueled products and features that will transform the definition and delivery of medicine. Akili is seeking a Principal Data Engineer to steer our operational and analytical data infrastructure into that digital future.
Responsibilities
Anchoring the Data Engineering team in solid engineering discipline, while delivering exceptional code.
Driving the strategic selection, design, and implementation of technologies to enable comprehensive availability and usability of data.
Managing computing/storage resources and third-party tooling, to balance performance and cost (both person-time and money).
Qualifications
10+ years as a software engineer, 5+ years specifically in data engineering
Expert-level SQL, high proficiency with Python (especially the Pandas/NumPy data stack)
Deep knowledge of storage, transport, and encoding of diverse formats.
Fluency with the AWS data ecosystem
Fundamental understanding of the principles for managing privacy and security of user data
Bonus: DevOps chops, productionisation of ML models
At Akili, We Are Committed To
Be Bold - We are pioneers. We take risks, embrace discomfort, aim high, and act fearlessly. We break down barriers for our patients.
Be Creative - We are inventors. Our success is dependent upon thinking beyond the status quo. We are extremely open-minded, take nothing for granted, and create unprecedented solutions for our patients.
Be Inclusive - We hear ALL voices. We encourage all input, embracing our strengths and leveraging our differences, and treat each other (and our patients) with utmost respect.
Be Accountable - We own it. We take responsibility for our own work and the success of our colleagues. We make, and keep our commitments. Our patients are counting on us to hold each other to the highest standard.
Be Well -We improve lives. We are committed and obsessed with the health of our patients, employees, and ourselves. All in service of improving the well being of the world.
About Akili Interactive
At Akili Interactive, our mission is to ignite new hope for those living with cognitive impairment. We’re bringing together world-class neuroscience with the latest technology and video game entertainment to create a new class of digital medicines, delivered through captivating video game experiences. We are out to prove that effective medicine for serious illnesses can also be fun and engaging.
Our flagship product,
EndeavorRx*, is the first ever FDA-cleared prescription treatment delivered through a video game to treat children living with ADHD. This is only the beginning, as we continue to boldly challenge the status quo of today’s medicine. *
Diversity, Equity and Inclusion
Akili is, by our very nature, founded on diversity. We are composed of employees from incredibly diverse industries such as gaming, software, clinical research, biotechnology, and the pharmaceutical industry, all coming together with a common mission; to challenge the status quo of medicine. Inclusivity and wellness are 2 of our foundational cultural commitments.
Diversity and inclusion are incredibly important to all of us. We are committed to hearing all voices. Akili Interactive provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, sexual orientation, gender identity or gender expression.
Show more
Show less","Data Engineering, SQL, Python, Pandas, NumPy, AWS, Data Privacy, Data Security, DevOps, Machine Learning, Software Engineering","data engineering, sql, python, pandas, numpy, aws, data privacy, data security, devops, machine learning, software engineering","aws, data engineering, data privacy, data security, devops, machine learning, numpy, pandas, python, software engineering, sql"
Staff Data Engineer,Recruiting from Scratch,"San Rafael, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744393938,2023-12-17,Petaluma,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, ETL, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Data Warehouses, Kafka, Storm, SparkStreaming, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment","data engineering, etl, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, data warehouses, kafka, storm, sparkstreaming, tdd, pair programming, continuous integration, automated testing, deployment","airflow, automated testing, business intelligence, continuous integration, data engineering, data science, data warehouses, deployment, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Staff Data Engineer,Recruiting from Scratch,"Mill Valley, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744397314,2023-12-17,Petaluma,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, ETL, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, Data Management Tools, Data Classification, Retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, etl, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, data management tools, data classification, retention","airflow, automated testing, continuous integration, data classification, data management tools, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, retention, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"San Rafael, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748832235,2023-12-17,Petaluma,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data Warehouses, ETL, Legal compliance, Data classification, Data retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, legal compliance, data classification, data retention","airflow, continuous integration, data classification, data retention, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"San Rafael, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744396423,2023-12-17,Petaluma,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Agile Engineering Practices, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Legal Compliance, Data Management Tools, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering practices, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, legal compliance, data management tools, data classification, data retention","agile engineering practices, airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Mill Valley, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748830449,2023-12-17,Petaluma,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Software Development, Data Engineering, Realtime Streaming Technologies, TDD, Automation, Continuous Delivery, Data Governance, Data Security, Data Scalability, ETL, S3, Snowflake, Kafka, Spark, Python, SQL, Relational Databases, Agile Engineering, Pair Programming, Continuous Integration, Automated Testing, Data Warehouses, Data Classification, Data Retention","software development, data engineering, realtime streaming technologies, tdd, automation, continuous delivery, data governance, data security, data scalability, etl, s3, snowflake, kafka, spark, python, sql, relational databases, agile engineering, pair programming, continuous integration, automated testing, data warehouses, data classification, data retention","agile engineering, automated testing, automation, continuous delivery, continuous integration, data classification, data engineering, data governance, data retention, data scalability, data security, data warehouses, etl, kafka, pair programming, python, realtime streaming technologies, relational databases, s3, snowflake, software development, spark, sql, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Mill Valley, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744392725,2023-12-17,Petaluma,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","TDD, Continuous delivery, Realtime streaming technologies, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Dimensional data modeling, Schema design","tdd, continuous delivery, realtime streaming technologies, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, data warehouses, etl, dimensional data modeling, schema design","airflow, continuous delivery, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, python, realtime streaming technologies, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Analyst, Mobile",2K,"Novato, CA",https://www.linkedin.com/jobs/view/senior-data-analyst-mobile-at-2k-3762363633,2023-12-17,Petaluma,United States,Mid senior,Remote,"Who We Are
Founded in 2005, 2K Games is a global video game company, publishing titles developed by some of the most influential game development studios in the world. Our studios responsible for developing 2K’s portfolio of world-class games across multiple platforms, include Visual Concepts, Firaxis, Hangar 13, CatDaddy, Cloud Chamber, and HB Studios. Our portfolio of titles is expanding due to our global strategic plan, building and acquiring exciting studios whose content continues to inspire all of us! 2K publishes titles in today’s most popular gaming genres, including sports, shooters, action, role-playing, strategy, casual, and family entertainment.
Our team of engineers, marketers, artists, writers, data scientists, producers, problem solvers and doers, are the professional publishing stewards of our growing library of critically-acclaimed franchises such as NBA 2K, Battleborn, BioShock, Borderlands, The Darkness, Mafia, Sid Meier’s Civilization, WWE 2K, and XCOM.
At 2K, we pride ourselves on creating an inclusive work environment, which means encouraging our teams to Come as You Are and do your best work! We are dedicated to diversity and inclusion, and want our community of candidates to reflect this commitment. We encourage all qualified applicants to explore our global positions.
2K is headquartered in Novato, California and is a wholly owned label of Take-Two Interactive Software, Inc. (NASDAQ: TTWO).
What We Need
We are seeking a senior data analyst who will enable the game Studio behind one of our top titles to confidently make data driven decisions. The ideal candidate will be responsible for delivering data tools, telemetry, dashboards and compelling insights to influence game development and growth. You will be someone who has a good balance of business thinking alongside analytical and technical skills.
What You Will Do
Reporting directly to the Senior Director of Mobile Analytics, you will be responsible for:
Business Insight: Understand the problems our Studios and partners want to solve for, and then define the right data, metrics, analysis and interpretation to lead to the right recommendations and decisions
Understand players: Help us to further understand, model, predict, segment and retain our players and enable our Studios to launch new titles, develop features,
and live ops events.
ETL pipeline: Owning ETL pipeline for one of our top titles by partnering closely with data engineering
Telemetry design: Collaborating with our studio developers and designing in-game telemetry to measure player behavior
Being the technical data expert in communications with stakeholders and providing regular game health updates to stakeholders as well as leadership teams.
Creating and maintaining reporting dashboards: overseeing data quality, amalgamate data from multiple sources for creating and tracking key KPIs.
What Will Make You a Great Fit
4+ years of work experience in data science or analytics role
SQL: The ability to write complex SQL queries to analyze our databases with millions of players and work with relational database systems.
Familiar with querying in Snowflake, Redshift, Postgres etc.
Analytical coding: Fluency in programming via languages such as Python or R
Visualization: Adept in at least one visualization tool such as Tableau or Looker
Tech skill: Proven experience with some or all of the following: statistics, experimental design, machine learning, predictive modeling, deep learning
Ability to work effectively in a fast-paced environment, with different functions with changing priorities
Ability to collaborate effectively with product managers, game designers, game engineers, marketing and other analysts to deliver unambiguous business impact
Excellent attention to detail, natural curiosity, and ability to problem-solve creatively
Great communication and presentation skills to be able to converse with both technical and non-technical audiences.
Self starter- proactively dive deep into the data, develop analytical hypotheses and come up with actionable insights for improving the product.
Bonus Points
Consistent track record in data challenges in similar roles, preferably within mobile gaming or similarly customer- facing digital businesses
Familiarity with telemetry/analytics SaaS tools (e.g., Mixpanel, Amplitude)
Familiarity with massive data sets and tools to deal with them (E.g. Snowflake, Databricks, AWS)
Experience in behavioral psychology/economics and experimental design
Knowledge and love for the NBA and WWE franchises preferred
Passion for mobile games
As an equal opportunity employer, we are committed to ensuring that qualified individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform their essential job functions, and to receive other benefits and privileges of employment. Please contact us if you need reasonable accommodation.
Please note that 2K Games and its studios never uses instant messaging apps or personal email accounts to contact prospective employees or conduct interviews and when emailing, only use 2K.com accounts.
This is a fully remote role that may be based anywhere in the United States. The pay range for this position for applicants based in Colorado at the start of employment is expected to be between $98,000 and $140,000 per year. However, base pay offered is based on market location, and may vary further depending on individualized factors for job candidates, such as job-related knowledge, skills, experience, and other objective business considerations. Subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards, in addition to a full range of medical, financial, and/or other benefits. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. If hired, employee will be in an “at-will position” and the company reserves the right to modify base salary (as well as any other discretionary payment or compensation or benefit program) at any time, including for reasons related to individual performance, company or individual department/team performance, and market factors.
Show more
Show less","SQL, Data Science, Data Analysis, Machine learning, R, Python, Tableau, Looker, Snowflake, Redshift, Postgres, Experimental Design, Predictive modeling, ETL, AWS, Databricks, Mixpanel, Amplitude","sql, data science, data analysis, machine learning, r, python, tableau, looker, snowflake, redshift, postgres, experimental design, predictive modeling, etl, aws, databricks, mixpanel, amplitude","amplitude, aws, data science, dataanalytics, databricks, etl, experimental design, looker, machine learning, mixpanel, postgres, predictive modeling, python, r, redshift, snowflake, sql, tableau"
Senior Staff AI Data Engineer,Recruiting from Scratch,"San Rafael, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759706907,2023-12-17,Petaluma,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML Data Ops, Data Pipelines, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Statistical Analysis, Data Visualization, Pandas, R, Orchestration Frameworks, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, NoSQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine Learning, Legal Compliance, Data Classification, Data Retention, Data Projects","data engineering, ml data ops, data pipelines, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, orchestration frameworks, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, legal compliance, data classification, data retention, data projects","airflow, aws, azure, bash, data classification, data cleaning, data engineering, data mining, data normalization, data projects, data retention, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, legal compliance, machine learning, ml data ops, nlp, nosql, orchestration frameworks, pandas, python, r, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Mill Valley, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759706904,2023-12-17,Petaluma,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, Spark Streaming, Applied Machine Learning, Legal Compliance, Data Classification, Data Retention, 401K Plan, Health Insurance, Dental Insurance, Vision Insurance, Flexible Vacation Policy, Cell Phone Stipend, Internet Stipend, Wellness Stipend, Food Stipend, Home Office Setup Stipend, CompanySponsored Events","data engineering, machine learning, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, spark streaming, applied machine learning, legal compliance, data classification, data retention, 401k plan, health insurance, dental insurance, vision insurance, flexible vacation policy, cell phone stipend, internet stipend, wellness stipend, food stipend, home office setup stipend, companysponsored events","401k plan, airflow, applied machine learning, aws, azure, bash, cell phone stipend, companysponsored events, data classification, data engineering, data retention, dental insurance, docker, dynamodb, etl, flexible vacation policy, food stipend, gcp, git, health insurance, helm, home office setup stipend, internet stipend, java, kafka, kubernetes, legal compliance, machine learning, python, snowflake, spark, spark streaming, sql, storm, vision insurance, wellness stipend"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Mill Valley, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773086837,2023-12-17,Petaluma,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","ML Data Engine, ML Data OPs, Data pre/post processing pipelines, Data mining, Data cleaning, Data normalizing, Data modeling, Data governance, Data risk and compliance, Data infrastructure, Python, Java, bash, SQL, Git, Airflow, KubeFlow, Snowflake, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming","ml data engine, ml data ops, data prepost processing pipelines, data mining, data cleaning, data normalizing, data modeling, data governance, data risk and compliance, data infrastructure, python, java, bash, sql, git, airflow, kubeflow, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming","airflow, aws, azure, bash, data cleaning, data governance, data infrastructure, data mining, data normalizing, data prepost processing pipelines, data risk and compliance, datamodeling, docker, dynamodb, gcp, git, helm, java, kafka, kubeflow, kubernetes, ml data engine, ml data ops, python, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"San Rafael, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773090547,2023-12-17,Petaluma,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Pandas, R, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Classification, Data Retention, 401K Plan, Company Bonus, Equity Programs, GenderAffirming Offerings, Included Health, HRT","data engineering, machine learning, data mining, data cleaning, data normalization, data modeling, pandas, r, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, data retention, 401k plan, company bonus, equity programs, genderaffirming offerings, included health, hrt","401k plan, airflow, applied machine learning, aws, azure, bash, company bonus, data classification, data cleaning, data engineering, data mining, data normalization, data retention, datamodeling, docker, dynamodb, equity programs, etl, gcp, genderaffirming offerings, git, helm, hrt, included health, java, kafka, kubeflow, kubernetes, machine learning, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Data Engineer,Stellar Development Foundation,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-stellar-development-foundation-3783147755,2023-12-17,Berkeley,United States,Mid senior,Onsite,"Interested in measuring blockchain's impact on the real world? The Stellar Development Foundation (SDF) is looking for a talented, experienced, and hands-on Data Engineer to join our team. In this role your primary focus will be all things data. You'll be designing, building, and implementing data pipelines in our publicly available analytics dataset, Hubble.
As a member of our team, you'll have the opportunity to work on a wide variety of projects that measure and assess the Stellar Network's real world impact on the global financial system. This includes monitoring and understanding Soroban (smart contracts) adoption, tracking liquidity and transaction volume, and identifying anomalous activity. Our aim is to make data more accessible, actionable, and easier to use by building custom data products and visualizations for the broader Stellar ecosystem.
In this role, you will:
Design, build, orchestrate, and maintain data pipelines that give us a unique perspective into the liquidity, adoption, and usage of the Stellar network.
Conduct ad hoc data analysis to clean, transform, and distill key insights about the Stellar Network.
Create engaging data visualizations and dashboards to help stakeholders easily understand and act upon recommendations.
Improve observability and maintainability of our data marts, including usage, quality, and freshness.
Translate business priorities into measurable insights that aid in data-driven decision-making.
Improve data accessibility by encouraging self-service adoption of Dashboards, KPIs, and SQL Interfaces.
You have:
5+ years of professional data engineering, ML, or analytics experience
Modern data warehousing fundamentals with experience in an ETL framework like dbt, Fivetran, Databricks, or Talend
Deep expertise in SQL, especially working with raw, large datasets
Proficiency with scripting languages such as Python or R
Familiarity with software engineering best practices
Experience scheduling and orchestrating pipelines and managing complex data dependencies
A passion for exploring complex datasets and learning business domains
Data Governance best practices: data cataloging, quality control, and usability
Experience building compelling data visualizations with dashboard tools
Excellent communication skills, with the ability to explain complex data insights to non-technical stakeholders
Enthusiasm about working on a small, growing team with the freedom to innovate and set direction
Bonus Points if:
You have cloud development experience, with a preference towards the GCP data stack
You have a strong curiosity in blockchain technologies and cryptocurrencies, and understand the fundamentals of these systems
We offer competitive pay with a base salary range for this position of $165,000 - $215,000 depending on job-related knowledge, skills, experience, and location. In addition, we offer lumen-denominated grants along with the following perks and benefits:
USA Benefits/Perks:
Competitive health, dental & vision coverage
Flexible time off + 15 company holidays including a company-wide holiday break
Paid parental leave
Life & ADD
Short & Long term disability
FSA & Dependent Care Accounts
401K (4% match)
Employee Assistance Program
Monthly gym allowance
Daily lunch and snacks in-office
L&D budget of $1,500/year
Company retreats
About Stellar
Stellar is more than a blockchain. Powered by a decentralized, fast, scalable, and uniquely sustainable network made for financial products and services and a thriving and passionate ecosystem that includes a non-profit organization driven by a mission, Stellar is paving the path to unlock the world's economic potential through blockchain technology. Built with speed and low costs in mind, the Stellar network provides builders and financial institutions worldwide a platform to issue assets, and to send and convert currencies in real time creating real world utility. Founded in 2014, the Stellar Development Foundation (SDF) supports the continued development and growth of the Stellar network and also serves the ecosystem of NGOs, corporations, universities, small businesses, governments, and solo entrepreneurs building on the Stellar network through tooling, funding and strategic collaborations. Together, Stellar is where blockchain meets the real world.
About The Stellar Development Foundation
The Stellar Development Foundation (SDF) is a non-profit organization focused on working with and supporting changemakers to create equitable access to the global financial system through blockchain technology. SDF provides grants, investments, funding, and other awards to builders and organizations. SDF also develops resources and tooling on the Stellar network to help unlock real world utility. As a nonprofit foundation, SDF puts the health of the Stellar network and the Stellar ecosystem and its mission above all else.
We look forward to hearing from you!
Privacy Policy
By submitting your application, you are agreeing to our use and processing of your data in accordance with our Privacy Policy.
SDF is committed to diversity in its workforce and is proud to be an equal opportunity employer. SDF does not make hiring or employment decisions on the basis of race, color, religion, creed, gender, national origin, age, disability, veteran status, marital status, pregnancy, sex, gender expression or identity, sexual orientation, citizenship, or any other basis protected by applicable local, state or federal law.
Show more
Show less","Data engineering, Machine learning, Analytics, ETL frameworks, SQL, Python, R, Software engineering best practices, Cloud development experience, GCP data stack, Blockchain technologies, Cryptocurrencies","data engineering, machine learning, analytics, etl frameworks, sql, python, r, software engineering best practices, cloud development experience, gcp data stack, blockchain technologies, cryptocurrencies","analytics, blockchain technologies, cloud development experience, cryptocurrencies, data engineering, etl frameworks, gcp data stack, machine learning, python, r, software engineering best practices, sql"
Data Recovery Engineer - Windows Platform,DriveSavers Data Recovery,"Novato, CA",https://www.linkedin.com/jobs/view/data-recovery-engineer-windows-platform-at-drivesavers-data-recovery-3787797294,2023-12-17,Berkeley,United States,Mid senior,Onsite,"Seeking a candidate with 1-2 years of IT / Desktop Support and troubleshooting experience on the Windows PC platform who is excited to learn the art of data recovery.
Education
Associate/Bachelor Degree or equivalent work experience
Required Skills And Experience
1-2 years experience in an IT / Desktop Support environment with an emphasis on corporate support
Experience with encryption software is a plus
Knowledge of MS Office products
Maintains knowledge of current technology
Experience and understanding of computer hardware components for troubleshooting and upgrading purposes
Using Terminal to complete tasks
Must have passion for staying ahead of current and future technologies
Excellent interpersonal, organizational, and communication skills
Great customer service skills are required as you will need to work with customers over the phone on some projects
Strong attention to detail
Able to work independently as well as part of a team
Able to analyze and evaluate customer needs
Able to prioritize and meet deadlines in a fast-paced environment
If you have some of the qualifications but not all, please tell us why you think you should be considered.
Benefits
Competitive Salary
Monthly Bonuses
401K Retirement Plan
Medical Insurance
Dental and Vision Plan
Ongoing Training
Paid Holidays
Maternity/Paternity Leave
Events and Celebrations
Subsidized Stocked Refrigerator
Friendly Workspace
Green Business
From Employees on Glassdoor
Fun work environment, rewarding knowing that we get irreplaceable data back for customers.
Amazing work environment. Excellent incentives that encourage us all.
Friendly, inviting, and supportive work environment.
Great training, room for growth.
Engaging and manageable workload.
Each day brings new challenges with new and older technology.
Snacks and beverages are stocked regularly.
You feel valued as an employee. Comfortable work environment and strong incentives.
Drawings for various concerts, regular season and championship playoff (NHL, NFL, MLB, NBA) tickets, and other cool attractions are commonplace.
It’s been great to work as an individual to make a difference to customers and work next to others with that same goal.
Small business with a work environment that feels like a family, while also being known as the leader in data recovery.
Powered by JazzHR
OCsbGgh5jM
Show more
Show less","Windows PC, Troubleshooting, Data recovery, Encryption software, MS Office, Terminal, Computer hardware, Prioritization, Team work, Customer analysis, Time management, 401k retirement plan, Medical insurance, Dental and Vision plan, Ongoing training","windows pc, troubleshooting, data recovery, encryption software, ms office, terminal, computer hardware, prioritization, team work, customer analysis, time management, 401k retirement plan, medical insurance, dental and vision plan, ongoing training","401k retirement plan, computer hardware, customer analysis, data recovery, dental and vision plan, encryption software, medical insurance, ms office, ongoing training, prioritization, team work, terminal, time management, troubleshooting, windows pc"
Staff Data Engineer,Linktree,"San Francisco, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-linktree-3727987340,2023-12-17,Berkeley,United States,Mid senior,Onsite,"The Role
Linktree’s Data Engineering team is at the forefront of transforming the way Linktree leverages data, working closely with the insights, data platform product and marketing teams to become truly data driven.
As a Staff Data Engineer at Linktree, you will be the driving force behind scaling how data is consumed at Linktree. Not only will you be modelling some of our most important data sets to gain insights, such as the data that helps us understand the driving factors behind the success of our product or our revenue, we take self-service one step further. You will also build the platform that can be used by Data Analysts, Marketers, Engineers and anyone else who is interested to model their own data to fit their specific needs and not be dependent on a data team.
This is your opportunity to make an impact at Linktree and push the boundaries of what is possible with data self-service!
Location Expectations:
Hybrid. We're growing our team in LA and the Bay Area, and plan to have offices in both locations. We expect team members to come into their respective office 2x/week.
What You Will Do
Build the platform that allows data analysts, product managers, marketers and other heavy data users to model their own data as well as making that data available in tools of their choosing, such as product analytics tools, BI tools, an experimentation platform or our CRM system.
Continue leading the transformation to make data a first-class citizen in software development at Linktree. This includes defining standards and providing guidance to product teams who emit the vast majority of our data.
Build and maintaining robust, efficient and integrated data models. We don’t believe that Analytics Engineers should be painfully cleaning up after others and deal with bad data all day long, if you are faced with a “garbage in, garbage out” situation, you will work with the engineers in our product teams to come up with a better way to emit the raw data.
You will also provide support to other data users including peer-reviews, training and acting as a data modeling/SQL/dbt SME across various business initiatives.
What We Are Looking For
A platform mindset. Data teams main focus is not to do repetitive data transformation and integration jobs for others. The team builds the infrastructure and tools that allows Linktree to perform these tasks at scale by enabling all teams to perform data-related jobs themselves.
You understand data-driven product development and have experience with the typical tools used by high-performing product teams, such product analytics, experimentation, but also general purpose BI tools.
Experience in data modeling with the ability to translate business requirements to fit for purpose data products for critical use-cases (reporting to the board, understanding key drivers of product success).
Experience operating at scale. You have worked on data systems that power a product that serves hundreds of millions of active users.
Proficient in SQL and experience with working on cloud-based warehouses (experience in Snowflake is a bonus), as well as working with DBT.
Linktree is committed to providing a competitive compensation package. Our cash compensation amount for this role is targeted at $175,000-$225,000 in the San Francisco Bay or Los Angeles area. Final offer amounts are determined by multiple factors including candidate expertise, the scope of role and level, and may vary from the amounts listed above.
P.S. If you don’t tick every box in this ad, please don’t rule yourself out. We take pride in inclusion and hiring incredible human beings with great potential over ticking boxes – so if this role resonates with you, hit that apply button!
Where And How We Work
We are a global and diverse group offering a truly flexible and family friendly work environment. Kids, pets, and the occasional delivery person are all actively encouraged to appear on our Zoom screens. All of us at Linktree work either fully remote or a hybrid ""remote, but in-office sometimes"" approach.
We currently have offices in Melbourne, Sydney and LA, but our team is spread across Australia, United States, and New Zealand. As our team approaches 200 people, our company will be 10x the size we were in 2020.
We offer autonomy and flexibility in how you structure your days and weeks. There will be the need for some collaboration outside of a ""normal"" 9-5 being a global company, but we aim to work asynchronously where possible.
Our Culture And Benefits
Linktree's company culture and values are based around collaboration, diversity, inclusion, and flexibility. Those are all nice words but to give you some more specific examples:
We recognize that our team are individually unique and have designed our benefits with this in mind. Each person has an annual allowance ($7,000 AUD / $4,900 USD) to use on things like (but not limited to) fitness memberships, development courses, childcare, travel, charitable donations, pet insurance, home office set up - the choice is yours!
We provide top-flight medical, dental, vision, disability and life insurance - we cover 100% of your monthly premiums (and 80% for your dependents).
Employee Stock Option Program - we want each and every employee to share in the company’s success as we go further together.
To learn more about our benefits, including our parental leave program, volunteering leave, DE&I initiatives, and more, click here!
Our Story
We're on a mission to empower anyone to curate and grow their digital universe. We created the ""link in bio"" category and are trusted by some of the world's biggest brands and celebrities including TikTok, The UN Environmental Program, The White House, F1, Manchester United, Selena Gomez, Alicia Keys, and Dwayne “The Rock” Johnson. With a flexible work environment and a team spread across multiple time zones, we offer autonomy and flexibility. Join us in empowering people to control their online presence!
At Linktree, we celebrate and support everyone’s perspective and background, and we’re proud to be an equal opportunity workplace. We aim to foster a diverse and inclusive environment where all team members have a sense of belonging, because we believe in
going further together.
Linktree welcomes all people regardless of sex, gender identity, race, ethnicity, disability, pregnancy, age, or other lived experience. If you require accommodations to fully participate in our opportunities, please don't hesitate to reach us at
recruiting@linktr.ee
– your needs are important to us.
Show more
Show less","SQL, Snowflake, dbt, Data modeling, BI tools, Product analytics, Experimentation, Cloudbased warehouses, Platform mindset, Datadriven product development, Data integration, Data transformation, Big data, Data engineering, Data platform, Data science, Data analysis, Software development","sql, snowflake, dbt, data modeling, bi tools, product analytics, experimentation, cloudbased warehouses, platform mindset, datadriven product development, data integration, data transformation, big data, data engineering, data platform, data science, data analysis, software development","bi tools, big data, cloudbased warehouses, data engineering, data integration, data platform, data science, data transformation, dataanalytics, datadriven product development, datamodeling, dbt, experimentation, platform mindset, product analytics, snowflake, software development, sql"
Senior Health Data Engineer,Nuna Inc.,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-health-data-engineer-at-nuna-inc-3723645058,2023-12-17,Berkeley,United States,Mid senior,Onsite,"At Nuna, our mission is to make high-quality healthcare affordable and accessible for everyone. We are dedicated to tackling one of our nation’s biggest problems with ingenuity, creativity, and a keen moral compass.
Nuna is committed to simple principles: a rigorous understanding of data, modern technology, and most importantly, compassion and care for our fellow human. We want to know what really works, what doesn't—and why.
Nuna partners with healthcare payers, including government agencies and health plans, to turn data into learnings and information into meaning.
YOUR TEAM
Nuna’s product offerings enable delivery of value-based care contracts, at scale across the United States. Value-based healthcare aims to shift reimbursement from fee-for-service to fee-for-value where providers are incentivized to deliver higher quality care to their patients at a lower cost. It is incredibly difficult to operationalize value-based arrangements due to the variability in both contract configurations and measurement of health outcomes. The Health Data Engineering team is accountable for creating automated and scalable tools for analysts to accurately and efficiently implement complex value-based care programs.
YOUR OPPORTUNITIES
In this role, you’ll have the opportunity to form deep partnerships with a cross-functional team of analysts, engineers and product managers to drive continuous improvement of delivery speed and analytic quality through introduction and adoption of software development and data management best practices. You will be responsible for design and deployment of reusable healthcare-based logic in support of accurate and scalable value-based payment solutions. You will provide thought leadership to product managers and influence Nuna’s product roadmap with a focus on analytic tooling maturity and scalable healthcare solutions. Finally, you will be a force multiplier for Nuna’s analysts by introducing software development and testing practices tuned to value-based care payment models.
In This Role, You Will
Train and support health data analysts in design and deployment of scalable products for healthcare plans and providers
Participate in the development of industry accepted healthcare data models and rules-based engines
Automate and optimize data pipelines with a focus on logical transformations and statistical models
Build, maintain and optimize data pipelines using big data technologies
QUALIFICATIONS
Required Qualifications
6+ years of technical software development via Python, Spark, SQL, Git Integration and testing frameworks
Domain experience working with healthcare data paired with a passion to connect with, understand, and shape the healthcare industry
Experience building production-hardened data pipelines, with consideration for performance, scalability, reliability, and repeatability using orchestration tools like Airflow or Prefect
Experience building Spark jobs, as well as profiling and debugging them
Experience rapidly prototyping new product concepts, especially for enterprise clients
Understanding of data testing concepts and the ability to consistently apply them
Knowledge of database fundamentals like indexing and SQL queries
Strong oral and written communication skills, keen attention to detail and deadline driven
Preferred Qualifications
Experience with Machine Learning concepts such as statistical modeling & diagnostic testing (Continuous + Categorical)
Experience with AWS
Experience working at or with health plans or providers
We take into account an individual’s qualifications, skillset, and experience in determining final salary. This role is eligible for health insurance, life insurance, retirement benefits, participation in the company’s equity program, paid time off, including vacation and sick leave. The expected salary range for this position is $114,000 to $188,000. The actual offer will be at the company’s sole discretion and determined by relevant business considerations, including the final candidate’s qualifications, years of experience, and skillset.
Nuna is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics and/or veteran status.
Show more
Show less","Python, Spark, SQL, Git, Integration frameworks, Testing frameworks, Healthcare data, Data pipelines, Big data technologies, Airflow, Prefect, Machine Learning, AWS","python, spark, sql, git, integration frameworks, testing frameworks, healthcare data, data pipelines, big data technologies, airflow, prefect, machine learning, aws","airflow, aws, big data technologies, datapipeline, git, healthcare data, integration frameworks, machine learning, prefect, python, spark, sql, testing frameworks"
Data Analyst,Stripe,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-analyst-at-stripe-3779949197,2023-12-17,Berkeley,United States,Mid senior,Onsite,"About Stripe
Stripe is a financial infrastructure platform for businesses. Millions of companies—from the world’s largest enterprises to the most ambitious startups—use Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Our mission is to increase the GDP of the internet, and we have a staggering amount of work ahead. That means you have an unprecedented opportunity to put the global economy within everyone’s reach while doing the most important work of your career.
About The Team
Data Science at Stripe is a vibrant community where data analysts, data scientists and engineers learn and grow together. You will work with some of Stripe’s most fundamental and exciting data, and use that data to help drive company-wide initiatives. We have a variety of Data Analytics roles and teams across Stripe and will seek to align you to the most relevant team based on your background.
What you'll do
In this role, you will partner deeply with teams across Stripe to ensure that our users, our products, and our business have the models, data products, and insights needed to make decisions and grow responsibly. You will work closely with partners to extract insights from Stripe's rich and complex data. You will also work with leaders to translate business needs into data problems. You will build metrics, scalable data pipelines, dashboards, and reports to inform and run the business. You will deliver actionable business recommendations through analyses and data storytelling.
Who you are
We’re looking for someone who meets the minimum requirements to be considered for the role. If you meet these requirements, you are encouraged to apply. The preferred qualifications are a bonus, not a requirement.
Minimum Requirements
2-8+ years experience in Business Intelligence Engineering, Data Engineering, Data Analysis or Data Science roles, building data pipelines and analyzing large datasets to solve problems
Proficiency in SQL and Python
Strong statistical knowledge
Expertise in visualization and using data insights to make recommendations and achieve goals
Proven ability to manage and deliver on multiple projects with great attention to detail
Ability to clearly communicate results and drive impact
Comfortable collaborating across functions to identify data analytics problems and execute solutions with technical rigor and data-driven insights.
Preferred Qualifications
Master’s degree in Mathematics, Statistics, Economics, Engineering, or a related technical field.
Prior experience at a growth stage internet or software company.
Experience with distributed data frameworks like Hadoop and Spark to write and debug data pipelines.
Good understanding of development processes and best practices like engineering standards, code reviews, and testing.
Show more
Show less","Business Intelligence Engineering, Data Engineering, Data Analysis, Data Science, SQL, Python, Statistics, Visualization, Hadoop, Spark, Development processes, Engineering standards, Code reviews, Testing, Data pipelines, Dashboards, Reports","business intelligence engineering, data engineering, data analysis, data science, sql, python, statistics, visualization, hadoop, spark, development processes, engineering standards, code reviews, testing, data pipelines, dashboards, reports","business intelligence engineering, code reviews, dashboard, data engineering, data science, dataanalytics, datapipeline, development processes, engineering standards, hadoop, python, reports, spark, sql, statistics, testing, visualization"
Devops/Data Engineer,TekIntegral,"San Francisco, CA",https://www.linkedin.com/jobs/view/devops-data-engineer-at-tekintegral-3660523508,2023-12-17,Berkeley,United States,Mid senior,Onsite,"6 months
$62/hr
Hybrid -
San Francisco, CA area will need to go onsite for monthly meeting
Top 3-5 Skills
Must be very strong with Kubernetes (5+ years of experience)
Cassandra Experience Required
Azure experience required (migrating apps to cloud)
Need to understand database architecture and how to transfer data between databases
Need to understand data pipeline infrastructure
Show more
Show less","Kubernetes, Cassandra, Azure, Cloud Migration, Database Architecture, Data Pipeline Infrastructure","kubernetes, cassandra, azure, cloud migration, database architecture, data pipeline infrastructure","azure, cassandra, cloud migration, data pipeline infrastructure, database architecture, kubernetes"
Senior Data Engineer,Jobot,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-jobot-3786068492,2023-12-17,Berkeley,United States,Mid senior,Onsite,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!
Job details
Remote Data Start Up - Hiring Data Engineers (Amazing Benefits + Flexibility + Culture)
This Jobot Job is hosted by Ryan Kilroy
Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.
Salary $200,000 - $250,000 per year
About Us
A bit about us
The future of data is decentralized, and we're making it a reality. We're leading the charge to reshape the data landscape through our data-mesh-native platform. Crafted to address the challenge of decentralized data at scale, we're reimagining developer interactions with data, enabling responsible sharing through innovative data product containers.
Our Vision
We're on a mission to create a world where the potential of AI/ML and analytics stems from decentralized, responsible, and equitable data ownership. Our vision breaks down organizational, technological, and trust barriers.
Our Purpose
We're transforming the data journey - from creation to sharing, discovery, and utilization. Powered by data mesh principles, our technology strives for connectivity, speed, and fairness.
Why join us?
About Us
The future of data is decentralized, and we're making it a reality. We're leading the charge to reshape the data landscape through our data-mesh-native platform. Crafted to address the challenge of decentralized data at scale, we're reimagining developer interactions with data, enabling responsible sharing through innovative data product containers.
Our Technology
Our technology empowers data developers, users, and owners by delivering a delightful experience where data products take center stage, grounded in inherent trust.
Join Our Journey
In a world of complex and messy data realities, we're here to redefine norms. Data models become obsolete instantly, data transcends trust boundaries, spans platforms, and serves varied purposes. Data needs protection, and past centralized approaches have fallen short. We're here to reimagine, and we're inviting you to join us.
Job Details
The Role Senior Data Engineer
As one of the pioneering Data Engineers, you'll shape the foundation by constructing data products atop our platform. Collaborating with our initial customers, you'll translate their business needs into Proof of Concept (PoC) data products, which you'll build upon our OS.
Your expertise in the full analytics lifecycle - from translating business requirements into data models, pipelines, analytics dashboards, and even machine learning pipelines - will be pivotal. You'll create data products, providing valuable feedback to enhance Nextdata OS and tooling. Managing the entire lifecycle of PoC data products, from ideation to realistic data synthesis for testing and eventual production deployment, will be at the core of your role.
Qualifications
You're the right fit if you bring
Over 10 years of experience as a Data Engineer or Product Developer
Proficiency in data modeling and dashboard development using tools like Looker, PowerBI, etc. for demonstrative purposes
Skill in data processing pipeline development, including tools like airflow, dagster, prefect, dbt, and ML environments like PySpark and Pandas
Experience working on intricate data pipelines within large data organizations
Knowledge of software engineering best practices and tools, including CI/CD and Git
Curiosity, passion, and practical application of data in ML and Analytics
Understanding and experience with data mesh concepts
Contributions to big data/analytics open-source projects or internal data infrastructure products
A track record of creating end-to-end data solutions for diverse stakeholders and clients from undefined business problems
Ability to quantify the value of your analyses and effectively present findings
Exposure to continuous integration and deployment practices, optimizing build and release processes
A penchant for test-first data pipeline development
Travel is generally not anticipated for this role, except for regular company-wide events/meetings.
Perks and Benefits
Despite being an early-stage company, we prioritize your well-being. We offer
$2,000 for setting up your home workspace
Company laptop
Premium health, vision, and dental insurance coverage for you and your family
A competitive salary and equity in Nextdata, reflecting your pivotal role in our journey
If you're ready to drive the future of decentralized data, seize this opportunity to join Nextdata OS. Together, we'll redefine the data landscape. Apply now and be part of a transformative team.
Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.
Want to learn more about this role and Jobot?
Click our Jobot logo and follow our LinkedIn page!
Show more
Show less","Data Engineering, Data Modeling, Dashboard Development, Looker, PowerBI, Data Processing Pipeline Development, Airflow, Dagster, Prefect, Dbt, ML Environments, PySpark, Pandas, Software Engineering Best Practices, Tools including CI/CD and Git, Data Application in ML and Analytics, Data Mesh Concepts, Big Data/Analytics OpenSource Projects, Internal Data Infrastructure Products, EndtoEnd Data Solutions, Continuous Integration, Deployment Practices, TestFirst Data Pipeline Development","data engineering, data modeling, dashboard development, looker, powerbi, data processing pipeline development, airflow, dagster, prefect, dbt, ml environments, pyspark, pandas, software engineering best practices, tools including cicd and git, data application in ml and analytics, data mesh concepts, big dataanalytics opensource projects, internal data infrastructure products, endtoend data solutions, continuous integration, deployment practices, testfirst data pipeline development","airflow, big dataanalytics opensource projects, continuous integration, dagster, dashboard development, data application in ml and analytics, data engineering, data mesh concepts, data processing pipeline development, datamodeling, dbt, deployment practices, endtoend data solutions, internal data infrastructure products, looker, ml environments, pandas, powerbi, prefect, software engineering best practices, spark, testfirst data pipeline development, tools including cicd and git"
Staff Data Engineer,SIPE Education,"San Francisco, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-sipe-education-3757941996,2023-12-17,Berkeley,United States,Mid senior,Onsite,"The Role
Linktree’s Data Engineering team is at the forefront of transforming the way Linktree leverages data, working closely with the insights, data platform product and marketing teams to become truly data driven.
As a Staff Data Engineer at Linktree, you will be the driving force behind scaling how data is consumed at Linktree. Not only will you be modelling some of our most important data sets to gain insights, such as the data that helps us understand the driving factors behind the success of our product or our revenue, we take self-service one step further. You will also build the platform that can be used by Data Analysts, Marketers, Engineers and anyone else who is interested to model their own data to fit their specific needs and not be dependent on a data team.
This is your opportunity to make an impact at Linktree and push the boundaries of what is possible with data self-service!
Location Expectations:
Hybrid. We're growing our team in LA and the Bay Area, and plan to have offices in both locations. We expect team members to come into their respective office 2x/week.
What You Will Do
Build the platform that allows data analysts, product managers, marketers and other heavy data users to model their own data as well as making that data available in tools of their choosing, such as product analytics tools, BI tools, an experimentation platform or our CRM system.
Continue leading the transformation to make data a first-class citizen in software development at Linktree. This includes defining standards and providing guidance to product teams who emit the vast majority of our data.
Build and maintaining robust, efficient and integrated data models. We don’t believe that Analytics Engineers should be painfully cleaning up after others and deal with bad data all day long, if you are faced with a “garbage in, garbage out” situation, you will work with the engineers in our product teams to come up with a better way to emit the raw data.
You will also provide support to other data users including peer-reviews, training and acting as a data modeling/SQL/dbt SME across various business initiatives.
What We Are Looking For
A platform mindset. Data teams main focus is not to do repetitive data transformation and integration jobs for others. The team builds the infrastructure and tools that allows Linktree to perform these tasks at scale by enabling all teams to perform data-related jobs themselves.
You understand data-driven product development and have experience with the typical tools used by high-performing product teams, such product analytics, experimentation, but also general purpose BI tools.
Experience in data modeling with the ability to translate business requirements to fit for purpose data products for critical use-cases (reporting to the board, understanding key drivers of product success).
Experience operating at scale. You have worked on data systems that power a product that serves hundreds of millions of active users.
Proficient in SQL and experience with working on cloud-based warehouses (experience in Snowflake is a bonus), as well as working with DBT.
Linktree is committed to providing a competitive compensation package. Our cash compensation amount for this role is targeted at $175,000-$225,000 in the San Francisco Bay or Los Angeles area. Final offer amounts are determined by multiple factors including candidate expertise, the scope of role and level, and may vary from the amounts listed above.
P.S. If you don’t tick every box in this ad, please don’t rule yourself out. We take pride in inclusion and hiring incredible human beings with great potential over ticking boxes – so if this role resonates with you, hit that apply button!
Where And How We Work
We are a global and diverse group offering a truly flexible and family friendly work environment. Kids, pets, and the occasional delivery person are all actively encouraged to appear on our Zoom screens. All of us at Linktree work either fully remote or a hybrid ""remote, but in-office sometimes"" approach.
We currently have offices in Melbourne, Sydney and LA, but our team is spread across Australia, United States, and New Zealand. As our team approaches 200 people, our company will be 10x the size we were in 2020.
We offer autonomy and flexibility in how you structure your days and weeks. There will be the need for some collaboration outside of a ""normal"" 9-5 being a global company, but we aim to work asynchronously where possible.
Our Culture And Benefits
Linktree's company culture and values are based around collaboration, diversity, inclusion, and flexibility. Those are all nice words but to give you some more specific examples:
We recognize that our team are individually unique and have designed our benefits with this in mind. Each person has an annual allowance ($7,000 AUD / $4,900 USD) to use on things like (but not limited to) fitness memberships, development courses, childcare, travel, charitable donations, pet insurance, home office set up - the choice is yours!
We provide top-flight medical, dental, vision, disability and life insurance - we cover 100% of your monthly premiums (and 80% for your dependents).
Employee Stock Option Program - we want each and every employee to share in the company’s success as we go further together.
To learn more about our benefits, including our parental leave program, volunteering leave, DE&I initiatives, and more, click here!
Our Story
We're on a mission to empower anyone to curate and grow their digital universe. We created the ""link in bio"" category and are trusted by some of the world's biggest brands and celebrities including TikTok, The UN Environmental Program, The White House, F1, Manchester United, Selena Gomez, Alicia Keys, and Dwayne “The Rock” Johnson. With a flexible work environment and a team spread across multiple time zones, we offer autonomy and flexibility. Join us in empowering people to control their online presence!
At Linktree, we celebrate and support everyone’s perspective and background, and we’re proud to be an equal opportunity workplace. We aim to foster a diverse and inclusive environment where all team members have a sense of belonging, because we believe in
going further together.
Linktree welcomes all people regardless of sex, gender identity, race, ethnicity, disability, pregnancy, age, or other lived experience. If you require accommodations to fully participate in our opportunities, please don't hesitate to reach us at
recruiting@linktr.ee
– your needs are important to us.
Show more
Show less","Data engineering, Data modeling, SQL, DBT, Snowflake, Data analysis, Datadriven product development, Big data, BI tools, Product analytics, Experimentation, Cloudbased warehouses, Scalability, Data integration, Data transformation, Business intelligence, Data visualization, Data storytelling, Data governance, Data security","data engineering, data modeling, sql, dbt, snowflake, data analysis, datadriven product development, big data, bi tools, product analytics, experimentation, cloudbased warehouses, scalability, data integration, data transformation, business intelligence, data visualization, data storytelling, data governance, data security","bi tools, big data, business intelligence, cloudbased warehouses, data engineering, data governance, data integration, data security, data storytelling, data transformation, dataanalytics, datadriven product development, datamodeling, dbt, experimentation, product analytics, scalability, snowflake, sql, visualization"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"San Francisco, CA",https://www.linkedin.com/jobs/view/expression-of-interest-data-engineer-at-fingerprint-for-success-f4s-3787775759,2023-12-17,Berkeley,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
FkAz72pjXD
Show more
Show less","F4S work style assessment, Motivations, Behaviors, Performance, Predictive analytics, JazzHR","f4s work style assessment, motivations, behaviors, performance, predictive analytics, jazzhr","behaviors, f4s work style assessment, jazzhr, motivations, performance, predictive analytics"
"Lead Data Engineer, Data Productivity",SiriusXM,"Oakland, CA",https://www.linkedin.com/jobs/view/lead-data-engineer-data-productivity-at-siriusxm-3768758695,2023-12-17,Berkeley,United States,Mid senior,Hybrid,"Who We Are
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are. This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
How You'll Make An Impact
We are seeking a highly skilled and motivated Lead Data Productivity Engineer to join our dynamic team at SiriusXM. As a Lead Data Productivity Engineer, you will play a key role in designing, building, and maintaining the tools and services used by our data professionals in order to effectively drive the business in a data-driven manner. The ideal candidate will have a strong background in cloud technologies, data engineering, infrastructure as code, API design, and experience applying software engineering and DevOps best practices.
What You'll Do
Design, develop, and maintain tools and services that empower data professionals to streamline their workflows and enhance productivity.
Collaborate with cross-functional teams to understand data analysis, engineering, and modeling needs and translate them into effective and user-friendly solutions.
Implement best practices for optimizing data processing workflows, ensuring efficient utilization of resources, and minimizing latency in data-related tasks.
Identify and address bottlenecks in existing tools and services to improve overall system performance.
Integrate data productivity tools with existing data infrastructure and platforms, fostering seamless collaboration among teams.
Develop and implement automation solutions to streamline repetitive tasks and enhance the efficiency of data processes.
Create comprehensive documentation for tools and services, ensuring that users have access to clear and concise instructions.
Provide training and support to data professionals, enabling them to effectively leverage the tools and services developed.
Work closely with data scientists, engineers, and analysts to understand their requirements and challenges, fostering a collaborative and innovative environment.
Communicate project status, issues, and solutions effectively to stakeholders and team members.
What You’ll Need
Bachelor's degree in a relevant technical field (Computer Science, Information Technology, etc.), and 5+ years' career experience
Proven experience in software development, with a focus on tools and services for data professionals.
Proficiency in programming languages such as Python, Scala, or Java.
Experience with infrastructure as code tools such as CDK or Terraform.
Experience with big data technologies (e.g., Apache Spark, Hadoop) and data processing frameworks.
Knowledge of data storage solutions, database systems, and data warehousing.
Familiarity with machine learning frameworks and model development is a plus.
Excellent problem-solving skills and a proactive approach to addressing challenges.
Strong communication and collaboration skills.
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $126,000 to $145,800 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.
Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-11-87
Show more
Show less","Python, Scala, Java, CDK, Terraform, Apache Spark, Hadoop, Data storage, Database systems, Data warehousing, Machine learning, Model development, Data analysis, Data engineering, Data processing, Infrastructure as code, API design, DevOps","python, scala, java, cdk, terraform, apache spark, hadoop, data storage, database systems, data warehousing, machine learning, model development, data analysis, data engineering, data processing, infrastructure as code, api design, devops","apache spark, api design, cdk, data engineering, data processing, data storage, dataanalytics, database systems, datawarehouse, devops, hadoop, infrastructure as code, java, machine learning, model development, python, scala, terraform"
AI Data Engineer Lead (Principal),Grindr,"San Francisco, CA",https://www.linkedin.com/jobs/view/ai-data-engineer-lead-principal-at-grindr-3756733738,2023-12-17,Berkeley,United States,Mid senior,Hybrid,"This is a
hybrid
role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We at Grindr believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for for our ML engineers in Grindr to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to Grindr, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: Grindr is the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
About Grindr
Grindr is the world’s largest dating app for gay, bi, trans, and queer people. With around 13 million monthly active users, Grindr has become a fundamental part of the global LGBTQ community, and we take pride in empowering our users to connect, express themselves, and discover the queer world around them.
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
Grindr is an equal-opportunity employer
To learn more about how we handle the personal data of applicants, visit our
Employee and Candidate Privacy Policy
.
Grindr is committed to fair and equitable compensation practices. This base pay range is for the U.S. and is not applicable to locations outside of the U.S. The actual base pay is dependent upon many factors, such as training, transferable skills, work experience, business needs, location, and market demands. The base pay range is subject to change and may be modified in the future. This role will also be eligible for equity, benefits, and a company bonus program.
Base Pay Range
$160,000—$280,000 USD
Show more
Show less","Python, Java, Bash, SQL, Git, Pandas, R, Airflow, KubeFlow, Spark, PySpark, Snowflake, Kubernetes, Docker, Helm, DynamoDB, Big Data, ETL, Kafka, Storm, SparkStreaming, Machine Learning, NLP, Large Language Models, Microservices, Data Pre/Post Processing, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Data Pipelines, Data Platforms, Data Frameworks, Data Infrastructure, Data Governance, Risk and Compliance, Relational Databases, NoSQL Databases","python, java, bash, sql, git, pandas, r, airflow, kubeflow, spark, pyspark, snowflake, kubernetes, docker, helm, dynamodb, big data, etl, kafka, storm, sparkstreaming, machine learning, nlp, large language models, microservices, data prepost processing, data mining, data cleaning, data normalization, data modeling, data pipelines, data platforms, data frameworks, data infrastructure, data governance, risk and compliance, relational databases, nosql databases","airflow, bash, big data, data cleaning, data frameworks, data governance, data infrastructure, data mining, data normalization, data platforms, data prepost processing, datamodeling, datapipeline, docker, dynamodb, etl, git, helm, java, kafka, kubeflow, kubernetes, large language models, machine learning, microservices, nlp, nosql databases, pandas, python, r, relational databases, risk and compliance, snowflake, spark, sparkstreaming, sql, storm"
Sr. Staff AI Data Engineer,Grindr,"San Francisco, CA",https://www.linkedin.com/jobs/view/sr-staff-ai-data-engineer-at-grindr-3755148798,2023-12-17,Berkeley,United States,Mid senior,Hybrid,"This is a
hybrid
role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We at Grindr believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for for our ML engineers in Grindr to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to Grindr, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: Grindr is the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
About Grindr
Grindr is the world’s largest dating app for gay, bi, trans, and queer people. With around 13 million monthly active users, Grindr has become a fundamental part of the global LGBTQ community, and we take pride in empowering our users to connect, express themselves, and discover the queer world around them.
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
Grindr is an equal-opportunity employer
To learn more about how we handle the personal data of applicants, visit our
Employee and Candidate Privacy Policy
.
Grindr is committed to fair and equitable compensation practices. This base pay range is for the U.S. and is not applicable to locations outside of the U.S. The actual base pay is dependent upon many factors, such as training, transferable skills, work experience, business needs, location, and market demands. The base pay range is subject to change and may be modified in the future. This role will also be eligible for equity, benefits, and a company bonus program.
Base Pay Range
$160,000—$280,000 USD
Show more
Show less","Data Engineering, Machine Learning, Data Pipelines, Natural Language Processing, Python, Java, Bash, SQL, Git, Airflow, KubeFlow, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, ETL, Kafka, Storm, SparkStreaming, AWS, GCP, Azure, DynamoDB, Pandas, R","data engineering, machine learning, data pipelines, natural language processing, python, java, bash, sql, git, airflow, kubeflow, snowflake, kubernetes, docker, helm, spark, pyspark, etl, kafka, storm, sparkstreaming, aws, gcp, azure, dynamodb, pandas, r","airflow, aws, azure, bash, data engineering, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, natural language processing, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Informatics Data Analyst,Community Health Group,"Chula Vista, CA",https://www.linkedin.com/jobs/view/informatics-data-analyst-at-community-health-group-3683745216,2023-12-17,Chula Vista,United States,Associate,Onsite,"Position Summary
Meet the needs of system users by meeting with the appropriate people to determine business needs; gather information in order to define and program reports, dashboards, and other solutions to meet those business needs; coordinate business critical projects from beginning to end; monitor, maintain, and complete all required regulatory reports and processes.
COMPLIANCE WITH REGULATIONS
Work closely with all departments necessary to ensure that the processes, programs, and services are accomplished in a timely and efficient manner in accordance with CHG policies and procedures and in compliance with applicable state and federal regulations, including the Department of Health Care Services (DHCS) and the Centers for Medicare & Medicaid Services (CMS).
Requirements
RESPONSIBILITIES
Support company leadership by building reports, dashboards, and business intelligence solutions based on current business and compliance needs
Perform thorough data validation and reconciliation of reports to ensure accuracy
Analyze healthcare costs, member demographics, call center information, referral patterns, provider network accessibility, and other company data as needed
Provide analysis on enrollment, claims, authorization, and financial data as it pertains to Managed Care and construct necessary reports and datasets using SQL
Provide technical assistance by working independently or with Informatics Manager to assist users in the design and planning of projects; write, test, troubleshoot, and implement solutions using the standards established by the department; provide maintenance support; work with Informatics Manager to develop necessary databases; attend ongoing meetings related to projects; working on multiple projects concurrently
Provide information by writing documentation such as specifications, flowcharts, procedures, and training documentation; coordinate the development of solutions including reports and dashboards as required by the department, company, state and federal agencies; provide instruction and training for users and staff regarding new reports and solutions; participate in the evaluation of new software systems; provide solutions to users for technical and procedural problems; provide regular status reports to supervisor
Define and execute appropriate methods; maintain and develop the technical skills necessary to support the current computer system; adhere to standard testing methodology to ensure that new or modified programs have no adverse impact on systems operation
Enhance professional growth and development by participating in educational programs, reading current literature, attending in-service training and workshops, attending meetings as required, and participating on committees as directed
Support the team effort by arranging and conducting meetings; assist in the care and maintenance of department facilities, computer equipment, and supplies; maintain departmental policies and procedures, objectives, quality assurance program, safety, environmental and infection control standards; perform other related duties as requested or assigned
Maintain product and company reputation and contribute to the team effort by conveying professional image
EDUCATION
Bachelor’s degree in Computer Science, Information Systems, Mathematics, or related technical discipline.
Experience/Skills
Five years of experience in database analysis, advanced SQL programming in SQL Server, Microsoft Access or equivalent relational database management tool
Experience in developing database and report specifications, test plans, and system documentation is preferred
Knowledge of SQL Server Reporting Services (SSRS), Microsoft Excel, and Visual Basic
Excellent oral and written communication; good analytical, logical and organization skills; excellent time management; ability to work on multiple projects concurrently; adherence to security requirements and confidentiality as required by the company and HIPAA
Experience with Tableau, Power BI, or other BI tool preferred
Experience in healthcare preferred
Experience with QNXT, Vistar, or eVIPs a plus
Physical Requirements
Minimum 30 lb. Lifting
May be required to work evenings and/or weekends
Show more
Show less","SQL Server, Microsoft Access, SQL Server Reporting Services, Microsoft Excel, Visual Basic, Tableau, Power BI, QNXT, Vistar, eVIPs","sql server, microsoft access, sql server reporting services, microsoft excel, visual basic, tableau, power bi, qnxt, vistar, evips","evips, microsoft access, microsoft excel, powerbi, qnxt, sql server, sql server reporting services, tableau, vistar, visual basic"
Principal Communications Data Analyst Engineer,Northrop Grumman,"San Diego, CA",https://www.linkedin.com/jobs/view/principal-communications-data-analyst-engineer-at-northrop-grumman-3784989719,2023-12-17,Chula Vista,United States,Associate,Onsite,"At Northrop Grumman, our employees have incredible opportunities to work on revolutionary systems that impact people's lives around the world today, and for generations to come. Our pioneering and inventive spirit has enabled us to be at the forefront of many technological advancements in our nation's history - from the first flight across the Atlantic Ocean, to stealth bombers, to landing on the moon. We look for people who have bold new ideas, courage and a pioneering spirit to join forces to invent the future, and have fun along the way. Our culture thrives on intellectual curiosity, cognitive diversity and bringing your whole self to work - and we have an insatiable drive to do what others think is impossible. Our employees are not only part of history, they're making history.
At Northrop Grumman we develop cutting-edge technology that preserves freedom and advances human discovery. Our pioneering and inventive spirit has enabled us to be at the forefront of technological advancement in our nation's history - from the first flight across the Atlantic Ocean, to stealth bombers, to landing on the moon. We continue to innovate with developments from launching the first commercial flight to space, to discovering the early beginnings of the universe. Our employees are not only part of history, they are making history.
Northrop Grumman Aerospace Systems has an opening for a
Principal Communications Data Analyst Engineer
familiar with communications subsystems and advanced payload technologies to join our team of qualified, diverse individuals. This position is primarily supported out of
Rancho Bernardo, CA
but
Palmdale, CA
will be considered. Candidates hired out of Rancho Bernardo may have an occasional travel requirement between 10-25%. Most of the travel is limited to the Southern California area.
The candidate will be expected to lead a team of engineers to develop, integrate, and maintain an enterprise level toolset in support of cross-discipline engineering teams.
Essential Functions
Software design with MATLAB, Python, Java, and C++ in order to enable the exploitation of large sets of operational data.
Perform predictive trend analysis using AI/ML techniques.
Building and maintaining complex databases across multiple IT infrastructures.
Interpret complex Interface Control Documents (ICDs) and drawings to build new data structures from raw source information.
Communicate the progress and accomplishments of the team to senior leadership.
A candidate that is self-motivated and detailed oriented is desired in order to drive multiple tasks to completion simultaneously.
Basic Qualifications
Bachelor's Degree in a STEM (Science, Technology, Engineering and Mathematics) discipline from an accredited university with 5 years of related experience OR a Master's Degree in a STEM discipline with 3 years of related experience.
Active DoD Top Secret security clearance.
Ability to obtain and maintain Special Program Access (PAR).
Experience in Communication Subsystems or technologies.
Preferred Qualifications:
Bachelor's Degree in Data Science or Software Engineering from an accredited university with 5 years of experience or Master's Degree in Data Science or Software Engineering with 3 years of related experience.
Have software design experience with MATLAB, Python, Java, and C++.
Experience performing predictive trend analysis using AI/ML techniques.
Experience interpreting complex Interface Control Documents (ICDs) and drawings.
Northrop Grumman is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO/AA and Pay Transparency statement, please visit www.northropgrumman.com/EEO . U.S.
Salary Range:
$107,300 - $160,900
The above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions.
Employees may be eligible for a discretionary bonus in addition to base pay. Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results. Employees in Vice President or Director positions may be eligible for Long Term Incentives. In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business.
Northrop Grumman is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO/AA and Pay Transparency statement, please visit http://www.northropgrumman.com/EEO . U.S. Citizenship is required for most positions.
Show more
Show less","Software Design, MATLAB, Python, Java, C++, AI/ML, Predictive Trend Analysis, Databases, Interface Control Documents, ICDs, Data Science, Communication Subsystems","software design, matlab, python, java, c, aiml, predictive trend analysis, databases, interface control documents, icds, data science, communication subsystems","aiml, c, communication subsystems, data science, databases, icds, interface control documents, java, matlab, predictive trend analysis, python, software design"
Senior Data Engineer,Ivy Energy,"San Diego, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-ivy-energy-3677009979,2023-12-17,Chula Vista,United States,Mid senior,Onsite,"About Ivy
We are an early-stage, startup based out of San Diego, CA, incubated out of the Southern California Energy Innovation Network. Our team is on a mission to evolve the way that shared real estate properties create, use, and think about energy.
Ivy’s platform is providing the missing intelligence, enabling a turnkey shared solar system that provides equitable benefits to tenants and predictable returns to real estate owners. The platform uses innovative software design that includes proprietary load algorithms and community solar cost averaging logic to make onsite shared solar a win-win-win for owners, residents, and grid operators.
Role
We're seeking an experienced and dedicated Senior Data Engineer to join our growing team. This role will be critical in tackling the data challenges that come with being a rapidly evolving, data-centric organization. As a Senior Data Engineer, you will be responsible for planning, implementing, maintaining, and improving our data architecture and infrastructure to meet the needs of multiple stakeholders.
In this position, you will play a critical role in shaping how our platform evolves, ensuring it continues to deliver accurate, real-time insights that drive our mission of revolutionizing shared solar systems. You will work closely with various product teams to design and build out the data infrastructure that supports our large enterprise clients who are key to our continued growth. Lastly, as we continue to scale and evolve, you will spearhead the efforts to expand our data engineering team, playing a pivotal role in both recruitment and mentorship.
The Senior Data Engineer will report directly to the Chief Technology Officer (CTO) and will be an integral part of the Engineering team. This position will involve cross-functional collaboration across various roles and projects, requiring a unique blend of technical expertise and interpersonal skills.
Primary Responsibilities:
Design, develop, and maintain our data architecture, including databases, data pipelines, data warehouses, data lakes, and other data storage systems
Optimize data systems for performance, reliability, and scalability
Develop and maintain ETL workflows
Monitor and troubleshoot data systems to ensure data quality and integrity
Build tools to automate, streamline, and improve data engineering processes
Team Collaboration:
Collaborate with our data scientists and analysts to ensure our data architecture meets business requirements
Work with security engineers to meet security compliance requirements
Support hiring efforts for future data engineers
Provide mentorship and coaching to junior data engineers and other data-related roles
Requirements
Passionate about working in a startup environment and applying technology to solve environmental problems
Bachelor's degree or higher in Computer Science, Engineering, or a related field
Strong experience in data engineering, with a focus on data architecture and infrastructure
Great experience with AWS for data ingestion, storage, and processing
Strong experience in SQL, ETL workflows, and data warehousing
Proven ability to write clean, efficient, and maintainable code in a team environment
Expertise in data modeling, ETL development, and data warehousing
Proficiency in SQL and Python
Demonstrated problem-solving skills, with a track record of identifying, analyzing, and resolving complex problems in a fast-paced environment
Adaptable and flexible, with the ability to thrive in a startup setting where rapid iteration, continuous learning, and pivoting is common
Strong communication skills and ability to work collaboratively with cross-functional teams
Benefits
Stock Option Plan
Fringe benefits (Car, electric vehicle stipend)
Health insurance
Great office environment
Sick pay
PTO
Salary: $130K-160K
Show more
Show less","AWS, Data architecture, Data engineering, Data modeling, Data pipelines, Data science, Data storage, Data warehousing, ETL workflows, Java, Python, SQL, Cloud computing, Data ingestion, Data processing, Data quality, Data integrity, Data analytics, Problemsolving, Communication, Collaboration","aws, data architecture, data engineering, data modeling, data pipelines, data science, data storage, data warehousing, etl workflows, java, python, sql, cloud computing, data ingestion, data processing, data quality, data integrity, data analytics, problemsolving, communication, collaboration","aws, cloud computing, collaboration, communication, data architecture, data engineering, data ingestion, data integrity, data processing, data quality, data science, data storage, dataanalytics, datamodeling, datapipeline, datawarehouse, etl workflows, java, problemsolving, python, sql"
Senior Data Engineer II,Biological Dynamics,"San Diego, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-ii-at-biological-dynamics-3725577909,2023-12-17,Chula Vista,United States,Mid senior,Onsite,"Biological Dynamics, Inc. is a biotechnology company committed to improving global health outcomes by detecting disease in its earliest stages. Our proprietary Verita™ platform simplifies access to biomarkers, nanoparticles, and nucleic acids, enabling differentiated multiomics applications. We are applying our proprietary methods with machine learning to detect early-stage cancer and other diseases. For more information, please visit
Our talented staff thrive in an open working environment where curiosity, insights and vision are part of the equation. With an inclusive culture and comprehensive benefits package, we offer a terrific opportunity to discover, learn and grow. If you are ready to be inspired, challenged, and want to join a dynamic team where you can contribute to developing a revolutionary new way to understand diseases and save lives, you have come to the right place.
Job Summary
Biological Dynamics is seeking a Senior Data Engineer II to work hybrid in San Diego, Ca. The Senior Data Engineer will play a crucial role in designing, building, and maintaining data infrastructure and pipelines that support our vision statement – “A world where illness is never diagnosed too late.” The Senior Data Engineer will take responsibility for Biological Dynamic’s Extract Transform Load (ETL) process, transitioning raw measurement results to scalable pipelines, datasets, and cloud infrastructure. As the primary interface for Biological Dynamics' datasets, this position will collaborate with cross-functional teams to ensure data availability, reliability, and scalability, enabling data-driven decision-making and innovation within the organization.
Design, develop, and maintain robust and scalable data pipelines that collect, process, and store data from various sources, including medical devices and diagnostics tools in a regulated environment.
Integrate disparate data sources, both structured and unstructured, to create a unified and comprehensive data repository for analysis and reporting.
Implement data quality checks and validation processes to ensure the accuracy, completeness, and consistency of data.
Optimize data pipelines and infrastructure for maximum performance and efficiency, ensuring minimal latency in data processing.
Work closely with data scientists, analysts, and other stakeholders to understand their data requirements and provide data engineering support for analytical and machine learning projects.
Proactively monitor data pipelines and infrastructure for issues, troubleshoot and resolve data-related problems, and perform routine maintenance tasks.
Create and maintain documentation for data pipelines, infrastructure, and processes to facilitate knowledge sharing and onboarding of new team members.
Keep up to date with industry best practices, emerging technologies, and trends in data engineering and healthcare informatics.
Design and implement data pipelines using Azure Cloud Services and Python. Build scalable data warehouses using Azure Cloud Services.
Design and build robust data models for various cases such as reporting, API, or web applications.
Maintain and improve existing data pipeline processes, services, and web applications (Django).
Proactively contribute to a high level of understanding of product requirements, industry needs, and patent applications.
Experience/Education/Skills
Typically requires a minimum of 8 years of related experience and a Bachelor's and/or at least 6 years and a related Master's degree in a scientific/engineering discipline, or a PhD and at least 4 years, equivalent combination of education and experience.
5+ years of experience developing API services using Python/Azure Cloud Services.
5+ years of experience utilizing Apache Spark (Pyspark) and Databricks.
Must be knowledgeable in Python and Python packages - pandas, AzureML.core, MLflow.
Must be proficient with Python and SQL (can solve easy and medium difficulty Leetcode problems).
Strong experience with data warehousing, ETL processes, and data modeling.
Familiarity with industry compliance regulations, including HIPAA and FDA, is highly preferred.
Must be well versed in container services such as Azure Container Apps.
Must be knowledgeable in other technology stocks such as web framework (Django).
The estimated base salary range for this role is: $140,000 - $160,000 annually. Compensation decisions are dependent on several factors including, but not limited to, an individual’s qualifications, location where the role is to be performed, internal equity, and alignment with market data.
Biological Dynamics is an Equal Opportunity Employer that does not discriminate on the basis of actual or perceived race, color, religious creed, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth and related medical conditions), gender identity or expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, protected medical condition as defined by applicable state or local law (such as cancer), genetic information, or any other characteristic protected by applicable federal, state or local laws and ordinances.
Show more
Show less","Azure, Apache Spark, Azure ML.core, Azure Container Apps, Databricks, Django, Hadoop, ETL, Leetcode, MLflow, pandas, Python, Pyspark, SQL","azure, apache spark, azure mlcore, azure container apps, databricks, django, hadoop, etl, leetcode, mlflow, pandas, python, pyspark, sql","apache spark, azure, azure container apps, azure mlcore, databricks, django, etl, hadoop, leetcode, mlflow, pandas, python, spark, sql"
Depot Database Engineer (Access) SD23-84,Centurum,"San Diego, CA",https://www.linkedin.com/jobs/view/depot-database-engineer-access-sd23-84-at-centurum-3787925725,2023-12-17,Chula Vista,United States,Mid senior,Onsite,"Duties of Position
Provides MS Access Database engineering expertise to support recommendations, designs, development, integration, testing, troubleshooting, installations, configurations, maintenance, and documentation required of current and future technologies.
Significant experience developing business software solutions that help automate company workflow and tasks. - Actively participate in and implement feedback from code reviews
Proactively identify product/systems enhancements which may improve tool appeal, process flow, and overall functionality and performance
Work well autonomously and within a team environment.
Great team attitude and able to work comfortably with technical staff as well as business and non-technical individuals
Required Skills & Experience
Access database development experience using linked tables, unbound forms, VBA code, DAO/ADO
SQL Server Administration/Development
Schema design
T-SQL with stored procedures and views
Query Analyzer
Indexing
SQL scripting
User defined functions/common table expressions
Application/Database roles
Reporting Services
Integration Services
Basic networking skills
PHP/Web development
AWS/RDS experience
Visual Studio/Dev Ops
This position requires a Secret US DoD security clearance.
Preferred Skills
SEI CMMI ML 2 or 3 experience desirable. - Good working knowledge of security best practices
Remarks
Must be able to communicate effectively in English, both verbally and in writing; and interface effectively with all levels of technical personnel, explain technical information, respond to routine issues or situations and maintain the confidentiality of sensitive information.
Compensation: $55.32 - $64.04 an hour.
Compensation for positions at Centurum vary depending on a wide range of factors including, but not limited to, location, responsibilities, skill set, and level of experience.
EOE M/F/Disability/Veteran
When responding to this posting please reference job # SD23-84 Depot Database Engineer (Access)***
The majority of our positions require a DoD security clearance
Benefits
Full-time employees are eligible for the following benefits enrollment from their date of hire:
Health Insurance - Centurum provides insurance for employee and dependent in a comprehensive package. Coverage for vision care is included. This option is available on a cost-sharing basis.
Dental Insurance - Available in conjunction with Health Insurance for an additional cost. Provides oral maintenance care for employee and dependent.
Basic Life Insurance - Company provided benefit for all full-time employees.
Supplemental Life Insurance - Optional life insurance coverage to employees at group rates.
Dependant Life Insurance - Optional coverage for dependents at a group rate.
Long Term Disability Insurance - Optional coverage available to employees at group rates.
Vacation and Sick Leave - Leave accrual is determined by length of service.
Holidays - The company observes ten paid holidays each year.
Retirement 401(k) Plan - Centurum’s corporate benefits package includes 401K with a company bi-weekly match and a year-end profit sharing company match for all eligible employees. Investments can be made into selected funds under this plan.
Centurum is an Equal Opportunity Employer, providing employment opportunities for all persons without discrimination on the basis of race, color, religion, sex, sexual orientation, national origin, age, disability, marital status, citizenship or any other characteristic protected by U.S. law. Centurum makes reasonable accommodations for persons with disabilities.
Powered by JazzHR
FlLKJf3DF3
Show more
Show less","Microsoft Access, VBA, SQL Server, TSQL, Stored procedures, Views, Query Analyzer, Indexing, SQL scripting, User defined functions, Common table expressions, Application roles, Database roles, Reporting Services, Integration Services, PHP, Web development, AWS, RDS, Visual Studio, Dev Ops, SEI CMMI, AWS, Networking, Security best practices","microsoft access, vba, sql server, tsql, stored procedures, views, query analyzer, indexing, sql scripting, user defined functions, common table expressions, application roles, database roles, reporting services, integration services, php, web development, aws, rds, visual studio, dev ops, sei cmmi, aws, networking, security best practices","application roles, aws, common table expressions, database roles, dev ops, indexing, integration services, microsoft access, networking, php, query analyzer, rds, reporting services, security best practices, sei cmmi, sql scripting, sql server, stored procedures, tsql, user defined functions, vba, views, visual studio, web development"
Senior Data Warehouse Engineer (Database Administrator),San Diego Metropolitan Transit System (MTS),"San Diego, CA",https://www.linkedin.com/jobs/view/senior-data-warehouse-engineer-database-administrator-at-san-diego-metropolitan-transit-system-mts-3687374315,2023-12-17,Chula Vista,United States,Mid senior,Onsite,"Job Description
What a Senior Data Warehouse Engineer Does:
Essential Functions
Under the direction of the IT Development Manager, the Senior Data Warehouse Engineer is responsible first and foremost for the administration and maintenance of the MS SQL Server database infrastructure and dependent systems. The position acts as the primary technical resource for database integrity, backup and restoration, and performance monitoring. In addition, the Senior Data Warehouse Engineer is responsible for the overall design, implementation, and maintenance of the Enterprise Data Warehouse (EDW). Essential duties and responsibilities of this position include, but are not limited to, the following:
Administers, monitors, maintains, upgrades and troubleshoots existing database infrastructure across development, testing and production database environments.
Designs, implements and improves new database infrastructure components, applications, interfaces, replications, SSIS packages (ETL), stored procedures, etc. for both transactional and non-transactional databases.
Develops, documents and maintains enterprise best practices standards and procedures for database creation, upgrades, patches, backups, restoration, replication, index maintenance, tuning, monitoring, alerting and security.
Performs and monitors regular data imports from disparate internal and external systems, ranging from fully automated to manual processes, in order to meet the MTS operational reporting requirements.
Performs required server and software patches in conjunction with Datacenter Operations schedules and System Administrators (may require occasional after-hours support).
Establishes and maintains security and access controls for MTS database systems, applications, data, indexes, database services, replication packages and processes in conjunction with Datacenter Operations and System Administrators.
Manages the data integrity and replication of databases, monitoring database health and security, tuning database objects, creating, documenting, and maintaining Extract, Transform, Load (ETL) and Extract, Load, Transform (ELT) code, systems and storage capacity planning.
Plans, designs, builds, tests, implements, and troubleshoots complex data warehouse systems utilizing dimensional designs and schemas.
Creates and executes complex Structured Query Language (SQL) queries to retrieve data and create views for both customers and developers.
Follows processes and procedures for creation of Key Performance Indicators (KPIs) for the organization.
Designs, implements, and maintains both transactional and non-transactional databases.
Integrates data between various data sources and databases including but not limited to:Transactional and Non-Transactional SQL Server Databases,Oracle Databases,Vendor-hosted Cloud Databases, SAP ECC databases, SQL Azure Databases,Excel Spreadsheets, Files (e.g. Excel, .csv).
Creates Data Warehouse Roadmaps, Dataflow Diagrams, Entity Relationship Diagrams (ERD)s, Schema Diagrams, Data Dictionaries, and other Technical Documentation as required.
Duties May Include, But Are Not Limited To, The Following:
Designs, implements and maintains high availability database architecture.
Maintains business critical replication infrastructure, including legacy systems.
Designs and executes database queries and data analysis in response to requests from MTS departments.
Creates and maintains documentation for the Enterprise Data Warehouse (EDW) databases, database infrastructure and database processes and procedures.
Collaborates with IT staff and MTS departments in the design, development, tuning and troubleshooting of database infrastructure, services, servers and applications.
Performs other duties as assigned.
What MTS Is Looking For
Knowledge, Skills and Abilities
Knowledge of or ability to learn MTS policies and regulations; ability to read, understand and apply MTS policies, regulations and union labor contracts; ability to write letters, memoranda and reports using clear, concise and grammatically correct English; ability to speak clearly, distinctly and effectively in person-to-person or small group situations using tact and diplomacy; ability to coordinate and initiate actions necessary to implement decisions and delegate responsibilities to appropriate personnel; ability to establish and maintain priorities in order to complete assignments by deadlines without detailed instructions; skill in verifying the accuracy and completeness of forms and reports; knowledge of Microsoft Word and Excel and the ability to learn and use other software that MTS might have or acquire; exceptional verbal and written communication skills; ability to clearly communicate complex technical concepts to individuals or groups with varying technical understanding; exceptional organizational, prioritization and multi-tasking skills; exceptional interpersonal skills and understanding of customer relationship management; exceptional collaborative and team-centric working style; good stress-management coping skills and ability to work well under pressure.
Special Skills/Knowledge
Knowledge of MS SQL 2008/2012/2014/2016/2019 RDBMS and MS Windows Back Office systems.
Knowledge of T-SQL and writing and maintaining scripts and queries.
Knowledge of MS SSIS, DTS, triggers and stored procedures.
Knowledge of reporting tools, in particular MS SSRS and Business Objects.
Knowledge, of Extract Transform Load (ETL) and Extract Load Transform (ELT) processes utilizing tools such as SQL Server Integration Services (SSIS).
Knowledge of Enterprise Data Warehouse concepts, MS SQL Server enterprise database administration standards, processes and procedures.
Knowledge of enterprise MS SQL backup and disaster-recovery processes, procedures, policies and best practices (experience with CommVault desirable).
Knowledge of MS SQL Server performance tuning, and the design and implementation of high availability database architecture.
Familiarity with MS development technologies, C#, ASP.NET and web technologies, HTML, Javascript, CSS, XML, etc. is highly desirable.
Knowledge of data modeling -- normalization, schemas (star and snowflake), facts, dimensions.
Knowledge of SQL Azure and extension of On-premises environments to the cloud is highly desirable.
Knowledge of SAP is a plus.
Physical Requirements
The successful candidate must be able to fulfill the physical demands of the job such as walking, stooping, sitting, bending, reaching for overhead files and occasional lifting (must be able to lift up to 15 pounds). Must be able to operate a motor vehicle and perform tasks involving manual dexterity, such as use of a computer and 10-key. Work will at times require more than 8 hours per day or an irregular work week to perform the essential duties of the position. Duties will be performed primarily in an office type environment and may require travel to external locations and agencies. Must be able to work on-call.
Experience/Education/Certificates/License(s)
Possess a bachelor's degree from an accredited college or university in Computer Science, Information Technology, Management Information Systems or related field. Must have a minimum of five (5) years of experience administering MS SQL server databases in an enterprise environment. Prior experience as the technical lead in administering, configuring, monitoring, tuning and troubleshooting mission-critical database infrastructure is required. Must possess and maintain a valid California driver's license.
Current Certifications in one or more the following are highly desirable:
Microsoft Certified Database Administrator (MCDBA) (Legacy)
Microsoft Certified Solutions Expert (MCSE): Data Platform
Microsoft Certified Solutions Master (MCSM)
General
Must satisfactorily pass all applicable examinations including, but not limited to, a pre-employment physical, drug screen and background check.
Salary Range
#11 ($84,899 - $172,663)*
Hiring Range is usually mid-range & DOQ
DISCLAIMER: The above described job elements are intended to indicate the general nature and levels of work being performed by employees assigned to the job. They are not intended to be an exhaustive list of duties, responsibilities and skills required of employees so classified. Management retains the discretion to add to or change the duties of the position at any time.
Show more
Show less","MS SQL Server, TSQL, SSIS, DTS, Stored procedures, SSRS, Business Objects, SQL Azure, C#, ASP.NET, HTML, JavaScript, CSS, XML, Data modeling, SAP, Dimensional designs, Schemas, Star and snowflake schemas, Facts, Dimensions, ETL, ELT, Data Warehouse Roadmaps, Dataflow Diagrams, ERDs, Schema Diagrams, Data Dictionaries, High availability database architecture, CommVault, California driver's license, Microsoft Certified Database Administrator, Microsoft Certified Solutions Expert: Data Platform, Microsoft Certified Solutions Master","ms sql server, tsql, ssis, dts, stored procedures, ssrs, business objects, sql azure, c, aspnet, html, javascript, css, xml, data modeling, sap, dimensional designs, schemas, star and snowflake schemas, facts, dimensions, etl, elt, data warehouse roadmaps, dataflow diagrams, erds, schema diagrams, data dictionaries, high availability database architecture, commvault, california drivers license, microsoft certified database administrator, microsoft certified solutions expert data platform, microsoft certified solutions master","aspnet, business objects, c, california drivers license, commvault, css, data dictionaries, data warehouse roadmaps, dataflow diagrams, datamodeling, dimensional designs, dimensions, dts, elt, erds, etl, facts, high availability database architecture, html, javascript, microsoft certified database administrator, microsoft certified solutions expert data platform, microsoft certified solutions master, ms sql server, sap, schema diagrams, schemas, sql azure, ssis, ssrs, star and snowflake schemas, stored procedures, tsql, xml"
Senior Azure Data Engineer,"Kyocera International, Inc. (North America)","San Diego, CA",https://www.linkedin.com/jobs/view/senior-azure-data-engineer-at-kyocera-international-inc-north-america-3703506168,2023-12-17,Chula Vista,United States,Mid senior,Onsite,"Company Overview
With nearly 80,000 employees globally, Kyocera is a leading manufacturer of high-tech Ceramics which are used in a variety of industries including aerospace, automotive, medical applications, and semiconductor processing. You will find our innovative materials in everything from smartphones to space shuttles! Check out our profile video!
Look at these
PERKS
!
Competitive pay, benefits, and hours
120 hours of vacation accrued per year to start (that's 3 weeks/year for regular 8-hour shifts!)
10 Paid Holidays per year
401(k)
401(k) company match
Pension
Medical insurance
Dental insurance
Vision insurance
Life insurance
Flexible Spending Account (FSA)
Employee Assistance Program
Flexible schedules
Tuition reimbursement
We have a long-tenured staff (many with over 30 years of service!), a strong company mission, and an excellent benefits package that includes Medical, Dental, Vision, Life Insurance, Paid time off to Volunteer, paid Leave options, Tuition Reimbursement, and employer-paid Pension and a 401(k) with both Roth and a healthy company match. Many of our larger locations also feature onsite gyms, walking tracks, exercise rooms, and even employee gardens.
We strive to have a diverse workforce made up of people from all backgrounds, including minorities, women, and veterans, who bring their experience to support the innovation and quality that Kyocera is known for.
Kyocera International, Inc. also has a robust corporate culture and philosophy based on the experiences and writings of our founder, Dr. Kazuo Inamori, which you can learn more about here: https://global.kyocera.com/inamori/philosophy/. Our company motto is “Do the right thing as a human being,” and we try to use that in our decision-making constantly.
Senior Azure Data Engineer (IT6032)
Job Description
GENERAL DESCRIPTION OF POSITION
Pay Range - $122,491.15 - 177,362.83 (Actual base pay based on factors such as relevant experience, education, market, qualifications, and skills)
Summary
The Senior Azure Data Engineer will invent mechanisms and processes for speed and scale for solution development and support. This position is for an innovative senior data engineer who is a self-driven and results-oriented individual who is not afraid of big challenges but is also able to focus on excellent end user support, and successfully drive programs from start to finish.
The Senior Azure Data Engineer oversees program management of medium to large projects with a well-rounded technical background in business intelligent solutions. The position drives business transformative, high-level initiatives as part of a large Digital Transformation strategic platform.
Requirements
5+ years of industry experience in data engineering and building world class data solutions.
Excellent communication skills and a passion for collaborating with a diverse team and help foster an environment that promotes effective teamwork, communication, collaboration, and commitment.
Builds relationships, effective teams, and networks that extend throughout the company.
Able to effectively communicate with senior management on deployment plans, project deliverables, and related matters.
Ability to think abstractly and deal with ambiguous/undefined problems.
Obsession with quality, operational excellence, and customer experience
Demonstrated ability to manage tradeoffs and evaluate opportunistic new ideas.
Excellent written and verbal communication skills
Willingness to roll up one’s sleeves to get the job done.
Eligible to work in an ITAR environment.
Ability to effectively speak and read English.
Essential Duties And Responsibilities
Build, design, maintain, and optimize data structures for data collection, management, transformation, and access in enterprise data lakes and warehouses.
Preparing data for predictive and prescriptive modeling
Review and improve where possible business processes, best practices, and data quality, efficiency, and reliability.
Proactively consults with management to determine goals and priorities of new projects and consider process management improvements.
Lead or oversee medium and large-scale change solutions focused on business process improvement.
Create and implement strategies and plans to ensure user adoption and proficiency of all enterprise data solutions, as well as to minimize overall risk.
Ensure project management techniques are used for all solutions and critical issue resolution including communications plan, training plan, impact assessment / resistance management plans.
Resolve system, applications, and process problems by leveraging Systems, Applications, Analysts, Business Functional Experts, Vendors, and contract personnel as required.
Drive engagement, organizational assessment, impact analysis, learning/training, communications, business readiness, deployment, and adoption measurement throughout projects.
Work with internal and external auditors to provide clarity on business practices and resolve issues.
Qualifications
Bachelor’s degree in computer science or equivalent experience.
Minimum 5 years of experience architecting enterprise data solutions
Inclusive of Data Warehouse fact and dimensional modeling concepts
Architecting data solutions for simplicity, massive scale, resiliency, and maintainability
Experience with ETL/ELT on transactional systems, such as SAP S4/HANA, Odoo, HubSpot.
Proficiency in Azure Synapse, Visual Studio, Azure DevOps, CI/CD, Pipelines, Data Factory, Python, Databricks, SQL
Experience with data tools such as Power BI and Tabular models (DAX) in Visual Studio, Purview
Expertise with cloud automation tooling such as ARM Templates and Terraform
Experience with distributed (multi-tiered) systems and databases
Experience/knowledge of cloud computing, enterprise computing, PaaS, IaaS and/or SaaS products
Experience in technical leadership in driving engineering roadmaps, mentoring, and helping others grow technically.
Experience taking a leading role in building a complex data modeling system.
Knowledge of functional areas, such as, order to cash, plan to produce, etc. is plus.
Education And Experience
Bachelor’s degree or 5 years of related experience. Working in a fast-paced manufacturing environment is a plus. Deep understanding of data warehousing and modeling techniques. Experience using Agile methodologies. Understanding of corporate audit programs (such as SOX) and ensuring system and procedure experience a plus.
Supervisory Responsibilities
Directly supervises a total of 0 employee(s). Carries out supervisory responsibilities in accordance with the organization’s policies and applicable laws. Responsibilities include recruiting, interviewing, hiring, and training employees; planning, assigning, and directing work; appraising performance, rewarding, and disciplining employees; addressing complaints and resolving problems.
PLANNING
Limited responsibility regarding specific assignments in planning time, method, manner, and/or sequence of performance of own work operations.
DECISION MAKING
Performs work operations which permit frequent opportunity for decision-making of importance, and which would significantly affect the operating efficiency of the department.
ANALYTICAL ABILITY / PROBLEM SOLVING
Moderately repetitive. Activities with slight variation using a definite set of processes or directions with some degree of supervision. Choice of learned things in situations which conform to clearly established patterns and modes.
PUBLIC CONTACT
No public contact.
Software Skills Required
Strong: Alphanumeric Data Entry, Database, Presentation/PowerPoint, Spreadsheet, Word Processing/Typing
Strong: Contact Management, Enterprise Resource Planning, Human Resources Systems, Programming Languages
PHYSICAL ACTIVITIES
The following physical activities described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions and expectations.
While performing the functions of this job, the employee is regularly required to walk, talk or hear; frequently required to stand, sit; and occasionally required to use hands to finger, handle, or feel, reach with hands and arms, stoop, kneel, crouch, or crawl.
The employee must occasionally lift and/or move up to 25 pounds. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, and ability to adjust focus.
ENVIRONMENTAL CONDITIONS
The following work environment characteristics described here are representative of those an employee encounters while performing essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
While performing the functions of this job, the employee is occasionally exposed to risk of electrical shock.
The noise level in the work environment is usually quiet.
Additional Information
The above statements are intended to describe the work being performed by people assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties and skills required. The duties and responsibilities of this position are subject to change and other duties may be assigned or removed at any time.
This position may require exposure to information subject to US export control regulations, i.e. the International Traffic in Arms Regulation (ITAR) or the Export Administration Regulations (EAR).
Kyocera International, Inc. values diversity in its workforce, and is proud to be an AAP/EEO employer. All qualified applicants will receive consideration for employment without regard to race, sex, color, religion, sexual orientation, gender identity, national origin, protected veteran status, or based on disability.
If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact Kyocera International, Inc.'s Human Resources team directly. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions.
Show more
Show less","Azure, Synapse, Visual Studio, DevOps, CI/CD, Pipelines, Data Factory, Python, Databricks, SQL, Power BI, Tabular models (DAX), ARM Templates, Terraform, SAP S4/HANA, Odoo, HubSpot, Alphanumeric Data Entry, Database, Presentation/PowerPoint, Spreadsheet, Word Processing/Typing, Contact Management, Enterprise Resource Planning, Human Resources Systems, Programming Languages","azure, synapse, visual studio, devops, cicd, pipelines, data factory, python, databricks, sql, power bi, tabular models dax, arm templates, terraform, sap s4hana, odoo, hubspot, alphanumeric data entry, database, presentationpowerpoint, spreadsheet, word processingtyping, contact management, enterprise resource planning, human resources systems, programming languages","alphanumeric data entry, arm templates, azure, cicd, contact management, data factory, database, databricks, devops, enterprise resource planning, hubspot, human resources systems, odoo, pipelines, powerbi, presentationpowerpoint, programming languages, python, sap s4hana, spreadsheet, sql, synapse, tabular models dax, terraform, visual studio, word processingtyping"
Senior Data Engineer,KORE1,"San Diego, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kore1-3683869119,2023-12-17,Chula Vista,United States,Mid senior,Onsite,"KORE1, a nationwide provider of staffing and recruiting solutions, has an immediate opening for a Senior Data Engineer in San Diego.
Essential Functions:
Develop and maintain robust and scalable ETL/ELT processes and pipelines, handling large volumes of data in various formats and destinations.
Identify, design, and implement process and architecture improvements to enhance infrastructure scalability, data delivery, and automated manual processes.
Proficiently write complex SQL queries and demonstrate a strong command of relational database systems, showcasing exceptional analytical and problem-solving abilities.
Represent complex algorithms in software, showcasing a deep understanding of database technologies, management systems, data structures, and algorithms, as well as expertise in database architecture testing methodologies.
Design and execute comprehensive test plans, develop debugging and testing scripts, and build testing tools to ensure the reliability and quality of data pipelines and processes, while documenting test results and procedures for future reference.
Oversee Data Governance and Data Cleansing initiatives, ensuring data integrity and compliance with relevant regulations, while also providing support for production issues and customer requests.
Offer engineering support for customer issues and bugs, conducting research and implementing effective fixes to maintain high-quality data solutions
Bachelor's or Master's (Preferred)
5+ years of relevant software development industry experience building and operating scalable, fault-tolerant, distributed systems
Database and software development experience with Python, SQL, Redshift, and experience with Amazon Web Services, Snowflake along with pub/sub streaming technologies like Kafka, Kinesis, Spark Streaming, etc.
Bonus: familiarity with Change Data Capture (CDC) tools such as Debezium, Oracle GoldenGate, or similar technologies.
Experience with container services.
Fluid with Amazon Web Services.
Experience with concurrency, multithreading, and the deployment of distributed system architectures.
Experience leading and shipping large scope technical projects in collaboration with multiple experienced engineers.
Excellent communication skills and the ability to work well within a team and across engineering teams.
Strong problem solver and have solid production debugging skills.
Thrive in a fast-paced environment and sees themselves as a partner with the business with the shared goal of moving the business forward.
High level of responsibility, ownership, and accountability
About Kore1
Specializing in professional and technical recruiting, KORE1 is committed to supporting top IT, Engineering, Creative, Scientific, Accounting and Finance professionals in their career paths. We build deep relationships with leading companies, connecting them to exceptional talent every day. With extensive industry expertise and unmatched opportunities, our goal is to provide a unique experience for our contractors and consultants as they prepare for their next role. We are passionate about matching the right people with the right companies.
Kore1 provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Kore1 complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Kore1 expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of Kore1's employees to perform their job duties may result in discipline up to and including discharge.
Show more
Show less","SQL, Python, Redshift, AWS, Snowflake, Kafka, Kinesis, Spark Streaming, Debezium, Oracle GoldenGate, ETL/ELT, Data Governance, Data Cleansing, Container Services, Concurrency, Multithreading, Debugging","sql, python, redshift, aws, snowflake, kafka, kinesis, spark streaming, debezium, oracle goldengate, etlelt, data governance, data cleansing, container services, concurrency, multithreading, debugging","aws, concurrency, container services, data governance, datacleaning, debezium, debugging, etlelt, kafka, kinesis, multithreading, oracle goldengate, python, redshift, snowflake, spark streaming, sql"
Senior Database Engineer,"Intrepid Studios, Inc","San Diego, CA",https://www.linkedin.com/jobs/view/senior-database-engineer-at-intrepid-studios-inc-3787785215,2023-12-17,Chula Vista,United States,Mid senior,Onsite,"Intrepid Studios is looking for a highly motivated and talented
Senior
Database Engineer
to join our platform development team, here in beautiful Southern California. This is a great opportunity to unleash your true potential. You will be responsible for building and deploying a robust, scalable database architecture for our next generation MMORPG, Ashes of Creation.
In this role you will be responsible for performing Database Engineering duties, including, but not limited to, the following: analyzing data models to maximize scalability and performance; evaluating/implementing/testing/maintaining different database technologies; deploying cloud database solutions; migrating databases; identifying and troubleshooting database performance issues; testing capacity and scalability; designing security measures and strategies for enterprise databases; setting standards for database operations; automating database operations; maintaining/tuning production databases.
Responsibilities
Build, deploy, maintain, and optimize mission-critical production database systems.
Recommend improvements to existing and proposed data models and access controls.
Analyze end-to-end usage scenarios, from game code to the storage layer.
Manage data and schema migrations in development and production.
Implement security best practices and procedures.
Document, manage, and normalize legacy data and suggest improvements.
Manage SQL databases, NoSQL databases and in-memory key-value stores.
Evaluate caching strategies for enhancing database performance and scalability.
Leverage coding/scripting skills to automate tasks and improve analysis and monitoring.
Implement and test high availability scaling, backups, and disaster recovery processes.
Configure metrics, monitoring, and alerting for database operations.
Ensure compliance with PII, CCPA, GDPR, PCI, etc requirements.
Required Qualifications
BS or MS in Computer Science, Engineering, or equivalent experience.
5+ years as a DBA, Database Engineer, or equivalent role.
3+ years experience engineering, administering and managing multiple NoSQL and/or SQL databases, preferably MongoDB and CockroachDB.
2+ years of experience with cloud platforms such as AWS and GCP.
3+ years of experience with at least one scripting language (Shell, PowerShell, Python, etc).
Deep understanding of database internals and best practices.
Experience managing production database downtimes, upgrades, and rollbacks.
Additional Desirable Experience
Game industry experience, specifically with deploying production game databases at scale.
Experience deploying databases within a Kubernetes environment.
Experience with complex cross-table and cross-database transactional locking strategies.
Experience with sharding large database systems at scale.
Experience with database triggers and associated code execution.
Experience with flat file databases and data warehouses/lakes.
Experience with database analysis and testing tools.
Experience with Linux and containerized environments (Docker).
Required Application Materials:
Resume & Portfolio
Cover Letter which should include:
Relevant information about yourself that goes beyond your resume
Why you are interested in working at Intrepid
What games you are currently playing
What We Offer:
Medical health benefits covered 100% for employee & dependents
Paid vacation & Sick days
Company Bonus’
Company matched 401k
Fun & creative environment
Healthy work life balance
Free onsite gym
Interview Overview:
Below you'll find an outline of the interview plan for this role. Please note that while this is what we expect the process to look like; we may ask you for supplemental information or require an additional step before making a final decision.
30 min interview with a member of our Talent Team
60 min interview with the hiring manager
Final interview with the executive team
References calls & a background check
The Legal Bits:
We offer competitive pay based on market standards plus a significant annual bonus structure. The base salary for this role is between $125,000 - $187,500, depending on skills and experience. Individual compensation decisions are based on a number of factors, including experience level, skillset, and balancing internal equity relative to peers at the company. We expect the majority of the candidates who are offered roles at our company to fall throughout the range based on these factors.
Job descriptions are not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that may be required of the employee.
We encourage individuals from all backgrounds, including race, gender identity, sexual orientation, disability status, to apply for positions. We are an equal opportunity employer and we're committed to a fair and consistent interview process and employer experience.
About Us:
Founded in 2015, Intrepid Studios is the independent developer and publisher of the upcoming and Most Anticipated MMORPG, Ashes of Creation. Based in beautiful Southern California, Intrepid has assembled a veteran AAA team of tremendous talent and experience in the MMORPG industry. The gaming industry can be a turbulent storm of hiring cycles and layoffs. We’ve set out to do something different, to break this cycle and create an independent studio capable of withstanding the chaos of this industry. We’ve brought on-board a team of professionals that hold a diverse set of skills, and who all share a singular goal: making fun games. The studio is designed to be an open place for collaboration and communication, a place that is conducive to team building and success. Our goal is to build a family that will laugh together, play together, succeed together, and stay together.
All posted positions are full-time with generous medical benefit options covered 100% by employer for employees and dependents plus paid vacation & sick days, and 401k with matching.
We're proud to be an Equal Opportunity Employer.
We are committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. Intrepid Studios will not tolerate discrimination or harassment based on any of these characteristics. Applicants of all ages, identities, and backgrounds are encouraged to apply. Intrepid Studios will provide reasonable accommodation to employees who have protected disabilities consistent with local law.
See more details on your right to work here
:
This employer participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. See more details here:
Note to Recruiters and Placement Agencies: We do not accept unsolicited agency resumes. Unsolicited resumes received will be considered our property and will be processed accordingly.
Powered by JazzHR
IdvM3OzrmC
Show more
Show less","SQL, NoSQL, MongoDB, CockroachDB, AWS, GCP, Shell, PowerShell, Python, Docker, Kubernetes, YML, API, JSON, Linux, Crosstable locking strategies, Crossdatabase transactional locking strategies, Sharding, Database triggers, Flat file databases, Data warehouses, Data lakes, Database analysis tools, Database testing tools","sql, nosql, mongodb, cockroachdb, aws, gcp, shell, powershell, python, docker, kubernetes, yml, api, json, linux, crosstable locking strategies, crossdatabase transactional locking strategies, sharding, database triggers, flat file databases, data warehouses, data lakes, database analysis tools, database testing tools","api, aws, cockroachdb, crossdatabase transactional locking strategies, crosstable locking strategies, data lakes, data warehouses, database analysis tools, database testing tools, database triggers, docker, flat file databases, gcp, json, kubernetes, linux, mongodb, nosql, powershell, python, sharding, shell, sql, yml"
Senior Data Warehouse Engineer,San Diego Metropolitan Transit System (MTS),"San Diego, CA",https://www.linkedin.com/jobs/view/senior-data-warehouse-engineer-at-san-diego-metropolitan-transit-system-mts-3748835115,2023-12-17,Chula Vista,United States,Mid senior,Onsite,"Advertised Title:
Data Warehouse Engineer
Internal Title:
Senior Data Warehouse Engineer
Under the direction of the IT Development Manager, the Senior Data Warehouse Engineer is responsible first and foremost for the administration and maintenance of the MS SQL Server database infrastructure and dependent systems. The position acts as the primary technical resource for database integrity, backup and restoration, and performance monitoring. In addition, the Senior Data Warehouse Engineer is responsible for the overall design, implementation, and maintenance of the Enterprise Data Warehouse (EDW). Essential duties and responsibilities of this position include, but are not limited to, the following:
Essential Functions
Administers, monitors, maintains, upgrades and troubleshoots existing database infrastructure across development, testing and production database environments.
Designs, implements and improves new database infrastructure components, applications, interfaces, replications, SSIS packages (ETL), stored procedures, etc. for both transactional and non-transactional databases.
Develops, documents and maintains enterprise best practices standards and procedures for database creation, upgrades, patches, backups, restoration, replication, index maintenance, tuning, monitoring, alerting and security.
Performs and monitors regular data imports from disparate internal and external systems, ranging from fully automated to manual processes, in order to meet the MTS operational reporting requirements.
Performs required server and software patches in conjunction with Datacenter Operations schedules and System Administrators (may require occasional after-hours support).
Establishes and maintains security and access controls for MTS database systems, applications, data, indexes, database services, replication packages and processes in conjunction with Datacenter Operations and System Administrators.
Manages the data integrity and replication of databases, monitoring database health and security, tuning database objects, creating, documenting, and maintaining Extract, Transform, Load (ETL) and Extract, Load, Transform (ELT) code, systems and storage capacity planning.
Plans, designs, builds, tests, implements, and troubleshoots complex data warehouse systems utilizing dimensional designs and schemas.
Creates and executes complex Structured Query Language (SQL) queries to retrieve data and create views for both customers and developers.
Follows processes and procedures for creation of Key Performance Indicators (KPIs) for the organization.
Designs, implements, and maintains both transactional and non-transactional databases.
Integrates data between various data sources and databases including but not limited to:Transactional and Non-Transactional SQL Server Databases,Oracle Databases,Vendor-hosted Cloud Databases, SAP ECC databases, SQL Azure Databases,Excel Spreadsheets, Files (e.g. Excel, .csv).
Creates Data Warehouse Roadmaps, Dataflow Diagrams, Entity Relationship Diagrams (ERD)s, Schema Diagrams, Data Dictionaries, and other Technical Documentation as required.
Duties May Include, But Are Not Limited To, The Following:
Designs, implements and maintains high availability database architecture.
Maintains business critical replication infrastructure, including legacy systems.
Designs and executes database queries and data analysis in response to requests from MTS departments.
Creates and maintains documentation for the Enterprise Data Warehouse (EDW) databases, database infrastructure and database processes and procedures.
Collaborates with IT staff and MTS departments in the design, development, tuning and troubleshooting of database infrastructure, services, servers and applications.
Performs other duties as assigned.
What MTS is Looking For:
Knowledge, Skills and Abilities
Knowledge of or ability to learn MTS policies and regulations; ability to read, understand and apply MTS policies, regulations and union labor contracts; ability to write letters, memoranda and reports using clear, concise and grammatically correct English; ability to speak clearly, distinctly and effectively in person-to-person or small group situations using tact and diplomacy; ability to coordinate and initiate actions necessary to implement decisions and delegate responsibilities to appropriate personnel; ability to establish and maintain priorities in order to complete assignments by deadlines without detailed instructions; skill in verifying the accuracy and completeness of forms and reports; knowledge of Microsoft Word and Excel and the ability to learn and use other software that MTS might have or acquire; exceptional verbal and written communication skills; ability to clearly communicate complex technical concepts to individuals or groups with varying technical understanding; exceptional organizational, prioritization and multi-tasking skills; exceptional interpersonal skills and understanding of customer relationship management; exceptional collaborative and team-centric working style; good stress-management coping skills and ability to work well under pressure.
Special Skills/Knowledge
Knowledge of MS SQL 2008/2012/2014/2016/2019 RDBMS and MS Windows Back Office systems.
Knowledge of T-SQL and writing and maintaining scripts and queries.
Knowledge of MS SSIS, DTS, triggers and stored procedures.
Knowledge of reporting tools, in particular MS SSRS and Business Objects.
Knowledge, of Extract Transform Load (ETL) and Extract Load Transform (ELT) processes utilizing tools such as SQL Server Integration Services (SSIS).
Knowledge of Enterprise Data Warehouse concepts, MS SQL Server enterprise database administration standards, processes and procedures.
Knowledge of enterprise MS SQL backup and disaster-recovery processes, procedures, policies and best practices (experience with CommVault desirable).
Knowledge of MS SQL Server performance tuning, and the design and implementation of high availability database architecture.
Familiarity with MS development technologies, C#, ASP.NET and web technologies, HTML, Javascript, CSS, XML, etc. is highly desirable.
Knowledge of data modeling -- normalization, schemas (star and snowflake), facts, dimensions.
Knowledge of SQL Azure and extension of On-premises environments to the cloud is highly desirable.
Knowledge of SAP is a plus.
Physical Requirements
The successful candidate must be able to fulfill the physical demands of the job such as walking, stooping, sitting, bending, reaching for overhead files and occasional lifting (must be able to lift up to 15 pounds). Must be able to operate a motor vehicle and perform tasks involving manual dexterity, such as use of a computer and 10-key. Work will at times require more than 8 hours per day or an irregular work week to perform the essential duties of the position. Duties will be performed primarily in an office type environment and may require travel to external locations and agencies. Must be able to work on-call.
Experience/Education/Certificates/License(s)
Possess a bachelor's degree from an accredited college or university in Computer Science, Information Technology, Management Information Systems or related field. Must have a minimum of five (5) years of experience administering MS SQL server databases in an enterprise environment. Prior experience as the technical lead in administering, configuring, monitoring, tuning and troubleshooting mission-critical database infrastructure is required. Must possess and maintain a valid California driver's license.
Current Certifications in one or more the following are highly desirable:
Microsoft Certified Database Administrator (MCDBA) (Legacy)
Microsoft Certified Solutions Expert (MCSE): Data Platform
Microsoft Certified Solutions Master (MCSM)
GENERAL:
Must satisfactorily pass all applicable examinations including, but not limited to, a pre-employment physical, drug screen and background check.
SALARY RANGE:
#11 ($84,899 - $172,663)*
*Hiring Range is usually mid-range & DOQ
DISCLAIMER:
The above described job elements are intended to indicate the general nature and levels of work being performed by employees assigned to the job. They are not intended to be an exhaustive list of duties, responsibilities and skills required of employees so classified. Management retains the discretion to add to or change the duties of the position at any time.
Show more
Show less","SQL, RDBMS, TSQL, SSIS, DTS, Stored Procedures, SSRS, Business Objects, ETL, ELT, EDW, SQL Azure, C#, ASP.NET, HTML, Javascript, CSS, XML, Data Modeling, Star Schemas, Snowflake Schemas, Facts, Dimensions","sql, rdbms, tsql, ssis, dts, stored procedures, ssrs, business objects, etl, elt, edw, sql azure, c, aspnet, html, javascript, css, xml, data modeling, star schemas, snowflake schemas, facts, dimensions","aspnet, business objects, c, css, datamodeling, dimensions, dts, edw, elt, etl, facts, html, javascript, rdbms, snowflake schemas, sql, sql azure, ssis, ssrs, star schemas, stored procedures, tsql, xml"
Data Engineer,"The Marlin Alliance, Inc.","San Diego, CA",https://www.linkedin.com/jobs/view/data-engineer-at-the-marlin-alliance-inc-3774669040,2023-12-17,Chula Vista,United States,Mid senior,Onsite,"The Marlin Alliance, Inc. is seekinga highly qualified and motivated mid-level
Data Engineer
with a background in Testingto join our team. This individual will play a critical role in supporting various technical initiatives and projects. This role requires a Bachelor's or Master's degree in Computer Science, Data Science, or a related field and an active Secret Clearance.
Established in 2002, The Marlin Alliance is seeking to hire highly skilled individuals to support mission critical projects within the Navy. We are looking for motivated individuals to lead and support digital transformation, data science and analytics, and automation projects for variety of Navy clients. Individuals must be able to function in a fast-paced work environment and able to adapt quickly to rapidly changing requirements and technologies. Using your comprehensive knowledge of various technologies, you will design, develop, and implement solutions to support Navy mission owners in their digital transformation journey.
Duties And Responsibilities
Collaborate with stakeholders to gather requirements and understand business objectives.
Write routines in scripting languages.
Design, develop, and maintain predictive analytics tools using Power BI and Power Apps.
Create interactive and visually appealing dashboards and reports to present insights effectively.
Implement data cleansing, transformation, and modeling processes to support predictive analytics.
Utilize machine learning algorithms and statistical techniques to build predictive models.
Ensure data accuracy and consistency throughout the analytics process.
Perform data analysis to identify trends, patterns, and anomalies.
Optimize and fine-tune predictive models for accuracy and performance.
Qualifications
Bachelor's or Master's degree in Computer Science, Data Science, or a related field.
3 years’ experience using Power BI and Power Apps.
Power Query or Python proficiency.
Data cleaning or transformation.
US Navy and/or Testing background.
Strong proficiency in data analysis, data visualization, and data modeling.
Expertise in machine learning, statistical analysis, and predictive modeling.
Excellent problem-solving and analytical skills.
Ability to translate business requirements into technical solutions.
Knowledge of best practices in data governance and security.
Relevant certifications (e.g., Microsoft Certified: Power BI, Azure AI Engineer) are a plus.
U.S. Citizenship is required for this position.
Work Environment and Mental/Physical Demands
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this position. Reasonable accommodations may be made to enable individuals with disabilities to perform the functions.
Typical office environment with no unusual hazards.
The noise level in the work environment is usually moderate.
Constant sitting while using the computer terminal.
Constant use of sight abilities while reviewing documents.
Constant use of speech/hearing abilities for communication.
Occasional reaching, stooping, kneeling, or crouching may be required.
Occasional lifting up to 20 pounds.
Constant use of mental alertness.
Frequent work under deadlines.
Job Classification
Associate II
$85,000 - $120,000
Disclaimer
This job description in no way states or implies that these are the only duties to be performed by the employee(s) incumbent in this position. Employees will be required to follow any other job-related instructions and to perform any other job-related duties requested by any person authorized to give instructions or assignments. All duties and responsibilities are essential functions and requirements and are subject to possible modification to reasonably accommodate individuals with disabilities.
To perform this job successfully, the incumbents will possess the skills, aptitudes, and abilities to perform each duty proficiently. Some requirements may exclude individuals who pose a direct threat or significant risk to the health or safety of themselves or others. The requirements listed in this document are the minimum levels of knowledge, skills, or abilities.
This document does not create an employment contract, implied or otherwise, other than an “at-will” relationship.
An Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities .
Show more
Show less","Data Engineering, Data Science, Computer Science, Power BI, Power Apps, Power Query, Python, Data Analysis, Data Visualization, Data Modeling, Machine Learning, Statistical Analysis, Predictive Modeling, Data Cleansing, Data Transformation, Microsoft Certified: Power BI, Azure AI Engineer, Business Intelligence, Data Governance, Data Security","data engineering, data science, computer science, power bi, power apps, power query, python, data analysis, data visualization, data modeling, machine learning, statistical analysis, predictive modeling, data cleansing, data transformation, microsoft certified power bi, azure ai engineer, business intelligence, data governance, data security","azure ai engineer, business intelligence, computer science, data engineering, data governance, data science, data security, data transformation, dataanalytics, datacleaning, datamodeling, machine learning, microsoft certified power bi, power apps, power query, powerbi, predictive modeling, python, statistical analysis, visualization"
Senior Software Engineer Database,MillenniumSoft Inc,"San Diego, CA",https://www.linkedin.com/jobs/view/senior-software-engineer-database-at-millenniumsoft-inc-3681905154,2023-12-17,Chula Vista,United States,Mid senior,Onsite,"Position : Senior Software Engineer Database
Location : San Diego, CA
Duration : 12 Months Contract
Total Hours/week : 40.00
Description
Department Overview
The Software Technology Solutions department is responsible for developing and maintaining on-premise and hosted software solutions that provide integration, reporting, and analytics capabilities for the company’s portfolio of products. Using industry best practices, the department is responsible for delivering high-quality, robust software applications that meet present and future requirements.
Purpose
The database developer is responsible for the design and development of database objects and procedural code to support various products for the organization. The database developer also creates programs to populate and maintain the data in the department’s transaction processing and data warehousing systems. This involves the creation of ETL programs along with the supporting stored procedures, functions, triggers, and constraints. The position requires extensive hands-on experience developing database queries, ETL, and associated code in a Microsoft SQL Server environment.
Responsibilities
Specific Duties, Activities, and Responsibilities
Understand and apply database design and development principles for both transactional and dimensional schemas to support application requirements.
Design, plan, and develop programs to optimally extract, transform, and load data from data sources to the target systems.
Design, develop and maintain SQL Server Reporting Services reports.
Maintain source code for database and ETL projects. Ensure that projects build successfully in a continuous integration environment.
Diagnose and resolve database performance issues.
Follow and improve development processes including but not limited to conducting peer/code reviews, complying with organizational standards, ensuring code maintainability, following build processes, implementing unit tests, and following other industry standard best practices.
Position Requirements
Education or Equivalent
BS in Computer Science or Information Systems
Experience or Equivalent
8+ years of experience working with Microsoft SQL Server with emphasis on database code, ETL and Reporting Services development.
Requirements
Knowledge, Skills, and Abilities Requirements
Database design skills including normalization and data warehouse design.
Strong hands-on T-SQL development skills including stored procedure, function, and trigger creation.
Excellent query optimization and performance tuning skills.
Extensive experience developing SSRS reports.
Strong SSIS skills.
Detailed analytical skills.
Structured troubleshooting skills.
Experience with data modeling tools such as ER/Studio or ERWin.
Experience using version control systems (e.g., TFS, Git).
Familiar with Agile software development process.
Working experience with Azure DB technologies and Azure deployments is a plus.
Experience in the healthcare industry is a plus.
Toolset
Microsoft SQL Server 2012 and above
Microsoft SQL Server Integration Services (SSIS)
Microsoft SQL Server Reporting Services (SSRS)
Microsoft Visual Studio 2012 and above
Embarcadero ER/Studio is a plus
Physical/Mental Requirements
Ability to communicate clearly both verbally and in writing.
Ability to analyze complex application and business operational issues.
Ability to multi-task and respond to quickly-shifting priorities.
Ability to work in a team and/or independently.
Any Additional/Important Information
The database developer will additionally be responsible for the following tasks:
Analyzing requirements in order to create software designs.
Estimation and timely completion of tasks.
Documenting developed modules.
Supporting application developers and software testers.
Creating and executing unit tests for developed code.
Resolving problems in the production system.
Following quality assurance processes.
Participate in daily team stand-ups, planning sessions, etc.
Show more
Show less","Microsoft SQL Server, SSIS, SSRS, TSQL, ER/Studio, Azure DB, Visual Studio, ETL, Database design, Data warehouse design, Query optimization, Performance tuning, SSRS reports, Agile development, Data modeling, Version control, Healthcare industry","microsoft sql server, ssis, ssrs, tsql, erstudio, azure db, visual studio, etl, database design, data warehouse design, query optimization, performance tuning, ssrs reports, agile development, data modeling, version control, healthcare industry","agile development, azure db, data warehouse design, database design, datamodeling, erstudio, etl, healthcare industry, microsoft sql server, performance tuning, query optimization, ssis, ssrs, ssrs reports, tsql, version control, visual studio"
"SR. Scala Engineer, Database Engineering",Experfy,"San Diego, CA",https://www.linkedin.com/jobs/view/sr-scala-engineer-database-engineering-at-experfy-3590303127,2023-12-17,Chula Vista,United States,Mid senior,Remote,"As a Sr. Software Engineer for our Data Platform Engineering team you will join skilled Scala engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL
processing frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership.
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, and others)
3+ years experience with Scala and Apache Spark
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Scala, Apache Spark, Apache Arrow, SQL, Data warehousing, Distributed computing, Query planning, Query optimization, Data routing, Cluster management, Scalability, Data management, Meta data management, Software engineering, Team leadership, Technical recruiting, Engineering fundamentals, Rapid development, Scripting, Test automation, Web3, Blockchain, Decentralization","scala, apache spark, apache arrow, sql, data warehousing, distributed computing, query planning, query optimization, data routing, cluster management, scalability, data management, meta data management, software engineering, team leadership, technical recruiting, engineering fundamentals, rapid development, scripting, test automation, web3, blockchain, decentralization","apache arrow, apache spark, blockchain, cluster management, data management, data routing, datawarehouse, decentralization, distributed computing, engineering fundamentals, meta data management, query optimization, query planning, rapid development, scala, scalability, scripting, software engineering, sql, team leadership, technical recruiting, test automation, web3"
Sr. Data Engineer,Motion Recruitment,"San Diego, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-motion-recruitment-3730275313,2023-12-17,Chula Vista,United States,Mid senior,Remote,"Job Description | DOES NOT SUPPORT: C2C OR Sponsorship Our client is a people search engine that utilizes deep web crawlers to aggregate data. Their main goal is to reconnect friends, reunite families, prevent fraud, and their most notable usage of their platform has been connecting adoptees to their biological parents.
They are looking for a Data Engineer to work fully remote on their ‘Data Operations Team,’ and help optimize their platform, data pipelines, data extractions, and data preparations. This will involve working with infrastructure built in AWS, including Spark EMR, S3, and DynamoDB. Additionally, this role is going to help build statistical tools, develop unit & stress tests, and create automation in the orchestrating of ETL pipelines.
As the role grows you will help in the determining of data governance, data quality, data cleansing, and ETL processes. Overall, you will have huge impact for the only data company that serves millions of users directly. They bolster a transparent culture and are promoters of flexibility and work-life balance. Required Skills & Experience
7 yrs. of experience
5 yrs. of Python experience
5 yrs. of Spark OR PySpark experience
Airflow
Strong understanding of DAG
AWS
Computer Science Degree (Highly Preferred)
The Offer You Will Receive The Following Benefits
12-15% bonus (50% performance; 50% on company performance)
Stock Options
Unlimited PTO
Sign on Bonus
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Connor Hart
Show more
Show less","Python, Spark, PySpark, Airflow, DAG, AWS, ETL, Data Governance, Data Quality, Data Cleansing, EMR, S3, DynamoDB","python, spark, pyspark, airflow, dag, aws, etl, data governance, data quality, data cleansing, emr, s3, dynamodb","airflow, aws, dag, data governance, data quality, datacleaning, dynamodb, emr, etl, python, s3, spark"
Staff Software Engineer - Datalake,Dremio,"San Diego, CA",https://www.linkedin.com/jobs/view/staff-software-engineer-datalake-at-dremio-3783882984,2023-12-17,Chula Vista,United States,Mid senior,Remote,"Be Part of Building the Future
Dremio is The Easy and Open Data Lakehouse, providing self-service analytics with data warehouse functionality and data lake flexibility across all of your data. Dremio increases agility with a revolutionary data-as-code approach that adopts Git concepts to enable data experimentation, version control, and governance. In addition, Dremio breaks down data silos by simplifying ingestion into the lakehouse, and also allowing queries directly on databases and data warehouses. All of this is available through a fully managed service that not only eliminates the need to maintain infrastructure and software, but also automatically optimizes the data in the lakehouse to maximize performance for every workload.
Founded in 2015, Dremio is headquartered in Santa Clara, CA. Investors include Cisco Investments, Insight Partners, Lightspeed Venture Partners, Norwest Venture Partners, Redpoint Ventures, and Sapphire Ventures. For more information, visit www.dremio.com. Connect with Dremio on GitHub, LinkedIn, Twitter, and Facebook.
If you, like us, say “bring it on” to exciting challenges that really do change the world, we have endless opportunities where you can make your mark.
About The Role
We are looking for an experienced Staff Software Engineer to enhance Dremio’s data warehouse capabilities on top of the datalakes across all major table/file formats and object stores. These capability advancements will increase our competitive position in the market and enable Dremio adoption for a larger set of customers.
What You’ll Be Doing
Develop core components for Dremio’s query engine
Deliver key features and feature enhancements for our customers in the Datalake like DML operations, time travel, schema evolution along with performance and reliability improvements
Work with open source projects like Apache Iceberg, Parquet, Arrow and Calcite
Own design, implementation, testing, and support of next-generation features related to scalability, reliability, robustness, and performance of the product
Collaborate with Product Management to innovate and deliver on customer requirements and with Support and field teams to ensure customer success
Understand and reason about concurrency and parallelization to deliver scalability and performance in a multithreaded and distributed environment
Solve complex technical problems and customer issues while improving our telemetry and instrumentation to proactively detect issues before they arise and make debugging more efficient
Work with engineering leaders to establish solid designs/architecture for upcoming features.
Develop the future leaders of Dremio by providing continuous mentorship and coaching of junior software engineers, help with hiring and onboarding
What We’re Looking For
8+ year of industry experience
B.S./M.S/Equivalent in Computer Science or a related technical field or equivalent experience
Fluency in Java, C++ or another modern language
Strong database fundamentals including SQL, performance, and schema design and background in large scale data processing systems (e.g., Hadoop, Spark, etc.)
Understanding of distributed file systems such as S3, ADLS, or HDFS
Experience with Apache Iceberg, Parquet, AVRO and/or Delta Lake
Experience with Hive and AWS Glue
Ability to solve ambiguous, unexplored, and cross-team problems effectively
Interested and motivated to be part of a fast-moving startup with a fun and accomplished team
Big picture thinking, ability to scope and plan solutions for big problems and mentor others on the same
Bonus points if you have
Hands-on experience with distributed query engines, query processing or optimization, distributed systems, concurrency control, data replication, code generation, or storage systems
Hands on experience with AWS, Azure and Google Cloud Platform
What We Offer
Medical, dental and vision insurance
401(k) Plan
Short term / long term disability and life insurance
Pre-IPO stock options
Flexible PTO
16 hours of volunteer time off
12 company paid holidays, including Juneteenth
Remote work options
Paid parental leave
Employee Assistance Program (EAP)
Biannual swag surprise
Certain benefits are only allowed to full-time Dremio employees and may not be the same across all locations.
The base salary range for this position is $154,545 to $209,091 per year. The base salary actually offered to a successful candidate will take into account various relevant and non-discriminatory business factors including, without limitation, the candidate’s geographic location, job-related experience, knowledge, and skills, and education, as well as internal equity considerations. A successful candidate may also be eligible to earn additional compensation including commissions and/or bonuses.
What We Value
At Dremio, we hold ourselves to high standards when it comes to People, Thinking, and Action. Our Gnarlies (that's what we call our employees) communicate with clarity, drive accountability, and are respectful towards each other. We confront brutal facts and focus on results while operating with a sense of urgency and building a ""flywheel"". People who like to jump in and drive momentum will thrive in our #GnarlyLife.
Dremio is an equal opportunity employer supporting workforce diversity. We do not discriminate on the basis of race, religion, color, national origin, gender identity, sexual orientation, age, marital status, protected veteran status, disability status, or any other unlawful factor.
Dremio is committed to providing any necessary accommodations for individuals with disabilities within our application and interview process. To request accommodation due to a disability, please inform your recruiter.
Dremio has policies in place to protect the personal information that employees and applicants disclose to us. Please click
here
to review the privacy notice.
Show more
Show less","Dremio, Software Engineering, Data Warehouse, Big Data, Data Lakehouse, Analytics, Java, C++, SQL, Hadoop, Spark, Apache Iceberg, Parquet, Arrow, Calcite, Hive, AWS Glue, S3, ADLS, HDFS, Distributed Systems, Concurrency Control, Data Replication, Code Generation, Storage Systems, AWS, Azure, Google Cloud Platform","dremio, software engineering, data warehouse, big data, data lakehouse, analytics, java, c, sql, hadoop, spark, apache iceberg, parquet, arrow, calcite, hive, aws glue, s3, adls, hdfs, distributed systems, concurrency control, data replication, code generation, storage systems, aws, azure, google cloud platform","adls, analytics, apache iceberg, arrow, aws, aws glue, azure, big data, c, calcite, code generation, concurrency control, data lakehouse, data replication, datawarehouse, distributed systems, dremio, google cloud platform, hadoop, hdfs, hive, java, parquet, s3, software engineering, spark, sql, storage systems"
Cloud Data Architect - Remote,Get It Recruit - Information Technology,"San Diego, CA",https://www.linkedin.com/jobs/view/cloud-data-architect-remote-at-get-it-recruit-information-technology-3774133679,2023-12-17,Chula Vista,United States,Mid senior,Remote,"We are a dynamic and innovative tech company bringing the latest in technology and automation to over 10,000 legal and accounting firms. At our core, we empower highly skilled professionals to refocus their expertise on what truly matters. Our suite of powerful and secure practice management, document automation, and payment processing solutions takes firms, professionals, and their clients further.
Our Values
Our team-defined values guide how we show up for each other, for our partners, and for our customers:
We succeed together.
We embrace progress.
We care big.
We create space.
The Role
Are you a dynamic and visionary Cloud Data Architect with a desire to make a difference and be a key player in shaping the future of Cloud Data Analytics? If you're hands-on with managing cloud data technologies related to Data Analytics, Big Data, Data Warehouses, and Data Lakes, we want you to join our growing engineering team.
In this role, you'll unleash your innovation, strategic thinking, and technical prowess to revolutionize how we store, manage, and analyze data in the cloud. You'll be working on building a cloud data platform that allows our customers and partners access to tools and technologies, providing actionable insights into their business.
Requirements
Bachelor’s degree in computer science, Information Systems, or a related discipline.
7+ years of experience in Data Engineering with expert-level hands-on experience as a data architect or solution architect.
Highly skilled with hands-on experience in both data and analytics technologies, as well as cloud-native data architecture design and best practices.
3+ years of experience in implementing advanced concepts in Snowflake.
Hands-on experience with either Microsoft Azure or AWS.
Experience with data processing and workflow frameworks like Apache Beam, Spark, PySpark, Apache Kafka, and Apache Airflow.
Solid understanding of business intelligence enterprise and Cloud-native architectures.
Certifications such as Microsoft Certified: Azure Data Engineer Associate, Snowflake Certified Professional are a plus.
Responsibilities
As a Cloud Data Architect, you will:
Provide leadership, mentorship, and guidance for software development teams.
Collaborate extensively with our Product team to ensure a unified vision.
Drive engineering teams to adapt secure data and analytics best practices.
Own, manage, and execute the Data and Analytics technology roadmap.
Guide engineering teams on optimal data architectures.
Participate in the architecture discovery track to manage technical risks.
Evaluate and prototype new technologies for new and existing products.
Adhere to the agile software development life cycle best practices.
Demonstrate strong coding and data modeling skills.
Utilize expertise in data modeling to create logical and physical data models.
Design data structures optimized for business intelligence data.
Collaborate with data engineers to clean and preprocess data.
Design and manage ETL/ELT processes.
Define and enforce best practices for data and analytics storage.
Design security features to protect sensitive analytical data.
Stay updated on emerging trends in business intelligence and analytics.
Recommend and integrate new technologies that enhance data capabilities.
Leadership Qualifications
Results-oriented self-starter with exceptional interpersonal communication skills.
Strong remote team leadership and collaboration qualities.
Ownership mentality and entrepreneurial approach.
Experience in leading data architecture governance and software development practices.
Verifiable presentation skills.
Benefits
Flexible PTO
Summer Fridays
No meeting Fridays
Medical, Dental, Paid Sick Days, Vision, and Supplemental Coverage
Flexible Spending Account
Health Savings Account
401(k) match
Equal Employment Opportunity
We are an Equal Opportunity, Affirmative Action Employer.
Compensation
Pay range: $140,000 - $150,000. Actual base pay will depend on varying circumstances, including the position, location, individual qualifications, market finances, and other operational business needs. Depending on the position, compensation may also include commission, bonuses, etc. Potential for bonuses is based on company performance, and potential for merit increases is based on performance.
Employment Type: Full-Time
Show more
Show less","Cloud Data Architect, Data Warehouses, Data Lakes, Snowflake, Apache Beam, Spark, PySpark, Apache Kafka, Apache Airflow, Microsoft Certified: Azure Data Engineer Associate, Snowflake Certified Professional, ETL, ELT, DevOps, Agile, Data Engineering, Software Development, Business Intelligence, Enterprise Architecture","cloud data architect, data warehouses, data lakes, snowflake, apache beam, spark, pyspark, apache kafka, apache airflow, microsoft certified azure data engineer associate, snowflake certified professional, etl, elt, devops, agile, data engineering, software development, business intelligence, enterprise architecture","agile, apache airflow, apache beam, apache kafka, business intelligence, cloud data architect, data engineering, data lakes, data warehouses, devops, elt, enterprise architecture, etl, microsoft certified azure data engineer associate, snowflake, snowflake certified professional, software development, spark"
"Data Conversion Developer, Senior Associate",PwC,"San Diego, CA",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749933940,2023-12-17,Chula Vista,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Azure, PowerPlant, AWS Glue, Python, Maximo, Java, SQL, PySpark, Oracle, IBM DB2, JSON, DataBricks, Data Architect, Structured Query Language (SQL), Microsoft SQL Server, SSIS, Apache Spark, XML, Automation Scripts, RESTful, Programming Languages, Scala, SOAP APIs, Database Configuration, ETL, Data cleansing, Data migration","azure, powerplant, aws glue, python, maximo, java, sql, pyspark, oracle, ibm db2, json, databricks, data architect, structured query language sql, microsoft sql server, ssis, apache spark, xml, automation scripts, restful, programming languages, scala, soap apis, database configuration, etl, data cleansing, data migration","apache spark, automation scripts, aws glue, azure, data architect, data migration, database configuration, databricks, datacleaning, etl, ibm db2, java, json, maximo, microsoft sql server, oracle, powerplant, programming languages, python, restful, scala, soap apis, spark, sql, ssis, structured query language sql, xml"
Cloud Data Architect - Remote,Get It Recruit - Information Technology,"San Diego, CA",https://www.linkedin.com/jobs/view/cloud-data-architect-remote-at-get-it-recruit-information-technology-3774135262,2023-12-17,Chula Vista,United States,Mid senior,Remote,"We're at the forefront of technological innovation, bringing the latest advancements in technology and automation to over 10,000 legal and accounting firms. Our mission is to empower highly skilled professionals, enabling them to refocus their expertise on what truly matters. Harnessing powerful and secure practice management, document automation, and payment processing, CARET propels firms, professionals, and their clients further.
Our Team Thrives On a Set Of Defined Values That Guide Our Interactions With Each Other, Our Partners, And Our Customers
We succeed together.
We embrace progress.
We care big.
We create space.
The Role: Cloud Data Architect
Are you a dynamic and visionary Cloud Data Architect with a desire to make a difference and be a key player in shaping the future of CARET Cloud Data Analytics? If you're hands-on with managing cloud data technologies related to Data Analytics, Big Data, Data Warehouses, and Data Lakes, we want you to join our growing engineering team. Unleash your innovation, strategic thinking, and technical prowess to revolutionize how we store, manage, and analyze data in the cloud.
Responsibilities
As a Cloud Data Architect at our company, you will:
Provide leadership, mentorship, and guidance for software development teams onshore and offshore.
Collaborate extensively with our Product team to ensure a unified vision and continuous improvements to the platform and multiple products.
Drive engineering teams to adapt secure data and analytics best practices and assist with data security architecture and design.
Guide engineering teams on optimal data architectures, ensuring databases are optimized for data visualization tools.
Own, manage, and execute the Data and Analytics technology roadmap.
Assist in the creation of product roadmaps and sprint/release planning.
Evaluate new technologies, prototype new product concepts, and manage technical debt.
Demonstrate strong coding and data modeling skills, contributing to the team's efforts while following established software engineering practices.
Requirements
To be successful in this role, you should have:
A Bachelor’s degree in computer science, Information Systems, or a related discipline.
7+ years of experience in Data Engineering with expertise as a data architect or solution architect.
Highly skilled with hands-on experience and domain expertise in both data and analytics technologies, as well as cloud-native data architecture design and best practices.
3+ years of experience in implementing Snowflake advanced concepts.
Hands-on experience with either Microsoft Azure or AWS.
In-depth knowledge and solid experience with Microsoft Fabric components.
Experience with data processing and workflow frameworks like Apache Beam, Spark, PySpark, Apache Kafka, and Apache Airflow.
Certifications such as Microsoft Certified: Azure Data Engineer Associate, Snowflake Certified Professional, and SnowPro Certification are a plus.
Leadership Qualifications
We are looking for a results-oriented self-starter with exceptional interpersonal communication skills, remote team leadership experience, and an ownership mentality. You should be well-organized, have strong delegation skills, and the ability to build and maintain relationships with senior management, stakeholders, and team members.
Benefits
Flexible PTO
Summer Fridays
No meeting Fridays
Medical, Dental, Paid Sick Days, Vision, and Supplemental Coverage
Flexible Spending Account
Health Savings Account
401(k) match
Work Environment
This position offers remote work options, providing the flexibility needed to maintain a healthy work-life balance.
If you thrive in a fast-paced, innovative environment and have a passion for cutting-edge technologies, we invite you to apply for this exciting opportunity to shape the future of CARET Cloud Data Analytics.
Employment Type: Full-Time
Show more
Show less","Cloud Data Architect, Data Analytics, Big Data, Data Warehouses, Data Lakes, Software Development, Product Management, Data Security, Data Visualization, Data Modeling, Computer Science, Information Systems, Data Engineering, Data Architect, Solution Architect, Snowflake, Microsoft Azure, AWS, Microsoft Fabric, Apache Beam, Spark, PySpark, Apache Kafka, Apache Airflow, Microsoft Certified: Azure Data Engineer Associate, Snowflake Certified Professional, SnowPro Certification","cloud data architect, data analytics, big data, data warehouses, data lakes, software development, product management, data security, data visualization, data modeling, computer science, information systems, data engineering, data architect, solution architect, snowflake, microsoft azure, aws, microsoft fabric, apache beam, spark, pyspark, apache kafka, apache airflow, microsoft certified azure data engineer associate, snowflake certified professional, snowpro certification","apache airflow, apache beam, apache kafka, aws, big data, cloud data architect, computer science, data architect, data engineering, data lakes, data security, data warehouses, dataanalytics, datamodeling, information systems, microsoft azure, microsoft certified azure data engineer associate, microsoft fabric, product management, snowflake, snowflake certified professional, snowpro certification, software development, solution architect, spark, visualization"
Senior Cloud Data Engineer,BDO USA,"San Diego, CA",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765472151,2023-12-17,Chula Vista,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, Cloudbased Data Analytics, SQL, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Azure, AWS, C#, Python, Java, Scala, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, Computer Vision, DataOps, Purview, Delta, Pandas, Spark SQL, SSIS, SSAS, SSRS, PySpark, Bicep, Terraform","data analytics, business intelligence, artificial intelligence, application development, cloudbased data analytics, sql, data warehousing, data modeling, semantic model definition, star schema construction, azure, aws, c, python, java, scala, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, computer vision, dataops, purview, delta, pandas, spark sql, ssis, ssas, ssrs, pyspark, bicep, terraform","ai algorithms, application development, artificial intelligence, automation tools, aws, azure, azure analysis services, batch data ingestion, bicep, business intelligence, c, cloudbased data analytics, computer vision, data lake medallion architecture, dataanalytics, datamodeling, dataops, datawarehouse, delta, devops, git, java, linux, machine learning, microsoft fabric, pandas, powerbi, purview, python, scala, semantic model definition, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, streaming data ingestion, terraform"
Senior Software Engineer (Data Platform),Apixio,"San Diego, CA",https://www.linkedin.com/jobs/view/senior-software-engineer-data-platform-at-apixio-3658967937,2023-12-17,Chula Vista,United States,Mid senior,Hybrid,"Who We Are
At the intersection of health plans and providers, Apixio and ClaimLogiq are creating a leading Connected Care platform to minimize reimbursement inaccuracies and high-quality patient care so they can thrive as the industry moves toward value-based reimbursement models.
The combination brings together healthcare expertise, AI/machine learning technology and data-driven analytics solutions to deliver innovative solutions and value to our customers and the healthcare ecosystem. We aim to accelerate the shift toward alternative payment models, while enhancing efficiency and supporting better patient outcomes.
The Opportunity At Apixio
Apixio is a healthcare analytics company that leverages artificial intelligence and big data to improve healthcare outcomes. We are seeking a talented Senior Data Platform Engineer to join our team and help build and maintain our data platform. As a Senior Data Platform Engineer at Apixio, you will work on a fast-paced team of talented individuals who are dedicated to improving healthcare. You will have the opportunity to lead and mentor other data platform engineers and work closely with data analysts and data scientists to enable both analytics and AI models.
Who You Are
You are an experienced data platform engineer with multiple years of ETL experience and coding expertise in Scala, Java, and Python. You have experience working with one or more of Spark, Airflow, Kafka, MySQL, Cassandra, Delta Lake, and Data Bricks. You are familiar with distributed systems, messaging queues, NoSQL and SQL databases, API design, microservice architecture, and streaming architecture. You are a problem solver who enjoys collaborating with others to build innovative solutions. You have excellent communication and collaboration skills.
What You Will Own
As a Senior Data Platform Engineer at Apixio, you will own the maintenance, design, and implementation of our data platform. This will include data pipelines, ETL workflows, OCR, developing and optimizing our internal data lakehouse, and ensuring data security and compliance with regulatory requirements. You will work closely with the application teams, data analysts, and data scientists to provide data access and enable analytics. You will automate processes and develop tools to improve the efficiency of the data platform. You will lead and mentor other data platform engineers, stay current with emerging trends and technologies in data engineering, and work to incorporate them into the Apixio data platform.
In addition to the above, you will also be responsible for:
Maintaining, designing, and implementing scalable, reliable, and high-performance data architectures
Developing and implementing best practices for data integration, data quality, and data governance
Ensuring the scalability, reliability, and security of the data platform
Collaborating with other teams at Apixio to understand their data needs and develop solutions to meet those needs
Managing and optimizing cloud-based infrastructure for the data platform
Keeping up-to-date with emerging technologies and trends in data engineering and healthcare tech
What You Bring To The Table
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field
5+ years of experience in data platform engineering with coding expertise in Scala, Java, and Python
Experience with technologies such as Spark, Airflow, Kafka, MySQL, Cassandra, Delta Lake, and Data Bricks
Experience with distributed systems, messaging queues, NoSQL and SQL databases, API design, microservice architecture, and streaming architecture
Experience leading and mentoring other engineers
Strong problem-solving and analytical skills
Excellent communication and collaboration skills
Nice to have: healthcare experience and familiarity with healthcare tech standards like x12 EDI, HL7 FHIR, CCDA, and V2 messaging
The salary range below is for Base Salary. Total compensation also includes benefits and variable compensation. Compensation will be determined based on several factors including, but not limited to, skill set, years of experience, and the employee’s geographic location.
Base Compensation
$96,000—$185,000 USD
We recognize that people come with experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence. Therefore, we encourage people from all backgrounds to apply to our positions. Your skills and background may be more translatable to this role than you initially thought. Allow us the opportunity to get to know you. Please let us know if you require accommodations during the interview process.
What Apixio Can Offer You
Meaningful work to advance healthcare
Competitive compensation
Exceptional benefits, including medical, dental and vision, FSA
401k with company matching up to 4%
Generous vacation policy
Remote-first & hybrid work philosophies
A hybrid work schedule (2 days in office & 3 days work from home) (Note: If the position is designated as REMOTE it will stay REMOTE)
Modern open office in beautiful San Mateo, CA; Los Angeles, CA; San Diego, CA; Austin, TX and Dallas, TX
Subsidized gym membership
Catered, free lunches
Parties, picnics, and wine-downs
Free parking
We take your privacy very seriously. Please review our privacy policy to see exactly how we protect your information here
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.
LI-RB1
Show more
Show less","Scala, Java, Python, Spark, Airflow, Kafka, MySQL, Cassandra, Delta Lake, Data Bricks, Distributed Systems, Messaging Queues, NoSQL, SQL Databases, API Design, Microservice Architecture, Streaming Architecture, Data Pipelines, ETL Workflows, OCR, Data Lakehouse, Data Security, Data Compliance, Regulatory Requirements, Data Analytics, AI Models, Cloud Computing, Data Integration, Data Quality, Data Governance, Healthcare Experience, Healthcare Tech Standards, x12 EDI, HL7 FHIR, CCDA, V2 Messaging","scala, java, python, spark, airflow, kafka, mysql, cassandra, delta lake, data bricks, distributed systems, messaging queues, nosql, sql databases, api design, microservice architecture, streaming architecture, data pipelines, etl workflows, ocr, data lakehouse, data security, data compliance, regulatory requirements, data analytics, ai models, cloud computing, data integration, data quality, data governance, healthcare experience, healthcare tech standards, x12 edi, hl7 fhir, ccda, v2 messaging","ai models, airflow, api design, cassandra, ccda, cloud computing, data bricks, data compliance, data governance, data integration, data lakehouse, data quality, data security, dataanalytics, datapipeline, delta lake, distributed systems, etl workflows, healthcare experience, healthcare tech standards, hl7 fhir, java, kafka, messaging queues, microservice architecture, mysql, nosql, ocr, python, regulatory requirements, scala, spark, sql databases, streaming architecture, v2 messaging, x12 edi"
Data Analyst - Full Time Temporary,ActionLink,"San Diego, CA",https://www.linkedin.com/jobs/view/data-analyst-full-time-temporary-at-actionlink-3781370580,2023-12-17,Chula Vista,United States,Mid senior,Hybrid,"Are you seeking an interim growth opportunity to elevate your business intelligence resume? Come apply your analytical talents at Sony , where today's innovation is transforming tomorrow's entertainment! ActionLink is seeking a Temporary Full-Time Data Analyst to support our Global 500 client, Sony, and their Service Group Operations. The Data Analyst is responsible for analyzing the Service Group business data, value, competitive intelligence, and financial statistics to determine status versus metrics and suggest methods of improvement. This individual must be able to interpret and communicate results using a variety of techniques, ranging from simple data aggregation and statistical analysis to complex data mining. The Data Analyst will work closely with other service groups and will report to ActionLink's Sony Director of Account & Client Services. This person should be able to analyze reports and guide business stakeholders through the data by making insightful suggestions and helping with different analytical requests. The ideal candidate will have some applied business and project-based experience implementing concepts and practices within Power BI, Excel, and SAP as well as strong interpersonal skills to communicate results and business recommendations. Come be part of a team that continues to redefine audiovisual technology and push the boundaries of what's possible!
Schedule & Location This is a full-time temporary assignment running for 6 months with the possibility of being extended 40 hours per week - Monday through Friday 8:00am - 5:00pm This position works on-site at Sony's Corporate Office at 16535 Via Esprillo in San Diego (candidate must reside in the San Diego metro area to be considered) What We Offer The wage range for this position is $30.00 to $33.00 per hour commensurate with experience Powerful resume and professional connection building experience with an international leader at the forefront of tech innovation Opportunities to enhance your data skills in a highly visible role within a global-facing and diverse corporate work environment Benefits eligibility - medical insurance, paid time off, paid holidays W2 employment with biweekly payroll schedule Duties Responsible for analysis of Key Performance Indicators for the Service Group that include: Call Center statistics, CSAT (Customer Satisfaction), NPS (Net Promoter Score), and TAT (Turnaround time) Create survey templates using Qualtrics and NICE software, as well as compile responses into Power BI for daily, weekly, and monthly KPI metric reporting Assists Service Group business operation with making necessary recommendations by providing insightful reports and commentary Collaborate with other Service groups such as Business Planning, Operations, and Finance Track employee performance and reports for numerous projects Prepare charts and presentations for senior management Improve and automate reporting processes Qualifications: Must reside in the San Diego, CA metro area Must be authorized to work for any employer in the U.S. ActionLink is unable to sponsor or take over sponsorship of an employment visa at this time. 2+ years of data analysis and reporting experience in a corporate environment Strong proficiency with Power BI, SAP, and Excel Experience with data visualization techniques and communicating information to stakeholders Excellent interpersonal skills, organizational skills, and attention to detail Positive attitude and the ability to work as a team with co-workers, customers, vendors, and other collaborators Ability to multi-task and work productively in a team environment and meet deadlines Must be able to consistently maintain a high level of confidentiality Smartphone with internet access/data plan Equal Opportunity Employer ActionLink, in good faith, believes that this posted range of compensation is the accurate range for this role at the time of this posting. ActionLink may ultimately pay more or less than the posted range depending on candidate qualifications and locations in CA. This range may be modified in the future.
Show more
Show less","Data Analysis, Business Intelligence, Data Mining, Power BI, Excel, SAP, Qualtrics, NICE, KPI Reporting, Data Visualization, Project Management, Report Automation, Statistical Analysis, Business Planning, Operations, Finance, Employee Performance Tracking, Communication, Teamwork, Time Management, Confidentiality","data analysis, business intelligence, data mining, power bi, excel, sap, qualtrics, nice, kpi reporting, data visualization, project management, report automation, statistical analysis, business planning, operations, finance, employee performance tracking, communication, teamwork, time management, confidentiality","business intelligence, business planning, communication, confidentiality, data mining, dataanalytics, employee performance tracking, excel, finance, kpi reporting, nice, operations, powerbi, project management, qualtrics, report automation, sap, statistical analysis, teamwork, time management, visualization"
Senior Database Engineer - Hybrid,"BAE Systems, Inc.","San Diego, CA",https://www.linkedin.com/jobs/view/senior-database-engineer-hybrid-at-bae-systems-inc-3734116364,2023-12-17,Chula Vista,United States,Mid senior,Hybrid,"Job Description
Come see what you’re missing. Our employees work on the world’s most advanced systems – Command, Control, Communications, Computers, Intelligence, Surveillance, and Reconnaissance (C4ISR) Systems. You'll help develop systems that sense, control, exploit and disseminate actionable information to warfighters supporting a variety of missions. Spanning air, land, sea, and space, we are developing the technology of tomorrow, delivered today. Drawing strength from our differences, we’re innovating for the future. And you can, too. Our flexible work environment provides you a chance to change the world without giving up your personal life. We put our customers first – exemplified by our mission: “We Protect Those Who Protect Us®.” Sound like a team you want to be a part of? Come build your career with BAE Systems.
BAE Systems offers competitive pay, benefits, and important work-life balance initiatives including every other Friday Off, Flextime, and Telecommuting. BAE also believes in a culture of recognition for the extraordinary contributions of our skilled employees. Our engineers are the lifeblood of our company and we’re more than 5,000 strong. With our robust offering of educational and career development opportunities, your chances to grow are limitless.
BAE Systems is looking for a Database Engineer with 6+ years of database development and administration experience to join an engineering team in defining and developing significant upgrades to an enterprise-wide Imagery Archive and Geospatial Intelligence System. The candidate will work as part of a multi-disciplinary engineering team to design, code, test, and deploy new database configurations in a rapid development environment, and will support a large, complex systems capable of ingesting large amounts of data. This position may require business travel in support of customer meetings and operational system deployment and support activities.
Because this role involves a combination of collaborative/in-person and independent work, it will take the form of a hybrid work format, with time split between working onsite and remotely.
#SWSD
Required Education, Experience, & Skills
US Citizenship with TS/SCI clearance, and ability to obtain additional clearances
Bachelor’s degree in engineering/technical discipline, and 6+ years database development and administration experience across the entire development lifecycle
Strong experience as an Oracle developer with Oracle RDBMS versions 12c and 19c
Strong experience in building Oracle PL/SQL functions, procedures, packages and types
Strong experience with SQL query syntax
Strong experience with Oracle basic server operations including pluggable database
Strong experience on RHEL Linux versions 7/8 (including bash and csh shell programming)
Strong experience with database design and development processes
Strong experience with Oracle STIG and hardening related procedures
Experience with AWS RDS
Experience with Jira and Git
Experience with DAO related programming in C++ or JAVA
Familiarity with Oracle Instant Client installation
Team player with a proactive attitude and the ability to be productive in a highly secure, and dynamic/collaborative environment
Strong oral and written communications skills
Preferred Education, Experience, & Skills
Bachelor of Science degree in Computer Science or related field
Experience with Oracle Spatial option
Experience with Oracle RAC basic operations
Experience with Oracle Label Security (OLS) option
Experience with Oracle Identity Management (OIM) and Enterprise User Security (EUS)
Experience with Oracle Dataguard
Pay Information
Full-Time Salary Range: $122870 - $208890
Please note: This range is based on our market pay structures. However, individual salaries are determined by a variety of factors including, but not limited to: business considerations, local market conditions, and internal equity, as well as candidate qualifications, such as skills, education, and experience.
Employee Benefits: At BAE Systems, we support our employees in all aspects of their life, including their health and financial well-being. Regular employees scheduled to work 20+ hours per week are offered: health, dental, and vision insurance; health savings accounts; a 401(k) savings plan; disability coverage; and life and accident insurance. We also have an employee assistance program, a legal plan, and other perks including discounts on things like home, auto, and pet insurance. Our leave programs include paid time off, paid holidays, as well as other types of leave, including paid parental, military, bereavement, and any applicable federal and state sick leave. Employees may participate in the company recognition program to receive monetary or non-monetary recognition awards. Other incentives may be available based on position level and/or job specifics.
About BAE Systems Electronic Systems
BAE Systems, Inc. is the U.S. subsidiary of BAE Systems plc, an international defense, aerospace and security company which delivers a full range of products and services for air, land and naval forces, as well as advanced electronics, security, information technology solutions and customer support services. Improving the future and protecting lives is an ambitious mission, but it’s what we do at BAE Systems. Working here means using your passion and ingenuity where it counts – defending national security with breakthrough technology, superior products, and intelligence solutions. As you develop the latest technology and defend national security, you will continually hone your skills on a team—making a big impact on a global scale. At BAE Systems, you’ll find a rewarding career that truly makes a difference. Electronic Systems (ES) is the global innovator behind BAE Systems’ game-changing defense and commercial electronics. Exploiting every electron, we push the limits of what is possible, giving our customers the edge and our employees opportunities to change the world. Our products and capabilities can be found everywhere – from the depths of the ocean to the far reaches of space. At our core are more than 14,000 highly talented Electronic Systems employees with the brightest minds in the industry, we make an impact – for our customers and the communities we serve.
Our Commitment To Diversity, Equity, And Inclusion
At BAE Systems, we work hard every day to nurture an inclusive culture where employees are valued and feel like they belong. We are conscious of the need for all employees to see themselves reflected at every level of the company and know that in order to unlock the full potential of our workforce, everyone must feel confident being their best, most sincere self and be equipped to thrive. We provide impactful professional development experiences to our employees and invest in social impact partnerships to uplift communities and drive purposeful change. Here you will find significant opportunities to do meaningful work in an environment intentionally designed to be one where you will learn, grow and belong.
Show more
Show less","Oracle, SQL, Oracle PL/SQL, RHEL Linux, Oracle STIG, AWS RDS, Jira, Git, DAO, Oracle Spatial option, Oracle RAC, Oracle Label Security (OLS), Oracle Identity Management (OIM), Enterprise User Security (EUS), Oracle Dataguard","oracle, sql, oracle plsql, rhel linux, oracle stig, aws rds, jira, git, dao, oracle spatial option, oracle rac, oracle label security ols, oracle identity management oim, enterprise user security eus, oracle dataguard","aws rds, dao, enterprise user security eus, git, jira, oracle, oracle dataguard, oracle identity management oim, oracle label security ols, oracle plsql, oracle rac, oracle spatial option, oracle stig, rhel linux, sql"
Quantitative Data Engineer,Jobs for Humanity,"New York, NY",https://www.linkedin.com/jobs/view/quantitative-data-engineer-at-jobs-for-humanity-3786349944,2023-12-17,New York,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
Quantitative Data Engineer
We are looking for a driven and innovative Data Engineer to join our team. In this role, you will have the opportunity to work on exciting technology projects in a fast-paced and collaborative environment. We value both technical skills and emotional intelligence, and we believe that effective problem-solving and communication are essential for success. If you have a strong background in Data Engineering, workflow orchestration, CI/CD pipelines, and experience with cloud platforms and system architectures, we would love to hear from you!
The Role
As a Quantitative Data Engineer, you will be part of our Quantitative Investment & Developer Operations team. This team provides portfolio risk solutions to our managers and is made up of highly skilled professionals from the financial industry. Our team culture emphasizes collaboration, cross-functionality, and achieving high-performance results while maintaining a healthy work/life balance.
Your Impact
In this role, you will have the opportunity to:
Design, build, and maintain complex ETL jobs that deliver business value.
Translate high-level business requirements into technical specifications.
Ingest data from different sources into various data stores.
Cleanse and enrich data, ensuring data quality controls.
Guide the future development of our data platform.
Create reusable tools to streamline project delivery.
Collaborate closely with other developers and provide mentorship.
Evaluate and recommend tools, technologies, processes, and architectures.
Deploy reusable workflow and orchestration.
Work in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements.
Minimum Qualifications
Bachelor's degree in Computer Science, Finance, Business, or a related field.
2+ years of experience in IT and/or finance industry.
Understanding of ETL methodologies and proficiency in Python tools.
Hands-on experience with Python and advanced data processing using Python libraries and AWS services.
Experience working in a cloud environment, such as AWS.
Experience in relational databases (SQL Server/PostgreSQL) and NoSQL databases (Mongo).
Experience with GIT, code review/deployment, and operations scheduling.
Experience in Agile/SCRUM and developing Story/Acceptance criteria.
Ideal Qualifications
Master's degree in Computer Science, Engineering, or a related field.
4+ years of experience in IT and/or quantitative, investment, or finance industry.
Experience with troubleshooting, root cause analysis, and issue remediation.
Good knowledge of orchestration and scheduling tools.
Experience with data reporting through tools like MicroStrategy, Tableau, Looker, or Python libraries.
Entrepreneurial mindset and ability to work in a rapid and iterative development environment.
Experience delivering mobile apps and app store ecosystems.
Strong organizational, analytical, and problem-solving skills.
Excellent communication skills and ability to present complex information clearly.
Data-driven mindset and ability to use quantitative tools for solution development.
Adaptability to changing business priorities and strong work ethic.
Curiosity about emerging digital and technology trends.
Proven ability to collaborate cross-functionally and influence outcomes.
What to Expect as Part of MassMutual and the Team
Regular meetings with the Quantitative and ETX project teams.
One-on-one meetings with your manager focused on your growth and development.
Mentorship opportunities to enhance your skills.
Networking opportunities with various Business Resource Groups focused on diversity and inclusion.
Access to learning content on platforms like Degreed.
Industry-leading pay and benefits with a company that values ethics and integrity.
To apply, please fill out the form below. We welcome applications from all individuals, and if you require any accommodation during the application process, please let us know the specifics of the assistance you need. Veterans are also encouraged to apply, regardless of their discharge status.
MassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran.
Show more
Show less","Data Engineering, ETL, Python, AWS, SQL Server, PostgreSQL, Mongo, GIT, Agile/SCRUM, MicroStrategy, Tableau, Looker","data engineering, etl, python, aws, sql server, postgresql, mongo, git, agilescrum, microstrategy, tableau, looker","agilescrum, aws, data engineering, etl, git, looker, microstrategy, mongo, postgresql, python, sql server, tableau"
"Software Engineer [Senior, Data Intelligence]",NauWork,Utica-Rome Area,https://www.linkedin.com/jobs/view/software-engineer-senior-data-intelligence-at-nauwork-3776573811,2023-12-17,New York,United States,Mid senior,Remote,"A NauWork client is seeking a
Software Engineer [Senior, Data Intelligence]
to join their team. The position is
fully remote
.
This B2B2C fintech company is unlocking the full value of compensation by rebuilding benefits as a simple payment experience — fast, flexible, and transparent. Their mission is to empower people to build better financial futures, and they are accomplishing that by transforming the status quo of benefits. With a diverse team from industry leading companies, this company is creating a new payments tech stack to help employers offer more accessible and personalized benefits for their teams.
Responsibilities:
Full-stack software engineering (leaning mostly back-end).
Design and build scalable real-time systems to solve problems that increase access to benefits.
Produce detailed component designs, and core interface and API designs. Implement and optimize designed solutions, and enhance them with new features.
Ingest external static data to support the card processing and operational requirements.
Build, monitor and manage pipelines to ensure data quality.
Work cross-functionally with product, operations and data teams to realize and launch company products.
Required Experience:
5+ years of software engineering experience handling big data.
Experience with data pipelines, data management, API development and textual search scenarios.
Exposure to machine learning and NLP techniques.
Experience with End-to-end ownership in developing new features that have been launched to completion.
Experience working cross-functionally on high performing teams and handling sensitive data
Passion to build highly available and scalable processing systems and architectures.
To Learn More:
503-388-9585
833-NAU-WORK
careers.nauwork.com
Category: Technology - Software, Mobile, Web & Game
Position: Software Engineer [Senior, Data Intelligence]
Location: Fully Remote
Job Type: Direct-Hire, Full-Time
Show more
Show less","Software engineering, Data pipelines, Data management, API development, Machine learning, Natural language processing, Fullstack development, Realtime systems, Component design, Interface design, API design, Data quality, Crossfunctional team collaboration, Endtoend ownership, High performing teams, Sensitive data handling, Highly available systems, Scalable architectures","software engineering, data pipelines, data management, api development, machine learning, natural language processing, fullstack development, realtime systems, component design, interface design, api design, data quality, crossfunctional team collaboration, endtoend ownership, high performing teams, sensitive data handling, highly available systems, scalable architectures","api design, api development, component design, crossfunctional team collaboration, data management, data quality, datapipeline, endtoend ownership, fullstack development, high performing teams, highly available systems, interface design, machine learning, natural language processing, realtime systems, scalable architectures, sensitive data handling, software engineering"
Sr. Staff DevOps Engineer – Streaming Data Systems,Medidata Solutions,"New York, United States",https://www.linkedin.com/jobs/view/sr-staff-devops-engineer-%E2%80%93-streaming-data-systems-at-medidata-solutions-3663813458,2023-12-17,New York,United States,Mid senior,Remote,"Medidata: Powering Smarter Treatments and Healthier People
Medidata, a Dassault Systèmes company, is leading the digital transformation of life sciences, creating hope for millions of people. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 2,000+ customers and partners access the world's most trusted platform for clinical development, commercial, and real-world data. Known for its ground-breaking technological innovations, Medidata has supported more than 30,000 clinical trials and 9 million study participants. And Medidata’s ongoing commitment to infusing the patient voice into trial designs and solutions is helping to create a better and more inclusive experience for all participants in clinical studies. Medidata is involved in nearly 40% of company-initiated trial starts globally, with studies conducted in more than 140 countries. More than 70% of novel drugs approved by the Food and Drug Administration (FDA) in 2022 were developed with Medidata software. Medidata is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www.medidata.com and follow us @medidata.
Our Platform:
Medidata’s data platform is the backbone of all our products and services. It powers a significant percentage of clinical development across the globe and plays a key role in finding new treatments for disease, contributing to the advancement of science and a healthier world.
Our Team:
Our team is responsible for Medidata’s self-service data mesh platform. Our customers depend on our ability to process and analyze large quantities of data in real time. We focus on solving complex real-world problems while shipping practical solutions. What we do is as much art as it is science, and as Steve Jobs put it, “Real artists ship.”
We’re building our platform on top of a few core principles and technologies:
A single unified view of data.
We bridge the gap between business and internal product teams by speaking a ubiquitous domain language. Exposing our domain model as a GraphQL API, our teams can consistently access data across the platform, ignoring the underlying details of how this data is stored.
Event Streams as a Source of Truth.
We’re embracing event-based streaming architecture. The stream acts as a single source of truth for enterprise events and gives us a consistent way to perform real-time calculations and materialize data to various stores for analysis.
Add meaning to all data.
Data is infinitely more valuable when it’s meaningful. Context matters. Relationships matter. Structure matters. This is why semantic web technologies and schema definition languages help us build and represent knowledge in a more meaningful way.
We’re looking for a Sr. Staff DevOps Engineer with 8+ years of experience who shares our passion for solving real-world engineering challenges and thrives on iterating and shipping frequently. You should have experience designing, developing, and maintaining robust event streaming systems and writing concise, clean, maintainable code to deliver infrastructure solutions.
Here is a list of tools and approaches we use. We don’t ask you to be an expert in all of them, but we expect you to be productive in this environment.
We build our event streaming platform on top of Kafka and Flink, but if you’ve worked in high throughput environments with Spark, Storm, or other streaming systems, you’ll fit right in.
We deploy our infrastructure on top of Kubernetes (AWS EKS) and automate everything.
You should have extensive experience in container-based environments and deep knowledge of Terraform, Ansible, or similar tools.
We love great developer experiences. Not only do we focus on clean, automated, and robust production infrastructure, but we also ensure that engineers can have a great local development environment that’s as close to the real infrastructure as possible. Containers make this dream a reality and we hope you have just as much passion for simplicity, robustness, and user experience.
We embrace ShapeUp concepts when we work, so our engineers don’t just operate a conveyor belt of stories and tickets. Instead, we rely on engineers to help us solve complex problems, consider various constraints, and think through the tradeoffs.
Criteria we use to consider hiring a
Sr. Staff DevOps Engineer
at Medidata
Capable of working completely autonomously
Helps set and maintain professional standards for the team
Deep, substantial expertise in multiple deployment environments
Capable of running and directing small teams for substantial projects
Capable of executing projects across multiple domains and environments
Invents new concepts and pushes the whole team forward regularly
** When applying, please include a cover letter describing why you think you’ll be a great candidate for this role and anything relevant that would help you and your experience stand out. Please also, include links to your prior work (i.e. Github/Gitlab projects, blog posts, white papers, or anything else you can share). Thanks!**
As with all roles, Medidata sets ranges based on a number of factors including function, level, candidate expertise and experience, and geographic location.
The salary range for positions that will be physically based in the NYC Metro Area is $158,000-186,000.
The salary range for positions that will be physically based in the California Bay Area is $167,000-222,000.
The salary range for positions that will be physically based in the Boston Metro Area is $155,000-207,000.
The salary range for positions that will be physically based in Texas or Ohio is $139,000-186,000.
The salary range for positions that will be physically based in all other locations within the United States is $141,000-188,000.
Base pay is one part of the Total Rewards that Medidata provides to compensate and recognize employees for their work. Most sales positions are eligible for a commission on the terms of applicable plan documents, and many of Medidata’s non-sales positions are eligible for annual bonuses. Medidata believes that benefits should connect you to the support you need when it matters most and provides best-in-class benefits, including medical, dental, life and disability insurance; 401(k) matching; unlimited paid time off (subject to management discretion); and 10 paid holidays per year.
Equal Employment Opportunity
In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Medidata are based on merit, qualifications and abilities. Medidata is committed to a policy of non-discrimination and equal opportunity for all employees and qualified applicants without regard to race, color, religion, gender, sex (including pregnancy, childbirth or medical or common conditions related to pregnancy or childbirth), sexual orientation, gender identity, gender expression, marital status, familial status, national origin, ancestry, age, disability, veteran status, military service, application for military service, genetic information, receipt of free medical care, or any other characteristic protected under applicable law. Medidata will make reasonable accommodations for qualified individuals with known disabilities, in accordance with applicable law.
Show more
Show less","Kubernetes, Ansible, Terraform, Kafka, Flink, ShapeUp, AWS EKS, Spark, Storm, Gitlab, GitHub, GraphQL API, Kubernetes, Domain languages, Semantic web technologies, Schema definition languages","kubernetes, ansible, terraform, kafka, flink, shapeup, aws eks, spark, storm, gitlab, github, graphql api, kubernetes, domain languages, semantic web technologies, schema definition languages","ansible, aws eks, domain languages, flink, github, gitlab, graphql api, kafka, kubernetes, schema definition languages, semantic web technologies, shapeup, spark, storm, terraform"
Senior Data Engineer,Big Cloud,"New York, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-big-cloud-3735927247,2023-12-17,New York,United States,Mid senior,Remote,"Do you have experience with the Azure Cloud stack and Azure Synapse?
Are you interested in working with stakeholders to solve complex data pipeline and cloud problems?
Are you open and able to work on a full-time/perm/W2 contract basis?
A leading tech consulting company is seeing huge market success working with East Coast finance companies, Bay Area AI start-ups, European marketing agencies and much more. They have a specialised team, working on projects ranging from data to AI and cloud cybersecurity.
They pride themselves in business customer satisfaction and using the latest technology to provide end-to-end solutions. In the next few months, they team are hiring for Senior Data Engineers to work with a finance client account they're scaling.
You'd be working end-to-end on data pipelines built on the Azure platform, working closely with their mission-driven tech team and external stakeholders.
What you need:
· 3+ years of data engineering experience
· Strength with
Azure Stack
,
Synapse
· understanding of the DevOps process and cloud migration
· added bonus: experience with Python, PySpark, Databricks
Believe this could be a fit? Apply below!
Big Cloud is a machine learning recruiting firm. We’re lucky enough to recruit the best candidates in the most exciting companies all over the world. We try to reply to all applications, but we’re only human, for now! So, you may only hear from us if you are successful.
Check out www.bigcloud.io/jobs to see what else we’re recruiting for.
Show more
Show less","Azure Cloud, Azure Synapse, Data Pipelines, DevOps, Cloud Migration, Python, PySpark, Databricks, Machine Learning","azure cloud, azure synapse, data pipelines, devops, cloud migration, python, pyspark, databricks, machine learning","azure cloud, azure synapse, cloud migration, databricks, datapipeline, devops, machine learning, python, spark"
Data Engineer (US),BASE life science,"New York, United States",https://www.linkedin.com/jobs/view/data-engineer-us-at-base-life-science-3734686973,2023-12-17,New York,United States,Mid senior,Remote,"Do you want to join a fast-growing global consultancy in the life science industry?
We are a fast-growing consultancy in the life science industry and are looking for innovative, enthusiastic, and versatile data engineers to join us. This is an exciting opportunity to work on cutting-edge projects and gain experience in a dynamic and fast-paced environment.
We are a fast-growing consultancy in the life science industry, seeking to build our BASE US Consultancy Organization to successfully spearhead our first US engagements.
BASE life science is a business and technology consulting company working exclusively within the life science industry. We specialize in working on the customer side of technology projects and IT systems.
From our presence in Denmark, Switzerland, Germany, Spain, UK, France and Italy we serve clients across Europe. Over the last three years, we have experienced substantial growth and we believe that the combination of a problem-solving mindset and a tailored approach to each client will deliver more growth in the years to come.
We are excited to begin our expansion in the U.S.
We are aiming to establish a team of 4-5 consultants initially and aim to expand over time. The team will be focused on the US market, but there is the opportunity to contribute on European projects as well.
At BASE we partner with industry leading solution providers, including Veeva, Salesforce and IQVIA and help to carve business value from digital platforms and data.
You will be responsible for:
Building and maintaining data pipelines and data architecture for our clients.
Collaborating with cross-functional teams to design and implement data-driven solutions.
Helping to improve data quality and data governance practices.
Participating in the development and testing of data-driven solutions.
What experiences do you bring?
Master’s degree in STEM (Science, Technology, Engineering, and Mathematics) or another quantitative field.
3+ Experience with Python and/or other programming languages (e.g., R, Java, C++, SQL, etc.).
Knowledge of databases, data warehousing, ETL, and/or data modelling concepts.
Knowledge of using a bash-terminal.
Strong problem-solving skills and ability to work independently.
Strong communication and teamworking skills.
Additionally, you might have experience with: collaborative software development using Git, UNIX and in working with GxP or other regulated data, preferably from the pharmaceutical industry. You have an advantage if you come with an understanding of pharmaceutical data in the R&D space (clinical, regulatory, quality, safety).
We further presume that you have experience working in a client-facing role.
Why join BASE life science?
BASE life science has been recognized as one of the 25 best workplaces in Denmark and what makes us special is the diversity, we value with more than 27 different nationalities in our team. We have a Scandinavian culture across all of BASE and offer our employees a flexible work-life opportunity, not just on paper but also in real life. In BASE we trust our employees and the boundaries within which you can operate are very wide, as long as the customers are prioritized.
What do we offer?
Flexible schedule: Fit it into your routine! ⏰
Remote friendly: 🌍
Home office setup: You will get a laptop + electronic devices 💻
Culture of trust, diversity & inclusion,✨
An entrepreneurial and multicultural work environment, where you will be able to quickly make an impact for our customers.⚡
Ongoing learning and development opportunities to grow your career. ⬆️
Are you interested?
If you are interested in this position do not hesitate to apply as soon as possible. We will screen and evaluate incoming candidates on an ongoing basis and look forward to receiving your application.
Show more
Show less","Python, R, Java, C++, SQL, Databases, Data warehousing, ETL, Data modelling, Git, UNIX, Bashterminal, Collaboration software, GxP, Regulated data, Pharmaceutical data, Clinical data, Regulatory data, Quality data, Safety data","python, r, java, c, sql, databases, data warehousing, etl, data modelling, git, unix, bashterminal, collaboration software, gxp, regulated data, pharmaceutical data, clinical data, regulatory data, quality data, safety data","bashterminal, c, clinical data, collaboration software, data modelling, databases, datawarehouse, etl, git, gxp, java, pharmaceutical data, python, quality data, r, regulated data, regulatory data, safety data, sql, unix"
"Senior Data Engineer - $180k-$220k (Snowflake, Coding)",CyberCoders,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-%24180k-%24220k-snowflake-coding-at-cybercoders-3766359896,2023-12-17,New York,United States,Mid senior,Remote,"Permanently Remote in US
Job Title:
Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding)
Salary:
$180k-$220k Base + Bonus, No Stock, 401k, Benefits
Requirements:
Expert w/ Snowflake & Coding Ability
Based in beautiful New York City, we are a cutting edge ad-tech org for television-based ads.
We are founded and owned by T.V.s largest publishers.
Our mission is to be bring simplicity & scale to audience based campaigns in television.
We're working with over 100 advertisers and anticipating another year of significant growth!
As a rapidly growing company
(founded in 2017 & up 140% year over year)
we've recently elevated our C-Suite Team in preparation for our next stage of growth and are building our Technology, Product, and Operations Executive Teams.
We have been in business for 7 years and have around 40 employees.
Due to growth, we are actively hiring a Senior Data Engineer with
Snowflake experience (ideally certified)
Ability to write production level code (ideally JavaScript or Python)
Experience building data pipelines from scratch and/or working with APIs
Experience with the following is a big plus!
Fivetran and/or DBT
You will be working with 12 other engineers on the data side + several other technical folks.
This role will consist of tasks such as building out data pipelines, data architecture via snowflake, and data modeling.
If you have this experience, please apply immediately. We are actively interviewing this week and next for this high profile position.
Top Reasons to Work with Us
Compensation: $180k-$220k Base + Bonus, No Stock, 401k, Benefits
Raid Growth: Founded in 2017 & up 140% year over year
Culture: Fast paced, mission driven culture
Technology: Cutting edge technology
What You Will Be Doing
Building data pipelines from scratch
Data architecture via Snowflake
Data modeling
Technical review of everything this group builds.
Mange development velocity, team capacity, and backlogs
Partner closely with the product team
Take on key assignments and delegate as needed
Act as the main technical point of contact for engineering
Translate technical requirements to the rest of the engineering team
What You Need for this Position
Must Have Experience
Snowflake experience
Ability to write production level code (ideally JavaScript or Python)
Experience building data pipelines from scratch and/or working with APIs
Some Experience With:
-
Fivetran and/or DBT
What's In It for You
$180k-$220k Base + Bonus, No Stock, 401k, Benefits
401k
Vacation/PTO
Medical
Dental
Vision
Bonus
401k
Benefits
Vacation/PTO
Medical
Dental
Vision
Bonus
So, if you are a Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding) with experience, please apply today!
Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Nitu Gulati-Pauly
Email Your Resume In Word To
Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:
Nitu.Gulati-Pauly@cybercoders.com
Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : MA5-1745335 -- in the email subject line for your application to be considered.***
Nitu Gulati-Pauly - VP of Recruiting - CyberCoders
Applicants must be authorized to work in the U.S.
CyberCoders is proud to be an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work
– In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.
Show more
Show less","Snowflake, JavaScript, Python, Data pipelines, APIs, Fivetran, DBT, Data architecture, Data modeling","snowflake, javascript, python, data pipelines, apis, fivetran, dbt, data architecture, data modeling","apis, data architecture, datamodeling, datapipeline, dbt, fivetran, javascript, python, snowflake"
Data Engineer,Durlston Partners,"New York, United States",https://www.linkedin.com/jobs/view/data-engineer-at-durlston-partners-3772420601,2023-12-17,New York,United States,Mid senior,Hybrid,"Data Engineer - Quantitative Trading - Global Hedge Fund
Our client is a multi-manager hedge fund deploying systematic and fundamental trading strategies. They're well established, with 500+ employees across numerous financial hubs globally.
They are seeking a talented Data Engineer to be part of an ambitious, newly formed quantitative team, sitting within a revenue-generating unit. The team is responsible for implementing strategies across several different disciplines. This role will involve multiple greenfield technology buildouts on cloud-based environments (including projects around machine learning, analytics, and infrastructure-as-code) and you'll sit next to the Quantitative Researchers, helping them define data outputs needed for research.
You will need:
Proficiency in Python and SQL
Strong communication skills, both written and verbal
Familiarity with cloud-based data warehouse technologies (Databricks, Snowflake, Redshift, etc.)
Familiarity with cluster compute (Spark, Dask, etc.) and schedulers (Airflow, Dagster, Prefect)
Experience collaborating with data analysts to define data models, publish metrics, and build dashboards
Analytical problem-solving skills with logical thought processes and quantitative aptitude
Demonstrated sense of project ownership and a history of delivering results
Qualifications
Computer Science or relevant STEM degree from a top university.
Buy-side financial services experience preferred, but not required.
Please get in touch to learn more.
Data Engineer - Quantitative Trading - Global Hedge Fund
Show more
Show less","Python, SQL, Databricks, Snowflake, Redshift, Spark, Dask, Airflow, Dagster, Prefect, Machine learning, Analytics, Infrastructureascode, Data modeling, Metrics, Dashboards","python, sql, databricks, snowflake, redshift, spark, dask, airflow, dagster, prefect, machine learning, analytics, infrastructureascode, data modeling, metrics, dashboards","airflow, analytics, dagster, dashboard, dask, databricks, datamodeling, infrastructureascode, machine learning, metrics, prefect, python, redshift, snowflake, spark, sql"
Python Data Engineer,Euclid Innovations,"New York, United States",https://www.linkedin.com/jobs/view/python-data-engineer-at-euclid-innovations-3712948251,2023-12-17,New York,United States,Mid senior,Hybrid,"Integration engineer responsible for daily support and project-based development of
credit risk
management systems.
This is an opportunity to gain experience in risk management processing using new technologies.
Skills:
5 years of full-time development experience using Python
Experience building data pipelines using Azure Data Factory and Databricks
Experience with Python application frameworks (Django, Flask, Pyramid, Tornado)
Experience with Python testing and code analysis tools (Pytest, Pylint)
Strong SQL skills
Familiarity with SSIS & SnowFlake
Strong troubleshooting skills
On-point communication skills
Education
Bachelors degree in Computer Science or Finance
Show more
Show less","Python, Azure Data Factory, Databricks, Django, Flask, Pyramid, Tornado, Pytest, Pylint, SQL, SSIS, SnowFlake, Troubleshooting","python, azure data factory, databricks, django, flask, pyramid, tornado, pytest, pylint, sql, ssis, snowflake, troubleshooting","azure data factory, databricks, django, flask, pylint, pyramid, pytest, python, snowflake, sql, ssis, tornado, troubleshooting"
"Data Engineer, Data Platform",Grammarly,"New York, United States",https://www.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3656899029,2023-12-17,New York,United States,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Python, Scala, Java, Relational databases, APIs, Microservices, AWS, Azure, GCE, Data lakes, Docker, Kubernetes, System design, Internal tools, Admin sites, Cloud computing, Big data, Data engineering, Machine learning, Artificial intelligence, Natural language processing","python, scala, java, relational databases, apis, microservices, aws, azure, gce, data lakes, docker, kubernetes, system design, internal tools, admin sites, cloud computing, big data, data engineering, machine learning, artificial intelligence, natural language processing","admin sites, apis, artificial intelligence, aws, azure, big data, cloud computing, data engineering, data lakes, docker, gce, internal tools, java, kubernetes, machine learning, microservices, natural language processing, python, relational databases, scala, system design"
Senior Data Engineer,Harnham,"New York, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-harnham-3765860785,2023-12-17,New York,United States,Mid senior,Hybrid,"Senior Data Engineer
Hybrid NYC
$120,000 -$160,000 + Equity
You will be working with a rapidly growing government contracting technical solutions firm in the US. They work on extremely impactful projects with multiple government agencies. They are looking to bring on a full stack software engineer who will hit the ground running and have major growth opportunities in their organization.
Challenges:
We are building systems that can automate the most complex knowledge work in the world, e.g., financial analysis, research, due diligence, and more.
Creating financial research that's worth paying attention to: aggregating, analyzing, and producing insights from real time information feeds.
Dealing with the most sensitive data in the world: client data from the largest financial services companies on earth.
Working past the edge of published AI research: tackling problems beyond the complexity of existing AI benchmarks.
Unsolved product, architectural, and business problems: natural language interfaces, prohibitively expensive evaluation of models, massive marginal costs, versioning/training/segregating models per task, client, and so on.
As a Data Engineer you would be the first dedicated Data Engineer, building out our data architecture to help guide over 50+ billion dollars of investment. You would work closely with our backend ML team to help feed better inputs into the model and improving the best information into our models.
Hard Requirements:
2+ years of industry experience as a data engineer.
Strong grasp of Python and SQL
Experience deploying Apache Airflow in production
Passionate about Machine Learning, is excited to share interesting research papers they come across.
Strong proficiency in Docker
Strong knowledge of AWS: OpenSearch, RDS, S3
Bonus Requirements:
Financial Services work experience
Startup experience
Knowledge of Datadog and other Telemetry tooling
Proficiency in typescript
Proficiency in cloudflare workers
Good sense of humor and interest in Sci-Fi
Interested in strategy games
Compensation Range: $120K - $160K
Show more
Show less","Data Engineering, Python, SQL, Apache Airflow, Machine Learning, Docker, AWS, OpenSearch, RDS, S3, Datadog, Telemetry tooling, Typescript, Cloudflare workers, SciFi, Strategy games","data engineering, python, sql, apache airflow, machine learning, docker, aws, opensearch, rds, s3, datadog, telemetry tooling, typescript, cloudflare workers, scifi, strategy games","apache airflow, aws, cloudflare workers, data engineering, datadog, docker, machine learning, opensearch, python, rds, s3, scifi, sql, strategy games, telemetry tooling, typescript"
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"New York, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-engineering-at-acv-auctions-3762874779,2023-12-17,New York,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Compensation:
$124,000.00 - $152,000.00 annually. Please note that final compensation will be determined based upon the applicant's relevant experience, skillset, location, business needs, market demands, and other factors as permitted by law.
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, Data Engineering, Data Modeling, Cloud Services, NoSQL, Unit and Integration Testing, DataLayer Development","sql, data engineering, data modeling, cloud services, nosql, unit and integration testing, datalayer development","cloud services, data engineering, datalayer development, datamodeling, nosql, sql, unit and integration testing"
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-engineering-at-acv-auctions-3762880106,2023-12-17,New York,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Compensation:
$124,000.00 - $152,000.00 annually. Please note that final compensation will be determined based upon the applicant's relevant experience, skillset, location, business needs, market demands, and other factors as permitted by law.
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","Computer Science, Information Technology, Computer Information Systems, Management Information Systems, SQL, ETL, SSIS, C#, Airflow, Python, AWS RDS, AWS S3, AWS SQS, AWS SNS, MongoDB, DBT, OLTP, OLAP, Snowflake","computer science, information technology, computer information systems, management information systems, sql, etl, ssis, c, airflow, python, aws rds, aws s3, aws sqs, aws sns, mongodb, dbt, oltp, olap, snowflake","airflow, aws rds, aws s3, aws sns, aws sqs, c, computer information systems, computer science, dbt, etl, information technology, management information systems, mongodb, olap, oltp, python, snowflake, sql, ssis"
Data Engineer IV - Max Digital (Data Operations),ACV Auctions,"New York, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-operations-at-acv-auctions-3762879091,2023-12-17,New York,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack (SSIS, C#, Web APIs) outages during our operational support window.
Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required.
Leverage DMVs and monitoring tools to ensure system performance; work with data operations and engineering to improve as required.
Ensure existing HADR (availability groups) solution is functional and meets requirements.
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively. This may include being part of the emergency after-hours on-call rotation.
Mentor junior data engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert in SQL Query optimization
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (e.g., MongoDB)
Experience developing Windows services in C#
Experience writing unit and integration testing
Expert SQL and data-layer development experience; OLTP schema design.
Experience using and integrating with cloud services, specifically: AWS RDS, S3, SQS, SNS.
Nice to Have
Experience with Airflow
Experience with DBT
Compensation:
$127,000.00 - $152,000.00 annually. Please note that final compensation will be determined based upon the applicant's relevant experience, skillset, location, business needs, market demands, and other factors as permitted by law.
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, Data Engineering, ETL, SSIS, C#, Python, AWS, RDS, S3, SQS, SNS, MongoDB, Windows services, Airflow, DBT, OLTP, True360, Data Services","sql, data engineering, etl, ssis, c, python, aws, rds, s3, sqs, sns, mongodb, windows services, airflow, dbt, oltp, true360, data services","airflow, aws, c, data engineering, data services, dbt, etl, mongodb, oltp, python, rds, s3, sns, sql, sqs, ssis, true360, windows services"
"Data Engineer, Consultant",MigrationIT,"Cairo, NY",https://www.linkedin.com/jobs/view/data-engineer-consultant-at-migrationit-3786249824,2023-12-17,Cairo,United States,Mid senior,Onsite,"Requirements
3-6 years of experience in data engineering
Gather requirements from stakeholders and translate them into technical specifications.
Design and implement data pipelines to collect, transform, and load data into data warehouses and data lakes.
Maintain and optimize data systems to ensure high availability and performance.
Work with other engineering teams to integrate data engineering solutions with other systems.
Stay up to date on the latest data engineering technologies and trends.
Excellent problem-solving and analytical skills.
Strong communication and interpersonal skills
Cloud computing platforms: AWS/Azure/GCP
Programming languages: Python/Java/Scala & Unix Shell scripting
Data warehouse technologies: Redshift/Snowflake/BigQuery
Data lake technologies: Hadoop, Spark, and Hive
Orchestration: Airflow/Control-M/(Any Orchestration tool)
Machine learning frameworks: TensorFlow, PyTorch, and scikit-learn
Show more
Show less","Data engineering, Data pipelines, Data warehouses, Data lakes, AWS, Azure, GCP, Python, Java, Scala, Unix Shell scripting, Redshift, Snowflake, BigQuery, Hadoop, Spark, Hive, Airflow, ControlM, Orchestration, TensorFlow, PyTorch, scikitlearn, Machine learning","data engineering, data pipelines, data warehouses, data lakes, aws, azure, gcp, python, java, scala, unix shell scripting, redshift, snowflake, bigquery, hadoop, spark, hive, airflow, controlm, orchestration, tensorflow, pytorch, scikitlearn, machine learning","airflow, aws, azure, bigquery, controlm, data engineering, data lakes, data warehouses, datapipeline, gcp, hadoop, hive, java, machine learning, orchestration, python, pytorch, redshift, scala, scikitlearn, snowflake, spark, tensorflow, unix shell scripting"
"Data Engineer, Sr. Consultant",MigrationIT,"Cairo, NY",https://www.linkedin.com/jobs/view/data-engineer-sr-consultant-at-migrationit-3786253425,2023-12-17,Cairo,United States,Mid senior,Onsite,"Requirements
Cloud computing platforms: AWS/Azure/GCP
Programming languages: Python/Java/Scala & Unix Shell scripting
Data warehouse technologies: Redshift/Snowflake/Big Query
Data lake technologies: Hadoop, Apache Beam, Spark, and Hive
Orchestration: Airflow/Control-M/(Any Orchestration tool)
Machine learning frameworks: TensorFlow, PyTorch, and scikit-learn
Lead a team of data engineers in the design, development, and implementation of data engineering solutions.
Gather requirements from stakeholders and translate them into technical specifications.
Design and implement data pipelines to collect, transform, and load data into data warehouses and data lakes.
Maintain and optimize data systems to ensure high availability and performance.
Work with other engineering teams to integrate data engineering solutions with other systems.
Stay up to date on the latest data engineering technologies and trends.
Show more
Show less","AWS, Azure, GCP, Python, Java, Scala, Unix Shell Scripting, Redshift, Snowflake, Big Query, Hadoop, Apache Beam, Spark, Hive, Airflow, ControlM, Data Pipeline, Tensorflow, PyTorch, Scikitlearn, Data Engineering, Data Warehouse, Data Lake","aws, azure, gcp, python, java, scala, unix shell scripting, redshift, snowflake, big query, hadoop, apache beam, spark, hive, airflow, controlm, data pipeline, tensorflow, pytorch, scikitlearn, data engineering, data warehouse, data lake","airflow, apache beam, aws, azure, big query, controlm, data engineering, data lake, data pipeline, datawarehouse, gcp, hadoop, hive, java, python, pytorch, redshift, scala, scikitlearn, snowflake, spark, tensorflow, unix shell scripting"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744387993,2023-12-17,Batavia,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Deployment, Stream processing, Kafka, Storm, Spark Streaming, Dimensional data modeling, Schema design, Data warehousing, ETL, Data pipelines, Data classification, Data retention, Legal compliance","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, stream processing, kafka, storm, spark streaming, dimensional data modeling, schema design, data warehousing, etl, data pipelines, data classification, data retention, legal compliance","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data retention, data science, datapipeline, datawarehouse, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, spark streaming, sql, storm, stream processing, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748833031,2023-12-17,Batavia,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Spark, Kafka, Kubernetes, Docker, Helm, Airflow, Agile engineering practices, TDD, ETL, Data Warehouses, Streamprocessing systems","python, sql, snowflake, spark, kafka, kubernetes, docker, helm, airflow, agile engineering practices, tdd, etl, data warehouses, streamprocessing systems","agile engineering practices, airflow, data warehouses, docker, etl, helm, kafka, kubernetes, python, snowflake, spark, sql, streamprocessing systems, tdd"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Kansas City, KS",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783187504,2023-12-17,Kansas,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Kansas-DataResearchAn.014
Show more
Show less","Python, JavaScript, JSON, Generative AI, Data Analytics, Programming, Data Science, Product Development, R, OOP","python, javascript, json, generative ai, data analytics, programming, data science, product development, r, oop","data science, dataanalytics, generative ai, javascript, json, oop, product development, programming, python, r"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Kansas City, KS",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783188279,2023-12-17,Kansas,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Kansas-DataScientist.008
Show more
Show less","Python, JavaScript, JSON, R, Generative AI, Machine learning, Data science, OOP languages, Product development, Data analytics, Coaching, Communication, Problem solving, Research, Education technology, EdTech, Startups","python, javascript, json, r, generative ai, machine learning, data science, oop languages, product development, data analytics, coaching, communication, problem solving, research, education technology, edtech, startups","coaching, communication, data science, dataanalytics, edtech, education technology, generative ai, javascript, json, machine learning, oop languages, problem solving, product development, python, r, research, startups"
Sr. Data Analyst,"Core Catalysts, LLC","Overland Park, KS",https://www.linkedin.com/jobs/view/sr-data-analyst-at-core-catalysts-llc-3694043137,2023-12-17,Kansas,United States,Mid senior,Onsite,"Core Catalysts, LLC is seeking a Sr. Data Analyst to join our team of consultants in Kansas City. As a management consulting firm that specializes in solving challenges across various industries in the U.S., we are looking for an individual who can analyze, interpret, and make recommendations on large and complex data sets that are essential for business performance improvement. The Sr. Data Analyst will play a critical role in helping businesses develop effective strategies and create lasting, impactful solutions that drive growth and sustainability. At Core Catalysts, we offer a results-oriented work environment where professionals can apply their skills and expertise to make a difference.
Responsibilities
Analyze large and complex data sets using analytical tools and methods to identify trends, patterns, and opportunities that support business decisions
Collaborate with consultants to design and implement data-driven strategies that improve business performance and customer satisfaction
Develop data models, dashboards, and reports that provide meaningful insights to stakeholders across all levels of the organization
Manage and maintain data integrity by designing and implementing data quality controls and validation methods
Identify and implement process improvements that increase efficiency and accuracy of data analysis
Stay up-to-date with market trends and best practices in data analysis, visualization, and modeling
Communicate data insights and recommendations in clear and concise terms to both technical and non-technical audiences
Requirements
Bachelor's degree in Data Science, Statistics, Economics, Mathematics, Information Systems, or related field
5+ years of experience as a data analyst or similar role in a consulting or professional services firm
Advanced proficiency in data analysis tools such as SQL, Python, Tableau, or Power BI
Experience with data modeling, data warehousing, and data visualization techniques
Strong analytical and problem-solving skills to interpret complex data sets and identify trends and patterns
Excellent communication skills to articulate data insights and recommendations to clients and team members
Ability to work independently and in a team environment, managing multiple priorities and deadlines
Benefits
Health Care Plan (Medical, Dental & Vision)
Retirement Plan (401k, IRA)
Life Insurance (Basic, Voluntary & AD&D)
Paid Time Off (Vacation, Sick & Public Holidays)
Short Term & Long Term Disability
Work From Home
Show more
Show less","SQL, Python, Tableau, Power BI, Data Analysis, Data Modeling, Data Warehousing, Data Visualization, Statistics, Data Integrity, Process Improvement, Data Quality, Data Analytics, Data Science, ProblemSolving, Communication","sql, python, tableau, power bi, data analysis, data modeling, data warehousing, data visualization, statistics, data integrity, process improvement, data quality, data analytics, data science, problemsolving, communication","communication, data integrity, data quality, data science, dataanalytics, datamodeling, datawarehouse, powerbi, problemsolving, process improvement, python, sql, statistics, tableau, visualization"
Senior Data Engineer,Intrepid Direct Insurance,"Overland Park, KS",https://www.linkedin.com/jobs/view/senior-data-engineer-at-intrepid-direct-insurance-3764405471,2023-12-17,Kansas,United States,Mid senior,Onsite,"Company Details
Intrepid Direct Insurance (IDI) is a rapidly growing innovative direct property and casualty company and a member of the W. R. Berkley Corporation, a fortune 500 company, rated A+ (Superior) by A.M. Best. Our vision is to make life better for business. We choose select industries in the US to offer our direct to consumer model. Our direct model allows to get close to our customers and become experts in their business. Our team is comprised of “A” level talent and innovative thinkers. You will be part of a highly collaborative team of talented and focused professionals. Join a group that enjoys working together, trusts each other, and takes pride in our hard-earned success.
https://intrepiddirect.com/
The Company is an equal employment opportunity employer.
Responsibilities
Intrepid Direct Insurance is looking for an experienced
Senior Data Engineer
to mentor, orchestrate, implement, and monitor the flow through our organization. This opportunity will have a direct influence on how data is made available to our business units, as well as our customers. You’ll primarily be working with our operations and engineering teams to create and enhance data pipelines, conform and enrich data, and deliver information to business users. Learn the ins and outs of what we do so that you can focus on improving availability and quality of the data we use to service our customers.
Key functions include but are not limited to the following:
Assist with long-term strategic planning for modern data warehousing needs.
Contribute to data modeling exercises and the buildout of our data warehouse.
Monitor, support, and analyze existing pipelines and recommend performance and process improvements to address gaps in existing processes.
Automate manual processes owned by data team.
Troubleshoot and remediate ingestion and reporting related issues.
Design and build new pipelines to ingest data from additional disparate sources.
Responsible for the accuracy and availability of data in our data warehouse.
Collaborate with a multi-disciplinary team to develop data-driven solutions that align with our business and technical needs.
Create and deploy reports as needed.
Assist with cataloging and classifying existing data sets.
Participate in peer reviews with emphasis on continuous improvement.
Respond to regulatory requests for information.
Assumes other tasks and duties as assigned by management.
Mentor team members and advise on best practices.
Qualifications
Bachelor’s degree in Mathematics, Statistics, Computer Science, or equivalent experience.
6+ years of relevant data engineering experience.
Analytical thinker with experience working in a fast-paced, startup environment.
Technical expertise with Microsoft SQL Server.
Familiarity with ETL tools and concepts.
Hands-on experience with database design and data modeling, preferable experience with Data Vault methodology.
Experience supporting and troubleshooting SSIS packages.
Experience consuming event-based data through APIs or queues.
Experience in Agile software development.
Experience with insurance data highly desired.
Detail oriented, solid organizational, and problem-solving.
Strong written, visual, and verbal communication skills.
Team oriented with a strong willingness to serve others in an agile startup environment.
Flexible in assuming new responsibilities as they arise.
Experience with Power Bi desired.
Additional Company Details
We do not accept unsolicited resumes from third party recruiting agencies or firms.
Sponsorship Details
Sponsorship not Offered for this Role
Show more
Show less","Mathematics, Statistics, Computer Science, Data Engineering, Microsoft SQL Server, ETL tools, SSIS packages, Agile software development, Data Vault methodology, Data modeling, Power Bi","mathematics, statistics, computer science, data engineering, microsoft sql server, etl tools, ssis packages, agile software development, data vault methodology, data modeling, power bi","agile software development, computer science, data engineering, data vault methodology, datamodeling, etl tools, mathematics, microsoft sql server, powerbi, ssis packages, statistics"
Data Analyst - Defense,Textron Aviation,"Wichita, KS",https://www.linkedin.com/jobs/view/data-analyst-defense-at-textron-aviation-3768157078,2023-12-17,Kansas,United States,Mid senior,Onsite,"JOB SUMMARY:
Data Analysts support the development and optimization of strategic initiatives by generating actionable insights and recommendations from internal and external data. Data Analysts support business growth by using business intelligence tools to mine complex data, identify historic trends, and visualize large datasets. Data Analysts play a critical role in data governance, helping to create and maintain data standards and quality.
The Data Analyst will work as part of the skilled and passionate Defense Aftermarket team delivering business solutions that enable the global sales and marketing teams. The position’s primary role will be to leverage internal and external data using data science techniques to solve business problems.
At Textron Aviation, we are building a community of Data & Analytics professionals with an emphasis on collaboration and cross functional support. You will have the opportunity to work closely with your peers throughout the organization toward a vision of data driven strategy
JOB RESPONSIBILITIES:
Develop and maintain data reports measuring daily business activities in Power BI, Alteryx, or similar tools using query languages and formula libraries such as SQL and DAX
Serves as an internal consultant to business leaders by conducting regular and ad-hoc analyses to convert data into strategic assets.
Develop statistical and machine learning models to answer business questions and make predictions using R, Python, and other programming languages
Develop technical as well as process-based solutions addressing business needs
Work to improve data quality by assisting data governance efforts in creating and maintaining data quality standards
Develop best practices and document proven solutions
Interface with other data professionals throughout the organization to embrace cross functional growth in analytics capabilities
Gather and document analytics project requirements from business stakeholders
Explore and understand the inner workings and market context of an aircraft OEM
Coordinate data management and resource requirements with data engineers
Conduct training to business analysts and internal customers on data and resources available through the analytics system
EDUCATION/EXPERIENCE:
Bachelor’s degree required in MIS, Computer Science, Data Analytics, Business Administration, Engineering, Mathematics, Economics, Statistics, or related technical field/coursework
Minimum 2 years relevant technical experience required, focused on data collection, utilization, and analysis.
Aviation experience preferred
QUALIFICATIONS:
Strong written and verbal communication skills
Experience with Microsoft Office including Excel and PowerPoint
Practical application experience with one or more analytics packages such as SAS, R, SQL, Python (& associated libraries), or similar
Experience with data visualization tools such as Power BI, Tableau, or similar tools
Experience working with relational databases and developing complex data sets
Ability to identify relevant metrics and explain technical information to a broad audience
Desire and ability to learn and leverage new software, tools, and processes in a self-learning environment
Textron Aviation Inc. must comply with U.S. export control laws.  If a position requires access to information controlled under U.S. regulations applicant must be eligible to meet any requirements to access controlled information.
The above statements are intended to describe the general nature and level of work being performed by employees assigned to this job. They are not intended to be an exhaustive list of all responsibilities, duties, and skills required of personnel so classified.
Textron Aviation has been inspiring the journey of flight for nine decades through the iconic and beloved Cessna and Beechcraft brands. We are passionate advocates of aviation, empowering people with the freedom of flight. As you join our legacy as a global leader in private aviation, you’ll have opportunities to try new fields, expand your skills and knowledge, stretch your abilities, and build your career. We provide a competitive and extensive total rewards package that includes pay and innovative benefits to support you and your family members – now and in the future, beginning day one. Your success is our success.
Join Textron Aviation’s Kansas team and you may be eligible for a $5,000 state of Kansas Aviation tax credit for up to five years. Visit https://www.aircapitaloftheworld.com/taxcredits for more information on the tax credit.
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.
Recruiting Company
Textron Aviation
Primary Location
US-Kansas-Wichita
Job Field
Business Development
Schedule
Full-time
Job Level
Individual Contributor
Job Type
Experienced
Shift
First Shift
Job Posting
12/08/2023, 9:39:41 AM
Show more
Show less","Power BI, Alteryx, SQL, DAX, SAS, Python, R, Tableau, Microsoft Excel, PowerPoint, Relational databases, Data visualization","power bi, alteryx, sql, dax, sas, python, r, tableau, microsoft excel, powerpoint, relational databases, data visualization","alteryx, dax, microsoft excel, powerbi, powerpoint, python, r, relational databases, sas, sql, tableau, visualization"
Staff Data Engineer,Recruiting from Scratch,"Wichita, KS",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744391991,2023-12-17,Kansas,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, Pyspark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Kafka, Storm, Spark streaming, Dimensional data modeling, Schema design, Data warehouses, ETL, Data management, Data classification, Retention, Legal compliance","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, spark streaming, dimensional data modeling, schema design, data warehouses, etl, data management, data classification, retention, legal compliance","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, retention, schema design, snowflake, spark, spark streaming, sql, storm, tdd"
Senior QA Engineer - Data & Analytics Platform,CommunityAmerica Credit Union,"Lenexa, KS",https://www.linkedin.com/jobs/view/senior-qa-engineer-data-analytics-platform-at-communityamerica-credit-union-3738683907,2023-12-17,Kansas,United States,Mid senior,Onsite,"Newly created position! Join our innovative and expanding IT team as we add top talent who will help us achieve our significant growth goals and objectives in the upcoming years. Recognized as a best place to work in Kansas City for multiple years, we are building out our future IT platforms and capabilities to be poised to take full advantage of digital banking advancements, AI, machine learning, data analysis and modeling and more. We are committed to a sophisticated and forward-thinking approach to technology and to being a leader and driver of innovation in the financial services industry.
As a key member of the data and analytics platform team, you will be responsible for the creation of well-structured and comprehensive test plans to identify issues with our data and analytics platforms both before and following scheduled releases. In a senior capacity, you will also be responsible for working with business and IT stakeholders to identify clear acceptance criteria, working with our engineering team to establish and refine quality engineering processes, and help to inform and develop automated testing against our data and analytics platforms at CACU. Specifically, the successful candidate will be a member of an Agile Test Team working with other Agile data engineers, data and business analysts, product owners and managers and offshore QA teams. The senior QA engineer will participate and advise in defining QA processes and procedures and also participate and advise in the assessment of QA tools and technologies.
Education and Experience Requirements
Bachelor’s Degree with 7 plus years of experience
Required Knowledge, Skills and Abilities
Experience developing automation tests using testing platforms/engines such as Selenium, Postman, HP UFT/QTP, IBM Rational, etc.– 5 years.
Experience with SQL and data/ETL validation patterns
Experience developing test scripts to implement Features, Test Scenarios and Step-Definitions in a customized test automation framework.
Experience with ETL tools such as SSIS and Azure Data Factory
Experience testing in Scrum and Kanban Agile development/test processes.
Strong communication and collaborative skills in providing technical direction for automated testing processes and procedures.
Other duties as assigned.
Preferred Knowledge, Skills and Abilities
Experience in cloud technologies with preference towards Microsoft Azure
Experience working with data platforms and data engineering teams
Experience implementing and executing automated regression suite tests
Exposure to load testing and performance testing
Experience with source code versioning and Pull Requests with Git and Stash.
Experience with Azure DevOps or similar cloud dev ops platforms.
Experience with environments utilizing Cloud/Virtualization Technologies such as Docker and container management solution like Kubernetes.
Experience with CI/CD pipelines and integrations
Experience in banking or financial institutions with a strong background on supporting business partners.
Show more
Show less","Selenium, Postman, HP UFT/QTP, IBM Rational, SQL, SSIS, Azure Data Factory, Scrum, Kanban, Azure, Git, Stash, Azure DevOps, Docker, Kubernetes, CI/CD, Banking, Financial services","selenium, postman, hp uftqtp, ibm rational, sql, ssis, azure data factory, scrum, kanban, azure, git, stash, azure devops, docker, kubernetes, cicd, banking, financial services","azure, azure data factory, azure devops, banking, cicd, docker, financial services, git, hp uftqtp, ibm rational, kanban, kubernetes, postman, scrum, selenium, sql, ssis, stash"
Senior Data Center Engineer - Facilities Engineer,U.S. Bank,"Olathe, KS",https://www.linkedin.com/jobs/view/senior-data-center-engineer-facilities-engineer-at-u-s-bank-3781973402,2023-12-17,Kansas,United States,Mid senior,Onsite,"At U.S. Bank, we’re on a journey to do our best. Helping the customers and businesses we serve to make better and smarter financial decisions and enabling the communities we support to grow and succeed. We believe it takes all of us to bring our shared ambition to life, and each person is unique in their potential. A career with U.S. Bank gives you a wide, ever-growing range of opportunities to discover what makes you thrive at every stage of your career. Try new things, learn new skills and discover what you excel at—all from Day One.
Job Description
U.S. Bank is seeking a
Data Center Engineer with electrical/mechanical/HVAC and facility engineering support experience
to contribute toward the success of our technology initiatives.
The position will be a
night shift position
.
Hours of a normal shift are 12 hours 6:00PM to 6:00AM. The shift requires an employee to work every other weekend in a 5 on-2 off, 2 on-5 off schedule. The position also includes a 15% Premium on all hours worked.
** this is an
on-site
role from
Olathe, Kansas
**
What You Will Do
U.S. Bank is seeking a
Data Center Engineer with electrical/mechanical/HVAC and facility engineering support experience
to contribute toward the success of our technology initiatives.
Responsible for daily operation of the facility engineering environment supporting one of the company's main data centers: electrical and mechanical distribution, power generation, cooling, UPS, automated building controls, and Fire/Life/Safety
.
Performs preventative maintenance programs for all critical and non-critical
plant operations
with minor supervision. Works with management to develop, recommend, and implement departmental policy, guidelines, standards, and procedures; assures compliance with all governing corporate standards and guidelines.
Ability to read/comprehend architectural, plumbing, electrical, and mechanical drawings.
Builds and maintains effective communication and relationships with management team, vendors, and peers. Develops and maintains interpersonal communication channels to convey information internally and externally.
Performs
on-call and after-hours
support functions as directed by departmental policy or management. Works all required overtime. Demonstrates skills, behaviors, and attitudes consistent with the strategic objectives and values of the organization. Follows all
OSHA, safety, and environmental programs including training and certification
, as required, to maintain compliance.
Basic Qualifications
Associate degree or equivalent work experience
At least one year of experience with the electrical engineering, maintenance, environmental, health, safety, energy, controls/instrumentation, civil engineering, and HVAC needs of a facility or plant
Preferred Skills/Experience
Working experience in: IT environment, Hardware infrastructure, Computer facilities management, Computer operations.
If there’s anything we can do to accommodate a disability during any portion of the application or hiring process, please refer to our disability accommodations for applicants.
Learn how
the way we work at U.S. Bank drives meaningful relationships with our customers and collaboration across the company.
Benefits:
Our approach to benefits and total rewards considers our team members’ whole selves and what may be needed to thrive in and outside work. That's why our benefits are designed to help you and your family boost your health, protect your financial security and give you peace of mind. Our benefits include the following (some may vary based on role, location or hours):
Healthcare (medical, dental, vision)
Basic term and optional term life insurance
Short-term and long-term disability
Pregnancy disability and parental leave
401(k) and employer-funded retirement plan
Paid vacation (from two to five weeks depending on salary grade and tenure)
Up to 11 paid holiday opportunities
Adoption assistance
Sick and Safe Leave accruals of one hour for every 30 worked, up to 80 hours per calendar year unless otherwise provided by law
EEO is the Law
U.S. Bank is an equal opportunity employer committed to creating a diverse workforce. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, sexual orientation, gender identity, disability or veteran status, among other factors. Applicants can learn more about the company’s status as an equal opportunity employer by viewing the federal
KNOW YOUR RIGHTS
EEO poster.
E-Verify
U.S. Bank participates in the U.S. Department of Homeland Security E-Verify program in all facilities located in the United States and certain U.S. territories. The E-Verify program is an Internet-based employment eligibility verification system operated by the U.S. Citizenship and Immigration Services. Learn more about the E-Verify program.
The salary range reflects figures based on the primary location, which is listed first. The actual range for the role may differ based on the location of the role. In addition to salary, US Bank offers a comprehensive benefits package, including incentive and recognition programs, equity stock purchase 401k contribution and pension (all benefits are subject to eligibility requirements). Pay Range: $27.45 - $36.59 - $40.25
Show more
Show less","Data Center Engineer, Electrical engineering, HVAC, Facility engineering, UPS, OSHA, Environmental programs, Architectural drawings, Plumbing drawings, Electrical drawings, Mechanical drawings, IT environment, Hardware infrastructure, Computer facilities management, Computer operations","data center engineer, electrical engineering, hvac, facility engineering, ups, osha, environmental programs, architectural drawings, plumbing drawings, electrical drawings, mechanical drawings, it environment, hardware infrastructure, computer facilities management, computer operations","architectural drawings, computer facilities management, computer operations, data center engineer, electrical drawings, electrical engineering, environmental programs, facility engineering, hardware infrastructure, hvac, it environment, mechanical drawings, osha, plumbing drawings, ups"
Data Analyst (Process Mining) – Automotive,Energy Jobline,"Wichita, KS",https://www.linkedin.com/jobs/view/data-analyst-process-mining-%E2%80%93-automotive-at-energy-jobline-3773709186,2023-12-17,Kansas,United States,Mid senior,Onsite,"Job Description
Akkodis è un leader globale nel mercato dell'ingegneria e della ricerca e sviluppo che mira ad accelerare l'innovazione e la trasformazione digitale utilizzando la forza dei connected data.
Con la passione per la tecnologia e il talento, il gruppo composto da 50.000 ingegneri ed esperti digitali offre una profonda esperienza intersettoriale in 30 paesi tra Nord America, EMEA e APAC.
Akkodis vanta un’ampia esperienza e un forte know-how in settori tecnologici chiave come la mobilità, i servizi software e tecnologici, la robotica, i test, le simulazioni, la sicurezza dei dati, l'intelligenza artificiale e l'analisi dei dati.
La combinazione di competenze informatiche e ingegneristiche porta a un'offerta unica di soluzioni end-to-end, con quattro linee di servizio - Consulting, Solutions, Talents e Academy - per supportare i clienti nel ripensare ai loro processi di sviluppo prodotto e di business, migliorare la produttività, ridurre al minimo il time-to-market e dare forma a un domani più smart e sostenibile.
Nell’ottica Di Un Potenziamento Del Team, Ricerchiamo Un
Data Analyst (Process Mining) - Automotive
La risorsa verrà inserita all'interno di un team e si occuperà di individuare, con l'utilizzo di strumenti avanzati di Process Mining, le inefficienze, migliorare l'efficacia operativa e aumentare la produttività.
Principali Attività
gestire di iniziative end-to-end di Data Analytics / Process Mining presso vari clienti e settori industriali
collaborare con i dipartimenti aziendali per la raccolta dei requisiti necessari
eseguire di operazioni di identificazione, estrazione e trasformazione dei dati (ETL)
supportare l'integrazione tra gli strumenti di Data Analytics / Process Mining e i sistemi IT del cliente
progettare e sviluppare dashboard di monitoraggio dei processi
monitorare le performance dei processi
redigere analisi e reportistica
definire piani per le azioni correttive.
Profile
Profilo
Il candidato ideale è in possesso di una
Laurea Magistrale i
n Economia, Matematica applicata, Ingegneria, Statistica o equivalente ed h
a maturato almeno 2 anni
come Data Scientist, Data Engineer o Data Analyst oltre ad avere competenze nell'analisi dei processi aziendali e nell'ottimizzazione degli stessi.
Completano il profilo proattività, teamworking, competenze organizzative e di problem solving, oltre ad una
buona conoscenza della lingua italiana ed inglese.
Requisiti
Sono Richieste Le Seguenti Competenze
buona conoscenza degli strumenti di Data Analytics (SAP BW, Power BI, Qlik, SAC, ecc.) e Process Mining (Celonis, Signavio, ecc.)
capacità di creare scripting per la raccolta dati da fonti diverse (SQL)
Offerta
Tempo Indeterminato
Sede di lavoro
Modena (modalità ibrida)
Apply in one click Apply for job opening
Show more
Show less","Data Analytics, Process Mining, SQL, SAP BW, Power BI, Qlik, SAC, Celonis, Signavio, ETL, Dashboard, Reporting, Data visualization, Process optimization, Business analysis, Problem solving, Teamworking, Communication, SQL","data analytics, process mining, sql, sap bw, power bi, qlik, sac, celonis, signavio, etl, dashboard, reporting, data visualization, process optimization, business analysis, problem solving, teamworking, communication, sql","business analysis, celonis, communication, dashboard, dataanalytics, etl, powerbi, problem solving, process mining, process optimization, qlik, reporting, sac, sap bw, signavio, sql, teamworking, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Wichita, KS",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744392544,2023-12-17,Kansas,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, Kafka, SQL, Data Warehouses, ETL, TDD, Pair Programming, Continuous Integration, Automated testing, Dimensional data modeling, Schema design, Data classification, Data retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, sql, data warehouses, etl, tdd, pair programming, continuous integration, automated testing, dimensional data modeling, schema design, data classification, data retention","airflow, automated testing, continuous integration, data classification, data retention, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sql, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Overland Park, KS",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748829419,2023-12-17,Kansas,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Spark, Kafka, ETL, Kubernetes, TDD, Continuous Integration, Agile engineering","python, sql, snowflake, spark, kafka, etl, kubernetes, tdd, continuous integration, agile engineering","agile engineering, continuous integration, etl, kafka, kubernetes, python, snowflake, spark, sql, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Wichita, KS",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748827509,2023-12-17,Kansas,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","TDD, Automation, Kafka, Continuous Delivery, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, SQL, Agile Engineering Practices, Pair Programming, Continuous Integration, Automated Testing, Deployment, ETL, Data Warehouses, Dimensional Data Modeling, Schema Design, Data Classification, Data Retention","tdd, automation, kafka, continuous delivery, python, snowflake, airflow, kubernetes, docker, helm, spark, sql, agile engineering practices, pair programming, continuous integration, automated testing, deployment, etl, data warehouses, dimensional data modeling, schema design, data classification, data retention","agile engineering practices, airflow, automated testing, automation, continuous delivery, continuous integration, data classification, data retention, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sql, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Overland Park, KS",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744395166,2023-12-17,Kansas,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data Warehouses, ETL, Legal compliance, Data classification, Data retention, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, legal compliance, data classification, data retention, tdd, pair programming, continuous integration, automated testing, deployment","airflow, automated testing, continuous integration, data classification, data retention, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Data Analyst (Remote),Cambridge Technology (CT),"Kansas City, KS",https://www.linkedin.com/jobs/view/senior-data-analyst-remote-at-cambridge-technology-ct-3598176811,2023-12-17,Kansas,United States,Mid senior,Remote,"Join the Cambridge Technology team and grow your career. We are solving real world problems with creative innovation at some of the most recognizable corporations in the world.
From building entire infrastructures or platforms to solving complex IT challenges, we help businesses accelerate their digital transformation and become AI-first businesses. With over 20 years of expertise as a technology services company, we enable our customers to stay ahead of the curve by helping them figure out the perfect approach, solutions, and ecosystem for their business. Our experts help customers leverage the right AI, big data, cloud solutions, and intelligent platforms that will help them become and stay relevant in a rapidly changing world.
Data analyst who can turn data into information, information into insight, and insight into business decisions. He or she should be highly skilled in all aspects of data analytics, including mining, generation, and visualization. You should be able to do advanced business analysis with the help of techniques, e.g. statistical analysis, explanatory and predictive modeling, and data mining, and should be committed to transforming data into readable, goal-driven reports for continued innovation and growth of business.
Skills Required
7+ years relevant experience in financial/business analysis
Experience defining requirements and using data and metrics to draw business insights
Experience making business recommendations and influencing stakeholders using innovative analytical insights within the Data Products Program.
Experience in Conducting detailed data analysis on data used across business units to evaluate business processes and improve or create new processes or features.
Perform various data analytics in SQL and MS Excel using statistical models or industry accepted tools.
Proficiency in SQL and experience extracting data following standard Data Pipelines (ideally loading Databases using ETL approaches using python, SQL, & no SQL data sources)
Provide relational database expertise to construct and execute SQL queries to be used in data analysis activities
Strong skills in Excel as well as proven experience using business intelligence reporting/visualization tools like OBIEE, Google, Data Studio, Tableau.
Good experience on cloud platforms (AWS, Azure, or Google Cloud Platform). Experience in GCP is a big plus.
Good relevant experience in Python, Spark, SQL, Java and JavaScript / PowerShell
Experience in analysing data to solve business problems and presenting impactful insights to business teams, using SQL, or other Big Data technologies, data manipulation using a procedural language.
Deep understanding on Data Analysis, schema design and dimensional data modelling, Data lake/Data Bricks, Snow Flake is a big plus
Demonstrated record of partnering with and influencing cross-functional teams.
Preferred Qualifications
Master's degree in Information Technology, Computer Science, Data Analytics, Information Science, Operations research or a related field.
Experience in business process improvement, Lean/Six Sigma. Analytical and quantitative skills ability to use hard data and metrics to back up assumptions and develop business cases.
The right person will be highly technical and analytical, possess software design and development and/or IT and networking implementation/consulting experience.
We are currently considering applicants located in the US only.
Show more
Show less","Data analysis, Data mining, Data visualization, Statistical analysis, Explanatory modeling, Predictive modeling, SQL, MS Excel, Python, Spark, Java, JavaScript, PowerShell, AWS, Azure, Google Cloud Platform, GCP, OBIEE, Google Data Studio, Tableau, Data lake, Data Bricks, Snow Flake","data analysis, data mining, data visualization, statistical analysis, explanatory modeling, predictive modeling, sql, ms excel, python, spark, java, javascript, powershell, aws, azure, google cloud platform, gcp, obiee, google data studio, tableau, data lake, data bricks, snow flake","aws, azure, data bricks, data lake, data mining, dataanalytics, explanatory modeling, gcp, google cloud platform, google data studio, java, javascript, ms excel, obiee, powershell, predictive modeling, python, snow flake, spark, sql, statistical analysis, tableau, visualization"
Healthcare Data Analyst III (Remote capable),Geisinger,"Home, KS",https://www.linkedin.com/jobs/view/healthcare-data-analyst-iii-remote-capable-at-geisinger-3766862959,2023-12-17,Kansas,United States,Mid senior,Remote,"Job Summary
Uses data expertise, programing abilities, and critical thinking skills to provide analytic expertise, deliver innovative solutions, and act as a strategic thought partner. Responsible for supporting investigator driven research from inception to closeout by providing feedback on study design, data collection strategies, effort estimation, analytic strategies, and stewardship. Develops code to deliver high quality data products on time to customers. Mentors junior staff by offering project feedback and code review. Works with research staff to determine goals, priorities, and decision making. Participates on high visibility projects for both internal and external clients. Leads efforts to assess, recommend, implement, troubleshoot, validate, and interpret results. Experience in developing programs using large relational database models written in SQL and/or SAS required. Experience using data from an electronic health record or health care claims in the area of health research or healthcare analytics (preference for Epic’s Clarity models) required.
Job Duties
Performs complex data extraction, manipulation, and summarization of large databases to create analytical datasets and provides a range of solutions to support Research activities.
Takes lead role in the design, testing, validation, analysis and merging of complicated data structures from a wide variety of source systems.
Highly effective in written and oral communication of complex, analytical, technical information, results/recommendations to research project team.
Able to create summarized findings and recommendations that are clearly presented and adapted for audiences that have a varying range of technical and clinical experience.
Actively engages and takes lead role in the development of complex phenotype algorithms and increasingly significant tasks.
Acts as a strategic partner for investigators to identify key areas and priorities based on analytic insights and data analysis.
Utilizes extensive knowledge of research methodology and research data collection to drive discussions with clients and the team.
Leads discussions that proactively educate the requester of any complexities associated with the data request.
Oversees, leads and consults on all phases of SAS/SQL/analytical programming, data management, quality control, and reporting.
Expert ability to identify and resolve problems using knowledge, background and troubleshooting skills.
Leads efforts to ensure accuracy, data integrity and validity of data and analysis in all work.
Work is typically performed in an office environment. Accountable for satisfying all job specific obligations and complying with all organization policies and procedures. The specific statements in this profile are not intended to be all-inclusive. They represent typical elements considered necessary to successfully perform the job.
Relevant experience may be a combination of related work experience and/or Master's degree obtained (Master's Degree = 2 years relevant experience).
Education
Bachelor's Degree-Computer Science, Information Systems, Information Science (Required)
Experience
Minimum of 5 years-Relevant experience* (Required)
Skills
Database Skills; Critical Thinking; Working Independently
About Geisinger
Founded more than 100 years ago by Abigail Geisinger, the system now includes ten hospital campuses, a 550,000-member health plan, two research centers and the Geisinger Commonwealth School of Medicine. With nearly 24,000 employees and more than 1,700 employed physicians, Geisinger boosts its hometown economies in Pennsylvania by billions of dollars annually. Learn more at geisinger.org or connect with us on Facebook , Instagram , LinkedIn and Twitter .
Our Commitment to Diversity, Equity and Inclusion
Geisinger values who you are, where you are from, and where you are going. We seek out people of various backgrounds and cultures with unique abilities, non-traditional career paths and ambitious aspirations. We are an Affirmative Action, Equal Opportunity Employer. Women and Minorities are encouraged to apply. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of disability or their protected veteran status. Read more about Our Commitment to Inclusive Recruiting .
Our Vision & Values
Everything we do is about making better health easier for our patients, our members, our students, our Geisinger family and our communities.
KINDNESS
: We strive to treat everyone as we would hope to be treated ourselves.
EXCELLENCE
: We treasure colleagues who humbly strive for excellence.
LEARNING
: We share our knowledge with the best and brightest to better prepare the caregivers for tomorrow.
INNOVATION
: We constantly seek new and better ways to care for our patients, our members, our community, and the nation.
SAFETY
: We provide a safe environment for our patients and members and the Geisinger family.
Our Benefits
We offer healthcare benefits for full time and part time positions from day one, including vision, dental and prescription coverage.
Show more
Show less","SQL, SAS, Programming, Data Management, Data Analysis, Data Collection, Database Models, Data Manipulation, Data Summaries, Data Structures, Analytical Datasets, Data Visualization, Data Reporting, Data Quality Control, Algorithm Development, Phenotype Algorithms, Decision Making, Communication, Critical Thinking, Project Management, Teamwork, Research Methodology, Healthcare Analytics, Electronic Health Records, Health Care Claims, Epic's Clarity Models","sql, sas, programming, data management, data analysis, data collection, database models, data manipulation, data summaries, data structures, analytical datasets, data visualization, data reporting, data quality control, algorithm development, phenotype algorithms, decision making, communication, critical thinking, project management, teamwork, research methodology, healthcare analytics, electronic health records, health care claims, epics clarity models","algorithm development, analytical datasets, communication, critical thinking, data collection, data management, data manipulation, data quality control, data reporting, data structures, data summaries, dataanalytics, database models, decision making, electronic health records, epics clarity models, health care claims, healthcare analytics, phenotype algorithms, programming, project management, research methodology, sas, sql, teamwork, visualization"
Licensed Civil Engineer - Data Center (Remote),Olsson,"Overland Park, KS",https://www.linkedin.com/jobs/view/licensed-civil-engineer-data-center-remote-at-olsson-3784204618,2023-12-17,Kansas,United States,Mid senior,Remote,"Company Description
We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As a Licensed Civil Engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Qualifications
You are passionate about:
Working collaboratively with others
Having ownership in the work you do
Using your talents to positively affect communities
Solving problems
Providing excellence in client service
You bring to the team:
Strong communication skills
Ability to contribute and work well on a team
Bachelor's Degree in civil engineering
At least 6 years of related civil engineering experience
Proficient in Civil 3D software
Must be a registered professional engineer
Additional Information
Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
Engage in work that has a positive impact in communities
Receive an excellent 401(k) match
Participate in a wellness program promoting balanced lifestyles
Benefit from a bonus system that rewards performance
Have the possibility for flexible work arrangements
Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
Show more
Show less","Civil Engineering, Project Management, Design Engineering, Planning, Design Documents, Design Calculations, Team Standards, Client Standards, Quality Assurance, Quality Control, AutoCAD Civil 3D, Communication Skills, Teamwork, Bachelor's Degree, Registered Professional Engineer","civil engineering, project management, design engineering, planning, design documents, design calculations, team standards, client standards, quality assurance, quality control, autocad civil 3d, communication skills, teamwork, bachelors degree, registered professional engineer","autocad civil 3d, bachelors degree, civil engineering, client standards, communication skills, design calculations, design documents, design engineering, planning, project management, quality assurance, quality control, registered professional engineer, team standards, teamwork"
Systems Analyst - Senior (Data Engineering/Architecture),UPMC,"Home, KS",https://www.linkedin.com/jobs/view/systems-analyst-senior-data-engineering-architecture-at-upmc-3769293564,2023-12-17,Kansas,United States,Mid senior,Hybrid,"Purpose
Under the general direction of the management team and senior staff of the HCMS IT department, the
Systems Analyst - Senior
(Data Engineer)
requires a proficient level of experienced analytical services, defining requirements, developing and/or maintaining computer applications/systems, and providing services to meet client IT and business needs.
Work from Home Opportunity!!
Responsibilities
System Integration: May be responsible for coordination of tasks and resources related to system integration, validation of testing and implementation.
SDLC (System Development Life Cycle): Have a proficient understanding of multiple system/application development life cycles.
Data Quality: Maintain data quality at all times.
Application Upgrades and Implementation: Identify new functionality and/or hardware requirements related to application upgrades and implementations. Creates test plans. Responsible for review and validation of functionality. Report back any problems. Create and/or manage cutover plans including downtime, etc. Responsible for evaluating impact and coordinating efforts across multiple platforms as necessary.
Interactions with Others: Successfully completes projects, tasks, and initiatives by embracing a team-first approach. Works in collaboration with team and offers feedback, where appropriate, to complete individual and group efforts. Shows the ability to adjust and be flexible to change by adapting approach when necessary. Mentors less experienced staff.
Communication: Responsible for demonstrating appropriate, clear, concise, and effective written and oral communications in all interactions to build relationships and accomplish day to day work and projects.
Data Confidentiality/Security: Maintain confidentiality of sensitive information at all times.
Project Management: Take ownership of a project and have the ability to distribute tasks to team members and meet milestone completion. Update all project management and time tracking tools accordingly.
Vendor Relationships: Interact with vendors (technical issues, project initiatives) independently, as necessary. Ability to act as the point person for issue escalation.
End User Training: Ability to create training content. Facilitate more detailed user training sessions. Ability to train peers.
Documentation: Complete detail-oriented documentation for new and moderately complex processes. Responsible for the quality and validity of produced documents. Extract and document customer/business requirements and needs for use by enterprise architecture and engineering teams (network, system, and software).
Second and Third Level Support (Including Maintenance Activities): Independently triage and resolve Level 2 and Level 3 support issues. Act as a mentor to less experienced staff in resolution of Level 2 and Level 3 issues. Ability to handle problem management as appropriate.
Report Writing/Analysis: Write and analyze complex reports. Make modifications to complex reports. Mentor less experienced team members. Communicate with the business/act as business analyst.
Process Improvement: Ability to manage process improvement efforts. Create and update processes, as necessary. Ability to independently recognize opportunity for process improvements.
Self-Development: Responsible for continuous self-study, trainings, partnering with more senior members of team, and/or seeking out opportunities to broaden scope to stay up to date with industry and organizational trends. Seeks feedback from senior team members for development and effectively incorporates feedback into work and behaviors.
Typically has 5+ years' experience with modern technology and application support through education or practical experience. Highly driven and self-motivated to exceed expectations.
Ability to work independently and in a team-based environment.
Demonstrates thorough understanding of information technology fundamental tools and concepts (SDLC) of one of the information technology professional disciplines and applies that understanding to make independent practical contributions to IT work within a UPMC department or function.
Completes on-going training on-the-job, through courses, self-study, certifications and/or advanced degrees to maintain and enhance technical and business capabilities.
Additionally, this position may be required to maintain a standby status as part of a rotation within the team. This requires 24 hours per day, 7 days per week availability during the standby period. The frequency varies based upon the number of colleagues in the rotation.
Must Have Experience
7 or more years of related role experience
XML/XSL/Java and Excel Proficiency
Demonstrate consistent productivity against KPIs
Excellent in communication and responsiveness
Implementation/onboarding experience
Contribute as an SME in import, exports or payroll.
Proficiency in payroll, imports and exports
Client facing communication skills.
Leadership recognition among peers
Contributions towards leadership and mentorship
Work from Home Opportunity!
Licensure, Certifications, And Clearances
Preferred Licensure:ACBT - Avaya CBTCXADMIN - AVST Cert CX AdminITIL - IT Infrastructure Library
UPMC is an Equal Opportunity Employer/Disability/Veteran
Show more
Show less","System Integration, SDLC, Data Quality, XML, XSL, Java, Excel, KPIs, Report Writing, Process Improvement, SelfDevelopment, Vendor Relationships, End User Training, Documentation, Second and Third Level Support, ITIL, Avaya CBTCXADMIN, AVST Cert CX Admin","system integration, sdlc, data quality, xml, xsl, java, excel, kpis, report writing, process improvement, selfdevelopment, vendor relationships, end user training, documentation, second and third level support, itil, avaya cbtcxadmin, avst cert cx admin","avaya cbtcxadmin, avst cert cx admin, data quality, documentation, end user training, excel, itil, java, kpis, process improvement, report writing, sdlc, second and third level support, selfdevelopment, system integration, vendor relationships, xml, xsl"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Overland Park, KS",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712033,2023-12-17,Kansas,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","ML, Java, SQL, Python, Docker, Kubernetes, Scala, Kafka, Spark, NoSQL, ETL, Relational databases, Bash, Git, Airflow, Kubeflow, Snowflake, Helm, DynamoDB, AWS, GCP, Azure, Storm, SparkStreaming, Pandas, R","ml, java, sql, python, docker, kubernetes, scala, kafka, spark, nosql, etl, relational databases, bash, git, airflow, kubeflow, snowflake, helm, dynamodb, aws, gcp, azure, storm, sparkstreaming, pandas, r","airflow, aws, azure, bash, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, ml, nosql, pandas, python, r, relational databases, scala, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Wichita, KS",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707761,2023-12-17,Kansas,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Science, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Management Tools, Data Classification, Data Retention","data engineering, machine learning, data science, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, data retention","airflow, applied machine learning, aws, azure, bash, data classification, data engineering, data management tools, data retention, data science, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, python, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Wichita, KS",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773089685,2023-12-17,Kansas,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, SQL, bash, Kubernetes, NoSQL, ML, Airflow, Kafka, Storm, SparkStreaming, AIcentric features, Data mining, Data cleaning, Data normalizing, Data modeling, Data processing, Data pre/post processing, Data pipelines, Data enrichment, Data monitoring, Data visualization, Orchestration frameworks, Git, Snowflake, Docker, Helm, Spark, pySpark, ETL, Distributed systems, Microservices","python, java, sql, bash, kubernetes, nosql, ml, airflow, kafka, storm, sparkstreaming, aicentric features, data mining, data cleaning, data normalizing, data modeling, data processing, data prepost processing, data pipelines, data enrichment, data monitoring, data visualization, orchestration frameworks, git, snowflake, docker, helm, spark, pyspark, etl, distributed systems, microservices","aicentric features, airflow, bash, data cleaning, data enrichment, data mining, data monitoring, data normalizing, data prepost processing, data processing, datamodeling, datapipeline, distributed systems, docker, etl, git, helm, java, kafka, kubernetes, microservices, ml, nosql, orchestration frameworks, python, snowflake, spark, sparkstreaming, sql, storm, visualization"
Data Scientist,Koch Industries,"Wichita, KS",https://www.linkedin.com/jobs/view/data-scientist-at-koch-industries-3756945258,2023-12-17,Kansas,United States,Mid senior,Hybrid,"Your Job
We are seeking a motivated and innovative Data Scientist to help advance our HR Analytics journey in support of our vision. A successful candidate will bring advanced knowledge of best-in-class data science methodologies, deep understanding of AI/ML space, proven track record of solving business problems using data science and ability to work with global teams to deliver and execute complex analytics problems. You must be enthusiastically collaborative, value seeking, open to challenge and be challenged with new ideas and established approaches with an appetite for learning and innovation.
Our Team
This role is part of the Global KGS HR Technology organization requiring excellent communication and collaboration skills with other technical groups as well as business leaders. You will work with Business HR partners and leaders to develop and implement advanced analytics solutions to capture unmet opportunities and create value for our customers.
What You Will Do
Exhibits a can-do, growth mindset, consistently pursuing skill enhancement and enthusiastically tackling new challenges to drive advancement of business and data science capabilities
Communicate with clients to understand the challenges they face and explore data driven solutions
Exploring and extracting data from multiple sources and interrogate it to discover trends and patterns, creating meaningful insights
Assess the effectiveness and accuracy of new data sources and data gathering techniques
Programs data science and analytics solutions in languages such as Python, R
Applies strong knowledge of statistics and mathematics including hypothesis testing, probability and regression for making informed decisions based on data
Researching, developing, and deploying machine learning models (supervised and unsupervised) to deliver predictive analytics
Maintaining and improving the machine learning pipelines, processes and products
Design/Develops data visualizations for effectively communicating data findings to both technical and non-technical audiences
Develop processes and tools to monitor and analyze model performance and data accuracy.
Provide best practices and guidance to other team members on data science projects.
Who You Are (Basic Qualifications)
3+ years of experience on the whole life cycle of machine learning (ML) model building for solving real-world complex problems.
Experience in Data Analysis techniques, EDA, Data Visualization to effectively communicate to stakeholders, clarify requirements and make effective suggestions to achieve business values.
Experience in Time Series Forecasting /Tree based models/and linear models.
3+ years of experience in SQL coding, python programming skills coupled with demonstrable experience using data science packages (numpy, pandas, sklearn, etc.)
Knowledge of bringing machine learning models from prototype to production.
What Will Put You Ahead
Experience working in HR/Recruiting/Talent analytics
Bachelor or Masters degree in a quantitative field such as Finance, Mathematics, Analytics, Data Science, Computer Science, Engineering or relevant work experience
At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.
Hiring Philosophy
All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.
Who We Are
As a Koch company, Koch Global Services (KGS) creates solutions spanning technology, human resources, finance, project management and anything else our businesses need. With locations in India, Mexico, Poland and the United States, our employees have the opportunity to make a global impact.
At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.
Our Benefits
Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.
Equal Opportunities
Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information: http://www.kochcareers.com/doc/Everify.pdf
Show more
Show less","Data Science, AI/ML, Data Analysis, Data Visualization, Python, R, SQL, Statistics, Mathematics, Hypothesis Testing, Probability, Regression, Machine Learning, Supervised Learning, Unsupervised Learning, Predictive Analytics, Data Accuracy, Model Performance, EDA, TreeBased Models, Linear Models, Time Series Forecasting, HR Analytics, Recruiting Analytics, Talent Analytics","data science, aiml, data analysis, data visualization, python, r, sql, statistics, mathematics, hypothesis testing, probability, regression, machine learning, supervised learning, unsupervised learning, predictive analytics, data accuracy, model performance, eda, treebased models, linear models, time series forecasting, hr analytics, recruiting analytics, talent analytics","aiml, data accuracy, data science, dataanalytics, eda, hr analytics, hypothesis testing, linear models, machine learning, mathematics, model performance, predictive analytics, probability, python, r, recruiting analytics, regression, sql, statistics, supervised learning, talent analytics, time series forecasting, treebased models, unsupervised learning, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Overland Park, KS",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773090527,2023-12-17,Kansas,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Pandas, Big Data, Spark, Python, ML, Kubernetes, Docker, AWS, SQL, Airflow, GCP, Java, Bash, Git, Statistical Analysis, Visualisation, Data Mining, Cleaning, Modeling, Automation, ETL, NoSQL, Kafka, Storm, SparkStreaming","pandas, big data, spark, python, ml, kubernetes, docker, aws, sql, airflow, gcp, java, bash, git, statistical analysis, visualisation, data mining, cleaning, modeling, automation, etl, nosql, kafka, storm, sparkstreaming","airflow, automation, aws, bash, big data, cleaning, data mining, docker, etl, gcp, git, java, kafka, kubernetes, ml, modeling, nosql, pandas, python, spark, sparkstreaming, sql, statistical analysis, storm, visualisation"
"Manager, Automation & Commercial Data",Haleon,"New Jersey, United States",https://www.linkedin.com/jobs/view/manager-automation-commercial-data-at-haleon-3785523363,2023-12-17,Lakehurst,United States,Mid senior,Onsite,"Hello. We’re Haleon. A new world-leading consumer health company. Shaped by all who join us. Together, we’re improving everyday health for billions of people. By growing and innovating our global portfolio of category-leading brands – including Sensodyne, Advil, Voltaren, Theraflu, and Centrum – through a unique combination of deep human understanding and trusted science. What’s more, we’re achieving it in a company that we’re in control of. In an environment that we’re co-creating. And a culture that’s uniquely ours. Care to join us. It isn’t a question.
Are you interested in supporting an organization that makes things easier? Do you have a passion for business process simplification and data? As the Commercial Business Process Engineer, you will be responsible for analyzing, designing, and implementing business processes to improve overall business efficiency and productivity. This role requires a deep understanding of commercial business operations, process engineering principles, and analytical problem-solving skills. You must have strong communication skills and attention to detail as you collaborate cross functionally.
This role will provide YOU the opportunity to lead key activities to progress YOUR career. These responsibilities include some of the following:
Analyze existing business processes to identify areas for improvement and efficiency gains to help day to day operation of the commercial organization.
Design and implement new business processes, leveraging engineering principles to optimize efficiency and effectiveness.
Collaborate with cross functionals (commercial) to understand and document business critical operations with an eye toward streamlining and simplifying, ensuring understanding and alignment on any process changes.
Use data-driven methods to measure the impact of process changes, validate that business objectives are met, and identify areas for further improvement.
Manage projects related to process redesign and implementation, ensuring they are completed on time and within budget.
Provide training and support to employees impacted by process changes.
Understand and manage data related to business processes, ensuring data integrity, accuracy, and compliance with Haleon's data policies.
Use data analysis tools to interpret complex data sets and provide insights to stakeholders.
Continually monitor and evaluate industry trends and advancements in process engineering and data management to keep Haleon at the forefront of business process innovation.
Collaborate with the Senior Manager of Automation Transformation to develop and execute automation strategies.
This is a hybrid on-site role based in Warren, NJ
Why you?
Basic Qualifications:
We are looking for professionals with these required skills to achieve our goals:
Bachelor's Degree in a relevant field (e.g. Business Administration, Engineering, etc.)
Proven experience in business process mapping, data analysis tools and data management systems.
Strong knowledge of industry best practices.
Proven experience leading projects from start to completion.
Experience with project management and cross-functional collaboration
Creative problem-solving skills, attention to detail, and a commitment to producing work of the highest quality.
Preferred Qualifications:
If you have the following characteristics, it would be a plus:
3-5 years' experience in an OTC/Packaged Goods business environment
Microsoft Certified: Power Platform Fundamentals
RPA, AI, ML, Business Process/Task Mining and related Intelligent Automation
Agile / Process certification (SCRUM, S.A.F.E., PMP, Six Sigma, ABPMP, Prosci and/or other)
Proficiency in SAP/CERPS data setup and supported commercial processes.
Operating knowledge of hardware platforms.
Knowledge of big data technologies such as Azure Data Lake is highly preferred.
Haleon offers a robust Total Reward package that consists of competitive pay and a comprehensive benefits program. This includes a generous 401(k) plan, tuition reimbursement and time off programs including 6 months paid parental leave. On day one, you are eligible for benefits, including our healthcare programs where the company pays for the majority of your medical coverage for you and your family. We also offer the opportunity to receive a discretionary
bonus based on the achievement of key business performance and other incentive/recognition programs as part of the offering.
Salary range for this role is $122,400 -$144,000 USD .
This is a hybrid on-site role based in Warren, NJ
Care to join us. Find out what life at Haleon is really like www.haleon.com/careers/
At Haleon we embrace our diverse workforce by creating an inclusive environment that celebrates our unique perspectives, generates curiosity to create unmatched understanding of each other, and promotes fair and equitable outcomes for everyone. We're striving to create a climate where we celebrate our diversity in all forms by treating each other with respect, listening to different viewpoints, supporting our communities, and creating a workplace where your authentic self belongs and thrives. We believe in an agile working culture for all our roles. If flexibility is important to you, we encourage you to explore with our hiring team what the opportunities are.
As you apply, we will ask you to share some personal information, which is entirely voluntary. We want to have an opportunity to consider a diverse pool of qualified candidates and this information will assist us in meeting that objective and in understanding how well we are doing against our inclusion and diversity ambitions. We would really appreciate it if you could take a few moments to complete it. Rest assured, Hiring Managers do not have access to this information and we will treat your information confidentially.
Haleon is an Equal Opportunity Employer and, in the US, we adhere to Affirmative Action principles. This ensures that all qualified applicants will receive equal consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, marital status, sexual orientation, gender identity/expression, age, disability, genetic information, military service, covered/protected veteran status or any other federal, state or local protected class.
Please note that if you are a US Licensed Healthcare Professional or Healthcare Professional as defined by the laws of the state issuing your license, Haleon may be required to capture and report expenses Haleon incurs, on your behalf, in the event you are afforded an interview for employment. This capture of applicable transfers of value is necessary to ensure Haleon’s compliance to all federal and state US Transparency requirements.
Show more
Show less","Business process analysis, Business process design, Business process engineering, Data analysis, Data management, Process improvement, Process simplification, Project management, Crossfunctional collaboration, Agile/Process certification, SAP/CERPS, Big data technologies, Microsoft Power Platform, RPA, AI, ML, Business Process/Task Mining, Data Lake, Six Sigma, ABPMP, Prosci, Scrum","business process analysis, business process design, business process engineering, data analysis, data management, process improvement, process simplification, project management, crossfunctional collaboration, agileprocess certification, sapcerps, big data technologies, microsoft power platform, rpa, ai, ml, business processtask mining, data lake, six sigma, abpmp, prosci, scrum","abpmp, agileprocess certification, ai, big data technologies, business process analysis, business process design, business process engineering, business processtask mining, crossfunctional collaboration, data lake, data management, dataanalytics, microsoft power platform, ml, process improvement, process simplification, project management, prosci, rpa, sapcerps, scrum, six sigma"
Customer Service Representative/Data Analyst/Data Entry Clerk/Full Time,Dooleyboyer,"Port Macquarie, New South Wales, Australia",https://au.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-dooleyboyer-3742323341,2023-12-17,Kempsey, Australia,Mid senior,Onsite,"Customer Service Representative/Data Analyst/Data Entry Clerk/Full Time /Office will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.If you are interested in this position, please send your resume, contact information and salary requirements to : hrteam@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data analysis, Data interpretation, Data visualization, Statistical modeling, Hypothesis testing, A/B testing, Data management, ETL (extract transform load) processes, SQL, R, Python, Tableau, Power BI, Crossfunctional collaboration","data analysis, data interpretation, data visualization, statistical modeling, hypothesis testing, ab testing, data management, etl extract transform load processes, sql, r, python, tableau, power bi, crossfunctional collaboration","ab testing, crossfunctional collaboration, data interpretation, data management, dataanalytics, etl extract transform load processes, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Data Engineer - Hybrid,Net2Source Inc.,"Sunrise, FL",https://www.linkedin.com/jobs/view/data-engineer-hybrid-at-net2source-inc-3705708135,2023-12-17,Boca Raton,United States,Mid senior,Onsite,"Net
2
Source Inc. is an award-winning total workforce solutions company recognized by Staffing Industry Analysts for our accelerated growth of 300% in the last 3 years with over 5500+ employees globally, with over 30+ locations in the US and global operations in 32 countries. We believe in providing staffing solutions to address the current talent gap  Right Talent  Right Time  Right Place  Right Price and acting as a Career Coach to our consultants.
Role: Data Engineer
Location: Sunrise, Florida - Hybrid
Duration: Long Term Contract
Job Description:
Hands-on experience with GCP, google composer, bigQuery and bigTable
3 to 7 years of experience within Data Engineering/ Data Warehousing using Big Data
Hands-on experience on writing and understanding complex SQL(Hive/PySpark-dataframes), optimizing joins while processing huge amount of data
Hands-on experience with programming using Python/Scala
Understanding of Cloud Native Principles and architectures and Experience in creating platform level cloud native system architecture with low latency, high throughput, and high availability.
Have experience in designing and building Cloud native applications. Experience in cloud platforms like Docker, Kubernetes, OpenShift are a plus.
Expert on Hadoop and Spark Architecture and its working principle
Experience in UNIX shell scripting
Experience with NoSQL i.e., HBase, Couchbase, MongoDB is good to have
Teamwork & ability to multi-task
Excellent communication skills
Why work with us -
At Net2Source, we believe everyone has an opportunity to lead. We see the importance of your perspective and your ability to create value. We want you to fit inwith an inclusive culture, focus on work-life fit and well-being, and a supportive, connected environment; but we also want you to stand outwith opportunities to have a strategic impact, innovate, and take necessary steps to make your mark. We help clients with new skilling, talent strategy, leadership development, employee experience, transformational change management and beyond.
Equal Employment Opportunity Statement:
Net2Source is an Equal Opportunity Employer. We believe that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion or sexual orientation. Our rich diversity makes us more innovative, more competitive, and more creative, which helps us better serve our clients and our communities. All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.
Awards and Accolades:
America's Most Honored Businesses (Top 10%)
Awarded by USPAAC for Fastest Growing Business in the US
12th Fastest Growing Staffing Company in USA by Staffing industry Analysts in the US (2020, 2019, 2020)
Fastest 50 by NJ Biz (2020, 2019, 2020)
INC 5000 Fastest growing for 8 consecutive years in a row (only 1.26% companies make it to this list)
Top 100 by Dallas Business Journal (2020 and 2019)
Proven Supplier of the Year by Workforce Logiq (2020 and 2019)
2019 Spirit of Alliance Award by Agile1
2018 Best of the Best Platinum Award by Agile1
2018 TechServe Alliance Excellence Awards Winner
2017 Best of the Best Gold Award by Agile1(Act1 Group)
Show more
Show less","Google Cloud Platform (GCP), Google Composer, BigQuery, BigTable, Apache Hive, PySpark, Apache Spark, SQL, Python, Hadoop, Docker, Kubernetes, OpenShift, Scala, HBase, Couchbase, MongoDB, UNIX, Shell scripting, NoSQL","google cloud platform gcp, google composer, bigquery, bigtable, apache hive, pyspark, apache spark, sql, python, hadoop, docker, kubernetes, openshift, scala, hbase, couchbase, mongodb, unix, shell scripting, nosql","apache hive, apache spark, bigquery, bigtable, couchbase, docker, google cloud platform gcp, google composer, hadoop, hbase, kubernetes, mongodb, nosql, openshift, python, scala, shell scripting, spark, sql, unix"
Senior Data Analyst,CGI,"Fort Lauderdale, FL",https://www.linkedin.com/jobs/view/senior-data-analyst-at-cgi-3750072763,2023-12-17,Boca Raton,United States,Mid senior,Onsite,"Position Description
We're a passionate team of tech enthusiasts and professionals on a mission to reshape the future. We've got everything from cutting-edge projects that challenge your skills to a supportive and inclusive work environment that fuels your growth. Plus, we offer unparalleled opportunities for professional development, empowering you to stay at the forefront of the ever-evolving tech landscape.
We are on the lookout for a seasoned Senior Data Analyst to join our team. If you're a passionate self-starter ready to make a real impact, CGI is the perfect fit for you. Don't miss this chance to elevate your career – apply now!
This position is located in Fort Lauderdale, FL. Selected candidate must be onsite 3 days per week.
Your future duties and responsibilities
Responsible for eliciting, understanding, interpreting and representing business requirements and act as the conduit between the customer and technical teams to ensure requirements are understood.
Provide subject matter expertise on the use of data as well as educate teams on business model, metadata and standards.
Responsible for understanding source systems and its data models.
Develop source to target mappings for data lineage.
Document source architecture to include data flows.
Responsible for analyzing data to validate business domains and requirements.
Responsible for data profiling and ensuring data quality requirements are accurate and complete.
Act in an advisory capacity in data model reviews, architecture approach and solution design to ensure high quality deliverables.
Responsible for partnering with management and business units on innovative ways to successfully utilize data and related tools to advance business objectives.
Works with governance council to establish data governance standards and guidelines.
Assist with business data lake testing / experimentation
Assist with coordinating data dictionary completions
Mentor Project DA resources
Required Qualifications To Be Successful In This Role
Validated experience on projects involving data analysis and profiling, data integration, data cleansing, data mapping, and data conversion activities
Proficient in data management concepts, data lifecycle and methodologies
Knowledge and experience with an ERP system highly preferred. Experience using Sales Force a plus.
Experience and hands-on involvement in operational system modernization and transformation. Knowledge of Microsoft Dynamics 360 a big plus.
Excellent analytical, problem-solving, and decision-making skills, demonstrating both logic and creativity
Excellent written and verbal communication, as well as, strong organizational and presentation skills
Highly motivated and a strong desire to understand the organization, its industry, and its strategies
Resourceful at applying business and technical skills to drive innovation and performance improvement
Demonstrated ability to balance multiple contending priorities in a dynamic environment
Demonstrated facilitations and meeting management skills
Proven ability to work with business representatives to understand and detail their business and functional requirements and document it in an organized ‘functional design’ format
Excellent interpersonal skills with the ability to build relationships within and between individuals and multi-functional teams
Must be a self-starter and show strong initiative
Must exhibit strong customer service orientation
Ability to influence and motivate individuals and teams to drive mutually beneficial outcomes
Solid grasp of agile methodology framework is a plus
4+ years experience working as a data analyst using SQL, BI and other data analysis tools
3+ years hands-on experience working with SQL and a solid understanding of different data structures (flat files, relational, etc.)
Understands data modeling concepts and techniques
Experience working BI/Analytics tools such as Power BI and Tableau is a plus
Experience with MICROSOFT DYNAMICS 360 A BIG PLUS
Education
Bachelor’s degree or equivalent plus 5+ years of related professional experience
Degree in Technology and/or Finance related area preferred
`CGI is required by law in some jurisdictions to include a reasonable estimate of the compensation range for this role. The determination of this range includes various factors not limited to skill set, level, experience, relevant training, and licensure and certifications. To support the ability to reward for merit-based performance, CGI typically does not hire individuals at or near the top of the range for their role. Compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for this role in the U.S. is $105,500- $150,200.
At CGI we call our professionals “members” to reinforce that all who join our team are, as owners, empowered to participate in the challenges and rewards that come from building a world-class company. CGI’s benefits include:
Competitive base salaries
Eligibility to participate in an attractive Share Purchase Plan (SPP) in which the company matches dollar-for-dollar contributions made by eligible employees, up to a maximum, for their job category
401(k) Plan and Profit Participation for eligible members
Generous holidays, vacation, and sick leave plans
Comprehensive insurance plans that include, among other benefits, medical, dental, vision, life, disability, out-of-county emergency coverage in all countries of employment;
Back-up child care, Pet insurance, a Member Assistance Program, a 529 college savings program, a personal financial management tool, lifestyle management programs and more
#DICE
Insights you can act on
While technology is at the heart of our clients’ digital transformation, we understand that people are at the heart of business success.
When you join CGI, you become a trusted advisor, collaborating with colleagues and clients to bring forward actionable insights that deliver meaningful and sustainable outcomes. We call our employees “members” because they are CGI shareholders and owners and owners who enjoy working and growing together to build a company we are proud of. This has been our Dream since 1976, and it has brought us to where we are today — one of the world’s largest independent providers of IT and business consulting services.
At CGI, we recognize the richness that diversity brings. We strive to create a work culture where all belong and collaborate with clients in building more inclusive communities. As an equal-opportunity employer, we want to empower all our members to succeed and grow. If you require an accommodation at any point during the recruitment process, please let us know. We will be happy to assist.
Ready to become part of our success story? Join CGI — where your ideas and actions make a difference.
Qualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, pregnancy, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status, political affiliation, genetic information, height, weight, or any other legally protected status or characteristics.
CGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com. You will need to reference the Position ID of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you.
Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a Position ID will not be returned
.
We make it easy to translate military experience and skills! Click here to be directed to our site that is dedicated to veterans and transitioning service members.
All CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held. CGI will consider for employment qualified applicants with arrests and conviction records in accordance with all local regulations and ordinances.
CGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGI’s legal duty to furnish information.
Show more
Show less","Data Management, Data Modeling, Business Intelligence, Data Profiling, Data Cleansing, Data Mapping, Data Integration, Data Conversion, Data Analysis, Data Profiling, Data Lineage, Data Governance, SQL, Power BI, Tableau, Microsoft Dynamics 360, Sales Force, Agile Methodology, BI Analytics, ERP, Share Purchase Plan (SPP), 401(k) Plan, Profit Participation, Disability Insurance, Life Insurance, Medical Insurance, Dental Insurance, Vision Insurance, Pet Insurance, Child Care, 529 College Savings Program","data management, data modeling, business intelligence, data profiling, data cleansing, data mapping, data integration, data conversion, data analysis, data profiling, data lineage, data governance, sql, power bi, tableau, microsoft dynamics 360, sales force, agile methodology, bi analytics, erp, share purchase plan spp, 401k plan, profit participation, disability insurance, life insurance, medical insurance, dental insurance, vision insurance, pet insurance, child care, 529 college savings program","401k plan, 529 college savings program, agile methodology, bi analytics, business intelligence, child care, data conversion, data governance, data integration, data lineage, data management, data mapping, data profiling, dataanalytics, datacleaning, datamodeling, dental insurance, disability insurance, erp, life insurance, medical insurance, microsoft dynamics 360, pet insurance, powerbi, profit participation, sales force, share purchase plan spp, sql, tableau, vision insurance"
Sr Data Analyst,Protingent,"Fort Lauderdale, FL",https://www.linkedin.com/jobs/view/sr-data-analyst-at-protingent-3765370922,2023-12-17,Boca Raton,United States,Mid senior,Onsite,"Job Description
Position Title: Sr Direct Channel Data Analyst
Position Description:
Protingent has an exciting contract opportunity for a Sr Direct Channel Data Analyst for our client located in Redmond, WA. This role will work 100% remotely and must work PST hours.
Job Description:
This Sr. Direct Channel Data Analyst will work with our client's Supply Chain team, internal IT/SCE, Finance, F&L, and other functional areas as needed to successfully deliver the portfolio.
Job Responsibilities:
Support the foundational need to have reporting integrity, automation insight based on data, collaboration, and leveraging of opportunities across regions and potentially functions, to further the impact and speed to decision of the Direct Channel.
Leverage systems, business, and process knowledge to support the creation of reporting and automation to allow for more aligned and efficient operation of the Direct Channel.
Work with various stakeholders to define, plan, and coordinate activities in pursuit of reporting excellence in all aspects – accuracy, timeliness, complete business capture, flexibility, automation and productivity enhancements.
Serve as a regional point of contact to support company-wide initiatives to improve supply chain reporting that will further be leveraged across the globe in the Direct Channel.
Design high impact reports for supply chain teams and leadership, enabling data-driven decisions to support agility and efficiency in supply chain management.
Conduct data analyses, architect metrics, synthesize information, solve problems, and influence business decision-making by presenting data-driven insights to optimize business performance.
Support ad-hoc requests for data and analysis.
Drive a global level view and alignment of reporting to promote cross-region collaboration, operational alignment (as possible), and execution that align to the Direct Channel priorities.
Job Qualifications:
10+ years related experience in data analysis
10+ years experience with SAP
Proficient experience with Power BI
5+ years retail/supply chain experience
Understanding and application of key retail and inventory management concepts
Exceptional skills managing large amounts of data
Exceptional skills with SAP, Microsoft suite and tools, IBP, PBI.
Understand the business, metrics, challenges, and help us develop data/reporting/automation to improve.
Heavy balance on data analytics, creation of reporting, cleansing, etc.
Preferred Qualifications:
Experience in retail, preferably with consumer electronics, to give insight and understanding into the root of reporting/automation requirements.
Job Details:
Job Type - Contract hire
Location: Remote - Must work PST hours
Pay Range: $49.00-60.00/hr. DOE
An offer of employment is contingent on successfully passing a background check, and applicants who do not successfully pass a background check will not be considered for employment.
Benefits Package:
Protingent offers competitive salaries, insurance plan options (HDHP plan or POS plan), education/certification reimbursement, pre-tax commuter benefits, Paid Time Off (PTO) and an administered 401k plan.
About Protingent:
Protingent is a niche provider of top Engineering and IT talent to Software, Electronics, Medical Device, Telecom, and Aerospace companies nationwide. Protingent exists to make a positive impact and contribution to the lives of others as well as our community by providing relevant, rewarding, and exciting work opportunities for our candidates.
Show more
Show less","Data Analysis, SAP, Power BI, IBP, Microsoft Suite, Retail/Supply Chain Management, Data Cleansing, Reporting","data analysis, sap, power bi, ibp, microsoft suite, retailsupply chain management, data cleansing, reporting","dataanalytics, datacleaning, ibp, microsoft suite, powerbi, reporting, retailsupply chain management, sap"
Oracle Database Developer,Orion Innovation,"Coral Springs, FL",https://www.linkedin.com/jobs/view/oracle-database-developer-at-orion-innovation-3782090615,2023-12-17,Boca Raton,United States,Mid senior,Onsite,"Orion Innovation is a premier, award-winning, global business and technology services firm. Orion delivers game-changing business transformation and product development rooted in digital strategy, experience design, and engineering, with a unique combination of agility, scale, and maturity. We work with a wide range of clients across many industries including financial services, professional services, telecommunications and media, consumer products, automotive, industrial automation, professional sports and entertainment, life sciences, ecommerce, and education.
Job Title: Oracle Database Developer
Hybrid Onsite
Job Summary
We are looking for an experienced Oracle Database Developer to help us reverse engineer a legacy fintech product developed with Oracle as a backend database. The ideal candidate will have a strong understanding of Oracle databases, including their structure, data types, and constraints. They will also be familiar with reverse engineering techniques and tools.
Responsibilities
Reverse engineer the existing Oracle database schema, Functions, Stored Procedures and data to create a comprehensive understanding of the database structure and data flow diagrams.
Identify all dependencies between tables, views, and other database objects and document the database schema and dependencies in a clear and concise manner.
Identify and document all the challenges in the current legacy platform and come up with potential solution for those challenges
Help develop a business case and for Modernize the platform
Work closely with the Business Analyst, Java / Angular UI Developer, Java Backend Developer, Product Owner to reverse engineer the product as a whole, understand the the User journeys and help create the require Business and Technical documentation for the product
Qualifications
7+ years of experience with Oracle databases and PL/SQL
Strong understanding of Oracle database schema, data types, and constraints
Experience with reverse engineering techniques and tools
Excellent analytical and problem-solving skills
Ability to work independently and as part of a team
Experience with fintech products
Orion is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, citizenship status, disability status, genetic information, protected veteran status, or any other characteristic protected by law.
Candidate Privacy Policy
Orion Systems Integrators, LLC And Its Subsidiaries And Its Affiliates (collectively, “Orion,” “we” Or “us”) Are Committed To Protecting Your Privacy. This Candidate Privacy Policy (orioninc.com) (“Notice”) Explains
What information we collect during our application and recruitment process and why we collect it;
How we handle that information; and
How to access and update that information.
Your use of Orion services is governed by any applicable terms in this notice and our general Privacy Policy.
Show more
Show less","Oracle Database, PL/SQL, Data Types, Constraints, Reverse Engineering, Dependencies, Fintech, Angular, Java, Business Analysis, User Journeys, Technical Documentation","oracle database, plsql, data types, constraints, reverse engineering, dependencies, fintech, angular, java, business analysis, user journeys, technical documentation","angular, business analysis, constraints, data types, dependencies, fintech, java, oracle database, plsql, reverse engineering, technical documentation, user journeys"
"Associate Data Analyst, Revenue Management",Priceline,"Hollywood, FL",https://www.linkedin.com/jobs/view/associate-data-analyst-revenue-management-at-priceline-3774914675,2023-12-17,Boca Raton,United States,Mid senior,Hybrid,"This role is eligible for our hybrid work model: Two days in-office
Associate Data Analyst - Revenue and Inventory Management
We’re a data-driven organization, which makes our Analytics and Data Science teams the brains of our operation. On the cutting edge of customer and business analytics, they make sure all our decisions and innovations are based on the latest insights.
Why This Job’s a Big Deal
As an Associate Business Analyst at Priceline Partner Solution, you will closely examine supply quality and booking performance. Through insightful data analysis you will uncover opportunities that drive performance improvements. You will partner with stakeholders to measure performance and work in close collaboration with account management and accommodations teams to deliver innovative solutions. The ideal candidate for this role has prior experience in an e-commerce or travel environment, strong analytical skills with an excellent track record of execution.
In This Role You Will Get To
Hold ownership for monitoring business metrics, explain trends and drivers
Be at the forefront of action identifying and solving booking issues
Coordinate and connect with internal and external teams to alert and resolve issues
Prepare and analyze pricing and distribution data to support maximize yielding
Become a subject matter expert on rate verification and distribution flow process
Provide concurrent analytical support for initiatives across variety of teams such as Account Management, Supply, Finance and Business Intelligence
Work with other analysts with other business units and teams
Who You Are
2+ years of analysis/consulting experience, travel or ecommerce/hospitality industry preferred
Able to interpret data and trends and to translate analysis in a compelling way to improve performance
Demonstrate ability to extract value from data and reducing this data to intuitive models, charts and bullet points for communication
Excel and Tableau required. Preferably with SQL and BigQuery
A self-starter who is inquisitive and able to dissect data to uncover the root cause of an issue
Ability to work and thrive in a multi-tasked, fast-paced environment
Exhibit the highest level of professionalism, integrity, and ethical values
Bachelor’s Degree in Statistics / Math / Business / Hotel education background
There are a variety of factors that go into determining a salary range, including but not limited to external market benchmark data, geographic location, and years of experience sought/required. In addition to a competitive base salary, certain roles may be eligible for an annual bonus and/or equity grant.
The salary range for this position is $50000-$60000
Who We Are
WE ARE PRICELINE.
Our success as one of the biggest players in online travel is all thanks to our incredible, dedicated team of talented employees. Priceliners are focused on being the best travel deal makers in the world, motivated by our passion to help everyone experience the moments that matter most in their lives. Whether it’s a dream vacation, your cousin’s graduation, or your best friend’s wedding - we make travel affordable and accessible to our customers.
Our culture is unique and inspiring (that’s what our employees tell us). We’re a grown-up, startup. We deliver the excitement of a new venture, without the struggles and chaos that can come with a business that hasn’t stabilized.
We’re on the cutting edge of innovative technologies. We keep the customer at the center of all that we do. Our ability to meet their needs relies on the strength of a workforce as diverse as the customers we serve. We bring together employees from all walks of life and we are proud to provide the kind of inclusive environment that stimulates innovation, creativity and collaboration.
Priceline is part of the Booking Holdings, Inc. (Nasdaq: BKNG) family of companies, a highly profitable global online travel company with a market capitalization of over $80 billion. Our sister companies include Booking.com, BookingGo, Agoda, Kayak and OpenTable.
If you want to be part of something truly special, check us out!
Flexible work at Priceline
Priceline is following a hybrid working model, which includes two days onsite as determined by you and your manager (ideally selecting among Tuesday, Wednesday, or Thursday). On the remaining days, you can choose to be remote or in the office.
Diversity and Inclusion are a Big Deal!
To be the best travel dealmakers in the world, it’s important we have a workforce that reflects the diverse customers and communities we serve. We are committed to cultivating a culture where all employees have the freedom to bring their individual perspectives, life experiences, and passion to work.
Priceline is a proud equal opportunity employer. We embrace and celebrate the unique lenses through which our employees see the world. We’d love you to join us and add to our rich mix!
Applying for this position
We're excited that you are interested in a career with us. For all
current employees
, please use the internal portal to find jobs and apply.
External candidates are required to have an account before applying. When you click Apply, returning candidates can log in, or new candidates can quickly create an account to save/view applications.
Show more
Show less","Data Analysis, Data Science, Customer Analytics, Business Analytics, Tableau, SQL, BigQuery, Excel, Statistics, Mathematics, Business, Ecommerce, Hospitality, Travel","data analysis, data science, customer analytics, business analytics, tableau, sql, bigquery, excel, statistics, mathematics, business, ecommerce, hospitality, travel","bigquery, business, business analytics, customer analytics, data science, dataanalytics, ecommerce, excel, hospitality, mathematics, sql, statistics, tableau, travel"
Data Entry/Economic Data Analyst,Ttgtalentsolutions,"Worcester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-entry-economic-data-analyst-at-ttgtalentsolutions-3733276309,2023-12-17,Cheltenham, United Kingdom,Mid senior,Remote,"The Data Entry Clerk/Human will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
Powered by Webbtree
Show more
Show less","Data Analysis, Data Interpretation, Statistical Techniques, Data Mining, Data Visualization, Tableau, Power BI, SQL, R, Python, A/B Testing, Data Quality Control, Data Management, ETL Processes, Hypothesis Testing, Statistical Modeling, DataDriven Decision Making","data analysis, data interpretation, statistical techniques, data mining, data visualization, tableau, power bi, sql, r, python, ab testing, data quality control, data management, etl processes, hypothesis testing, statistical modeling, datadriven decision making","ab testing, data interpretation, data management, data mining, data quality control, dataanalytics, datadriven decision making, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Data Scientist,Brooksource,"Indianapolis, IN",https://www.linkedin.com/jobs/view/data-scientist-at-brooksource-3775446681,2023-12-17,Guilford,United States,Associate,Hybrid,"Associate Data Scientist
Indianapolis, IN, hybrid
6 month, reoccurring contract
Pay: $36.43/hr.
*not able to provide sponsorship
As the Associate Data Scientist, you will work with partners and outside stakeholders to gather and analyze data regarding a variety of issues and topics to develop data driven solutions to difficult business challenges.
Responsibilities:
Mines and analyzes data from organization’s databases to drive optimization and improvement of program development, evaluation, and business insights
Assesses the effectiveness and accuracy of new data sources and data quality
Standardize operational procedures, reviewing code, analyzing, and interpreting datasets, and working with Git for version control
Data analysis (Assist in analysis for evaluation, EDA)
Performs exploratory data analysis to understand the organization’s data
Interprets results from multiple sources using a variety of techniques, ranging from simple data aggregation via statistical analysis to complex data mining independently
Develops analytic and predictive models for business needs
Coordinates with different functional teams to implement models and monitor outcomes
Create meaningful visualizations that communicate information about business questions
Work with data engineers/architects to create data pipelines
Work with data scientists to deploy sustainable solutions
Develops compelling data visualizations to communicate findings to target audience(s)
Proficient in: SQL, R, Python
Qualifications:
Bachelor’s degree in Data Science, Statistics, Math, Computer science, Informatics, or related fields
1 – 1.5 years of educational, internship, or professional experience in statistical techniques and concepts (regression, properties of distributions, statistical tests, etc.)
Confidence in working with Python, and/or, R
Working knowledge using SQL to manipulate and draw insights from large data sets
Experience creating visualizations and storytelling understanding
Ability to research and document findings
Mission driven personality with a desire to learn
Brooksource (Eight Eleven Group) provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.
Show more
Show less","SQL, R, Python, Data analysis, EDA, Statistical techniques, Regression, Statistical tests, Data visualization, Storytelling, Data engineering, Data architecture, Data pipelines, Data science, Machine learning, Git, Version control","sql, r, python, data analysis, eda, statistical techniques, regression, statistical tests, data visualization, storytelling, data engineering, data architecture, data pipelines, data science, machine learning, git, version control","data architecture, data engineering, data science, dataanalytics, datapipeline, eda, git, machine learning, python, r, regression, sql, statistical techniques, statistical tests, storytelling, version control, visualization"
QC Data Analyst,United Consulting Hub,"Buck, PA",https://www.linkedin.com/jobs/view/qc-data-analyst-at-united-consulting-hub-3785360761,2023-12-17,Pennsylvania,United States,Associate,Onsite,"Role – QC Data Analysts / Reviewer
Location –
Bucks County
, Pennsylvania - (on site Monday – Friday)
Duration – long term contract
Consultant’s Day-to-day Responsibilities
Bench testing of pharma products using Empower HPLC and Malvern Particle Size Analyzer
Performing wet chemistry testing including viscosity, pH and dissolution.
Documentation of results into LabVantage LIMS.
Developing trend reports and presenting to QC and Manufacturing stakeholders
Writing and reviewing analytical testing SOP’s for individual products
Writing deviation investigations/reports in partnership with QC leadership
Required skills:
Bachelor’s degree in Chemistry, Biochemistry or related field.
3 years previous experience reviewing analytical testing data in a non-academic setting.
3 years prior bench testing experience in a non-academic setting
Previous experience performing product analysis using HPLC, GC, Dissolution Testing and Particle Size Analysis
Ability to analyze and interpret a chromatograms
Prior experience tracking and trending data in MS excel or similar product
Show more
Show less","Chemistry, Biochemistry, HPLC (HighPerformance Liquid Chromatography), Malvern Particle Size Analyzer, Wet chemistry, Viscosity, pH measurement, Dissolution testing, LabVantage LIMS, Trend reports, Analytical testing SOPs, Deviation investigations/reports, Microsoft Excel","chemistry, biochemistry, hplc highperformance liquid chromatography, malvern particle size analyzer, wet chemistry, viscosity, ph measurement, dissolution testing, labvantage lims, trend reports, analytical testing sops, deviation investigationsreports, microsoft excel","analytical testing sops, biochemistry, chemistry, deviation investigationsreports, dissolution testing, hplc highperformance liquid chromatography, labvantage lims, malvern particle size analyzer, microsoft excel, ph measurement, trend reports, viscosity, wet chemistry"
"Power BI Developer, Data Analytics",MSA - The Safety Company,"Cranberry, PA",https://www.linkedin.com/jobs/view/power-bi-developer-data-analytics-at-msa-the-safety-company-3786548397,2023-12-17,Pennsylvania,United States,Associate,Hybrid,"Overview
Are you someone who is passionate, motivated, and driven to make a difference? If so, MSA Safety is the perfect fit for your career.
At MSA, SAFETY is who we are AND it is what we do. We are a purpose-driven company committed to deploying innovation and technology to deliver on our Mission to help protect people and assets all around the world. We continue to be relentless in our pursuit of solving our customers greatest problems so they can go home safe each and every day.
Are you in? Read on for more details about this particular role.
Responsibilities
As a Power BI Developer on our Data Analytics Team, you will play an important role in MSA’s efforts to maximize the value of our data. You will design, develop, and implement visual analytics solutions for business leaders and business teams to drive understanding and conclusion-making from data. You will also maintain strong relationships with business partners to help shape requirements and provide both analytical and technical support.
For this position, we put a high focus on interpersonal skills and attitude. You are passionate about people, customer value, data, and storytelling. You are intellectually curious with a strong desire to drive continuous improvement. You have an ability to create strong networks, energize people, mobilize teams, and act as a positive influencer of change.
Responsibilities
Engage, collaborate, and partner with business leaders and business teams to understand business workflows, the decision-making process, and their information needs.
Translate information needs into analytics and business intelligence solutions that support the decision-making process and provide new business insights.
Design, develop and deliver compelling data products using a variety of techniques including data storytelling and user-centric design.
Provide analytical and technical support, thought guidance, and best practices to deliver effective and sustainable data visualization and analytic solutions.
Teach and advocate the use of data visualization, data storytelling, analytics and business intelligence tools across all levels of the organization.
Be imaginative, enthusiastic and show high creativity and initiative with a distinct bias for action and an ability to express your own point of view with confidence and humility.
Qualifications
Skills, Knowledge & Abilities Required:
Solid experience developing analytical solutions with Microsoft Power BI.
Excellent analytical and problem-solving skills, with the ability to translate complex business requirements.
Strong communication skills, both written and verbal, with the ability to present complex data concepts to non-technical stakeholders.
Proven ability to work independently and collaborate effectively within cross-functional teams.
Ability to prioritize and plan complex tasks in a rapidly changing environment.
Continuous learning mindset, staying updated with the latest trends, techniques, and tools in data analytics and business intelligence.
Skills, Knowledge & Abilities Preferred:
Experience with Power Apps, Power Automate, and Power Virtual Agents.
Experience in Microsoft Fabric technologies.
Familiarity with AWS Technologies.
Experience working with data sourced from SAP ECC, Salesforce, or Oracle EPM.
Expertise in SQL and proficiency in programming languages such as Python or R.
Familiarity with data engineering, data science, machine learning and artificial intelligence.
Education and Experience Required:
Bachelor’s or Master’s degree in Information Science, Computer Science, Data Science or related discipline
This position is available at two different career levels based upon experience, education:
Level one: 2 years' experience in IT or related industry.
Level two: 3 years' experience in IT or related industry.
#Hybrid
Show more
Show less","Microsoft Power BI, Analytical, ProblemSolving, Communication, Data Storytelling, Power Apps, Power Automate, Power Virtual Agents, Microsoft Fabric, AWS, SQL, Programming Languages (Python R), Data Engineering, Data Science, Machine Learning, Artificial Intelligence, SAP ECC, Salesforce, Oracle EPM","microsoft power bi, analytical, problemsolving, communication, data storytelling, power apps, power automate, power virtual agents, microsoft fabric, aws, sql, programming languages python r, data engineering, data science, machine learning, artificial intelligence, sap ecc, salesforce, oracle epm","analytical, artificial intelligence, aws, communication, data engineering, data science, data storytelling, machine learning, microsoft fabric, microsoft power bi, oracle epm, power apps, power automate, power virtual agents, problemsolving, programming languages python r, salesforce, sap ecc, sql"
"Director, Data Scientist - Biopharma",Pfizer,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/director-data-scientist-biopharma-at-pfizer-3729743563,2023-12-17,Pennsylvania,United States,Associate,Hybrid,"ROLE SUMMARY:
The Commercial Analytics team at Pfizer is looking for a Director, Data Science who is passionate about crafting and implementing predictive modeling and statistical analysis to build end-to-end solutions and insights that have a direct impact on patient's lives and the future of Pfizer as a data-driven organization. You will be a thought partner to the business, understand strategic goals, and then use your skills and subject matter expertise to surface impactful insights that drive business decisions and patient benefits. With colleagues across the globe, our rigorous analytical expertise is relied upon as the compass and decision support for the enterprise. Our dynamic, exciting team of subject-matter experts comes from diverse backgrounds and experiences, including market research, data science, digital analytics, finance, and consulting. Our culture is about getting things done iteratively and rapidly, with open feedback and debate along the way. We believe Data Science is a team sport, but we strive for independent decision-making and taking smart risks.
ROLE RESPONSIBILITIES:
This role is accountable for delivering data science driven support for an assigned brand and will partner with US Commercial, business, and digital teams, to develop and implement models, insights, and data products that drive brands’ strategic priorities. This is an individual contributor role.
Take deep dives in large-scale data to identify key insights that will shape future product/brand strategy for a specific therapeutic area.
Collaborate with cross-functional teams to identify new growth opportunities, develop data requirements, establish critical metrics, and evangelize data products.
Design, deploy, and evaluate experiments that help define opportunities for higher adoption, improved business performance, and better patient experience.
Conduct hypothesis-driven exploratory analyses, select appropriate ML algorithms, and build complex optimization engines to deliver impactful data solutions!
Research new technologies and methods across data science and data engineering to improve the technical capabilities of the team.
Communicate insights to senior management by distilling complex analysis and concepts into concise business-focused takeaways.
QUALIFICATIONS:
Bachelor’s degree with 10+ years of experience OR Masters Degree with 9+ years of experience OR PhD with 7+ years of experience
Degree preferably in engineering, economics, statistics, computer science, or related quantitative field.
Preferred experience in Applied Econometrics, Statistics, Data Mining, Machine Learning, Analytics, Mathematics, Operations Research, Industrial Engineering, or related field preferred.
Working knowledge of relational databases, including SQL, and large-scale distributed systems such as Hadoop and or working in Snowflake/Databricks
Ability to implement data science pipelines and applications in a general programming language such as Python, Scala, Java, or R.
Practical experience with and theoretical understanding of ML algorithms for classification, regression, clustering, and anomaly detection
Well versed with and have experience applying various statistical methodologies including Bayesian and non-parametric techniques, hypothesis testing, ANOVA, Regression, fixed and random effects etc. to measure the impact of experiments
Hands on experience working in big data environments such as Hadoop, Spark, and using Python / SQL or comparable languages for manipulating and analyzing complex clickstream and or unstructured data
Ability to extract significant business insights from data and identify the roots behind the patterns
Experience working with a data visualization tool/package, including Dash, Tableau, and Angular etc.
Communication skills for communicating complex quantitative analyses to senior business executives
Candidate demonstrates a breadth of diverse leadership experiences and capabilities including: the ability to influence and collaborate with peers, develop and coach others, oversee and guide the work of other colleagues to achieve meaningful outcomes and create business impact.
Other Job Details:
Last Date to Apply for Job: October 31, 2023
The annual base salary for this position ranges from $144,900.00 to $241,500.00.* In addition, this position is eligible for participation in Pfizer’s Global Performance Plan with a bonus target of 20.0% of the base salary and eligibility to participate in our share based long term incentive program. We offer comprehensive and generous benefits and programs to help our colleagues lead healthy lives and to support each of life’s moments. Benefits offered include a 401(k) plan with Pfizer Matching Contributions and an additional Pfizer Retirement Savings Contribution, paid vacation, holiday and personal days, paid caregiver/parental and medical leave, and health benefits to include medical, prescription drug, dental and vision coverage. Learn more at Pfizer Candidate Site – U.S. Benefits | (uscandidates.mypfizerbenefits.com). Pfizer compensation structures and benefit packages are aligned based on the location of hire. The United States salary range provided does not apply to Tampa, FL or any location outside of the United States.
The annual base salary for this position in Tampa, FL ranges from $130,400.00 to $217,300.00.
Relocation assistance may be available based on business needs and/or eligibility.
Sunshine Act
Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative.
EEO & Employment Eligibility
Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer.
Marketing and Market Research
Show more
Show less","Data Science, Predictive Modeling, Statistical Analysis, DataDriven Insights, Data Requirements, Critical Metrics, HypothesisDriven Analysis, Machine Learning Algorithms, Bayesian Techniques, Nonparametric Techniques, Regression, Data Visualization, Dash, Tableau, Angular, SQL, Hadoop, Spark, Python, R, Scala, Java, Leadership, Communication Skills, Applied Econometrics, Statistics, Data Mining, Machine Learning, Analytics, Mathematics, Operations Research, Industrial Engineering","data science, predictive modeling, statistical analysis, datadriven insights, data requirements, critical metrics, hypothesisdriven analysis, machine learning algorithms, bayesian techniques, nonparametric techniques, regression, data visualization, dash, tableau, angular, sql, hadoop, spark, python, r, scala, java, leadership, communication skills, applied econometrics, statistics, data mining, machine learning, analytics, mathematics, operations research, industrial engineering","analytics, angular, applied econometrics, bayesian techniques, communication skills, critical metrics, dash, data mining, data requirements, data science, datadriven insights, hadoop, hypothesisdriven analysis, industrial engineering, java, leadership, machine learning, machine learning algorithms, mathematics, nonparametric techniques, operations research, predictive modeling, python, r, regression, scala, spark, sql, statistical analysis, statistics, tableau, visualization"
"Senior Data Engineer - Bethlehem, PA (Onsite) - Full Time Employment/ Perm",Lorven Technologies Inc.,"Bethlehem, PA",https://www.linkedin.com/jobs/view/senior-data-engineer-bethlehem-pa-onsite-full-time-employment-perm-at-lorven-technologies-inc-3726682203,2023-12-17,Pennsylvania,United States,Mid senior,Onsite,"Our client is looking
Senior Data Engineer
for
a long-term
project in
Bethlehem, PA (Onsite).
below are the detailed requirements.
Role: Senior Data Engineer
Location: Bethlehem, PA (Onsite)
Duration: Long-term contract
Job Description
Bachelor's or Master's degree in Computer Science and/or equivalent combination of education with 10+ Years of work experience.
Design, develop, implement, and maintain code, information architecture, and conceptual models to support data processing, and flows through data lake.
AWS: S3 Data Lake, Athena, Redshift, EMR, Glue, ECS
Proficiency with Python in a data engineering context. Ex: Pandas, PySpark
Experience using VCS like Github, Gitlab
Excellent analytical and problem-solving skills.
A knack for independence and group work.
Capacity to successfully manage a pipeline of duties with minimal supervision.
Work with clients to get the requirements and build codes accordingly
Translate client business requirements into data requirements
Communicate the data requirements to other members of the team
Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed.
Reformulating existing frameworks to optimize their functioning.
Testing such structures to ensure that they are fit for use.
Preparing raw data for manipulation by data scientists or for visualization team.
Proven experience as a data engineer, software developer, or similar.
Show more
Show less","AWS, S3 Data Lake, Athena, Redshift, EMR, Glue, ECS, Python, Pandas, PySpark, VCS, Github, Gitlab, Data Engineering, Software Development, Data Manipulation, Data Visualization, Data Processing, Data Flows, Data Lake, Data Requirements, Data Analysis, Infrastructure, Big Data, Conceptualization, Framework Optimization, Testing","aws, s3 data lake, athena, redshift, emr, glue, ecs, python, pandas, pyspark, vcs, github, gitlab, data engineering, software development, data manipulation, data visualization, data processing, data flows, data lake, data requirements, data analysis, infrastructure, big data, conceptualization, framework optimization, testing","athena, aws, big data, conceptualization, data engineering, data flows, data lake, data manipulation, data processing, data requirements, dataanalytics, ecs, emr, framework optimization, github, gitlab, glue, infrastructure, pandas, python, redshift, s3 data lake, software development, spark, testing, vcs, visualization"
"Data Engineer, Data Platform",Grammarly,"Pennsylvania, United States",https://www.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3689964225,2023-12-17,Pennsylvania,United States,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Python, Scala, Java, Relational databases, APIs, Services, System design, AWS, Azure, GCE, Data lake, Admin sites, EAGER values, MOVE principles","python, scala, java, relational databases, apis, services, system design, aws, azure, gce, data lake, admin sites, eager values, move principles","admin sites, apis, aws, azure, data lake, eager values, gce, java, move principles, python, relational databases, scala, services, system design"
"Data Engineer, Data Analytics and Data Quality",PRI Technology,"Bethlehem, PA",https://www.linkedin.com/jobs/view/data-engineer-data-analytics-and-data-quality-at-pri-technology-3783811401,2023-12-17,Pennsylvania,United States,Mid senior,Hybrid,"Position Title: Data Engineer, Data Analytics and Data Quality
My name is Bill Stevens and I have a new six month plus Hybrid Scheduled Data Engineer, Data Analytics and Data Quality opportunity available for a major firm located in Bethlehem, Pennsylvania that could be of interest to you, please review my specification below and I am available at any time to speak with you so please feel free to call me. The work schedule will be a hybrid one, three days a week in the office and two days remote.
The firm will NOT entertain a remote candidate. The ideal candidate should also possess a green card or be of citizenship.
This position pays $75.00 per hour on a w-2 hourly basis or $85.00 per hour on a Corp basis. The Corp rate is for independent contractors only and not third-party firms.
Description:
The firm is seeking an experienced Data Engineer to be part of their Data and Analytics organization. You will be playing a key role in building and delivering best-in-class data and analytics solutions aimed at creating value and impact for the organization and our customers. As a member of the data engineering team, you will help developing and delivery of Data Products with quality backed by best-in-class engineering. You will collaborate with analytics partners, business partners and IT partners to enable the solutions.
The Qualified Candidate will:
Architect, build, and maintain scalable and reliable data pipelines including robust data quality as part of data pipeline which can be consumed by analytics and BI layer.
Design, develop and implement low-latency, high-availability, and performant data applications and recommend & implement innovative engineering solutions.
Design, develop, test and debug code in Python, SQL, PySpark, bash scripting as per the firms standards.
Design and implement data quality framework and apply it to critical data pipelines to make the data layer robust and trustworthy for downstream consumers.
Design and develop orchestration layer for data pipelines which are written in SQL, Python and PySpark.
Apply and provide guidance on software engineering techniques such as design patterns, code refactoring, framework design, code reusability, code versioning, performance optimization, and continuous build and Integration (CI/CD) to make the data analytics team robust and efficient.
Performing all job functions consistent with the firms policies and procedures, including those which govern handling PHI and PII.
Work closely with various IT and business teams to understand systems opportunities and constraints for maximally utilizing the firms Enterprise Data Infrastructure.
Develop relationships with business team members by being proactive, displaying an increasing understanding of the business processes and by recommending innovative solutions.
Communicate project output in terms of customer value, business objectives, and product opportunity.
The Qualified Candidate should possess:
5+ years of experience with Bachelors / master’s degree in computer science, Engineering, Applied mathematics or related field.
Extensive hands-on development experience in Python, SQL and Bash.
Extensive Experience in performance optimization of data pipelines.
Extensive hands-on experience working with cloud data warehouse and data lake platforms such as Databricks, Redshift or Snowflake.
Familiarity with building and deploying scalable data pipelines to develop and deploy Data Solutions using Python, SQL, PySpark.
Extensive experience in all stages of software development and expertise in applying software engineering best practices.
Experience in developing and implementing Data Quality framework either home grown or using any open-source frameworks such as Great Expectations, Soda, Deequ.
Extensive experience in developing end-to-end orchestration layer for data pipelines using frameworks such as Apache Airflow, Prefect, Databricks Workflow.
Familiar with RESTful Webservices (REST APIs) to be able to integrate with other services.
Familiarity with API Gateways such as APIGEE to secure webservice endpoints.
Familiarity with concurrency and parallelism.
Familiarity with Data pipelines and ML development cycle.
Experience in creating and configuring continuous integration/continuous deployment using pipelines to build and deploy applications in various environments and use best practices for DevOps to migrate code to Production environment.
Ability to investigate and repair application defects regardless of component: front-end, business logic, middleware, or database to improve code quality, consistency, delays and identify any bottlenecks or gaps in the implementation.
Ability to write unit tests in python using unit test library such as pytest.
Additional Qualifications (Nice to have and NOT required.):
Experience in using and implementing data observability platforms such as Monte Carlo Data, Metaplane, Soda, bigeye or any other similar products.
Expertise in debugging issues in Cloud environment by monitoring logs on the VM or use AWS features such as Cloudwatch.
Experience with DevOps tech stack such as Jenkins and Terraform.
Experience working with concept of Observability in software world and experience with tools such as Splunk, Zenoss, Datadog or similar.
Ability to learn and adopt to new concepts and frameworks and create proof of concept using newer technologies.
Ability to use agile methodology throughout the development lifecycle and provide update on regular basis, escalating issues or delays in a timely manner.
The interview process will include an initial telephone or Zoom screening.
Please let me know your interest for this position, availability to interview and start for this position along with a copy of your recent resume or please feel free to call me at any time with any questions.
Regards
Bill Stevens
Senior Technical Recruiter
PRI Technology
Denville, New Jersey 07834
1-973-732-5454 x21
Bill.Stevens@PRITechnology.com
www.PriTechnology.com
Show more
Show less","Data Analytics, Data Quality, Python, SQL, PySpark, Bash Scripting, Cloud Data Warehouse, Data Lake Platforms, RESTful Webservices, API Gateways, Data Pipelines, ML Development Cycle, Continuous Integration/Continuous Deployment, Unit Testing, Data Observability Platforms, Cloudwatch, DevOps, Observability, Agile Methodology","data analytics, data quality, python, sql, pyspark, bash scripting, cloud data warehouse, data lake platforms, restful webservices, api gateways, data pipelines, ml development cycle, continuous integrationcontinuous deployment, unit testing, data observability platforms, cloudwatch, devops, observability, agile methodology","agile methodology, api gateways, bash scripting, cloud data warehouse, cloudwatch, continuous integrationcontinuous deployment, data lake platforms, data observability platforms, data quality, dataanalytics, datapipeline, devops, ml development cycle, observability, python, restful webservices, spark, sql, unit testing"
Senior Data Engineer,Energy Jobline,"Allentown, PA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-energy-jobline-3780215981,2023-12-17,Pennsylvania,United States,Mid senior,Hybrid,"Senior Data Engineer Job DescriptionAt Air Products, our purpose is to bring people together to reimagine what’s possible, collaborate and innovate solutions to the world’s most significant energy and environmental sustainability challenges. Grow with us as we embark on building tomorrow together by being the safest, most diverse, and most profitable industrial gas company in the world.
Reimagine What’s Possible
Senior Data Engineer Roles And Responsibilities
We are seeking a Senior Data Engineer within our Digital Technology team to work on a hybrid basis at our headquarters located in Allentown, PA. This position requires 5% domestic / global travel.
Requirements
As a Senior Data Engineer, you will be hands-on defining requirements, creating solution architectures, developing, and supporting ingestion pipelines and completing proof of concepts to stay up to date on the latest trends. You will be collaborating with a team of data engineers to bring about a productive and efficient work environment by defining standards for building collaborative solutions with other teams. You will guide junior team members where necessary, assist in establishing standard processes, driving quality, and encouraging reliability and governance standards. You will also lead all aspects of bringing about data products and improving existing data infrastructures along with aiding in scaling the company. The Senior Data Engineer will also:
Work with third-party partners in creating data sources.
Be a technical leader to help adopt data principles and event-driven architecture across teams and promote the idea of data as a product.
Taking ownership of different hands-on projects and managing many stakeholders.
Estimate and plan individual workload.
Develop standards and reusable ingestion pipelines.
Provide support for existing data ingestion pipelines.
Build and maintain a roadmap of both business and technical initiatives and deliverables.
Required Skills / Abilities
Seasoned data engineer with experience in data infrastructure.
Proficient in using Python, SQL, Redshift, and AWS.
Competent in creating data lakes and integrating data in AWS.
Outstanding at communicating technical concepts.
Experience building and operating large scale production data pipelines.
Passion for data solutions and willingness to pick up new programming languages, technologies, and frameworks.
CI/CD processes and source control tools such as GitHub and related DevOps processes
Desire to learn new technologies and ability to analyze the applicability of a technology in business context.
Adept at defining and articulating a product vision, future roadmap in collaboration with leadership and key partners.
Proven track record of leading projects and delivering projects on time and on budget.
Strong analytical skills with an emphasis on data-driven decision making.
Education And Licensing Requirements
Bachelor’s degree in computer science preferred. Relevant technical experience will be considered in lieu of a degree.
We are the world’s largest hydrogen producer with over 80 years of industrial gas experience. We are hydrogen experts delivering safe, end-to-end solutions, investing in real, clean energy projects at scale, and driving the industry forward to generate a cleaner future.
At Air Products, we work in an environment where we put safety first, diversity is essential, inclusion is our culture, and each person knows they belong and matter. To learn more, visit About Air Products.
We offer a comprehensive benefits package including paid holidays and vacation, affordable medical, dental, life insurance, retirement plans/401(k), and sick time. You will be eligible for benefits and also be 100% vested in your retirement benefits on your first day of employment.
We are an Equal Opportunity Employer (U.S.). You will receive consideration for employment without regard to race, color, religion, national origin, age, citizenship, gender, marital status, pregnancy, sexual orientation, gender identity and expression, disability, or veteran status.
Air Products is committed to working with and providing a reasonable accommodation to individuals with disabilities. If you have a disability and you believe you need a reasonable accommodation to search for a job opening or to submit an online application, please e-mail us at talent@airproducts.com. General application status inquiries are not answered by this mailbox rather you’ll receive an e-mail directly from our Career Center and/or the Talent Acquisition Specialist.
Show more
Show less","Data Engineering, Data Infrastructure, Python, SQL, Redshift, AWS, Data Lakes, CI/CD, GitHub, DevOps, Programming Languages, Technologies, Frameworks, DataDriven Decision Making","data engineering, data infrastructure, python, sql, redshift, aws, data lakes, cicd, github, devops, programming languages, technologies, frameworks, datadriven decision making","aws, cicd, data engineering, data infrastructure, data lakes, datadriven decision making, devops, frameworks, github, programming languages, python, redshift, sql, technologies"
Senior Data Engineer,Air Products,"Allentown, PA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-air-products-3779184231,2023-12-17,Pennsylvania,United States,Mid senior,Hybrid,"Job Description And Qualifications
Senior Data Engineer Job Description
At Air Products, our purpose is to bring people together to reimagine what’s possible, collaborate and innovate solutions to the world’s most significant energy and environmental sustainability challenges. Grow with us as we embark on building tomorrow together by being the safest, most diverse, and most profitable industrial gas company in the world.
Reimagine What’s Possible
Senior Data Engineer Roles and Responsibilities:
We are seeking a
Senior Data Engineer
within our Digital Technology team to work on a hybrid basis at our headquarters located in Allentown, PA. This position requires 5% domestic / global travel.
As a Senior Data Engineer, you will be hands-on defining requirements, creating solution architectures, developing, and supporting ingestion pipelines and completing proof of concepts to stay up to date on the latest trends. You will be collaborating with a team of data engineers to bring about a productive and efficient work environment by defining standards for building collaborative solutions with other teams. You will guide junior team members where necessary, assist in establishing standard processes, driving quality, and encouraging reliability and governance standards. You will also lead all aspects of bringing about data products and improving existing data infrastructures along with aiding in scaling the company. The Senior Data Engineer will also:
Work with third-party partners in creating data sources.
Be a technical leader to help adopt data principles and event-driven architecture across teams and promote the idea of data as a product.
Taking ownership of different hands-on projects and managing many stakeholders.
Estimate and plan individual workload.
Develop standards and reusable ingestion pipelines.
Provide support for existing data ingestion pipelines.
Build and maintain a roadmap of both business and technical initiatives and deliverables.
Required Skills / Abilities:
Seasoned data engineer with experience in data infrastructure.
Proficient in using Python, SQL, Redshift, and AWS.
Competent in creating data lakes and integrating data in AWS.
Outstanding at communicating technical concepts.
Experience building and operating large scale production data pipelines.
Passion for data solutions and willingness to pick up new programming languages, technologies, and frameworks.
CI/CD processes and source control tools such as GitHub and related DevOps processes
Desire to learn new technologies and ability to analyze the applicability of a technology in business context.
Adept at defining and articulating a product vision, future roadmap in collaboration with leadership and key partners.
Proven track record of leading projects and delivering projects on time and on budget.
Strong analytical skills with an emphasis on data-driven decision making.
Education and Licensing Requirements:
Bachelor’s degree in computer science preferred. Relevant technical experience will be considered in lieu of a degree.
We are the world’s largest hydrogen producer with over 80 years of industrial gas experience. We are hydrogen experts delivering safe, end-to-end solutions, investing in real, clean energy projects at scale, and driving the industry forward to generate a cleaner future.
At Air Products, we work in an environment where we put safety first, diversity is essential, inclusion is our culture, and each person knows they belong and matter. To learn more, visit About Air Products.
We offer a comprehensive benefits package including paid holidays and vacation, affordable medical, dental, life insurance, retirement plans/401(k), and sick time. You will be eligible for benefits and also be 100% vested in your retirement benefits on your first day of employment.
We are an Equal Opportunity Employer (U.S.). You will receive consideration for employment without regard to race, color, religion, national origin, age, citizenship, gender, marital status, pregnancy, sexual orientation, gender identity and expression, disability, or veteran status.
Air Products is committed to working with and providing a reasonable accommodation to individuals with disabilities. If you have a disability and you believe you need a reasonable accommodation to search for a job opening or to submit an online application, please e-mail us at talent@airproducts.com. General application status inquiries are not answered by this mailbox rather you’ll receive an e-mail directly from our Career Center and/or the Talent Acquisition Specialist.
Req No.
48355BR
Employment Status
Full Time
Organization
Corporate
Business Sector / Division
Digital Technology
Region
North America
Country
United States
Show more
Show less","Python, SQL, Redshift, AWS, Apache Airflow, Data Lakes, CI/CD, GitHub, DevOps, Data Pipelines, Data Infrastructure, Cloud Computing, Data Architecture, Data Governance, Data Quality, DataDriven Decision Making, Data Visualization, Data Analytics, Data Mining, Machine Learning, Artificial Intelligence, Hadoop, Spark, Kafka, Flink, NoSQL, MongoDB, Cassandra, HBase","python, sql, redshift, aws, apache airflow, data lakes, cicd, github, devops, data pipelines, data infrastructure, cloud computing, data architecture, data governance, data quality, datadriven decision making, data visualization, data analytics, data mining, machine learning, artificial intelligence, hadoop, spark, kafka, flink, nosql, mongodb, cassandra, hbase","apache airflow, artificial intelligence, aws, cassandra, cicd, cloud computing, data architecture, data governance, data infrastructure, data lakes, data mining, data quality, dataanalytics, datadriven decision making, datapipeline, devops, flink, github, hadoop, hbase, kafka, machine learning, mongodb, nosql, python, redshift, spark, sql, visualization"
Senior Data Engineer,Diverse Lynx,"Charlotte, AR",https://www.linkedin.com/jobs/view/senior-data-engineer-at-diverse-lynx-3708547120,2023-12-17,Poughkeepsie,United States,Mid senior,Onsite,"Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less",I am unable to extract the requested data from the provided context as it does not contain any information about technical skills frameworks languages softwares or concepts relevant to the job posting.,i am unable to extract the requested data from the provided context as it does not contain any information about technical skills frameworks languages softwares or concepts relevant to the job posting,i am unable to extract the requested data from the provided context as it does not contain any information about technical skills frameworks languages softwares or concepts relevant to the job posting
Data Engineer,Virtusa,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-virtusa-3771831341,2023-12-17,Redcliffe, Australia,Associate,Hybrid,"Implementation experience in BigData Platform ; preferably in Cloudera Hadoop platform
Minimum 2 years of Development experience using Hadoop eco system tools & utilities: MapReduce, Spark, Kafka, Sqoop, Impala, Hive etc
Ability to work independently and also contribute to overall architecture and design
Experience in writing Shell scripts in Linux Platform
Knowledge on API management concepts and design
Developed Apache Spark applications and comfortable developing in Python. (Preferred)
Performed debugging and performance tuning of Spark applications.
Show more
Show less","Hadoop, Cloudera, MapReduce, Spark, Kafka, Sqoop, Impala, Hive, Shell scripting, Linux, API management, Python, Apache Spark","hadoop, cloudera, mapreduce, spark, kafka, sqoop, impala, hive, shell scripting, linux, api management, python, apache spark","apache spark, api management, cloudera, hadoop, hive, impala, kafka, linux, mapreduce, python, shell scripting, spark, sqoop"
