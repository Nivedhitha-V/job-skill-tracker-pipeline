job_title,company,job_location,job_link,first_seen,search_city,search_country,job level,job_type,job_summary,job_skills,job_skills_raw,job_skills_cleaned
Software Engineer - Senior Data Engineer,Capgemini,"Ontario, CA",https://www.linkedin.com/jobs/view/software-engineer-senior-data-engineer-at-capgemini-3782281488,2023-12-17,Anaheim,United States,Mid senior,Hybrid,"Life at Capgemini
Capgemini Supports All Aspects Of Your Well-being Throughout The Changing Stages Of Your Life And Career. For Eligible Employees, We Offer
Collaborating with teams of creative, fun, and driven colleagues
Flexible work options enabling time and location-based flexibility
Company-provided home office equipment
Virtual collaboration and productivity tools to enable hybrid teams
Comprehensive benefits program (Health, Welfare, Retirement and Paid time off)
Other perks and wellness benefits like discount programs, and gym/studio access.
Paid Parental Leave and coaching, baby welcome gift, and family care/illness days
Back-up childcare/elder care, childcare discounts, and subsidized virtual tutoring
Tuition assistance and weekly hot skill development opportunities
Experiential, high-impact learning series events
Access to mental health resources and mindfulness programs
Access to join Capgemini Employee Resource Groups around communities of interest
About Capgemini
Capgemini is a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided everyday by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get The Future You Want | www.capgemini.com
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Please be aware that Capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process.
Applicants for employment in Canada must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in Canada by Capgemini.
Name of the position:
Senior Data Engineer
Reports to
: Team Lead/Delivery Manager
Department/Project:
Engineering
Job Description
As a Senior Engineer, you will build distributed data processing solution and highly loaded database solutions for various cases including reporting, product analytics, marketing optimization and financial reporting. Chip in as part of self-organized team of authority data engineers working in an adventurous, innovative environment for our client, crafting the foundation for decision-making at a company dealing with billions of events per day.
You will Investigate, build, and implement the solutions for existing technical challenges. Provide mentorship, instruction, direction, leadership to a development team with the purpose of achieving project goals.
Key Responsibilities
Acquires tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all collaborators.
Ensures that assigned area are delivered within set expectations and required quality objectives.
Provides estimations, agrees task duration with the manager and chips in to project plan of assigned area.
Analyzes scope of alternative solutions and makes decision about area implementation based on their experience and technical expertise.
Leads functional and architectural design of assigned areas. Makes sure design decisions on the project meet architectural and design requirements.
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raises red flags in crisis situations which are beyond responsibilities.
Responsible for resolving crisis situations within responsibilities.
Initiates and conducts code reviews, crafts code standards, conventions and guidelines.
Suggests technical and functional improvements to contribute to the product;
Constantly improves your professional level.
Collaborates with other teams.
Required Skills
5+ years of professional experience in data engineering, business intelligence, or a similar role.
Proficiency in Python programming languages.
proven track record in ETL orchestration and workflow management tool Airflow.
Authority in Database fundamentals, SQL and distributed computing.
Excellent communication skills and experience with technical and non-technical teams.
Able to clear hacker rank code test.
Nice To Have
Experience in AWS (EC2/S2/IAM).
Experience with the Distributed data/similar ecosystem (Spark, Hive, Presto) or streaming technologies such as Kafka/Flink.
Experience working with Snowflake, Redshift, PostgreSQL and/or other DBMS platforms.
Strong in Python, SQL, Spark, Airflow, AWS.
Show more
Show less","Python, Airflow, AWS, Distributed data, Spark, Hive, Presto, Kafka, Flink, Snowflake, Redshift, PostgreSQL, DBMS, SQL","python, airflow, aws, distributed data, spark, hive, presto, kafka, flink, snowflake, redshift, postgresql, dbms, sql","airflow, aws, dbms, distributed data, flink, hive, kafka, postgresql, presto, python, redshift, snowflake, spark, sql"
Software Engineer - Senior Data Engineer,Capgemini,"Ontario, CA",https://www.linkedin.com/jobs/view/software-engineer-senior-data-engineer-at-capgemini-3782277918,2023-12-17,Anaheim,United States,Mid senior,Hybrid,"Life at Capgemini
Capgemini Supports All Aspects Of Your Well-being Throughout The Changing Stages Of Your Life And Career. For Eligible Employees, We Offer
Collaborating with teams of creative, fun, and driven colleagues
Flexible work options enabling time and location-based flexibility
Company-provided home office equipment
Virtual collaboration and productivity tools to enable hybrid teams
Comprehensive benefits program (Health, Welfare, Retirement and Paid time off)
Other perks and wellness benefits like discount programs, and gym/studio access.
Paid Parental Leave and coaching, baby welcome gift, and family care/illness days
Back-up childcare/elder care, childcare discounts, and subsidized virtual tutoring
Tuition assistance and weekly hot skill development opportunities
Experiential, high-impact learning series events
Access to mental health resources and mindfulness programs
Access to join Capgemini Employee Resource Groups around communities of interest
About Capgemini
Capgemini is a global leader in partnering with companies to transform and handle their business by harnessing the power of technology. The Group is guided everyday by its purpose of sparking human energy through technology for an inclusive and balanced future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get The Future You Want | www.capgemini.com
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Please be aware that Capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process.
Applicants for employment in Canada must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in Canada by Capgemini.
Name of the position
: Senior Data Engineer
Reports to:
Team Lead/Delivery Manager
Department/Project:
Engineering
Job Description
As a Senior Engineer, you will build distributed data processing solution and highly loaded database solutions for various cases including reporting, product analytics, marketing optimization and financial reporting. Chip in as part of self-organized team of authority data engineers working in an adventurous, innovative environment for our client, crafting the foundation for decision-making at a company dealing with billions of events per day.
You will Investigate, build, and implement the solutions for existing technical challenges. Provide mentorship, instruction, direction, leadership to a development team with the purpose of achieving project goals.
Key Responsibilities
Acquires tasks from the project lead or Team Lead (TL), prepares functional and design specifications, approves them with all collaborators.
Ensures that assigned area are delivered within set expectations and required quality objectives.
Provides estimations, agrees task duration with the manager and chips in to project plan of assigned area.
Analyzes scope of alternative solutions and makes decision about area implementation based on their experience and technical expertise.
Leads functional and architectural design of assigned areas. Makes sure design decisions on the project meet architectural and design requirements.
Addresses area-level risks, provides and implements mitigation plan.
Reports about area readiness/quality, and raises red flags in crisis situations which are beyond responsibilities.
Responsible for resolving crisis situations within responsibilities.
Initiates and conducts code reviews, crafts code standards, conventions and guidelines.
Suggests technical and functional improvements to contribute to the product;
Constantly improves your professional level.
Collaborates with other teams.
Required Skills
5+ years of professional experience in data engineering, business intelligence, or a similar role.
Proficiency in Python programming languages.
proven track record in ETL orchestration and workflow management tool Airflow.
Authority in Database fundamentals, SQL and distributed computing.
Excellent communication skills and experience with technical and non-technical teams.
Able to clear hacker rank code test.
Nice To Have
Experience in AWS (EC2/S2/IAM).
Experience with the Distributed data/similar ecosystem (Spark, Hive, Presto) or streaming technologies such as Kafka/Flink.
Experience working with Snowflake, Redshift, PostgreSQL and/or other DBMS platforms.
Strong in Python, SQL, Spark, Airflow, AWS
Show more
Show less","Data Engineering, Business Intelligence, Python, Airflow, SQL, Distributed Computing, AWS, EC2, S2, IAM, Spark, Hive, Presto, Kafka, Flink, Snowflake, Redshift, PostgreSQL","data engineering, business intelligence, python, airflow, sql, distributed computing, aws, ec2, s2, iam, spark, hive, presto, kafka, flink, snowflake, redshift, postgresql","airflow, aws, business intelligence, data engineering, distributed computing, ec2, flink, hive, iam, kafka, postgresql, presto, python, redshift, s2, snowflake, spark, sql"
Principal Data Engineer,Motion Recruitment,"Long Beach, CA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-motion-recruitment-3779719261,2023-12-17,Anaheim,United States,Mid senior,Hybrid,"We’re looking for a Principal Data Engineer to join our Data Platform team. We are a Series D startup in Los Angeles, CA, and have been listed as one of the most influential startups of 2023 by Time's Magazine.
Our main mission is to empower Creators by allowing them to receive cash for their video catalogs, through licensing their existing videos (and/or future video uploads) and receiving instant payouts. Creators then use the funds to fuel their growth through hiring resources, investing, or anyway they choose all while remaining independent.
In addition to funding, we provide creators with in-depth data insights into the performance of all existing content to further help educate the Creator on the value of their library, the value of future uploads and how they can improve performance in the future.
As a Principal Data Engineer you will be using Python & Spark/PySpark to develop and maintain scalable data pipelines. This will include ETL pipeline development for both single and multi-node solutions, as well as creating derived datasets with augmented properties. You will also play a pivotal role in optimizing big data frameworks, improving the availability & quality of data as it's integrated into the cloud (AWS).
This is a great role for someone that is eager to work with vast amounts of data. We have licensed content comprised of over 700,000 videos that are generating approximately 100 billion monthly watch-time minutes. Required Skills & Experience
Bachelor’s degree, preferably in Computer Science or Computer Information Systems
6+ years of software engineering experience
5+ years of data engineering experience with Spark or Flink
4+ years of experience running software and services in the cloud
DataFrame APIs (Pandas and Spark)
Proficiency in Python, Scala, with data optimized file formats such as Parquet and Avro
Proficiency with SQL on RDBMS and data warehouse solutions like Redshift
Experience with Data Lake technologies (Delta Lake, Iceberg, etc…)
The Offer You Will Receive The Following Benefits
Medical and vision insurance covered up to 100%
Dental insurance
401(k) matching
Stock options
Complimentary gym access
Autonomy and upward mobility
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Connor Hart
Show more
Show less","Python, Spark/PySpark, ETL, Big data frameworks, AWS, DataFrame APIs, Pandas, SQL, RDBMS, Redshift, Data Lake technologies, Delta Lake, Iceberg, Parquet, Avro, Scala","python, sparkpyspark, etl, big data frameworks, aws, dataframe apis, pandas, sql, rdbms, redshift, data lake technologies, delta lake, iceberg, parquet, avro, scala","avro, aws, big data frameworks, data lake technologies, dataframe apis, delta lake, etl, iceberg, pandas, parquet, python, rdbms, redshift, scala, sparkpyspark, sql"
Customer Service Representative/Data Analyst/Data Entry Clerk/Urgent,Toyandsons,"Granby, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-urgent-at-toyandsons-3756471499,2023-12-17,Granby, Canada,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk/Urgent will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Data Visualization, A/B Testing, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL Processes","data analysis, statistical techniques, data visualization, ab testing, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk,Drmartens,"Farnham, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-drmartens-3750637532,2023-12-17,Granby, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analytics, Statistical Analysis, Business Intelligence, Data Visualization, Machine Learning, A/B Testing, Data Quality Management, Data Integration, Data Warehousing, SQL, R, Python, Tableau, Power BI","data analytics, statistical analysis, business intelligence, data visualization, machine learning, ab testing, data quality management, data integration, data warehousing, sql, r, python, tableau, power bi","ab testing, business intelligence, data integration, data quality management, dataanalytics, datawarehouse, machine learning, powerbi, python, r, sql, statistical analysis, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Villarestaurantgroup,"Granby, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-villarestaurantgroup-3756469708,2023-12-17,Granby, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Data Visualization, SQL, R, Python, Data Manipulation, Tableau, Power BI, Hypothesis Testing, A/B Testing, ETL","data analysis, statistical techniques, data visualization, sql, r, python, data manipulation, tableau, power bi, hypothesis testing, ab testing, etl","ab testing, data manipulation, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Majorleaguebaseball,"St-Hyacinthe, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-majorleaguebaseball-3752012620,2023-12-17,Granby, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Data Visualization, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL","data analysis, statistical techniques, data visualization, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Newyorkuniversity,"Granby, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-newyorkuniversity-3757205745,2023-12-17,Granby, Canada,Mid senior,Hybrid,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
Powered by Webbtree
Show more
Show less","Data Analysis, Data Interpretation, DataDriven DecisionMaking, Statistical Techniques, Advanced Statistical Tools, SQL, R, Python, tableau, Power BI, Hypothesis Testing, A/B Testing, Data Quality, Data Integrity, Data Accuracy, Data Completeness, A/B Testing, Data Visualization","data analysis, data interpretation, datadriven decisionmaking, statistical techniques, advanced statistical tools, sql, r, python, tableau, power bi, hypothesis testing, ab testing, data quality, data integrity, data accuracy, data completeness, ab testing, data visualization","ab testing, advanced statistical tools, data accuracy, data completeness, data integrity, data interpretation, data quality, dataanalytics, datadriven decisionmaking, hypothesis testing, powerbi, python, r, sql, statistical techniques, tableau, visualization"
"Construction Manager, MLZ Data Center Construction",Amazon Web Services (AWS),"Portage, IN",https://www.linkedin.com/jobs/view/construction-manager-mlz-data-center-construction-at-amazon-web-services-aws-3726533919,2023-12-17,Michigan City,United States,Mid senior,Onsite,"Description
As our Data Center Construction Manager (CM)], you will be a part of a diverse, upbeat, and creative team tasked with solving fascinating problems constructing Amazon Data Centers. Our data centers are industry-leading examples of energy efficient and cost-effective designs.
You Will
Own project scope, quality, schedule, and budget for construction of new builds or general capital projects.
Own construction project management and oversight of construction related activities.
Work alongside partner teams such as Operations, Networking, Controls, Security, and Commissioning to build Data Centers that directly support our Customers.
Be a leader in your specific discipline (construction management, building services, architectural, electrical or mechanical engineering).
Represent Amazon’s owner’s representative on construction sites daily, as, interacting with construction trades.
At Amazon, we are all Owners and leverage unique opportunities presented to us by owning everything from the design review to construction bidding, execution, and final hand-off to our customers. We develop innovative data centers for our Customers.
Key job responsibilities
Be Amazon’s construction representative on multiple data centers building and capital improvement construction project sites from construction start through hand-off to operations.
Direct interface with construction general contractors during the construction bidding, award, execution, punch-list, and closeout phases.
Create construction project scope and request for proposals.
Conduct negotiations with general contractors and evaluate bids and proposals with detail and accuracy.
Manage and drive cost, schedule, and quality while managing construction contractors and vendors.
Perform construction project management activities, including management of documents, submittals, RFIs, change orders, invoices, quality, scope, and schedule.
Drive construction teams to troubleshoot and perform root-cause failure analysis on equipment failures.
Support commissioning and integrated system testing and oversight.
Support operations of installed facilities, including review of procedures, best practices, and maintenance initiatives.
Support capital request creation.
Analyze and report construction progress and financials.
Record and report key construction metrics to team members and management.
Contribute to construction initiatives aimed at improving construction efficiency and increasing data center resiliency.
Provide constructability reviews for electrical and mechanical designs for new or optimization of existing data centers.
About The Team
Portage, Indiana will NOT be the permanent reporting location. This is a temporary location until the final location in the Indiana region is finalized.
We are open to hiring candidates to work out of one of the following locations:
Portage, IN, USA
Basic Qualifications
Bachelor’s degree in Mechanical Engineering, Electrical Engineering, Construction Management, or an equivalent engineering science plus 5+ years of relevant construction experience, OR 9+ years of relevant construction experience in lieu of a degree.
4+ years of experience in construction management of large, complex projects involving large-scale mechanical, electrical, and plumbing (MEP) plants.
4+ years of general contractor and vendor management experience (request for proposals, bidding, change orders, quality control, and RFI and submittal tracking) associated with construction and project execution.
Preferred Qualifications
MS in Construction Management or Engineering (Mechanical, Electrical, Civil, Structural)
7+ Yrs. Exp in Construction Management of large-scale projects
7+ Yrs. Exp. Project Management and Vendor Management
3+ Yrs. Exp. In Data Center system-level architecture and electrical engineering principals
3+ Yrs. Exp. In Data Center system-level architecture and mechanical engineering principals
Experience designing data centers or critical MEP infrastructure
Certified as a Professional Engineer (PE), LEED, or Certified Construction Manager (CCM)
Prior AWS/Amazon experience
5+ Yr. Military Service
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.
Company
- Amazon Data Services, Inc.
Job ID: A2456818
Show more
Show less","Data Center Construction Management, Energy Efficiency, CostEffective Design, Project Scope, Quality, Schedule, Budget, Construction Project Management, Oversight, Operations, Networking, Controls, Security, Commissioning, Construction Bidding, Execution, HandOff, Request for Proposals, Cost, Schedule, Quality, Construction Contractors, Vendors, Documents, Submittals, RFIs, Change Orders, Invoices, Construction Progress, Financials, Key Construction Metrics, Construction Efficiency, Data Center Resiliency, Constructability Reviews, Electrical Design, Mechanical Design","data center construction management, energy efficiency, costeffective design, project scope, quality, schedule, budget, construction project management, oversight, operations, networking, controls, security, commissioning, construction bidding, execution, handoff, request for proposals, cost, schedule, quality, construction contractors, vendors, documents, submittals, rfis, change orders, invoices, construction progress, financials, key construction metrics, construction efficiency, data center resiliency, constructability reviews, electrical design, mechanical design","budget, change orders, commissioning, constructability reviews, construction bidding, construction contractors, construction efficiency, construction progress, construction project management, controls, cost, costeffective design, data center construction management, data center resiliency, documents, electrical design, energy efficiency, execution, financials, handoff, invoices, key construction metrics, mechanical design, networking, operations, oversight, project scope, quality, request for proposals, rfis, schedule, security, submittals, vendors"
"Construction Manager, MLZ Data Center Construction",Amazon Web Services (AWS),"Portage, IN",https://www.linkedin.com/jobs/view/construction-manager-mlz-data-center-construction-at-amazon-web-services-aws-3726727745,2023-12-17,Michigan City,United States,Mid senior,Onsite,"Description
As our Data Center Construction Manager (CM)], you will be a part of a diverse, upbeat, and creative team tasked with solving fascinating problems constructing Amazon Data Centers. Our data centers are industry-leading examples of energy efficient and cost-effective designs.
You Will
Own project scope, quality, schedule, and budget for construction of new builds or general capital projects.
Own construction project management and oversight of construction related activities.
Work alongside partner teams such as Operations, Networking, Controls, Security, and Commissioning to build Data Centers that directly support our Customers.
Be a leader in your specific discipline (construction management, building services, architectural, electrical or mechanical engineering).
Represent Amazon’s owner’s representative on construction sites daily, as, interacting with construction trades.
At Amazon, we are all Owners and leverage unique opportunities presented to us by owning everything from the design review to construction bidding, execution, and final hand-off to our customers. We develop innovative data centers for our Customers.
Key job responsibilities
Be Amazon’s construction representative on multiple data centers building and capital improvement construction project sites from construction start through hand-off to operations.
Direct interface with construction general contractors during the construction bidding, award, execution, punch-list, and closeout phases.
Create construction project scope and request for proposals.
Conduct negotiations with general contractors and evaluate bids and proposals with detail and accuracy.
Manage and drive cost, schedule, and quality while managing construction contractors and vendors.
Perform construction project management activities, including management of documents, submittals, RFIs, change orders, invoices, quality, scope, and schedule.
Drive construction teams to troubleshoot and perform root-cause failure analysis on equipment failures.
Support commissioning and integrated system testing and oversight.
Support operations of installed facilities, including review of procedures, best practices, and maintenance initiatives.
Support capital request creation.
Analyze and report construction progress and financials.
Record and report key construction metrics to team members and management.
Contribute to construction initiatives aimed at improving construction efficiency and increasing data center resiliency.
Provide constructability reviews for electrical and mechanical designs for new or optimization of existing data centers.
About The Team
Portage, Indiana will NOT be the permanent reporting location. This is a temporary location until the final location in the Indiana region is finalized.
We are open to hiring candidates to work out of one of the following locations:
Portage, IN, USA
Basic Qualifications
Bachelor’s degree in Mechanical Engineering, Electrical Engineering, Construction Management, or an equivalent engineering science plus 5+ years of relevant construction experience, OR 9+ years of relevant construction experience in lieu of a degree.
4+ years of experience in construction management of large, complex projects involving large-scale mechanical, electrical, and plumbing (MEP) plants.
4+ years of general contractor and vendor management experience (request for proposals, bidding, change orders, quality control, and RFI and submittal tracking) associated with construction and project execution.
Preferred Qualifications
MS in Construction Management or Engineering (Mechanical, Electrical, Civil, Structural)
7+ Yrs. Exp in Construction Management of large-scale projects
7+ Yrs. Exp. Project Management and Vendor Management
3+ Yrs. Exp. In Data Center system-level architecture and electrical engineering principals
3+ Yrs. Exp. In Data Center system-level architecture and mechanical engineering principals
Experience designing data centers or critical MEP infrastructure
Certified as a Professional Engineer (PE), LEED, or Certified Construction Manager (CCM)
Prior AWS/Amazon experience
5+ Yr. Military Service
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.
Company
- Amazon Data Services, Inc.
Job ID: A2456826
Show more
Show less","Data Center Construction, Project Management, Construction Oversight, Building Services, Architectural Engineering, Electrical Engineering, Mechanical Engineering, Construction Bidding, Cost Management, Schedule Management, Quality Management, Construction Contracts, Vendors Management, Construction Documents Management, Submittals Management, RFIs Management, Change Orders Management, Invoices Management, Troubleshooting, RootCause Failure Analysis, Commissioning, Integrated System Testing, Data Center Operations, Maintenance Initiatives, Capital Request Creation, Construction Metrics, Constructability Reviews, Electrical Design, Mechanical Design, MEP Plants, LEED, Certified Construction Manager (CCM), AWS/Amazon","data center construction, project management, construction oversight, building services, architectural engineering, electrical engineering, mechanical engineering, construction bidding, cost management, schedule management, quality management, construction contracts, vendors management, construction documents management, submittals management, rfis management, change orders management, invoices management, troubleshooting, rootcause failure analysis, commissioning, integrated system testing, data center operations, maintenance initiatives, capital request creation, construction metrics, constructability reviews, electrical design, mechanical design, mep plants, leed, certified construction manager ccm, awsamazon","architectural engineering, awsamazon, building services, capital request creation, certified construction manager ccm, change orders management, commissioning, constructability reviews, construction bidding, construction contracts, construction documents management, construction metrics, construction oversight, cost management, data center construction, data center operations, electrical design, electrical engineering, integrated system testing, invoices management, leed, maintenance initiatives, mechanical design, mechanical engineering, mep plants, project management, quality management, rfis management, rootcause failure analysis, schedule management, submittals management, troubleshooting, vendors management"
"Construction Manager, MLZ Data Center Construction",Amazon Web Services (AWS),"Portage, IN",https://www.linkedin.com/jobs/view/construction-manager-mlz-data-center-construction-at-amazon-web-services-aws-3727281267,2023-12-17,Michigan City,United States,Mid senior,Onsite,"Description
As our Data Center Construction Manager (CM)], you will be a part of a diverse, upbeat, and creative team tasked with solving fascinating problems constructing Amazon Data Centers. Our data centers are industry-leading examples of energy efficient and cost-effective designs.
You Will
Own project scope, quality, schedule, and budget for construction of new builds or general capital projects.
Own construction project management and oversight of construction related activities.
Work alongside partner teams such as Operations, Networking, Controls, Security, and Commissioning to build Data Centers that directly support our Customers.
Be a leader in your specific discipline (construction management, building services, architectural, electrical or mechanical engineering).
Represent Amazon’s owner’s representative on construction sites daily, as, interacting with construction trades.
At Amazon, we are all Owners and leverage unique opportunities presented to us by owning everything from the design review to construction bidding, execution, and final hand-off to our customers. We develop innovative data centers for our Customers.
Key job responsibilities
Be Amazon’s construction representative on multiple data centers building and capital improvement construction project sites from construction start through hand-off to operations.
Direct interface with construction general contractors during the construction bidding, award, execution, punch-list, and closeout phases.
Create construction project scope and request for proposals.
Conduct negotiations with general contractors and evaluate bids and proposals with detail and accuracy.
Manage and drive cost, schedule, and quality while managing construction contractors and vendors.
Perform construction project management activities, including management of documents, submittals, RFIs, change orders, invoices, quality, scope, and schedule.
Drive construction teams to troubleshoot and perform root-cause failure analysis on equipment failures.
Support commissioning and integrated system testing and oversight.
Support operations of installed facilities, including review of procedures, best practices, and maintenance initiatives.
Support capital request creation.
Analyze and report construction progress and financials.
Record and report key construction metrics to team members and management.
Contribute to construction initiatives aimed at improving construction efficiency and increasing data center resiliency.
Provide constructability reviews for electrical and mechanical designs for new or optimization of existing data centers.
About The Team
Portage, Indiana will NOT be the permanent reporting location. This is a temporary location until the final location in the Indiana region is finalized.
We are open to hiring candidates to work out of one of the following locations:
Portage, IN, USA
Basic Qualifications
Bachelor’s degree in Mechanical Engineering, Electrical Engineering, Construction Management, or an equivalent engineering science plus 5+ years of relevant construction experience, OR 9+ years of relevant construction experience in lieu of a degree.
4+ years of experience in construction management of large, complex projects involving large-scale mechanical, electrical, and plumbing (MEP) plants.
4+ years of general contractor and vendor management experience (request for proposals, bidding, change orders, quality control, and RFI and submittal tracking) associated with construction and project execution.
Preferred Qualifications
MS in Construction Management or Engineering (Mechanical, Electrical, Civil, Structural)
7+ Yrs. Exp in Construction Management of large-scale projects
7+ Yrs. Exp. Project Management and Vendor Management
3+ Yrs. Exp. In Data Center system-level architecture and electrical engineering principals
3+ Yrs. Exp. In Data Center system-level architecture and mechanical engineering principals
Experience designing data centers or critical MEP infrastructure
Certified as a Professional Engineer (PE), LEED, or Certified Construction Manager (CCM)
Prior AWS/Amazon experience
5+ Yr. Military Service
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.
Company
- Amazon Data Services, Inc.
Job ID: A2458297
Show more
Show less","Construction Management, Data Center Construction, MEP (Mechanical Electrical and Plumbing), Project Management, Bidding, Quality Control, RFI (Request for Information), Submittal Tracking, LEED (Leadership in Energy and Environmental Design), AWS (Amazon Web Services)","construction management, data center construction, mep mechanical electrical and plumbing, project management, bidding, quality control, rfi request for information, submittal tracking, leed leadership in energy and environmental design, aws amazon web services","aws amazon web services, bidding, construction management, data center construction, leed leadership in energy and environmental design, mep mechanical electrical and plumbing, project management, quality control, rfi request for information, submittal tracking"
"Construction Manager, MLZ Data Center Construction",Amazon Web Services (AWS),"Portage, IN",https://www.linkedin.com/jobs/view/construction-manager-mlz-data-center-construction-at-amazon-web-services-aws-3727281277,2023-12-17,Michigan City,United States,Mid senior,Onsite,"Description
As our Data Center Construction Manager (CM), you will be a part of a diverse, upbeat, and creative team tasked with solving fascinating problems constructing Amazon Data Centers. Our data centers are industry-leading examples of energy efficient and cost-effective designs.
You Will
Own project scope, quality, schedule, and budget for construction of new builds or general capital projects.
Own construction project management and oversight of construction related activities.
Work alongside partner teams such as Operations, Networking, Controls, Security, and Commissioning to build Data Centers that directly support our Customers.
Be a leader in your specific discipline (construction management, building services, architectural, electrical or mechanical engineering).
Represent Amazon’s owner’s representative on construction sites daily, as, interacting with construction trades.
At Amazon, we are all Owners and leverage unique opportunities presented to us by owning everything from the design review to construction bidding, execution, and final hand-off to our customers. We develop innovative data centers for our Customers.
Key job responsibilities
Be Amazon’s construction representative on multiple data centers building and capital improvement construction project sites from construction start through hand-off to operations.
Direct interface with construction general contractors during the construction bidding, award, execution, punch-list, and closeout phases.
Create construction project scope and request for proposals.
Conduct negotiations with general contractors and evaluate bids and proposals with detail and accuracy.
Manage and drive cost, schedule, and quality while managing construction contractors and vendors.
Perform construction project management activities, including management of documents, submittals, RFIs, change orders, invoices, quality, scope, and schedule.
Drive construction teams to troubleshoot and perform root-cause failure analysis on equipment failures.
Support commissioning and integrated system testing and oversight.
Support operations of installed facilities, including review of procedures, best practices, and maintenance initiatives.
Support capital request creation.
Analyze and report construction progress and financials.
Record and report key construction metrics to team members and management.
Contribute to construction initiatives aimed at improving construction efficiency and increasing data center resiliency.
Provide constructability reviews for electrical and mechanical designs for new or optimization of existing data centers.
About The Team
Portage, Indiana will NOT be the permanent reporting location. This is a temporary location until the final location in the Indiana region is finalized.
We are open to hiring candidates to work out of one of the following locations:
Portage, IN, USA
Basic Qualifications
Bachelor’s degree in Mechanical Engineering, Electrical Engineering, Construction Management, or an equivalent engineering science plus 5+ years of relevant construction experience, OR 9+ years of relevant construction experience in lieu of a degree.
4+ years of experience in construction management of large, complex projects involving large-scale mechanical, electrical, and plumbing (MEP) plants.
4+ years of general contractor and vendor management experience (request for proposals, bidding, change orders, quality control, and RFI and submittal tracking) associated with construction and project execution.
Preferred Qualifications
MS in Construction Management or Engineering (Mechanical, Electrical, Civil, Structural)
7+ Yrs. Exp in Construction Management of large-scale projects
7+ Yrs. Exp. Project Management and Vendor Management
3+ Yrs. Exp. In Data Center system-level architecture and electrical engineering principals
3+ Yrs. Exp. In Data Center system-level architecture and mechanical engineering principals
Experience designing data centers or critical MEP infrastructure
Certified as a Professional Engineer (PE), LEED, or Certified Construction Manager (CCM)
Prior AWS/Amazon experience
5+ Yr. Military Service
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.
Company
- Amazon Data Services, Inc.
Job ID: A2458296
Show more
Show less","Construction Management, Project Management, Mechanical Engineering, Electrical Engineering, Building Services, Architectural Engineering, Data Center Design, Data Center Construction, General Contracting, Vendor Management, Request for Proposals, Bidding, Change Orders, Quality Control, Commissioning, Integrated System Testing","construction management, project management, mechanical engineering, electrical engineering, building services, architectural engineering, data center design, data center construction, general contracting, vendor management, request for proposals, bidding, change orders, quality control, commissioning, integrated system testing","architectural engineering, bidding, building services, change orders, commissioning, construction management, data center construction, data center design, electrical engineering, general contracting, integrated system testing, mechanical engineering, project management, quality control, request for proposals, vendor management"
Sr. Software Engineer - Air Combat Datalinks (Onsite),Collins Aerospace,"Cedar Rapids, IA",https://www.linkedin.com/jobs/view/sr-software-engineer-air-combat-datalinks-onsite-at-collins-aerospace-3767481572,2023-12-17,Iowa City,United States,Mid senior,Onsite,"Date Posted:
2023-10-30
Country:
United States of America
Location:
HIA32: Cedar Rapids, IA 400 Collins Rd NE , Cedar Rapids, IA, 52498-0505 USA
Position Role Type:
Onsite
The Air Combat Test and Training (ACTT) engineering department is responsible for the design, analysis, test and certification of Air Combat Test and Training systems. We solve the challenges of next-generation pilot proficiency training so they can “Train as they Fight.” We field Test and Training range systems with state-of-the-art technology in cryptography, datalinks, navigation, communication, networking and command and control. We specialize in highly accurate, highly secure solutions for U.S. Navy and Air Force customers. We are a system of systems development team that designs, integrates, and supports some of the largest, most complex solutions offered by Collins Aerospace.
The Datalinks team within the ACTT department is responsible for the development of the mesh networking waveform that links together the training participants with the mission commanders. The team is looking for an experienced Senior Software Engineer to support the expansion of our Air Combat system capabilities. This position will be located onsite at our
Cedar Rapids, IA
site.
Eligible for relocation.
What you will do:
Actively participate as a member of an agile development team.
Effectively communicate with internal teams and leadership
Design, Develop, Review, and Test new software features.
Package software builds and deploy to an embedded target environment.
Support integration and system test activities in a lab environment.
Education and experience:
Typically requires a degree in Science, Technology, Engineering or Mathematics (STEM) unless prohibited by local laws/regulations and minimum 5 years prior relevant experience or an Advanced Degree in a related field and minimum 3 years of experience or in absence of a degree, 9 years of relevant experience.
Must be a U.S. Citizen
Must have or be capable of obtaining a US Department of Defense (DoD) security clearance. Candidate selected will be subject to a government security investigation/reinstatement and must meet eligibility requirements.
Skills you must have:
Application software development experience
Experience coding in C++ and Python
Experience with Windows and Linux Operating Systems
Experience investigating problems to root cause and championing corrective actions.
Experience with Object Oriented architectures and design patterns
Skills we value:
Experience with software configuration management (Git, SVN)
Experience with Agile development methodology
Experience working in Atlassian tools (JIRA, Bitbucket, Confluence, Bamboo)
Experience with CI/CD technologies
Experience with LVC communication standards (DIS, TENA, HLA)
Motivated self-starter with strong analytical skills
Collins Aerospace, a Raytheon Technologies company, is a leader in technologically advanced and intelligent solutions for the global aerospace and defense industry. Collins Aerospace has the capabilities, comprehensive portfolio and expertise to solve customers’ toughest challenges and to meet the demands of a rapidly evolving global market.
Do you want to be a part of something bigger? A team whose impact stretches across the world, and even beyond? At Collins Aerospace, our Mission Systems team helps civilian, military and government customers complete their most complex missions — whatever and wherever they may be. Our customers depend on us for intelligent and secure communications, missionized systems for specialized aircraft and spacecraft and collaborative space solutions. By joining our team, you’ll have your own critical part to play in ensuring our customer succeeds today while anticipating their needs for tomorrow. Are you up for the challenge? Join our mission today.
#reempowerprogram
This role is also eligible for the Re-Empower Program. The Re-Empower Program helps support talented and committed professionals as they rebuild their capabilities, enhance leadership skills, and continue their professional journey. Over the course of the 14-week program, experienced professionals will gain paid, on-the-job experience, have an opportunity to participate in sessions with leadership, develop personalized plans for success and receive coaching to guide their return-to-work experience. Upon completion of the program, based on performance and contributions participants will be eligible for a career at RTX.
Minimum Program Qualifications
Be on a career break of one or more year at time of application
Have prior experience in functional area of interest
Have interest in returning in either a full-time or part-time position
Diversity drives innovation; inclusion drives success
. We believe a multitude of approaches and ideas enable us to deliver the best results for our workforce, workplace, and customers. We are committed to fostering a culture where all employees can share their passions and ideas so we can tackle the toughest challenges in our industry and pave new paths to limitless possibility.
WE ARE REDEFINING AEROSPACE.
Please consider the following role type definitions as you apply for this role.
Onsite:
Employees who are working in Onsite roles will work primarily onsite. This includes all production and maintenance employees, as they are essential to the development of our products.
Regardless of your role type, collaboration and innovation are critical to our business and all employees will have access to digital tools so they can work with colleagues around the world – and access to Collins sites when their work requires in-person meetings.
Some of our competitive benefits package includes:
Medical, dental, and vision insurance
Three weeks of vacation for newly hired employees
Generous 401(k) plan that includes employer matching funds and separate employer retirement contribution, including a Lifetime Income Strategy option
Tuition reimbursement program
Student Loan Repayment Program
Life insurance and disability coverage
Optional coverages you can buy: pet insurance, home and auto insurance, additional life and accident insurance, critical illness insurance, group legal, ID theft protection
Birth, adoption, parental leave benefits
Ovia Health, fertility, and family planning
Adoption Assistance
Autism Benefit
Employee Assistance Plan, including up to 10 free counseling sessions
Healthy You Incentives, wellness rewards program
Doctor on Demand, virtual doctor visits
Bright Horizons, child and elder care services
Teladoc Medical Experts, second opinion program
And more!
At Collins, the paths we pave together lead to limitless possibility. And the bonds we form – with our customers and with each other -- propel us all higher, again and again.
Apply now and be part of the team that’s redefining aerospace, every day.
RTX is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.
Privacy Policy and Terms:
Click on this link to read the Policy and Terms
01659815
Show more
Show less","Software Engineering, C++, Python, Windows, Linux, Object Oriented Architectures, Agile Development, Atlassian Tools, CI/CD, LVC Communication Standards, Git, SVN, Jira, Bitbucket, Bamboo, Confluence","software engineering, c, python, windows, linux, object oriented architectures, agile development, atlassian tools, cicd, lvc communication standards, git, svn, jira, bitbucket, bamboo, confluence","agile development, atlassian tools, bamboo, bitbucket, c, cicd, confluence, git, jira, linux, lvc communication standards, object oriented architectures, python, software engineering, svn, windows"
Azure Data Architect,CGI,"Lafayette, LA",https://www.linkedin.com/jobs/view/azure-data-architect-at-cgi-3768829459,2023-12-17,Lafayette,United States,Mid senior,Onsite,"Position Description
CGI has an immediate need for an Azure Data Architect. Become part of a team to help a large data warehouse cloud transformation to support the digital transformation capabilities and data analytics reporting capabilities for one of CGIs financial services clients. CGI is seeking an Enterprise Data Architect to support this large transformation project.
Location: This is a remote role that will require travel to the client site during the transformation project.
Your future duties and responsibilities
How you’ll make an impact
Lead by example and be hands on working closely with the team giving them technical direction and help with issue resolutions to keep the team focused on the solution
Facilitate the establishment and execution of the roadmap and vision for information delivery and management; including the enterprise data warehouse, big data, BI & analytics, content management and data management.
Lead data engineers to architect and deliver the solutions that fulfill the Business information needs and align with the information vision and strategy.
Apply business strategy while driving technology strategy, balancing short term and long term needs to ensure that the architecture can scale and evolve accordingly.
Engage an end-to-end approach by connecting all the pieces of data to deliver the data while leveraging all available assets.
Provide advice, guidance, direction, and authorization to carry out major plans and procedures to ensure schedule attainment, product development process adherence, and performance and budget targets are met.
Take an active role in the performance management process by completing goal setting, interim review, and year-end review on a timely basis and soliciting ongoing feedback
Required Qualifications To Be Successful In This Role
What you’ll bring
8-10+ years of hands-on experience as an Data Architect, Data Engineer (Lead Developer), or similar role for large scale, complex enterprise data warehousing solutions using agile practices. Experience with both development and infrastructure efforts.
Solid understanding of and experience with provisioning and managing infrastructure as well as data applications in Azure environments.
Have experience in implementing data warehouse cloud transformations and data analytics tools like Power BI and Tableau.
Assist with performing security assessments and in mitigating risks during development and, implementation
Experience with technologies like Data Factory, Jenkins, Gradle, Maven, DevSecOps and related Infrastructure as Code (IaC) tools, JIRA, including Docker, Kubernetes, OpenShift, Jenkins, GitHub, Puppet, Git, Terraform, Splunk and others.
Strong written and verbal communication skills.
Experience in working in an agile environment
Experience in leading by example and being a “hands on” member of the team
Experience in directing and managing technical solutioning
Bachelor’s degree in IT, Computer Science or related field
#DICE
Let’s Talk About Benefits
Competitive compensation
Comprehensive insurance options
Matching contributions through the 401(k) plan and the share purchase plan
Paid time off for vacation, holidays, and sick time
Paid maternity and parental leave
Learning opportunities and tuition assistance
Member assistance and wellness programs
“CGI is required by law in some jurisdictions to include a reasonable estimate of the compensation range for this role. The determination of this range includes various factors not limited to skill set, level, experience, relevant training, and licensure and certifications. To support the ability to reward for merit-based performance, CGI typically does not hire individuals at or near the top of the range for their role. Compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for this role in the U.S. is $97,700 - $192,400.”
Insights you can act on
While technology is at the heart of our clients’ digital transformation, we understand that people are at the heart of business success.
When you join CGI, you become a trusted advisor, collaborating with colleagues and clients to bring forward actionable insights that deliver meaningful and sustainable outcomes. We call our employees “members” because they are CGI shareholders and owners and owners who enjoy working and growing together to build a company we are proud of. This has been our Dream since 1976, and it has brought us to where we are today — one of the world’s largest independent providers of IT and business consulting services.
At CGI, we recognize the richness that diversity brings. We strive to create a work culture where all belong and collaborate with clients in building more inclusive communities. As an equal-opportunity employer, we want to empower all our members to succeed and grow. If you require an accommodation at any point during the recruitment process, please let us know. We will be happy to assist.
Ready to become part of our success story? Join CGI — where your ideas and actions make a difference.
Qualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, pregnancy, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status, political affiliation, genetic information, height, weight, or any other legally protected status or characteristics.
CGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com. You will need to reference the Position ID of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you.
Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a Position ID will not be returned
.
We make it easy to translate military experience and skills! Click here to be directed to our site that is dedicated to veterans and transitioning service members.
All CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held. CGI will consider for employment qualified applicants with arrests and conviction records in accordance with all local regulations and ordinances.
CGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGI’s legal duty to furnish information.
Show more
Show less","Azure, Data Architect, Data Warehousing, Big Data, BI, Analytics, Content Management, Data Management, Data Factory, Jenkins, Gradle, Maven, DevSecOps, Infrastructure as Code (IaC), JIRA, Docker, Kubernetes, OpenShift, GitHub, Puppet, Git, Terraform, Splunk, Power BI, Tableau","azure, data architect, data warehousing, big data, bi, analytics, content management, data management, data factory, jenkins, gradle, maven, devsecops, infrastructure as code iac, jira, docker, kubernetes, openshift, github, puppet, git, terraform, splunk, power bi, tableau","analytics, azure, bi, big data, content management, data architect, data factory, data management, datawarehouse, devsecops, docker, git, github, gradle, infrastructure as code iac, jenkins, jira, kubernetes, maven, openshift, powerbi, puppet, splunk, tableau, terraform"
Senior System/Data Analyst,Mastech Digital,"Lafayette, LA",https://www.linkedin.com/jobs/view/senior-system-data-analyst-at-mastech-digital-3726982873,2023-12-17,Lafayette,United States,Mid senior,Hybrid,"Description
Mastech Digital
provides digital and mainstream technology staff as well as Digital Transformation Services for all American Corporations. We are currently seeking a
Senior System/Data Analyst
for our client in the
IT Services
domain. We value our professionals, providing comprehensive benefits and the opportunity for growth. This is a
Permanent
position, and the client is looking for someone to start immediately.
Duration:
Full Time
Location:
Lafayette (LA), Columbia (SC), Remote
Role: Senior System/Data Analyst
Primary Skills: Data Analysis
Role Description:
The
Senior System/Data Analyst
must have at least 6+ years of experience.
Required Experience And Skills
Looking for 7+ years of IT experience.
Required Data Analysis with strong SQL skills and mortgage background.
Need strong SQL experience to write complex queries.
Good knowledge or experience in Python is required for this position.
Required 2 years of data engineer experience in EMR, RedShift, and Glue.
Works well with an Agile Scrum team and requires little to no handholding.
Experienced with data warehouse background (data lakes, data mining, and data mart).
Experience with cloud dev concepts, particularly AWS is required.
Excellent PL/SQL skills (including complex SQL queries - Oracle analytic functions like RANK, LAG, LEAD, LISTAGG).
Good to Have GSE's Mortgage business.
Has cross-functional skills who will step in to fill a gap when needed (i.e., help with business analysis, testing, etc.).
Education:
Bachelor’s degree in Computer Science, Electrical/Electronic Engineering, Information Technology or another related field or Equivalent
Experience:
Minimum 6+ years of experience
Relocation:
This position will not cover relocation expenses
Travel:
No
Local Preferred:
Yes
Note: Must be able to work on a W2 basis (No C2C)
Recruiter Name: Vinti Seth
Recruiter Phone: 1 412.214.8805
Equal Employment Opportunity
Show more
Show less","Data Analysis, SQL, Python, EMR, Redshift, Glue, Agile Scrum, Data warehouse, Data lakes, Data mining, Data mart, Cloud dev, AWS, PL/SQL, Oracle analytic functions, GSE's Mortgage business, Business analysis, Testing, Computer Science, Electrical/Electronic Engineering, Information Technology","data analysis, sql, python, emr, redshift, glue, agile scrum, data warehouse, data lakes, data mining, data mart, cloud dev, aws, plsql, oracle analytic functions, gses mortgage business, business analysis, testing, computer science, electricalelectronic engineering, information technology","agile scrum, aws, business analysis, cloud dev, computer science, data lakes, data mart, data mining, dataanalytics, datawarehouse, electricalelectronic engineering, emr, glue, gses mortgage business, information technology, oracle analytic functions, plsql, python, redshift, sql, testing"
Senior Data Analyst - Quality Consultant,CVS Health,"Oklahoma City, OK",https://www.linkedin.com/jobs/view/senior-data-analyst-quality-consultant-at-cvs-health-3778694735,2023-12-17,Oklahoma,United States,Mid senior,Remote,"Bring your heart to CVS Health.
Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.
Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
Position Summary
Aetna Better Health of Oklahoma, a CVS Health company, is a trusted health partner in the local Oklahoma communities we serve. We provide a full array of innovative services that enhance overall wellness and improve everyday life for our members. At Aetna Better Health of Oklahoma, we value professional development and career growth. You will work along other colleagues who align on Heart at Work behaviors and bringing your heart to every moment of health. We will support you all the way!
This is a fulltime teleworker opportunity in Oklahoma. Instate travel may be required based on business needs; including travel to the Oklahoma City office.
The Sr. Data Analyst, Quality Consultant works closely with business partners to identify and improve key business processes and improve member experience. Serves as an operations champion through measuring and monitoring the KPIs and effectiveness of operational processes that impact customer satisfaction, cost management, and operational efficiency. You will evaluate reporting and management dashboards, perform data analysis, develop workflows, and offer support for assigned initiatives that impact the delivery of products and services to internal and external customers.
Applies critical and analytical methods and procedures to identify root causes to recommend, assist in implementing and measuring durable solutions.
Ability to understand healthcare data operations and systems
Experience with data manipulation, data visualization, and presentation.
Act as liaison between internal business units to facilitate new business process plans.
Manage multiple initiatives with multiple customer priorities and successfully meet targeted deadlines.
Communication with all levels of management and relevant stakeholders to encourage internal problem solving.
Collaborate with management to create new processes and document them for future use.
Drive process improvements and effectively manage change with demonstrated ability to challenge status quo.
Proactively identify inefficiencies and process improvement opportunities.
Presentation of findings to both small groups and larger audiences.
Performs other duties as assigned.
Required Qualifications
Must reside in Oklahoma.
3+ years of project management skills and experience.
3+ years’ experience working with data, data analytics or data management systems.
Demonstrated ability to facilitate cross-functional process improvement teams.
Ability to manage multiple initiatives with multiple customer priorities and successfully meet targeted deadlines.
Strong capability to communicate with all levels of management and large groups to achieve desired outcomes.
Must possess strong presentation and communication skills, verbal and written.
Ability to proactively identify inefficiencies and process improvement opportunities.
Ability to collaborate with small teams and cross functionally across business units.
2+ years’ experience using personal computer, keyboard navigation, navigating multiple systems and applications; and using MS Office Suite applications (Teams, Outlook, Word, Excel, PowerPoint, SharePoint, etc.).
Must possess reliable transportation and be willing and able to travel in-state up to 10% of the time. Mileage is reimbursed per our company expense reimbursement policy.
Preferred Qualifications
Experience with Quickbase, or PowerBI.
Healthcare or managed care experience.
Medicaid experience.
Knowledge of data management systems.
Bachelor’s degree preferred.
Education
Bachelor’s Degree in health informatics, information technology, computer science, statistics, applied mathematics, or equivalent relevant work experience.
Pay Range
The typical pay range for this role is:
$43,700.00 – $90,000.00
This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.
In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.
For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits
CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services through ColleagueRelations@CVSHealth.com If you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.
Show more
Show less","Project Management, Data Analytics, Data Visualization, MS Office Suite (Teams Outlook Word Excel PowerPoint SharePoint), Presentation Skills, Communication Skills, Collaborative Work, Analytical Thinking, Efficiency Optimization, Process Improvement, Problem Solving, Healthcare Data Operations, Healthcare Systems, Healthcare Informatics, Information Technology, Computer Science, Statistics, Applied Mathematics, Quickbase, PowerBI","project management, data analytics, data visualization, ms office suite teams outlook word excel powerpoint sharepoint, presentation skills, communication skills, collaborative work, analytical thinking, efficiency optimization, process improvement, problem solving, healthcare data operations, healthcare systems, healthcare informatics, information technology, computer science, statistics, applied mathematics, quickbase, powerbi","analytical thinking, applied mathematics, collaborative work, communication skills, computer science, dataanalytics, efficiency optimization, healthcare data operations, healthcare informatics, healthcare systems, information technology, ms office suite teams outlook word excel powerpoint sharepoint, powerbi, presentation skills, problem solving, process improvement, project management, quickbase, statistics, visualization"
EOI - Data Analyst - Contract,ConocoPhillips,"Bartlesville, OK",https://www.linkedin.com/jobs/view/eoi-data-analyst-contract-at-conocophillips-3687034901,2023-12-17,Oklahoma,United States,Mid senior,Hybrid,"The Data Analyst expression of interest (EOI) job is used to fill high-volume roles or frequently hired positions to be used for future openings. By filling out an EOI job application you are joining our Talent Community, where you have the opportunity to be matched with openings that are the right fit for you.
ConocoPhillips is a leading oil and gas company, based in Houston, TX, and is hiring Data Analysts for contract positions. The Data Analyst is responsible for providing quality data to the businesses and functions in an efficient manner so that critical decisions can be made effectively and efficiently.
Key Accountabilities
Responsible for providing quality data to businesses and functions in an efficient manner so that critical decisions can be made effectively and efficiently.
Supports, promotes, and documents all Data Management processes and procedures.
Develops workflows and tools that automate data loading processes and help ensure data quality and integrity.
Ensures the quality, consistency, and integrity of data in a timely, effective and reliable manner.
Receives, archives distributes, loads, and disposes of technical data according to policies, standards, and procedures.
Required And Preferred Skills And Experience
Entry level, typically 0-3 years of experience.
Requires basic knowledge of discipline.
Uses established procedures to solve standard, straight-forward problems.
Has the ability to perform routine assignments based on detailed instructions/past practices.
ConocoPhillips contract opportunities are for project-based or other short-term engagements that require specialized skills. Successful candidates for contract opportunities will not be considered employees of ConocoPhillips or any of its subsidiaries nor will candidates be eligible for employment benefits. Candidates looking for regular full-time employment opportunities should begin their search here:
https://careers.conocophillips.com/
Show more
Show less","Data Analysis, Data Management, Data Quality, Data Integrity, Data Loading, Data Distribution, Data Disposal, Data Archiving, Workflows, Tools, Automation, Standard Procedures, Routine Assignments","data analysis, data management, data quality, data integrity, data loading, data distribution, data disposal, data archiving, workflows, tools, automation, standard procedures, routine assignments","automation, data archiving, data disposal, data distribution, data integrity, data loading, data management, data quality, dataanalytics, routine assignments, standard procedures, tools, workflows"
Technical Data Developer III,ECS,"Norman, OK",https://www.linkedin.com/jobs/view/technical-data-developer-iii-at-ecs-3779922411,2023-12-17,Oklahoma,United States,Mid senior,Hybrid,"ECS is seeking a
Technical Data Developer III
to work in our
Norman, OK
office .
Job Description:
Utilizes engineering and OEM source documentation to research, develop, publish, and maintain Technical Documentation such as maintenance procedures, assembly, subassembly, and component-level configuration information, and troubleshooting workflows for Industrial/Commercial Mail Processing Equipment to include desktop or on-location verification and on-equipment validation tasks with little to no support from Subject Matter Expert(s).
Identifies contractual noncompliance issues, informational inaccuracies, and documentation inconsistencies in Maintenance Documentation Deliverables such as technical and engineering source data and takes corrective action(s) via detailed correspondence for submittal to Customer’s Program Office.
Works with a small team to research, develop, publish, and maintain Technical Documentation while adhering to prescribed schedules and budgets accordingly.
Travels within the United States approximately 25% annually to support or perform maintenance documentation related tasks and technical information collection.
Required Skills:
Must be able to obtain a Public Trust Clearance.
Working knowledge of System Engineering or Engineering Lifecycle Management.
Proficient with MS Office or other productivity software suites.
Proficient in technical and/or engineering documentation development to include having a baseline level of writing proficiency or ability to apply stylistic writing requirements based on a set of guidelines.
Possess demonstrable combination of technical training/education and general working level knowledge of industrial equipment systems (mechanical, electrical, electromechanical, software, and machine communications).
Desired Skills:
Familiar with graphics and photo editing software suites.
Familiar with CAD editing software suites.
Proficient with XML authoring tools such as XMetaL or Adobe FrameMaker.
Proficient with ServiceNow or other workflow management platforms.
Working knowledge of Agile Methodology practices including performing and training Scrum Master duties.
Associates or higher-level degree preferred. Will consider a combination of education, technical / military training, professional certifications, and work experience.
ECS is an equal opportunity employer and does not discriminate or allow discrimination on the basis of race, color, religion, gender, age, national origin, citizenship, disability, veteran status or any other classification protected by federal, state, or local law. ECS promotes affirmative action for minorities, women, disabled persons, and veterans.
ECS is a leading mid-sized provider of technology services to the United States Federal Government. We are focused on people, values and purpose. Every day, our 3800+ employees focus on providing their technical talent to support the Federal Agencies and Departments of the US Government to serve, protect and defend the American People.
Show more
Show less","Public Trust Clearance, System Engineering, Engineering Lifecycle Management, MS Office, Technical Documentation Development, Writing Proficiency, Industrial Equipment Systems, Graphics Editing Software, Photo Editing Software, CAD Editing Software, XML Authoring Tools, ServiceNow, Workflow Management Platforms, Agile Methodology, Scrum Master, Associates Degree","public trust clearance, system engineering, engineering lifecycle management, ms office, technical documentation development, writing proficiency, industrial equipment systems, graphics editing software, photo editing software, cad editing software, xml authoring tools, servicenow, workflow management platforms, agile methodology, scrum master, associates degree","agile methodology, associates degree, cad editing software, engineering lifecycle management, graphics editing software, industrial equipment systems, ms office, photo editing software, public trust clearance, scrum master, servicenow, system engineering, technical documentation development, workflow management platforms, writing proficiency, xml authoring tools"
Data Analyst,Four Squared Recruitment,"Stourport-on-Severn, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-four-squared-recruitment-3784547019,2023-12-17,Telford, United Kingdom,Associate,Hybrid,"Data Analyst
Stourport
£25,000-£30,000
Four-Squared Recruitment have partnered with a leading manufacturing company to recruit a Data Analyst, providing analytical and statistical support to improve processes and productivity.
This is a great opportunity for someone to progress there career in one of the largest manufacturing companies in the UK, contributing to the success and improvement of the business' productivity and functionality.
Responsibilities
· Gather production data, MRP output info, H&S statistics and produce reports and highlight discussion points
· Work with Excel and extracted data from the MRP system
· Generate and analyse daily, weekly and monthly KPI reports - including productivity, hours worked, time and attendance, etc
· Accurately input data into Excel and produce graphs and pivot tables
· Analyse data and identify trends to help improve the production team’s output
Requirements
· Strong attention to detail and accuracy in data entry and management
· An analytical and numerical mind set
· Proficient in using database management systems and tools
· Knowledge of basic mechanical principles is a plus
· Familiarity with manufacturing MRP systems is preferred
· Excellent organisational skills with the ability to prioritise tasks effectively
· Strong analytical and problem-solving abilities
Data Analyst
Stourport
£25,000-£30,000
If you feel like this is the role for you please APPLY, alternatively contact james.may@four-squared.com for more information!
Show more
Show less","Data Analysis, Data Management, Statistics, Excel, MRP, Database Management, Analytical Thinking, Problem Solving, Pivot Tables, Trend Analysis, KPI Reporting, Data Entry, Visualization, Mechanical Principles","data analysis, data management, statistics, excel, mrp, database management, analytical thinking, problem solving, pivot tables, trend analysis, kpi reporting, data entry, visualization, mechanical principles","analytical thinking, data entry, data management, dataanalytics, database management, excel, kpi reporting, mechanical principles, mrp, pivot tables, problem solving, statistics, trend analysis, visualization"
Senior Data Engineer,Nigel Frank International,"Shrewsbury, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-nigel-frank-international-3731259654,2023-12-17,Telford, United Kingdom,Mid senior,Onsite,"I am working with a market leading manufacturer of specialist medical equipment who are looking for a Senior Data Engineer at the start of a number of exciting data driven projects. You would be joining at the start of a greenfield project where you will be responsible for the data migration to the brand new Azure Data Platform!
The overall goal of the business and ultimately this role is to deliver data to healthcare professionals to aid with rehabilitation. You will join a close-knit team, with talented professionals who specialise in a number of areas including software development and data science. You will become the expert for this team on all things data engineering and will take the lead on the creation, development and maintenance of the Azure data platform.
As part of this role, you will be responsible for some of the following areas.
Design, maintain and optimise on premise database solutions.
Develop and maintain robust ETL pipelines.
Work with other specialists within the team to make technical decisions that will benefit the overall business.
Take the lead of migration of data to the new Azure data platform.
This is a salaried role paying up to £65,000 per annum depending on experience and a company benefits package. This role would be largely a remote opportunity with occasional visits to the office in Stafford. These visits would be as and when needed in line with project requirements.
To be successful in the role you will have.
Strong knowledge of Databricks for data ingestion and transformation.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Strong coding experience with languages such as Python or C#.
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Databricks, Azure Data Factory, Synapse, Azure Data Lake, ETL, Python, C#, Azure Data Platform, Azure, SQLBits, Power Platform World Tour","databricks, azure data factory, synapse, azure data lake, etl, python, c, azure data platform, azure, sqlbits, power platform world tour","azure, azure data factory, azure data lake, azure data platform, c, databricks, etl, power platform world tour, python, sqlbits, synapse"
Back Office Data Analyst - Commercial Trading m/f/t,RWE,"Swindon, England, United Kingdom",https://uk.linkedin.com/jobs/view/back-office-data-analyst-commercial-trading-m-f-t-at-rwe-3762806404,2023-12-17,Telford, United Kingdom,Mid senior,Onsite,"To start as soon as possible,
About You
If you have a passion for data, data quality and consider maintenance to be part of this, then we might have the right opportunity for you.
The Role
We are growing our Back Office Services Operational team at RWE Supply and Trading in Swindon. You will support the operation of our international energy trading business by ensuring data quality of our master data.
This includes:
Setup and maintain counterparty master data, as well as contracts in our IT systems to ensure smooth execution of our business processes across IT systems
Support and coordinate our master data management process, incl. maintenance of process documentation and performance indicators; introduce and explain process improvements to users and other process stakeholders
Close collaboration with stakeholders and other departments and coordination of knowledge exchange
Your profile
Passion for or experience in Data Management & Data Quality related topics
Basic knowledge and/or experience in one of the following applications: Office 365, Endur, SAP, Brady, Tableau, Oracle
Strong analytical skills and great attention to detail, paired with curiosity and willingness to learn
Fluent in English, both verbally and written; fluent in German is a plus
Basic knowledge of energy trading and products is a plus
Willingness to travel (also abroad)
Our offer
You will be working in an international and cross-locational team (DE and UK) using latest Web 2.0 technologies (Chat, Wiki, etc.).
You’ll become part of dynamic and modern company culture, at Europe’s biggest energy trader, with flat hierarchies, close collaboration between teams and departments, and manifold development opportunities.
Be the heart and soul of the energy revolution. Be the future. Apply now. It’s time to choose what kind of world you want.
Apply now with just a few clicks:
ad code
83677
,
Any questions?
Contact in Recruiting:
Vince Manning,
Contact in the specialist department:
Ian Nash
We value diversity and therefore welcome all applications, irrespective of gender, disability, nationality, ethnic and social background, religion and beliefs, age or sexual orientation and identity. #inclusionmatters
Show more
Show less","Data Management, Data Quality, Master Data Management, Data Quality Management, Process Management, Data Analysis, Office 365, Endur, SAP, Brady, Tableau, Oracle, English, German, Energy Trading, Web 2.0, Communication, Leadership, Collaboration, Problem Solving, Attention to Detail, Analytical Skills","data management, data quality, master data management, data quality management, process management, data analysis, office 365, endur, sap, brady, tableau, oracle, english, german, energy trading, web 20, communication, leadership, collaboration, problem solving, attention to detail, analytical skills","analytical skills, attention to detail, brady, collaboration, communication, data management, data quality, data quality management, dataanalytics, endur, energy trading, english, german, leadership, master data management, office 365, oracle, problem solving, process management, sap, tableau, web 20"
Data Architect,IRIS | Networx | Recruitment Software & Services,"The Home, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-architect-at-iris-networx-recruitment-software-services-3750654523,2023-12-17,Telford, United Kingdom,Mid senior,Hybrid,"Hybrid
Department
Sitting within Product & Technology, Enterprise Data Services is responsible for providing data, reporting, integration, process automation and CRM capabilities across Cox Automotive Europe.
We own the data and business platforms, design and develop reusable enterprise assets, and drive best practices to support adoption and effective utilisation of the shared enterprise data capabilities.
Job Role
As a Data Architect you will be working with the key stakeholders across the company turning business and customer problems into effective data designs. Your focus will be on building out strong data foundations and connecting the dots across the enterprise with relentless focus on delivering internal and external value.
You will act as the key data expert on our strategic initiatives and major projects to ensure data-first approach. You will establish and champion the best practices around data standards, modelling, and governance.
You will be on hand to support troubleshooting and resolving complex data issues for business processes that cut across multiple systems and business units.
Data Best Practice Frameworks
Develop and set data standards / best practices across technology, product, and business taking enterprise first approach, identify where local exceptions might be required.
Champion these standards, provide oversight and advice on implementing these. Communicate the business benefit of data standards.
Build a framework for governing and monitoring the standards across the organisation. Align with and proactively support wider data governance and management efforts.
Data Profiling & Modelling
Undertake data profiling and source system analysis. Reverse engineer and documents data models for the key systems.
Design new enterprise data models using industry-recognised data modelling patterns and standards.
Drive opportunities for the reuse and alignment of data models across the enterprise.
Provide data modelling support for individual products and solutions.
Data Solution Design
Provide high level design for new enterprise data solutions that balance security, scalability, fault-tolerance, performance as well as cost effectiveness.
Define, document and champion best practice architectural patterns and work with wider technology development teams to proactively seek out new opportunities to utilise our technology and data assets.
Review existing solutions to see whether improvements can be made, weighing up the cost of changing the existing solution against perceived benefits.
Data Troubleshooting
Provide guidance and support around conducting root cause analysis of data issues for complex integrations spanning multiple systems and business units and covering both technology and business process aspects of data flows.
Co-ordinate across different teams to resolve problems identified and implement any necessary preventative measures.
Stakeholder Management
Act as a trusted advisor on data technologies to senior stakeholders including the Board.
Act as a point of contact for data best practices and work with both technical and non-technical stakeholders from the project inception to final implementation to offer technological steer and help design, develop, and maintain best of breed data solutions.
Work with the relevant technology, product, and business teams to ensure all technical prerequisites for data deliverables are met and use your influence to remove any major blockers.
Technical Leadership
Demonstrate self-direction and initiative in all aspects of the role, investigating new industry practices and technology and engaging, educating, and influencing the business around value of data.
Support internal communities of practice, work alongside your peers to provide technical leadership and direction for a company’s product and engineering teams.
Provide coaching and mentoring to Enterprise Data Services team members and beyond where required.
Skills & Knowledge
Experience in a similar role, preferably in a fast paced, global, multi-brand automotive or retail environment.
Experience supporting major enterprise business change initiatives form data perspective, developing and applying data best practices.
Demonstrable experience in implementing and driving data quality management, designing data security & privacy controls, and supporting wider data strategy efforts.
Experience implementing and managing best practices across data management lifecycle.
Excellent data modelling skills across different level of abstraction and target applications. Experience creating and managing canonical data models at Enterprise level across multiple business units.
Experience implementing data management tooling and platforms covering data quality, lineage, cataloguing, access management and GDPR compliance.
Strong knowledge of industry best practices around data architecture in both cloud-based and on-premises solutions. This included good exposure to data technologies in the cloud across multiple providers (Azure, AWS, GCP).
Experience working across and integrating different data platforms including big data, reporting, integration, process automation and CRM capabilities.
Excellent interpersonal skills, verbal and written communication skills when working with both business and technical teams.
An influencer who can articulate arguments well and is comfortable working with a wide range of people from different areas of the business and at different levels of seniority.
A strong natural focus on what matters; delivering value and designing to minimise risk.
Hands-on experience of working through the end-to-end project lifecycle with exposure to both Waterfall and Agile methodologies.
The ability to work across multiple projects in parallel and the ability to contribute to wider group strategies.
Strictly No Agencies Please
We work with a carefully selected set of recruitment agencies and we're not looking to add to our PSL.
We do not accept unsolicited agency CV's sent to the recruitment team or directly to the hiring manager. We will not be responsible for any fees related to unsolicited CV's
#INDTR
Show more
Show less","Agile, Azure, AWS, Big Data, CRM, Cloud, Data Architecture, Data Governance, Data Management, Data Modelling, Data Profiling, Data Quality Management, Data Security, Data Troubleshooting, Database Management, GCP, GDPR Compliance, Industry Best Practices, Integration, Process Automation, Reporting, Software Development, Waterfall","agile, azure, aws, big data, crm, cloud, data architecture, data governance, data management, data modelling, data profiling, data quality management, data security, data troubleshooting, database management, gcp, gdpr compliance, industry best practices, integration, process automation, reporting, software development, waterfall","agile, aws, azure, big data, cloud, crm, data architecture, data governance, data management, data modelling, data profiling, data quality management, data security, data troubleshooting, database management, gcp, gdpr compliance, industry best practices, integration, process automation, reporting, software development, waterfall"
Data Scientist (Geoscience),Dassault Systèmes,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-scientist-geoscience-at-dassault-syst%C3%A8mes-3788302756,2023-12-17,Queensland, Australia,Associate,Hybrid,"Role Description & Responsibilities
Reporting to the Geoscience Interpretation Manager, you will develop advanced cloud-native systems to extract, transform, and clean data for complex geoscience models.
You will collaborate with data engineers, data scientists, software engineers, business leaders and other partners to implement data science engineering solutions based on our priorities and technology projects
You will solve complex business questions where situations or data require in-depth evaluation of variable factors.
You will understand business requirements, assess the level of effort, and break down the development solution to the granular task level.
You will design and support data pipelines to serve product engineering and Geoscience use cases (optional)
Qualifications
Master degree or PHD in data science
At least 3 – 5 years of experience as Data Scientist
Experience in Algorithms and tools (Python and SQL)
Ability to understand Business requirement, design and product engineering
Knowledge about Software Engineering (Optional)
What’s in it for you
Work for the one of the biggest software company in Europe
Gain exposure to a wide variety of industry experiences and IT technologies
An international work environment with brilliant colleagues around the globe
Hybrid ""Work from home"" arrangement in compliance with company policies and guidelines
Commitment to diversity and inclusion
Dynamic career development policy: training plan, internal mobility, etc.
Diversity statement
As a game-changer in sustainable technology and innovation, Dassault Systèmes is striving to build more inclusive and diverse teams across the globe. We believe that our people are our number one asset and we want all employees to feel empowered to bring their whole selves to work every day. It is our goal that our people feel a sense of pride and a passion for belonging. As a company leading change, it’s our responsibility to foster opportunities for all people to participate in a harmonized Workforce of the Future.
Show more
Show less","CloudNative Systems, Data Extraction, Data Transformation, Data Cleaning, Data pipelines, Python, SQL, Algorithms, Data Science, Software Engineering","cloudnative systems, data extraction, data transformation, data cleaning, data pipelines, python, sql, algorithms, data science, software engineering","algorithms, cloudnative systems, data cleaning, data extraction, data science, data transformation, datapipeline, python, software engineering, sql"
Senior Cloud Data engineer,ManVision Consulting,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/senior-cloud-data-engineer-at-manvision-consulting-3773087402,2023-12-17,Queensland, Australia,Mid senior,Onsite,"Role:
Senior Cloud Data engineer
Location:
Brisbane,Queensland,Australia
Job Type:
Contract
Requirements:
Hands-on experience with designing, building and maintaining data storage
Hands-on experience with Snowflake
Advanced Technical proficiency required in AWS Services, e.g. AWS DynamoDB, AWS Glue, AWS IAM, AWS Lambda, ECS, AWS Step Function, AWS Batch, AWS S3 Bucket,
Experience with Azure Services, e.g. Azure SQL Database, Azure Cosmo DB, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Blob Storage,
Working experience in database development, data engineering (ETL in Cloud), and experience with handling JSON, CSV files etc.,
Understanding of information/data management, data structures, data storage, and data modelling techniques,
Advanced proficiency in Programming languages – Python and Object Oriented Design, Powershell, Bash Scripting Skills,
Extensive experience in SQL, including designing and optimizing complex queries,
Working experience in Terraform,
Experience with CI/CD pipelines.
LinkedIn: https://www.linkedin.com/in/amara-ramya-sree-ab655a206/
If you have a CV, click apply to be immediately considered. Email : ramya.s@manvision.net
Show more
Show less","Snowflake, AWS Services (DynamoDB Glue IAM Lambda ECS Step Function Batch S3 bucket), Azure Services (SQL Database Cosmo DB Data Factory Logic Apps Functions Blob Storage), ETL, Data Engineering, JSON, CSV, Information Management, Data Structures, Data Storage, Data Modeling, Python, ObjectOriented Design, Powershell, Bash Scripting, SQL, Terraform, CI/CD","snowflake, aws services dynamodb glue iam lambda ecs step function batch s3 bucket, azure services sql database cosmo db data factory logic apps functions blob storage, etl, data engineering, json, csv, information management, data structures, data storage, data modeling, python, objectoriented design, powershell, bash scripting, sql, terraform, cicd","aws services dynamodb glue iam lambda ecs step function batch s3 bucket, azure services sql database cosmo db data factory logic apps functions blob storage, bash scripting, cicd, csv, data engineering, data storage, data structures, datamodeling, etl, information management, json, objectoriented design, powershell, python, snowflake, sql, terraform"
Senior Data Engineer,Comparethemarket.com.au,"Toowong, Queensland, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-comparethemarket-com-au-3775980191,2023-12-17,Queensland, Australia,Mid senior,Onsite,"Brief Description
#S-DNI
About The Company
Compare the Market, yep, the one with the meerkats, is on an exciting growth trajectory and we are inviting talented Senior Data Engineers to join us in making a significant impact on the lives of people all over Australia.
At CTM, you won't just be an employee; you'll be a crucial part of our mission to empower our customers to make informed purchasing decisions across various insurance, finance, and household product categories; thereby improving and simplifying their lives, while saving them time, effort, and money.
We are all about Integrity, Growth, Excellent Customer Service, and being Peerless - without compromise.
About The Role
As CTM continues to enhance our modern data platform (Databricks/AWS), we want to ensure we can best service our analytical use-cases and drive value for the business. Reporting to the Head of Data Science, you will have the opportunity to establish foundational engineering principles and provide technical leadership to support our business intelligence, data science, and machine learning practices.
As a Senior Data Engineer, in addition to your strong hands-on development experience, you’ll be able to provide guidance around architecture and solution design. You will contribute to the data strategy from an engineering perspective and help the broader team in the development of the data platform and overall ecosystem. You will work within and be supported by a great team that includes data scientists, business intelligence developers, data engineers, data architects, data modellers, and business analysts, in addition to the broader IT and business teams.
What You’ll Do
Work with data producers, data consumers, and other supporting roles to ensure data is well managed for consumption.
Build, maintain, and optimize high quality data pipelines for serving analytics use-cases, including business intelligence and machine learning.
Work with architects to develop architectural patterns for data pipelines, including for event based real-time processing.
Support the development and implementation of data engineering roadmap.
Champion best practices for data management and data governance. This includes data modelling; ETL and processing performance; data quality, observability, and discoverability; security and sensitive data handling; and codebase quality.
What You’ll Bring
5+ years of experience as a professional data engineer or analytics engineer, developing ETL pipelines for production environments.
Solid experience building analytical data pipelines in cloud systems.
Solid data management experience covering data modelling, data quality monitoring, maintaining data dictionaries, row/column level security, sensitive data masking or tokenization, and access controls.
Ability to write high quality code for data pipelines, preferably in Python.
Excellent SQL skills.
Experience with CI/CD and Infrastructure as Code builds.
Experience providing data engineering leadership and mentorship.
Solid written and verbal communication skills.
Highly Regarded:
Experience developing on Databricks and/or using Spark or PySpark.
Experience building robust feature stores with batch and streaming data for use by machine learning models.
Experience building highly automated metadata driven solutions.
What We Enjoy At Compare The Market
Whether we're writing a single line of code or capturing requirements, our dedication to our customers is unwavering, and is at the very heart of all we do.
At CTM, our workplace is both diverse and inclusive, and we cultivate a culture that is collaborative and innovative within our close-knit teams. We are a “work from the office” company as we believe that performance, culture, innovation and collaboration are at their best when we’re working together in person.
If you are a high-performing individual, with a passion for problem-solving and a desire to make a positive difference, we encourage you to apply. Your expertise, combined with our supportive environment and exciting growth prospects, can create a successful and fulfilling career at Compare the Market, where you will enjoy:
City fringe location - close to public transport, street parking, and the riverwalk.
Compare the Market is launching a 9-day fortnight trial in January 2024. This program is designed to offer employees greater flexibility in their work schedules while maintaining business performance and ensuring a healthy work-life balance. In summary participants will work additional hours across the 9 working days to access every second Friday off. Your role will be eligible to participate in this trial if it suits your circumstances.
We have vibrant social and community activities including annual celebrations, family fun days and regular events across each of our sites.
Enjoy additional leave days - ‘ME’ leave and ‘Volunteer Day’ leave.
Employee discounts on Car, Home, Travel insurance plus discounts with a broad range of providers, including gym memberships, accommodation, dental care and more.
Income protection insurance provided to support you in the event of non-work related illness or injury.
Access to the Employee Assistance Program
Compare the Market is an Equal Opportunity Employer. We acknowledge that we are strengthened by diversity, and we encourage all applicants to apply who have the right to live and work in Australia.
Intrigued? Don't limit yourself. Apply now … it’s Simples!!
Note to Recruiters: We politely ask that you avoid any approaches or sending unsolicited resumes to our recruitment team and hiring leaders across our business.
Show more
Show less","Data Engineering, Data Science, Machine Learning, Python, SQL, ETL, Data Pipelines, Data Modeling, Data Quality, Data Governance, Data Observability, Data Discoverability, Data Security, Codebase Quality, CI/CD, Infrastructure as Code, Spark, PySpark, Databricks, Feature Stores, Metadata","data engineering, data science, machine learning, python, sql, etl, data pipelines, data modeling, data quality, data governance, data observability, data discoverability, data security, codebase quality, cicd, infrastructure as code, spark, pyspark, databricks, feature stores, metadata","cicd, codebase quality, data discoverability, data engineering, data governance, data observability, data quality, data science, data security, databricks, datamodeling, datapipeline, etl, feature stores, infrastructure as code, machine learning, metadata, python, spark, sql"
Senior Data Engineer,Comparethemarket.com.au,"Toowong, Queensland, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-comparethemarket-com-au-3687426478,2023-12-17,Queensland, Australia,Mid senior,Onsite,"Brief Description
#S-DNI
About The Company
Compare the Market, yep, the one with the meerkats, is a high-growth FinTech company on a transformational journey that is focused on designing, building, and delivering the very best money, home loan, energy, insurance and health comparison and fulfilment product solutions in Australia. Okay, in the world
So, do you love working with a team and achieving great outcomes together? Will you do what it takes to ensure the team is successful? Do you care passionately about the customer and want to work somewhere that aligns with your core values? If so, then we are looking for you.
About the role:
CTM is continuing to enhance our modern data platform (AWS / Databricks), with this role reporting to Lead Data Engineer you will have opportunity to establish data foundations and provide technical leadership to engineering team and broader business stakeholders.
As the Senior Data Engineer, you will be a strong hands-on data engineer along with providing guidance around architecture and solution design. You will be actively contributing to Data Strategy with engineering capabilities, mentoring team towards design and development of data platform and overall ecosystem.
Job Specific:
Collaborate with both the business and IT teams to define the business problem, refine the requirements, and design and develop data deliverables.
Align build of data engineering solutions in accordance with data architecture and component designs.
Drive the development and implementation of data engineering roadmap.
Manage data pipelines consisting of a series of stages through which data flows (from data sources or endpoints of acquisition to integration to consumption for specific use cases) whilst architecting, creating and maintaining data pipelines.
Champion Data Management Best Practices including Data Lake, Data Warehouse, Data Quality, Lineage and Data Ops.
Collaborate with Data Architect, DevOps, Data Engineers to design an integrated data application.
Drive Automation through effective metadata management by using innovative and modern tools, techniques and architectures to partially or completely automate the most-common, repeatable and tedious data preparation and integration tasks in order to minimize manual and error-prone processes and improve productivity.
Identify, design, and implement solutions for greater scalability and future reuse.
Develop production ready data application and pipelines in cloud ecosystem.
Performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Qualifications:
Minimum Bachelor’s Degree qualified in Computer Science, IT or a relevant discipline
Experience & Skills:
The key to your success will be your enthusiasm, ability to work cohesively with a great team, combined with extensive experience and knowledge of delivering great software efficiently.
Minimum 5 years’ experience in a similar role, a minimum of 10 years’ experience in IT/data
Strong experience with Cloud EDWs (AWS and/or GCP), Data Lakes, Information Governance Suites (Lineage, Glossary, Reference Data).
Proven development experience in Python, and ETL tools in addition to data modelling skills.
Additional experience in traditional on prem EDW / ETL environments well regarded.
Experience building metadata driven highly automated solutions very well regarded.
Experience with Databricks well regarded.
Experience with deployment of data workloads using CI/CD tooling.
Experience in API development for analytic/ML end points.
Experience providing leadership and mentoring to team members (code reviews, direction, guidance, performance management, kpi’s, 1on1’s, reviews etc)
Understanding of end-to-end Data & Insights life cycle is a must
Outstanding communication and business engagement
Previous experience working in an Agile environment using tools such as Jira and Confluence.
Show more
Show less","AWS, Databricks, Python, ETL, Data Lake, Data Warehouse, Data Quality, Lineage, Data Ops, Data Architect, DevOps, Data Engineers, Cloud EDWs, Information Governance Suites, CI/CD, API, Agile, Jira, Confluence","aws, databricks, python, etl, data lake, data warehouse, data quality, lineage, data ops, data architect, devops, data engineers, cloud edws, information governance suites, cicd, api, agile, jira, confluence","agile, api, aws, cicd, cloud edws, confluence, data architect, data engineers, data lake, data ops, data quality, databricks, datawarehouse, devops, etl, information governance suites, jira, lineage, python"
Data Analyst,Woolworths Group,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-analyst-at-woolworths-group-3776711306,2023-12-17,Queensland, Australia,Mid senior,Onsite,"We are Woolworths Group
We are Woolworths Group. 200,000+ bright minds, passionate hearts and unique perspectives connected by a shared Purpose – ‘to create better experiences together for a better tomorrow.’ It’s that Purpose that fuels our ambition to explore new ideas, make brave commitments and innovate better ways to meet the food and everyday needs of more than 24 million customers every week.
If you’re excited to turn today’s blue sky thinking into a better tomorrow for future generations, you’ll find yourself supported and enriched in an dynamic, inclusive and empowering workplace that reflects the diverse communities we serve. With a culture of genuine care, a flexible approach to work and opportunities across the group to grow your career and make a meaningful impact, the possibilities for what we can achieve together are endless.
Data Analyst
Join an innovative and dynamic team with an inclusive culture
Shape the future of critical business functions
Strong opportunities for growth and career development in role
Welcome to the People Team
Better experiences for our Team Members are the key to unlocking a better future for our customers and communities. Our People Team is responsible for powering the trajectory of our business and the wellbeing of our people – through the design and delivery of a seamless and enriching Team Member experience. One that strengthens the foundations of our organisation by empowering passionate Team Members to become purpose-led Woolworths Group advocates and attracting a new generation of talent to the group.
What You’ll Do
Partner with and support the Business to solve issues and answer questions relating to our Team Members through People data analysis reporting
Build people data analytical/reporting solutions; through the use of SQL/BiqQuery querying logic
Undertake data analysis to identify trends and points of note/interest, telling a story using data
Be a leader and contributor to problem resolution, team decisions and project planning
Maintain and update documentation including process; specifications, support documents and testing scripts
What You’ll Bring
Advanced knowledge and application of table relationships, advanced querying in applications, aggregate queries, macros, and pivot tables
Knowledge of Tableau is advantageous
Preferably worked with: SuccessFactors (Employee Central, Employee Profile), SAP Payroll (Spinifex), Microsoft Suite (Excel), Salesforce
2 + years experience with data and reporting, ideally with People data or other dynamic data
A Degree in Information Technology or a relevant discipline
Advanced skills in querying, extracting and wrangling data
Not required, but advantageous if you have previously worked in an Agile team and used Jira for backlog management and Confluence for documentation
What You’ll Experience
Work from Anywhere - A progressive and flexible ‘Work from Anywhere’ policy that gives you more control over your work, life and wellbeing.*
Team Discounts -
Team discounts across our range of Woolworths Group brands you know and love and a robust rewards program that celebrates and incentivises purpose-driven work.
Wellness -
Access to Sonder. Sonder provides free confidential 24/7 personalised financial, medical safety, psychological or physical support for team members and their families.
""Please note that our Talent Chapter is taking a well deserved break over the holidays, from Friday December 22nd and returning Monday 8th January 2024.
We thank you for your patience over this period and wish you a happy holiday season.""
Everyone belongs at Woolworths Group
Diversity, equity, inclusion, and belonging are key to realising our purpose of better together for a better tomorrow. We recognise the value our team’s diversity brings to our business, customers, and communities and that teams with diverse experiences and backgrounds enrich our group and are better able to innovate and solve problems. As one of the largest employers in Australia and New Zealand, we aim to create a truly inclusive workplace where everyone feels that they belong, can be their best selves, and reach their full potential.
We encourage all candidates to apply; please let us know in your application if we can support you with any adjustments in the hiring process.
You can learn more about working with us on LinkedIn or via www.wowcareers.com.au.  #work180.
Our Talent Acquisition Team and Hiring Leaders kindly request no unsolicited resumes or approaches from Recruitment Agencies. Woolworths Group is not responsible for any fees related to unsolicited resumes.
Show more
Show less","SQL, BiqQuery, Tableau, SuccessFactors, SAP Payroll, Microsoft Suite, Salesforce, Jira, Confluence, Agile, Data analysis, Data reporting, Data wrangling, Information Technology, Querying, Extracting","sql, biqquery, tableau, successfactors, sap payroll, microsoft suite, salesforce, jira, confluence, agile, data analysis, data reporting, data wrangling, information technology, querying, extracting","agile, biqquery, confluence, data reporting, data wrangling, dataanalytics, extracting, information technology, jira, microsoft suite, querying, salesforce, sap payroll, sql, successfactors, tableau"
Principal ITS Data Engineer - Brisbane,WSP in Australia,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/principal-its-data-engineer-brisbane-at-wsp-in-australia-3785147982,2023-12-17,Queensland, Australia,Mid senior,Onsite,"WSP provides a unique opportunity to work on projects that make a difference and shape our communities. As part of the Intelligent Transport Systems team, you will work on cutting edge projects that are re-defining the transport industry, transitioning into the digital age of connected, autonomous and zero emission vehicles. You will be designing the future of transport, embedding technology into major project infrastructure and being on the forefront of technological transformation shaping our cities. WSP works with major engineering clients including Department of Transport and Main Roads, Transport for NSW, Department of Transport (Vic), Main Roads WA, Transurban, major construction contractors and like minded consultancies.
WSP’s ITS team is industry leading and a unique opportunity exists for the right person to join our team.
The role will be to lead the digital delivery capability for the ITS team with a focus on solving transport specific problems for our clients.
Our Recent Projects Include
QLD - DTMR - Cooperative and Automated Vehicle Initiative
QLD - DTMR - STREAMS platform maintenance project –
QLD – DTMR – Transport Advisory Unit - Cloud Data Engineer
WA - MRWA – Cooperative ITS (C-ITS) strategy
VIC - DTP - C-ITS Advice to ITS & Ops Teams
What You’ll Do
Advising our clients on data-based solutions for transport applications, with emphasis on emerging technologies such as connected and automated vehicles, smart city infrastructure, new mobility, data aggregation, analytics and systems.
Development of specific solutions to assist our clients to better use their data to solve problems relating to transport systems such as road safety and network optimisation.
Use of cloud systems and architectures to allow transport authorities to better utilise their data for practical applications.
Use of digital technologies such as Amazon Web Services (AWS), dashboard development (e.g. Tableau) and other emerging digital systems
Development of plans and advice on solution roadmaps, data management, security controls and access management,
Analysis, architecture. development and testing of intelligent transport system solutions.
Advice and design of communication networks (e.g. network design, suitability assessments).
Interact and collaborate clients and other external companies to understand requirements and ensure compatible solutions.
Undertaking client relationship management and business development activities across existing and new client opportunities
Raising the profile of the ITS team and Planning and Mobility team and maintaining close collaboration with our colleagues in WSP Digital.
Bidding of projects including commercial considerations such as margin, risk and accurate estimating.
WSP’s ITS team has strong connections into the Digital engineering, planning and mobility smart cities teams and provides a broad range of possibilities for the right person, with an agile approach, a curious mind and ability to adapt engineering principles to new engineering areas.
About You
10+ years experience within the data, engineering or transport systems fields would be an advantage.
Tertiary qualifications in a relevant discipline such as engineering or, computer science would be desirable.
Experience with agile project management methodologies.
Experience within a consulting environment with a proven record of leading quality engineering design and consulting services.
Ability to work within a high performing team, with a proven ability to prioritise team achievements and support others.
Ability to work well with others and be open minded to new ideas and technologies.
Able to produce quality written communication and support technical engineering work with communication skills matched appropriately to the audience.
About WSP
WSP is one of the world's leading engineering professional services consulting firms, bringing together approximately 5,000 talented people across 14 offices in Australia. We are technical experts who design and provide strategic advice on sustainable solutions and engineer Future ReadyTM projects that will help societies grow for lifetimes to come. wsp.com/au
Our vibrant workplace culture is an important part of what makes this a great place to work. As a leader in WSP, you will have the opportunity to influence and contribute to our social and community initiatives while collaborating with your colleagues to make a positive impact on people’s lives. You will also enjoy the rewarding experience of mentoring younger professionals and seeing them succeed and progress in their careers over time.
WSP prioritises the health and wellbeing of our people by offering flexible working, 12 months parental leave, opportunities for global mobility, purchased additional leave, and a competitive salary among many more benefits designed to help you thrive.
We are committed to providing an inclusive and equal opportunity workplace. We promote Indigenous voices and are actively delivering a ‘Stretch’ level Reconciliation Action Plan. We are a Workplace Gender Equality Agency Employer of Choice, support the LGBTQI+ community, and encourage all employees to bring their whole selves to work.
Think this could be the opportunity for you? Apply now to begin your journey with WSP.
Show more
Show less","Agile, AWS, Cloud systems, Data aggregation, Data management, Data security, Engineering, ITS, Network design, Road safety, Tableau","agile, aws, cloud systems, data aggregation, data management, data security, engineering, its, network design, road safety, tableau","agile, aws, cloud systems, data aggregation, data management, data security, engineering, its, network design, road safety, tableau"
Data Migration Analyst,CareVision,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-migration-analyst-at-carevision-3685493098,2023-12-17,Queensland, Australia,Mid senior,Onsite,"We are looking for a seasoned and experienced Data Migration Specialist to join our consulting team based in Milton, Brisbane.
As a data migration specialist within a high growth SaaS software company, you will play a crucial role in the streamlining of client onboarding and implementations, and have a pivotal role in the accelerating growth of a platform that is improving the lives of families and care givers across the country.
You will work with clients on data migration planning, writing scripts to extract, transform and load data into our software platform and collaborate with a team of data engineers, API developers, test analysts and Business Analysts to deliver great client outcomes. As part of the process you will provide data quality and import success reports to clients and facilitate discussions with clients about how to bring their data into our software for optimal outcomes.
Key Responsibilities
Lead the design and implementation of data migration scripts and processes, data evaluations and analysis
Engage with client application teams to plan and execute the migration and upgrade projects, with the ability to multitask and work effectively during cutover
Ensure client success by executing successful migration and/or upgrade activities including to plan and execute, migration and/or upgrade projects across the application lifecycle and route to live/production
Lead the design and implementation of standard connectors that pull data out of competitive software products and transform it into the required structures for import to our platform
Partner with internal colleagues from the Security, Provisioning, Product Support to align business goals with implementation results.
Adhere to ISO27001 compliance requirements when handling sensitive client data
Skills and experience:
You are a technical, hands-on, motivated and collaborative individual and proven experience working with diverse teams of technical architects, business users, and IT teams
Additionally, you have a history of consulting with customer organizations on all phases of the data migration life cycle.
Experience in data migration projects for CRM, ERP and/or Payroll implementations, health care experience desirable
Technical experience regarding data models, database design, ETL development and data mining, Typescript and PHP
Experience and understanding of Data cleansing, extraction, normalizing, reconciliation and balancing experience as related to an ETL process
Strong knowledge of and experience with database technologies such as MS SQL, MySQL
Good communication and planning skills and proven track record of working in a consultative environment
Show more
Show less","Data migration, Data quality, Data extraction, Data transformation, Data loading, Data modeling, Database design, ETL development, Data mining, Typescript, PHP, Data cleansing, Data extraction, Data normalization, Data reconciliation, Data balancing, MS SQL, MySQL, CRM, ERP, Payroll, Health care","data migration, data quality, data extraction, data transformation, data loading, data modeling, database design, etl development, data mining, typescript, php, data cleansing, data extraction, data normalization, data reconciliation, data balancing, ms sql, mysql, crm, erp, payroll, health care","crm, data balancing, data extraction, data loading, data migration, data mining, data normalization, data quality, data reconciliation, data transformation, database design, datacleaning, datamodeling, erp, etl development, health care, ms sql, mysql, payroll, php, typescript"
Senior Data Applications Engineer,Glencore,"Mount Isa, Queensland, Australia",https://au.linkedin.com/jobs/view/senior-data-applications-engineer-at-glencore-3786598780,2023-12-17,Queensland, Australia,Mid senior,Onsite,"Technical Services Team at George Fisher Mine
Mount Isa, Queensland, Australia
Residential: 5 days on, 2 days off, 4 days on, 3 days off or FIFO: 8 days on, 6 days off
Senior Data Applications Engineer
George Fisher Mine – Technical Services
Residential or FIFO
Mount Isa
, Queensland, Australia
Reference No: 41979
Please note this opportunity is only open to current employees.
Mount Isa Mines is one of the oldest, largest and most complex mining operations in Australia located in Mount Isa, Queensland. Employing over 4,000 people, Mount Isa Mines is now looking for a new member to join the team.
The Operation
Mount Isa Mines is the second largest producer of copper in Australia and one of the largest zinc mines in the world. It is a key asset in Glencore’s global mining portfolio.
The Mount Isa Copper and Zinc operations are integrated across the entire supply chain, from resource identification and development to production and distribution of copper, zinc and lead to customers across the world.
We are attracting the best in the industry into our teams, as we trust in our people’s talent and support them in their development to become leaders in their field. We offer structured career development plans and mentoring from industry leaders in your field to ensure our people reach their desired potential.
We also provide an excellent remuneration package and benefits including above market superannuation, rental assistance, and retailing discounts. Please take a look at our Liveability website to learn more about what Mount Isa has to offer.
This role is also eligible for relocation to Mount Isa.
Mount Isa Mines location and roster offers an attractive lifestyle option. With a 5:2/4:3 residential roster or a 8/6 FIFO work roster, our people return home every day and enjoy a thriving town of 23,000 people. You will always have something to do, go to restaurants, movies, join a gym and sporting activities, play golf, as well as fishing and camping and activities at the lake.
The operation is situated on land traditionally owned by the Kalkadoon People and we respect their rights and interests.
The Role
Glencore is seeking a Senior Data Applications Engineer for our George Fisher Mine Operations. Reporting to the Superintendent of Operations Technology, this role is about using data-driven solutions to improve Mount Isa Mine operations in safe, cost-effective ways. Youll find new opportunities to add value and deliver continuous improvements. This fantastic opportunity is offered on a, 8/6 FIFO roster or a 5:2/4:3 residential based work roster.
Your Key Responsibilities Include
Responsible for full software development lifecycle from design to deployment
Train and mentor other engineers
Engage and influence key stakeholders to obtain organizational commitments on technology roadmaps.
Lead provisioning of current and future focused operational technology software solutions on-premises or as a cloud service
Business partner with local, regional, and global Information Systems and Technology
Follow agreed project management framework. Ensure Project support, operational readiness, change management.
Facilitate analysis of stakeholder objectives into business requirements
Create business requirements documents and mapping to technical requirements.
Document and maintain business processes across vendor and in-house applications to ensure business continuity. As-built documentation, test plans, end-user guides etc.
Compliance to OT/IT systems governance framework
Vendor management
Develop business cases for initiatives, approval, funding, and prioritization.
CAPEX / OPEX cost center management, forecast, and budget planning
Demonstrate safety leadership across Mount Isa Mines
The Successful Applicants Will Have
At least bachelor’s d egree or equivalent qualification in Computer Science / Information Technology / Software engineering
Over 8 years’ experience in managing delivery of software implementation projects in complex multi-disciplinary setting, including architectural design, build, test, deployment.
Detailed knowledge and experience of software development lifecycle, design patterns and principles using Microsoft technologies: Asp.net Core, Microsoft SQL Server, Power Platform, GitHub
Strong command of data structures and algorithms. Fluent in front/back-end programming languages: SQL, C#, Javascript, Html, CSS, python
In-depth understanding of database management systems, online analytical processing, ETL (Microsoft Power BI, SSAS/SSRS/SSIS, Azure Cloud Services.)
Strong problem solving, analytical, communication and documentation skills.
Knowledge and experience with enterprise software applications, operational technology network topologies
Experience supervising and developing team of junior software engineers.
Ability to work across teams and departments, driving engagement through multiple tiers of the workforce, including ability to break down complex systems into key components and communicate those to non-technical staff in clear and concise manner.
Experience in resources sector, Fleet/Mine production management systems, grade reconciliation systems
Please note this opportunity is only open to current employees.
For further information, please contact:
Bek Lewis, Recruitment Advisor, on phone (07) 4744 6073 or email
Applications Close:
Wednesday 3 rd January 2024
Apply online now! Applications will not be accepted via email.
Our mining operation is on land traditionally owned by the Kalkadoon People and we respect their rights and interests. Mount Isa Mines encourages suitably qualified indigenous applicants to appl y
Show more
Show less","Data Applications Engineering, Software Development Lifecycle, Microsoft Technologies, Asp.net Core, Microsoft SQL Server, Power Platform, GitHub, C#, Javascript, Html, CSS, SQL, Power BI, SSAS, SSRS, SSIS, Azure Cloud Services, Enterprise Software Applications, Operational Technology Network Topologies, Fleet/Mine Production Management Systems, Grade Reconciliation Systems","data applications engineering, software development lifecycle, microsoft technologies, aspnet core, microsoft sql server, power platform, github, c, javascript, html, css, sql, power bi, ssas, ssrs, ssis, azure cloud services, enterprise software applications, operational technology network topologies, fleetmine production management systems, grade reconciliation systems","aspnet core, azure cloud services, c, css, data applications engineering, enterprise software applications, fleetmine production management systems, github, grade reconciliation systems, html, javascript, microsoft sql server, microsoft technologies, operational technology network topologies, power platform, powerbi, software development lifecycle, sql, ssas, ssis, ssrs"
Senior Data Scientist/Engineer,The University of Queensland,"Toowong, Queensland, Australia",https://au.linkedin.com/jobs/view/senior-data-scientist-engineer-at-the-university-of-queensland-3777009171,2023-12-17,Queensland, Australia,Mid senior,Onsite,"UQ Poche Centre for Indigenous Health
Apply data engineer expertise to the ATLAS Research and Surveillance Network
Be part of an organisation with a meaningful purpose and impact
Plan for your future with a generous 17% superannuation allowance
Based at the centrally located Toowong office
About UQ
As part of the UQ community, you'll have the opportunity to work alongside the brightest minds, who have joined us from all over the world.
Everyone here has a role to play. As a member of our professional staff cohort, you will be actively involved in working towards our vision of a better world. By supporting the academic endeavour across teaching, research, and the student life, you'll have the opportunity to contribute to activities that have a lasting impact on our community.
Join a community where excellence is at the core of our culture, contributions are valued and a range of benefits and rewards are available, such as:
26 weeks paid parental leave or 14 weeks paid primary caregiver leave
17% superannuation contributions
17.5% annual leave loading
Access to flexible working arrangements including hybrid working options and flexible start/finish times
Health and wellness discounts - fitness passport access, free yearly flu vaccinations, discounted health insurance, and access to our Employee Assistance Program for staff and their immediate family
Salary packaging options
About This Opportunity
We have an exciting opportunity for a
Senior Data Scientist/Engineer
to join the ATLAS Indigenous Primary Care Surveillance and Research Network within the UQ Poche Centre for Indigenous Health.
The ATLAS Indigenous Primary Care Surveillance and Research Network is a sentinel surveillance system led from the UQ Poche Centre for Indigenous Health. It was established to monitor sexually transmissible infection and blood-borne virus testing, diagnosis and management data, with Aboriginal Community-Controlled Health Organisations (ACCHOs) as principal stakeholders. There are currently 38 sites involved in the ATLAS Network, contributing over 9 million records from more than 200,000 individuals. We are working to triple the number of ACCHOs participating in the network, connect with the mainstream services that also provide primary care to Indigenous communities, and expand surveillance to capture a wider range of clinical domains, such as vaccine-preventable and other infectious diseases. The project is building the largest connected Indigenous primary care surveillance network in Australia. Our work supports increased capacity for ACCHOs to provide high quality, evidence-based, best practice clinical care for improved health outcomes for Indigenous peoples.
The primary purpose of the Senior Data Scientist/Engineer is to lead the ongoing development and delivery of the ATLAS Network's data pipeline.
The ATLAS Indigenous Primary Care Surveillance and Research Network uses the GRHANITE data extraction software-developed by the University of Melbourne-to link deidentified person-level clinical data within and between participating sites. Rapid expansion of both the reach and scope of the ATLAS network, adding both new sites and additional clinical domains, requires ongoing development of the project's data pipeline, SQL server infrastructure and online dashboard security protocols. The Senior Data Scientist/Engineer will play a critical role in these activities, alongside the ATLAS Data Manager and ATLAS team Analysts.
Key responsibilities will include:
Working as the senior member of the data team, provide specialist expertise on appropriate infrastructure, approaches, and methodologies as relevant to ATLAS.
Leading the ongoing development of the ATLAS data pipeline and server infrastructure to ensure sustainable growth.
Providing expert scientific and technical advice to maximise research outcomes at all stages of the research process life cycle.
Contributing to the data infrastructure's ongoing maintenance by undertake quality assurance and improvement activities in collaboration with the ATLAS Data Manager and Analysts.
Contributing to quality assurance, control, and improvement activities, in collaboration with senior research staff and stakeholders.
Contributing significant subject matter expertise to the maintenance of metadata, codebooks, and other administrative documentation to ensure the integrity and sustainability of the ATLAS network.
Managing compliance with approved research ethics protocols.
Complying with Safe Operating Procedures, Risk Assessments, and safety processes.
Supporting a proactive safety culture and implement continuous improvement.
This is a full-time (100%), fixed-term position for up to 2 years.
At HEW level 8 the full-time equivalent base salary will be in the range $106,838 - $119,780, plus a generous super allowance of up to 17%. The total FTE package will be up to $125,000 - $140,143 annually. As this role is covered by an Enterprise Agreement, you will also receive regular remuneration increases - at least once a year.
About You
Completion of a post graduate qualification in software engineering, computer science, information technology or related discipline with considerable relevant experience or an equivalent combination of relevant experience and/or education/training.
A PhD, Research Masters or progress towards a PhD/Research Masters in software engineering, computer science, information technology or related discipline would be highly regarded in this role.
Substantial experience building and optimising data pipelines, architectures, and schemas (e.g., DBT, SSIS).
Substantial experience in data manipulation and analysis using SQL and/or Python.
Experience managing data system access and security.
Experience with database administration (e.g., Microsoft SQL Server, Oracle).
Experience with full-stack web development is highly desirable.
Experience working with healthcare data, particularly primary healthcare data. Working knowledge of the GRHANITE tool is a significant advantage.
High level of organisational ability and initiative, and the capacity to manage competing demands in a complex team environment.
Excellent communication skills and the ability to communicate effectively with a range of audiences.
A commitment to upholding the highest standard of research integrity, ethics and workplace health and safety.
Commitment to upholding the University's values, and with the outstanding personal qualities of openness, respectfulness, and integrity.
In addition, the following mandatory requirements apply:
Work Rights:
You must have unrestricted work rights in Australia for the duration of this appointment to apply. Visa sponsorship is not available for this appointment.
Background Checks:
All final applicants for this position may be asked to consent to a criminal record check. Please note that people with criminal records are not automatically barred from applying for this position. Each application will be considered on its merits.
Questions?
For more information about this opportunity, please contact Dr Clare Bradley at clare.bradley@uq.edu.au.
For application queries, please contact recruitment@uq.edu.au stating the job reference number (below) in the subject line.
Want to Apply?
All applicants
must
upload the following documents in order for your application to be considered:
Cover letter addressing the 'About You' section
Resume
Other Information
At UQ we know that our greatest strengths come from our diverse mix of colleagues, this is reflected in our ongoing commitment to creating an environment focused on equity, diversity and inclusion. We ensure that we are always attracting, retaining and promoting colleagues who are representative of the diversity in the broader community, whether that be gender identity, LGBTQIA+, cultural and/or linguistic, Aboriginal and/or Torres Strait Islander peoples, or people with a disability. Accessibility requirements and/or adjustments can be directed to recruitment@uq.edu.au
If you are a current employee (including casual staff and HDR scholars) or hold an unpaid/affiliate appointment, please login to your staff Workday account and visit the internal careers board to apply for this opportunity. Please do NOT apply via the external job board.
Applications close Sunday 7 January 2024 at 11.00pm AEST (Job Reference Number - R-33210).
Please note that interviews have been tentatively scheduled for week commencing 15 January 2024.
Please note that UQ will be closed from 23 December until 2 January (inclusive). All applicants for this position will be contacted in the new year. We thank you for your patience during this time.
Show more
Show less","Data engineering, Data pipeline development, SQL, Python, Database administration, Web development, Healthcare data analysis, GRHANITE, Data integrity, Research ethics, Data security, Data management, Data analysis, Data visualization, DBT, SSIS, Microsoft SQL Server, Oracle","data engineering, data pipeline development, sql, python, database administration, web development, healthcare data analysis, grhanite, data integrity, research ethics, data security, data management, data analysis, data visualization, dbt, ssis, microsoft sql server, oracle","data engineering, data integrity, data management, data pipeline development, data security, dataanalytics, database administration, dbt, grhanite, healthcare data analysis, microsoft sql server, oracle, python, research ethics, sql, ssis, visualization, web development"
Data Insights Consultant,Core Asset Co,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-insights-consultant-at-core-asset-co-3763132138,2023-12-17,Queensland, Australia,Mid senior,Onsite,"About Us
Founded in 2018, Core Asset Co. is an
asset performance improvement consultant
that helps leaders make smarter decisions about their heavy mobile equipment (HME) and fixed assets.
We work with some of the
largest mining companies in the world
, significantly improving their performance and return on investment.
Opportunity Overview
Core Asset Co. is seeking a
Data Insights Consultant
to join our Brisbane team. This role is suited for professionals with 2+ years of analytical experience who are keen to apply their skills in a client-focused consulting environment. Your primary responsibilities will include analysing data to generate actionable insights, contributing to project delivery, and aiding in strategic decision-making. The role offers significant opportunities for professional growth and a meaningful impact on client outcomes.
Candidate Profile
Ideal for individuals with a foundational background in data analysis, eager to hone their skills in a dynamic, client-oriented consulting environment.
Requirements
2+ years of experience in data analysis or a related field.
Proven ability in interpreting data and developing insights.
Strong problem-solving skills and attention to detail.
Collaborative team player.
Bachelors degree in a relevant field (e.g., Business, Economics, Statistics).
Full time working capacity.
Australian working rights.
Preferred Qualifications
Experience in asset management.
Background in mining or heavy asset industries.
Proficient in methodical data analysis and insight development.
Skilled in Excel; openness to using other analytical tools as needed.
Proactive in learning and skill development.
Contract Details
Full-time position with a clear path for career progression based on performance and team contributions.
Benefits
Core Asset Co. offers a competitive salary, opportunities for professional development, and a collaborative work culture. You will be integral to a team that values innovation, impact, and meaningful contributions to client projects and success.
Best regards
Josh Old
www.coreasset.co/careers
Note - only those with current Australian working rights will be considered for this position.
Show more
Show less","Data analysis, Data interpretation, Insight development, Problemsolving, Teamwork, Business, Economics, Statistics, Asset management, Mining, Heavy assets, Excel, Analytical tools, Proactive learning","data analysis, data interpretation, insight development, problemsolving, teamwork, business, economics, statistics, asset management, mining, heavy assets, excel, analytical tools, proactive learning","analytical tools, asset management, business, data interpretation, dataanalytics, economics, excel, heavy assets, insight development, mining, proactive learning, problemsolving, statistics, teamwork"
Data Analytics Consultant,Auto & General Australia,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-analytics-consultant-at-auto-general-australia-3738506991,2023-12-17,Queensland, Australia,Mid senior,Onsite,"Brief Description
The role:
Design, develop, deploy and embed end-to-end analytics solutions that intelligently leverage data for commercial outcomes, enabling data-driven decision-making throughout Auto & General.
This is an exciting opportunity for individuals passionate about the Data Analytics space and wanting to progress in their careers. This role will be based in our Toowong Head Office.
Accountabilities
As part of a team of analytics specialists, design, develop, deploy and embed end-to-end development of analytics solutions (descriptive, diagnostic, predictive and prescriptive), working in partnership with commercial teams and decision-makers
Build and sustain an applied understanding of business operations within a particular domain/business area of Auto & General, translating this into a strong understanding of data, systems and decision-making processes
Maintain a continual cadence of automated analytics solution delivery, aligned to business strategy and priorities, and focused on strengthening decision-making at all levels
Support the growth of data acumen and self-service analytic solutions throughout Auto & General, providing training and support as solutions are embedded
Contribute to the ongoing evolution of Auto & General’s systems and processes, considering the role of data in systems and process design
Skills, Knowledge And Qualifications
Data science, actuarial science, mathematics or computer science qualification (or relevant experience)
Commercial acumen, using data to solve complex business problems and inform decision-making at all levels
Strong customer orientation and business acumen, demonstrated through the ability to translate business operations into powerful data solutions
Analytic solution development expertise using cloud-based data analytics platforms to deliver self-service analytics solutions and/or real-time decision services, preferably with multi-lingual development experience
Self-driven learner, with a strong appetite for data innovation
About Us
At Auto & General (A&G), we provide great products and services to safeguard our customers in their time of need. Our range of general insurance products protects customers on the road, at home and on holiday with various Car, Motorcycle, Home, Contents, and Pet products as well as Roadside Assistance.
Our culture of ‘high performance with high integrity’ underpins our values and the way we interact with our customers, the community and each other. We’re excited about the future and we’re always on the lookout for talented, passionate individuals who can help us achieve our goal of being Australia’s best insurer! If this sounds like you, apply today.
Our Perks
Location – Our office is conveniently based in Toowong Village shopping centre - a city fringe location with access to retail, restaurants, various parking options and public transport.
Extra leave - Enjoy additional leave days on us! You’ll receive a paid ‘ME’ day and one paid volunteer leave day annually. Team members can also purchase up to two additional weeks of leave per year.
Paid parental leave - We support our new parents with paid parental leave and other benefits.
Workplace giving - If you’re passionate about a cause, then we are too – we offer workplace giving and we’ll dollar-match your donations to registered charities.
Development opportunities - We’re championing your development with internal programs and access to a wide range of online courses.
Employee discounts - You’ll receive discounts on Budget Direct insurance products.
Reward and recognition - We reward high performance with employee recognition, reward and incentive schemes.
Onsite facilities – There are excellent end-of-trip facilities on offer and private spaces for nursing mothers.
Get social - Join our vibrant social and community activities including annual celebrations, family fun days and regular events across each of our sites.
Perks App - Access to an employee benefits and discounts app called ‘Perks’ offering your great discounts, offers and programs across a range of areas.
Please ensure your manager or team leader is aware of your application.
If you are seeking a new challenge and would enjoy the opportunity to work in a growing and changing business, click on the APPLY NOW button and submit your application.
Auto & General values individual differences and believes in fostering an inclusive culture that creates a great place to work for all.
A note from Auto & General to recruitment agencies: We politely ask that you avoid making any approaches or sending any unsolicited resumes to our Recruitment Team or Hiring Leaders across our business. Auto & General is not responsible for any fees related to unsolicited resumes.
Show more
Show less","Data science, Actuarial science, Mathematics, Computer science, Commercial acumen, Customer orientation, Business acumen, Analytic solution development, Cloudbased data analytics platforms, Selfservice analytics solutions, Realtime decision services, Multilingual development experience, Selfdriven learning, Data innovation","data science, actuarial science, mathematics, computer science, commercial acumen, customer orientation, business acumen, analytic solution development, cloudbased data analytics platforms, selfservice analytics solutions, realtime decision services, multilingual development experience, selfdriven learning, data innovation","actuarial science, analytic solution development, business acumen, cloudbased data analytics platforms, commercial acumen, computer science, customer orientation, data innovation, data science, mathematics, multilingual development experience, realtime decision services, selfdriven learning, selfservice analytics solutions"
Data Analyst and Reporting Officer,STAFF X - Talent Solutions,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-analyst-and-reporting-officer-at-staff-x-talent-solutions-3766874862,2023-12-17,Queensland, Australia,Mid senior,Onsite,"Unlock Opportunities as a Data Analyst and Reporting Officer - Join Our Client for Legal Justice Advocacy
Why Join Our Client?
Impactful Contribution: Be part of a dynamic, inclusive team advancing legal rights, safety, and justice by collaboratively addressing challenges.
Tech-Driven Environment: Thrive in a tech-savvy workplace, utilising your Data Analyst skills for streamlined processes and organisational growth.
Advocacy Impact: Play a vital role in impactful advocacy and law reform initiatives, contributing to positive changes through evidence-based approaches with broad impact.
Positive Culture: Experience a positive work culture that recognises contributions, fostering professional growth and fulfillment. Enjoy a competitive salary package reflecting your skills.
Flexible Working Arrangement: Embrace a hybrid working setup, providing flexibility to balance work and life. The role is based in Annerley, Brisbane, QLD.
Additional Benefits:
Generous Leave: Enjoy extra paid leave over holidays and additional parental leave.
Financial Flexibility: Maximise financial benefits with attractive salary sacrifice options.
Continuous Learning: Fuel your professional growth with exciting learning and development opportunities for a dynamic and fulfilling career journey.
About The Client
STAFF X Talent Solutions proudly represents a trailblazing organization established in 1984, dedicated to addressing the unique legal challenges faced by women in Queensland. Starting as a volunteer service providing telephone legal advice, they have evolved into a dynamic team committed to accessible legal and social justice. With a focus on impactful advocacy, innovative tech solutions, and a positive work culture, they are a key player in advancing legal rights, safety, and justice for women in the region.
About The Role
Our respective client is seeking a
Data Analyst and Reporting Officer
to play a pivotal role in optimising business operations through insightful analytics and reporting. Responsibilities span from developing and maintaining data systems to creating meaningful reports and dashboards, ensuring compliance with quality standards.
Key Responsibilities
Develop, maintain, and improve data systems, reporting, and insights.
Collaborate with business units to define and refine processes related to data sourcing, exchange, and quality assurance.
Create and implement reports and dashboards using Microsoft Power Platform applications, providing analysis and commentary to stakeholders.
Oversee data collection from internal and external sources, ensuring accuracy and quality.
Provide data intelligence for monitoring and evaluating services.
Investigate and address operational inefficiencies, proposing and implementing solutions.
Manage quality compliance and evaluation against KPIs, generating timely and accurate reports.
Run complex reports, analyse results, and produce business reports for funding partners.
Interpret data to provide actionable insights and recommendations, measuring outputs against organisational and contractual requirements.
Write and produce reports at various organisational levels for diverse stakeholders.
Key Requirements
Minimum of three (3) years of relevant experience.
Clear National Police Check required.
Strong stakeholder management skills.
Expertise in data analysis and management, including proficiency in Microsoft Power BI and Excel.
Tertiary qualifications in Data Science, Business Analytics, or related fields preferred.
Demonstrated ability to source and analyse data from various databases.
Experience in quality management.
Exceptional written and verbal communication skills.
Commitment to the mission and values of the organisation, working within a feminist framework.
How To Apply
If you're passionate about leveraging your data analysis skills to make a meaningful impact and contribute to the advancement of legal rights and justice for women, we invite you to apply.
Please submit your resume and a cover letter detailing your relevant experience and addressing the key requirements outlined above. Send your application to patricia@staffx.com.au.
We look forward to welcoming a dedicated Data Analyst and Reporting Officer to our client’s dynamic team!
Show more
Show less","Data Analysis, Reporting, Data Systems, Data Sourcing, Data Quality Assurance, Microsoft Power Platform, Microsoft Power BI, Excel, Data Science, Business Analytics, Data Management, Stakeholder Management, Quality Management, Communication, Microsoft Office Suite, Databases, Tableau","data analysis, reporting, data systems, data sourcing, data quality assurance, microsoft power platform, microsoft power bi, excel, data science, business analytics, data management, stakeholder management, quality management, communication, microsoft office suite, databases, tableau","business analytics, communication, data management, data quality assurance, data science, data sourcing, data systems, dataanalytics, databases, excel, microsoft office suite, microsoft power bi, microsoft power platform, quality management, reporting, stakeholder management, tableau"
Data Engineer,Humanised Group,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-humanised-group-3784216982,2023-12-17,Queensland, Australia,Mid senior,Hybrid,"Work for a technology company that values your contribution and creates a positive impact globally!
Full Job Description
A new role has become available for a mid to senior Engineer within a growing Data team! Come and work for a technology company that values your contribution and creates a positive impact globally.
Rapidly expanding software house with worldwide growth plans
Work with various stakeholders to deliver effective data solutions
Career progression/development on offer
Brisbane based
Your role will involve:
Implementing data pipelines using AWS Databricks
Integrating the end-to-end data pipeline
Developing frameworks for data sets
Experience required:
Python and SQL proficiency
Azure or AWS cloud
Showcase your experience using data modelling techniques
Minimum 5 years of experience in a similar role
Excellent written and verbal communication skills
What’s in it for you:
Flexible/Hybrid working environment
Professional development and collaborative culture
This company promotes health and wellbeing
Join a team of people who are working with the end goal of a positive impact
Does this sound like you? Please apply here, or reach out directly:
Hannah Watson | Data Analytics & AI | hwatson@humanisedgroup.com
Show more
Show less","AWS Databricks, Python, SQL, Azure, AWS, Data modeling, Data pipelines","aws databricks, python, sql, azure, aws, data modeling, data pipelines","aws, aws databricks, azure, datamodeling, datapipeline, python, sql"
Data Engineer,Edison Talent,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-edison-talent-3784968670,2023-12-17,Queensland, Australia,Mid senior,Hybrid,"Your new company
Our client is seeking an experienced Senior Data Engineer to join their Technology team. The successful candidate will be responsible for data pipeline and visualisation, investigating raw data, build data pipelines and validate transformation logic.
Your new role
Responsibilities:
Highly experienced using Snowflake platform
Experience designing and implementing data warehouse solutions
Experience designing, building, and reviewing data pipelines using Snowflake’s platform capability
Experience with python
Developing and maintaining data models (conceptual, logical, and physical), metadata management, and data cataloguing processes.
Can engage effectively with Developers and Architects (Data, Integration, etc)
Data governance experience
What you’ll get in return
Benefits:
Hybrid working environment – with flexible work arrangements.
Competitive hourly/day rate
Initial 6 month contract starting ASAP
What to do now
Apply now or contact Tracey Vela for a confidential chat on 0420375479 or tracey@edisontalent.com.au
Show more
Show less","Snowflake, Python, Data pipeline, Data visualization, Data governance, Data modeling, Metadata management, Data cataloging, Data warehouse","snowflake, python, data pipeline, data visualization, data governance, data modeling, metadata management, data cataloging, data warehouse","data cataloging, data governance, data pipeline, datamodeling, datawarehouse, metadata management, python, snowflake, visualization"
Data Engineer,Lime Recruitment,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-lime-recruitment-3768776515,2023-12-17,Queensland, Australia,Mid senior,Hybrid,"Lime Recruitment is currently working with a Queensland government department to recruit a Data Engineer contract for 6 months with possible extension based in the Brisbane CBD and working from home.
Key Duties and Accountabilities will include
Designing, and building major aspects of the data warehouse, including data pipelines, and related components within cloud environments
Creating technical designs, options papers conceptual designs and documentation
Contribute to creating and updating data platform roadmaps
Provide guidance to a small team of data specialists
Ongoing ownership of solutions you build to ensure they remain fit for purpose, meet customer needs, meet technology health metrics, and align with architectural guidelines.
Collaborate with internal and external stakeholders to understand business requirements and translate them into effective data solutions.
Implement data warehousing and dimensional data modelling concepts to ensure data accuracy and integrity.
Contribute and adhere to technology standards, data governance & privacy policies and IT security standards
Provide expert technical advice and contribute to project phases including strategy, advisory, design, testing, and deployment.
Collaborating with other stakeholders to ensure the data platform is aligned with business requirements and architecture
Assessing the platform and recommending solutions to improve and ensure it meets cost, security and privacy requirements
Provide vision and strategy for existing and future applications and expand knowledge into other Cloud and/or hybrid solutions
Queensland based candidates only please.
This position closes soon so APPLY NOW or for more information please contact Cassandra Manthey on 0434 717 634 or email cass@limejobs.com.au. Lime Recruitment is Queensland owned and operated and is a Tier 1 preferred supplier to Queensland Government. For more information, visit www.limerecruitment.com.au.
Show more
Show less","Data Warehousing, Data Pipelines, Data Modeling, Dimensional Modeling, Data Accuracy, Data Integrity, Data Governance, Data Privacy, Data Security, Business Requirements, Technical Design, Data Platform Roadmaps, Data Solutions, Cost Optimization, Security Optimization, Privacy Optimization","data warehousing, data pipelines, data modeling, dimensional modeling, data accuracy, data integrity, data governance, data privacy, data security, business requirements, technical design, data platform roadmaps, data solutions, cost optimization, security optimization, privacy optimization","business requirements, cost optimization, data accuracy, data governance, data integrity, data platform roadmaps, data privacy, data security, data solutions, datamodeling, datapipeline, datawarehouse, dimensional modeling, privacy optimization, security optimization, technical design"
Senior Data Platform Engineer - Level 3,Transmax Pty Ltd,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/senior-data-platform-engineer-level-3-at-transmax-pty-ltd-3769880493,2023-12-17,Queensland, Australia,Mid senior,Hybrid,"Who is Transmax?
Transmax is a transport solutions provider of the ITS platform, STREAMS. We exist to improve people’s quality of life and helps move millions of commuters around Australian road networks every day. We partner with customers to deliver creative solutions that optimise transport networks and support safer and more reliable road journeys.
It’s an exciting time in our history as we modernise our core STREAMS product and work with customers to enable the future of mobility.
About the role:
Transmax has grown significantly over the last 18 months, and we are not slowing down!
We’re in the next exciting stage of our growth phase which is why we are now recruiting for a Senior Data Platform Engineer to play a vital role in our company’s future.
Working closely with our empowered product team you will be responsible for conceiving and creating an ITS data platform that delivers highly performant data focusing on:
Activities such as Ingestion, cleansing and fusion, analysis and reporting
Creation of real-time dashboards, predictive analytics and machine learning
Producing strategic response capabilities built around extensible long-term data repositories and industry open data integrations
Requirements
About You:
You will be preferably tertiary-level qualified in Software Engineering, Information Technology or Computer Science, with at least 5 years of industry experience in designing, building, and supporting data systems.
You will be an empathetic team player displaying excellent problem solving and communication skills and passionate about data, analytical techniques, uncovering insights and empowering users!
Your toolkit for success:
Experience with working in a software-as-a-service environment that utilises DevOps and SRE practices to continuously develop, test, deploy and support scalable, highly-available software services
Comfortability working in both Windows and Linux environments, and with utilising public cloud infrastructure and services, such as offered by Amazon Web Services (AWS)
Demonstrated experience in developing data lakes and data marts (including ingestion pipelines and transformations)
Experience with relational (e.g. PostgreSQL) and non-relational (e.g. Elasticsearch) database technologies and query syntax, and optimisation of query performance
Proficiency with associated data processing technologies (such as Apache Spark) as well as including hands-on experience with utilising extensible data platform frameworks (such as Databricks)
Experience with source code control and CI/CD pipeline technologies (such as GitLab), with some exposure to IaaC concepts and tools
Be adept at understanding and utilising internet protocols and application interface technologies such as REST, AMQP, JSON, geoJSON, Parquet, XML and Protobuf
Benefits
What does Transmax offer?
We offer:
A competitive salary
A high-trust, high-empowering, openly supportive working environment & culture
WFH options (2 days in office per week)
Uncapped training budgets to support your career development
Employee Assistance Program with access up to 10 sessions
A recently renovated office in Milton (with some pretty great views, if you ask us)
Corporate private health discounts through Bupa
Flexible working hours & arrangements (the right to disconnect & enjoy life!)
Lots of support for your career plans and advancement
An environment where you’re encouraged to succeed
With our culture of recognition, career development, and a supportive and stimulating and satisfying work environment - your next career chapter is waiting for you at Transmax.
If you’re interested in pursuing this opportunity, please apply by submitting your CV and a brief cover letter outlining your experience and suitability for the role. We would love to hear from you!
Successful applicants will be subject to a National Criminal History Check and a Qualification check.
Applicants must hold full working rights within Australia.
We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.
Show more
Show less","Apache Spark, Databricks, PostgreSQL, Elasticsearch, DevOps, SRE, REST, AMQP, JSON, geoJSON, Parquet, XML, Protobuf, Linux, Windows, AWS, GitLab, CI/CD","apache spark, databricks, postgresql, elasticsearch, devops, sre, rest, amqp, json, geojson, parquet, xml, protobuf, linux, windows, aws, gitlab, cicd","amqp, apache spark, aws, cicd, databricks, devops, elasticsearch, geojson, gitlab, json, linux, parquet, postgresql, protobuf, rest, sre, windows, xml"
Senior Data Engineer,AJEKA,"Queensland, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-ajeka-3748873187,2023-12-17,Queensland, Australia,Mid senior,Hybrid,"We are actively seeking a highly skilled data engineer to engage with one of our esteemed financial services clients and lead the junior data engineers. If you possess a genuine enthusiasm for the art of turning data into actionable insights and have a robust knowledge of Data Engineering principles, platforms and continuous development, we invite you to get in touch with us.
Requirements:
Strong data engineering skills with ideally 5+ years of experience in a similar role.
Understanding of big data considerations, such as security, governance, reliability & scalability.
Demonstrated experience in engineering with contemporary data platforms, specifically in the cloud, with a minimum of two years of hands-on experience.
Proven experience working on data migration and/or system integration projects.
A passion for creating and maintaining best-in-class data practices in a cloud environment.
Proven industry experience on large projects within the financial services sector or others highly regulated industries.
Problem-solving and self-learning skills.
Self-motivated, able to take initiative, contribute to the team and develop others.
Preferred:
Experience with BI, Analytics, Visualisation tools, Power BI.
Experience with ETL / ELT and Data Pipelines.
Experience with CI/CD and Infrastructure as a Code.
Certification and experience in Azure Databricks.
Experience with Data platform solution such as Informatica, Snowflake, DBT, ADF, Purview.
Responsibilities:
Collaborate with interstate teams to design, develop and maintain scalable data pipelines that drive business insights and the support decision making process.
Working with cloud-based data storage and processing services to build scalable and cost-effective data solutions.
Create and optimize ETL processes to move and transform data from various sources into our data systems, ensuring data consistency and integrity.
Develop, construct, install, test, and maintain data architectures (databases, large-scale processing systems, and data pipelines) that support data extraction, transformation, and loading (ETL) processes.
Adhering to the standards and data governance practices of the organisation, e.g. ensuring data quality, privacy, security, auditability and control throughout the data lifecycle.
Planning and execution of unit testing, then supporting all phases of testing through to implementation.
Develop and maintain data models, schemas, and database designs to support business needs and reporting requirements.
Adhering to code controls with appropriate tools, e.g. GitHub.
Why Ajeka?
Extensive training & education programs
Flexible working arrangements
Competitive salary packaging
Exciting innovation opportunities
Show more
Show less","Data Engineering, Big Data, Cloud Computing, Data Migration, System Integration, Data Best Practices, Financial Services, ProblemSolving, SelfLearning, Data Visualization, Power BI, ETL / ELT, Data Pipelines, CI/CD, Infrastructure as a Code, Azure Databricks, Informatica, Snowflake, DBT, ADF, Purview, Data Storage, Data Processing, Data Architecture, Data Extraction, Data Transformation, Data Loading, Data Governance, Data Quality, Data Privacy, Data Security, Data Auditability, Data Control, Unit Testing, Data Modeling, Schema Design, Database Design, Code Controls, GitHub","data engineering, big data, cloud computing, data migration, system integration, data best practices, financial services, problemsolving, selflearning, data visualization, power bi, etl elt, data pipelines, cicd, infrastructure as a code, azure databricks, informatica, snowflake, dbt, adf, purview, data storage, data processing, data architecture, data extraction, data transformation, data loading, data governance, data quality, data privacy, data security, data auditability, data control, unit testing, data modeling, schema design, database design, code controls, github","adf, azure databricks, big data, cicd, cloud computing, code controls, data architecture, data auditability, data best practices, data control, data engineering, data extraction, data governance, data loading, data migration, data privacy, data processing, data quality, data security, data storage, data transformation, database design, datamodeling, datapipeline, dbt, etl elt, financial services, github, informatica, infrastructure as a code, powerbi, problemsolving, purview, schema design, selflearning, snowflake, system integration, unit testing, visualization"
Senior Cloud Data engineer,iVEGA Consulting,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/senior-cloud-data-engineer-at-ivega-consulting-3779950625,2023-12-17,Queensland, Australia,Mid senior,Hybrid,"Requirements:
Hands-on experience with designing, building and maintaining data storage
Hands-on experience with Snowflake
Advanced Technical proficiency required in AWS Services, e.g. AWS DynamoDB, AWS Glue, AWS IAM, AWS Lambda, ECS, AWS Step Function, AWS Batch, AWS S3 Bucket,
Experience with Azure Services, e.g. Azure SQL Database, Azure Cosmo DB, Azure Data Factory, Azure Logic Apps, Azure Functions, Azure Blob Storage,
Working experience in database development, data engineering (ETL in Cloud), and experience with handling JSON, CSV files etc.,
Understanding of information/data management, data structures, data storage, and data modelling techniques,
Advanced proficiency in Programming languages – Python and Object Oriented Design, Powershell, Bash Scripting Skills,
Extensive experience in SQL, including designing and optimizing complex queries,
Working experience in Terraform,
Experience with CI/CD pipelines.
Show more
Show less","Data storage design and maintenance, Snowflake, AWS services (DynamoDB Glue IAM Lambda ECS Step Function Batch S3), Azure services (SQL Database Cosmo DB Data Factory Logic Apps Functions Blob Storage), Database development, Data engineering (ETL in Cloud), JSON and CSV file handling, Information/data management, Data structures, Data modelling techniques, Python, Object Oriented Design, Powershell, Bash Scripting, SQL query design and optimization, Terraform, CI/CD pipelines","data storage design and maintenance, snowflake, aws services dynamodb glue iam lambda ecs step function batch s3, azure services sql database cosmo db data factory logic apps functions blob storage, database development, data engineering etl in cloud, json and csv file handling, informationdata management, data structures, data modelling techniques, python, object oriented design, powershell, bash scripting, sql query design and optimization, terraform, cicd pipelines","aws services dynamodb glue iam lambda ecs step function batch s3, azure services sql database cosmo db data factory logic apps functions blob storage, bash scripting, cicd pipelines, data engineering etl in cloud, data modelling techniques, data storage design and maintenance, data structures, database development, informationdata management, json and csv file handling, object oriented design, powershell, python, snowflake, sql query design and optimization, terraform"
Data Engineer,TechnologyOne,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-technologyone-3769010401,2023-12-17,Queensland, Australia,Mid senior,Hybrid,"As a Data Engineer at TechnologyOne, you will play a pivotal role in shaping the future of our data platform. You will be a key member of the Data Team in the design and implementation of Databricks. You will also be responsible for guiding the business on how data should be emitted, ingested, and stored within the Data platform.
What you'll be doing …
Contributing to the solution designs that aligns with the operational and analytical needs of the business, ensuring seamless technical delivery and transition to operations for data and analytical products
Assisting with transitioning dependencies away from tooling such as AWS Kinesis and OpenSearch
Collaborating with teams to design effective data ingestion patterns, while adhering to enterprise strategies and incorporating robust data and access security controls
Contributing to the delivery of a highly performant, secure, and cost-effective cloud-based data solutions, including monitoring mechanisms to facilitate solution management, while contributing to system decommissioning AWS OpenSearch
Implementing data quality control processes to maintain data accuracy, integrity, and consistency across all layers of the data platform
Continuously monitoring, optimising, and troubleshooting data pipelines, addressing any issues that arise and making improvements as needed to ensure optimal performance and reliability
Your talents...
Proven experience with developing and delivering data platform solutions on Databricks and AWS.
Proven experience with Databricks to deliver high quality ETL, Stream and batch workflows
Experience with implementing solutions for infrastructure and application performance monitoring, as well as emitting events to a query engine in a way that is scalable, performant, and cost effective.
Proven engineering experience in Python, Spark and SQL.
Proven ability to translate business requirements into technical solutions.
Experience in designing and managing Terraform modules and deployments.
Benefits…
Competitive remuneration package
Industry leading employee share plan
Free gym membership onsite
Additional 2.5 days of leave per year dedicated to volunteering at a charity of your choice
Free breakfast and coffee on-site
TechnologyOne is proud to be an Equal Opportunity Employer who values diversity and an inclusive workplace. We aim to recruit a diverse range of people with a diverse range of talents to help us to deliver on our mission to better our community.
If you meet a number of the requirements (and not all), we encourage you to submit your application.
Who we are…
TechnologyOne (ASX: TNE) is Australia's largest enterprise software company and one of Australia's top 100 ASX-listed companies, with locations across six countries. We provide a global SaaS ERP solution that transforms business and makes life simple for our customers. Our deeply integrated enterprise SaaS solution is available on any device, anywhere and anytime and is incredibly easy to use. Over 1,200 leading corporations, government agencies, local councils and universities are powered by our software.
For more than 36 years, we have been providing our customers enterprise software that evolves and adapts to new and emerging technologies, allowing them to focus on their business and not technology.
Show more
Show less","Databricks, AWS, AWS Kinesis, OpenSearch, Python, Spark, SQL, Terraform, ETL, Streaming, Batch workflows, Data quality control, Data pipelines, Cloudbased data solutions, Enterprise strategies, Access security controls, Infrastructure performance monitoring, Application performance monitoring","databricks, aws, aws kinesis, opensearch, python, spark, sql, terraform, etl, streaming, batch workflows, data quality control, data pipelines, cloudbased data solutions, enterprise strategies, access security controls, infrastructure performance monitoring, application performance monitoring","access security controls, application performance monitoring, aws, aws kinesis, batch workflows, cloudbased data solutions, data quality control, databricks, datapipeline, enterprise strategies, etl, infrastructure performance monitoring, opensearch, python, spark, sql, streaming, terraform"
L3 Data Engineer,TechConnect IT Solutions,"Gold Coast, Queensland, Australia",https://au.linkedin.com/jobs/view/l3-data-engineer-at-techconnect-it-solutions-3774129099,2023-12-17,Queensland, Australia,Mid senior,Hybrid,"TechConnect are seeking a passionate Senior Data Engineer with proven hands-on experience within the field.  They will be part of a team who will deliver exceptional value to our customers.  You must be well-organised, work exceptionally well with different people (in different locations and time zones), and be able to technically support the implementation of the Customer’s vision. It is necessary for you to be able to speak with various stakeholders both internal and external all the while thriving in a high-pressure environment with highly technical challenges and demanding operational requirements.
Responsibilities
Design and implement data pipelines that extract data from multiple sources and load it into a central data store.
Work with stakeholders to understand business requirements and design data models to meet those requirements.
Optimize and improve existing data pipelines to ensure high performance and reliability.
Develop and maintain data quality checks to ensure the accuracy and completeness of data.
Continuously monitor and maintain data pipelines to ensure they are running smoothly and identify areas for improvement.
Participate in code reviews and collaborate with other engineers to maintain high quality code.
Create visualisations with tools like Power BI.
Stay current with industry trends and recommend new tools and technologies to improve the data infrastructure.
Involved in discover workshops and solutions design activities with customers to elicit requirement and scope projects
Skills and Experience
AWS, Azure, Databrick
Visualisation skills with an eye for aesthetics
Descriptive Data Analytics skills with PowerBI - ability to create reports that can answer specific business questions
Experience in Data Lake establishment and/or data pipeline development.
Dashboard: Tableau; Kibana; Quicksight; PowerBI;
Commercial ETL tools: DBT, Talend, Informatica, or Attunity.
Exposure to Infrastructure as code and cloud deployment
Experience with Bitbucket, GitHub, Azure DevOps
Show more
Show less","AWS, Azure, Databrick, Visualization, Data Analytics, PowerBI, Data Lake, Data Pipeline, Tableau, Kibana, Quicksight, DBT, Talend, Informatica, Attunity, Infrastructure as Code, Cloud Deployment, Bitbucket, GitHub, Azure DevOps","aws, azure, databrick, visualization, data analytics, powerbi, data lake, data pipeline, tableau, kibana, quicksight, dbt, talend, informatica, attunity, infrastructure as code, cloud deployment, bitbucket, github, azure devops","attunity, aws, azure, azure devops, bitbucket, cloud deployment, data lake, data pipeline, dataanalytics, databrick, dbt, github, informatica, infrastructure as code, kibana, powerbi, quicksight, tableau, talend, visualization"
Data Analyst,Talent Street,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-analyst-at-talent-street-3785891371,2023-12-17,Queensland, Australia,Mid senior,Hybrid,"Data / Reporting Analyst | 6 months Contract | Brisbane Government |
Talent Street is looking for a
Data Analyst
with experience in Enterprise Data Lake systems like Azure Synapse, Bricks or similar platforms building end-to-end data pipelines.
Commencement Date : 08/01/2024 to 30/06/2024 (with possible extension)
Location : Brisbane CBD
The Data Analyst / Reporting Analyst will:
Review and document existing legacy data warehouse outputs. Confirm continued requirement for and utilisation of outputs in conjunction with SME, Business Analyst and Delivery Lead.
Review static reports from new system and document where required. Confirm continued requirement for and utilisation of outputs in conjunction with SME, Business Analyst and Delivery Lead.
Develop and maintain data model/relationships (conceptual/logical) required to replicate legacy data warehouse outputs in new DW system.
Design, develop, and maintain data model/relationships (conceptual/logical) required to ingest and process new data sources as well as provide outputs required by business.
Develop and maintain project artefacts such as business problem statements, business requirements, change impact assessment, and transition to operation documents.
Escalate issues, such as scope change or misalignment, to the Project Manager.
Engage with stakeholders to conduct business and technical workshops to elicit and define requirements and business processes.
Engage with internal and external stakeholders to conduct business and technical workshops to elicit and define requirements and business processes.
Engage with external departments that provide clients with data-on-data processes.
Foster and model a collaborative team environment and open and transparent communication.
Help develop future state models utilising appropriate modeling techniques to document end-to-end service delivery.
Identify and document data sources and schemas of source data to be ingested in a new data lake.
Develop and support metadata management and business rules management for any data transformation required for data progression through the lakehouse.
Develop test scenarios together with Testers/business analysts.
Review data solutions with EDW Solution Architect.
Assist and support developers and testers throughout the development lifecycle or bug fixes.
Develop and support data security and associated user roles.
Monitor data processing and integration.
Coordinate and manage data solution-related projects together with solution architects and project managers.
Skills and Experience:
Experience working on
Enterprise Data Lake systems like Azure Synapse, Bricks
or similar platforms building end-to-end data pipelines
Good understanding of
Data Warehousing principles
and
ETL/ELT practices and governance concepts
and approaches
Experience
designing and developing SQL queries
.
Technical skills in creating visualisations in
Spark, Power BI, Tableau etc.
Familiar with ITSM tools such as
ServiceNow, JIRA, or Azure DevOps.
Attention to detail, precision, and able to troubleshoot data quality issues.
Experience in maintaining conceptual, logical, and dimensional data models.
Professional presentation and communication skills (written and verbal), formal documentation, diagramming, and modeling.
Critical thinking and problem-solving skills
If you are ready for an exciting opportunity, please
APPLY
here, or contact Kruti Patel, at 0404 225 344 or send your updated CV with a suitability statement on kpatel@talentstreet.com.au
Show more
Show less","Data Lake systems, Azure Synapse, Bricks, Data Warehousing, ETL/ELT, Data Modeling, Data Integration, Data Governance, SQL, Data Visualization, Spark, Power BI, Tableau, ServiceNow, JIRA, Azure DevOps, Data Quality, Conceptual Data Models, Logical Data Models, Dimensional Data Models, Presentation Skills, Communication Skills, Critical Thinking, ProblemSolving","data lake systems, azure synapse, bricks, data warehousing, etlelt, data modeling, data integration, data governance, sql, data visualization, spark, power bi, tableau, servicenow, jira, azure devops, data quality, conceptual data models, logical data models, dimensional data models, presentation skills, communication skills, critical thinking, problemsolving","azure devops, azure synapse, bricks, communication skills, conceptual data models, critical thinking, data governance, data integration, data lake systems, data quality, datamodeling, datawarehouse, dimensional data models, etlelt, jira, logical data models, powerbi, presentation skills, problemsolving, servicenow, spark, sql, tableau, visualization"
Data Engineering Lead,AJEKA,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-engineering-lead-at-ajeka-3769062221,2023-12-17,Queensland, Australia,Mid senior,Hybrid,"We are currently looking for a Data Engineering Lead to become a valuable member of our team and collaborate with our esteemed clients in the financial services sector. If you have a true passion for leading a team and a strong understanding of data engineering principles, we encourage you to reach out to us.
Requirements:
5+ years of experience in a Senior Data Engineer role.
Deep understanding of data engineering methodologies, frameworks and processes.
Experience mentoring, developing, and growing a team of data engineers.
Understanding of big data considerations, such as security, governance, reliability and scalability.
Proven experience in engineering within modern data platforms.
A passion for creating and maintaining best-in-class data practices in a cloud environment.
Proven experience on large data migration projects within the financial services sector.
Proficiency in handling ETL/ELT processes and expertise in designing and managing data pipelines.
Problem-solving and self-learning skills.
Self-motivated, capable of taking the initiative and contributing to the team.
Preferred:
Experience with BI, Analytics, Visualisation tools, Power BI.
Experience with CI/CD and Infrastructure as a Code.
Responsibilities:
Leading the development and performance of a data engineering team including mentoring junior engineers.
Building repeatable data engineering methodologies and practice materials.
Collaborate with interstate teams to design, develop, and maintain scalable data pipelines that drive business insights and the support decision making process.
Working with cloud-based data storage and processing services to build scalable and cost-effective data solutions.
Create and optimize ETL processes to move and transform data from various sources into our data systems, ensuring data consistency and integrity.
Develop, construct, install, test, and maintain data architectures (databases, large-scale processing systems, and data pipelines) that support data extraction, transformation, and loading (ETL) processes.
Adhering to the standards and data governance practices of the organisation, e.g. ensuring data quality, privacy, security, auditability and control throughout the data lifecycle.
Planning and execution of unit testing, then supporting all phases of testing through to implementation.
Develop and maintain data models, schemas, and database designs to support business needs and reporting requirements.
Adhering to code controls with appropriate tools, e.g. GitHub.
Why Ajeka?
Extensive training and education programs
Flexible working arrangements
Competitive salary packaging
Exciting innovation opportunities
Show more
Show less","Data Engineering, Data Pipelines, ETL/ELT, Data Governance, Data Security, Data Architecture, Cloud Computing, Big Data, Data Migration, BI/Analytics/Visualization, Power BI, CI/CD, Infrastructure as a Code, Unit Testing, Data Modeling, Schema Design, Database Design, GitHub","data engineering, data pipelines, etlelt, data governance, data security, data architecture, cloud computing, big data, data migration, bianalyticsvisualization, power bi, cicd, infrastructure as a code, unit testing, data modeling, schema design, database design, github","bianalyticsvisualization, big data, cicd, cloud computing, data architecture, data engineering, data governance, data migration, data security, database design, datamodeling, datapipeline, etlelt, github, infrastructure as a code, powerbi, schema design, unit testing"
Data Analyst,Centacare Brisbane,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-analyst-at-centacare-brisbane-3775092404,2023-12-17,Queensland, Australia,Mid senior,Hybrid,"The Organisation
Centacare, an Agency of the Catholic Archdiocese of Brisbane, is a values-based organisation dedicated to providing services to the entire community, irrespective of religion, circumstance, ethnicity, economic situation, age, gender, or ability. With a workforce of over 3,000 employees and volunteers, Centacare operates in more than 200 locations, supporting tens of thousands of people each year across various Directorates.
The Role
We currently have an exciting career progression opportunity for the role of Data Analyst. This role will be a permanent full time position. The Data Analyst will play a pivotal role supporting Centacare Business Intelligence and Data Analytics program by providing timely and accurate insights for decision-making, ensuring data quality, collaborating with various teams to optimise data usage, and contributing to operational improvements.
Key Duties
Collaborate with key stakeholders to ensure accurate documentation, planning, and efficient use of data while fostering continuous improvement and compliance with data governance policies.
Follow change management processes, including analysis, planning, development, documentation, implementation, testing, and rollback of data-related changes with a focus on risk evaluation.
Generate reports to support business goals, maintain data quality, collaborate across teams, analyse performance, communicate findings effectively, follow data governance, and stay updated on industry trends for data analysis and reporting in the aged care and disability sector.
Enhance privacy, data protection, workplace gender equity, and anti-discrimination measures to minimize the Archdiocese's exposure to contractual and professional liability.
Skills And Experience
A minimum of 3 years previous experience in data analysis or a related role within the Not For Profit space, preferably in a complex program/project or enterprise environment.
Strong data analysis skills, with experience in using data analytics tools and techniques (with certification in data analytics and visualisation tools like SQL, Boomi, Power BI desirable)
Excellent interpersonal skills, including problem resolution, negotiation, the ability to guide system users and communicate both technical and non-technical information at all levels.
Excellent time management skills with the ability to manage activities within established timeframes.
Ability to create business process models (people, processes, and technology)
Sound decision-making, conflict resolution and problem-solving skills.
Ability to obtain and maintain a current blue card, criminal history check, NDIS Worker Screening Clearance (when required) and evidence of rights to work in Australia.
Why Work for us?
This is a unique opportunity to work for a values-based organisation and develop your skills at one of the largest employers in Queensland. In addition to a competitive remuneration package including 12% superannuation,6 weeks paid parental leave, 13 weeks Long Service leave plus PBI packaging benefits of up to $18,550, you will be able to be part of an organisation that truly values their employees and providing client centric care to our clients.
How To Apply
If you are passionate about using data to make a difference in the community and meet the qualifications outlined, we invite you to apply for this role.
Click
APPLY
and submit a cover letter that clearly addresses the above-listed skills, experience and essential criteria, as well as a copy of your current résumé. If you would like to know more about this role, please contact bradley.shannon@bne.centacare.net.au
Please note shortlisting and interviews will commence as applications are received.
The Archdiocese of Brisbane has standards of conduct for workers to maintain a safe and healthy environment for children. Our commitment to these standards requires that we conduct working with children checks and background referencing for all persons who will engage in direct and regular involvement with children and young people (0 - 18 years) and/or vulnerable adults. The organisation is fully committed to child safety and has a zero tolerance to abuse of children or vulnerable adults.
Show more
Show less","Data analysis, SQL, Boomi, Power BI, Data analytics, Data visualisation, System user guidance, Time management, Business process modelling, Decisionmaking, Conflict resolution, Problemsolving","data analysis, sql, boomi, power bi, data analytics, data visualisation, system user guidance, time management, business process modelling, decisionmaking, conflict resolution, problemsolving","boomi, business process modelling, conflict resolution, data visualisation, dataanalytics, decisionmaking, powerbi, problemsolving, sql, system user guidance, time management"
"Data Engineering Team Lead - South Bank, QLD",Flight Centre Travel Group,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-engineering-team-lead-south-bank-qld-at-flight-centre-travel-group-3780863407,2023-12-17,Queensland, Australia,Mid senior,Hybrid,"Hello, Hej, 你好, Salut!
At Flight Centre Travel Group (FCTG) our purpose is to 'open up the world for those who want to see' . Every day, we give people all around the world the opportunity to experience something really amazing – travel!
As the Data Engineering Team Lead you'll serve as a skilled and experienced professional responsible for leading a team of data engineers in designing, developing, and maintaining data infrastructure. The primary purpose of this role is to ensure the team's efficiency and the reliability of data processing, storage, and retrieval, while also providing technical leadership, guidance, and strategic direction to the team.
If you're looking for a hands on role where you will leverage your senior engineering skills to deliver work while managing the team, please apply!
Day to day:
Lead, mentor, and manage a dynamic team of data engineers, fostering an environment of growth and collaboration
Provide strategic guidance in the design and architecture of data infrastructure, ensuring alignment with organisational objectives, such as scalability, performance, and data quality
Collaborate with your team to design, develop, and maintain ETL processes and data pipelines
Oversee the management of data lakes and databases, optimising for performance and reliability
Spearhead initiatives to optimise data pipelines, databases, and data storage for efficiency and cost-effectiveness
Guarantee data accuracy, consistency, and reliability
Establish and uphold robust data security measures, including access controls and encryption, to safeguard sensitive information
Ensure compliance with data privacy regulations, industry standards, and best practices
Develop and maintain comprehensive documentation for data engineering processes, data schemas, and best practices
Ensure team members adhere to established standards and protocols
Collaborate with BI teams, data analysts, data scientists & other stakeholders to gain an understanding of their requirements
Evaluate the suitability of new tools and technologies for the organisation, making recommendations for adoption when necessary
Plan and oversee data engineering projects
Lead the team in addressing complex data engineering challenges and provide guidance in identifying and resolving issues
Offer guidance and support for team members' professional growth, fostering a culture of learning and development
We'd love to hear from you if you have:
At least 5 years of experience in data engineering, with experience leading teams of engineers and delivering high-quality products
Experience working closely with data scientists, data analysts, and business intelligence teams
Demonstrated experience working across data lake, big data, and data pipelines in an enterprise environment
A degree in Computer Science, Software Engineering, or a related field
You'll earn brownie points with:
Databricks Certified Data Engineer Professional
What You'll Enjoy...
Tick destinations off your bucket list with our discounts on travel and accommodation
Employee Share Scheme through Equate Plus
Access to in-house financial and health services, internal 24/7 gym and End-of-Trip Facility
Receive ongoing training and professional development
Hybrid working model
Closing the Gender Gap in Super Balances! FCTG paid super contributions during parental leave & top up payments when Flightie parents return to work for the first 2 years!
Preference for internal progression through Brightness of Future
Global career opportunities in a network of brands and businesses at your fingertips
Proud Corporate Social Responsibility platform through the Flight Centre Foundation, and Brighter Futures programs
Opportunity to attend global awards events, including Global Gathering (Las Vegas 2022, Bali 2023 and Lisbon 2024)
Various social events to promote networking, the celebration of wins, and sometimes just for fun!
We value you...
Flight Centre Travel Group is committed to creating an inclusive and diverse workplace that supports your unique identity to create better, safer experiences for everyone. We encourage you to come as you are; to foster inclusivity and collaboration. We celebrate you.
Who We Are...
Since our beginning, our vision has always been to open up the world for those who want to see.
As a global travel retailer, our people come from all different backgrounds, and our connections spread to the far reaches of the globe - 20+ countries and counting! Together, we are a family (we call ourselves Flighties).
We offer genuine opportunities for people to grow and evolve. We embrace new experiences, we celebrate the wins, seize all opportunities, and empower all of our people to find their Brightness of Future.
We encourage you to DREAM BIG through collaboration and innovation, and make sure you are supported to make incredible ideas a reality. Together, we deliver quality, innovative solutions that delight our customers and achieve our strategic priorities.
Irreverence. Ownership. Egalitarianism
Show more
Show less","Data Engineering, Data Infrastructure, ETL Processes, Data Pipelines, Data Lakes, Databases, Data Security, Data Privacy, Data Governance, Data Standards, Big Data, Data Analytics, Business Intelligence, Data Science, Computer Science, Software Engineering, Databricks","data engineering, data infrastructure, etl processes, data pipelines, data lakes, databases, data security, data privacy, data governance, data standards, big data, data analytics, business intelligence, data science, computer science, software engineering, databricks","big data, business intelligence, computer science, data engineering, data governance, data infrastructure, data lakes, data privacy, data science, data security, data standards, dataanalytics, databases, databricks, datapipeline, etl, software engineering"
Cloud Software Engineer 2 (Data Management),"Farfield Systems, Inc","Linthicum, MD",https://www.linkedin.com/jobs/view/cloud-software-engineer-2-data-management-at-farfield-systems-inc-3787765353,2023-12-17,Towson,United States,Mid senior,Onsite,"About Farfield Systems, Inc
At Farfield we are committed to delivering trusted expertise to our government clients. As we grow, our focus is on increasing opportunities for you to grow with us while still delivering the same excellence customers have grown to expect from us. We continually evaluate our environment to provide a place where your career is packed with opportunities to grow and you have the ability to demonstrate your passion to our customers. We focus on building a Team where each employee is a valued member.
Farfield provides support to multiple agencies across the United States Government in many locations. That means many different opportunities to follow your career path without changing companies every few years.
""Employee driven...customer focused."" We build, operate and secure networks and infrastructure.
*** Requires a Top Secret/SCI clearance with a polygraph and U.S. Citizenship***
Cloud Software Engineer 2
(Data Management):
Essential Duties: The Cloud Software Engineer will create, test, and maintain the various analytics running in Pig and/or PySpark to create various reports for the users. The Engineer will also provide report updates and create new reports based on requirements. Ensure the data is accurate to the end users as well as ensuring day to day operations are successful. The Engineer will also capture raw data from data feeds and normalize the data. Provide rapid response to customer needs and requirements as well as maintain a dictionary of all analytics.
Required: Experience designing and developing automated analytic software, techniques, and algorithms. Experience developing and deploying: data driven analytics; event driven analytics; set of analytics orchestrated through rules engine. Experience documenting data models, schemas, data element dictionaries, and other technical specifications.
Pig, PySpark, Piranhas
Desired: Python, Java, Scala, Apache NiFi, Ansible, Agile Experience, Experience deploying applications in a cloud environment, MongoDB
Powered by JazzHR
Wzawl3Nkhz
Show more
Show less","Pig, PySpark, Piranhas, Python, Java, Scala, Apache NiFi, Ansible, Agile Experience, MongoDB","pig, pyspark, piranhas, python, java, scala, apache nifi, ansible, agile experience, mongodb","agile experience, ansible, apache nifi, java, mongodb, pig, piranhas, python, scala, spark"
Principal Data and Integration Engineer * 100% Remote / W2 Only *,Amerit Consulting,"Maryland Heights, MO",https://www.linkedin.com/jobs/view/principal-data-and-integration-engineer-100%25-remote-w2-only-at-amerit-consulting-3780541715,2023-12-17,East Saint Louis,United States,Mid senior,Remote,"Overview:
Our client, a US Fortune 500 company and a provider of managed Health care, Pharmacy benefits & specialty areas for managed care organizations / employers / government agencies, seeks an accomplished
Principal Data and Integration Engineer.
*** Candidate must be authorized to work in USA without requiring sponsorship ***
********************************************************
*** Location: 100% Remote (Maryland Heights, MO)
*** Duration: 3+ months with possible conversion
Working Schedule: Hours: 8am to 5pm Central
Summary:
Develops, tests and maintains code using software development methodology and appropriate technologies for the system being used. Works closely with Business Analysts to develop detail systems design and written test plans for online and report application programs. Participates in design walkthroughs with appropriate focus groups and related users to verify the accuracy of design in meeting business needs. Prepares installation instructions and coordinates installation procedures. Supports and troubleshoots application code problems. Provides status reports that give a detailed description of the current projects progress and indicate time devoted to each task of the project. Coordinates, guides, and mentors programming efforts performed by in-house programmers or outside consultants to ensure that all programming is completed according to the project plan.
Responsibilities:
Develops, tests and maintains code using software development methodology and appropriate technologies for the system being used.
Works closely with Business Analysts or System Analyst to develop detailed systems design and written test plans for online and report application programs.
Performs analysis on projects and provides a project plan that shows the tasks needing to be completed and a time estimate for each task.
Provides status reports that give a detailed description of the current project's progress and indicate time devoted to each task.
3 Must-Haves (Hard Skills):
Hands-on experience in designing, coding, enhancing, testing and production support of custom SQL Server Datawarehouse to meet business process requirements.
Proficient with Excel and creating Apps in Excel using VBCode.
Confident in Microsoft SQL Server (using SSMS – Advanced TSQL, Stored Procedures, best practices in RDBMS) best practices for writing clean effective code and balancing Declarative customizations with Programmatic customizations.
Nice-To-Haves (Hard Skills)
Knowledge/experience in Oracle (PLSQL using TOAD)
Salesforce knowledge and experience
SnapLogic knowledge and experience
Tableau knowledge and experience
***********************************************************
I'd love to talk to you if you think this position is right up your alley, and assure a prompt communication, whichever direction.
If you're looking for rewarding employment and a company that puts its employees first, we'd like to work with you.
Sam Banga
Lead Recruiter
Company Overview:
Amerit Consulting
is an extremely fast-growing staffing and consulting firm. Amerit Consulting was founded in 2002 to provide consulting, temporary staffing, direct hire, and payrolling services to Fortune 500 companies nationally; as well as small to mid-sized organizations on a local & regional level. Currently, Amerit has over 2,000 employees in 47 states. We develop and implement solutions that help our clients operate more efficiently, deliver greater customer satisfaction, and see a positive impact on their bottom line. We create value by bringing together the right people to achieve results. Our clients and employees say they choose to work with Amerit because of how we work with them - with service that exceeds their expectations and a personal commitment to their success. Our deep expertise in human capital management has fueled our expansion into direct hire placements, temporary staffing, contract placements, and additional staffing and consulting services that propel our clients’ businesses forward.
Amerit Consulting provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Applicants, with criminal histories, are considered in a manner that is consistent with local, state and federal laws.
Show more
Show less","SQL Server, Datawarehouse, TSQL, Visual Basic, Oracle, PLSQL, Salesforce, SnapLogic, Tableau","sql server, datawarehouse, tsql, visual basic, oracle, plsql, salesforce, snaplogic, tableau","datawarehouse, oracle, plsql, salesforce, snaplogic, sql server, tableau, tsql, visual basic"
Data Architect - Government Healthcare (Remote),S2Tech,"Chesterfield, MO",https://www.linkedin.com/jobs/view/data-architect-government-healthcare-remote-at-s2tech-3786832965,2023-12-17,East Saint Louis,United States,Mid senior,Remote,"Job Details
Description
Data Architect – Government Healthcare
Location: Remote
About Us
Known for “Delighting the Client” through performance, innovation and an employee-centric culture, S2Tech is a fast-growing IT consulting company serving clients in over a quarter of the United States. We are widely recognized as a leading provider of both technical and business services in support of Health and Human Services related projects. Feel free to learn more at www.s2tech.com .
Why S2Tech?
:
Stable privately-owned company with a strong reputation for building long-term client relationships through the delivery of consistent value-based service
25 year history of providing IT and Business services to private customers and government programs throughout the United States
Expansive client portfolio and active projects – employees benefit from innovative project exposure and in-house skill development training/courses
Corporate culture that emphasizes the importance of family and promotes healthy work-life balance
Offer competitive pay and a range of benefits including:
Medical / Dental / Vision Insurance – insurance premium assistance provided
Additional Insurance (Life, Disability, etc.)
Paid Time Off (Vacation & Sick Leave)
401(k) Retirement Savings Plan & Health Savings Account
Various training courses to promote continuous learning
Corporate Wellness Program
Be part of a company that gives back through its non-profit organization, Fortune Fund, which was launched in 2001. The goal of the Fortune Fund is to close the rural/urban divide by ensuring children in rural communities in India and the United States understand the importance of education & are aware of professional career opportunities allowing them to link their professional & educational goals
Job Overview
We are seeking a highly skilled and experienced Data Architect that will be responsible for designing and engineering scalable, integrated solutions, focusing on data structures such as data warehouses, data lakes, and data marts. The primary objective is to enhance data quality, streamline data flow, and maintain security standards, particularly aligning with the Health Insurance Portability and Accountability Act (HIPAA) requirements.
Responsibilities
Design and engineer scalable, integrated solutions encompassing data warehouses, data lakes, data marts, applications, and infrastructure
Improve data quality, optimize data flow, and ensure compliance with HIPAA standards
Architect and model an enterprise data presentation layer, semantic layer, distribution layer, dependent data marts, and business intelligence reporting solutions against an enterprise-wide data warehouse
Act as the primary interface between business stakeholders, enterprise architecture, and technical resources in the development of information management solutions
Collaborate with cross-functional teams to understand business requirements and translate them into effective data management solutions
Ensure adherence to industry standards in the design and implementation of data architectures
Stay abreast of emerging trends and technologies in data management and make recommendations for continuous improvement
Qualifications
Bachelor's Degree in Information Systems Engineering, Computer Science, or a related field is preferred
Minimum of five (5) years of experience in designing and engineering data management solutions for government healthcare systems
Extensive experience in architecting and modeling enterprise data presentation layers, semantic layers, distribution layers, dependent data marts, and business intelligence reporting solutions against enterprise-wide data warehouses
Superior communication skills with the ability to serve as the primary interface between business, enterprise architecture, and technical teams
Proficiency in data modeling, database design, and data architecture principles
In-depth knowledge of data warehousing, data lakes, and data mart concepts
Strong understanding of HIPAA standards and experience implementing security measures in data management solutions
Expertise in business intelligence reporting solutions and tools
Excellent problem-solving skills and the ability to think strategically about data architecture
S2Tech is committed to hiring and retaining a diverse workforce. We are an equal opportunity employer making decisions without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status or any other protected class.
Show more
Show less","Data Architect, Data Structures, Data Warehouses, Data Lakes, Data Marts, Data Quality, Data Flow, HIPAA, Enterprise Data Presentation Layer, Semantic Layer, Distribution Layer, Dependent Data Marts, Business Intelligence Reporting Solutions, Data Modeling, Database Design, Data Architecture Principles, Business Requirements, Data Management Solutions, Industry Standards, Emerging Trends, Data Management, Information Systems Engineering, Computer Science, Government Healthcare Systems, Communication Skills, Business Intelligence Reporting Tools, ProblemSolving Skills","data architect, data structures, data warehouses, data lakes, data marts, data quality, data flow, hipaa, enterprise data presentation layer, semantic layer, distribution layer, dependent data marts, business intelligence reporting solutions, data modeling, database design, data architecture principles, business requirements, data management solutions, industry standards, emerging trends, data management, information systems engineering, computer science, government healthcare systems, communication skills, business intelligence reporting tools, problemsolving skills","business intelligence reporting solutions, business intelligence reporting tools, business requirements, communication skills, computer science, data architect, data architecture principles, data flow, data lakes, data management, data management solutions, data marts, data quality, data structures, data warehouses, database design, datamodeling, dependent data marts, distribution layer, emerging trends, enterprise data presentation layer, government healthcare systems, hipaa, industry standards, information systems engineering, problemsolving skills, semantic layer"
Lead Electrical Engineer (Mission Critical/Data Center),WSP in the U.S.,"Creve Coeur, MO",https://www.linkedin.com/jobs/view/lead-electrical-engineer-mission-critical-data-center-at-wsp-in-the-u-s-3757436536,2023-12-17,East Saint Louis,United States,Mid senior,Hybrid,"Who We Are
At WSP, we are driven by inspiring future-ready pioneers to innovate. We’re looking to grow our teams with people who are ready to collaborate in building communities and expanding our skylines. To do this, we hire candidates of all experiences, skillsets, backgrounds and walks of life. We actively foster a work environment and culture where inclusion and diversity is part of our fundamental structure. This is delivered behaviorally, through our policies, trainings, local partnerships with professional diverse organizations, internal networks and most importantly with the support and sponsorship of our leaders who help drive our commitment to an inclusive, diverse, welcoming and equitable work environment. Anything is within our reach and yours as a WSP employee. Come join us and help shape the future!
Great people. Great places. Great projects. kW Mission Critical Engineering, a WSP company, is a high-performance, fast-paced consulting engineering firm designing data centers and mission critical environments across the globe. We hire smart, responsive, team players to work in collaborative and mentoring office settings. Our mechanical, electrical, plumbing, fire protection, controls, telecommunications, and security building system designs keep many of the world’s top Fortune 100 financial, technology, enterprise, hyperscale, and colocation companies up and running 24 hours a day, 365 days a year.
We work on innovative, award-winning, large-scale projects. We travel to construction sites to see our designs being built. As part of WSP, we are able to offer our employees increased professional development and career opportunities in addition to kW MCE’s office culture which is consistently recognized as one of the “Best Places to Work.” Join our great people at our great places designing great projects.
This Opportunity
What You’ll Do:
kW Mission Critical Engineering
is currently initiating a search for a
Lead Electrical Engineer
that can be located for our
kW Tempe, Arizona office or our St. Louis, MO office.
As a Lead Electrical Engineer with us, you will design complex power and other building systems including generator plants, medium voltage distribution, uninterruptible power systems, lighting, fire alarm, and grounding while leading projects and a team of electrical engineers.
Your Impact
Produce high quality technical and professional deliverables for projects and proposals
Apply deep knowledge of engineering techniques across multiple technical functions
Utilize advanced analytical and design techniques to solve technical problems
Exemplify well-developed advanced experience in electrical discipline
Lead the development of initial electrical system concepts
Present complex technical solutions to clients
Manage and coordinate project teams and projects
Develop work plans to address technical issues within project time and budget
Work within multi-discipline project teams to develop drawing and specification documents for issuance to architects, contractors and building owners
Attend and lead client meetings
Manage and mentor junior staff
Collaborate and coordinate with internal project discipline team members, equipment vendors and manufacturers
Perform project management activities including writing proposals, establishing budgets, and managing client interactions
Coordinate activities concerned with technical development, scheduling, and resolving engineering design issues
Coordinate the activities of technical staff from project award through project completion
Design complex and large electrical medium voltage and low voltage distribution systems and electrical building systems (i.e. general power, lighting, grounding, etc.)
Survey and evaluation of existing conditions
Develop project specifications
Perform construction administration
Develop and maintain client relationships
Contribute and interact with team, develop and manage high quality technical and professional deliverables on projects and proposals
Participate in local professional organization (attend meetings/lectures), i.e., poster sessions, participate in conference panel
Exercise responsible and ethical decision-making regarding company funds, resources and conduct and adhere to WSP""s Code of Conduct and related policies and procedures
Proven track record of upholding workplace safety and ability to abide by WSP""s health, safety and drug/alcohol and harassment policies
Who You Are
The ideal candidate has familiarity with Building Information Modeling using Revit, has strong communication skills, and an interest in liaising with internal and external design, client and construction team members. Candidate will have previous experience as a lead project electrical engineer capable of directing the project team.
Required Qualifications
Bachelor’s degree in Electrical Engineering or Architectural Engineering with electrical building systems emphasis
7+ years of experience in designing electrical systems for the high performing, commercial, industrial or mission critical/data center buildings
Registered Professional Engineer (PE)
Experience mentoring and training others in field
Strong verbal and written communication skills
Ability to interact well with others as well as develop and contribute to high quality technical and professional deliverables on projects and proposals
Strong working knowledge of electrical systems and codes
Attention to detail, highly organized, self-starter
Participate in conference programs including panels, lectures, poster sessions, papers and presentations
Preferred Qualifications:
Enhancing credentials (LEED, Uptime ATD, etc.) preferred
Experience with the analysis and modeling of short circuit coordination and arc flash studies
Mission Critical/Data Center experience
Experience with international projects and knowledge of international codes and standards
Additional Requirements
To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform essential job functions.
Additional Details
Travel Required: 10%
Job Status: Regular
Employee Type: Full
Primary Location: TEMPE - E RIO SALADO PKWY
All locations: US-AZ-Phoenix, US-AZ-Tempe, US-AZ-Tucson, US-MO-Creve Coeur, US-MO-St Louis
About WSP
WSP USA is the U.S. operating company of WSP, one of the world's leading engineering and professional services firms. Dedicated to serving local communities, we are engineers, planners, technical experts, strategic advisors and construction management professionals. WSP USA designs lasting solutions in the buildings, transportation, energy, water and environment markets. With more than 15,000 employees in over 300 offices across the U.S., we partner with our clients to help communities prosper. www.wsp.com
WSP provides a flexible and agile workplace model while meeting client needs. Employees are also afforded a comprehensive suite of benefits including medical, dental, vision, disability, life, and retirement savings focused on providing health and financial stability throughout the employee’s career.
At WSP, we want to give our employees the challenges they seek to grow their careers and knowledge base. Your daily contributions to your team will be essential in meeting client objectives, goals and challenges. Are you ready to get started?
WSP USA (and all of its U.S. companies) is an Equal Opportunity Employer Race/Age/Color/Religion/Sex/Sexual Orientation/Gender Identity/National Origin/Disability or Protected Veteran Status.
The selected candidate must be authorized to work in the United States.
NOTICE TO THIRD PARTY AGENCIES:
WSP does not accept unsolicited resumes from recruiters, employment agencies, or other staffing services. Unsolicited resumes include any resume or hiring document sent to WSP in the absence of a signed Service Agreement where WSP has expressly requested recruitment/staffing services specific to the position at hand. Any unsolicited resumes, including those submitted to hiring managers or other business leaders, will become the property of WSP and WSP will have the right to hire that candidate without reservation – no fee or other compensation will be owed or paid to the recruiter, employment agency, or other staffing service.
Show more
Show less","Revit, Electrical engineering, Building information modeling, LEED, Uptime ATD, Short circuit coordination, Arc flash studies, Mission critical/data center, Medium voltage distribution, Low voltage distribution, General power, Lighting, Grounding, Fire alarm, Electrical building systems, Power systems, Uninterruptible power systems, Generator plants, Construction management, Microsoft Office Suite, AutoCAD, Project management, Engineering","revit, electrical engineering, building information modeling, leed, uptime atd, short circuit coordination, arc flash studies, mission criticaldata center, medium voltage distribution, low voltage distribution, general power, lighting, grounding, fire alarm, electrical building systems, power systems, uninterruptible power systems, generator plants, construction management, microsoft office suite, autocad, project management, engineering","arc flash studies, autocad, building information modeling, construction management, electrical building systems, electrical engineering, engineering, fire alarm, general power, generator plants, grounding, leed, lighting, low voltage distribution, medium voltage distribution, microsoft office suite, mission criticaldata center, power systems, project management, revit, short circuit coordination, uninterruptible power systems, uptime atd"
Cloud Data Engineer,Kelly,"Bridgeton, MO",https://www.linkedin.com/jobs/view/cloud-data-engineer-at-kelly-3778866801,2023-12-17,East Saint Louis,United States,Mid senior,Hybrid,"Kelly Technology is seeking a Cloud Data Engineer to work with our premier client in the St. Louis. MO area.
Cloud Data Engineer
Bridgeton, Missouri
Full Time/Direct Hire
sorry no C2C or relocation.
Our premier client is the global industry leader in wheel alignment, wheel balancing, and vehicle inspection systems, as a Cloud Data Engineer based at our headquarters in St. Louis, Missouri. We are seeking a candidate with a passion for creating robust and scalable cloud data architectures.
Qualifications:
· Strong knowledge of SQL and data modeling techniques.
· Proven experience with cloud data services such as Azure SQL Databases/Amazon RDS, Azure Data Factory/AWS Glue, Azure or AWS Data Lake, or Google Cloud equivalents.
· Familiarity with Airflow and Python.
· Proficiency in Linux systems.
· Excellent problem-solving skills and attention to detail.
· Good communication skills and the ability to work collaboratively.
Responsibilities:
As a Cloud Data Engineer, you will:
· Design, develop, and maintain reliable, scalable data infrastructure solutions using cloud services.
· Monitor, troubleshoot, and resolve issues within data operations infrastructure.
· Collaborate with the data engineering team, data scientists, and application developers to build maintainable solutions that meet business requirements.
· Ensure data privacy and compliance standards are met.
· Stay updated on the latest Microsoft data technologies and best practices.
· Drive technology direction by making recommendations based on experience and research.
· Create test plans and validation controls.
Additional Qualifications:
· Bachelor’s degree in a computer-related field strongly preferred; equivalent combination of education and experience considered.
· 5+ years of relevant experience required.
· Relevant Cloud or Data Engineering certifications.
· Experience with Software Development Life Cycle and DevOps.
· Familiarity with monitoring tools like Datadog.
· Experience with data and computing tools, including Airflow, dbt, and messaging queues like Kafka.
· Experience working on real-time data and streaming applications.
#TJP2023-SPEC
Show more
Show less","SQL, Data modeling, Cloud data services, Azure SQL Databases, Amazon RDS, Azure Data Factory, AWS Glue, Azure Data Lake, AWS Data Lake, Google Cloud, Airflow, Python, Linux, Problemsolving skills, Attention to detail, Communication skills, Collaboration skills, Data infrastructure solutions, Data operations infrastructure, Data engineering, Data science, Application development, Data privacy, Compliance standards, Microsoft data technologies, Best practices, Technology direction, Software Development Life Cycle, DevOps, Monitoring tools, Datadog, Data and computing tools, Messaging queues, Kafka, Realtime data, Streaming applications","sql, data modeling, cloud data services, azure sql databases, amazon rds, azure data factory, aws glue, azure data lake, aws data lake, google cloud, airflow, python, linux, problemsolving skills, attention to detail, communication skills, collaboration skills, data infrastructure solutions, data operations infrastructure, data engineering, data science, application development, data privacy, compliance standards, microsoft data technologies, best practices, technology direction, software development life cycle, devops, monitoring tools, datadog, data and computing tools, messaging queues, kafka, realtime data, streaming applications","airflow, amazon rds, application development, attention to detail, aws data lake, aws glue, azure data factory, azure data lake, azure sql databases, best practices, cloud data services, collaboration skills, communication skills, compliance standards, data and computing tools, data engineering, data infrastructure solutions, data operations infrastructure, data privacy, data science, datadog, datamodeling, devops, google cloud, kafka, linux, messaging queues, microsoft data technologies, monitoring tools, problemsolving skills, python, realtime data, software development life cycle, sql, streaming applications, technology direction"
ETL Data Engineer with Informatica,Extend Information Systems Inc.,"Mount Laurel, NJ",https://www.linkedin.com/jobs/view/etl-data-engineer-with-informatica-at-extend-information-systems-inc-3621512721,2023-12-17,Philadelphia,United States,Mid senior,Onsite,"Hello,
We are actively hiring Senior ETL Data Engineer with Informatica. It's a full Time. Please have a look of below job description and let me know if you are interested. Zeenat@extendinfosys.com
Title: Senior ETL Data Engineer with Informatica
Location: Mount Laurel, NJ / Wilmington, DE / Charlotte, NC (prefer Mt. Laurel, NJ mostly)
Experince: 8+ years
Full-time
Mandatory Skills: Informatica Power Centre, Oracle, Unix, Autosys.
Design and develop ETL processes based on functional and non-functional requirements
Understand the full end to end development activities from design to go live for ETL development
Recommend and execute improvements
Document component design for developers and for broader communication.
Understand and adopt an Agile (SCRUM like) software development mindset
Follow established processes/standards, business technology architecture for development, release management and deployment process
Execute and provide support during testing cycles and post-production deployment, engage in peer code reviews.
Elicit, analyze, interpret business and data requirements to develop complete business solutions, includes data models (entity relationship diagrams, dimensional data models), ETL and business rules, data life cycle management, governance, lineage, metadata and reporting elements.
Apply automation and innovation on new and on-going data platforms for those development projects aligned to business or organizational strategies.
Design, develop and implement reporting platforms (e.g. modeling, ETL, BI framework) and complex ETL frameworks that meet business requirements.
Deliver business or enterprise data deliverables (that adhere to enterprise frameworks) for various platforms/servers/applications/systems.
Implement processes aligned to data management standards, ensure data quality and requirements are embedded in/adhered to within system development deliverables.
Develop, maintain knowledge of data available from upstream sources and data within various platforms.
Thank - You
ZEENAT NAYER | TECHNICAL RECRUITER | EXTEND INFORMATION SYSTEMS
CELL:571-800-0882
EMAIL: zeenat@extendinfosys.com
ADDRESS:44355 Premier Plaza UNIT 220, Ashburn, VA, USA - 20147
WEB:WWW.extendinfosys.com
Show more
Show less","Informatica Power Centre, Oracle, Unix, Autosys, ETL, Agile, SCRUM, Data models, Entity relationship diagrams, Dimensional data models, Data life cycle management, Governance, Lineage, Metadata, Reporting elements, Automation, Innovation, Data platforms, Business intelligence (BI) framework","informatica power centre, oracle, unix, autosys, etl, agile, scrum, data models, entity relationship diagrams, dimensional data models, data life cycle management, governance, lineage, metadata, reporting elements, automation, innovation, data platforms, business intelligence bi framework","agile, automation, autosys, business intelligence bi framework, data life cycle management, data models, data platforms, dimensional data models, entity relationship diagrams, etl, governance, informatica power centre, innovation, lineage, metadata, oracle, reporting elements, scrum, unix"
Data Engineer,Brains Workgroup,"Langhorne, PA",https://www.linkedin.com/jobs/view/data-engineer-at-brains-workgroup-3453771540,2023-12-17,Philadelphia,United States,Mid senior,Onsite,"DATA ENGINEER
One of our clients, a Market Research Company that transforms Consumer Packaged Goods (CPG) and Retail Industries into true data driven digital enterprises, is looking for a talented DATA ENGINEER.
Permanent (full time) position with excellent compensation package and benefits.
Location:
100% Remote position
Sorry, No H1b Visa Support for this role
Please read the description below and to be considered immediately email your resume to barryr @brainsworkgroup.com
Requirements
10 years hands on experience as a Data Engineer or similar position.
10 years of commercial experience with Python or Scala Programming Language
10 years of SQL and experience working with relational databases (Postgres preferred).
Knowledge of Databricks, Spark, Hadoop or Kafka.
Experience developing data pipelines to automate data processing workflows
Experience in data modeling
Knowledge of data warehousing, business intelligence, and application data integration solutions
Experience in developing applications and services that run on a cloud infrastructure (Azure preferred)
Excellent problem-solving and communication skills
Responsibilities
Build unique high-impact business solutions utilizing advanced technologies for use by world class clients.
Create and maintain underlying data pipeline architecture for the solution offerings from raw client data to final solution output.
Create, populate, and maintain data structures for machine learning and other analytics.
Use quantitative and statistical methods to derive insights from data.
Combine machine learning, artificial intelligence (ontologies, inference engines and rules) and natural language processing under a holistic vision to scale and transform businesses — across multiple function and process.
Create and maintain optimal data pipeline architecture, incorporating data wrangling and Extract-Transform-Load (ETL) flows.
Assemble large, complex data sets to meet analytical requirements – analytics tables, feature-engineering etc.
Build the infrastructure required for optimal, automated extraction, transformation, and loading of data from a wide variety of data sources using SQL and other ‘big data’ technologies such as Databricks.
Build automated analytics tools that utilize the data pipeline to derive actionable insights.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
Design and develop data integrations and data quality framework
Develop appropriate testing strategies and reports for the solution as well as data from external sources.
Configure the data pipelines to accommodate client-specific requirements to onboard new clients
Implement processes and tools to monitor data quality - investigate and remedy any data-related issues in daily solution operations.
Additional skills a big plus:
Knowledge of one or more of the following technologies: Data Science, Machine Learning, Natural Language Processing, Business Intelligence, and Data Visualization.
Knowledge of statistics and experience using statistical or BI packages for analyzing large datasets (Excel, R, Python, Power BI, Tableau etc.).
Experience with container management and deployment, e.g., Docker and Kubernetes
Bachelor’s Degree in Computer Science, Software Engineering, or other STEM fields.
Use This Link To Apply Directly
https://brainsworkgroup.catsone.com/careers/index.php?m=portal&a=details&jobOrderID=16014494
Or email: barryr@brainsworkgroup.com
Check ALL our Jobs: http://brainsworkgroup.catsone.com/careers
Keys:Data Python Scala SQL Databricks spark hadoop kafka cloud Azure AWS Postgres ETL tableau
Show more
Show less","Data Engineering, Python, Scala, SQL, Databricks, Spark, Hadoop, Kafka, Data Pipelines, Data Modeling, Data Warehousing, Business Intelligence, Data Integration, Cloud Computing, Azure, Postgres, ETL, Tableau, Machine Learning, Artificial Intelligence, Natural Language Processing, Data Science, Statistics, Docker, Kubernetes, Computer Science, Software Engineering","data engineering, python, scala, sql, databricks, spark, hadoop, kafka, data pipelines, data modeling, data warehousing, business intelligence, data integration, cloud computing, azure, postgres, etl, tableau, machine learning, artificial intelligence, natural language processing, data science, statistics, docker, kubernetes, computer science, software engineering","artificial intelligence, azure, business intelligence, cloud computing, computer science, data engineering, data integration, data science, databricks, datamodeling, datapipeline, datawarehouse, docker, etl, hadoop, kafka, kubernetes, machine learning, natural language processing, postgres, python, scala, software engineering, spark, sql, statistics, tableau"
Database Developer,"Susquehanna International Group, LLP (SIG)","Bala-Cynwyd, PA",https://www.linkedin.com/jobs/view/database-developer-at-susquehanna-international-group-llp-sig-3759643749,2023-12-17,Philadelphia,United States,Mid senior,Onsite,"Overview
SIG is hiring a Data Engineer into our Operations Development team. This team provides support to middle and back office application teams with all their database needs. We are looking for a database administrator/developer to develop and maintain large and complex database systems using Hadoop, Oracle, and other technologies.
As a Member Of This Team, You Will
Design, develop, and support database applications for our middle/backoffice data systems
Design solutions using Hadoop, Hive, Druid, Spark technologies
Maintain and support our Oracle database instance and applications in a high-transaction environment
Perform routine and customary operational activities to ensure a stable database environment
Become a data domain expert
Design, develop, and support Linux shell scripts and Python for batch processing
Provide production support off hours/weekend, when needed
What We're Looking For
At least 5 years of experience with Hadoop development
Hands-on experience building solutions in Oracle
Experience with query performance tuning
Experience with Python and Unix Shell scripting required
Ability and flexibility to provide occasional off hour/weekend support
A Bachelor's degree in Computer Science, Engineering, Mathematics, or a related discipline or its foreign equivalent. Relevant technical experience may be substituted for education
SIG does not post salary information, so any salary information you find online may not be accurate because it was not provided or verified by SIG.
SIG does not accept unsolicited resumes from recruiters or search firms. Any resume or referral submitted in the absence of a signed agreement will become the property of SIG and no fee will be paid.
Show more
Show less","Hadoop, Oracle, Hive, Druid, Spark, Linux Shell scripts, Python, Query performance tuning","hadoop, oracle, hive, druid, spark, linux shell scripts, python, query performance tuning","druid, hadoop, hive, linux shell scripts, oracle, python, query performance tuning, spark"
Big Data engineer with Java & Spark,TechTammina LLC,"Wilmington, DE",https://www.linkedin.com/jobs/view/big-data-engineer-with-java-spark-at-techtammina-llc-3667459710,2023-12-17,Philadelphia,United States,Mid senior,Onsite,"Role: Big Data engineer with Java & Spark
Location: Wilmington, DE
Duration: 6 months contract to hire
Rate: Market
Responsibilities
Supporting big data platforms
Going through a modernization and data center migration
Moving from Informatica tools to Java/Spark, big data, data modeling etc Java is a MUST HAVE
Looking for strong sr devs with Java/Spark skillsets
Any modernization or migration experience is a huge plus
Transitioning from ETL skillsets into java/spark
Required Skills
5+ years of professional experience as a developer in a data warehousing or other data oriented, batch processing environment
5+ year experience in analysis of data or complex processes and systems, demonstrating strong analytical skills
3+ years of hands on experience with Java
2+ years of hands on experience with Spark
3+ years of hands on experience with relational databases such as Teradata or Oracle
3+ years using SQL
2+ years creating complex technical designs which included data mappings
Experience with Unix shell scripting (is a plus)
Experience with Tableau or other BI Reporting tools is a plus
Experience with AbInitio is a plus
Experience with a scheduling tool, especially Control-M, is a plus.
Experience with issue analysis and resolution including usage of issue resolution processes for application production problems.
Experience working in an Agile setting
Demonstrated ability to work in a team environment with a structured SDLC and interface and coordinate with a variety of business and I/T groups.
Show more
Show less","Java, Spark, Data modeling, Data migration, ETL, SQL, Teradata, Oracle, Unix shell scripting, Tableau, BI Reporting tools, AbInitio, ControlM, Agile, Scrum, SDLC","java, spark, data modeling, data migration, etl, sql, teradata, oracle, unix shell scripting, tableau, bi reporting tools, abinitio, controlm, agile, scrum, sdlc","abinitio, agile, bi reporting tools, controlm, data migration, datamodeling, etl, java, oracle, scrum, sdlc, spark, sql, tableau, teradata, unix shell scripting"
Senior Infrastructure Engineer/Database Architect,Smartz Inc,"Champaign, IL",https://www.linkedin.com/jobs/view/senior-infrastructure-engineer-database-architect-at-smartz-inc-3720787073,2023-12-17,Seymour,United States,Mid senior,Onsite,"At Smartz Inc, we rely on our dynamic team of engineers to solve the many challenges and puzzles that come with our rapidly evolving technical stack. We are seeking a results-oriented database engineer to help optimize the performance of our organization's databases. You will be enhancing our data storage capacity, writing new support programs and scripts, and troubleshooting database code.
To ensure success as a database engineer, you should exhibit knowledge of best practices in database management and experience in a similar role. A top-notch database engineer will be someone whose database optimization skills translate into the efficient flow of information throughout an organization.
About Us
Smartz was founded in June of 2021 by Dr. Kevin Wan who has been a successful entrepreneur in the smart home industry for more than 10 years. Our team at Smartz wants to disrupt the PropTech Industry by providing advanced AI software that is compatible with industry leading smart home devices and will provide property owners and tenants with an all-in-one solution to all of their property management needs.
Our team uses the latest mobile technologies for all features of Smartz apps, which include live video streaming/recording, motion/sound alerts, AI powered object detection, video access sharing functions, etc. We always strive to develop innovative features to create secure and smart device management experiences for our users and business partners. On the Hardware Integration team you have the opportunity to have significant ownership and impact over major user-facing features that millions of people love to use.
Salary range listed is the starting range for candidates with basic qualifications
Job Type
Full-time (Champaign, IL)
Job Responsibilities
Maintaining and enhancing the performance of existing database programs.
Assisting database development teams in designing new database programs that meet the
organization's data storage needs.
Monitoring databases and related systems to ensure optimized performance.
Writing new support programs and scripts to increase data storage capacity.
Reviewing database and user reports, as well as system information.
Performing debugging procedures on database scripts and programs, as well as resolving conflicts.
Mentoring database administrators and providing them with technical support.
Adhering to best practices in securely storing, backing up, and archiving data.
Documenting processes related to database design, configuration, and performance.
Keeping abreast of developments and best practices in database engineering.
Technical Skills Requirements
Applicable licensing, certification, and registration.
A minimum of 4 years’ experience in database engineering.
In-depth knowledge of Structured Query Language (SQL) and NoSQL technologies
Extensive experience with database technologies and architecture.
Sound knowledge of best practices in database engineering and data security.
Strong organizational skills and attention to detail.
Exceptional problem-solving and critical thinking skills.
Excellent collaboration and communication skills.
Experience & Qualifications
Bachelor's Degree in information systems, information technology, computer science, or similar.
Experience in building enterprise applications
Strong organizational and project management skills.
Excellent verbal communication skills.
Good problem-solving skills and strong attention to detail.
Benefits
Company paid Medical, Dental, Vision, and Life insurance
Retirement and savings plan with company match
Unlimited PTO
Salary range listed is the starting range for candidates with basic qualifications
Smartz’s commitment of diversity and inclusion is one we strive to continuously cultivate. We aim to provide everyone regardless of identity a space where they can grow and feel seen as a person first. These are more than just words to us; they are guidelines for how we build and foster our team, our leaders, and the core culture of our company. We are also an equal opportunity employer that does not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. In accordance with the law, we make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
As a part of our commitment to health and safety, we have implemented various COVID-related health and safety requirements for our workforce. These requirements may include sharing information in the company's HRM system, regular testing, mask wearing, social distancing and daily health checks. Requirements may change in the future with the evolving public health landscape.
Show more
Show less","Database Engineering, Structured Query Language (SQL), NoSQL Technologies, Database Technologies and Architecture, Database Design, Database Configuration, Database Performance, Database Security, Data Storage Capacity, Data Storage Needs, Database Optimization, Data Security, Strong ProblemSolving Skills, Strong Attention to Detail, Excellent Collaboration Skills, Excellent Communication Skills, Building Enterprise Applications, Strong Organizational Skills, Project Management Skills, Verbal Communication Skills","database engineering, structured query language sql, nosql technologies, database technologies and architecture, database design, database configuration, database performance, database security, data storage capacity, data storage needs, database optimization, data security, strong problemsolving skills, strong attention to detail, excellent collaboration skills, excellent communication skills, building enterprise applications, strong organizational skills, project management skills, verbal communication skills","building enterprise applications, data security, data storage capacity, data storage needs, database configuration, database design, database engineering, database optimization, database performance, database security, database technologies and architecture, excellent collaboration skills, excellent communication skills, nosql technologies, project management skills, strong attention to detail, strong organizational skills, strong problemsolving skills, structured query language sql, verbal communication skills"
"Sr. Engineer, Database Infrastructure - Slack",Slack,Urbana-Champaign Area,https://www.linkedin.com/jobs/view/sr-engineer-database-infrastructure-slack-at-slack-3760628831,2023-12-17,Seymour,United States,Mid senior,Remote,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Software Engineering
Job Details
About Salesforce
We’re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good – you’ve come to the right place.
Slack's DataStores team builds and operates the database platform powering Slack. We write software to manage thousands of stateful hosts, providing several petabytes of online database capacity. We are building one of the fastest-growing database platforms in the world. Our MySQL databases run in Vitess. You can read more about our migration to Vitess at: Scaling Datastores at Slack with Vitess
Background
Slack enables people all over the world to communicate and collaborate together. Teams of all scales — from the world’s largest public companies to the smallest of startups — use Slack to get work done, so we take performance and reliability very seriously. A taste of our scale:
The average user spends over 10 hours connected and 2.5 hours active in Slack every single day
1.5 billion+ messages are sent per month, half of those outside the United States
Every day we see over 10M+ daily active users, over a billion web requests, and 10s of billions of database queries.
For millions of people, Slack is the primary communication tool they use at work all day long. They expect it to be exceptionally reliable and fast, all the time.
Infrastructure at Slack
We operate at tremendous scale with systems that process millions of events per second. Teams in our group maintain and build the lower levels of our stack, including:
Edge services
Data Stores and Caches
Real-time messaging
Asynchronous background job processing
We know we’ve done our job correctly when
none of our users think about us.
We don’t typically ship new user-facing features, but rather ensure our systems are incredibly performant, highly available, reliable, and scalable. In other words, Slack just works seamlessly.
Slack's API and web backend is built on PHP/Hack, our backend services are written in Java and Go, and we use Vitess as our storage engine. Our architecture is constantly evolving to handle millions more users. You can read about how we scaled our datastores with Vitess, how we respond to incidents, and much more on our blog.
What you will be doing:
Operate and enhance our large, highly-available database infrastructure, utilizing technologies such as MySQL and Vitess.
Develop tools to enable self-service and self-managing capabilities of our database infrastructure so that other teams can operate full-stack while rapidly building new features for our customers.
Collaborate with engineering teams on their database storage needs, and advise them throughout the development lifecycle.
Write code to capture database performance, and create tools and dashboards to provide actionable insight into that data.
Participate in our on-call rotation and collaborate with our operations team to triage and resolve production issues.
Support FedRAMP and DoD SRG activities.
What you should have:
Have been working in data storage, core infrastructure, or distributed system-owning teams with increasing responsibilities for 5+ years.
Have professional experience using Go, PHP/Hacklang, Python, Ruby, or Java.
Write code that can be easily understood by others with an eye towards clarity and maintainability. Collaborate with other teams to integrate new features of your platform or adopt self-service features.
Operated at least one distributed system, at scale and in a team environment. Some examples include: a relational database like MySQL/Postgres, or systems like Kafka, Cassandra, or ElasticSearch.
Deployed server software on Linux, and then operated it at scale. You’ve debugged its problems, and analyzed and optimized its performance.
Have experience operating cloud infrastructure, especially AWS.
Are familiar with deployment automation/configuration management tools like Chef, Ansible, Puppet, or Terraform.
Are a very strong communicator. You’re excited to explain complex technical concepts and share your knowledge with different audiences.
Are curious how things work; when they break you are eager and able to help fix them.
Infrastructure is a diverse and inclusive team that treats their colleagues exceptionally well. We are happy to help you learn what you need to know; we encourage and support each other’s growth and thus it’s not expected that you would have expertise across all of these areas.
Come join us!
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com .
Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce .
﻿Salesforce welcomes all.
For Colorado-based roles, the base salary hiring range for this position is $156,800 to $215,600.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.
Show more
Show less","MySQL, Vitess, Java, Go, PHP, Hacklang, Python, Ruby, Linux, AWS, Chef, Ansible, Puppet, Terraform","mysql, vitess, java, go, php, hacklang, python, ruby, linux, aws, chef, ansible, puppet, terraform","ansible, aws, chef, go, hacklang, java, linux, mysql, php, puppet, python, ruby, terraform, vitess"
"Tech Lead, Data Engineer",Lovelytics,"Arlington, VA",https://www.linkedin.com/jobs/view/tech-lead-data-engineer-at-lovelytics-3787794183,2023-12-17,Rogers,United States,Mid senior,Remote,"Lovelytics is seeking a Technical Lead (Level 4) Consultant with experience delivering strategic Databricks client engagements to join our Data & AI practice!
As a Tech Lead, you will gain people management skills in order to develop to the next level. In addition, you will play a key role, often as an engagement lead, on client engagements related to data warehousing, ETL development, data integrations, and data modeling. This is a client-facing and stakeholder management role, focused on using and migrating to our partner technologies; Databricks, AWS, and Azure to name a few. In addition to the technical capabilities for this role, we are looking for someone who wants to work in a collaborative, dynamic, and inclusive environment and has a passion for bringing meaning to data.
Role Location: Arlington, VA, or Remote in the US (MD, DC, CA, IA, ID, IN, MA, NC, SC, TX, TN, GA, CO, NY, NJ, VA, FL, PA)
This role is not eligible for sponsorship at this time.
Primary Responsibilities:
Utilize consulting and technical skills to be able to work in a client-facing project environment independently.
Be responsible for your own execution and often others' on client projects, communicating directly with internal and external stakeholders on status updates and potential roadblocks.
Collaborate with other team members to successfully deliver on projects.
Work effectively and directly communicate with both internal and client and/or partner teams.
Develop full ownership of your execution on client engagements, play a role in the project planning and solution stages of engagements as well.
Lead the end-to-end design and implementation of multiple ETL/ELT pipelines, demonstrating efficient data transformation.
Mentor junior data engineers, and their growth is evident in their project contributions
Successfully lead small data warehousing projects with measurable performance enhancements under the management of an engagement lead- may also play the role of an engagement lead.
Contribute to real-time data processing solutions and managed streaming data.
Implement security and compliance measures for data pipelines.
Design and implement version control and branching strategies and integrate them into CI/CD for promoting and testing in higher environments.
Our Ideal Candidate's Skills and Experiences:
B.S. in Computer Science or equivalent
4-6 years' experience in data engineering and big data. 2 years' of professional services experience interacting directly with clients.
Extensive knowledge of data warehousing concepts and hands-on experience deploying pipelines using Databricks and/or Spark
Databricks Solution Architect certification a plus.
Data modeling and database design skills and knowledge of version control
Excellent verbal and written communication skills
Experience architecting scalable and fault-tolerant data solutions across Azure, AWS, and Databricks
Understands and utilizes Lovelytics tools and client tools
What We Promise You:
Exciting projects with great clients in varying departments and verticals across the world
The ability to work closely with experienced data engineers and quickly grow and expand your skillset
The ability to work closely with all sizes of companies, ranging from Fortune 100 to small local businesses
A workplace where you are encouraged to challenge the status quo and develop new technologies, methodologies, and processes
A diverse team consisting of data gurus, experience seekers, and entrepreneurial minds that are always pushing to be better
Lovelytics is an Equal Opportunity Employer. This means you don’t have to worry about whether your application process will be fair. We consider all applicants without regard to race, color, religion, age, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, veteran status, or disability. US Salary for this position is $125,000-160,000, however, actual salary is based on a number of various factors including skillset, experience, credentials, etc.
Powered by JazzHR
w2mSVnuYhb
Show more
Show less","Consulting, Databricks, AWS, Azure, ETL, ELT, Data warehousing, Data modeling, Version control, CI/CD, Data engineering, Big data, Scalable data solutions, Faulttolerant data solutions, Data security, Compliance, Communication skills, SQL, Python, R","consulting, databricks, aws, azure, etl, elt, data warehousing, data modeling, version control, cicd, data engineering, big data, scalable data solutions, faulttolerant data solutions, data security, compliance, communication skills, sql, python, r","aws, azure, big data, cicd, communication skills, compliance, consulting, data engineering, data security, databricks, datamodeling, datawarehouse, elt, etl, faulttolerant data solutions, python, r, scalable data solutions, sql, version control"
Data Engineer,Motion Recruitment,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-engineer-at-motion-recruitment-3762366344,2023-12-17,Rogers,United States,Mid senior,Remote,"Big Data Engineer
A client of ours in the financial space is looking to hire a Big Data Engineer to join their team. You will be responsible for developing and designing software applications as well as modifying existing applications to meet business requirements.
Required Skills & Experience
5+ years of software development experience
3+ years of experience with Map-Reduce, Hive, Spark
Hands-on experience writing and understanding complex SQL
Experience in UNIX shell-scripting
Bachelor’s degree in Engineering or Computer Science or equivalent
What You Will Be Doing
Tech Breakdown
100% Data Engineering
Daily Responsibilities
100% Hands On
Applicants must be currently authorized to work in the US on a full-time basis now and in the future. This role cannot be done on a C2C basis.
Posted By:
Julie Bennett
Show more
Show less","Software Development, MapReduce, Hive, Spark, SQL, UNIX Shellscripting, Engineering, Computer Science","software development, mapreduce, hive, spark, sql, unix shellscripting, engineering, computer science","computer science, engineering, hive, mapreduce, software development, spark, sql, unix shellscripting"
Senior Staff Data Engineer,Act Digital Consulting,United States,https://www.linkedin.com/jobs/view/senior-staff-data-engineer-at-act-digital-consulting-3643169587,2023-12-17,Rogers,United States,Mid senior,Remote,"Our Client is looking for a Senior Staff Data Engineer to lead the design, development, and maintenance of our data infrastructure. You will lead a team of engineers and data analysts to build data pipelines, implement data models, and optimize the performance of our data systems. You will be responsible for ensuring that our data is accurate, reliable, and accessible to the right people at the right time. This is a leadership role, and you will be responsible for mentoring and guiding junior team members.
Duties & Responsibilities
Lead the design, development, and maintenance of our data infrastructure, including data pipelines, databases, and data warehouses.
Work with cross-functional teams to define data requirements and design data models that meet those requirements
Optimize the performance of our data systems, including query performance and data ingestion speed
Ensure data accuracy, reliability, and accessibility, and implement processes to monitor and maintain data quality
Mentor and guide junior team members, providing technical guidance and career development support
Stay up-to-date with industry trends and emerging technologies in data engineering and apply them to our data infrastructure
Technical Skills
Expertise in SQL and NoSQL databases, data warehousing, and data modeling.
Fluency in Python and SQL. Additional data or system languages (e.g. Java, Scala, Go, R) a plus.
Experience designing cloud-based data systems such as AWS, Azure, or GCP. Multi-cloud experience, experience with infrastructure-as-code and/or DevOps tooling a plus.
Experience with data modeling and data governance best practices.
Experience with data visualization tools such as Tableau or Power BI.
Minimum Qualifications
Bachelor's or Master's degree in computer science, software engineering, or a related field.
7+ years of experience in data engineering, with a proven track record of designing and implementing data systems at scale.
Strong analytical and problem-solving skills.
Excellent communication and leadership skills, with the ability to mentor and guide junior team members.
Preferred Qualifications
Master's degree in Computer Science, Engineering, or a related field.
Experience in machine learning or data science.
All positions with the client require an applicant who has accepted an offer to undergo a background check. The specific checks are based on the nature of the position. Background checks may include some or all of the following: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, fingerprint verification, credit check, and/or drug test. By applying for a position with the client, you understand that you will be required to undergo a background check should you be made an offer. You also understand that the offer is contingent upon successful completion of the background check and results consistent with client's employment policies. You will be notified during the hiring process which checks are required for the position.
Client is an Equal Employment Opportunity/Affirmative Action employer and well committed to a diverse workforce. We do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, veteran status, and basis of disability or any other federal, state or local protected class.
Pay Transparency Non-Discrimination Notice Client will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information.
Show more
Show less","SQL, NoSQL, Data warehouse, Data modeling, Python, Java, Scala, Go, R, AWS, Azure, GCP, Infrastructureascode, DevOps, Tableau, Power BI","sql, nosql, data warehouse, data modeling, python, java, scala, go, r, aws, azure, gcp, infrastructureascode, devops, tableau, power bi","aws, azure, datamodeling, datawarehouse, devops, gcp, go, infrastructureascode, java, nosql, powerbi, python, r, scala, sql, tableau"
Enterprise Data Engineer,Open Systems Inc.,United States,https://www.linkedin.com/jobs/view/enterprise-data-engineer-at-open-systems-inc-3769269259,2023-12-17,Rogers,United States,Mid senior,Remote,"Enterprise Data Engineer – (Contract Position)
Atlanta, GA Remote
Job Description
Our client is seeking an experienced hands-on enterprise Data Engineer lead. The successful candidate must have Big Data engineering experience and must demonstrate an affinity for working with others to create successful solutions. They must be a great communicator, both written and verbal, and have some experience working with business areas to translate their business data needs and data questions into project requirements. The candidate will participate in all phases of the Data Engineering life cycle and will work independently and collaboratively write project requirements, architect solutions, and perform data ingestion development and support duties.
This Data Engineer will be responsible for creating new data flows into AWS including coordinating with business and functional areas to establish and communicate our data fabric – best practices, framework, and tools. Together we will establish a new data fabric for our client that will help create a common view of data and provide a centralized mechanism for its aggregation, cleansing, transformation, augmentation, validation, and syndication.
Education
B.A./B.S. degree or equivalent work experience in computer science, information technology, business administration, engineering, or another relevant field
Responsibilities
Sharing project solutions and outcomes with colleagues to improve delivery on future projects
Analyzing and translating business needs into long-term solution data pipelines.
Evaluating existing data systems.
Working with the development team to create conceptual data flows.
Developing best practices for data coding to ensure consistency within the system.
Reviewing modifications of existing systems for cross-compatibility.
Implementing data strategies and developing data integration points.
Evaluating implemented data systems for variances, discrepancies, and efficiency.
Troubleshooting and optimizing data systems.
Interpreting and delivering impactful strategic plans improving data integration, data quality, and data delivery in support of business initiatives and roadmaps
Formulating and articulating architectural trade-offs across solution options before recommending an optimal solution ensuring technical requirements are met
Motivating and developing staff through teaching, empowering, and influencing technical and consulting “soft” skills
Driving innovative technology solutions through thought leadership on emerging trends
Skills
Required:
6+ years of overall IT experience
3+ years of experience with high-velocity high-volume stream processing: Apache Spark
Experience with real-time data processing and streaming techniques using Spark structured streaming and Kafka
Deep knowledge of troubleshooting and tuning Spark applications
3+ years of experience with data ingestion from Message Queues (Tibco, IBM, etc.) and different file formats across different platforms like JSON, XML, CSV
3+ years of experience with Big Data tools/technologies like Hadoop, Spark, Spark SQL, Kafka, Sqoop, Hive, S3, or HDFS
3+ years of experience building, testing, and optimizing ‘Big Data’ data ingestion pipelines, architectures, and data sets
2+ years of experience with Python (and/or Scala) and PySpark/Scala-Spark
3+ years of experience with AWS cloud platform
3+ years of experience with database solutions like Databricks or Snowflake
2+ years of experience with NoSQL databases, including HBASE and/or Cassandra
Knowledge of Unix/Linux platform and shell scripting is a must
Strong analytical and problem-solving skills
Preferred (Not Required)
Experience with Cloudera/Hortonworks CDP, HDP and HDF platforms
Strong SQL skills with ability to write intermediate complexity queries
Strong understanding of Relational & Dimensional modeling
Experience with GIT code versioning software
Experience with REST API and Web Services
Good business analyst and requirements gathering/writing skills
Who We Are
Open Systems Inc. (OSI) was founded in 1994 to provide information technology solutions and staffing services to large and mid-size companies across the U.S. Our corporate office is located at 6495 Shiloh Road, Ste 310 Alpharetta, GA 30005. We provide a full range of staffing services including contract, contract-to-hire, and direct hire solutions. Our technical recruiting experts are experienced in technical screening, candidate sourcing, and behavioral interviewing techniques. They focus on providing candidates who match your technical requirements and fit seamlessly into your company culture.
Contact Open Systems, Inc. anytime by website, phone or email. We look forward to hearing from you!!
Show more
Show less","Apache Spark, Realtime data processing, Spark structured streaming, Kafka, Troubleshooting, Hadoop, Spark SQL, Hive, S3, HDFS, Python, PySpark, AWS, Databricks, Snowflake, NoSQL, HBASE, Cassandra, Unix/Linux, Shell scripting, GIT, REST API, Web Services, SQL, Relational & Dimensional modeling","apache spark, realtime data processing, spark structured streaming, kafka, troubleshooting, hadoop, spark sql, hive, s3, hdfs, python, pyspark, aws, databricks, snowflake, nosql, hbase, cassandra, unixlinux, shell scripting, git, rest api, web services, sql, relational dimensional modeling","apache spark, aws, cassandra, databricks, git, hadoop, hbase, hdfs, hive, kafka, nosql, python, realtime data processing, relational dimensional modeling, rest api, s3, shell scripting, snowflake, spark, spark sql, spark structured streaming, sql, troubleshooting, unixlinux, web services"
REMOTE:  Data Engineer Scala Spark,"Conch Technologies, Inc",United States,https://www.linkedin.com/jobs/view/remote-data-engineer-scala-spark-at-conch-technologies-inc-3766017679,2023-12-17,Rogers,United States,Mid senior,Remote,"Hi,
Greetings from Conch Technologies Inc
We have a few openings for our client in NC. The roles are remote.
Data Engineer (scala/spark and AWS).
Remote and long-term.
Must work est time.
Data Engineer Requirements
3+ years of experience building scalable data pipelines with Scala and Spark
Strong Scala programming skills and knowledge of functional programming
Experience with Spark Scala, DataFrames, Datasets, and Hadoop Filesystem
Knowledge of AWS services like EMR, S3, OpenSearch etc.
Familiarity with CI/CD best practices and experience with Jenkins, Git, and Azure DevOps
Ability to optimize Spark jobs for performance and cost efficiency
Experience supporting production data pipelines and ETL processes
Excellent communication skills and ability to quickly ramp up on our tech stack
--
With Regards,
Teja Maripeti
Desk:
901-317-3455
Email: teja@conchtech.com
Web: www.conchtech.com
Show more
Show less","Scala, Spark, AWS, EMR, S3, OpenSearch, Jenkins, Git, Azure DevOps, CI/CD, ETL, DataFrames, Datasets, Hadoop Filesystem","scala, spark, aws, emr, s3, opensearch, jenkins, git, azure devops, cicd, etl, dataframes, datasets, hadoop filesystem","aws, azure devops, cicd, dataframes, datasets, emr, etl, git, hadoop filesystem, jenkins, opensearch, s3, scala, spark"
Lead Data Engineer,Lorven Technologies Inc.,United States,https://www.linkedin.com/jobs/view/lead-data-engineer-at-lorven-technologies-inc-3714698407,2023-12-17,Rogers,United States,Mid senior,Remote,"Job Title: Lead Data Engineer
Location: Remote
Duration: 6 to 12+ Months Contract
10+ years of prior experience Lead Data Engineer.
AWS/Snowflake/DBT experiences
Must be proficient in SQL, Python. Automate repetitive personal tasks through Python and SQL
Experience with Tableau or any visualization tool, Data Warehousing, Data Modeling
Strong experience with relational databases like SQL Server, MySQL and Vertica. NoSQL databases experience is a plus
Experience with user-defined workflows (e.g., Airflow)
Experience with writing Kafka consumers and producers.
Experience Apache Spark Streaming and Hive is plus.
Experience with Linux servers and Docker is required.
Problem solver that is action-oriented with the ability to look at problems in new ways.
Working knowledge of data management software like Airflow, or other ETL tools a plus.
Strong analytical and problem-solving ability to design an effective solution.
Ability to support multiple on-going projects in a fast-paced environment.
Strong communicational skills, organizational skills, negotiation skills, and flexibility to address competing demands.
Superior business judgement ability to flex between big picture thinking, understand and distill complex ideas, and analyze data to drive strategic objectives.
Show more
Show less","Data Engineering, AWS, Snowflake, DBT, SQL, Python, Tableau, Data Visualization, Data Warehousing, Data Modeling, SQL Server, MySQL, Vertica, NoSQL, Airflow, Kafka, Apache Spark, Hive, Linux, Docker, Data Management Software, ETL Tools","data engineering, aws, snowflake, dbt, sql, python, tableau, data visualization, data warehousing, data modeling, sql server, mysql, vertica, nosql, airflow, kafka, apache spark, hive, linux, docker, data management software, etl tools","airflow, apache spark, aws, data engineering, data management software, datamodeling, datawarehouse, dbt, docker, etl tools, hive, kafka, linux, mysql, nosql, python, snowflake, sql, sql server, tableau, vertica, visualization"
"Senior Engineer, Data Platform",TRM Labs,United States,https://www.linkedin.com/jobs/view/senior-engineer-data-platform-at-trm-labs-3778357880,2023-12-17,Rogers,United States,Mid senior,Remote,"At TRM, we're on a mission to build trust in digital assets, because the promise of crypto is too valuable to be impeded by bad actors. We provide a blockchain intelligence platform to law enforcement, financial institutions, and crypto firms to assist in the detection and prevention of cryptocurrency fraud and financial crime. Our vision is to build a company that can sustainably deliver on our mission for decades to come, enabling consumers to transact safely and securely on the blockchain.
The Data Platform team collaborates with an experienced group of data scientists, engineers, and product managers to build highly available and scalable data infrastructure for TRM's products and services. As a Software Engineer on the Data Platform team, you will be responsible for executing mission-critical systems and data services that analyze blockchain transaction activity at petabyte scale, and ultimately work to build a safer financial system for billions of people.
The impact you will have here
Build highly reliable data services to integrate with dozens of blockchains.
Develop complex ETL pipelines that transform and process petabytes of structured and unstructured data in real-time.
Design and architect intricate data models for optimal storage and retrieval to support sub-second latency for querying blockchain data.
Oversee the deployment and monitoring of large database clusters with an unwavering focus on performance and high availability.
Collaborate across departments, partnering with data scientists, backend engineers, and product managers to design and implement novel data models that enhance TRM’s products.
What We're Looking For
Bachelor's degree (or equivalent) in Computer Science or a related field.
A proven track record, with 5+ years of hands-on experience in architecting distributed system architecture, guiding projects from initial ideation through to successful production deployment.
Exceptional programming skills in Python, as well as adeptness in SQL or SparkSQL.
Versatility that spans the entire spectrum of data engineering in one or more of the following areas:
In-depth experience with data stores such as ClickHouse, ElasticSearch, Postgres, Redis, and Neo4j.
Proficiency in data pipeline and workflow orchestration tools like Airflow, DBT, Luigi, Azkaban, and Storm.
Expertise in data processing technologies and streaming workflows including Spark, Kafka, and Flink.
Competence in deploying and monitoring infrastructure within public cloud platforms, utilizing tools such as Docker, Terraform, Kubernetes, and Datadog.
Proven ability in loading, querying, and transforming extensive datasets.
Learn more on TRM's careers page: https://www.trmlabs.com/careers-list?gh_jid=4953326004
Show more
Show less","Python, SQL, SparkSQL, ClickHouse, ElasticSearch, Postgres, Redis, Neo4j, Airflow, DBT, Luigi, Azkaban, Storm, Spark, Kafka, Flink, Docker, Terraform, Kubernetes, Datadog","python, sql, sparksql, clickhouse, elasticsearch, postgres, redis, neo4j, airflow, dbt, luigi, azkaban, storm, spark, kafka, flink, docker, terraform, kubernetes, datadog","airflow, azkaban, clickhouse, datadog, dbt, docker, elasticsearch, flink, kafka, kubernetes, luigi, neo4j, postgres, python, redis, spark, sparksql, sql, storm, terraform"
Data Engineer with Python-US,Zortech Solutions,"Santa Clarita, CA",https://www.linkedin.com/jobs/view/data-engineer-with-python-us-at-zortech-solutions-3667475767,2023-12-17,Rogers,United States,Mid senior,Hybrid,"Role: Data Engineer with Python
Location: Bellevue, WA or Santa Clara, CA (Onsite day 1)
Duration: 6+ Months
Job Description
6+ years within specific technology domain areas (e.g. software development, cloud computing, systems engineering, infrastructure, security, networking, data & analytics)
Minimum 3 years' experience with Python, PySpark
Minimum 3 years' experience with AWS, DynamoDB
Show more
Show less","Python, PySpark, AWS, DynamoDB","python, pyspark, aws, dynamodb","aws, dynamodb, python, spark"
Senior Data Scientist with Security Clearance,ClearanceJobs,"Augusta, GA",https://www.linkedin.com/jobs/view/senior-data-scientist-with-security-clearance-at-clearancejobs-3753462535,2023-12-17,Aiken,United States,Mid senior,Onsite,"Two Six Technologies is looking to add a Senior Data Scientist to our team. As a Data Scientist, you'll use your technical experience to solve some of the most challenging intelligence issues around data. Job Responsibilities & Duties: * Devise strategies for extracting meaning and value from large datasets. * Make and communicate principled conclusions from data using elements of mathematics, statistics, computer science, and application specific knowledge. * Through analytic modeling, statistical analysis, programming, and/or another appropriate scientific method, develop and implement qualitative and quantitative methods for characterizing, exploring, and assessing large datasets in various states of organization, cleanliness, and structure that account for the unique features and limitations inherent in data holdings. * Translate practical needs and analytic questions related to large datasets into technical requirements and, conversely, assist others with drawing appropriate conclusions from the analysis of such data. * Effectively communicate complex technical information to non-technical audiences. Minimum Qualifications: * 10 years relevant experience with Bachelors in related field; or
8 years experience with Masters in related field; or
6 years experience with a Doctoral degree in a related field; or
12 years of relevant experience and an Associates may be considered for individuals with in-depth experience * Degree in an Mathematics, Applied Mathematics, Statistics, Applied Statistics, Machine Learning, Data Science, Operations Research, or Computer Science, or related field of technical rigor * Ability/willingness to work full-time onsite in secure government workspaces * Note: A broader range of degrees will be considered if accompanied by a Certificate in Data Science from an accredited college/university. Clearance Requirements: * This position requires a TS/SCI with CI Poly and eligibility/willingness to obtain FS Poly Two Six Technologies is an Equal Opportunity Employer and does not discriminate in employment opportunities or practices based on race (including traits historically associated with race, such as hair texture, hair type and protective hair styles (e.g., braids, twists, locs and twists)), color, religion, national origin, sex (including pregnancy, childbirth or related medical conditions and lactation), sexual orientation, gender identity or expression, age (40 and over), marital status, disability, genetic information, and protected veteran status or any other characteristic protected by applicable federal, state, or local law. If you are an individual with a disability and would like to request reasonable workplace accommodation for any part of our employment process, please send an email to . Information provided will be kept confidential and used only to the extent required to provide needed reasonable accommodations. Additionally, please be advised that this business uses E-Verify in its hiring practices. EOE, including disability/vets. By submitting the following application, I hereby certify that to the best of my knowledge, the information provided is true and accurate.
Show more
Show less","Data Science, Mathematics, Statistics, Machine Learning, Computer Science, Programming, Big Data, Data Analysis, Data Mining, Predictive Analytics, Artificial Intelligence, Python, R, SAS, SQL, Hadoop, Spark, Hive, Pig, Tableau, Power BI","data science, mathematics, statistics, machine learning, computer science, programming, big data, data analysis, data mining, predictive analytics, artificial intelligence, python, r, sas, sql, hadoop, spark, hive, pig, tableau, power bi","artificial intelligence, big data, computer science, data mining, data science, dataanalytics, hadoop, hive, machine learning, mathematics, pig, powerbi, predictive analytics, programming, python, r, sas, spark, sql, statistics, tableau"
Data Engineer - Scala(U.S. remote),Railroad19,"Augusta, GA",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3783327068,2023-12-17,Aiken,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Restful APIs, Spark 2.4, AWS, S3, EMR, Relational databases, Nonrelational databases, Apache Spark","scala 212, restful apis, spark 24, aws, s3, emr, relational databases, nonrelational databases, apache spark","apache spark, aws, emr, nonrelational databases, relational databases, restful apis, s3, scala 212, spark 24"
Salesforce Data Analyst,Extend Information Systems Inc.,"New York, NY",https://www.linkedin.com/jobs/view/salesforce-data-analyst-at-extend-information-systems-inc-3779198846,2023-12-17,Freeport,United States,Associate,Onsite,"Hi Jobseekers,
I hope you are doing well!
We have an opportunity
“Salesforce Data Analyst”
with one of our
clients
for
Chicago, IL
.
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details and a good time to connect with you.
Salesforce Data Analyst
Job Locations: New York, NY(Hybrid remote 2 days onsite)
Duration: Long term
Required Experience: 8- 10 Years
Skills : Salesforce Analytics, reporting, data sceince
The Data Analyst leverages Salesforce, Dynamics, IBM Mainframe, SQL Server, and SharePoint data to drive informed business decisions and optimize operational processes. This position involves analyzing, interpreting, and presenting data insights to support various aspects of the organization's Salesforce implementation.
Responsibilities
Data Analysis: Collect, clean, and analyze Legacy System data (Dynamics, IBM Mainframe, SQL Server, and SharePoint) to identify trends, patterns, and opportunities for improvement.
Reporting: Develop and maintain custom reports and dashboards within Salesforce to provide stakeholders with real-time insights into the data.
Data Integration: Collaborate with the Salesforce data migration team to ensure seamless integration of data from various sources into Salesforce, ensuring data accuracy and consistency.
Data Quality Management: Implement data quality best practices to maintain data integrity within Salesforce, including deduplication, data cleansing, and validation rules.
User Support: Assist Salesforce users in generating ad-hoc reports and resolving data-related issues, providing training and support as needed.
Business Insights: Translate data findings into actionable recommendations, helping customers and the Salesforce project team to improve efficiency, effectiveness, and customer satisfaction.
Process Improvement: Work closely with cross-functional teams to identify and implement process improvements and automation opportunities within Salesforce.
Compliance and Security: Ensure that data handling and storage within Salesforce comply with data protection regulations and security protocols.
Documentation: Maintain documentation of data models, processes, and reporting requirements to facilitate knowledge sharing and data governance.
Qualifications
10+ years of overall IT experience
Bachelor's degree in a related field (e.g., Data Science, Business Analytics, Information Technology).
5+ years of proven experience with Salesforce data analysis and reporting.
Proficiency in Salesforce reporting tools, such as Salesforce Reports and Dashboards.
Strong analytical skills with the ability to work with large datasets.
Familiarity with data visualization tools (e.g., Tableau, Power BI) is a plus.
Knowledge of data management best practices and data governance.
Excellent communication skills to convey complex data findings to non-technical stakeholders.
Salesforce certification(s) would be advantageous.
Attention to detail and a commitment to data accuracy.
Mandatory Skills
2 years of experience in SQL Server, Mainframe database, SharePoint, and Dynamics CRM
2 years of experience writing SQL queries·
Strong/Advanced experience in ETL and SSIS (SQL Server Integration Services)
Familiarity with the Salesforce platform, API, and data model·
Familiarity with Salesforce out-of-the-box functionality·
Strong/Advanced experience in data modeling and documentation
Work Location:On-site in NYC (40%) and remote
Thanks & Regards
Shankar Singh
Extend Information Systems
Cell: (571) 421-2684
Email: shankar@extendinfosys.com
Address: 44258 Mercure Circle, UNIT 102 A, Sterling VA, USA – 20166
Web: WWW.extendinfosys.com
We are E-verified company
Show more
Show less","Salesforce Analytics, Salesforce Reporting, Data Science, SQL Server, SharePoint, Dynamics CRM, ETL, SSIS (SQL Server Integration Services), Salesforce Platform, Salesforce API, Salesforce Data Model, Salesforce OutoftheBox Functionality, Data Modeling, Data Documentation","salesforce analytics, salesforce reporting, data science, sql server, sharepoint, dynamics crm, etl, ssis sql server integration services, salesforce platform, salesforce api, salesforce data model, salesforce outofthebox functionality, data modeling, data documentation","data documentation, data science, datamodeling, dynamics crm, etl, salesforce analytics, salesforce api, salesforce data model, salesforce outofthebox functionality, salesforce platform, salesforce reporting, sharepoint, sql server, ssis sql server integration services"
Data Analyst,Apex Systems,"New York, NY",https://www.linkedin.com/jobs/view/data-analyst-at-apex-systems-3783990465,2023-12-17,Freeport,United States,Associate,Hybrid,"Data Analyst
Location:
3x/week onsite in NYC
Client:
Large bank
Pay Rate:
$52/hr
Job Description:
Manages the analysis and design of data models and logical databases at the data element level consistent with the corporate architectural models. Coordinates the logical design of databases to promote data sharing and to reduce redundant data processes across multiple computing environments (e. g. , host based, client server, distributed systems, Web, e-commerce)
Must Haves:
MS Excel (Pivot Tables, queries)
SQL experience
Nice to Have:
Data Visualization (PowerBI, Tableau)
Financial industry experience
Show more
Show less","Data analysis, Data modeling, Database design, Data sharing, Redundant data processes, Data visualization, MS Excel (Pivot Tables queries), SQL, PowerBI, Tableau, Financial industry experience","data analysis, data modeling, database design, data sharing, redundant data processes, data visualization, ms excel pivot tables queries, sql, powerbi, tableau, financial industry experience","data sharing, dataanalytics, database design, datamodeling, financial industry experience, ms excel pivot tables queries, powerbi, redundant data processes, sql, tableau, visualization"
Database Reliability Engineer,Tickets.com,"New York, NY",https://www.linkedin.com/jobs/view/database-reliability-engineer-at-tickets-com-3766181277,2023-12-17,Freeport,United States,Associate,Hybrid,"Tickets.com
, an
MLB company
, delivers innovative, cutting-edge technologies to enable frictionless and unforgettable fan experiences in venues across the globe. Together with MLB, Tickets.com is changing the landscape of the live sports and entertainment industry, delivering new digital venue and ticketing experiences to millions of fans. Our Technology team builds platforms and products that provide a new smart ticketing solution and venue experience. Using cutting-edge technology, our platform and applications are consumed by fans, stadiums, and MLB teams.
We are assembling a world-class team to build on these experiences and to scale platforms and products that anticipate emerging opportunities, including dynamic pricing and offers and digital, contactless ticketing. Our mission is to provide premium, innovative live experiences for our clients and their patrons.
Tickets.com seeks a
Database Reliability Engineer
passionate about building engaging products for our fans.
The Opportunity: T
ickets.com and Major League Baseball are changing the landscape of the live sports and entertainment industry. Working together, we strive to deliver innovative, cutting-edge technologies to enable safe, unforgettable fan experiences across the globe. We are assembling a world-class technology team to build and support platforms and products that anticipate these emerging opportunities.
The Senior Data(base) Reliability Engineer will join the infrastructure team at Tickets.com while also working alongside MLB team members and be responsible for the following areas:
Essential Job Functions:
Uptime, High Availability, and Disaster recovery planning
Incident response
Optimization of data stores
Identify SLIs and define SLOs
Observability tooling
Debugging running systems and providing tools to assist runtime debugging
Optimizations for cost control
You will lead the planning, managing, and scaling of mission-critical transactional and analytic data stores and pipelines to ensure the business' complex data requirements are met, and it can easily access its data in a fast, reliable, and safe manner
.
Requirements:
Ability to interface with all levels of employees
Ability to work both independently with little supervision and in a team environment
Ensures availability, security, integrity, and recovery of data, pipelines, and data stores.
Define and configure relevant database metrics to ensure observability
Create and maintain dashboards and reports to visualize database performance and health
Create monitoring and alerting to trigger error conditions, degradation symptoms, and defined SLOs, as well as outages
Develops and implement data store maintenance plans, including performing integrity checks, updating statistics, and monitoring security and hardware resource utilization
Work with peers to roll out changes to production environments and help mitigate and prevent data-related production incidents
Work on automation of data store infrastructure and help engineering succeed by providing self-service tools
Resolves performance, capacity, replication, and other distributed data, pipeline, and data store issues
Support and debug data production issues across services and levels of the stack
Provide timely incident response and participate in on-call rotations
Continuously identify opportunities for process improvement and automation to enhance database performance, reliability, and efficiency
Prioritize unblocking your teammates, collaboration, and knowledge sharing
Salary Range is $140-160K
We offer an Outstanding Benefits Package that includes:
Medical
Dental
Vision
STD & LTD
401K Retirement Plan
Basic Life & AD&D
Supplemental Life Insurance
Paid Time Off (PTO, STO, Holidays including Year-End Holiday Break)
HSA & FSA
Legal Plan
Pet Insurance
Tuition Reimbursement
Flexible Hybrid Work Environment
MLB Tickets
Tickets.com is an Equal Opportunity Employer. Please click here to view our CCPA
Show more
Show less","Database Reliability, Uptime, High Availability, Disaster Recovery, Incident Response, Data Store Optimization, Service Level Indicators (SLIs), Service Level Objectives (SLOs), Observability Tooling, Debugging, Cost Control, Data Storage, Data Pipelines, Data Integrity, Data Security, Data Recovery, Database Metrics, Dashboards, Reports, Monitoring, Alerting, Data Store Maintenance, Integrity Checks, Statistics, Security Monitoring, Hardware Resource Utilization, Automation, SelfService Tools, Performance Optimization, Capacity Optimization, Replication, Distributed Data, Incident Response, Process Improvement, Collaboration, Knowledge Sharing","database reliability, uptime, high availability, disaster recovery, incident response, data store optimization, service level indicators slis, service level objectives slos, observability tooling, debugging, cost control, data storage, data pipelines, data integrity, data security, data recovery, database metrics, dashboards, reports, monitoring, alerting, data store maintenance, integrity checks, statistics, security monitoring, hardware resource utilization, automation, selfservice tools, performance optimization, capacity optimization, replication, distributed data, incident response, process improvement, collaboration, knowledge sharing","alerting, automation, capacity optimization, collaboration, cost control, dashboard, data integrity, data recovery, data security, data storage, data store maintenance, data store optimization, database metrics, database reliability, datapipeline, debugging, disaster recovery, distributed data, hardware resource utilization, high availability, incident response, integrity checks, knowledge sharing, monitoring, observability tooling, performance optimization, process improvement, replication, reports, security monitoring, selfservice tools, service level indicators slis, service level objectives slos, statistics, uptime"
Data Engineer (Contract),Imprint,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/data-engineer-contract-at-imprint-3730993078,2023-12-17,Freeport,United States,Mid senior,Remote,"The Data Platform team develops and maintains centralized data infrastructure, supports ETL and ML operations, and leverages the power of data to create actionable insights to help Imprint grow profitably.
You will:
Develop, implement, maintain and optimize data transformation, data pipeline validation/monitoring, data warehouse management and data cataloging process
Troubleshoot data issues and queries with business partners across the company
Work closely with analytics, business intelligence to build data model/marts for business metrics
Write and communicate end-user and technical documentation
Show more
Show less","Data infrastructure, ETL, ML, Data transformation, Data pipeline validation, Data pipeline monitoring, Data warehouse management, Data cataloging, Data troubleshooting, Data queries, Data model, Data marts, Business metrics, Enduser documentation, Technical documentation","data infrastructure, etl, ml, data transformation, data pipeline validation, data pipeline monitoring, data warehouse management, data cataloging, data troubleshooting, data queries, data model, data marts, business metrics, enduser documentation, technical documentation","business metrics, data cataloging, data infrastructure, data marts, data model, data pipeline monitoring, data pipeline validation, data queries, data transformation, data troubleshooting, data warehouse management, enduser documentation, etl, ml, technical documentation"
Data Engineer IV - Max Digital (Data Operations),ACV Auctions,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-operations-at-acv-auctions-3762880101,2023-12-17,Freeport,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack (SSIS, C#, Web APIs) outages during our operational support window.
Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required.
Leverage DMVs and monitoring tools to ensure system performance; work with data operations and engineering to improve as required.
Ensure existing HADR (availability groups) solution is functional and meets requirements.
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively. This may include being part of the emergency after-hours on-call rotation.
Mentor junior data engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert in SQL Query optimization
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (e.g., MongoDB)
Experience developing Windows services in C#
Experience writing unit and integration testing
Expert SQL and data-layer development experience; OLTP schema design.
Experience using and integrating with cloud services, specifically: AWS RDS, S3, SQS, SNS.
Nice to Have
Experience with Airflow
Experience with DBT
Compensation:
$127,000.00 - $152,000.00 annually. Please note that final compensation will be determined based upon the applicant's relevant experience, skillset, location, business needs, market demands, and other factors as permitted by law.
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","Python, C#, SQL, AWS RDS, S3, SQS, SNS, MongoDB, Airflow, DBT, ETL, SSIS, OLTP, DevOps","python, c, sql, aws rds, s3, sqs, sns, mongodb, airflow, dbt, etl, ssis, oltp, devops","airflow, aws rds, c, dbt, devops, etl, mongodb, oltp, python, s3, sns, sql, sqs, ssis"
Data Engineer/solution architect,Matrix-IFS,"Jersey City, NJ",https://www.linkedin.com/jobs/view/data-engineer-solution-architect-at-matrix-ifs-3768848146,2023-12-17,Freeport,United States,Mid senior,Hybrid,"Job Summary
Duties & Responsibilities
Service Delivery & Project Management: Led and managed the data engineering team to deliver projects on time and within budget. Collaborate with the Senior Data Solution Architect to translate client requirements into technical specifications. Oversee the development and implementation of data pipelines, workflows, and ETL processes.
Technical Expertise: Utilize expertise in DataBricks and Snowflake to design and implement scalable, reliable, and efficient data solutions. Conduct code reviews, ensuring adherence to best practices and coding standards. Troubleshoot and resolve technical issues, supporting the team as needed.
Team Leadership & Collaboration: Mentor and guide team members, fostering a culture of continuous learning and improvement. Facilitate cross-functional collaboration, ensuring smooth communication between engineering, sales, and other departments. Participate in recruitment, onboarding, and training of new team members.
Client Engagement: Serve as a technical liaison between clients and the engineering team, ensuring alignment and satisfaction. In collaboration with the senior data solution architect, assist with pre-sales support.
Continuous Improvement: Stay abreast of industry trends and emerging technologies, identifying opportunities for innovation and growth. Contribute to internal documentation, process improvement, and knowledge sharing.
Desired Qualifications
Bachelor’s or master’s degree in computer science, Engineering, or related field.
Minimum of 4+ years of experience in data engineering, focusing on DataBricks and/or Snowflake.
Proven experience with any of the following related technologies:
Big Data Technologies - Apache Spark, Apache Kafka, Apache Flink
Cloud Platforms – AWS, GCP, Azure
DevOps and CI/CD Tools
Proven experience developing in one or more of the following programming languages: Scala, Java, Python, SQL
Proven experience leading and managing technical teams.
Strong knowledge of data warehousing, big data technologies, cloud platforms, and ETL processes.
Excellent communication, problem-solving, and organizational skills.
Preferred Qualifications
Relevant certifications in DataBricks, Snowflake, or related technologies.
Experience working in an agile development environment.
Why Matrix
Matrix is a global, dynamic, fast-growing technical consultancy leading technology services company with 13000 employees worldwide. Since its foundation in 2001, Matrix has made more travelers and acquisitions and has executed some of the largest, most significant. The company specializes in implementing and developing leading technologies, software solutions, and products. It provides its customers with infrastructure and consulting services, IT outsourcing, offshore, training and assimilation, and Ves as representatives for the world's leading software vendors. With vast experience in private and public sectors, ranging from Finance, Telecom, Health, Hi-Tech, Education, Defense, and Secu city, Matrix's customer base includes guest organizations in Israel and a steadily growing client base worldwide.
We are comprised of talented, creative, and dedicated individuals passionate about delivering innovative solutions to the market. We source and foster the best talent and recognize that all employee's contributions are integral to our company's future.
Matrix- success is based on a challenging work environment, competitive compensation and benefits, and rewarding career opportunities. We encourage a diverse work environment of sharing, learning, and ceding together. Come and join the winning team! You'll be challenged and have fun in a highly respected organization. To Learn More, Visit Matrix -ifs. Com,
Show more
Show less","Data Engineering, Data Pipelines, Data Warehousing, ETL, AWS, GCP, Azure, Apache Spark, Apache Kafka, Apache Flink, Scala, Java, Python, SQL, DevOps, CI/CD, Agile, Snowflake, DataBricks","data engineering, data pipelines, data warehousing, etl, aws, gcp, azure, apache spark, apache kafka, apache flink, scala, java, python, sql, devops, cicd, agile, snowflake, databricks","agile, apache flink, apache kafka, apache spark, aws, azure, cicd, data engineering, databricks, datapipeline, datawarehouse, devops, etl, gcp, java, python, scala, snowflake, sql"
Senior Data Systems Engineer - BI,VISTRADA,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-systems-engineer-bi-at-vistrada-3767909878,2023-12-17,Freeport,United States,Mid senior,Hybrid,"Vistrada is seeking highly motivated candidates to serve as senior data systems engineers to work as part of our Business Intelligence team supporting multiple clients. Vistrada’s clients look to us to provide technical leadership and assist them in solving many of today’s most challenging digital transformation issues and complex data needs. In this role, you will help address operational challenges associated with data modeling, system architectures, and reporting requirements by applying advanced capabilities, modern technology, and best practices to real-world scenarios. Key Responsibilities: Work closely with clients to develop technology innovation plans and enhancements on a broad range of areas, including: Model-based system engineering Relational data architectures Data modeling Data integration processes and ETL development Report / Dashboard creation using leading visualization technology platforms (i.e., Tableau; Qlik; Power BI) Data analytics platform design, development, and testing Machine Learning and/or Statistical Analysis. Required Qualifications: Bachelor’s Degree and 8+ years of related experience 5+ years of experience with SQL databases, such as MSSQL, Oracle, MySQL, and/or PostgreSQL 5+ years of experience with leading visualization technology platforms (i.e., Tableau; Qlik; Power BI) Ability to work independently and eager to learn new technologies, techniques, processes, software languages, platforms, and systems Expertise to provide unbiased advice, formulate courses of action, analyze programs, and make recommendations across a wide spectrum of issues Passionate, goal driven, team-oriented, and outgoing Flexible, self-starter, and demonstrated ability to operate effectively with ambiguous and evolving objectives in a client-facing environment Effective communication skills. Preferred Qualifications 2+ years of experience with cloud architecture systems, such as Azure, Google Cloud, or AWS Competency with source code management systems Competency with Office365 Strong written communication skills. Powered by JazzHR
Show more
Show less","Data Modeling, System Architectures, Data Integration, ETL Development, SQL, MSSQL, Oracle, MySQL, PostgreSQL, Tableau, Qlik, Power BI, Data Analytics, Machine Learning, Statistical Analysis, Relational Data Architectures, Source Code Management, Office365, Azure, Google Cloud, AWS","data modeling, system architectures, data integration, etl development, sql, mssql, oracle, mysql, postgresql, tableau, qlik, power bi, data analytics, machine learning, statistical analysis, relational data architectures, source code management, office365, azure, google cloud, aws","aws, azure, data integration, dataanalytics, datamodeling, etl development, google cloud, machine learning, mssql, mysql, office365, oracle, postgresql, powerbi, qlik, relational data architectures, source code management, sql, statistical analysis, system architectures, tableau"
Senior Software Engineer (Data Strategy),SWARKY SOLUTIONS CORP,"New York, NY",https://www.linkedin.com/jobs/view/senior-software-engineer-data-strategy-at-swarky-solutions-corp-3787337166,2023-12-17,Freeport,United States,Mid senior,Hybrid,"Experience level: Mid-senior
Experience required: 8 Years
Education level: Master's degree
Job function: Information Technology
Industry: Financial Services
JOB DESCRIPTION:
1.	Data Strategy has a ""start-up style"" mandate (within a $2 billion company) to enhance the acquisition, storage, analysis, fidelity, and monetization of client, internal, and third-party data across the organization.
2.	This innovation spans our petabyte-scale insured assets, including property, business, marine, and aviation entities, and their associated risks, such as hurricanes, wildfires, cyber-attacks, and wars, in a financial and economic context.
3.	As a member of the Data Strategy group, the Senior Software Engineer will work with fellow data and web engineers, data scientists, product managers, business analysts, and stakeholders from other internal groups to design and improve data-centric projects with the dual mandate of (1) increasing the efficiency of the data collection and analysis process and (2) driving the monetization of data via newly designed and existing products.
4.	The Senior Software Engineer will be the head facilitator on multiple innovative initiatives and will have ownership over the design, development, and delivery of projects requiring direct reporting to senior-level management in both business and technical groups.
RESPONSIBILITIES:
7.	Work with a product manager as technical lead of a team of ~5 engineers, data scientists, and analysts to design, scope, and oversee work in an Agile environment.
8.	Manage junior data and web engineers, focusing on productivity, quality, and professional development.
9.	Partner with the head of Data Strategy and other senior engineers to create and evangelize best-in-class engineering competency and tooling within the organization.
10.	Enforce strong development standards across the team through code reviews, automated testing, and monitoring.
11.	Establish strong relationships with internal clients as an engineering representative for data strategy.
12.	Contribute to the overall Data Strategy vision and execution via quarterly planning and executive committee reporting.
13.	Partner regularly improving engineering recruiting process for the required skillsets and resourcing demands.
14.	Learn the complex business of reinsurance to coach data technologists and execute the team's initiatives more effectively.
15.	Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting, and new data products.
16.	Innovate new ways to leverage large and small datasets to drive revenue via the development of new products with the Data Strategy team, as well as the enhancement of existing products.
17.	Architect engineering solutions using the latest cloud technologies in a process that spans hypothesis-validating prototypes to large-scale production data products, ensuring internal security and regulatory compliance.
18.	Design solutions that account for unstructured data and document management system(s), including ingesting, tracking, parsing, analysing, and summarizing documents at scale.
19.	Perform exploratory and goal-oriented data analyses to understand and validate the requirements of data products and help create product roadmaps.
20.	Develop, implement, and deploy front-ends and APIs, which may involve business intelligence dashboards, data pipelines, machine learning algorithms, and file ingestion mechanisms.
QUALIFICATIONS:
21.	5-8+ years of relevant experience in data-focused software engineering
22.	Master's Degree or Ph.D. in data science, computer science, or related quantitative field such as applied mathematics, statistics, engineering or operations research, or equivalent experience.
23.	Experience working with Python-based server-side web frameworks like FastAPI or Django
24.	Strong knowledge of SQL and familiarity with the high-level properties of modern data stores.
25.	Strong understanding of the contemporary SDLC, including dev/QC/prod environments, unit/integration/UA testing, CI/CD, etc.
26.	Experience building and maintaining CI/CD pipelines with tools such as Azure DevOps, GitLab, Travis, Jenkins, etc.
27.	2+ years of data analysis, AI, or data science work.
28.	Experience with data cleaning, enrichment, and reporting to business users.
29.	Extensive experience with (py)Spark, Python, JSON, and SQL.
30.	Experience integrating data from semi-structured and unstructured sources.
31.	Knowledge of various industry-leading SQL and NoSQL database systems.
32.	Experience with or strong interest in learning about LLMs in a productized context.
ADDITIONAL QUALIFICATIONS:
33.	Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks.
34.	Experience with web scraping and crowdsourcing technologies.
35.	Experience with Databricks and optimizing Spark clusters.
36.	Experience architecting web ecosystems from the ground up, including monolith vs. microservice decisions, caching technologies, security integrations, etc.
37.	Experience working with data visualization dashboarding tools (PowerBI, Tableau).
38.	Insurance domain knowledge or strong interest in developing it.
39.	Experience with the MS Azure cloud environment.
Show more
Show less","Data Strategy, Machine learning, Python, SQL, Data science, Data analysis, Agile, Cloud computing, Data visualization, Data pipelines, Data monetization, Data storage, Data acquisition, Data fidelity, Data security, Data compliance, Data governance, Data integration, Data enrichment, Data reporting, Data products, Data warehousing, Data lakes, Data mining, Text analytics, Natural language processing, Artificial intelligence, Business intelligence, Software engineering, Web engineering, DevOps, CI/CD, Unit testing, Integration testing, User acceptance testing, Software development life cycle, Software quality assurance, Software deployment, Software maintenance, Software architecture, Systems analysis, Systems design, Systems implementation, Systems integration, Systems testing, Systems deployment, Systems maintenance, Systems administration, Project management, Product management, Business analysis, Risk management, Financial analysis, Economic analysis, Insurance domain knowledge","data strategy, machine learning, python, sql, data science, data analysis, agile, cloud computing, data visualization, data pipelines, data monetization, data storage, data acquisition, data fidelity, data security, data compliance, data governance, data integration, data enrichment, data reporting, data products, data warehousing, data lakes, data mining, text analytics, natural language processing, artificial intelligence, business intelligence, software engineering, web engineering, devops, cicd, unit testing, integration testing, user acceptance testing, software development life cycle, software quality assurance, software deployment, software maintenance, software architecture, systems analysis, systems design, systems implementation, systems integration, systems testing, systems deployment, systems maintenance, systems administration, project management, product management, business analysis, risk management, financial analysis, economic analysis, insurance domain knowledge","agile, artificial intelligence, business analysis, business intelligence, cicd, cloud computing, data acquisition, data compliance, data enrichment, data fidelity, data governance, data integration, data lakes, data mining, data monetization, data products, data reporting, data science, data security, data storage, data strategy, dataanalytics, datapipeline, datawarehouse, devops, economic analysis, financial analysis, insurance domain knowledge, integration testing, machine learning, natural language processing, product management, project management, python, risk management, software architecture, software deployment, software development life cycle, software engineering, software maintenance, software quality assurance, sql, systems administration, systems analysis, systems deployment, systems design, systems implementation, systems integration, systems maintenance, systems testing, text analytics, unit testing, user acceptance testing, visualization, web engineering"
Sr. Data and Software Engineer,Ovative Group,"New York, NY",https://www.linkedin.com/jobs/view/sr-data-and-software-engineer-at-ovative-group-3762353290,2023-12-17,Freeport,United States,Mid senior,Hybrid,"About Ovative Group
Ovative Group is the premier independent media and measurement firm in the United States. We help change-makers, in fast-growing, customer-centric organizations across industries reinvent their marketing and measurement programs. We leverage our media, measurement, and consulting capabilities to help brands like Coach, Kate Spade, Stuart Weitzman, Facebook, The Home Depot, CVS, Disney, and UnitedHealth Group transform their media and marketing programs. Our proprietary approach to measuring and optimizing marketing investment decisions, Enterprise Marketing Return (EMR), is disrupting the industry and setting the gold standard for customer and marketing strategy, activation, and measurement.
Recognized eight consecutive years on Star Tribune’s list of Top 150 Workplaces and five years on Inc. 5000’s list of the fastest-growing private companies in America, we pride ourselves in always overdelivering for our clients, our teams, and our communities.
About The Role
We are seeking a Sr. Data & Software Engineer to join our rapidly growing product and engineering development team. Our company specializes in enterprise-level media measurement and optimization across various industries. In this role, you will take a leadership position within a cross-functional team responsible for creating, optimizing, and maintaining scalable software and data solutions to enhance performance, stability, and scalability. You will play a pivotal role in guiding the team and working closely with stakeholders throughout the entire software development lifecycle, from concept to deployment.
The ideal candidate will bring extensive experience in iterative development practices, deep knowledge of version control (e.g., GitHub), and both conceptual and pragmatic problem-solving skills. Your outstanding attention to detail and strong written and oral communication skills will enable you to work directly with a variety of users, understanding their objectives and translating them into technical requirements and solutions. You will have the opportunity to mentor junior team members, contribute to innovative ideas, and drive the adoption of cutting-edge technologies within the team and foster your own professional growth.
Responsibilities
:
Lead the design, development, testing, and deployment of robust software solutions that meet business and technical goals.
Lead effort in identifying opportunities for automation with a focus on the operational stability of software applications and systems
Engage directly with Product Managers and stakeholders to understand their business goals, gather requirements, and translate into detailed, actionable technical requirements
Research, write and edit documentation and technical requirements, including software designs, evaluation plans, test results, technical manuals and formal recommendations and reports
Evaluate trade-offs between correctness, robustness, performance and customer impact to ensure we build the right solutions that scale
Mentor and level up the skills of your teammates through collaboration and by sharing your expertise
Ensure adherence to and advancement of software and product engineering best practices, including code quality, documentation, testing, security, deployment, and performance.
Evaluate and integrate new technologies, tools, and frameworks to optimize the software development process.
Troubleshoot complex software issues and provide effective solutions.
Provide guidance, mentorship, and contribute to architectural decisions, code reviews, and project direction.
Foster a culture of continuous learning to keep your team current with emerging technology and software engineering trends.
Requirements
:
5+ years of relevant data & software engineering development experience
Highly proficient working with ETL/ELT tooling for large data sets (e.g., Airbyte)
Highly proficient utilizing SQL, Python, and command line
Experience with cloud-based platforms (i.e. GCP, AWS)
Preferred
:
Experience working with marketing, analytics and customer data
Experience working with APIs for data retrieval
Experience working with data warehouses and big data tools (e.g., BigQuery, Databricks)
Experience creating data/table architecture
Experience implementing QA processes and QA automation
Experience integrating data models within software
Pay Transparency
At Ovative, we offer a transparent view into three core components of your total compensation package: Base Salary, Annual Bonus, and Benefits. The salary range for this position below is inclusive of an annual bonus. Actual offers are made with consideration for relevant experience and anticipated impact. Additional benefits information is provided below.
For our Manager positions, our compensation ranges from $79,000 to $132,000, which is inclusive of a 20% bonus.
Benefits Of Working At Ovative Group
We provide strong, competitive, holistic benefits that understand the importance of your life inside and out of work.
Culture:
Culture matters and we’ve been recognized as a Top Workplace for eight years running because of it. We demand trust and transparency from each other. We believe in doing the hard and complicated work others put off. We’re open in communication and floor plan. We’re flat – our interns sit next to VPs, our analysts work closely with senior leaders, and our CEO interacts with every single person daily. Put together, these elements help foster an environment where smart people can support each other in performing to their highest potential.
Compensation and Insurance:
We strive to hire and retain the best talent. Paying fair, competitive compensation, with a large bonus incentive, and phenomenal health insurance is an important part of this mix.
We’re rewarded fairly and when the company performs well, we all benefit.
Tangible amenities we enjoy:
Access to all office spaces in MSP, NYC, and CHI
Frequent, paid travel to our Minneapolis headquarters for company events, team events, and in-person collaboration with teams.
Flexible paid vacation policy
401k match program
Top-notch health insurance options
Monthly stipend for your mobile phone and data plan
Sabbatical program
Charitable giving via our time and a financial match program
Shenanigan’s Day
Working at Ovative won’t be easy, but if you like getting your hands dirty, driving results, and being surrounded by the best talent, it’ll be the most rewarding job you’ll ever have. If you think you can make us better, we want to hear from you!
Show more
Show less","Data & Software Engineering, Version control, GitHub, ETL/ELT, SQL, Python, Command line, Cloudbased platforms, GCP, AWS, Marketing data, Analytics, Customer data, APIs, Data warehouses, Big data tools, BigQuery, Databricks, Data architecture, QA processes, QA automation, Data models","data software engineering, version control, github, etlelt, sql, python, command line, cloudbased platforms, gcp, aws, marketing data, analytics, customer data, apis, data warehouses, big data tools, bigquery, databricks, data architecture, qa processes, qa automation, data models","analytics, apis, aws, big data tools, bigquery, cloudbased platforms, command line, customer data, data architecture, data models, data software engineering, data warehouses, databricks, etlelt, gcp, github, marketing data, python, qa automation, qa processes, sql, version control"
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"Manhattan, NY",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-engineering-at-acv-auctions-3762876476,2023-12-17,Freeport,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Compensation:
$124,000.00 - $152,000.00 annually. Please note that final compensation will be determined based upon the applicant's relevant experience, skillset, location, business needs, market demands, and other factors as permitted by law.
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, C#, SSIS, Airflow, MongoDB, AWS RDS, S3, SQS, SNS, OLTP, ETL, APIs, Data modeling, Data storage, Message brokers, Protocols, Interfaces, OLAP, Snowflake, DBT, Unit testing, Integration testing","sql, c, ssis, airflow, mongodb, aws rds, s3, sqs, sns, oltp, etl, apis, data modeling, data storage, message brokers, protocols, interfaces, olap, snowflake, dbt, unit testing, integration testing","airflow, apis, aws rds, c, data storage, datamodeling, dbt, etl, integration testing, interfaces, message brokers, mongodb, olap, oltp, protocols, s3, snowflake, sns, sql, sqs, ssis, unit testing"
Sr. Data and Software Engineer,Ovative Group,"New York, NY",https://www.linkedin.com/jobs/view/sr-data-and-software-engineer-at-ovative-group-3762351450,2023-12-17,Freeport,United States,Mid senior,Hybrid,"About Ovative Group
Ovative Group is the premier independent media and measurement firm in the United States. We help change-makers, in fast-growing, customer-centric organizations across industries reinvent their marketing and measurement programs. We leverage our media, measurement, and consulting capabilities to help brands like Coach, Kate Spade, Stuart Weitzman, Facebook, The Home Depot, CVS, Disney, and UnitedHealth Group transform their media and marketing programs. Our proprietary approach to measuring and optimizing marketing investment decisions, Enterprise Marketing Return (EMR), is disrupting the industry and setting the gold standard for customer and marketing strategy, activation, and measurement.
Recognized eight consecutive years on Star Tribune’s list of Top 150 Workplaces and five years on Inc. 5000’s list of the fastest-growing private companies in America, we pride ourselves in always overdelivering for our clients, our teams, and our communities.
About The Role
We are seeking a Sr. Data & Software Engineer to join our rapidly growing product and engineering development team. Our company specializes in enterprise-level media measurement and optimization across various industries. In this role, you will take a leadership position within a cross-functional team responsible for creating, optimizing, and maintaining scalable software and data solutions to enhance performance, stability, and scalability. You will play a pivotal role in guiding the team and working closely with stakeholders throughout the entire software development lifecycle, from concept to deployment.
The ideal candidate will bring extensive experience in iterative development practices, deep knowledge of version control (e.g., GitHub), and both conceptual and pragmatic problem-solving skills. Your outstanding attention to detail and strong written and oral communication skills will enable you to work directly with a variety of users, understanding their objectives and translating them into technical requirements and solutions. You will have the opportunity to mentor junior team members, contribute to innovative ideas, and drive the adoption of cutting-edge technologies within the team and foster your own professional growth.
Responsibilities
:
Lead the design, development, testing, and deployment of robust software solutions that meet business and technical goals.
Lead effort in identifying opportunities for automation with a focus on the operational stability of software applications and systems
Engage directly with Product Managers and stakeholders to understand their business goals, gather requirements, and translate into detailed, actionable technical requirements
Research, write and edit documentation and technical requirements, including software designs, evaluation plans, test results, technical manuals and formal recommendations and reports
Evaluate trade-offs between correctness, robustness, performance and customer impact to ensure we build the right solutions that scale
Mentor and level up the skills of your teammates through collaboration and by sharing your expertise
Ensure adherence to and advancement of software and product engineering best practices, including code quality, documentation, testing, security, deployment, and performance.
Evaluate and integrate new technologies, tools, and frameworks to optimize the software development process.
Troubleshoot complex software issues and provide effective solutions.
Provide guidance, mentorship, and contribute to architectural decisions, code reviews, and project direction.
Foster a culture of continuous learning to keep your team current with emerging technology and software engineering trends.
Requirements
:
5+ years of relevant data & software engineering development experience
Highly proficient working with ETL/ELT tooling for large data sets (e.g., Airbyte)
Highly proficient utilizing SQL, Python, and command line
Experience with cloud-based platforms (i.e. GCP, AWS)
Preferred
:
Experience working with marketing, analytics and customer data
Experience working with APIs for data retrieval
Experience working with data warehouses and big data tools (e.g., BigQuery, Databricks)
Experience creating data/table architecture
Experience implementing QA processes and QA automation
Experience integrating data models within software
Pay Transparency
At Ovative, we offer a transparent view into three core components of your total compensation package: Base Salary, Annual Bonus, and Benefits. The salary range for this position below is inclusive of an annual bonus. Actual offers are made with consideration for relevant experience and anticipated impact. Additional benefits information is provided below.
For our Manager positions, our compensation ranges from $79,000 to $132,000, which is inclusive of a 20% bonus.
Benefits Of Working At Ovative Group
We provide strong, competitive, holistic benefits that understand the importance of your life inside and out of work.
Culture:
Culture matters and we’ve been recognized as a Top Workplace for eight years running because of it. We demand trust and transparency from each other. We believe in doing the hard and complicated work others put off. We’re open in communication and floor plan. We’re flat – our interns sit next to VPs, our analysts work closely with senior leaders, and our CEO interacts with every single person daily. Put together, these elements help foster an environment where smart people can support each other in performing to their highest potential.
Compensation and Insurance:
We strive to hire and retain the best talent. Paying fair, competitive compensation, with a large bonus incentive, and phenomenal health insurance is an important part of this mix.
We’re rewarded fairly and when the company performs well, we all benefit.
Tangible amenities we enjoy:
Access to all office spaces in MSP, NYC, and CHI
Frequent, paid travel to our Minneapolis headquarters for company events, team events, and in-person collaboration with teams.
Flexible paid vacation policy
401k match program
Top-notch health insurance options
Monthly stipend for your mobile phone and data plan
Sabbatical program
Charitable giving via our time and a financial match program
Shenanigan’s Day
Working at Ovative won’t be easy, but if you like getting your hands dirty, driving results, and being surrounded by the best talent, it’ll be the most rewarding job you’ll ever have. If you think you can make us better, we want to hear from you!
Show more
Show less","Data & software engineering, Iterative development practices, Version control, GitHub, Problemsolving, Attention to detail, Written and oral communication, Mentoring, Teamwork, Automation, ETL/ELT, Airbyte, SQL, Python, Command line, Cloudbased platforms, GCP, AWS, Marketing, Analytics, Customer data, APIs, Data warehouses, Big data, BigQuery, Databricks, Data architecture, QA processes, QA automation, Data models","data software engineering, iterative development practices, version control, github, problemsolving, attention to detail, written and oral communication, mentoring, teamwork, automation, etlelt, airbyte, sql, python, command line, cloudbased platforms, gcp, aws, marketing, analytics, customer data, apis, data warehouses, big data, bigquery, databricks, data architecture, qa processes, qa automation, data models","airbyte, analytics, apis, attention to detail, automation, aws, big data, bigquery, cloudbased platforms, command line, customer data, data architecture, data models, data software engineering, data warehouses, databricks, etlelt, gcp, github, iterative development practices, marketing, mentoring, problemsolving, python, qa automation, qa processes, sql, teamwork, version control, written and oral communication"
Sr. Data Analyst,Mike Albert Fleet Solutions,"Cincinnati, OH",https://www.linkedin.com/jobs/view/sr-data-analyst-at-mike-albert-fleet-solutions-3729908374,2023-12-17,Park Forest,United States,Associate,Hybrid,"Sr. Reporting Analyst
Mike Albert Fleet Solutions is the fastest-growing fleet management company in the United States. At the forefront of the mobility evolution, we offer pioneering transportation solutions that empower businesses, government entities, municipalities, and more. Our innovative approaches aim to build scalable fleets that enhance cash flow, reduce total ownership costs, amplify brand visibility, and elevate productivity. Join us in reimagining the future of mobility.
Role Overview
We are looking for analytical thinkers, adept problem solvers, and insightful culture hackers to join our team. More specifically, we are seeking a Sr. Reporting Analyst. to strengthen our data & insights team. The Sr. Reporting Analyst at Mike Albert Fleet Solutions plays an essential role in driving informed business decisions by collecting, interpreting, and presenting data. This role demands an individual with a keen analytical mind, a flair for transforming raw data into actionable insights, and a natural inclination to explore cutting-edge technologies to enhance scalability and efficiency.
Key Responsibilities
Data Collection, Interpretation, and Reporting:
Regularly gather, clean, and interpret client data, ensuring its precision and relevance.
Create, maintain, and refine analytics, producing both scheduled and on-demand reports utilizing tools such as Tableau, Excel, and PowerPoint. Reports should be data-rich, yet easily digestible and actionable for various stakeholders.
Transmute key data points into insightful visualizations and models, forecasting both present-day and future trends across clients.
Tech-Driven Process & System Optimization:
Suggest and implement improvements to refine our current reporting system, ensuring adaptability and better efficiency.
Continuously explore and adopt innovative technologies like Python and others to bolster and scale our data processing capabilities.
Stay attuned to industry advancements, integrating best practices into our operational blueprint.
Collaboration & Client Interaction:
Collaborate closely with our tech department, focusing on the integration of vital performance metrics into our client-centric dashboard and tools.
Team up with Client Partnership and Business Development Managers to pinpoint reporting requirements, cater to client inquiries, and devise effective strategies driven by analytical insights.
Engage directly with clients, providing clarity on data interpretations, and imparting actionable insights.
Innovative Analytics & Exploration:
Seek alternative data repositories to benchmark our client data, overlaying both retrospective and prospective trends.
Pioneer innovative methods of data presentation, ensuring continuous relevance and insight depth.
Spearhead special projects, aligning them with overarching company goals and client aspirations.
Adaptability & Continuous Learning:
Display a relentless curiosity, diving into emerging technologies, methodologies, and tools that can enhance data processing, reporting, and visualization.
Other Tasks:
Undertake any other duties as dictated by management, always aligning with Mike Albert Fleet Solutions’ vision and maintaining our esteemed standards.
Qualifications
Educational Background:
A Bachelor’s degree in Data Science, Analytics, Computer Science, Information Systems, or a related field.
Relevant Work Experience:
Several years (typically 5-7) of experience in reporting, data analytics, or a closely related role.
Demonstrable experience in using tools like Tableau, Excel, and PowerPoint for data visualization and reporting.
Prior experience with or exposure to programming languages for data analysis and processing.
Technical Proficiency:
Proficiency in data analytics tools such as Tableau.
Advanced skills in Excel for data analysis, modeling, and visualization.
Experience with Python, particularly libraries related to data analytics and visualization such as Pandas, NumPy, Matplotlib, and Seaborn.
Familiarity with databases and SQL for extracting and managing data.
Analytical and Problem-Solving Skills:
An ability to translate complex data into clear insights and actionable recommendations.
A knack for identifying trends, patterns, and anomalies in detailed and complex data.
Communication Skills:
Stellar written and verbal communication capabilities, given the need to explain data interpretations to both technical and non-technical stakeholders.
Ability to design and present comprehensive reports that cater to diverse audiences.
Adaptability & Continuous Learning:
An intrinsic motivation to stay updated with the latest advancements in data analytics and reporting tools.
Openness to exploring and learning new technologies, tools, and methodologies to enhance efficiency and insights.
Team Collaboration & Client Interaction:
Proven track record of collaborating with multi-disciplinary teams, especially technology and client-facing teams.
Experience or willingness to directly engage with clients, understanding their needs, and providing data-driven recommendations.
Drug Free Employer
Show more
Show less","Tableau, Excel, PowerPoint, Python, Pandas, NumPy, Matplotlib, Seaborn, SQL","tableau, excel, powerpoint, python, pandas, numpy, matplotlib, seaborn, sql","excel, matplotlib, numpy, pandas, powerpoint, python, seaborn, sql, tableau"
Data Integration Engineer (m/f/d),Smart Steel Technologies,"Cincinnati, OH",https://www.linkedin.com/jobs/view/data-integration-engineer-m-f-d-at-smart-steel-technologies-3773533385,2023-12-17,Park Forest,United States,Mid senior,Onsite,"YOUR TASKS
In this role as Data Engineer you will be responsible for integrating our customers data sources from various production lines and systems (PLCs, L1, L2, L3) into our Data Platform. You will manage and monitor the data flow between various data sources and on-premise databases and cloud services.
This is a highly-technical position within the leading artificial intelligence company in the steel industry. Communication, proven problem-solving skills, and knowledge of integration best practices are critical to successful performance in this role.
Design and implement a data warehouse architecture that supports business operations
Support a wide range of data transformations and migrations (structured, unstructured data)
Investigate and mitigate data mismatches, detect data anomalies (incompleteness, accuracy)
Create data model matching production data according to material genealogy across multiple processing steps
Write scripts to automate data integration processes to improve efficiency and reduce human error
Work together with our data scientists to provide a data landscape for our AI applications
Implement data security policies to protect sensitive information from being compromised
YOUR PROFILE
Bachelor’s or Master’s degree in a technical field (e.g. Computer Science, Engineering)
Expert knowledge of designing Data Warehousing and data integration
Experience with SQL (PostgreSQL, Oracle, MS SQL, MySQL) and NoSQL databases
Experience with industrial L1 systems (OPC UA, PLC communications, iba
Quick, self-learning capabilities and creativity in problem-solving
Familiarity with Python, Unix Shell Scripting, ProtoBuf and/or JSON, Airflow, ETL tools, Kafka
Clickhouse and/or Milvus is a plus
Hands-on experience with Git and Jira is a plus
Benefits
We offer flexible work hours, a remote friendly environment and competitive compensation. Employee Benefits include medical, dental, vision, basic and supplemental life insurance, short and long-term disability and 401(k) with up to 4% company match. Paid Time Off up to 20 days per year, in addition to 10 holidays and parental leave.
We do not offer sponsorship for the visa process
About Us
Smart Steel Technologies supplies ready-to-use artificial intelligence software products for process optimization in steel mills that lead to improved quality, reduction of energy cost and improvement of CO2 efficiency. The machine learning and data science we employ comes in several varieties: image processing, signal processing, classical supervised modeling, unsupervised methods, and fundamental statistics and probability. We are hiring talented experts across disciplines to join our team and help us to further develop our industry leading solutions. Did you know that steel is the most recycled material worldwide?
Show more
Show less","Data Warehousing, Data Integration, SQL, PostgreSQL, Oracle, MS SQL, MySQL, NoSQL, Industrial L1 Systems, OPC UA, PLC Communications, Python, Unix Shell Scripting, ProtoBuf, JSON, Airflow, ETL Tools, Apache Kafka, Clickhouse, Milvus, Git, Jira","data warehousing, data integration, sql, postgresql, oracle, ms sql, mysql, nosql, industrial l1 systems, opc ua, plc communications, python, unix shell scripting, protobuf, json, airflow, etl tools, apache kafka, clickhouse, milvus, git, jira","airflow, apache kafka, clickhouse, data integration, datawarehouse, etl tools, git, industrial l1 systems, jira, json, milvus, ms sql, mysql, nosql, opc ua, oracle, plc communications, postgresql, protobuf, python, sql, unix shell scripting"
Project Data Engineer,STSI (Staffing Technical Services Inc.),"Cincinnati, OH",https://www.linkedin.com/jobs/view/project-data-engineer-at-stsi-staffing-technical-services-inc-3784809074,2023-12-17,Park Forest,United States,Mid senior,Onsite,"PROJECT DATA ENGINEER
JOB DESCRIPTION:
We are seeking a talented and detail-oriented Data Engineer to join our dynamic team. The ideal candidate will play a key role in ensuring data quality and maintaining project data lists in Excel and other data repositories on the project. As a Data Engineer, you will work closely with the project leadership team to monitor, analyze, and enhance data integrity, providing critical support in maintaining accurate and up-to-date project information.
Project Data Engineers are accountable to the Project Technical Lead of the specific projects to which they are assigned.
QUALIFICATIONS:
Bachelor’s degree in Computer Science, Information Technology, or a related
Proven experience as a Data Engineer or a similar role. Experience on industrial engineering projects is
Proficiency in Excel and experience with advanced
Strong understanding of data quality principles and best
Familiarity with data integration tools and
Excellent problem-solving and analytical
Effective communication and collaboration
JOB RESPONSIBILITIES:
Data Quality Monitoring:
Implement and manage processes for monitoring and ensuring the quality of project data.
Develop and execute data validation strategies to identify and rectify inconsistencies or errors.
Excel Data Management:
Create and maintain project data lists in Excel, ensuring accuracy and
Utilize advanced Excel functions and features to analyze and manipulate data effectively.
Data Integration:
Collaborate with cross-functional teams to integrate data from various sources into centralized
Develop and maintain data pipelines to streamline data flow and enhance
Data Cleansing and Transformation:
Identify and address data anomalies through data cleansing and transformation processes.
Implement data enrichment techniques to enhance the value of project
Data Entry
Assist Project Team in completing data entry as
Data Cleansing and Transformation:
Identify and address data anomalies through data cleansing and transformation processes.
Implement data enrichment techniques to enhance the value of project
Documentation:
Document data engineering processes, data models, and data
Ensure comprehensive documentation of Excel templates and data entry
Collaboration: A. Work closely with project engineers and other stakeholders to understand data requirements. B. Collaborate with IT teams to implement and optimize data-related
Continuous Improvement:
Stay informed about industry best practices and emerging trends in data
Proactively identify opportunities for process improvement and implement enhancements.
Show more
Show less","Data Quality Monitoring, Data Validation, Excel, Data Integration, Data Pipelines, Data Cleansing, Data Transformation, Data Enrichment, Documentation, Data Collaboration, Continuous Improvement, Data Entry","data quality monitoring, data validation, excel, data integration, data pipelines, data cleansing, data transformation, data enrichment, documentation, data collaboration, continuous improvement, data entry","continuous improvement, data collaboration, data enrichment, data entry, data integration, data quality monitoring, data transformation, data validation, datacleaning, datapipeline, documentation, excel"
"Lead Data Engineer/Data Stage - Cincinnati, OH (Hybrid)",Lorven Technologies Inc.,"Cincinnati, OH",https://www.linkedin.com/jobs/view/lead-data-engineer-data-stage-cincinnati-oh-hybrid-at-lorven-technologies-inc-3660517226,2023-12-17,Park Forest,United States,Mid senior,Onsite,"Our client is looking
Lead Data Engineer/Data Stage
for
Long Term
project in
Cincinnati, OH
below is the detailed requirements.
Job Title: Lead Data Engineer/Data Stage
Location: Cincinnati, OH
Duration: Long Term
Job Description
Bachelor's degree in Computer science or equivalent, with minimum 9+ years of relevant experience.
Must have experience in Datastage/Informatica ETL Database: DB2, Data Engineering, Data Architecture Methodologies: Agile.
Must have experience as 6 years of related experience, including at least 4 years in a hands-on data engineering role.
Should be proficient in SQL.
Must be able to communicate ideas both verbally and in writing to management, business and IT sponsors, and technical resources in language that is appropriate for each group.
Proficient in relational database modeling concepts and techniques.
Solid conceptual understanding of data engineering principles.
Financial industry experience is a plus, specifically FDIC.
Demonstrate excellent communication skills including the ability to effectively communicate with internal and external customers.
Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns and a high level of focus and attention to detail.
Strong work ethic with good time management with the ability to work with diverse teams and lead meetings.
Show more
Show less","Data Engineering, Data Architecture, DataStage, Informatica, SQL, DB2, Agile, Relational Database Modeling, Financial Industry Experience, FDIC, Communication Skills, Industry Knowledge, Time Management, Team Leadership","data engineering, data architecture, datastage, informatica, sql, db2, agile, relational database modeling, financial industry experience, fdic, communication skills, industry knowledge, time management, team leadership","agile, communication skills, data architecture, data engineering, datastage, db2, fdic, financial industry experience, industry knowledge, informatica, relational database modeling, sql, team leadership, time management"
Senior Data Platform Engineer,Western & Southern Financial Group,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-data-platform-engineer-at-western-southern-financial-group-3773538542,2023-12-17,Park Forest,United States,Mid senior,Onsite,"Western & Southern Financial Group (W&SFG) is a Fortune 500 Financial Services company with a long history of strength and stability. We provide life and health insurance, annuities, mutual funds, and investment management products and services for millions of people through our member companies.
We are seeking a highly motivated Senior Data Platforms Administrator
to support the growing Information Management team within our Information Technology (IT) department. Our IT department executes with passion and proficiency, delivering to our clients valued services that help them to achieve their financial goals. We make solutions simple, align resources and assets with strategic priorities and continually improve our skills, processes and IT capabilities. With your help, we can continue to unleash the possibilities!
What would I do?
As a
Senior Data Platform Administrator,
you will be a part of our Information Management Data Platforms team. You will be responsible for a set of integration tools that provides our organization the ability to build complex integration pipelines that help fuel decision making efforts within the business. In this role you will be a part of a Center of Excellence team that provides primary support to internal customers within the organization. Some of these Data Platforms include Informatica Powercenter, Informatica Powerexchange, Informatica MDM, Informatica Data Quality, Tableau, Knime, and IBM Cognos. We are looking for a thought leader with years of experience in the integration space. Additional responsibilities include driving standards, leading the CoE team, automating processes, continuous improvement, creating roadmaps, and assisting with data platform patching and upgrade efforts. You will have a wonderful opportunity to research and use new technologies as our dynamic team continues to implement and support Data Integration solutions across Western & Southern Financial Group.
This is a great opportunity for someone looking to expand their knowledge and lead a team in the integration tools administration world. Qualified candidates should have hands on experience with the tools listed above but the primary responsibility will be within Informatica. Other desired qualifications would include but are not limited to; windows server exposure, application monitoring, scheduling tools user provisioning, great analytical skill for troubleshooting, ETL development standards, SQL, RDBMS, SaaS, PaaS, and Cloud solutions, support of system maintenance and upgrade procedures, and hands on experience with IDMC (Informatica’s Intelligent Data Management Cloud Offering).
You would enjoy a very competitive benefit plan including medical, dental, prescription, vision, long term and short term disability coverage, tuition reimbursement, free fitness center, paid time off and holidays, a 401(k) plan with a company match, AND a defined benefit pension plan! We offer highly subsidized parking or bus subsidies, free breakfast and lunch in our onsite café, as well as numerous employee discounts.
Tell me about Western & Southern Financial Group
There's never been a better time to become an associate!
Financial Strength
: We're a
Fortune 500
company that's been in business for over 130 years, with a capital-to-asset ratio of over 19%. We are consistently recognized for our financial stability and operating performance by ratings agencies.
Supportive Culture
: We're a fast paced professional work environment with a focus on the welfare of our associates. Over 50% of our positions are filled from within.
Community Involvement
: Western & Southern is deeply rooted in Cincinnati as one of the city’s most generous corporations with a five year total of over $50 million in donations and sponsorships throughout the Greater Cincinnati area. From the Thanksgiving Day Race, to the W&S Open, to the W&S WEBN Fireworks, we impact our communities and our citizens every day through our corporate citizenship and by regularly inviting our associates’ participation in events.
What's next?
If you are looking for a career with an organization that is a leader in the financial industry, a committed community partner, and truly understands the value of its talented workforce,
apply today
!
Show more
Show less","Informatica Powercenter, Informatica Powerexchange, Informatica MDM, Informatica Data Quality, Tableau, Knime, IBM Cognos, Windows Server, Scheduling Tools, SQL, RDBMS, SaaS, PaaS, Cloud Computing, Informatica’s Intelligent Data Management Cloud Offering (IDMC), ETL Development Standards, Data Integration","informatica powercenter, informatica powerexchange, informatica mdm, informatica data quality, tableau, knime, ibm cognos, windows server, scheduling tools, sql, rdbms, saas, paas, cloud computing, informaticas intelligent data management cloud offering idmc, etl development standards, data integration","cloud computing, data integration, etl development standards, ibm cognos, informatica data quality, informatica mdm, informatica powercenter, informatica powerexchange, informaticas intelligent data management cloud offering idmc, knime, paas, rdbms, saas, scheduling tools, sql, tableau, windows server"
Senior Data Engineer,Gentis Solutions,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-at-gentis-solutions-3788342092,2023-12-17,Park Forest,United States,Mid senior,Onsite,"Gentis Solutions is seeking a Senior Data Analyst to join our team. This contract-to-hire position is with one of our Fortune 50 clients interested in full-time flex/remote consultants. The ideal candidates will have the required skills listed below and will be eligible and open to being hired by our client at the end of the project's duration. This position works alongside an existing team and leverages enterprise-level technologies and processes. If you would like to work at a company that has been recognized for its diversity and inclusion, its work to drive positive social change, and as an environmental leader, make sure you apply below.
Requirements
2+ years of direct experience in managing and curating data catalogs, with a proven track record of technical expertise
2+ years of experience working closely with data stewardship principles and practices, demonstrating a strong understanding of data governance and stewardship concepts
Experience with Cloud Platforms, including Azure Data Lake Storage (ADLS), Unity Catalog, Databricks, and Azure Synapse, with a strong understanding of metadata management, security principles, and fundamental cloud concepts
Expertise with Python and SQL for querying and managing databases for data storage and retrieval
Strong experience in network administration, with a solid foundation in managing data access and security in a networked environment
Strong previous experience working with Alation or any other leading data catalog platform, showcasing proficiency in configuring and customizing such tools
Proficiency integrating data catalog processes, customizing configurations, implementing connectors, and creating data processing scripts
Familiarity with other data catalog solutions in addition to Alation, highlighting versatility and adaptability in managing various cataloging technologies
Support, maintain, and document software functionality, best practices, standards, and processes
Demonstrable critical thinking
Experience with an E-commerce or multi-channel retail environment
Excellent verbal and written communication skills
Be passionate about Metadata and Databases!
Desirable Skills
Relevant certifications in data management, data governance, or related fields are a plus
Knowledge with Databricks or Jupyter Notebooks for data engineering, data preparation, collaborative data analytics, data analysis and documentation a plus
Familiarity with Tableau, Power BI or other data visualization and reporting tools a plus
Typical Duties
Collaborate with cross-functional teams to contribute to and validate the technical aspects of the Data Catalog Roadmap, ensuring alignment with organizational goals and objectives
Configure and customize the data catalog platform, such as Alation, to accurately represent our organization's data landscape, making it a valuable resource for data users
Implement and maintain permissions in Alation, ensuring that the Data Catalog is used in accordance with data access policies and security protocols
Take ownership of data source-specific configurations in Alation, and delegate specific actions to Data Source Administrators, overseeing their work to maintain consistency and accuracy
Set up and manage authentication mechanisms, user accounts, groups, and other relevant security components to safeguard data access and maintain compliance
Write, maintain, and execute Security Plans, IT Playbooks, Support Plans, and Data Access Policies to ensure the reliable and secure operation of the Data Catalog
Assign specific tasks and responsibilities to other members of the technical team, ensuring efficient collaboration and workload distribution
Work closely with the product management team to prioritize data source requirements and project plans, ensuring alignment with organizational objectives
Collaborate with product management to define operational standards and procedures for the data catalog, promoting consistency and best practices
Plan and coordinate the ingestion of new or updated Business Intelligence (BI) and data sources, ensuring seamless integration into the catalog
Work with product management to define the settings and repeatable patterns for configuring new data sources and managing access permissions
Coordinate with all data management and IT organizations as required, including compliance and governance organizations, to ensure cross-functional alignment
Collaborate with the Data Discoverability team to discuss the status of data discoverability initiatives and identify both technical and non-technical issues that may arise
Report on operational service level agreements (SLAs) and support performance, providing transparency and accountability
Investigate and resolve escalated technical issues, ensuring timely resolution and minimal disruption to data users
Delegate the investigation and resolution of issues to the technical team and Data Source Administrators as appropriate, fostering a collaborative problem-solving approach
Participate in IT-related communication planning and execution, ensuring that relevant stakeholders are informed of important developments
Plan and participate in training sessions to enhance the skills and knowledge of team members and data source administrators
Recruit, train, and mentor Data Stewards and/or other team members, fostering their professional development and growth within the organization
Actively participate in program status and planning meetings, contributing valuable insights and expertise
Show more
Show less","Data Catalog, Data Governance, Data Stewardship, Azure Data Lake Storage (ADLS), Unity Catalog, Databricks, Azure Synapse, Python, SQL, Network Administration, Data Security, Alation, Data Catalog Configuration, Data Catalog Customization, Data Connector Implementation, Data Processing Scripts, Data Visualization, Tableau, Power BI, Data Roadmap, Data Catalog Platform, Data Access Policies, Authentication Mechanisms, User Accounts, Groups, Security Plans, IT Playbooks, Support Plans, Data Access Policies, Business Intelligence (BI), Data Discoverability, Service Level Agreements (SLAs), Technical Issue Resolution, IT Communication Planning, Training, Mentoring, Data Stewards, Program Status and Planning Meetings","data catalog, data governance, data stewardship, azure data lake storage adls, unity catalog, databricks, azure synapse, python, sql, network administration, data security, alation, data catalog configuration, data catalog customization, data connector implementation, data processing scripts, data visualization, tableau, power bi, data roadmap, data catalog platform, data access policies, authentication mechanisms, user accounts, groups, security plans, it playbooks, support plans, data access policies, business intelligence bi, data discoverability, service level agreements slas, technical issue resolution, it communication planning, training, mentoring, data stewards, program status and planning meetings","alation, authentication mechanisms, azure data lake storage adls, azure synapse, business intelligence bi, data access policies, data catalog, data catalog configuration, data catalog customization, data catalog platform, data connector implementation, data discoverability, data governance, data processing scripts, data roadmap, data security, data stewards, data stewardship, databricks, groups, it communication planning, it playbooks, mentoring, network administration, powerbi, program status and planning meetings, python, security plans, service level agreements slas, sql, support plans, tableau, technical issue resolution, training, unity catalog, user accounts, visualization"
Advanced Data Engineer,Gentis Solutions,"Blue Ash, OH",https://www.linkedin.com/jobs/view/advanced-data-engineer-at-gentis-solutions-3747157854,2023-12-17,Park Forest,United States,Mid senior,Onsite,"Gentis Solutions is seeking an Advanced Data Engineer to join our team. This contract-to-hire position is with one of our Fortune 50 clients interested in full-time consultants. The ideal candidates will have the required skills listed below and will be eligible and open to being hired by our client at the end of the project's duration. This position works alongside an existing team and leverages enterprise-level technologies and processes. If you would like to work at a company that has been recognized for its diversity and inclusion, its work to drive positive social change, and as an environmental leader, make sure you apply below.
Requirements
7 years of applicable hands-on experience in data development and principles including end-to-end design patterns
7 years proven track record of designing and delivering large-scale, high-quality operational or analytical data systems
7 years of successful and applicable experience taking a lead role in building complex data solutions that have been successfully delivered to customers
Proven ability to think and contribute at the strategic level
Demonstrated written, oral and presentation/public speaking communication skills
Understanding of network and data security architecture
Strong experience driving evolutionary change in data solutions and underlying technologies
Knowledge in three of the following technical disciplines: data warehousing, big data management, analytics development, data science, application programming interfaces (APIs), data integration, cloud, servers and storage, and database management
Typical Duties
Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement
Mentor team members in data principles, patterns, processes and practices
Promote the reuse of data assets, including the management of the data catalog for reference
Draft and review architectural diagrams, interface specifications and other design documents
Proactively and holistically lead activities that create deliverables to guide the direction, development, and delivery of technological responses to targeted business outcomes.
Provide facilitation, analysis, and design tasks required for the development of an enterprise's data and information architecture, focusing on data as an asset for the enterprise.
Develop target-state guidance (i.e., reusable standards, design patterns, guidelines, individual parts and configurations) to evolve the technical infrastructure related to data and information across the enterprise, including direct collaboration with 84.51.
Show more
Show less","Data Development, Data Design, Data Systems, Data Warehousing, Big Data Management, Analytics Development, Data Science, Application Programming Interfaces (APIs), Data Integration, Cloud Computing, Servers, Storage, Database Management, Data Strategy, Data Standards, SQL, NoSQL, Data Migration, Technology Environment Analysis, Data Principles, Data Patterns, Data Processes, Data Practices, Data Catalog Management, Architectural Diagrams, Interface Specifications, Design Documents, Data Architecture, TargetState Guidance, Technical Infrastructure","data development, data design, data systems, data warehousing, big data management, analytics development, data science, application programming interfaces apis, data integration, cloud computing, servers, storage, database management, data strategy, data standards, sql, nosql, data migration, technology environment analysis, data principles, data patterns, data processes, data practices, data catalog management, architectural diagrams, interface specifications, design documents, data architecture, targetstate guidance, technical infrastructure","analytics development, application programming interfaces apis, architectural diagrams, big data management, cloud computing, data architecture, data catalog management, data design, data development, data integration, data migration, data patterns, data practices, data principles, data processes, data science, data standards, data strategy, data systems, database management, datawarehouse, design documents, interface specifications, nosql, servers, sql, storage, targetstate guidance, technical infrastructure, technology environment analysis"
Senior Data Engineer,Compunnel Inc.,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-at-compunnel-inc-3771325284,2023-12-17,Park Forest,United States,Mid senior,Onsite,"Description
Accountable for developing and delivering technological responses to targeted business outcomes. Analyze, design and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise. Understand and follow reusable standards, design patterns, guidelines, and configurations to deliver valuable data and information across the enterprise, including direct collaboration with 84.51, where needed.
Responsibility
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Strong project management and organizational skills.
Key Responsibilities
Utilize enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Ensure there is clarity between ongoing projects, escalating when necessary, including direct collaboration with 84.51
Leverage innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
Define high-level migration plans to address the gaps between the current and future state
Contribute to the development of cost/benefit analysis for leadership to shape sound architectural decisions
Analyze technology environments to detect critical deficiencies and recommend solutions for improvement
Promote the reuse of data assets, including the management of the data catalog for reference
Draft architectural diagrams, interface specifications and other design documents
Education:
Bachelors Degree
Show more
Show less","SQL, Data architecture, Data engineering, Data analysis, Data modeling, Data integration, Data governance, Data quality, Data visualization, Big data, NoSQL, Cloud computing, Data management, Data processing, Data transformation, Data warehousing, Data mining, Data science, Machine learning, Artificial intelligence, Project management, Communication, Problemsolving, Critical thinking, Analytical skills","sql, data architecture, data engineering, data analysis, data modeling, data integration, data governance, data quality, data visualization, big data, nosql, cloud computing, data management, data processing, data transformation, data warehousing, data mining, data science, machine learning, artificial intelligence, project management, communication, problemsolving, critical thinking, analytical skills","analytical skills, artificial intelligence, big data, cloud computing, communication, critical thinking, data architecture, data engineering, data governance, data integration, data management, data mining, data processing, data quality, data science, data transformation, dataanalytics, datamodeling, datawarehouse, machine learning, nosql, problemsolving, project management, sql, visualization"
Data Analyst CO-OP (SACB),The Boston Beer Company,"Cincinnati, OH",https://www.linkedin.com/jobs/view/data-analyst-co-op-sacb-at-the-boston-beer-company-3691641242,2023-12-17,Park Forest,United States,Mid senior,Onsite,"Job Description
We are currently hiring a Data Analyst Co-Op at our Samuel Adams Brewery in Cincinnati, OH.
This position is from January- June 2024.
This role reports to the Corporate Sr. Quality Manager supporting operations across the Boston Beer enterprise. We are looking to on-board a third or fourth-year undergraduate student whose primary area of study is focused on analyzing data (Data Analyst). The successful candidate will work on a project(s) with the goal of turning data into information, information into insight and insight into business decisions. The co-worker in this position will be asked to gather data from primary and secondary sources, identify trends, analyze data using statistical techniques, and interpret & present findings using visuals.
What You'll Brew:
Data gathering:
Familiarize self with sources of data and the data collection process.
Manage data primarily related to Quality-related processes.
Work with management to prioritize business needs and acquire data from primary or secondary data sources.
Modeling data:
Develop, implement, and maintain data mining, data cleansing, and other strategies that optimize statistical efficiency and quality.
Manage confidential data and information according to guidelines.
Provide ongoing updates via reports or dashboards.
Make data driven recommendations to improve.
Presenting data:
Communicate findings through creating visualizations like charts and graphs, developing dashboards, writing reports, and presenting information to internal stakeholders.
Validating data:
Ensure the quality and accuracy of data in spread sheets and databases, either manually or through a programming language, so that interpretations will not be wrong or skewed.
Filter and “clean” data by reviewing reports, printouts, and performance indicators to locate and correct errors.
Support data integrity and normalization initiatives
Interpreting data:
Identify, analyze, and interpret trends or patterns in complex data sets to provide business insights, identify opportunities, help answer questions, solve problems, and make better decisions.
Generate data reports and analysis with data pulled from multiple systems.
Provide ongoing updates via reports or dashboards.
Make data-driven recommendations to improve the business.
What Ingredients You’ll Bring:
Minimum Qualifications:
Must be a student currently enrolled studying Chemistry, Biology, Microbiology, Chemical Engineering, or brewing program.
Required:
Excellent data analytic skills
Curiosity when analyzing data.
Ability to change analysis direction based on interim findings.
Ability to integrate and extract from datasets using relevant programs and coding.
Experience using SQL.
Strong Excel skills
Preferred Qualifications:
Excellent communication skills
Experience using R or Python to extract, manipulate, and analyze data.
Statistical analyses experience: regression modeling, decision trees, clustering
Some Perks:
When candidates join Boston Beer’s Early Talent Program,
Brew Your Future,
our early talent coworkers are empowered with impactful projects and learning sessions that contribute to real business outcomes, in addition to an inclusive, welcoming experience. There are many employee resource groups they can join and development training programs they can participate in. Our Early Talent program is made up of hands-on training in the office and at its breweries. Interns and Co-ops have the chance to meet with leaders at weekly lunch and learns to give them more insight into different parts of the business as well as skill building sessions. Each co-op and intern are encouraged to have weekly 1-on-1 check-ins with their manager to ensure their goals are aligned, and they are on the path to be successful at their time with Boston Beer. They are also required to do a final presentation that they present to the business leaders at the end of each cohort to demonstrate what they have learned.
Boston Beer Corporation is an equal opportunity employer and is committed to a diverse workforce. In order to help ensure reasonable accommodation for individuals protected by Section 503 of the Rehabilitation Act of 1973, the Vietnam Veteran’s Readjustment Act of 1974, and Title I of the Americans with Disabilities Act of 1990, applicants who wish to request accommodation in the job application process can contact jobs@bostonbeer.com for assistance.
Show more
Show less","Data Analysis, Data Gathering, Data Modeling, Data Validation, Data Interpretation, Data Presentation, SQL, Excel, R, Python, Statistical Analysis, Regression Modeling, Decision Trees, Clustering, Communication Skills","data analysis, data gathering, data modeling, data validation, data interpretation, data presentation, sql, excel, r, python, statistical analysis, regression modeling, decision trees, clustering, communication skills","clustering, communication skills, data gathering, data interpretation, data presentation, data validation, dataanalytics, datamodeling, decision trees, excel, python, r, regression modeling, sql, statistical analysis"
Senior Data Engineer,MRINetwork,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-at-mrinetwork-3788599538,2023-12-17,Park Forest,United States,Mid senior,Onsite,"Data Engineer
Oxford Search group has been retained to work with a well-established, rapidly growing Fintech company that is seeking multiple seasoned data engineers to join their organization. The company has developed a technology (multiple patents) that is transforming the payment industry....including many transactions within banking and financial services sectors. Revenues are expected to double or triple over the next 12 months, with a projected IPO sometime in 2026.
This engineer will play an integral role, working with the Technology, Sales, Marketing and Finance teams within. Seeking five years plus corporate experience working in both SAS and SPSS software platforms. Prefer individuals that are comfortable working in SAS Base.
Engineers Should Possess Solid Knowledge In The Following Analytical
Programming: CHAID Decision Tree, Regression Analysis, Predictive Modeling, Acquisition and Retention Modeling and Value Segmentation.
Experience building a robust Database Architecture that includes monthly Database Hygiene.
The ability to program in SQL, related to both Analytics and Database duties, would be ideal.
This role will report directly to the Head of IT, with a dotted line to the President of the Company. Financial Industry Regulatory and Compliance experience a plus but not required.
Benefits
The company offers an attractive base salary, comprehensive benefits, 401k w/ match, plus other unique perks and equity potential.
Show more
Show less","SAS, SPSS, CHAID Decision Tree, Regression Analysis, Predictive Modeling, Acquisition and Retention Modeling, Value Segmentation, Database Architecture, Database Hygiene, SQL, Financial Industry Regulatory and Compliance","sas, spss, chaid decision tree, regression analysis, predictive modeling, acquisition and retention modeling, value segmentation, database architecture, database hygiene, sql, financial industry regulatory and compliance","acquisition and retention modeling, chaid decision tree, database architecture, database hygiene, financial industry regulatory and compliance, predictive modeling, regression analysis, sas, spss, sql, value segmentation"
"Day 1 Onsite : Sr. Big Data Engineer : Mason, OH , Atlanta, GA and Richmond, VA",SPAR Information Systems LLC,"Mason, OH",https://www.linkedin.com/jobs/view/day-1-onsite-sr-big-data-engineer-mason-oh-atlanta-ga-and-richmond-va-at-spar-information-systems-llc-3667477764,2023-12-17,Park Forest,United States,Mid senior,Onsite,"Senior Big Data Engineer
Job Location:- Mason, OH , Atlanta, GA and Richmond, VA (Day 1 Onsite)
Duration: Long Term
Mandatory Requirements
8+ yrs IT experience and good expertise in SDLC/Agile
8+ yrs in programing language (Python, Scala, Spark, SQL)
Hands on experience in writing advanced SQL queries
Experience in analyzing data using 'Big-Data' platform
Strong Analytical skills in relating multiple data sets and identify patterns
Tools: - Big-Data, Spark, Python, Scala. SQL
Experience in NoSQL databases like MongoDB and Cassandra
3+ yrs in Healthcare IT projects
Description
Client is looking for a Big Data Engineer to work with one of the leading healthcare providers in US. The ideal candidate may possess good background on Healthcare Business
Responsibilities
As a Data Engineer/ Tech Lead, you will
Write system code, end to end unit test and documentation.
Lead/mentor other developers.
Work with architects and other leads to design solutions that articulate the business context, conceptual design and component-level logical design.
Ensure development is in compliance with overall architecture vision for the platform and ensures specific components are appropriately designed and leveraged
Understand the construction of platform architecture components
Participate in design activities and own the development of the work assigned.
Work closely with QA and integration team to resolve issues.
Able to understand the technology roadmap and delivers cost effectiveness, business value, and competitiveness
Thanks,
Lokesh Kumar
Lokeshk@sparinfosys.com
Show more
Show less","Python, Scala, Spark, SQL, NoSQL, MongoDB, Cassandra, Big Data, Agile, SDLC, Healthcare IT","python, scala, spark, sql, nosql, mongodb, cassandra, big data, agile, sdlc, healthcare it","agile, big data, cassandra, healthcare it, mongodb, nosql, python, scala, sdlc, spark, sql"
Sr Data Analyst,Cushman & Wakefield,"Erlanger, KY",https://www.linkedin.com/jobs/view/sr-data-analyst-at-cushman-wakefield-3779630513,2023-12-17,Park Forest,United States,Mid senior,Onsite,"Job Title
Sr Data Analyst
Job Description Summary
The role is for a Senior Data Analyst as part of a team collecting and analyzing data supporting ad hoc and strategic client projects primarily involving architectural building design.
The candidate will provide advanced expertise in data analysis, collaborate with key client partners, support the data team with identifying project requirements, refining project work, and providing recommendations on optimizing data management and workflows.
Job Description
Core Responsibilities
Collect, clean, study, transform, load, and visualize data for ad hoc and strategic projects
Identify trends and provide insights from data that contribute to solving business problems
Code programs, as needed, to help capture and organize relevant data
Collaborate directly with internal and external partners to satisfy project needs
Clearly communicate useful information to business partners derived from data analysis
Assist in managing completion of team data analysis tasks
Lead problem-solving and refinement of project activities and tasks
Lead project identifying requirements from analysis of current state versus desired future state
Identify opportunities for workflow optimization
Qualifications
Three or more years of experience in data analytics, data management, or related roles
Advanced knowledge of data analytics, cleaning, preparation, and visualization techniques
Advanced experience with data analytics tools and programs (Microsoft Excel, Microsoft Power BI, Python, SQL, Tableau)
Advanced understanding of best practices in data management and visualization
Strong critical thinking and problem solving skills
Strong focus on solutions serving client/end user
Ability to write and speak clearly to both technical and non-technical audiences
Keen attention to both technical detail and quality of work acceptable to client/end user
Demonstrated ability to collaborate effectively with partners across multiple teams
Strong ability to prioritize work tasks in alignment with changing project and team needs
Preferred candidate will have experience managing/analyzing architectural design data
Cushman & Wakefield provides equal employment opportunity. Discrimination of any type will not be tolerated. Cushman & Wakefield is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veteran status or any other characteristic protected by state, federal, or local law.
In compliance with the Americans with Disabilities Act Amendments Act (ADAAA), if you have a disability and would like to request an accommodation in order to apply for a position at Cushman & Wakefield, please call the ADA line at
1-888-365-5406
or email
HRServices@cushwake.com
. Please refer to the job title and job location when you contact us.
Show more
Show less","Data analytics, Data management, Data visualization, Data cleaning, Data preparation, Microsoft Excel, Microsoft Power BI, Python, SQL, Tableau, Critical thinking, Problem solving, Client focus, Communication, Attention to detail, Collaboration, Prioritization, Data warehousing, Data mining, Machine learning, Artificial intelligence, Data governance, Data architecture, Data quality","data analytics, data management, data visualization, data cleaning, data preparation, microsoft excel, microsoft power bi, python, sql, tableau, critical thinking, problem solving, client focus, communication, attention to detail, collaboration, prioritization, data warehousing, data mining, machine learning, artificial intelligence, data governance, data architecture, data quality","artificial intelligence, attention to detail, client focus, collaboration, communication, critical thinking, data architecture, data cleaning, data governance, data management, data mining, data preparation, data quality, dataanalytics, datawarehouse, machine learning, microsoft excel, microsoft power bi, prioritization, problem solving, python, sql, tableau, visualization"
Sr. Investment Data Analyst,"Fort Washington Investment Advisors, Inc.","Cincinnati, OH",https://www.linkedin.com/jobs/view/sr-investment-data-analyst-at-fort-washington-investment-advisors-inc-3756662479,2023-12-17,Park Forest,United States,Mid senior,Onsite,"Welcome to Western & Southern Financial Group – Our focus on creating value for our customers through everything we do is foundational to who we are. We provide meaningful and challenging work for our associates, a key facet of our incredible culture that helps make Western & Southern a career destination.
More than 130 years of operating history;
Born as a modest, door-to-door life insurance business and built to last forever
Ranked 372 on Fortune 500
Award winning Talent Development Team
Top 10 for Top Spouse Employer and Military Friendly Employer in 2023
We consider Cincinnati the best place to live, work and celebrate life – that is why we are committed to doing good for our community
Our Culture
We are building our company to thrive and provide value to our customers, business partners and associates over the long term. To accomplish this, we look for professionals who embody our culture of integrity, top-notch performance, collaboration and teamwork. We are committed to hiring and developing associates who are driven to excel, have a strong work ethic and use astute, fact-based and ethical judgment in decision-making.
A Day in the Life –
The Senior Data Analyst is responsible for developing business solutions for new and/or existing data with an objective to streamline processes and ensuring quality. This position requires the ability to convert data into insights that lead to informed business decisions and working directly with stakeholders to understand their needs. Communication is key for translating requirements to IT but also for being able to listen to the business and understand the root cause of a problem, and to begin determining the appropriate solutions. Analyzes large sets of data for gaps or inefficiencies and is able to create an actionable plan to develop governance and transparency around an issue. Is able to recognize processes that require additional governance and works with management to ensure a framework is identified and implemented. Oversees data mastering activities and the development of workflows, user interfaces and exceptions. Partners with the business to roll out new data processes and provides any documentation and training required.
Key Skills
Visualization Tools
SQL Server Knowledge
Detail Oriented
‘Premier Benefits to Support YOU’
Comprehensive benefits including medical, dental, vision, prescription, pet insurance, life insurance, disability, Fertility, maternity/caregiver & parental leave, and more!
· Retirement solution with our 401(k) savings AND pension plan!
Paid time off that increases with tenure.
Tuition Reimbursement
Subsidized affordable parking
On-site Cafeteria and fitness center (including sports leagues)
On-site Registered Nurse
Career Coaching/Mentoring
Individualized development plans and career pathing
Learn more about who we are visit; www.westernsouthern.com/careers/home-office-careers.
Show more
Show less","Data Analysis, SQL, Data Visualization, Problemsolving, Communication, Business Intelligence, Governance, Data Warehousing, Workflow Management, User Interface Design","data analysis, sql, data visualization, problemsolving, communication, business intelligence, governance, data warehousing, workflow management, user interface design","business intelligence, communication, dataanalytics, datawarehouse, governance, problemsolving, sql, user interface design, visualization, workflow management"
Sr. BigData Developer (Onsite),SPAR Information Systems LLC,"Mason, OH",https://www.linkedin.com/jobs/view/sr-bigdata-developer-onsite-at-spar-information-systems-llc-3667472691,2023-12-17,Park Forest,United States,Mid senior,Onsite,"Hello,
Greeting for the day.
Please go through the requirement and share me
some suitable
resume with
contact details.
Email is the best way to reach me.
Senior Big Data Engineer
Job Location:- Mason, OH , Atlanta, GA and Richmond, VA Onsite from Day 1.
Open Position:- 1
Duration: Long Term
Description
Client is looking for a Big Data Engineer to work with one of the leading healthcare providers in US. The ideal candidate may possess good background on Healthcare Business
Responsibilities
As a Data Engineer/ Tech Lead, you will
Write system code, end to end unit test and documentation.
Lead/mentor other developers.
Work with architects and other leads to design solutions that articulate the business context, conceptual design and component-level logical design.
Ensure development is in compliance with overall architecture vision for the platform and ensures specific components are appropriately designed and leveraged
Understand the construction of platform architecture components
Participate in design activities and own the development of the work assigned.
Work closely with QA and integration team to resolve issues.
Able to understand the technology roadmap and delivers cost effectiveness, business value, and competitiveness
Mandatory Requirements
8+ yrs IT experience and good expertise in SDLC/Agile
8+ yrs in programing language (Python, Scala, Spark, SQL)
Hands on experience in writing advanced SQL queries
Experience in analyzing data using 'Big-Data' platform
Strong Analytical skills in relating multiple data sets and identify patterns
Tools: - Big-Data, Spark, Python, Scala. SQL
Experience in NoSQL databases like MongoDB and Cassandra
3+ yrs in Healthcare IT projects
Thanks,
Lokesh Kumar
469-829-4894
Show more
Show less","Big Data, Spark, Python, Scala, SQL, NoSQL, MongoDB, Cassandra, Agile, SDLC","big data, spark, python, scala, sql, nosql, mongodb, cassandra, agile, sdlc","agile, big data, cassandra, mongodb, nosql, python, scala, sdlc, spark, sql"
Staff Data Engineer,Recruiting from Scratch,"Cincinnati, OH",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744392649,2023-12-17,Park Forest,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Compliance, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data compliance, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data compliance, data engineering, data retention, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Health Outcomes Data Analyst,Elevance Health,"Cincinnati, OH",https://www.linkedin.com/jobs/view/health-outcomes-data-analyst-at-elevance-health-3777084773,2023-12-17,Park Forest,United States,Mid senior,Onsite,"Description
Job Title
: Health Outcomes Data Analyst
Location
: This position will work a hybrid model (remote and office). The ideal candidate will live within 50 miles of one of our Elevance Health PulsePoint locations. Norfolk, VA; Atlanta, GA; Indianapolis, IN; Cincinnati, OH; and St Louis, MO.
The Health Outcomes Data Analyst is responsible for analyzing, reporting and developing recommendations on data related to multiple, varied business metrics.
How You Will Make An Impact
Analyzes data and summarizes performance using summary statistical procedures.
Creates data models to track trends in cost and quality.
Develops and analyzes business performance reports (e.g. for claims data, provider data, utilization data) and provides notations of performance deviations and anomalies.
Creates and publishes periodic reports, makes necessary recommendations, and develops ad hoc reports as needed.
Present initiates and findings to leadership.
May require taking business issue and devising best way to develop appropriate diagnostic and/or tracking data that will translate business requirements into usable decision support tools.
Creates and maintains databases to track business performance.
Minimum Requirements
Requires a BS/BA degree in related field and a minimum of 2 years related operational and/or data analysis experience, experience in database structures, and standard query and reporting tools; or any combination of education and experience which would provide an equivalent background.
Preferred Skills, Capabilities, And Experiences
SQL, Excel, and PowerPoint skillsets strongly preferred.
Tableau or Power BI skills preferred.
Python or R skills preferred.
Cost analysis, predictive analysis, trend analysis experience preferred.
Preferred education backgrounds: Actuarial Science, Data Science, Mathematics, Statistics, Economics, Healthcare Economics, Healthcare Informatics, Finance, HealthCare Finance.
If this job is assigned to any Government Business Division entity, the applicant and incumbent fall under a 'sensitive' work designation and may be subject to additional requirements beyond those associates outside Government Business Divisions. Requirements include but are not limited to more stringent and frequent background checks and/or government clearances, segregation of duties principles, role specific training, monitoring of daily job functions, and sensitive data handling instructions. Associates in these jobs must follow the specific policies, procedures, guidelines, etc. as stated by the Government Business Division in which they are employed.
Please be advised that Elevance Health only accepts resumes for compensation from agencies that have a signed agreement with Elevance Health. Any unsolicited resumes, including those submitted to hiring managers, are deemed to be the property of Elevance Health.
Who We Are
Elevance Health is a health company dedicated to improving lives and communities – and making healthcare simpler. We are a Fortune 25 company with a longstanding history in the healthcare industry, looking for leaders at all levels of the organization who are passionate about making an impact on our members and the communities we serve.
How We Work
At Elevance Health, we are creating a culture that is designed to advance our strategy but will also lead to personal and professional growth for our associates. Our values and behaviors are the root of our culture. They are how we achieve our strategy, power our business outcomes and drive our shared success - for our consumers, our associates, our communities and our business.
We offer a range of market-competitive total rewards that include merit increases, paid holidays, Paid Time Off, and incentive bonus programs (unless covered by a collective bargaining agreement), medical, dental, vision, short and long term disability benefits, 401(k) +match, stock purchase plan, life insurance, wellness programs and financial education resources, to name a few.
Elevance Health operates in a Hybrid Workforce Strategy. Unless specified as primarily virtual by the hiring manager, associates are required to work at an Elevance Health location at least once per week, and potentially several times per week. Specific requirements and expectations for time onsite will be discussed as part of the hiring process. Candidates must reside within 50 miles or 1-hour commute each way of a relevant Elevance Health location.
The health of our associates and communities is a top priority for Elevance Health. We require all new candidates in certain patient/member-facing roles to become vaccinated against COVID-19. If you are not vaccinated, your offer will be rescinded unless you provide an acceptable explanation. Elevance Health will also follow all relevant federal, state and local laws.
Elevance Health is an Equal Employment Opportunity employer and all qualified applicants will receive consideration for employment without regard to age, citizenship status, color, creed, disability, ethnicity, genetic information, gender (including gender identity and gender expression), marital status, national origin, race, religion, sex, sexual orientation, veteran status or any other status or condition protected by applicable federal, state, or local laws. Applicants who require accommodation to participate in the job application process may contact elevancehealthjobssupport@elevancehealth.com for assistance.
Show more
Show less","SQL, Excel, PowerPoint, Tableau, Power BI, Python, R, Cost analysis, Predictive analysis, Trend analysis, Actuarial Science, Data Science, Mathematics, Statistics, Economics, Healthcare Economics, Healthcare Informatics, Finance, HealthCare Finance","sql, excel, powerpoint, tableau, power bi, python, r, cost analysis, predictive analysis, trend analysis, actuarial science, data science, mathematics, statistics, economics, healthcare economics, healthcare informatics, finance, healthcare finance","actuarial science, cost analysis, data science, economics, excel, finance, healthcare economics, healthcare finance, healthcare informatics, mathematics, powerbi, powerpoint, predictive analysis, python, r, sql, statistics, tableau, trend analysis"
Data Integration Engineer,The Intersect Group,Cincinnati Metropolitan Area,https://www.linkedin.com/jobs/view/data-integration-engineer-at-the-intersect-group-3769014945,2023-12-17,Park Forest,United States,Mid senior,Remote,"Job Title: Data Integration Engineer
Location: Remote
Job Type: 12 Month Contract
Job Summary:
Act as an Essential part of the software engineering team that is responsible for building, maintaining, and optimizing data pipelines to support an AI/ML based parts forecasting product called Brilliant Pricing
.
Collaborate with data scientists to understand data requirements.
Monitoring application processes and proactively resolve potential issues.
Troubleshooting data related issues and fixing the code bugs.
Manage AWS Services like Lambda and Step functions using Cloud Formation and AWS CI/CD tools.
Writing reusable, testable, and efficient code and familiarity with event-driven programming in Python.
Design, develop, and maintain data integration pipelines using Python and AWS Cloud Services.
Requirements:
In-depth knowledge and hands-on experience with AWS Cloud Services
Strong Experience and proficiency in Python programming
Senior Level Experience (4+ Years) in Data Engineering/Data Integration- Proven experience in designing, developing, and maintaining robust data integration pipelines
Show more
Show less","Data Integration, AWS Cloud Services, Python, Cloud Formation, AWS CI/CD tools, Eventdriven programming, Data pipelines, Data Engineering","data integration, aws cloud services, python, cloud formation, aws cicd tools, eventdriven programming, data pipelines, data engineering","aws cicd tools, aws cloud services, cloud formation, data engineering, data integration, datapipeline, eventdriven programming, python"
Lead Data Engineer _ Remote,Ekodus INC.,"Cincinnati, OH",https://www.linkedin.com/jobs/view/lead-data-engineer-remote-at-ekodus-inc-3730306480,2023-12-17,Park Forest,United States,Mid senior,Remote,"Title: Lead Data Engineer
Location: Cincinnati,OH
Duration: 6 Months
The Data Engineer will be part of our Debit Modernization workstream joining our one of our Debit squads and assisting with updating how the bank ingests and publishes our Debit product data as part of our larger platform modernization. The individual will be responsible for building mechanisms to receive data from an external partner (file batch and real time events via kafka) and route to our downstream teams for reporting and operational processing.
General Function
Designs and implements software & support solutions as a member of an agile squad. Software in scope are 3rd party software applications which support finance and regulatory reporting business functions. Technical support functions include ETL, file transfer services, job scheduling, multiple environment support, data quality, upgrades, incident & problem resolution, etc. Being assigned to an agile squad means this role also participates in all agile ceremonies driving activities from design to delivery. Follows best practices and standards and participates in communities of practice to continuously refine and document these standards, following any required compliance & governance requirements.
Responsible and accountable for risk by openly exchanging ideas and opinions, elevating concerns, and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues, and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite, achieves results by consistently identifying, assessing, managing, monitoring, and reporting risks of all types.
Essential Duties And Responsibilities
Provide technical knowledge, leadership and collaboration as an ETL developer & designer
Develop and maintain data transfer jobs & schedulers, as required, including to identify & execute opportunities to automate
Achieve operational excellence by automating processes and writing maintainable, supportable, and testable code
Assist with problem resolution for end users and customers, including incident & problem management
Support both application and environment upgrades, new implementations & patches, as required.
Develop software meeting code quality standards and metrics
Participate in communities of practice by contributing to and following standards, test driven development, reviewing others code, and sharing knowledge
Maintain effective partnerships with operations and engineering teams to drive service improvement
Remain current on IT trends pertaining to their area of practice
Contribute to the definition of operational procedures for software development
Maintain appropriate controls and documentation to ensure compliance of audit requirements
Minimum Knowledge, Skills And Abilities Required
Bachelor's degree in Computer Science/Information Systems
Understanding of Object-Oriented Programming Languages
Understanding of Software Development Lifecyle
Strong SQL Skills with ability to perform ETL
Familiarity with relational database architecture techniques like EDW, Postgrese, etc
Demonstrated practice for scripting languages, like Python, Java, Powershell
Strong Windows Server Experience and/or Application Support Experience
Prior experience with Alation, Snowflake, NIFI/KAFKA, PowerBI, Tableau, & Git is a plus
Understanding of Agile Software Development methodologies
Understanding of data management and info security best practices
Demonstrated problem solving skills
Demonstrated collaboration skills
Excellent verbal and written communication skills
Prior experience working for a financial institution or with Debit Cards a plus
Technical Skills
Must Have
Ability to write complex procedures/views/SQL
Apache Kafka
Apache NIFI & SQL
Cloud Data Warehousing - Snowflake preferred
DataStage, SQL, understanding of dimensionally-designed databases, and experience interacting with a relational database (DB2, Oracle, etc.). Mainframe experience would be a bonus, but not required. Ability to analyze complex data flows and understand existing jobs.
Please share your resume to career@ekodusinc.com
Show more
Show less","SQL, Java, Python, Powershell, Windows Server, Alation, Snowflake, NIFI/KAFKA, PowerBI, Tableau, Git, Apache Kafka, Apache NIFI, Cloud Data Warehousing, DataStage, Dimensionallydesigned databases, DB2, Oracle, Mainframe","sql, java, python, powershell, windows server, alation, snowflake, nifikafka, powerbi, tableau, git, apache kafka, apache nifi, cloud data warehousing, datastage, dimensionallydesigned databases, db2, oracle, mainframe","alation, apache kafka, apache nifi, cloud data warehousing, datastage, db2, dimensionallydesigned databases, git, java, mainframe, nifikafka, oracle, powerbi, powershell, python, snowflake, sql, tableau, windows server"
Senior Data Engineer,Red Frog Solutions,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-at-red-frog-solutions-3776293106,2023-12-17,Park Forest,United States,Mid senior,Remote,"Data Engineer
Cincinnati, OH - REMOTE
Full Time Perm
$120K - $150K
Only applicants able to obtain a US Government Security Clearance (Requires US Citizenship) need apply***
Established consulting company with deep expertise in cutting-edge technologies for big data, advanced analytics, modern web application frameworks, and cloud computing, are looking for a Data Engineer to join their growing team of IT professionals.
As a Data Engineer, you will work hands-on with challenging data engineering, data management, and analytics projects. You will collaborate with data scientists, analysts, business users, and IT teams to design, implement, and deploy data services and analytics.
Requirements:
10+ years of experience as a Data Engineer
Must have or be willing to obtain Secret Clearance (this requires US Citizenship)
Strong SQL skills in multiple database platforms
Experience with Snowflake, Databricks, Spark SQL, PySpark, and Python
Cloud experience: Azure, AWS, or GCP
Develop and maintain ETL pipelines.
Database design and principles
Data modeling, schema development, and data-centric documentation
Experience integrating data from a variety of data source types.
Recommend and advise on optimal data models for data ingestion, integration, and visualization.
Experience improving code performance and query optimization.
Use Continuous Integration/Continuous Delivery (CI/CD) concepts to engineer a standardized data environment.
Show more
Show less","Data Engineering, Data Management, Analytics, SQL, Snowflake, Databricks, Spark SQL, PySpark, Python, Azure, AWS, GCP, ETL Pipelines, Database Design, Data Modeling, Schema Development, DataCentric Documentation, Data Integration, Data Visualization, Code Performance, Query Optimization, Continuous Integration, Continuous Delivery","data engineering, data management, analytics, sql, snowflake, databricks, spark sql, pyspark, python, azure, aws, gcp, etl pipelines, database design, data modeling, schema development, datacentric documentation, data integration, data visualization, code performance, query optimization, continuous integration, continuous delivery","analytics, aws, azure, code performance, continuous delivery, continuous integration, data engineering, data integration, data management, database design, databricks, datacentric documentation, datamodeling, etl pipelines, gcp, python, query optimization, schema development, snowflake, spark, spark sql, sql, visualization"
Remote:::Data Engineers exp with implementing data solutions in Azure,IVY TECH SOLUTIONS INC,"Cincinnati, OH",https://www.linkedin.com/jobs/view/remote-data-engineers-exp-with-implementing-data-solutions-in-azure-at-ivy-tech-solutions-inc-3787774571,2023-12-17,Park Forest,United States,Mid senior,Remote,"We are looking for Data Engineers experienced in implementing data solutions in Azure. The Data Engineer will analyze, design and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise. The Data Engineer will also support the implementation of Infrastructure as Code (IaC) by working with teams to help engineer scalable, reliable, and resilient software running in the cloud.
Skills:
5+ years of hands-on experience with data platforms
Experience in designing data solutions in Azure including data distributions and partitions, scalability, disaster recovery and high availability
Experience in monitoring and optimizing data solutions in Azure including using Azure Monitor
Experience in implementing data solutions in Azure including Azure SQL, Azure Synapse, Cosmos DB, Databricks, ADLS, Blob Storage, ADF, Azure Stream Analytics
Experience in designing security for data policies and standards
In depth understanding and proficiency in automation of cloud platforms and data platforms
Expertise in on-prem and cloud database automation and platform automation with Azure
Proficiency with cloud automation tooling such as Ansible and Terraform
Proficiency with DevOps and CI/CD methodologies and tools for automated infrastructure code test, integration, deployment, and assurance
Proficiency with Languages such as Ruby, bash, Python or Go
Experience with Software Development and automation methodologies
Experience with data security best practices
Strong problem-solving skills
Strong collaboration skills and excellent verbal and written communication skills
Powered by JazzHR
4z9Bqs3s9H
Show more
Show less","Azure, Data Solutions, Data Platforms, Data Distribution, Data Partitions, Scalability, Disaster Recovery, High Availability, Data Monitoring, Data Optimization, Azure Monitor, Azure SQL, Azure Synapse, Cosmos DB, Databricks, ADLS, Blob Storage, ADF, Azure Stream Analytics, Data Security, Data Policies, Data Standards, Cloud Platform Automation, Data Platform Automation, Onprem Database Automation, Cloud Database Automation, Platform Automation, Ansible, Terraform, DevOps, CI/CD, Infrastructure Code Testing, Integration, Deployment, Assurance, Ruby, Bash, Python, Go, Software Development, Automation Methodologies, Data Security Best Practices, ProblemSolving Skills, Collaboration Skills, Verbal Communication Skills, Written Communication Skills","azure, data solutions, data platforms, data distribution, data partitions, scalability, disaster recovery, high availability, data monitoring, data optimization, azure monitor, azure sql, azure synapse, cosmos db, databricks, adls, blob storage, adf, azure stream analytics, data security, data policies, data standards, cloud platform automation, data platform automation, onprem database automation, cloud database automation, platform automation, ansible, terraform, devops, cicd, infrastructure code testing, integration, deployment, assurance, ruby, bash, python, go, software development, automation methodologies, data security best practices, problemsolving skills, collaboration skills, verbal communication skills, written communication skills","adf, adls, ansible, assurance, automation methodologies, azure, azure monitor, azure sql, azure stream analytics, azure synapse, bash, blob storage, cicd, cloud database automation, cloud platform automation, collaboration skills, cosmos db, data distribution, data monitoring, data optimization, data partitions, data platform automation, data platforms, data policies, data security, data security best practices, data solutions, data standards, databricks, deployment, devops, disaster recovery, go, high availability, infrastructure code testing, integration, onprem database automation, platform automation, problemsolving skills, python, ruby, scalability, software development, terraform, verbal communication skills, written communication skills"
"Data Conversion Developer, Senior Associate",PwC,"Cincinnati, OH",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749936649,2023-12-17,Park Forest,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Azure Data Engineer Associate, Databricks Certified Data Engineer Associate, Maximo, PowerPlant, IBM DB2, Oracle, Microsoft SQL Server, Maximo's Integration Framework (MIF), Azure ADF, AWS Glue, SSIS, DataBricks, SOAP, RESTful APIs, XML, JSON, SQL, Python, PySpark, Scala, Automation Scripts, Java Customizations, Database Configuration, Application Designer, ERP systems, GIS systems, Data Engineer, Data Architect","azure data engineer associate, databricks certified data engineer associate, maximo, powerplant, ibm db2, oracle, microsoft sql server, maximos integration framework mif, azure adf, aws glue, ssis, databricks, soap, restful apis, xml, json, sql, python, pyspark, scala, automation scripts, java customizations, database configuration, application designer, erp systems, gis systems, data engineer, data architect","application designer, automation scripts, aws glue, azure adf, azure data engineer associate, data architect, database configuration, databricks, databricks certified data engineer associate, dataengineering, erp systems, gis systems, ibm db2, java customizations, json, maximo, maximos integration framework mif, microsoft sql server, oracle, powerplant, python, restful apis, scala, soap, spark, sql, ssis, xml"
Data Engineer - Alation,Burtch Works,"Cincinnati, OH",https://www.linkedin.com/jobs/view/data-engineer-alation-at-burtch-works-3770152453,2023-12-17,Park Forest,United States,Mid senior,Hybrid,"**This role will require 2-3 days a week in-office in one of the following cities; Cincinnati, Chicago, Charlotte, Boca Raton, San Jose, or Portland**
As part of a dynamic Agile and Scrum team the Advance Data Engineer for the Data Discoverability team will play a pivotal role in ensuring the ongoing success and integrity of our data catalog platform. You will collaborate with a team of experts to maintain data source configurations, implement security measures, and resolve operational and data quality issues. You will contribute and be responsible to the technical curation of our Data Catalog Roadmap and ensure the optimal representation of our data landscape within the catalog. You will assist data source and administrative experts maintaining data source specific configurations including permissions, user provisioning, log ingestion, API integrations, dictionary uploads as well as resolve operational and data quality issues.
Minimum Qualifications
2+ years of direct experience in managing and curating data catalogs, with a proven track record of technical expertise.
2+ years of experience working closely with data stewardship principles and practices, demonstrating a strong understanding of data governance and stewardship concepts.
Experience with Cloud Platforms, including Azure Data Lake Storage (ADLS), Unity Catalog, Databricks, and Azure Synapse, with a strong understanding of metadata management, security principles, and fundamental cloud concepts
Expertise with Python and SQL for querying and managing databases for data storage and retrieval.
Strong experience in network administration, with a solid foundation in managing data access and security in a networked environment.
Strong previous experience working with Alation or any other leading data catalog platform, showcasing proficiency in configuring and customizing such tools.
Proficiency integrating data catalog processes, customizing configurations, implementing connectors, and creating data processing scripts.
Familiarity with other data catalog solutions in addition to Alation, highlighting versatility and adaptability in managing various cataloging technologies.
Support, maintain, and document software functionality, best practices, standards, and processes.
Demonstrable critical thinking.
Experience with an E-commerce or multi-channel retail environment.
Excellent verbal and written communication skills.
Be passionate about Metadata and Databases!
Nice to Haves
Relevant certifications in data management, data governance, or related fields are a plus.
Knowledge with Databricks or Jupyter Notebooks for data engineering, data preparation, collaborative data analytics, data analysis and documentation a plus.
Familiarity with Tableau, Power BI or other data visualization and reporting tools a plus.
Key Responsibilities
Collaborate with cross-functional teams to contribute to and validate the technical aspects of the Data Catalog Roadmap, ensuring alignment with organizational goals and objectives.
Configure and customize the data catalog platform, such as Alation, to accurately represent our organization's data landscape, making it a valuable resource for data users.
Implement and maintain permissions in Alation, ensuring that the Data Catalog is used in accordance with data access policies and security protocols.
Take ownership of data source-specific configurations in Alation, and delegate specific actions to Data Source Administrators, overseeing their work to maintain consistency and accuracy.
Set up and manage authentication mechanisms, user accounts, groups, and other relevant security components to safeguard data access and maintain compliance.
Write, maintain, and execute Security Plans, IT Playbooks, Support Plans, and Data Access Policies to ensure the reliable and secure operation of the Data Catalog.
Assign specific tasks and responsibilities to other members of the technical team, ensuring efficient collaboration and workload distribution.
Work closely with the product management team to prioritize data source requirements and project plans, ensuring alignment with organizational objectives.
Collaborate with product management to define operational standards and procedures for the data catalog, promoting consistency and best practices.
Plan and coordinate the ingestion of new or updated Business Intelligence (BI) and data sources, ensuring seamless integration into the catalog.
Work with product management to define the settings and repeatable patterns for configuring new data sources and managing access permissions.
Coordinate with all data management and IT organizations as required, including compliance and governance organizations, to ensure cross-functional alignment.
Collaborate with the Data Discoverability team to discuss the status of data discoverability initiatives and identify both technical and non-technical issues that may arise.
Report on operational service level agreements (SLAs) and support performance, providing transparency and accountability.
Investigate and resolve escalated technical issues, ensuring timely resolution and minimal disruption to data users.
Delegate the investigation and resolution of issues to the technical team and Data Source Administrators as appropriate, fostering a collaborative problem-solving approach.
Participate in IT-related communication planning and execution, ensuring that relevant stakeholders are informed of important developments.
Plan and participate in training sessions to enhance the skills and knowledge of team members and data source administrators.
Recruit, train, and mentor Data Stewards and/or other team members, fostering their professional development and growth within the organization.
Actively participate in program status and planning meetings, contributing valuable insights and expertise.
Show more
Show less","Agile, Scrum, Data Catalog, Data Curation, Data Stewardship, Data Governance, Cloud Platforms, Azure Data Lake Storage (ADLS), Unity Catalog, Databricks, Azure Synapse, Metadata Management, Security Principles, Python, SQL, Network Administration, Data Access, Data Security, Alation, Data Catalog Processes, Data Processing Scripts, Data Visualization, Tableau, Power BI, Data Analytics, Data Analysis, Data Documentation, Data Source Configurations, Authentication Mechanisms, User Accounts, Groups, IT Playbooks, Support Plans, Data Access Policies","agile, scrum, data catalog, data curation, data stewardship, data governance, cloud platforms, azure data lake storage adls, unity catalog, databricks, azure synapse, metadata management, security principles, python, sql, network administration, data access, data security, alation, data catalog processes, data processing scripts, data visualization, tableau, power bi, data analytics, data analysis, data documentation, data source configurations, authentication mechanisms, user accounts, groups, it playbooks, support plans, data access policies","agile, alation, authentication mechanisms, azure data lake storage adls, azure synapse, cloud platforms, data access, data access policies, data catalog, data catalog processes, data curation, data documentation, data governance, data processing scripts, data security, data source configurations, data stewardship, dataanalytics, databricks, groups, it playbooks, metadata management, network administration, powerbi, python, scrum, security principles, sql, support plans, tableau, unity catalog, user accounts, visualization"
"Hybrid Work - Need Lead Data Engineer in Cincinnati, OH",Steneral Consulting,"Cincinnati, OH",https://www.linkedin.com/jobs/view/hybrid-work-need-lead-data-engineer-in-cincinnati-oh-at-steneral-consulting-3645133570,2023-12-17,Park Forest,United States,Mid senior,Hybrid,"2-3 days a week onsite in Cincinnati, OH.
Must be under 45 mins commute
Need photo ID along with submittal and candidates need to take a video screen with prime vendor prior to end client submittal
General Function
Designs and implements software & support solutions as a member of an agile squad. Software in scope are 3rd party software applications which support finance and regulatory reporting business functions. Technical support functions include ETL, file transfer services, job scheduling, multiple environment support, data quality, upgrades, incident & problem resolution, etc. Being assigned to an agile squad means this role also participates in all agile ceremonies driving activities from design to delivery. Follows best practices and standards and participates in communities of practice to continuously refine and document these standards, following any required compliance & governance requirements.
Responsible and accountable for risk by openly exchanging ideas and opinions, elevating concerns, and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite, achieves results by consistently identifying, assessing, managing, monitoring, and reporting risks of all types.
Essential Duties And Responsibilities
Provide technical knowledge, leadership and collaboration as an ETL developer & designer
Develop and maintain data transfer jobs & schedulers, as required, including to identify & execute opportunities to automate
Achieve operational excellence by automating processes and writing maintainable, supportable, and testable code
Assist with problem resolution for end users and customers, including incident & problem management
Support both application and environment upgrades, new implementations & patches, as required.
Develop software meeting code quality standards and metrics
Participate in communities of practice by contributing to and following standards, test driven development, reviewing others code, and sharing knowledge
Maintain effective partnerships with operations and engineering teams to drive service improvement
Remain current on IT trends pertaining to their area of practice
Contribute to the definition of operational procedures for software development
Maintain appropriate controls and documentation to ensure compliance of audit requirements
Minimum Knowledge, Skills And Abilities Required
Bachelor's degree in Computer Science/Information Systems
Understanding of Object-Oriented Programming Languages
Understanding of Software Development Lifecyle
Strong SQL Skills with ability to perform ETL
Familiarity with relational database architecture techniques like EDW, Postgres, etc.
Demonstrated practice for scripting languages, like Python, Java, PowerShell
Strong Windows Server Experience and/or Application Support Experience
Prior experience with Alation, Snowflake, NIFI/KAFKA, PowerBI, Tableau, & Git is a plus
Understanding of Agile Software Development methodologies
Understanding of data management and info security best practices
Demonstrated problem solving skills
Demonstrated collaboration skills
Excellent verbal and written communication skills
Prior experience working for a financial institution or with Debit Cards a plus
Must Haves
Apache Kafka
Ability to write complex procedures/views/SQL
Apache NIFI & SQL
ETL
Scripting languages for automation (Java, Python, PowerShell); Experience working in Data Management framework; Experience in platform engineering activities (platform standup, software upgrades, patching, etc.); and proficiency in admin operating systems (Linux, CLI)
Cloud data warehousing Snowflake preferred
Show more
Show less","Agile Development, Alation, Apache Kafka, Apache NIFI, Cloud Data Warehousing, Data Management, Data Quality, EDW, ETL, Git, Java, Linux, ObjectOriented Programming, Postgres, PowerShell, Python, Snowflake, SQL, Tableau","agile development, alation, apache kafka, apache nifi, cloud data warehousing, data management, data quality, edw, etl, git, java, linux, objectoriented programming, postgres, powershell, python, snowflake, sql, tableau","agile development, alation, apache kafka, apache nifi, cloud data warehousing, data management, data quality, edw, etl, git, java, linux, objectoriented programming, postgres, powershell, python, snowflake, sql, tableau"
"Hybrid Onsite work opportunity - Need Lead Data Engineer in Cincinnati, OH",Steneral Consulting,"Cincinnati, OH",https://www.linkedin.com/jobs/view/hybrid-onsite-work-opportunity-need-lead-data-engineer-in-cincinnati-oh-at-steneral-consulting-3645639831,2023-12-17,Park Forest,United States,Mid senior,Hybrid,"Hi,
Please find the requirement below , If you find yourself comfortable with the requirement please reply with your updated resume and I will get back to you or I would really appreciate if you can give me a call back at my contact number
302-918-2347
Job Title:-
Lead Data Engineer
Job location:- Hybrid Onsite- Cincinnati, Ohio
Duration: 6+ Months Contract
Work Authorization:- All Visa
Interview Mode:- Video
Job Description
2-3 days a week onsite in Cincinnati, OH
Must be under 45 mins commute
Need photo ID along with submittal and candidates need to take a video screen with prime vendor prior to end client submittal
General Function
Designs and implements software & support solutions as a member of an agile squad. Software in scope are 3rd party software applications which support finance and regulatory reporting business functions. Technical support functions include ETL, file transfer services, job scheduling, multiple environment support, data quality, upgrades, incident & problem resolution, etc. Being assigned to an agile squad means this role also participates in all agile ceremonies driving activities from design to delivery. Follows best practices and standards and participates in communities of practice to continuously refine and document these standards, following any required compliance & governance requirements.
Responsible and accountable for risk by openly exchanging ideas and opinions, elevating concerns, and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite, achieves results by consistently identifying, assessing, managing, monitoring, and reporting risks of all types.
Essential Duties And Responsibilities
Provide technical knowledge, leadership and collaboration as an ETL developer & designer
Develop and maintain data transfer jobs & schedulers, as required, including to identify & execute opportunities to automate
Achieve operational excellence by automating processes and writing maintainable, supportable, and testable code
Assist with problem resolution for end users and customers, including incident & problem management
Support both application and environment upgrades, new implementations & patches, as required.
Develop software meeting code quality standards and metrics
Participate in communities of practice by contributing to and following standards, test driven development, reviewing others code, and sharing knowledge
Maintain effective partnerships with operations and engineering teams to drive service improvement
Remain current on IT trends pertaining to their area of practice
Contribute to the definition of operational procedures for software development
Maintain appropriate controls and documentation to ensure compliance of audit requirements
Minimum Knowledge, Skills And Abilities Required
Bachelor's degree in Computer Science/Information Systems
Understanding of Object-Oriented Programming Languages
Understanding of Software Development Lifecyle
Strong SQL Skills with ability to perform ETL
Familiarity with relational database architecture techniques like EDW, Postgres, etc.
Demonstrated practice for scripting languages, like Python, Java, PowerShell
Strong Windows Server Experience and/or Application Support Experience
Prior experience with Alation, Snowflake, NIFI/KAFKA, PowerBI, Tableau, & Git is a plus
Understanding of Agile Software Development methodologies
Understanding of data management and info security best practices
Demonstrated problem solving skills
Demonstrated collaboration skills
Excellent verbal and written communication skills
Prior experience working for a financial institution or with Debit Cards a plus
Must Haves
Apache Kafka
Ability to write complex procedures/views/SQL
Apache NIFI & SQL
ETL
Scripting languages for automation (Java, Python, PowerShell); Experience working in Data Management framework; Experience in platform engineering activities (platform standup, software upgrades, patching, etc.); and proficiency in admin operating systems (Linux, CLI)
Cloud data warehousing Snowflake preferred
Vinit Kumar
Sr. Talent Acquisition -North America
Desk:
302-918-2347
vinit@steneral.com
Note- In my absence please reach to Mr. Prince at prince@steneral.com #302-721-5165
Show more
Show less","ETL, Apache Kafka, Apache NIFI, SQL, Python, Java, PowerShell, Relational database architecture, EDW, Postgres, Snowflake, Git, Agile Software Development methodologies, Data management, Info security best practices, Problem solving, Collaboration, Verbal communication, Written communication, Data Management framework, Platform engineering activities, Admin operating systems, Linux, CLI, Cloud data warehousing","etl, apache kafka, apache nifi, sql, python, java, powershell, relational database architecture, edw, postgres, snowflake, git, agile software development methodologies, data management, info security best practices, problem solving, collaboration, verbal communication, written communication, data management framework, platform engineering activities, admin operating systems, linux, cli, cloud data warehousing","admin operating systems, agile software development methodologies, apache kafka, apache nifi, cli, cloud data warehousing, collaboration, data management, data management framework, edw, etl, git, info security best practices, java, linux, platform engineering activities, postgres, powershell, problem solving, python, relational database architecture, snowflake, sql, verbal communication, written communication"
Lead Data Engineer,Stellent IT,"Cincinnati, OH",https://www.linkedin.com/jobs/view/lead-data-engineer-at-stellent-it-3749080172,2023-12-17,Park Forest,United States,Mid senior,Hybrid,"Job Title - Lead Data Engineer
Location: Cincinnati , Ohio (Local candidate only, no relos)
Hybrid - Onsite 2-3 days per week
Interviews: Video
VISA: USC,GC,EAD,H4 (NO H1B)
Job Description
Looking for a highly motivated
Lead Data Engineer
to join our
Regulatory Reporting
space. The squad works directly with data producers, finance & accounting stakeholders to build a trusted Regulatory Reporting eco-system.
Must Have's
Local to Cincinnati with the ability to come to Cincinnati downtown 2-3 days a week.
Data Analytics background
Strong SQL skills - Ability to write complex procedures/views/SQL
Experience with Data Lineage and Data Mapping
Hands-on experience in working with ETL tools (Informatica / Datastage preferred)
Snowflake
Denodo Data Virtualization
Must have experience in working with Large Data Warehouses
Good understanding of Data Modeling and Data Architecture concepts.
Great communication skills
Ability to work in an fast moving and ever evolving data environment.
Past experience in working within an Agile squad/Agile development principles
Good To Have
Python
Data Management experience
Show more
Show less","Data Analytics, SQL, Data Lineage, Data Mapping, Informatica, Datastage, Snowflake, Denodo Data Virtualization, Agile, Python, Data Warehouses, Data Modeling, Data Architecture","data analytics, sql, data lineage, data mapping, informatica, datastage, snowflake, denodo data virtualization, agile, python, data warehouses, data modeling, data architecture","agile, data architecture, data lineage, data mapping, data warehouses, dataanalytics, datamodeling, datastage, denodo data virtualization, informatica, python, snowflake, sql"
SAS (Statistical -Software) Developer (Data Engineer),CBTS,"Cincinnati, OH",https://www.linkedin.com/jobs/view/sas-statistical-software-developer-data-engineer-at-cbts-3751268356,2023-12-17,Park Forest,United States,Mid senior,Hybrid,"6-12 Month Contract/$63 - $68 per hour /Onsite 5 days a week in CIncinnati, OH (45227) /
Must be able to read SAS (Software), Production Support of Base SAS, SAS (Software) to Snowflake Conversion
CBTS is searching for a SAS (Statistical -Software) Developer that has data engineering experience with Snowflake. You will be a part of a team converting SAS (Statistical -Software) jobs to Snowflake. Responsible for production support of existing SAS (Statistical -Software) jobs, trouble shooting, impact analysis and data mapping. Candidates must be able to read existing SAS (Statistical -Software) code and understand data sources, dependencies and impact analysis.
Skillset: SAS
(Statistical -Software)
, problem solving, production support, data mapping
The SAS (Statistical -Software) developer/Data Engineer designs and builds platforms, tools, and solutions that help manage, secure, and generate value from its data. The person in this role creates scalable and reusable solutions for gathering, collecting, storing, processing, and serving data on both small and very large (i.e. Big Data) scales. These solutions can include on-premise and cloud-based data platforms, and solutions in any of the following domains ETL, business intelligence, analytics, persistence (relational, NoSQL, data lakes), search, messaging, data warehousing, stream processing, and machine learning.
Responsible and accountable for risk by openly exchanging ideas and opinions, elevating concerns, and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues, and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite, achieves results by consistently identifying, assessing, managing, monitoring, and reporting risks of all types.
Responsibilities:
Responsible for design, Development, and Support of data solutions, APIs, tools, and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams, Enterprise architecture, infrastructure, information security, and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design, performance, integration, security, etc.
Conduct research and Development based on current trends and technologies related to the banking industry, data engineering and architecture, data security, and related topics.
Work with developers to Build CI/CD pipelines, Self-service Build tools, and automated deployment processes.
Evaluate software products and Provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed, including legacy system replacement, Monitoring and analytics improvements, tool Development, and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
Requirements:
Bachelor's degree in Computer Science/Information Systems or equivalent combination of education and experience.
Must have SAS (Statistical -Software) Development
experience and the ability to convert to
Snowflake.
Must be able to communicate ideas both verbally and in writing to management, business and IT sponsors, and technical resources in language that is appropriate for each group.
At least 6 years of related experience, including at least 4 years in a hands-on software development role.
Significant experience with at least one major RDBMS product.
Experience working with and supporting Unix/Linux and Windows systems.
Proficient in relational database modeling concepts and techniques.
Solid conceptual understanding of distributed computing principles.
Working knowledge of application and data security concepts, best practices, and common vulnerabilities.
Experience in one or more of the following disciplines preferred: big data technologies and distributions, metadata management products, commercial ETL tools, Bi and reporting tools, messaging systems, data warehousing, Java (language and run time environment), major version control systems, continuous integration/delivery tools, infrastructure automation and virtualization tools, major cloud, or rest API design and development.
Financial industry experience is a plus.
Cincinnati Bell Technology Solutions provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a protected veteran in accordance with applicable federal, state and local laws.
Show more
Show less","SAS, Snowflake, Production support, Data mapping, ETL, Business intelligence, Analytics, Data lakes, Search, Messaging, Data warehousing, Stream processing, Machine learning, Data security, Continuous integration/continuous delivery (CI/CD), Unix/Linux, Windows, Relational database modeling, Distributed computing, Application security, Data security, Big data technologies, Metadata management, Commercial ETL tools, BI and reporting tools, Java, Version control systems, Infrastructure automation, Virtualization tools, Cloud computing, REST API design and development","sas, snowflake, production support, data mapping, etl, business intelligence, analytics, data lakes, search, messaging, data warehousing, stream processing, machine learning, data security, continuous integrationcontinuous delivery cicd, unixlinux, windows, relational database modeling, distributed computing, application security, data security, big data technologies, metadata management, commercial etl tools, bi and reporting tools, java, version control systems, infrastructure automation, virtualization tools, cloud computing, rest api design and development","analytics, application security, bi and reporting tools, big data technologies, business intelligence, cloud computing, commercial etl tools, continuous integrationcontinuous delivery cicd, data lakes, data mapping, data security, datawarehouse, distributed computing, etl, infrastructure automation, java, machine learning, messaging, metadata management, production support, relational database modeling, rest api design and development, sas, search, snowflake, stream processing, unixlinux, version control systems, virtualization tools, windows"
Senior Big Data Engineer,SPAR Information Systems LLC,"Mason, OH",https://www.linkedin.com/jobs/view/senior-big-data-engineer-at-spar-information-systems-llc-3667477355,2023-12-17,Park Forest,United States,Mid senior,Hybrid,"Senior Big Data Engineer (10+ Years)
Job Location:- Mason, OH , Atlanta, GA and Richmond, VA ( Day 1 onsite)
Duration: Long Term
Mandatory Requirements:
PBM Area-
8+ yrs IT experience and good expertise in SDLC/Agile
8+ yrs in programing language (Python, Scala, Spark, SQL)
Hands on experience in writing advanced SQL queries
Experience in analyzing data using 'Big-Data' platform
Strong Analytical skills in relating multiple data sets and identify patterns
Tools: - Big-Data, Spark, Python, Scala. SQL
Experience in NoSQL databases like MongoDB and Cassandra
3+ yrs in Healthcare IT projects
Description
Client is looking for a Big Data Engineer to work with one of the leading healthcare providers in US. The ideal candidate may possess good background on Healthcare Business
Responsibilities
As a Data Engineer/ Tech Lead, you will
Write system code, end to end unit test and documentation.
Lead/mentor other developers.
Work with architects and other leads to design solutions that articulate the business context, conceptual design and component-level logical design.
Ensure development is in compliance with overall architecture vision for the platform and ensures specific components are appropriately designed and leveraged
Understand the construction of platform architecture components
Participate in design activities and own the development of the work assigned.
Work closely with QA and integration team to resolve issues.
Able to understand the technology roadmap and delivers cost effectiveness, business value, and competitiveness
Thanks & regards.
Arvind Kumar Bind
||
SPAR Information Systems
|| Phone:
469-750-0607
||
|| Email: Arvind.B@sparinfosys.com || Web: www.sparinfosys.com ||
Show more
Show less","SDLC, Agile, Python, Scala, Spark, SQL, NoSQL, MongoDB, Cassandra, Healthcare IT, Big Data, Data Engineer, Unit Testing, System Code, Documentation, Mentoring, Software Architecture, Design, Platform Architecture, Quality Assurance, Integration, Technology Roadmap, CostEffectiveness, Business Value, Competitiveness","sdlc, agile, python, scala, spark, sql, nosql, mongodb, cassandra, healthcare it, big data, data engineer, unit testing, system code, documentation, mentoring, software architecture, design, platform architecture, quality assurance, integration, technology roadmap, costeffectiveness, business value, competitiveness","agile, big data, business value, cassandra, competitiveness, costeffectiveness, dataengineering, design, documentation, healthcare it, integration, mentoring, mongodb, nosql, platform architecture, python, quality assurance, scala, sdlc, software architecture, spark, sql, system code, technology roadmap, unit testing"
Senior Cloud Data Engineer,BDO USA,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765466997,2023-12-17,Park Forest,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Cloud Computing, Data Warehousing, Data Modeling, SQL, Python, Java, Scala, C#, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, Computer Vision, Tableau, Qlik, Synapse, IoT, SSIS, SSAS, SSRS, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Athena, Data Pipeline, Glue, Star Schema, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL","data analytics, business intelligence, cloud computing, data warehousing, data modeling, sql, python, java, scala, c, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, computer vision, tableau, qlik, synapse, iot, ssis, ssas, ssrs, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql, aws, redshift, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, athena, data pipeline, glue, star schema, ssis, ssas, ssrs, pyspark, microsoft fabric, dbt, linux, terraform, bicep, data ops, purview, git, delta, pandas, spark sql","ai algorithms, athena, automation tools, aws, aws lake formation, azure analysis services, batch data ingestion, bicep, business intelligence, c, cloud computing, computer vision, data lake medallion architecture, data ops, data pipeline, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops, git, glue, iot, java, kinesis, linux, machine learning, microsoft fabric, pandas, powerbi, purview, python, qlik, quicksight, redshift, s3, sagemaker, scala, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema, streaming data ingestion, synapse, tableau, terraform"
Sr. Data Analyst,Creative Circle,"Cincinnati, OH",https://www.linkedin.com/jobs/view/sr-data-analyst-at-creative-circle-3733581563,2023-12-17,Park Forest,United States,Mid senior,Hybrid,"Calling all Sr. Data Analysts who have experience reviewing data and interpreting data for large brands or companies! Our client in the automotive industry is looking for a Sr. Data Analyst to join their team. This is an exciting opportunity to take on a new to the team role!
This is a direct hire role that will be hybrid onsite in Cincinnati two days per week and WFH three days per week.
The Sr. Data Analyst will be responsible for:
Reviewing analytics and reports to interpret data and provide insights for client managers for client reports
Working within their app to review analytics and reporting
Creating PowerPoint presentations with visual data of data interpretations
Regularly gather, cleaning, and interpreting client data, ensuring its precision and relevance.
Creating, maintaining, and refine analytics, producing both scheduled and on-demand reports utilizing tools such as Tableau, Excel, and PowerPoint. Reports should be data-rich, yet easily digestible and actionable
Interpreting key data points into insightful visualizations and models, forecasting both present-day and future trends across clients.
Suggest and implement improvements to refine and update current reporting system
Teaming up with Client Partnership and Business Development Managers to pinpoint reporting requirements, cater to client inquiries, and devise effective strategies driven by analytical insights.
Engaging directly with clients, providing clarity on data interpretations, and imparting actionable insights
Working with development team to update reporting and offerings for clients
The Sr. Data Analyst must have:
Experience analyzing and interpreting data - must be able to pull out actionable insights that are easily digestible for clients and sales teams
Minimum of 7 years of experience in reporting, data analytics, or similar
Experience using tools like Tableau, Excel, and PowerPoint for data visualization and reporting
Experience with or exposure to programming languages for data analysis and processing
Advanced skills in Excel for data analysis, modeling, and visualization.
Experience with Python, particularly libraries related to data analytics and visualization such as Pandas, NumPy, Matplotlib, and Seaborn.
Familiarity with databases and SQL for extracting and managing data.
A knack for identifying trends, patterns, and anomalies in detailed and complex data
Excellent organizational skills
Excellent written and verbal communication capabilities and can explain data interpretations to both technical and non-technical stakeholders
Experience with large name brands reviewing and interpreting data
#IND123
New role for team, ability to create role into what they think the company needs
Fast growing company
Profit sharing (get paid on company performance and goes directly into 401k)
Standard health benefits and 401k match
Submit resume (and samples if applicable) to: Tara.Freihofer@jobalert.creativecircle.com
Creative Circle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity or expression, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, or any other characteristic protected by law. Creative Circle will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you need a reasonable accommodation in the application process, please contact your Recruiter (the person you'll be interviewing with) or a member of our Human Resources team to make arrangements.
Show more
Show less","Data Analytics, Data Interpretation, Tableau, Excel, PowerPoint, Data Visualization, Programming Languages, Python, Pandas, NumPy, Matplotlib, Seaborn, SQL, Databases, Trend Analysis, Organizational Skills, Communication Skills","data analytics, data interpretation, tableau, excel, powerpoint, data visualization, programming languages, python, pandas, numpy, matplotlib, seaborn, sql, databases, trend analysis, organizational skills, communication skills","communication skills, data interpretation, dataanalytics, databases, excel, matplotlib, numpy, organizational skills, pandas, powerpoint, programming languages, python, seaborn, sql, tableau, trend analysis, visualization"
Senior Data Architect / Director of Data Engineering,CommuniCare Health Services,"Blue Ash, OH",https://www.linkedin.com/jobs/view/senior-data-architect-director-of-data-engineering-at-communicare-health-services-3777796038,2023-12-17,Park Forest,United States,Mid senior,Hybrid,"CommuniCare Health Services
is a fast-growing healthcare provider with over
130 facilities located in 7 states
. Our Family of Companies also includes an
integrated medical group, homecare and hospice, managed care plan, ACO, and a pharmacy.
Due to growth we are currently hiring a Sr. Data Architect / Director of Data Engineering to lead the design of data planning and architecture.
As the Director of Data Engineering, you will play a pivotal role in shaping the data strategy. You will lead a team of reporting analysts working in PowerBI, ensuring the effective development and delivery of insightful reports and analytics. The ideal candidate will have a strong background in data engineering, a deep understanding of healthcare data, and a proven track record of managing and inspiring high-performing teams. The position is based at our Corporate office in Blue Ash, Ohio (suburb of Cincinnati) and will be required to be in office when necessary but will also have some remote work opportunity.
Responsibilities
Team Leadership:
Lead, mentor, and inspire a team of reporting analysts
Recruit, retain, and focus strong data engineers (onshore and offshore).
Foster a collaborative and innovative team culture that values continuous learning and professional development.
Drive the team towards achieving key performance indicators and project milestones.
Data Infrastructure
Develop and implement a robust and scalable data infrastructure that supports the company's reporting and analytics needs.
Develop and maintain a technical metadata framework and repository of data events and ETL operations; assist with systems design
Oversee the design, development, and maintenance of data pipelines, ensuring data quality, integrity, and availability.
Design, model and develop data sets to support reporting and analytics in a cloud environment;
PowerBI Expertise
Lead and administer analytics tools
Leverage expertise in PowerBI to create compelling and actionable visualizations.
Work closely with reporting analysts to design, develop, and optimize PowerBI reports and dashboards.
Stay current with industry best practices and emerging trends in PowerBI and data visualization.
Strategic Planning
Collaborate with senior leadership to define and execute the data strategy for the organization.
Identify opportunities to enhance data-driven decision-making across the company.
Provide strategic guidance on technology and tools to support long-term business objectives.
Quality Assurance
Implement and enforce data governance and quality standards.
Conduct regular audits to ensure data accuracy and compliance with industry regulations.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
8+ Years as a Data engineer
Experience with AWS/Cloud services;
Expert understanding of data structures, algorithms, and effective software design;
Excellent written and verbal communication skills;
5+ Years using SQL/T-SQL;
Experience working with structured and unstructured data and data warehouses;
Proven experience in a leadership role overseeing data engineering teams.
Strong expertise in PowerBI and other relevant data visualization tools.
In-depth knowledge of healthcare data, including electronic health records and post-acute care data.
Experience with data modeling, ETL processes, and data warehouse architecture.
Excellent communication and interpersonal skills, with the ability to communicate complex technical concepts to non-technical stakeholders.
Strong analytical and problem-solving skills.
Understanding of scalable systems and you have large-scale engineering experience
Show more
Show less","Data Engineering, PowerBI, SQL, TSQL, AWS, Data Warehousing, Data Modeling, ETL, Data Governance, Cloud Services, Data Structures, Algorithms, Software Design, Data Visualization, Healthcare Data, Electronic Health Records, PostAcute Care Data, LargeScale Engineering, Scalable Systems, Leadership, Mentoring, Team Building, Communication, ProblemSolving, DecisionMaking","data engineering, powerbi, sql, tsql, aws, data warehousing, data modeling, etl, data governance, cloud services, data structures, algorithms, software design, data visualization, healthcare data, electronic health records, postacute care data, largescale engineering, scalable systems, leadership, mentoring, team building, communication, problemsolving, decisionmaking","algorithms, aws, cloud services, communication, data engineering, data governance, data structures, datamodeling, datawarehouse, decisionmaking, electronic health records, etl, healthcare data, largescale engineering, leadership, mentoring, postacute care data, powerbi, problemsolving, scalable systems, software design, sql, team building, tsql, visualization"
"Staff Industrial Engineer, Data Analytics",Rivian,"Normal, IL",https://www.linkedin.com/jobs/view/staff-industrial-engineer-data-analytics-at-rivian-3766981106,2023-12-17,Normal,United States,Mid senior,Onsite,"About Rivian
Rivian is on a mission to keep the world adventurous forever. This goes for the emissions-free Electric Adventure Vehicles we build, and the curious, courageous souls we seek to attract.
As a company, we constantly challenge what’s possible, never simply accepting what has always been done. We reframe old problems, seek new solutions and operate comfortably in areas that are unknown. Our backgrounds are diverse, but our team shares a love of the outdoors and a desire to protect it for future generations.
Role Summary
In this role, you will be responsible for creating impactful dashboards using Power BI, Tableau, Snowflake, and other analytical tools to drive data-informed decisions for our operations team. Additionally, you will play a crucial role in developing and implementing assembly planning tools for our industrial engineering processes. The opening is located at our Normal, IL facility and reports directly to the Industrial Engineer Sr. Manager.
Responsibilities
Collect and analyze large datasets from various sources to identify trends, patterns, and insights relevant to our operations team.
Design, develop, and maintain interactive and visually appealing dashboards using Power BI, Tableau, Snowflake, and other tools to effectively present data-driven insights.
Collaborate with cross-functional teams to understand their analytical needs and translate them into actionable visualizations and reports.
Ensure data accuracy, consistency, and integrity in all analytics deliverables.
Develop and implement assembly planning tools to optimize industrial engineering processes, including workflow design, resource allocation, and production scheduling.
Collaborate closely with the industrial engineering team to understand their requirements and provide technical solutions that improve efficiency and productivity.
Conduct testing, troubleshooting, and debugging of assembly planning tools to ensure optimal performance and user experience.
Stay updated with industry trends and emerging technologies related to assembly planning and propose innovative solutions to enhance our processes.
Qualifications
Bachelors degree in engineering discipline or a combination of combined experience.
Proven experience in data analysis, dashboard creation, and visualization using tools such as Power BI, Tableau, Snowflake, etc.
Strong programming skills in languages such as SQL, Python, or R for data manipulation and analysis.
Experience in software development and application engineering, including workflow design and development of planning tools.
Excellent problem-solving abilities and attention to detail.
Strong communication and collaboration skills to effectively work with cross-functional teams.
Ability to manage multiple projects simultaneously and deliver high-quality results within specified deadlines.
Equal Opportunity
Rivian is an equal opportunity employer and complies with all applicable federal, state, and local fair employment practices laws. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, ancestry, sex, sexual orientation, gender, gender expression, gender identity, genetic information or characteristics, physical or mental disability, marital/domestic partner status, age, military/veteran status, medical condition, or any other characteristic protected by law.
Rivian is committed to ensuring that our hiring process is accessible for persons with disabilities. If you have a disability or limitation, such as those covered by the Americans with Disabilities Act, that requires accommodations to assist you in the search and application process, please email us at candidateaccommodations@rivian.com .
Candidate Data Privacy
Rivian may collect, use and disclose your personal information or personal data (within the meaning of the applicable data protection laws) when you apply for employment and/or participate in our recruitment processes (“Candidate Personal Data”). This data includes contact, demographic, communications, educational, professional, employment, social media/website, network/device, recruiting system usage/interaction, security and preference information. Rivian may use your Candidate Personal Data for the purposes of (i) tracking interactions with our recruiting system; (ii) carrying out, analyzing and improving our application and recruitment process, including assessing you and your application and conducting employment, background and reference checks; (iii) establishing an employment relationship or entering into an employment contract with you; (iv) complying with our legal, regulatory and corporate governance obligations; (v) recordkeeping; (vi) ensuring network and information security and preventing fraud; and (vii) as otherwise required or permitted by applicable law.
Rivian may share your Candidate Personal Data with (i) internal personnel who have a need to know such information in order to perform their duties, including individuals on our People Team, Finance, Legal, and the team(s) with the position(s) for which you are applying; (ii) Rivian affiliates; and (iii) Rivian’s service providers, including providers of background checks, staffing services, and cloud services.
Rivian may transfer or store internationally your Candidate Personal Data, including to or in the United States, Canada, the United Kingdom, and the European Union and in the cloud, and this data may be subject to the laws and accessible to the courts, law enforcement and national security authorities of such jurisdictions.
Please note that we are currently not accepting applications from third party application services.
Show more
Show less","Power BI, Tableau, Snowflake, SQL, Python, R, Data analysis, Visualization, Software development, Application engineering, Workflow design, Assembly planning tools, Industrial engineering processes, Resource allocation, Production scheduling, Problemsolving, Communication, Collaboration, Project management","power bi, tableau, snowflake, sql, python, r, data analysis, visualization, software development, application engineering, workflow design, assembly planning tools, industrial engineering processes, resource allocation, production scheduling, problemsolving, communication, collaboration, project management","application engineering, assembly planning tools, collaboration, communication, dataanalytics, industrial engineering processes, powerbi, problemsolving, production scheduling, project management, python, r, resource allocation, snowflake, software development, sql, tableau, visualization, workflow design"
Data Center Building Technician,JLL,"Kings Mountain, NC",https://www.linkedin.com/jobs/view/data-center-building-technician-at-jll-3731766565,2023-12-17,Ridgewood,United States,Associate,Onsite,"What this job involves –
The Data Center Building Technician performs various skilled and semi-skilled tasks in the installation, repair, maintenance and operation of mechanical, electrical, and environmental controls and life safety systems. Applies specialized knowledge and expertise across many different disciplines and ensures an efficient and safe working environment. The Data Center Operating Engineer will receive day-to-day work assignments from a Lead Data Center Operating Engineer.
What is your day to day?
Each Data Center Building Technician must be able to independently plan work assignments, perform duties with a minimum of direct supervision, and assist as a helper in other trades and in the general maintenance and operation of buildings and grounds. In the absence of a supervisor, one of the tradesmen shall be capable of acting as the working foremen or lead man.
Installation, maintenance, operation and repair of mechanical and electrical equipment and systems, includes but is not limited to electrical switchgear, diesel generators, HVAC/CRAC systems, UPS systems, PDUs, RPPs, BMS/EPMS systems, and fire alarm and suppression systems.
Ensures proper operation of systems in compliance with required regulations and codes.
Test, maintain and evaluate equipment by using instrumentation.
Test and calibrate electronic HVAC and building environmental controls to ensure that equipment is functioning properly.
Perform, as required, skilled maintenance activities to include but not limited to construction, welding, soldering and plumbing.
Inspect and repair pumps, fans, valves and motors ensuring proper operation of the facility equipment and systems.
Perform all duties in a safe manner and in accordance with established work standards.
Comply with all Company policies and procedures and adhere to Company standards of business ethics and conduct.
Must be a team player committed to working in a quality environment.
Is willing to perform other duties as reasonably assigned and appropriate for the skill set.
Work Schedule
Rotating
Required
Desired experience and technical skills
High school graduate or GED.
Candidate must possess a high level of system knowledge in performing critical data center maintenance, repairs and emergency service on state-of-the-art critical support systems and must be able to demonstrate such capabilities.
Capable of performing duties as a stationary Operating engineer in a large facility.
Ability to use computers using the Windows-based software environment.
Must have valid state driver’s license.
Self-starter with strong interpersonal skills and a positive attitude.
Ability to communicate verbally in a clear and concise manner to supervisor and clients.
After-hours availability is essential (Critical system maintenance is performed during off-hours).
Must follow established Company practices and procedures and show a high level of safety awareness.
Must be able to physically access all spaces and systems to make quality inspection, including roofs and equipment rooms.
Preferred
Familiar with computer equipment and programs.
Knowledge of OSHA standards and local codes.
Prior knowledge of working within a Data Center environment is a plus.
4-year apprenticeship program or five years of field experience in Critical Facilities.
Switchgear and Generator Operations a plus.
Estimated compensation for this position is:
60,000.00 – 80,000.00 USD
The pay range listed is a total compensation range including bonus, if applicable. The provided range is an estimate and not guaranteed. An employment offer is based on applicant’s education, experience, skills, abilities, geographic location, internal equity and alignment with market data.
#OEjobs
Show more
Show less","HVAC/CRAC systems, UPS systems, PDUs, RPPs, BMS/EPMS systems, Fire alarm and suppression systems, Windows, OSHA standards, Switchgear, Generators, Construction, Welding, Soldering, Plumbing, Pumps, Fans, Valves, Motors","hvaccrac systems, ups systems, pdus, rpps, bmsepms systems, fire alarm and suppression systems, windows, osha standards, switchgear, generators, construction, welding, soldering, plumbing, pumps, fans, valves, motors","bmsepms systems, construction, fans, fire alarm and suppression systems, generators, hvaccrac systems, motors, osha standards, pdus, plumbing, pumps, rpps, soldering, switchgear, ups systems, valves, welding, windows"
Senior Data Engineer-- Data Governance,Brooksource,"Fort Mill, SC",https://www.linkedin.com/jobs/view/senior-data-engineer-data-governance-at-brooksource-2815549932,2023-12-17,Ridgewood,United States,Mid senior,Onsite,"<< Return to Search Results
Brooksource is searching for a
Data Engineer
to join our Carolina-based healthcare client's Enterprise Information Management Group. Our client is growing their Business Intelligence, Integration and Cognitive Computing practices and is seeking an experienced Data Engineer to assist with their Data Governance program/practices. This is a perfect opportunity for a detail oriented, data loving resource with experience using Informatica Axon, EDC and Azure Data Lakes to join a growing health system.
Basic Job Duties
Support implementation of data management & governance strategy by defining data management & governance processes and workflows
Develop and manage the business glossary, data lineage, data quality controls, reference data, and mapping documents across Customer, Product, and Employee domains in Informatica Axon and Informatica EDC.
Maintain a high level of information accuracy and data integrity in a data model mapping all products, hierarchies, item catalog, categories, attributes (user defined and operational), organizations, and location through all aspects of the change process
Define product definition workflows with appropriate role based user group approvals
Develop test plans and assist with unit, performance, integration, and user acceptance testing, particularly packaged application configurations.
Lead Data Governance Board along with its charter, procedures and standards.
Responsible for defining the Information Lifecycle Management (ILM) policy and implementing ILM Archiving and Data Retention initiatives.
Work with other business areas for training and providing thought leadership to data stewards in support of the Data Governance program.
Required Experience And Tools
Understanding of lineage, data flow, impact analysis, problem analysis, and dashboarding
Data Cataloging and Data Tagging
Business Glossary
Informatica (Axon)
Electronic Data Capturing (EDC)
SQL/SSMS/scripting experience
Azure Data Lake experience
Exposure to CLAIRE Engine
Work within PowerBI
Nice to haves: ITIL & CMDB background
Senior Data Engineer-- Data Governance Fort Mill, South Carolina
Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.
JO-2011-106461
Show more
Show less","Data Governance, Data Management, Informatica Axon, Informatica EDC, Azure Data Lakes, Data Lineage, Data Quality, Reference Data, Data Mapping, Data Modeling, Product Definition, User Acceptance Testing, Data Stewards, Data Lifecycle Management, Archiving, Data Retention, PowerBI, SQL, SSMS, Scripting","data governance, data management, informatica axon, informatica edc, azure data lakes, data lineage, data quality, reference data, data mapping, data modeling, product definition, user acceptance testing, data stewards, data lifecycle management, archiving, data retention, powerbi, sql, ssms, scripting","archiving, azure data lakes, data governance, data lifecycle management, data lineage, data management, data mapping, data quality, data retention, data stewards, datamodeling, informatica axon, informatica edc, powerbi, product definition, reference data, scripting, sql, ssms, user acceptance testing"
Senior Data Engineer,LPL Financial,"Fort Mill, SC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-lpl-financial-3756931611,2023-12-17,Ridgewood,United States,Mid senior,Hybrid,"Are you a team player? Are you curious to learn? Are you interested in working in meaningful projects? Do you want to work with cutting-edge technology? Are you interested in being part of a team that is working to transform and do things differently? If so, LPL Financial is the place for you!
LPL Financial (Nasdaq: LPLA) was founded on the principle that the firm should work for the advisor, and not the other way around. Today, LPL is a leader* in the markets we serve, supporting more than 18,000 financial advisors, 800 institution-based investment programs and 450 independent RIA firms nationwide. We are steadfast in our commitment to the advisor-centered model and the belief that Americans deserve access to personalized guidance from a financial advisor. At LPL, independence means that advisors have the freedom they deserve to choose the business model, services, and technology resources that allow them to run their perfect practice. And they have the freedom to manage their client relationships, because they know their clients best. Simply put, we take care of our advisors, so they can take care of their clients.
Job Overview:
We are looking for a Data Engineer with AWS experience to join our team of developers and engineers to participate in design and build of AWS data ingestion and transformation pipelines based on the specific needs driven by the Product Owners and Jira user stories. The candidate should possess strong knowledge and interest across data technologies with a background in data engineering. Candidate will also have to work directly with other senior data engineers, product owners to deliver data products in a collaborative and agile environment. They will also need to be able to integrate code into a cloud production environment.
Responsibilities:
Applicant must be able to collaborate with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL, AWS Glue and/or Informatica for big data technologies. We need someone that can work closely with product teams to deliver data products in a collaborative and agile environment.
Seek to continuously develop deep AWS engineering skills that optimizes code quality and performance
Collaborate on data engineering coding principles, standards, designs, frameworks, and chaos testing
Excellent verbal and written communication skills for expressing technical software requirements and designs
Work with on-prem database and warehouse solutions such as Oracle or SQL Server or Cloud based solutions.
Build data pipelines and other custom automated solutions to speed the ingestion, analysis, and visualization of large volumes of data
What are we looking for?
We want
strong collaborators
who can deliver a world-class client experience
. We are looking for people who thrive in a
fast-paced environment
,
are client-focused
,
team oriented
, and are able to execute in a way that encourages
creativity
and
continuous improvement
.
Requirements:
Minimum Bachelor's Degree in Computer Science, Software Engineering, or related experience
3+ years Cloud Development for data processing including experience-doing ETL processes.
3+ years in a technical management capacity for a large enterprise with a high performing technical team
3+ years’ experience with AWS Glue, AWS Data Brew, NoSQL server, SQL Server query experience, AWS Informatica for Relational Database Service (RDS) and the Cloud.
3+ years of experience in designing and developing Data Pipelines for Data Ingestion or Transformation using AWS technologies
Preferences:
Experience with release management and support cloud applications
Strong skills in querying languages like SQL and relational databases is required
Knowledge of AWS Cloud Data Ingestion Patterns and Practices
Experience with DevOps toolchain that enables CI/CD pipeline (e.g., Gitlab, Terraform, CloudFormation, Ansible, Puppet, Jenkins, TeamCity, Octopus, Puppet)
Experience with design and development of AWS services with high performance and scalability (e.g., Analytics, Integration, Compute, Container, Content Delivery, Storage)
Experience with development against non-relational database (e.g., key-value store, columnar store, graph database)
Experience with Agile Methodologies: Scrum/Kanban
Excellent oral and written communication skills
Why LPL?
At LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation’s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential.
We are one team on one mission. We take care of our advisors, so they can take care of their clients.
Because our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work.
Pay Range:
$103,360-$155,040/year
Actual base salary varies based on factors, including but not limited to, relevant skill, prior experience, education, base salary of internal peers, demonstrated performance, and geographic location. Additionally, LPL Total Rewards package is highly competitive, designed to support your success at work, at home, and at play – such as 401K matching, health benefits, employee stock options, paid time off, volunteer time off, and more. Your recruiter will be happy to discuss all that LPL has to offer!
Why LPL?
At LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation’s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential.
We are
one team on one mission.
We take care of our advisors, so they can take care of their clients.
Because our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work.
Want to hear from our employees on what it’s like to work at LPL? Watch this!
We take social responsibility seriously. Learn more here
Want to see info on our benefits? Learn more here
Join the LPL team and help us make a difference by turning life’s aspirations into financial realities. Please log in or create an account to apply to this position. Principals only. EOE.
Information on Interviews:
LPL will only communicate with a job applicant directly from an
@lp
lfinancial.com
email address and will never conduct an interview online or in a chatroom forum. During an interview, LPL will not request any form of payment from the applicant, or information regarding an applicant’s bank or credit card. Should you have any questions regarding the application process, please contact LPL’s Human Resources Solutions Center at (800) 877-7210.
Show more
Show less","SQL, AWS, AWS Glue, Informatica, ETL, DevOps, Gitlab, Terraform, CloudFormation, Ansible, Jenkins, TeamCity, Puppet, Octopus, Scrum, Kanban","sql, aws, aws glue, informatica, etl, devops, gitlab, terraform, cloudformation, ansible, jenkins, teamcity, puppet, octopus, scrum, kanban","ansible, aws, aws glue, cloudformation, devops, etl, gitlab, informatica, jenkins, kanban, octopus, puppet, scrum, sql, teamcity, terraform"
Data Engineer,Brooksource,"Charlotte, NC",https://www.linkedin.com/jobs/view/data-engineer-at-brooksource-3769502393,2023-12-17,Ridgewood,United States,Mid senior,Hybrid,"Data Engineer
Charlotte, NC
*This is a proactive recruitment posting*
Our Fortune 500 Energy & Utilities client is pushing out new products AND Generative AI pilots sweeping the enterprise and are in need of talented Data Engineers to join them! Over the coming weeks and months, we are expecting roles from ETL Data Engineers, Big Data Engineers, Data Warehousing Developers/Analysts, Data Scientists and more to open up.
The roles we are expecting to come out soon should be looking for 3-10 years of experience in the areas listed here. However, if you have significant professional experience of any level with at least 4/5 of the below, I encourage you to apply!
• Kafka
• Python w/ data product/engineering experience
• ServiceNow API experience
• AWS will always be nice to have
• Database background
What's the benefit of applying before the roles are out?
1. You get in on the ground floor of applications. Roles move fast - you don't want to miss them!
2. You can express your goals up-front to find a better match.
3. You have an advocate for you once roles do open.
Note: Unfortunately, this client cannot work C2C for this contract role as they are looking to bring someone on full-time and cannot offer sponsorship.
Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state, and local laws.
Show more
Show less","Data Engineering, ETL, Big Data, Data Warehousing, Data Science, Kafka, Python, ServiceNow API, AWS, Database","data engineering, etl, big data, data warehousing, data science, kafka, python, servicenow api, aws, database","aws, big data, data engineering, data science, database, datawarehouse, etl, kafka, python, servicenow api"
Senior Data Engineer,Synechron,"Charlotte, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-synechron-3768100074,2023-12-17,Ridgewood,United States,Mid senior,Hybrid,"Job Title
:
Senior Data Engineer
Job Location
:
Charlotte, NC
Summary:
We are looking for Senior ETL Datawarehouse Specialist who will take part in development and integration work, related to the local and global risk systems
.
The Role
Responsibilities:
Design and develop robust, scalable software solutions for acquiring, analyzing, and cleansing time series data
Enhance existing systems to improve performance, add functionalities, and integrate new technologies to enable easy access to time series data and metadata for both end users and systems
Implement algorithms for time series analysis, trend detection, and anomaly detection.
Lead engineers responsible for daily support and project based development of risk management systems.
Lead and mentor junior engineers and team members, fostering an environment of innovation and continuous improvement.
Implement best practices in software development, including DevOps
Collaborate with cross-functional teams to align system development with the company's strategic goals
Uphold and advocate for high standards in code quality, testing, and maintainability.
Requirements:
You are:
10+ years of experience in IT with more than 5 years in financial projects (preferably in the area of Market Risk)
Expertise in Python and C# or Java with automated testing
Strong experience in SQL and database programming (preferably MS SQL Server)
Good understanding of ETL/ELT and DWH concepts with hands on experience using ETL/ELT tools;
Strong testing and troubleshooting skills
Experience with Git, Jira, Confluence, Jenkins, and other DevOps tools
Functional expertise in time series management including fitting curves and vol surfaces
Good communication and presentation skills
It would be great if you also had
:
Experience with Databricks Delta lake, Delta live tables, PySpark, and Snowflake
Experience with Xenomorph Timescape EDM+ or other vendor time series management products
Hands-on experience with SSIS
We can offer you:
A highly competitive compensation and benefits package
A multinational organization with 45 offices in 19 countries and the possibility to work abroad
Laptop and a mobile phone
10 days of paid annual leave (plus sick leave and national holidays)
Maternity & Paternity leave plans
A comprehensive insurance plan including: medical, dental, vision, life insurance, and long-/short-term disability (plans vary by region)
Retirement savings plans
A higher education certification policy
Commuter benefits (varies by region)
Extensive training opportunities, focused on skills, substantive knowledge, and personal development
On-demand Udemy for Business for all Synechron employees with free access to more than 5000 curated courses
Coaching opportunities with experienced colleagues from our Financial Innovation Labs (FinLabs) and Center of Excellences (CoE) groups
Cutting edge projects at the world’s leading tier-one banks, financial institutions and insurance firms
A flat and approachable organization
A truly diverse, fun-loving and global work culture
SYNECHRON’S DIVERSITY & INCLUSION STATEMENT
Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Synclusive’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.
All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.
Show more
Show less","SQL, Python, C#, Java, Git, Jira, Confluence, Jenkins, SSIS, Time series management, DevOps, ETL/ELT, DWH, Risk management systems, Snowflake, PySpark, Databricks Delta lake, Delta live tables, Xenomorph Timescape EDM+","sql, python, c, java, git, jira, confluence, jenkins, ssis, time series management, devops, etlelt, dwh, risk management systems, snowflake, pyspark, databricks delta lake, delta live tables, xenomorph timescape edm","c, confluence, databricks delta lake, delta live tables, devops, dwh, etlelt, git, java, jenkins, jira, python, risk management systems, snowflake, spark, sql, ssis, time series management, xenomorph timescape edm"
Senior Data Engineer - Azure,"Connect Search, LLC","Charlotte, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-azure-at-connect-search-llc-3766975363,2023-12-17,Ridgewood,United States,Mid senior,Hybrid,"100% of health benefits are covered by the employer.
My client in Charlotte is looking for a strong, business facing, Data Engineer to join ASAP. This role will have you will be working with the business users/data consumers daily and helping them moving to Azure. In this role, you will work on a small part of business logic, but the focus is moving to Azure. They are using Synapse pipeline now so that is helpful and Tableau or PowerBI as well for reporting. They are a Tableau shop now but moving to PowerBI in the future. Overall, you will help drive data-driven decision making and advance the analytics team.
Tech Stack:
Design data tables, design experience
Want to talk to the business folks, they talk to the business and solve their problems quickly, need to be able to write requirements
Azure, Pipelines, data pools, storage procedures, data lakes, power BI modeling
Learn how synapse works and need to build stored procedure
SSIS packages (offshore manages this and enhances this) but it would be useful
Python would be helpful (data manipulation for AI manipulation)
Tableau shop but Power BI is also a preferred
Show more
Show less","Azure, Synapse, Pipeline, Data pools, Storage procedures, Data lakes, Power BI, Tableau, SSIS, Python, Data manipulation, AI","azure, synapse, pipeline, data pools, storage procedures, data lakes, power bi, tableau, ssis, python, data manipulation, ai","ai, azure, data lakes, data manipulation, data pools, pipeline, powerbi, python, ssis, storage procedures, synapse, tableau"
Sr. AWS Data Engineer,Eliassen Group,"Charlotte, NC",https://www.linkedin.com/jobs/view/sr-aws-data-engineer-at-eliassen-group-3769501347,2023-12-17,Ridgewood,United States,Mid senior,Hybrid,"**Due to client requirement, applicants must be willing and able to work on a w2 basis. For our w2 consultants, we offer a great benefits package that includes Medical, Dental, and Vision benefits, 401k with company matching, and life insurance**
About the Role:
We are assisting our industry-leading utility client on their search for a Senior Data Engineer to join their dynamic and growing team. As a Senior Data Engineer, you will play a key role in designing, developing, and maintaining our data infrastructure. You will be responsible for building and maintaining data pipelines, developing data models, and optimizing data performance.
Responsibilities:
Design, develop, and maintain data pipelines using AWS services such as EMR, Step Functions, Glue, and Redshift
Develop and maintain data models in DynamoDB and Athena
Optimize data performance for scalability and efficiency
Work closely with data scientists and analysts to understand their data needs and provide them with access to the data they need
Collaborate with other engineers to ensure that data infrastructure is integrated with other systems
Stay up-to-date on the latest data engineering technologies and trends
Qualifications:
Bachelor's degree in Computer Science, Data Science, or a related field
5+ years of experience as a Data Engineer
Strong understanding of AWS services such as EMR, Step Functions, Glue, Redshift, and DynamoDB
Proficient in Python and Spark
Experience with data modeling and data warehousing
Experience with data optimization techniques
Excellent communication and collaboration skills
Show more
Show less","AWS, EMR, Step Functions, Glue, Redshift, DynamoDB, Athena, Python, Spark, Data modeling, Data warehousing, Data optimization, Communication, Collaboration","aws, emr, step functions, glue, redshift, dynamodb, athena, python, spark, data modeling, data warehousing, data optimization, communication, collaboration","athena, aws, collaboration, communication, data optimization, datamodeling, datawarehouse, dynamodb, emr, glue, python, redshift, spark, step functions"
Lead Data Engineer,Synechron,"Charlotte, NC",https://www.linkedin.com/jobs/view/lead-data-engineer-at-synechron-3765464241,2023-12-17,Ridgewood,United States,Mid senior,Hybrid,"Job Tittle: Lead Data Engineer
Location: Charlotte, NC or Iselin, NJ
Note:
Hybrid 3 days in a week
Summary:
The role is aligned to the delivery team responsible for planning, development, and delivery of Insurance products for the Benchmarks and Indices business. This is a hands-on Data Engineer role more on Data warehouse, Python and Cloud (Azure/AWS) experience with focus on delivering results on-time, in-full to the expected quality levels.
Job Responsibilities:
Should provide architecture, design, implementation and operationalization of large-scale data and analytics solutions on Cloud Data Warehouse
Provide production support for Data Warehouse issues such data load problems, transformation translation problems.
Able to translate mapping specifications to data transformation design and development strategies and code, incorporating standards and best practices for optimal execution.
Able to work on data pipelines and modern ways of automating data pipeline using cloud based testing and clearly document implementations
Good to perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards.
Proficient to develop EL pipelines in and out of data warehouse using combination of Databricks, Python and SnowSQL.
Able to provide data ingestion and processing pipelines using Python, Databricks, SnowSql
Able to Establish and monitor Operational Level Agreements for the health and performance/cost of the warehouse environment (Loads, queries, data quality)
Qualifications:
Overall 8+ years of IT experience.
Minimum 6 years of experience in design and implementation of large-scale data solution on Azure Data Warehouse
Good experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modelling
Good knowledge of MS Azure configuration items with respect to Snowflake
Experience in writing scripts (UNIX, Python)
Good experience in Data warehouse, Python and Cloud (Azure preferred)
Excellent understanding of Snowflake Internals and integration of Snowflake with other data processing and reporting technologies
We can offer you:
A highly competitive compensation and benefits package
A multinational organization with 44 offices in 19 countries and the possibility to work abroad
Laptop and a mobile phone
10 days of paid annual leave (plus sick leave and national holidays)
Maternity & Paternity leave plans
A comprehensive insurance plan including: medical, dental, vision, life insurance, and long-/short-term disability (plans vary by region)
Retirement savings plans
A higher education certification policy
Commuter benefits (varies by region)
Extensive training opportunities, focused on skills, substantive knowledge, and personal development
On-demand Udemy for Business for all Synechron employees with free access to more than 5000 curated courses
Coaching opportunities with experienced colleagues from our Financial Innovation Labs (FinLabs) and Center of Excellences (CoE) groups
Cutting edge projects at the world’s leading tier-one banks, financial institutions and insurance firms
A flat and approachable organization
A truly diverse, fun-loving and global work culture
S​YNECHRON’S DIVERSITY & INCLUSION STATEMENT
Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.
All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.
Show more
Show less","Data Engineering, Data Warehousing, Python, Cloud Computing, Azure, AWS, Databricks, Snowflake, Unix, OLTP, OLAP, Data Modeling, Data Pipelines, Data Quality, SQL, Data Integration, Data Processing, Data Analytics","data engineering, data warehousing, python, cloud computing, azure, aws, databricks, snowflake, unix, oltp, olap, data modeling, data pipelines, data quality, sql, data integration, data processing, data analytics","aws, azure, cloud computing, data engineering, data integration, data processing, data quality, dataanalytics, databricks, datamodeling, datapipeline, datawarehouse, olap, oltp, python, snowflake, sql, unix"
Data Analyst,Eliassen Group,"Charlotte, NC",https://www.linkedin.com/jobs/view/data-analyst-at-eliassen-group-3778819024,2023-12-17,Ridgewood,United States,Mid senior,Hybrid,"**Charlotte NC**
We have a great opportunity for an experienced BI analyst with one of our clients in Charlotte.
Due to client requirement, applicants must be willing and able to work on a w2 basis. For our w2 consultants, we offer a great benefits package that includes Medical, Dental, and Vision benefits, 401k with company matching, and life insurance.
What you'll be doing:
The Data Analysts will be responsible for providing data analytics and managing the Fraud Ops Dashboards. Analyst work will include running SQL queries and analysis in a fast paced work environment.
Understands and maintains compliance with Regulation E and Visa Zero Liability Rules while adhering to policies and procedures.
Demonstrates regulatory and technical knowledge to provide guidance and support to internal staff and members on dispute items.
Ensures daily-automated tasks are complete through review of files and reports.
Work with internal and external partners to ensure systems and processes are effective and accurate.
Pull reports and other data to assist management in understanding the status and amount of disputed items.
What we'll be looking for:
In-depth knowledge of SQL, and Tableau.
Debit card, ACH and check fraud experience are preferred
Show more
Show less","SQL, Tableau, Regulation E, Visa Zero Liability Rules, Fraud Ops Dashboards","sql, tableau, regulation e, visa zero liability rules, fraud ops dashboards","fraud ops dashboards, regulation e, sql, tableau, visa zero liability rules"
Sr Data Engineer,"The Dignify Solutions, LLC","Providence, RI",https://www.linkedin.com/jobs/view/sr-data-engineer-at-the-dignify-solutions-llc-3769519906,2023-12-17,Coventry,United States,Associate,Onsite,"This is a 100% Remote job.
Deep knowledge and hands-on experience working with cloud data platform technologies in Azure, and scripting language Python
5+ years of hands-on experience in developing and deploying data architecture strategies or engineering practices
5+ years’ experience with complex SQL queries and knowledge of database technologies.
Strong experience working in Databricks SQL environment
Experience designing and developing data integration solutions using ETL tools such as dbt
Show more
Show less","Cloud data platform technologies, Azure, Python, Data architecture strategies, Engineering practices, SQL, Database technologies, Databricks SQL, ETL, Dbt","cloud data platform technologies, azure, python, data architecture strategies, engineering practices, sql, database technologies, databricks sql, etl, dbt","azure, cloud data platform technologies, data architecture strategies, database technologies, databricks sql, dbt, engineering practices, etl, python, sql"
Data Analyst,Talent Groups,"Providence, RI",https://www.linkedin.com/jobs/view/data-analyst-at-talent-groups-3778647194,2023-12-17,Coventry,United States,Associate,Onsite,"Location: Onsite in Providence, RI
Duration: 12-months
Pay: $44/hr - $54/hr W2
Job Description
Responsibilities include support to the Performance/Data Analytics and Reporting team. This includes, but not limited to, the development of business/functional/technical design documents, generating/tracking/analyzing reporting deliverables, as well as the development and reconciliation of reporting files. The candidate will also be responsible for identifying primary system components, critical success factors, risks, and overall technical strategy.
Show more
Show less","Performance/Data Analytics, Reporting, Business Design, Functional Design, Technical Design, Reporting Deliverables, Reporting Files, System Components, Critical Success Factors, Risks, Technical Strategy","performancedata analytics, reporting, business design, functional design, technical design, reporting deliverables, reporting files, system components, critical success factors, risks, technical strategy","business design, critical success factors, functional design, performancedata analytics, reporting, reporting deliverables, reporting files, risks, system components, technical design, technical strategy"
Future Opportunity- Data Engineering Consultant,Avanade,"Providence, RI",https://www.linkedin.com/jobs/view/future-opportunity-data-engineering-consultant-at-avanade-3781777910,2023-12-17,Coventry,United States,Associate,Onsite,"Job Description
RECRUITING FOR THE FUTURE
This posting is for a future job opportunity.
To support the needs of our clients, Avanade is actively recruiting and interviewing for the role outlined below, as we continue to build on another year of growth in our business. Now, with more than 60,000 employees around the globe, we are positioning our organization to effectively prepare for the future. What does this mean for you? We encourage you to apply and interview for this role without the need to decide now. This allows you to connect with leaders and hiring managers at a pace that works for you and when roles become available, you will be the first know.
Data Engineering Consultant
Our talented Data and AI Practice is made up of globally recognized experts- and there's room for more analytical and ambitious data professionals. If you're passionate about helping clients make better data-driven decisions to tackle their most complex business issues. let's talk. Take your skills to a new level and launch a career where you can can truly do what matters.
Come join us
As a Data Engineer within our Data and AI practice, you’ll design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.
Together we do what matters.
What You'll Do
Use your knowledge to plan and deliver data warehouses and storage
Take part in crafting and running bespoke data services for individual projects
Stay up to date with business best practice in using and retrieving data
Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining
Customize storage and extraction, metadata, and information repositories
Build and use effective metrics and monitoring processes
Help to develop business intelligence tools
Craft and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
Skills And Experiences
Strong knowledge of Python, Spark, and T-SQL
Microsoft Fabric/Synapse, Purview and Azure Databricks, PowerBI
Database, storage, collection and aggregation models, techniques, and technologies – and how to apply them in business
Ability to use technology to aim business problems using one or more Microsoft Analytics services for building data pipelines, data streams, and system integration
About You
Characteristics that can spell success for this role:
Advanced knowledge in entity and relationship extraction from unstructured data
Understanding of database table indexing, with emphasis on clustered column store tables
Proficient in data handling, understanding, analysis and interpretation of results
Experience with SQL technologies: Databricks (Spark), OR Azure Synapse
Data security: Must be able to demonstrate an understanding of data security at rest and in transit
Scale: Deliver performance at scale, and accurately anticipate system enhancement needs
Data integrity: Data manipulation, error identification and handling, and modeling
Enjoy your career
Some of the best things about working at Avanade
People-first culture that supports innovation and encourages people to move forward
Turn your ideas to human impact by cultivating a team that will help clients unlock what’s next
Opportunities to innovate in the Microsoft platform while charting your own career path
On-the-job growth through hackathons, innovation contests, and other expressions of our passion for innovating with purpose
Real-time access to technical and skilled resources globally
Find out more about some of our benefits here .
A great place to work
As you bring your skills and abilities to Avanade, you’ll get distinctive experiences, limitless learning, and ambitious growth in return. As we continue to build our diverse and inclusive culture, we become even more innovative and creative, helping us better serve our clients and our communities. You’ll join a community of smart, supportive collaborators to lift, mentor, and guide you, but to also lean on your expertise . You get a company purpose-built for business-critical, leading-edge technology solutions, committed to improving the way humans work, interact, and live. It’s all here, so take a closer look!
We work hard to provide an inclusive, diverse culture with a deep sense of belonging for all our employees. Visit our Inclusion & Diversity page.
Create a future for our people that focuses on
Expanding your thinking
Experimenting courageously
Learning and pivoting
Inspire greatness in our people by
Empowering every voice
Encouraging boldness
Celebrating progress
Accelerate the impact of our people by
Amazing the client
Prioritizing what matters
Acting as one
Learn more
To learn more about the types of projects our Data & AI team works on check out these case studies:
thyssenkrupp Materials Services uses data to help strike a delicate operational balance
What matters to SSE Renewables is innovating for a sustainable future
Hachette UK enables employees with machine learning-driven contract searches
Marston Holdings identifies significant cloud savings
Interested in knowing what’s going on inside Avanade? Check out our blogs:
Avanade Insights – exchange ideas that drive tomorrow’s innovation
Inside Avanade – explore what life is like working at Avanade
Compensation for roles at Avanade varies depending on a wide array of factors including but not limited to the specific office location, role, skill set and level of experience. As required by local law, Avanade provides a reasonable range of compensation for roles that may be hired in New York as set forth below and information on benefits offered is here.
Range of Starting Pay for role:
New York: $121,500 – $ 143,000
About Avanade
Avanade is the leading provider of innovative digital, cloud and advisory services, industry solutions and design-led experiences across the Microsoft ecosystem. Every day, our 59,000 professionals in 26 countries make a genuine human impact for our clients, their employees and their customers.
We have been recognized as Microsoft’s Global SI Partner of the Year more than any other company. With the most Microsoft certifications (60,000+) and 18 (out of 18) Gold-level Microsoft competencies, we are uniquely positioned to help businesses grow and solve their toughest challenges.
We are a people first company, committed to providing an inclusive workplace where employees feel comfortable being their authentic selves. As a responsible business, we are building a sustainable world and helping people from underrepresented communities fulfil their potential.
Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com .
Show more
Show less","Python, Spark, TSQL, Microsoft Fabric/Synapse, Purview, Azure Databricks, PowerBI, SQL technologies, Databricks (Spark), Azure Synapse, Entity and relationship extraction, Database table indexing, Data handling, Data analysis, Data interpretation, Data security, Data integrity, Data manipulation, Error identification, Data modeling","python, spark, tsql, microsoft fabricsynapse, purview, azure databricks, powerbi, sql technologies, databricks spark, azure synapse, entity and relationship extraction, database table indexing, data handling, data analysis, data interpretation, data security, data integrity, data manipulation, error identification, data modeling","azure databricks, azure synapse, data handling, data integrity, data interpretation, data manipulation, data security, dataanalytics, database table indexing, databricks spark, datamodeling, entity and relationship extraction, error identification, microsoft fabricsynapse, powerbi, purview, python, spark, sql technologies, tsql"
Senior Data Engineer,CVS Health,"Woonsocket, RI",https://www.linkedin.com/jobs/view/senior-data-engineer-at-cvs-health-3769074327,2023-12-17,Coventry,United States,Mid senior,Onsite,"Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.
Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
Position Summary
You will
collaborate with business partners to identify opportunities to leverage big data technologies in support of Pharmacy Personalization with a common set of tools and infrastructure to make analytics faster, more insightful, and more efficient
design highly saleable and extensible Big Data platforms which enables collection, storage, modeling, and analysis of massive data sets from numerous channels
define and maintain data architecture, focusing on applying technology to enable business solutions
assess and provide recommendations on business relevance, with appropriate timing and deployment
perform architecture design, data modeling, and implement CVS Big Data platforms and analytic applications
bring a DevOps mindset to enable big data and batch/real-time analytical solutions that leverage emerging technologies
develop prototypes and proof of concepts for the selected solutions, and implement complex big data projects
apply a creative mindset to a focus on collecting, parsing, managing, and automating data feedback loops in support of business innovation.
Required Qualifications
Strong in SQL and Python, with 3+ years hands-on coding experience with both
Experience building automated big data pipelines
Experience performing data analysis and data exploration
Experience working in an agile delivery environment
Strong critical thinking, communication, and problem solving skills and a quick learner
Experience with Snowflake
Experience with cloud-based platforms (i.e. Azure, GPC, AWS)
Experience with RDBMS and NoSQL Databases and hands-on query tuning/optimization
Experience working in multi-developer environment, using version control (i.e. Git)
Experience with orchestrating pipelines using tools (i.e. Airflow, Azure Data Factory)
Experience with real-time and streaming technology (i.e. Azure Event Hubs, Azure Functions Kafka, Spark Streaming)
Experience with data modeling and system architecture design
Experience partnering cross-functionally with other technical teams (i.e. data ingestion, data science, operational systems) to align priorities and achieve deliverable outcomes
Experience with setting coding standards, performing code reviews, and mentoring junior developers
Experience with REST API/Microservice development using Python
Experience with deployment/scaling of apps on containerized environment (i.e. Kubernetes, AKS)
Exposure/understanding DevOps best practice and CICD (i.e. Jenkins)
Ability to understand complex systems and solve challenging analytical problems
Preferred Qualifications
Experience with Databricks or any big data frameworks on Spark
Experience with bash shell scripts, UNIX utilities& UNIX Commands
Previous healthcare experience and domain knowledge
Education
Bachelor’s Degree in Computer Science, Engineering, Statistics, Physics, Math, or related field or equivalent experience
Preferred: Master’s Degree with coursework focused on advanced algorithms, mathematics in computing, data structures, etc.
Pay Range
The typical pay range for this role is:
$94,500.00 - $196,000.00
This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.
In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Companypolicies.
For more detailed information on available benefits, please visitjobs.CVSHealth.com/benefits
CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services throughColleagueRelations@CVSHealth.comIf you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.
Show more
Show less","Python, SQL, Snowflake, RDBMS, NoSQL, Data modeling, DevOps, REST API, Microservices, Git, Kubernetes, AKS, Jenkins, Bash shell scripts, UNIX utilities, UNIX Commands, Data science, Agile, Airflow, Azure Data Factory, Spark Streaming, Azure Event Hubs, Azure Functions Kafka, Databricks, Spark","python, sql, snowflake, rdbms, nosql, data modeling, devops, rest api, microservices, git, kubernetes, aks, jenkins, bash shell scripts, unix utilities, unix commands, data science, agile, airflow, azure data factory, spark streaming, azure event hubs, azure functions kafka, databricks, spark","agile, airflow, aks, azure data factory, azure event hubs, azure functions kafka, bash shell scripts, data science, databricks, datamodeling, devops, git, jenkins, kubernetes, microservices, nosql, python, rdbms, rest api, snowflake, spark, spark streaming, sql, unix commands, unix utilities"
Sr. Data Engineer,Diverse Lynx,"Providence County, RI",https://www.linkedin.com/jobs/view/sr-data-engineer-at-diverse-lynx-3684770861,2023-12-17,Coventry,United States,Mid senior,Onsite,"Position:
Sr. Data Engineer
Location:
Rhode, Island (ok for remote initially)
Duration: 12 months of contract
Must Have Skills
Maximum experience 15 years
SQL - 5&plus; years
Python/Pyspark in relation to ETL/Data jobs - 3&plus; years
Databricks/Synapse Analytics: 3&plus; years experience
Production Support / DataOps work: 3&plus; years, able to explain how to triage incidents/assign work.
Technical Team Leadership: 2&plus; years. Led technical team of 5&plus; people. Triage work, how he led & assisted junior and mid developers.
Able to explain Agile SDLC process.
Able to explain using ServiceNow or Another Ticketing system.
Able to explain using ServiceNow or Another Ticketing system
Do you have a passion for data? Then we are looking for you! We have an opportunity for a Senior Data Engineer to join our high-visibility Analytics team that is helping organization better reach consumers with compelling products and entertainment! In this role use your talents in software engineering, analytics cloud platforms and business collaboration to expand our rich analytics data ecosystem. This Sr. Data Engineer will bring deep knowledge and hands-on experience working with cloud data platform technologies such as Azure, AWS, Google and scripting language like Python.
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","SQL, Python, PySpark, ETL, DataOps, Databricks, Synapse Analytics, Agile SDLC, ServiceNow, Ticketing system, Azure, AWS, Google","sql, python, pyspark, etl, dataops, databricks, synapse analytics, agile sdlc, servicenow, ticketing system, azure, aws, google","agile sdlc, aws, azure, databricks, dataops, etl, google, python, servicenow, spark, sql, synapse analytics, ticketing system"
Data Tech Lead,Saransh Inc,"Lincoln, RI",https://www.linkedin.com/jobs/view/data-tech-lead-at-saransh-inc-3771443182,2023-12-17,Coventry,United States,Mid senior,Onsite,"Skills \ Competencies we are looking for –
8-12 years of experience in PL/SQL on Oracle cloud application.
Experience in creating data pipelines using Oracle.
Knowledge of Datawarehouse concepts and experience working on cloud data services (AWS)
Skillsets – Oracle Cloud Application PL/SQL data services, [Python and Mongo DB (good to have)]
Ability to build complex stored procedures using PL/SQL
Ability to write and reverse engineer complex SQL statements.
Experience in building data pipelines and data conversion activities
Good understanding of Datawarehouse and data modeling.
Experience in using AWS data services such as S3, Glue and Athena
Experience with any data ingestion or data integration processes
Show more
Show less","PL/SQL, Oracle Cloud Application, Data Pipelines, Oracle, Data Warehouse Concepts, AWS Cloud Data Services, Python, MongoDB, Stored Procedures, SQL, Data Modeling, AWS Data Services (S3 Glue Athena), Data Ingestion, Data Integration","plsql, oracle cloud application, data pipelines, oracle, data warehouse concepts, aws cloud data services, python, mongodb, stored procedures, sql, data modeling, aws data services s3 glue athena, data ingestion, data integration","aws cloud data services, aws data services s3 glue athena, data ingestion, data integration, data warehouse concepts, datamodeling, datapipeline, mongodb, oracle, oracle cloud application, plsql, python, sql, stored procedures"
Data Tech Lead,Saransh Inc,"Lincoln, RI",https://www.linkedin.com/jobs/view/data-tech-lead-at-saransh-inc-3766978194,2023-12-17,Coventry,United States,Mid senior,Onsite,"Skills \ Competencies we are looking for –
8-12 years of experience in PL/SQL on Oracle cloud application.
Experience in creating data pipelines using Oracle.
Knowledge of Datawarehouse concepts and experience working on cloud data services (AWS)
Skillsets – Oracle Cloud Application PL/SQL data services, [Python and Mongo DB (good to have)]
Ability to build complex stored procedures using PL/SQL
Ability to write and reverse engineer complex SQL statements.
Experience in building data pipelines and data conversion activities
Good understanding of Datawarehouse and data modeling.
Experience in using AWS data services such as S3, Glue and Athena
Experience with any data ingestion or data integration processes
Show more
Show less","PL/SQL, Oracle Cloud Application, Data Pipelines, Oracle, Datawarehouse, AWS, Python, Mongo DB, Stored Procedures, SQL, Data Modeling, AWS Data Services, S3, Glue, Athena, Data Ingestion, Data Integration","plsql, oracle cloud application, data pipelines, oracle, datawarehouse, aws, python, mongo db, stored procedures, sql, data modeling, aws data services, s3, glue, athena, data ingestion, data integration","athena, aws, aws data services, data ingestion, data integration, datamodeling, datapipeline, datawarehouse, glue, mongo db, oracle, oracle cloud application, plsql, python, s3, sql, stored procedures"
Staff Data Engineer,Recruiting from Scratch,"Providence, RI",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744398452,2023-12-17,Coventry,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data Warehouses, ETL, Data management tools, Data engineering, Business intelligence, Data science, Agile Software Development","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data engineering, business intelligence, data science, agile software development","agile software development, airflow, business intelligence, continuous integration, data engineering, data management tools, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Providence, RI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748833053,2023-12-17,Coventry,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Realtime Streaming Technologies, TDD, Automation, Continuous Delivery, Model Building, API Development, Data Governance & Policies, ETL Processes, Data Warehousing, Agile Engineering Practices, Pair Programming, Continuous Integration, Automated Testing, Deployment, Streamprocessing Systems, Dimensional Data Modeling, Schema Design, Legal Compliance, Data Classification, Data Retention, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Kafka, Storm, SparkStreaming","data engineering, realtime streaming technologies, tdd, automation, continuous delivery, model building, api development, data governance policies, etl processes, data warehousing, agile engineering practices, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, dimensional data modeling, schema design, legal compliance, data classification, data retention, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming","agile engineering practices, airflow, api development, automated testing, automation, continuous delivery, continuous integration, data classification, data engineering, data governance policies, data retention, datawarehouse, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, model building, pair programming, python, realtime streaming technologies, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Providence, RI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744901020,2023-12-17,Coventry,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","* Data Engineering, * Agile, * TDD, * Continuous Integration, * Continuous Delivery, * Automation, * Data Governance, * Data Warehousing, * ETL, * S3, * Snowflake, * Kafka, * Spark, * pySpark, * SQL, * Python, * Kubernetes, * Docker, * Helm, * Airflow, * Hadoop, * Storm, * Cassandra","data engineering, agile, tdd, continuous integration, continuous delivery, automation, data governance, data warehousing, etl, s3, snowflake, kafka, spark, pyspark, sql, python, kubernetes, docker, helm, airflow, hadoop, storm, cassandra","agile, airflow, automation, cassandra, continuous delivery, continuous integration, data engineering, data governance, datawarehouse, docker, etl, hadoop, helm, kafka, kubernetes, python, s3, snowflake, spark, sql, storm, tdd"
Principal Data Analyst,FM Global,"Johnston, RI",https://www.linkedin.com/jobs/view/principal-data-analyst-at-fm-global-3758740496,2023-12-17,Coventry,United States,Mid senior,Onsite,"More information about this job:
Overview:
FM Global is a leading property insurer of the world's largest businesses, providing more than one-third of FORTUNE 1000-size companies with engineering-based risk management and property insurance solutions. FM Global helps clients maintain continuity in their business operations by drawing upon state-of-the-art loss-prevention engineering and research; risk management skills and support services; tailored risk transfer capabilities; and superior financial strength. To do so, we rely on a dynamic, culturally diverse group of employees, working in more than 100 countries, in a variety of challenging roles.
This position is open to candidates seeking remote opportunities. Candidate must be open to periodic travel to headquarters located in Johnston, RI dependent on business needs. Must be willing to work EST hours.
Responsibilities:
Responsible for the management of FM Global’s metadata assets through the development and refinement of the enterprise data model and product specific logical data models.
The incumbent can be assigned to multiple product teams but may also work independently. An individual in this position performs two primary types of data analysis: reverse-engineering of complex applications which involve the development of data business rules through the analysis and refinement of existing applications as well as top-down data analysis of data rules on initiatives that are broad in scope and large in size which involve the extension of the enterprise data model through the gathering of data business rules through project related work. The incumbent collaborates with product teams, peer data analysts, their business partners, and the cross-functional data stakeholders to ensure that the business rules meet product needs and at the same time represent the needs of FM Global from an enterprise perspective. Although the incumbent may support a specific product team, he/she is accountable for ensuring that the data business rules integrate across subject areas and/or business domains. Acting as a mentor, the incumbent provides consultation to their business and product partners on data quality issues, data management, and data business rule development.
Qualifications:
EDUCATION
Bachelor degree required. Degree in Computer Science, Information technology, or a related discipline is preferred.
Experience
3+ years of experience in data analysis developing logical and enterprise data models one of the leading model management tools within the industry.
Skills And Knowlwdge
TECHNICAL KNOWLEDGE
Transactional data modeling skills in ER-Studio or ERWIN
Strong knowledge of data repositories and metadata management disciplines.
Strong knowledge of Agile methodologies and how to data modeling aligned to it
Strong knowledge of planning and task management.
Technical Skills
Expert level data analysis and data modeling skills on large, enterprise-based initiatives.
Working knowledge of data architectures and data normalization
Hands-on experience with industry leading data governance tools.
Experience working on Data Quality and Master Data Management tools are a plus
In depth experience in application project-related data analysis.
Excellent analytical, problem solving and conceptual skills.
Good project discipline, operating as part of a team or across teams to reach agreement to the enterprise view of the data
Hands-on knowledge of Data Governance tooling (Informatica is a plus)
Soft Skills
Excellent verbal and written communication skills with an ability to express complex business rules in a data model and to interpret the data rules from a data model.
Strong interpersonal skills.
Experience mentoring other team members
Ability to work well on cross-functional project teams within Technology, Product and the business environment.
The salary range for this position is $97,500 to $159,400. The final salary offer will vary based on geographic location, individual education, skills, and experience. The position is eligible to participate in FM Global’s comprehensive Total Rewards program that includes an incentive plan, generous health and well-being programs, a 401(k) and pension plan, career development opportunities, tuition reimbursement, flexible work, time off allowances and much more.
FM Global is an Equal Opportunity Employer and is committed to attracting, developing, and retaining a diverse workforce.
#FMG
Show more
Show less","Data Analysis, Data Modeling, Agile Methodologies, Data Governance, ERStudio, ERWIN, Informatica, Data Quality, Master Data Management, Data Architectures, Data Normalization, Project Management, Communication, Teamwork, Problem Solving, Analytical Thinking, Conceptual Skills","data analysis, data modeling, agile methodologies, data governance, erstudio, erwin, informatica, data quality, master data management, data architectures, data normalization, project management, communication, teamwork, problem solving, analytical thinking, conceptual skills","agile methodologies, analytical thinking, communication, conceptual skills, data architectures, data governance, data normalization, data quality, dataanalytics, datamodeling, erstudio, erwin, informatica, master data management, problem solving, project management, teamwork"
Lead Data Analyst,Brown University,"Providence, RI",https://www.linkedin.com/jobs/view/lead-data-analyst-at-brown-university-3689309085,2023-12-17,Coventry,United States,Mid senior,Onsite,"Job Description:
The Population Studies and Training Center (PSTC) at Brown University is seeking a Lead Data Analyst to work with Professor Elizabeth Fussell (PSTC (Research)) on several projects involving the analysis of data on the health and mobility outcomes of disaster-affected populations. The Lead Data Analyst will participate in data management, statistical analysis, assist with grant proposals, manuscript production for publication in peer-reviewed journals, and presentations at scientific conferences. Publications stemming from the designated research projects may be co-authored with Prof. Fussell and other co-investigators.
This 25 hour a week position is 100% grant funded and is a fixed-term position (until September 30, 2025) . There is the possibility of renewal, depending on funding source.
Education And Experience
Bachelor’s degree in a related field and 5 to 7 years experience or equivalent combination of education and experience. Master’s degree in Statistics, Health, or Social Sciences, preferred.
Experience in Stata required, SAS or R preferred.
Ability to work well both independently and collaboratively.
Effective at communicating with interdisciplinary audiences whose technical backgrounds vary widely.
Efficient time management skills.
Proven ability to work effectively on multiple projects at the same time.
Knowledge and experience in design and construction of databases with SAS.
Experience analyzing and managing large complex data sets
Highly proficient in statistical software packages (e.g., Stata and SAS or R)
Possess a willingness and ability to support and promote a diverse and inclusive campus community.
All offers of employment are contingent upon a criminal background check and education verification satisfactory to Brown University.
Recruiting Start Date:
2023-07-26
Job Posting Title:
Lead Data Analyst
Department:
Population Studies and Training Center
Grade:
Grade 10
Worker Type:
Employee
Worker Sub-Type
:
Fixed Term (Fixed Term)
Time Type:
Part time
Scheduled Weekly Hours:
25
Position Work Location:
Hybrid Eligible
Submission Guidelines:
Please note that in order to be considered an applicant for any staff position at Brown University you must submit an application form for each position for which you believe you are qualified. Applications are not kept on file for future positions. Please include a cover letter and resume with each position application.
Still Have Questions?
If you have any questions you may contact employment@brown.edu.
EEO Statement:
Brown University is an E-Verify Employer.
As an EEO/AA employer, Brown University provides equal opportunity and prohibits discrimination, harassment and retaliation based upon a person’s race, color, religion, sex, age, national or ethnic origin, disability, veteran status, sexual orientation, gender identity, gender expression, or any other characteristic protected under applicable law, and caste, which is protected by our University policies.
Show more
Show less","Data analysis, Data management, Statistical analysis, Stata, SAS, R, Database design, Database construction, Statistical software, Project management, Communication, Time management, Teamwork, Diversity and inclusion","data analysis, data management, statistical analysis, stata, sas, r, database design, database construction, statistical software, project management, communication, time management, teamwork, diversity and inclusion","communication, data management, dataanalytics, database construction, database design, diversity and inclusion, project management, r, sas, stata, statistical analysis, statistical software, teamwork, time management"
IT Enterprise Delivery (Data Engineer) Intern - Undergrad - Summer 2024,CVS Health,"Woonsocket, RI",https://www.linkedin.com/jobs/view/it-enterprise-delivery-data-engineer-intern-undergrad-summer-2024-at-cvs-health-3777371164,2023-12-17,Coventry,United States,Mid senior,Onsite,"You’ve invested a lot of time and energy in your education. Now you want the chance to make your mark. We offer challenging opportunities for you to learn and grow professionally. In our programs, you’ll be immersed in a culture of continuous improvement, with the goal of changing health care for the better.
Position Summary
This program is a 10-week full-time opportunity thatprovides real work experience with a team in your business area. You will be aligned to projects to lead or contribute to while alsoparticipatingin networking,development, and career explorationactivities.Our Corporate Internscanapply for future full-time roles during theprogram andhave an increased likelihood to receive an offer for a post-graduation role.
The Summer 2024 program will runJune 3rd-August 9th, 2024. Virtual onboarding activitieswill also take placeMay 29th-31st,2024.
This isa hybrid position with 3 days (generally Tuesday, Wednesday, and Thursday) in theWoonsocket, RIoffice location.The other 2 dayseach weekwill beworking remotely from home, office, or another location of your choice. No housing or housing stipendwill be provided, but interns will be given the opportunity to connect with each other prior to the start of the program to coordinate housing if desired.
The hourly rate is$23.00with40 hours/ week,no work on July 4th(company holiday), and two days of paid leave.
Work visa sponsorshipis notavailable for this role. This includes participation in Curricular Practical Training (CPT), Optional Practical Training (OPT), and F-1 Visa programs.
About this business area:
Required Qualifications
Meet educational requirements (see education section)
Programming Experience (c#, .net, Angular, REACT)
Preferred Qualifications
Demonstratedleadership experience. Examples include active roles in student or social organization activities, sports, or school or work-related projects
Experience leading a school or work-related project
Strong written and verbal communication skills
Prior experience analyzing data
Problemsolvingskills
Projectmanagementskills
An interest in obtaining full-time position within CVS Health upon graduation
Education
-Currently pursuing aBachelor’sdegree
Havean anticipatedgraduation date between September 2024 and August 2025
Have a major, minor or concentration relevant to the positionsuch asrelated majors:Business (Administration and/or Management),Computer Science, ProjectManagement, Information Systems,Software Engineering
Whether in our pharmacies or through our health service offerings, we are pioneering a bold new approach to total health care. As health care innovators, we are making quality care affordable, accessible, simple and seamless. We await your fresh ideas, new perspectives, and the unique contributions you will make to our organization.
Show more
Show less","C#, .NET, Angular, React, Programming, Leadership, Data Analysis, Problem Solving, Project Management, Written Communication, Verbal Communication","c, net, angular, react, programming, leadership, data analysis, problem solving, project management, written communication, verbal communication","angular, c, dataanalytics, leadership, net, problem solving, programming, project management, react, verbal communication, written communication"
IBM CloudPak for Data/Watson Engineer,IVY TECH SOLUTIONS INC,"Woonsocket, RI",https://www.linkedin.com/jobs/view/ibm-cloudpak-for-data-watson-engineer-at-ivy-tech-solutions-inc-3787772852,2023-12-17,Coventry,United States,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
Title: IBM CloudPak for Data/Watson Engineer
Location: 100% remote
Duration: 6 month contract + extensions
Please send the resume to
or 847- 350-1008
Must haves:
7+ years as an infrastructure engineer
Expertise with IBM products (CloudPak 4 Data + IBM Watson)
Knowledge in the following areas:
CP4D Admin - Openshift, PortWorx, CP4D patching, installation, operational management
Sub Products – Watson Knowledge Studio, Watson Discovery, WAVI Assistant Voice & Voice Gateway
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
mCjms8v4dV
Show more
Show less","CloudPak for Data, Watson, Infrastructure engineering, Openshift, PortWorx, Watson Knowledge Studio, Watson Discovery, WAVI Assistant Voice, Voice Gateway","cloudpak for data, watson, infrastructure engineering, openshift, portworx, watson knowledge studio, watson discovery, wavi assistant voice, voice gateway","cloudpak for data, infrastructure engineering, openshift, portworx, voice gateway, watson, watson discovery, watson knowledge studio, wavi assistant voice"
Data Engineer [73307],Onward Search,"Woonsocket, RI",https://www.linkedin.com/jobs/view/data-engineer-73307-at-onward-search-3780033663,2023-12-17,Coventry,United States,Mid senior,Hybrid,"Onward Search Can Not Support C2C For This Role
Required Qualifications :
Strong in SQL and Python, with 2+ years hands-on coding experience with both
Experience building automated big data pipelines
Experience performing data analysis and data exploration
Experience working in an agile delivery environment
Strong critical thinking, communication, and problem solving skills
Experience with big data frameworks (i.e. Hadoop and Spark)
Experience with cloud-based platforms (i.e. Azure, GPC, AWS)
Experience with Snowflake and hands-on query tuning/optimization.
Experience working in multi-developer environment, using version control (i.e. Git)
Experience with orchestrating pipelines using tools (i.e. Airflow, Azure Data Factory)
Experience with real-time and streaming technology (i.e. Azure Event Hubs, Azure Functions Kafka, Spark Streaming)
Experience with REST API/Microservice development using Python
Experience with deployment/scaling of apps on containerized environment (i.e. Kubernetes, AKS)
Preferred Qualifications
Exposure/understanding DevOps best practice and CICD (i.e. Jenkins)
Exposure/understanding of containerization (i.e. Kubernetes, Docker)
Show more
Show less","SQL, Python, Big data pipelines, Data analysis, Data exploration, Agile delivery, Critical thinking, Communication, Problem solving, Hadoop, Spark, Azure, GPC, AWS, Snowflake, Query tuning, Query optimization, Multideveloper environment, Version control, Git, Airflow, Azure Data Factory, Realtime technology, Streaming technology, Azure Event Hubs, Azure Functions Kafka, Spark Streaming, REST API, Microservice development, Kubernetes, AKS, DevOps, CICD, Jenkins, Docker","sql, python, big data pipelines, data analysis, data exploration, agile delivery, critical thinking, communication, problem solving, hadoop, spark, azure, gpc, aws, snowflake, query tuning, query optimization, multideveloper environment, version control, git, airflow, azure data factory, realtime technology, streaming technology, azure event hubs, azure functions kafka, spark streaming, rest api, microservice development, kubernetes, aks, devops, cicd, jenkins, docker","agile delivery, airflow, aks, aws, azure, azure data factory, azure event hubs, azure functions kafka, big data pipelines, cicd, communication, critical thinking, data exploration, dataanalytics, devops, docker, git, gpc, hadoop, jenkins, kubernetes, microservice development, multideveloper environment, problem solving, python, query optimization, query tuning, realtime technology, rest api, snowflake, spark, spark streaming, sql, streaming technology, version control"
Senior Data Engineer,Diverse Lynx,"Rhode Island, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-diverse-lynx-3668180317,2023-12-17,Coventry,United States,Mid senior,Hybrid,"Must Have Skills
Maximum experience 15 years
SQL - 5&plus; years
Python/Pyspark in relation to ETL/Data jobs - 3&plus; years
Databricks/Synapse Analytics: 3&plus; years experience
Production Support / DataOps work: 3&plus; years, able to explain how to triage incidents/assign work.
Technical Team Leadership: 2&plus; years. Led technical team of 5&plus; people. Triage work, how he led & assisted junior and mid developers.
Able to explain Agile SDLC process.
Able to explain using ServiceNow or Another Ticketing system.
Able to explain using ServiceNow or Another Ticketing system
Do you have a passion for data? Then we are looking for you! We have an opportunity for a Senior Data Engineer to join our high-visibility Analytics team that is helping organization better reach consumers with compelling products and entertainment! In this role use your talents in software engineering, analytics cloud platforms and business collaboration to expand our rich analytics data ecosystem. This Sr. Data Engineer will bring deep knowledge and hands-on experience working with cloud data platform technologies such as Azure, AWS, Google and scripting language like Python.
This position can be located remotely within the US.
A day in the life as a Sr. Data Engineer:
Design, develop and optimize the data pipelines and transformations to enable analytics insights.
Participate in the design, architecture, and implementation of data-engineering platform infrastructure.
Champion engineering excellence, including software design patterns, code reviews, and automated unit/functional testing.
Collaborate with product managers, data scientists, and data analysts in an open, creative, and agile environment.
What You'll Bring
5&plus; years of hands-on experience in developing and deploying data architecture strategies or engineering practices.
5&plus; years experience with complex SQL queries and knowledge of database technologies.
Self-starter with a passion and curiosity for solving unstructured data problems and ability to manipulate and optimize large data sets.
Knowledge of metadata management, data lineage, and principles of data governance.
Excellent written and verbal communication skills experience working with business and senior leaders.
Bachelors or Masters Degree in Computer Science, Information Systems or related field.
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","SQL, Python, Pyspark, ETL, DataOps, Databricks, Synapse Analytics, ServiceNow, Azure, AWS, Google, Agile SDLC, Software engineering, Analytics cloud platforms, Data architecture, Metadata management, Data lineage, Data governance, Business communication, Computer Science, Information Systems","sql, python, pyspark, etl, dataops, databricks, synapse analytics, servicenow, azure, aws, google, agile sdlc, software engineering, analytics cloud platforms, data architecture, metadata management, data lineage, data governance, business communication, computer science, information systems","agile sdlc, analytics cloud platforms, aws, azure, business communication, computer science, data architecture, data governance, data lineage, databricks, dataops, etl, google, information systems, metadata management, python, servicenow, software engineering, spark, sql, synapse analytics"
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"Rhode Island, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-engineering-at-acv-auctions-3762875656,2023-12-17,Coventry,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, C#, Python, ETL, AWS, RDS, S3, SQS, SNS, MongoDB, DBT, Hadoop, NoSQL, Airflow, Snowflake, OLAP, OLTP","sql, c, python, etl, aws, rds, s3, sqs, sns, mongodb, dbt, hadoop, nosql, airflow, snowflake, olap, oltp","airflow, aws, c, dbt, etl, hadoop, mongodb, nosql, olap, oltp, python, rds, s3, snowflake, sns, sql, sqs"
Data Engineer IV - Max Digital (Data Operations),ACV Auctions,"Rhode Island, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-operations-at-acv-auctions-3762876479,2023-12-17,Coventry,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack (SSIS, C#, Web APIs) outages during our operational support window.
Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required.
Leverage DMVs and monitoring tools to ensure system performance; work with data operations and engineering to improve as required.
Ensure existing HADR (availability groups) solution is functional and meets requirements.
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively. This may include being part of the emergency after-hours on-call rotation.
Mentor junior data engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert in SQL Query optimization
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (e.g., MongoDB)
Experience developing Windows services in C#
Experience writing unit and integration testing
Expert SQL and data-layer development experience; OLTP schema design.
Experience using and integrating with cloud services, specifically: AWS RDS, S3, SQS, SNS.
Nice to Have
Experience with Airflow
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, ETL, SSIS, Airflow, C#, Python, AWS RDS, S3, SQS, SNS, MongoDB, OLTP, DBT, Windows services","sql, etl, ssis, airflow, c, python, aws rds, s3, sqs, sns, mongodb, oltp, dbt, windows services","airflow, aws rds, c, dbt, etl, mongodb, oltp, python, s3, sns, sql, sqs, ssis, windows services"
Data Lead with (Oracle PL/SQL) and AWS,Extend Information Systems Inc.,"Providence, RI",https://www.linkedin.com/jobs/view/data-lead-with-oracle-pl-sql-and-aws-at-extend-information-systems-inc-3752612521,2023-12-17,Coventry,United States,Mid senior,Hybrid,"Job Title: Data Lead with (Oracle PL/SQL) and AWS
Location:
Providence RI – Hybrid
Duration: C2C
Experience level : 10+ yrs
Experience In Oracle Cloud Application Is Mandatory.
Must have Skill sets – Oracle PL/SQL AWS data services.
Python and Mongo DB -good to have.
Job Description
4-5 years of experience in PL/SQL on Oracle
Experience in creating data pipelines using Oracle.
Knowledge of Datawarehouse concepts and experience working on cloud data services (AWS)
Ability to build complex stored procedures using PL/SQL
Ability to write and reverse engineer complex SQL statements.
Experience in building data pipelines and data conversion activities
Good understanding of Datawarehouse and data modeling
Experience in using AWS data services such as S3, Glue and Athena
Experience with any data ingestion or data integration processes
Thanks & Regards !!
Anoop Tiwari
Extend Information Systems
Cell: -
Email: Anoop@extendinfosys.com
Show more
Show less","Oracle PL/SQL, AWS, Python, MongoDB, SQL, Data Pipelines, Data Warehousing, Data Modeling, AWS S3, AWS Glue, AWS Athena, Data Ingestion, Data Integration","oracle plsql, aws, python, mongodb, sql, data pipelines, data warehousing, data modeling, aws s3, aws glue, aws athena, data ingestion, data integration","aws, aws athena, aws glue, aws s3, data ingestion, data integration, datamodeling, datapipeline, datawarehouse, mongodb, oracle plsql, python, sql"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Providence, RI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759708653,2023-12-17,Coventry,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, SQL, Pandas, R, Airflow, KubeFlow, Spark, PySpark, Snowflake, Kafka, Storm, SparkStreaming, Kubernetes, Docker, Helm, Machine learning, Data mining, Data cleaning, Data normalization, Data modeling, Data governance, Data security, Data compliance, Data pipelines, Data engineering, Data platforms, Data processing, Data visualization, Data analysis, Distributed systems, Microservices, Relational databases, NoSQL databases, ETL pipelines, NLP, Large language models","python, java, sql, pandas, r, airflow, kubeflow, spark, pyspark, snowflake, kafka, storm, sparkstreaming, kubernetes, docker, helm, machine learning, data mining, data cleaning, data normalization, data modeling, data governance, data security, data compliance, data pipelines, data engineering, data platforms, data processing, data visualization, data analysis, distributed systems, microservices, relational databases, nosql databases, etl pipelines, nlp, large language models","airflow, data cleaning, data compliance, data engineering, data governance, data mining, data normalization, data platforms, data processing, data security, dataanalytics, datamodeling, datapipeline, distributed systems, docker, etl pipelines, helm, java, kafka, kubeflow, kubernetes, large language models, machine learning, microservices, nlp, nosql databases, pandas, python, r, relational databases, snowflake, spark, sparkstreaming, sql, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Providence, RI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773088743,2023-12-17,Coventry,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML Data Ops, Data pipelines, Data Enrichment, Data Pre/Post Processing, Data Mining, Data Cleaning, Data Normalizing, Data Modeling, NLP, Python, Java, bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Conversational AI APIs, Recommender Systems, Kafka, Storm, SparkStreaming, Machine Learning, Legal Compliance, Data Classification, Data Retention","data engineering, ml data ops, data pipelines, data enrichment, data prepost processing, data mining, data cleaning, data normalizing, data modeling, nlp, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, conversational ai apis, recommender systems, kafka, storm, sparkstreaming, machine learning, legal compliance, data classification, data retention","airflow, aws, azure, bash, conversational ai apis, data classification, data cleaning, data engineering, data enrichment, data mining, data normalizing, data prepost processing, data retention, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, legal compliance, machine learning, ml data ops, nlp, python, recommender systems, snowflake, spark, sparkstreaming, sql, storm"
Lead Value Based HealthCare Data Analyst,Blue Cross & Blue Shield of Rhode Island,"Providence, RI",https://www.linkedin.com/jobs/view/lead-value-based-healthcare-data-analyst-at-blue-cross-blue-shield-of-rhode-island-3774943766,2023-12-17,Coventry,United States,Mid senior,Hybrid,"Schedule:
Hybrid (MA, RI & CT)
Jump into the new world of health insurance:
At Blue Cross & Blue Shield of Rhode Island (BCBSRI), our business is healthcare. But our focus is on improving lives. Be part of a team that is large enough to make a difference but small enough to be innovative. Work in a rapidly changing field. Take a chance to be creative. Move outside the status quo. Shape new ideas with the power of a national brand behind you.
Join others who know diversity is strength:
We appreciate and celebrate everything that makes us unique: age, national origin, citizenship status, perspectives, experiences, physical or mental disability, military status, race, ethnicity, religion, gender, sexual orientation, gender identity and/or expression. Our diversity strengthens us as an organization and helps us better serve an increasingly diverse Rhode Island population.
Why this job matters:
Independently manages the financial analytics and reporting for all value-based programs and initiatives to internal and external customers. Responsible for monitoring, reporting, and presenting actionable insights to improve healthcare delivery, quality, and cost effectiveness. Provide recommendations and insights on key financial topics, develop new analyses and reporting metrics to assist management with strategic decision making.
What you will do:
Lead and initiate analytics projects to provide strategic information to assist decision making. Provide and recommend decision alternatives, through quantitative means. Prepare project results or recommendations for a wide range of audiences (internal and external) using written reports and presentation exhibits/graphics.
Participate in and lead projects within the department and report on status. Regularly review results to ensure accuracy and consistency with desired goals and objectives using financially sound principles. Effectively complete projects within the necessary time frames, while determining the appropriate amount of detail associated with each project and provide supplementary information as necessary.
Assist in the oversight, development and distribution of financial reports, reconciliations, analyses and settlements as they relate to value-based agreements (upside/downside risk models, PCP capitation, global capitation, episode-based payments and others). Collaborate with finance to ensure accruals are in place for value-based incentives and assist with the annual budgeting and forecasting cycle.
Provide recommendations and input to new contract provisions for performance guarantees or potential changes to financial models or data reporting requirements.
Collaborate with vendors to create business requirements documents, solutions requirements specifications, systems operation guide, system design documents. Monitor all aspects of project implementation.
Perform other duties as assigned.
What you need to succeed:
Bachelor’s Degree in Mathematics, Statistics, Economics, Business Administration, or related field; or an equivalent combination of education and experience
Seven or more years’ experience in quantitative business analyses and mathematical modeling
Knowledge of mathematical modeling with proven analytical skills
Analytical mindset with good problem-solving skills
High attention to detail
Strong interpersonal skills
Excellent written and verbal communication skills
Advanced Microsoft Office skills, including Access.
Extras:
Experience working on cross-functional team projects
Experience with value-based payment models
Experience in the health insurance industry
Location:
BCBSRI is headquartered in downtown Providence, conveniently located near the train station and bus terminal. We actively support associate well-being and work/life balance and offer the following schedules, based on role:
In-office:
onsite 5 days per week
Hybrid:
onsite 2-4 days per week
Remote:
onsite 0-1 days per week. Permitted to reside in the following states, pending approval from the Human Resources Department: Arizona, Connecticut, Florida, Georgia, Louisiana, Massachusetts, North Carolina, Oklahoma, Rhode Island, South Carolina, Texas, Virginia
At Blue Cross & Blue Shield of Rhode Island (BCBSRI), diversity and inclusion are central to our core values and strengthen our ability to meet the challenges of today's healthcare industry. BCBSRI is an equal opportunity, affirmative action employer. We provide equal opportunities without regard to race, color, religion, gender, age, national origin, disability, veteran status, sexual orientation, genetic information and gender identity or expression.
The law requires an employer to post notices describing the Federal laws prohibiting job discrimination based on race, color, sex, national origin, religion, age, equal pay, disability, veteran status, sexual orientation, and genetic information and gender identity or expression. Please visit
https://www.eeoc.gov/employers/eeo-law-poster to view the ""EEO is the Law"" poster.
Show more
Show less","Quantitative business analysis, Mathematical modeling, SQL, Advanced Microsoft Office, Financial reporting, Data analysis, Project management, Communication skills, Problemsolving skills, Analytical mindset, Attention to detail, Valuebased payment models, Health insurance industry","quantitative business analysis, mathematical modeling, sql, advanced microsoft office, financial reporting, data analysis, project management, communication skills, problemsolving skills, analytical mindset, attention to detail, valuebased payment models, health insurance industry","advanced microsoft office, analytical mindset, attention to detail, communication skills, dataanalytics, financial reporting, health insurance industry, mathematical modeling, problemsolving skills, project management, quantitative business analysis, sql, valuebased payment models"
Data Engineer,"Steampunk, Inc.","McLean, VA",https://www.linkedin.com/jobs/view/data-engineer-at-steampunk-inc-3683973201,2023-12-17,Bowie,United States,Mid senior,Onsite,"As a Steampunk
Salesforce Data Engineer
, you will be responsible for all aspects of data migration and integration for a Salesforce Call Center implementation. You will establish and follow standard processes and practices to analyze requirements and data to provide quality work products (including documentation) and a data design that supports overall team and customer objectives. You will work directly with Technical Architects, Solution Architects, Developers, and Functional Analysts in an Agile environment using DevSecOps best practices and technologies.
Responsible for leading data migration discovery sessions with clients, documenting data definitions and data flows
Perform source and target system analysis, specification, design and development activities
Conduct Data Discovery/Requirements sessions and design workshops
Perform legacy data extraction, normalization, and cleansing
Work with other technical and business stakeholders to perform data mapping from legacy sources to Salesforce objects, identifying gaps, risks, and issues – and advising customers on options and impacts.
Import data, sampling, validation, reconciliation, and cutover
Perform Unit testing and error handling
Lead Data User Acceptance sessions
Minimum of 5+ years in developing data migration strategies using a variety of RDBMS technologies and ETL tools
Experience defining the required functions (data mappings), interfaces and detailed technical plan including data formats, data extraction, data cleansing, and data migration from source systems to target systems
Demonstrated competency and proficiency in Salesforce with a strong knowledge of the salesforce.com object model highly desired.
Steampunk is a
Change Agent
in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our
Human-Centered delivery methodology
, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an
employee owned company
, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.
We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program.
Show more
Show less","Salesforce, Data Engineer, Data Migration, Data Integration, Data Analysis, Data Design, Data Warehousing, ETL, Data Extraction, Data Normalization, Data Cleansing, Data Mapping, Data Import, Data Validation, Data Reconciliation, Data Cutover, Unit Testing, Error Handling, Data User Acceptance Testing, RDBMS, Agile, DevSecOps, HumanCentered Delivery Methodology","salesforce, data engineer, data migration, data integration, data analysis, data design, data warehousing, etl, data extraction, data normalization, data cleansing, data mapping, data import, data validation, data reconciliation, data cutover, unit testing, error handling, data user acceptance testing, rdbms, agile, devsecops, humancentered delivery methodology","agile, data cutover, data design, data extraction, data import, data integration, data mapping, data migration, data normalization, data reconciliation, data user acceptance testing, data validation, dataanalytics, datacleaning, dataengineering, datawarehouse, devsecops, error handling, etl, humancentered delivery methodology, rdbms, salesforce, unit testing"
Senior IT Officer - Data Management,The World Bank,"Washington, DC",https://www.linkedin.com/jobs/view/senior-it-officer-data-management-at-the-world-bank-3778715216,2023-12-17,Dunkirk,United States,Associate,Hybrid,"Description
Do you want to build a career that is truly worthwhile? Working at the World Bank Group provides a unique opportunity for you to help our clients solve their greatest development challenges. The World Bank Group is one of the largest sources of funding and knowledge for developing countries; a unique global partnership of five institutions dedicated to ending extreme poverty, increasing shared prosperity and promoting sustainable development. With 189 member countries and more than 120 offices worldwide, we work with public and private sector partners, investing in groundbreaking projects and using data, research, and technology to develop solutions to the most urgent global challenges. For more information, visit www.worldbank.org
ITS Vice Presidency Context:
Information and Technology Solutions (ITS) enables the WBG to achieve its mission of ending extreme poverty and promote shared prosperity in a sustainable way by delivering transformative information and technologies to its staff working in over 150 locations.
Our vision is to transform how the Bank Group accomplishes its mission through information and technology. In this fast-paced, ever-changing world, the formulation and implementation of the ITS strategy is an ongoing, iterative process of learning and adaptation developed through extensive consultations with business partners throughout the World Bank Group.
ITS shapes its strategy in response to changing business priorities and leverages new technologies to achieve three high-level business outcomes:
business enablement,
by providing Bank Group units with innovative digital tools and technologies to transform how they deliver value for their clients;
empowerment & effectiveness,
by ensuring that all Bank Group staff are connected, able to find information, and productive to accelerate the delivery of development solutions globally; and
resilience,
by equipping the Bank Group to provide risk-based cybersecurity and robust data protection for a global network and a growing cloud platform.
Implementation of the strategy is guided by three core principles. The first is to deliver solutions for business partners that are customer-centric, innovative, and transformative. The second is to provide the Bank Group with value for money with selective and standard technologies. The third principle is to excel at the basics by providing a high performing, robust, and resilient IT environment for the organization.
As a unit within the WB Operations and Corporate (ITSOC), the Data and Analytics unit (ITSDA) provides state-of-art information and technology applications to support the operations of the World Bank Group. Functions provided ensure that the systems meet the business needs of users and external clients to manage business processes for the World Bank. The current technology landscape encompasses Cloud-based data platforms (Azure and AWS), Oracle, SQL Server, Business Objects, Tableau, Cisco Information Server (Composite), SAP BW/Hana, Informatica, .Net, HTML 5, CSS Frameworks, SharePoint and many others. Our plans are to migrate our on-prem data repositories and re-engineer based on new Cloud architectures in the coming years.
Responsibilities:
•Lead and manage a team, co-located in Washington DC and Chennai, to engineer, implement, review, and recommend cost effective data management solutions and develop code for automation of various aspects of data management, with the goals to improve data reliability, efficiency, and quality.
•Provide leadership in the design and engineering of data pipelines that are flexible, scalable, secure, and cost effective, both on-premise and in the cloud, to meet the growing needs of the World Bank’s data landscape.
•Maintain accountability of the integrity of the data pipelines and ensure the smooth daily operations to meet various needs such as ensuring the bank’s reporting data are always available with well-established testing protocols and troubleshoot any technical issues that arise.
•Ensure the design, architecture and security reviews of the data engineering framework and solution are in line with industry best practices, ITS standards and represent good practice.
•Continue to innovate and establish a cloud-based data engineering framework in ITSDA that will accommodate both “traditional” structured data sources and “non-traditional” data sources, support current and emerging needs.
•Research opportunities for data tools acquisition and new uses for existing data.
•Work closely with other teams including ITSDA Platform Owner, Product Owners, Solution Architects, Data use and Data Governance to achieve the best outcome.
•Ensure alignment and partnership client internal stakeholders and vendors, establishing strong linkages with their service and product teams to support ITSDA activities, covering on-premise and cloud technologies such as Azure PaaS services, Informatica Intelligent Cloud Services (IICS), Tableau, Tibco Data Virtualization, Collibra, Informatica MDM, SAP Business Objects and Power BI.
•Adapt to competing demands, take on new responsibilities, and adjust plans to meet changing priorities.
•Maintain positive, constructive approach to problem-solving internally and during client interactions with focus on timely and accurate issue resolution.
•Evaluate best of the breed tools in Data Management space and work with Gartner and Forrester to automate existing data management processes or new data management tools.
Selection Criteria
•Master's degree with 8 years’ experience OR equivalent combination of education and experience in relevant discipline such as Computer Science.
•Minimum 5 years of experience in each of the following areas: (i) in developing options, roadmaps and architectures (ii) large enterprise systems, integration, application development (iii) experience in managing teams (iv) experience in managing procurement processes (i.e. RFPs)
•Experience designing and deploying high performance production services with robust monitoring and logging practices and demonstrated ability to build and interact with large data processing pipelines, distributed data stores, and distributed file systems.
•Good working knowledge of cloud platforms covering Azure and on-premise platforms covering traditional data management databases, data governance tools and virtualization software.
•Experience in managing large teams - staffing, skills development, organizing and operationalize teams to deliver value.
•Experience in developing options, roadmaps, evaluations, decision frameworks for complex enterprise solutions.
•Demonstrated experience of working and navigating in large and matrixed organizations with multi-layered governance structures, complex IT landscapes, and diverse client bases.
•Excellent grasp and knowledge of industry best practices in the data management domain, with experience in successfully implementing theory to practice in complex IT and business environments.
•Actively seeks knowledge needed to complete assignments and shares knowledge with others, communicating and presenting information in a clear and organized manner.
•Organized, agile, persistent, and proactive with the ability to work and juggle multiple tasks within tight deadlines.
•Delivers information effectively in support of team or workgroup. Excellent communication, writing/documentation, and facilitation skills.
•Proven ability to collaborate with other team members across boundaries and contribute productively to the team's work and output, demonstrating respect for different points of view.
•Strong diplomatic, interpersonal and teamwork skills to cultivate effective, productive client relationships and partnerships across organizational boundaries.
•Able to take personal ownership and accountability to meet deadlines and achieve agreed-upon results and has the personal organization to do so.
•Ability to juggle multiple tasks in a fast-paced environment, and the maturity to participate in multiple complex programs at the same time in an agile environment.
The World Bank Group offers comprehensive benefits, including a retirement plan; medical, life and disability insurance; and paid leave, including parental leave, as well as reasonable accommodations for individuals with disabilities.
We are proud to be an equal opportunity and inclusive employer with a dedicated and committed workforce, and do not discriminate based on gender, gender identity, religion, race, ethnicity, sexual orientation, or disability.
Show more
Show less","Data Management, Data Engineering, Data Analytics, Cloud Computing, Azure, AWS, Oracle, SQL Server, Business Objects, Tableau, Cisco Information Server (Composite), SAP BW/Hana, Informatica, .Net, HTML 5, CSS Frameworks, SharePoint, Data Pipelines, Data Quality, Data Reliability, Data Security, Data Governance, Data Virtualization, Data Integration, Data Warehousing, Big Data, Data Lakes, Data Mining, Machine Learning, Artificial Intelligence, Cloud Architect, Cloud Engineer, DevOps, Agile, Scrum, Kanban, Jira, Confluence, GitHub, GitLab, Jenkins, Docker, Kubernetes, Ansible, Terraform","data management, data engineering, data analytics, cloud computing, azure, aws, oracle, sql server, business objects, tableau, cisco information server composite, sap bwhana, informatica, net, html 5, css frameworks, sharepoint, data pipelines, data quality, data reliability, data security, data governance, data virtualization, data integration, data warehousing, big data, data lakes, data mining, machine learning, artificial intelligence, cloud architect, cloud engineer, devops, agile, scrum, kanban, jira, confluence, github, gitlab, jenkins, docker, kubernetes, ansible, terraform","agile, ansible, artificial intelligence, aws, azure, big data, business objects, cisco information server composite, cloud architect, cloud computing, cloud engineer, confluence, css frameworks, data engineering, data governance, data integration, data lakes, data management, data mining, data quality, data reliability, data security, data virtualization, dataanalytics, datapipeline, datawarehouse, devops, docker, github, gitlab, html 5, informatica, jenkins, jira, kanban, kubernetes, machine learning, net, oracle, sap bwhana, scrum, sharepoint, sql server, tableau, terraform"
Senior Data Engineer,Jobs for Humanity,"Annapolis, MD",https://www.linkedin.com/jobs/view/senior-data-engineer-at-jobs-for-humanity-3772675817,2023-12-17,Dunkirk,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with Capital One to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Capital One
Job Description
Center 1 (19052), United States of America, McLean, Virginia Job Title: Senior Data Engineer Are you passionate about using data and emerging technologies to solve complex problems? At Capital One, we are a diverse and inclusive team of problem solvers who work collaboratively to meet our customers' needs. We are currently seeking a Senior Data Engineer to join our Enterprise Data Team and drive transformational change. What You'll Do: - Identify and address customer needs, ensuring we create the best solutions for important problems. - Design and develop scalable data architectures and systems for extracting, storing, and processing large amounts of data. - Build efficient data pipelines for data ingestion, transformation, and loading from different sources, while maintaining data quality and integrity. - Collaborate with Data Scientists, Machine Learning Engineers, Business Analysts, and Product Owners to understand requirements and provide efficient solutions for data exploration, analysis, and modeling. - Implement testing, validation, and observability measures to ensure data pipelines meet customer service level agreements (SLAs). - Utilize cutting-edge technologies to develop modern data pipelines supporting Machine Learning and Artificial Intelligence. Basic Qualifications: - Bachelor's Degree. - Minimum of 4 years of application development experience (Internship experience is not applicable). - Minimum of 1 year of experience in big data technologies. Preferred Qualifications: - 5+ years of application development experience, including Python or SQL. - 2+ years of experience with a public cloud platform (AWS, Microsoft Azure, Google Cloud). - 3+ years of experience with distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, or Spark). - 2+ years of data warehousing experience (Redshift or Snowflake). - 3+ years of experience with UNIX/Linux, including basic commands and shell scripting. - 2+ years of experience with Agile engineering practices. At this time, Capital One does not sponsor employment authorization for new applicants. We offer a comprehensive and competitive benefits package that supports your total well-being. To learn more, visit the Capital One Careers website. Eligibility varies based on employment status. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. We value all applicants and do not discriminate based on sex, race, age, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited by applicable law. Capital One promotes a drug-free workplace. If you require an accommodation during the application process, please contact Capital One Recruiting at 1-800-304-9102 or email RecruitingAccommodation@capitalone.com. All information provided will be kept confidential and used only as necessary to provide accommodations. For technical support or questions about the recruiting process, please email Careers@capitalone.com. Capital One does not endorse or guarantee any third-party products, services, or information available through this site. Please note that Capital One Financial is made up of multiple entities, and any positions posted in specific countries are for those entities. Thank you for considering this opportunity with Capital One. We look forward to reviewing your application.
Show more
Show less","Python, SQL, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Redshift, Snowflake, UNIX/Linux, Agile","python, sql, mapreduce, hadoop, hive, emr, kafka, spark, redshift, snowflake, unixlinux, agile","agile, emr, hadoop, hive, kafka, mapreduce, python, redshift, snowflake, spark, sql, unixlinux"
Data Analyst/Senior Data Analyst,Institute for Women's Policy Research,"Washington, DC",https://www.linkedin.com/jobs/view/data-analyst-senior-data-analyst-at-institute-for-women-s-policy-research-3726741840,2023-12-17,Dunkirk,United States,Mid senior,Onsite,"Who We Are
The Institute for Women’s Policy Research (IWPR) is the nation’s preeminent think tank committed to achieving economic equity for women and eliminating structural and institutional barriers to women’s full participation in the workforce and society.
IWPR’s mission is to build knowledge and evidence to support policies that help grow women’s economic power and influence in society, close inequality gaps, and improve the economic security and well-being of girls, women, and families. We generate the ideas that build power and economic equity for all women.
Statement on Racial Equity and Intersectionality
At IWPR, we believe social and economic mobility in the United States is determined and influenced by race, ethnicity, class, gender identity, sexual orientation, and other markers of difference. In working to achieve economic equity and to better understand the experiences of all women in the workforce and in society, we use both racial equity and intersectional frameworks in our research and analysis to help generate strategies and solutions that are grounded in the economic realities of women and families. We also prioritize building an inclusive and welcoming work environment that values the diverse perspectives and experiences of all staff.
Statement on Inclusivity and non-discrimination
IWPR is an equal opportunity employer. We value and support difference, diversity, and inclusion in our hiring and employment practices. We do not discriminate based upon race, ethnicity, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, disability, marital or veteran status, age or any other legally protected status.
Job Description Overview
The Data Analyst/Senior Data Analyst at IWPR is responsible for helping to manage the collection and analysis of data across all IWPR programs and issue areas. They will provide analytic and research support for data-driven projects including manipulating, merging, cleaning, interpreting, and analyzing large datasets. They will also expand on the traditional statistical tools of policy research to exploit emerging methods that can offer additional insights from data. They will be responsible for designing and conducting surveys and producing results and analysis; performing weighting, modeling, and other statistical procedures common to survey research; producing high-quality reports, briefs and fact sheets based on survey findings; and identifying opportunities and trends for high impact.
Duties and Responsibilities:
Strategy, Vision, and Leadership (Senior Data Analyst only)
Work in partnership with the SVP of Research to develop a strategic plan and vision for the collection and management of data for IWPR
Work across issue areas and teams to identify and assess emerging research and data gaps in the field and potential innovations and opportunities for IWPR;
Represent IWPR at meetings, coalitions, and events.
Research and Data Analysis
Perform advanced, complex quantitative data analysis of large datasets and trends, utilizing econometric and/or other statistical methods, including simulation; create data visuals; develop, model, program, and field online surveys; and conduct interviews and focus groups.
Conduct research and/or policy analysis on issues related to one or more of IWPR’s core research areas (e.g. poverty and income inequality, paid sick leave, the wage gap and pay equity, women’s and family economic security, quality jobs, reproductive health, and/or educational equity and access).
Collect, clean, and analyze quantitative and qualitative data for projects. This includes managing data, writing computer programs for data processing and analysis, producing, and reviewing literature reviews.
Write high quality working papers, reports, and presentations based on the data and analysis.
Assist in designing research to meet high standards of methodological rigor.
Assist and lead in reporting research findings, including producing tables, graphics, and maps for reports, as well as writing up findings for briefs and reports and presenting at meetings and conferences.
In collaboration with program area researchers, design and implement a data warehousing strategy for data from the federal statistical system that is used repeatedly throughout the year, such as the Current Population Survey Annual Social and Economic supplement, American Community Survey, National Health Interview Survey.
Program and Research Management
Coordinate logistical tasks for research proposals, internal and external research events, and research management.
Assist with project management tasks (such as contractual, financial, timeline, and staffing components of research projects).
Direct the work of research assistants, interns and other staff as necessary.
Oversee and report on data metrics and has manager-level responsibility for enterprise information/data management budgeting and initiatives.
Track data release schedules for updates and release notes for quality control issues. In addition, monitor news for new data sources for research teams.
Consult with program area researchers on methodological questions arising from data analysis needs and assist with designing new studies, including identifying data sources, confirming study design feasibility, and contributing to proposal development.
Demonstrated commitment to IWPR’s mission and vison, including IWPR’s commitment to diversity, equity, and inclusion.
And other duties and responsibilities as assigned.
Education And Experience
Senior Data Analyst
- PhD preferred, with five years of experience managing high-level projects and programs, or master’s degree with at least ten years of experience in a related field (e.g., Math, Statistics, Computer Science, Data Science, or Social Science related field).
Data Analyst
- Master’s degree within a related field (e.g., Math, Statistics, Computer Science, Data Science, or Social Science related field).
Experience with multivariate statistical methods, survey research and/or qualitative research and design and analysis.
Excellent written and verbal communications skills, including presentation skills.
Superior problem-solving, managerial, and interpersonal skills.
Ability to work strategically and collaboratively across the organization.
Effective, versatile, and action oriented.
Ability to work well under pressure with tight deadlines and multiple priorities.
SALARY
This is an onsite (hybrid), full-time exempt position. The salary for the
Senior Data Analyst
is $88,910.00-$99,370.00/year and for the
Data Analyst
is $60,668.00- $75,312.00. The salary will be commensurate with experience. IWPR provides competitive salaries and a generous benefits package. To learn more about the IWPR compensation program and role structures, please click . IWPR provides competitive salaries and a generous benefits package.
How To Apply
Please submit a resume, writing sample, and thoughtful, relevant cover letter. Incomplete applications will not be considered.
Applications will be accepted until the position is filled. Individuals from underrepresented groups are encouraged to apply. IWPR is an equal opportunity employer.
Vaccine Policy:
IWPR has made the safety of our staff and our surrounding community a top priority. As part of that commitment, IWPR recently implemented a mandatory COVID-19 vaccination policy. This policy states that all IWPR employees, interns, fellows, (i.e., IWPR staff) are required to be vaccinated against COVID-19 and to receive a booster shot. IWPR will require selected candidates to submit proof of complete vaccination against COVID-19 and having received a booster shot prior to extending a job offer. If a candidate has a qualifying medical condition that contraindicates a Covid-19 vaccination, a medical exemption form should be requested from Human Resources.
Show more
Show less","Quantitative data analysis, Econometrics, Statistical methods, Simulation, Data visualization, Online surveys, Interviews, Focus groups, Research, Policy analysis, Data warehousing, Data management, Budgeting, Project management, Research proposals, Research events, Research assistants, Research internships, Data metrics, Enterprise information management, Data release schedules, Data sources, Study design, Proposal development, Multivariate statistical methods, Survey research, Qualitative research, Presentation skills, Problemsolving, Managerial skills, Interpersonal skills, Strategic thinking, Collaboration, Action orientation, Pressure handling, Multitasking","quantitative data analysis, econometrics, statistical methods, simulation, data visualization, online surveys, interviews, focus groups, research, policy analysis, data warehousing, data management, budgeting, project management, research proposals, research events, research assistants, research internships, data metrics, enterprise information management, data release schedules, data sources, study design, proposal development, multivariate statistical methods, survey research, qualitative research, presentation skills, problemsolving, managerial skills, interpersonal skills, strategic thinking, collaboration, action orientation, pressure handling, multitasking","action orientation, budgeting, collaboration, data management, data metrics, data release schedules, data sources, datawarehouse, econometrics, enterprise information management, focus groups, interpersonal skills, interviews, managerial skills, multitasking, multivariate statistical methods, online surveys, policy analysis, presentation skills, pressure handling, problemsolving, project management, proposal development, qualitative research, quantitative data analysis, research, research assistants, research events, research internships, research proposals, simulation, statistical methods, strategic thinking, study design, survey research, visualization"
Senior Data Platform Engineer,Motion Recruitment,"Washington, DC",https://www.linkedin.com/jobs/view/senior-data-platform-engineer-at-motion-recruitment-3761197808,2023-12-17,Dunkirk,United States,Mid senior,Hybrid,"A leading entertainment company is seeking a Data Platform Engineer to join their content team. In this position you will be designing, developing, and supporting their data platform to be used across data organization and other groups.
Requirements
7+ years of experience developing infrastructure as code with python or typescript
3+ years of experience on a cloud platform with AWS
Job orchestration tool with Airflow
Niceto-have with Monte-carlo, Fivetran, Tableau/Looker
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
LI#-DP
Posted By:
Derek Progin
Show more
Show less","Python, Typescript, AWS, Airflow, Monte Carlo, Fivetran, Tableau, Looker","python, typescript, aws, airflow, monte carlo, fivetran, tableau, looker","airflow, aws, fivetran, looker, monte carlo, python, tableau, typescript"
Senior Data Engineer - Data Quality/Governance,SiriusXM,"Washington, DC",https://www.linkedin.com/jobs/view/senior-data-engineer-data-quality-governance-at-siriusxm-3619538050,2023-12-17,Dunkirk,United States,Mid senior,Hybrid,"Who We Are
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
How You’ll Make An Impact
In this role, you will be a member of SiriusXM’s Data Quality (DQ) Team within the larger Data Organization, and will be responsible for designing, developing and supporting data quality tools, applications and data marts for use by the DQ Team and other groups.
What You’ll Do
Define technical requirements for DQ tools and applications that support profiling, controls, alerts, KPIs and dashboards for business-critical partner and internal data utilized across SXM’s systems.
Design, develop and improve applications and tools to monitor data quality using data science algorithms and methodologies. This includes cloud-based data pipeline frameworks and workflow orchestration tooling.
Design and develop a data quality analytics framework to standardize the detection of data issues, identify root cause, and drive improvements in data processing procedures.
Develop subject matter expertise in SXM’s data domains (e.g., car/radio lifecycle, customer, marketing, streaming, contact center) to develop advanced quality controls and more easily navigate anomalies affecting a broad cross-functional audience.
Become proficient in using in-house and off-the-shelf DQ tools and applications.
Write documentation to encourage adoption of these tools and support users in their use; support analysts as needed with query/ETL optimization.
Strengthen corporate best practices around data engineering software development processes.
What You’ll Need
BS/MS or above in Computer Science or related field, or relevant experience
5+ years of overall work experience, including demonstrated understanding of the software development life cycle
5+ years of experience in a subscription services or data-driven industry or environment
5+ years of experience developing data ETL pipelines and data tools in Scala and/or Python
Experience with data technologies: e.g., Databricks, MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies: e.g., Kafka, Kafka Connect, KStreams, KSQL, Kinesis, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing: e.g., Google Cloud Platform, Amazon Web Services
Experience with Data Visualization or Data Notebook tools: e.g., Jupyter, Zeppelin, Tableau, etc.
Experience developing and deploying machine learning algorithms
Experience with workflow management systems: e.g., Airflow, Composer, Luigi
Experience with unit and integration testing frameworks
Experience with API design/development: e.g., RPC, REST
Experience with DevOps tools and practices: e.g., Version control, CI/CD, Infrastructure as Code, build/deployment systems, performance monitoring
Experience with data serialization systems: e.g., Avro, Protobuf
Good public speaking and presentation skills
Interpersonal skills and ability to interact and work with staff at all levels
Excellent written and verbal communication skills
Ability to work independently and in a team environment
Attention to detail and organizational skills
Ability to project professionalism over the phone and in person
Ability to handle multiple tasks in a fast-paced environment
Commitment to “internal client” and customer service principles
Willingness to take initiative and to follow through on projects
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $126,000 to $195,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.
Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-05-126
Show more
Show less","Scala, Python, Databricks, MapReduce, HDFS, Hive, Tez, Spark, Sqoop, Kafka, Kafka Connect, KStreams, KSQL, Kinesis, Beam, Flink, Jupyter, Zeppelin, Tableau, RPC, REST, Avro, Protobuf, Airflow, Composer, Luigi, SQL, Machine Learning Algorithms","scala, python, databricks, mapreduce, hdfs, hive, tez, spark, sqoop, kafka, kafka connect, kstreams, ksql, kinesis, beam, flink, jupyter, zeppelin, tableau, rpc, rest, avro, protobuf, airflow, composer, luigi, sql, machine learning algorithms","airflow, avro, beam, composer, databricks, flink, hdfs, hive, jupyter, kafka, kafka connect, kinesis, ksql, kstreams, luigi, machine learning algorithms, mapreduce, protobuf, python, rest, rpc, scala, spark, sql, sqoop, tableau, tez, zeppelin"
Senior Data Engineer - Data Quality/Governance,SiriusXM,"Washington, DC",https://www.linkedin.com/jobs/view/senior-data-engineer-data-quality-governance-at-siriusxm-3619538049,2023-12-17,Dunkirk,United States,Mid senior,Hybrid,"Who We Are
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
How You’ll Make An Impact
In this role, you will be a member of SiriusXM’s Data Quality (DQ) Team within the larger Data Organization, and will be responsible for designing, developing and supporting data quality tools, applications and data marts for use by the DQ Team and other groups.
What You’ll Do
Define technical requirements for DQ tools and applications that support profiling, controls, alerts, KPIs and dashboards for business-critical partner and internal data utilized across SXM’s systems.
Design, develop and improve applications and tools to monitor data quality using data science algorithms and methodologies. This includes cloud-based data pipeline frameworks and workflow orchestration tooling.
Design and develop a data quality analytics framework to standardize the detection of data issues, identify root cause, and drive improvements in data processing procedures.
Develop subject matter expertise in SXM’s data domains (e.g., car/radio lifecycle, customer, marketing, streaming, contact center) to develop advanced quality controls and more easily navigate anomalies affecting a broad cross-functional audience.
Become proficient in using in-house and off-the-shelf DQ tools and applications.
Write documentation to encourage adoption of these tools and support users in their use; support analysts as needed with query/ETL optimization.
Strengthen corporate best practices around data engineering software development processes.
What You’ll Need
BS/MS or above in Computer Science or related field, or relevant experience
5+ years of overall work experience, including demonstrated understanding of the software development life cycle
5+ years of experience in a subscription services or data-driven industry or environment
5+ years of experience developing data ETL pipelines and data tools in Scala and/or Python
Experience with data technologies: e.g., Databricks, MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies: e.g., Kafka, Kafka Connect, KStreams, KSQL, Kinesis, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing: e.g., Google Cloud Platform, Amazon Web Services
Experience with Data Visualization or Data Notebook tools: e.g., Jupyter, Zeppelin, Tableau, etc.
Experience developing and deploying machine learning algorithms
Experience with workflow management systems: e.g., Airflow, Composer, Luigi
Experience with unit and integration testing frameworks
Experience with API design/development: e.g., RPC, REST
Experience with DevOps tools and practices: e.g., Version control, CI/CD, Infrastructure as Code, build/deployment systems, performance monitoring
Experience with data serialization systems: e.g., Avro, Protobuf
Good public speaking and presentation skills
Interpersonal skills and ability to interact and work with staff at all levels
Excellent written and verbal communication skills
Ability to work independently and in a team environment
Attention to detail and organizational skills
Ability to project professionalism over the phone and in person
Ability to handle multiple tasks in a fast-paced environment
Commitment to “internal client” and customer service principles
Willingness to take initiative and to follow through on projects
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $126,000 to $195,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.
Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-05-126
Show more
Show less","Scala, Python, Data engineering, Data pipelines, Data visualization, Data analytics, Machine learning, Workflow management, Unit testing, Integration testing, APIs, DevOps, Data serialization, Cloud computing, Google Cloud Platform, Amazon Web Services, SQL, Databricks, MapReduce, HDFS, Hive, Tez, Spark, Sqoop, Kafka, Kafka Connect, KStreams, KSQL, Kinesis, Beam, Flink, Jupyter, Zeppelin, Tableau","scala, python, data engineering, data pipelines, data visualization, data analytics, machine learning, workflow management, unit testing, integration testing, apis, devops, data serialization, cloud computing, google cloud platform, amazon web services, sql, databricks, mapreduce, hdfs, hive, tez, spark, sqoop, kafka, kafka connect, kstreams, ksql, kinesis, beam, flink, jupyter, zeppelin, tableau","amazon web services, apis, beam, cloud computing, data engineering, data serialization, dataanalytics, databricks, datapipeline, devops, flink, google cloud platform, hdfs, hive, integration testing, jupyter, kafka, kafka connect, kinesis, ksql, kstreams, machine learning, mapreduce, python, scala, spark, sql, sqoop, tableau, tez, unit testing, visualization, workflow management, zeppelin"
Senior Database Engineer,"DMI (Digital Management, LLC)","Washington, DC",https://www.linkedin.com/jobs/view/senior-database-engineer-at-dmi-digital-management-llc-3785826687,2023-12-17,Dunkirk,United States,Mid senior,Hybrid,"DMI is a leading global provider of digital services working at the intersection of public and private sectors. With broad capabilities across IT managed services, cybersecurity, cloud migration and application development, DMI provides on-site and remote support to clients within governments, healthcare, financial services, transportation, manufacturing, and other critical infrastructure sectors. DMI has grown to over 2,100+ employees globally and has been continually recognized as a Top Workplace in both regional and national categories.
About The Opportunity
DMI, LLC is seeking a
Senior Database Engineer
to join our team.
Duties And Responsibilities
Designs, implements, and maintains highly complex databases and their data dictionaries.
Defines logical and physical structure and functional capabilities of databases.
Leads technical database design reviews.
Performs technical review on data elements, preparing reconciliation recommendations and backup materials.
Creates complex SQL statements, where applicable.
Develops backup and recovery procedures, designs database security, and configures database storage devices.
Monitors database performance and tunes databases for optimized performance.
Installs systems software and database engines.
Troubleshoots highly complex database-related problems.
Supports database connectivity for web-based applications.
Plans for upgrades/system release implementation.
Identifies, determines the feasibility of, and analyzes the costs and benefits of alternatives for new database technology.
Provides technical guidance to developers.
May be responsible for project progress and results, and assumes responsibilities of team leader, if required.
Qualifications
Education and Years of Experience:
Bachelor's Degrees in Computer Science or equivalent
Seven Years of relevant experience
Required And Desired Skills/Certifications
Maintaining, monitoring, tuning and optimizing Oracle databases
Provide expertise on Oracle Golden Gate and be able to configure Golden Gate and troubleshoot the issues related to Golden Gate independently.
Min Citizenship Status Required:
Green card/ H1B/EAD Ok (Minimum 3 year residency in USA)
Physical Requirements:
No Physical requirement needed for this position
.
Location:
Remote, US
Working at DMI
Benefits
DMI is a diverse, prosperous, and rewarding place to work. Being part of the DMI family means we care about your wellbeing. As such, we offer a variety of perks and benefits that help meet various interests and needs, while still having the opportunity to work directly with a number of our award-winning, Fortune 1000 clients. The following categories make up your DMI wellbeing:
Community – Blood drives, volunteering opportunities, Holiday parties, summer picnics, Tech Chef, Octoberfest just to name a few ways DMI comes together as a community
Convenience/Concierge - Virtual visits through health insurance, pet insurance, commuter benefits, discount tickets for movies, travel, and many other items to provide convenience
Development – Annual performance management, continuing education, and tuition assistance, internal job opportunities along with career enrichment and advancement to help each employee with their professional and personal development
Financial – Generous 401k match for both pre-tax and post-tax (ROTH) contributions along with financial wellness education, EAP, Life Insurance and Disability help provide financial stability for each DMI employee
Recognition – Great achievements do not go unnoticed by DMI through Annual Awards ceremony, service anniversaries, peer-to-peer acknowledgment through Spotlight, employee referral
Wellness – Healthcare benefits, Wellness programs provide employees with several wellness options
Employees are valued for their talents and contributions. We all take pride in helping our customers achieve their goals, which in turn contributes to the overall success of the company. The company does and will take affirmative action to employ and advance in employment individuals with disabilities and protected veterans, and to treat qualified individuals without discrimination based on their physical or mental disability or veteran status. DMI is an Equal Opportunity Employer Minority/Female/Veterans/Disability. DMI maintains a drug-free workplace.
***************** No Agencies Please *****************
Applicants selected may be subject to a government security investigation and must meet eligibility requirements for access to classified information. US citizenship may be required for some positions.
Job ID:
2023-26018
Show more
Show less","Oracle Golden Gate, SQL, Oracle, Database design, Database security, Database storage, Troubleshooting, Data elements, Backup and recovery, Physical and logical structure, Functional capabilities, Technical review, Reconciliation, Backup materials, Database performance, Tuning, Webbased applications, Database connectivity, System release implementation, Data dictionaries, Systems software, Database engines, Alternative analysis, Team leadership","oracle golden gate, sql, oracle, database design, database security, database storage, troubleshooting, data elements, backup and recovery, physical and logical structure, functional capabilities, technical review, reconciliation, backup materials, database performance, tuning, webbased applications, database connectivity, system release implementation, data dictionaries, systems software, database engines, alternative analysis, team leadership","alternative analysis, backup and recovery, backup materials, data dictionaries, data elements, database connectivity, database design, database engines, database performance, database security, database storage, functional capabilities, oracle, oracle golden gate, physical and logical structure, reconciliation, sql, system release implementation, systems software, team leadership, technical review, troubleshooting, tuning, webbased applications"
Sr Data Engineer - R&D Connected Data,BioSpace,"Washington, DC",https://www.linkedin.com/jobs/view/sr-data-engineer-r-d-connected-data-at-biospace-3788806651,2023-12-17,Dunkirk,United States,Mid senior,Hybrid,"HOW MIGHT YOU DEFY IMAGINATION?
Youve worked hard to become the professional you are today and are now ready to take the next step in your career. How will you put your skills, experience and passion to work toward your goals? At Amgen, our shared missionto serve patientsdrives all that we do. It is key to our becoming one of the worlds leading biotechnology companies, reaching over 10 million patients worldwide. Come do your best work alongside other innovative, driven professionals in this meaningful role.
Senior Data Engineer - R&D Connected Data
Live
What You Will Do
Lets do this. Lets change the world. In this vital role you will be responsible for managing and optimizing the company's data infrastructure and architecture. Your expertise in data engineering, big data technologies, and data manipulation will contribute to the effective storage, processing, and utilization of large-scale data sets. In partnership with enterprise data platform teams, functional technology teams and data scientists, the Sr Data Engineer R&D Connected Data will be part of a team delivering key multi-functional R&D data initiatives that are contributing to the advancement of a connected data vision, including the Enterprise Data Fabric (EDF). The Data Fabric is composed of data domains which are based on formal ontologies and presented to data scientists and applications in both a graph and relational formats. The Enterprise Data Fabric serves as an information backbone to accelerate the ability of Amgen business leaders to understand and optimize business processes through analytic systems. The enablement of a connected data ecosystem is a key dependency for the realization of critical business strategies across Research and Development, such as AI/ML and advancing precision medicine.
Key Responsibilities
Work with the Product Owner, Release Train Engineer and Scrum master to plan and execute assigned work
Work with Connected Data Architects to refine and optimize the technical environment for reuse, agility and performance.
Ensure that the data requirements for applications, data scientists, and multi-functional use cases are met.
Ensure that performance, reliability of the system is high.
Win
What We Expect Of You
We are all different, yet we all use our unique contributions to serve patients. The Data Engineer professional we seek is a critical thinker with these qualifications.
Basic Qualifications:
Doctorate degree
OR
Masters degree and 3 years of information systems experience
Or
Bachelors degree and 5 years of information systems experience
Or
Associates degree and 10 years of information systems experience
Or
High school diploma / GED and 12 years of information systems experience
Preferred Qualifications
3+ years of experience in the data warehouse space
3+ years of experience with one or more programming languages, Python, Scala, or Java.
5+ Experience architecting and building ETL pipelines; Hands-on experience with SQL
Experience with Semantic Layer technologies
Experience with data modeling, performance tuning, and experience on relational and graph databases.
Experience working with Apache Spark, Apache Airflow
Hands-on development experience with Databricks
Experience with Software engineering best-practices, including but not limited to version control, CI/CD, automated testing
Experience with AWS services: EC2, S3, EMR, RDS, Redshift/Spectrum, Lambda, Glue, Athena, API gateway, and design patterns (Containers, Serverless, Docker, etc.)
Thrive
What You Can Expect Of Us
As we work to develop treatments that take care of others, we also work to care for our teammates professional and personal growth and well-being.
The annual base salary range for this opportunity in the U.S. is
$114,797 to $162,296.
In Addition To The Base Salary, Amgen Offers a Total Rewards Plan Comprising Health And Welfare Plans For Staff And Eligible Dependents, Financial Plans With Opportunities To Save Towards Retirement Or Other Goals, Work/life Balance, And Career Development Opportunities Including
Comprehensive employee benefits package, including a Retirement and Savings Plan with generous company contributions, group medical, dental and vision coverage, life and disability insurance, and flexible spending accounts.
A discretionary annual bonus program, or for field sales representatives, a sales-based incentive plan
Stock-based long-term incentives
Award-winning time-off plans and bi-annual company-wide shutdowns
Flexible work models, including remote work arrangements, where possible
Apply now
for a career that defies imagination
Objects in your future are closer than they appear. Join us.
careers.amgen.com
Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Show more
Show less","Data engineering, Big data technologies, Data manipulation, Data warehouse, Python, Scala, Java, ETL pipelines, SQL, Semantic Layer technologies, Data modeling, Performance tuning, Relational databases, Graph databases, Apache Spark, Apache Airflow, Databricks, Software engineering bestpractices, Version control, CI/CD, Automated testing, AWS services, EC2, S3, EMR, RDS, Redshift/Spectrum, Lambda, Glue, Athena, API gateway, Containers, Serverless, Docker","data engineering, big data technologies, data manipulation, data warehouse, python, scala, java, etl pipelines, sql, semantic layer technologies, data modeling, performance tuning, relational databases, graph databases, apache spark, apache airflow, databricks, software engineering bestpractices, version control, cicd, automated testing, aws services, ec2, s3, emr, rds, redshiftspectrum, lambda, glue, athena, api gateway, containers, serverless, docker","apache airflow, apache spark, api gateway, athena, automated testing, aws services, big data technologies, cicd, containers, data engineering, data manipulation, databricks, datamodeling, datawarehouse, docker, ec2, emr, etl pipelines, glue, graph databases, java, lambda, performance tuning, python, rds, redshiftspectrum, relational databases, s3, scala, semantic layer technologies, serverless, software engineering bestpractices, sql, version control"
"Senior Associate, Data Analyst",Precision,"Washington, DC",https://www.linkedin.com/jobs/view/senior-associate-data-analyst-at-precision-3777352366,2023-12-17,Dunkirk,United States,Mid senior,Hybrid,"We’re Precision Strategies – an integrated strategy and marketing agency working with companies, causes, and candidates that change people’s minds and move them to action. We’ve won presidential elections, defined Fortune 50 brands, shaped public opinion, and created movements from the ground up.  Don’t just break through. Break new ground.
Precision is committed to building a diverse team that will positively and authentically impact the communities we serve. Centering our strategies around the authentic voices and cultures of the communities we are engaging with is paramount to our work – and our work can only be as inclusive as our team is representative. We strongly encourage women, Black, Latino, Hispanic, AAPI, Indigenous people, LGBTQ+, gender expansive or GNC folks, people of all ages, disabled people, and veterans to apply.
We are looking for someone who thrives in a fast-paced environment, wants to work in a team-oriented environment, and is inclined to seek out challenges and opportunities.
What You Will Be Doing
Develop easily repeatable, automated processes for analyzing data and developing insights.
Build automated data transformations to translate raw data into a format fit for ongoing analysis.
Analyze data and implement program insights, optimizations, and experiments
Assist in client account management on a day-to-day basis
Execute work on many different projects that are on time and precise
Work with colleagues and clients to build reports and develop actionable insights from data
Distilling results, insights, and findings into impactful, strategic memos and presentations
Craft and present stellar client presentations and memos
Demonstrate strong data visualization, data wrangling, and analytics skills
Collaborate cross-functionally with other departments
What We Are Looking For
2-3+ years of relevant experience in an in-house, government, agency, or consultancy environment
Meticulous attention to detail
Excellent written and verbal communication skills
Creative, energetic, and possesses an entrepreneurial spirit
Proficiency in Microsoft Office or Google Suite, including Excel/Sheets
Advanced knowledge of Excel, specifically pivot tables, data visualization, and formulae
Experience in social network analysis and data wrangling or aggregation
Experience with social listening, media monitoring, and advanced metrics analysis
Ability to synthesize research from several sources or conduct literature reviews
Plan and execute of work across a diverse range of client projects
Be a master of platform analytics (Instagram, TikTok, etc) and tools such as Brandwatch, Affinio, GWI, etc, and tell us what else we might need to uncover consumer truths.
Analyze crucial campaign data and optimize our comms with actionable recommendations.
Act as primary client lead for measurement, analytics, reporting, and goal setting.
Contribute and help expand agency POV, capabilities, and offerings within the analytics and measurement space.
Extract insights from different sources to help bolster our planning and creative
Segment audience data, sketching out consumer profiles - helping us get to know who we are talking to.
Identify opportunities for us to jump into with our brands - often realtime
Visualize data and storytelling, translating data into actionable recommendations and building into presentations.
Improve reporting and the way we report on campaign success. You speak in KPIs and ERs.
Help to identify trends and consumer behavioral shifts.
Provide fresh thinking and creative ways to solve client challenges.
Be on top of the changing social, digital, influencer, and measurement landscapes—looking for untapped opportunities and thinking to bring
Nice To Haves
Experience guiding junior talent
Proficient in SQL, Python, or R
Experience with voter file tools
Experience in administering surveys and analyzing survey data
What We Will Give You
Competitive salaries
Annual bonus opportunity
New business bonuses
Flexible hybrid remote/in-office plan
Retirement plan with automatic 3% safe-harbor contribution from Precision
Healthcare coverage - medical, dental and vision, with 90% of costs covered by Precision
Paid vacation and sick leave
Paid parental leave
Professional development stipend
Due to the pandemic, we required all employees to be vaccinated against COVID-19.  We are currently working in a hybrid model and require staff to work in the office 10 workdays per month. We are seeking applicants who live in the Washington, DC area.
Show more
Show less","Data visualization, Data wrangling, Analytics, Pivot tables, Formulas, Social network analysis, Social listening, Media monitoring, Advanced metrics analysis, Literature reviews, Platform analytics, Brandwatch, Affinio, GWI, Campaign data, Comms, Measurement, Reporting, Goal setting, Trend analysis, Storytelling, Datadriven recommendations, Presentation skills, KPIs, Earned reach, Consumer behavior shifts, Critical thinking, Problem solving, Python, R, SQL, Voter file tools, Survey administration, Survey data analysis","data visualization, data wrangling, analytics, pivot tables, formulas, social network analysis, social listening, media monitoring, advanced metrics analysis, literature reviews, platform analytics, brandwatch, affinio, gwi, campaign data, comms, measurement, reporting, goal setting, trend analysis, storytelling, datadriven recommendations, presentation skills, kpis, earned reach, consumer behavior shifts, critical thinking, problem solving, python, r, sql, voter file tools, survey administration, survey data analysis","advanced metrics analysis, affinio, analytics, brandwatch, campaign data, comms, consumer behavior shifts, critical thinking, data wrangling, datadriven recommendations, earned reach, formulas, goal setting, gwi, kpis, literature reviews, measurement, media monitoring, pivot tables, platform analytics, presentation skills, problem solving, python, r, reporting, social listening, social network analysis, sql, storytelling, survey administration, survey data analysis, trend analysis, visualization, voter file tools"
Senior Data Analyst/Data Scientist - TS/SCI Required,LMI,"Washington, DC",https://www.linkedin.com/jobs/view/senior-data-analyst-data-scientist-ts-sci-required-at-lmi-3666799774,2023-12-17,Dunkirk,United States,Mid senior,Hybrid,"Overview
LMI is seeking a Senior Data Analyst/Data Scientist to support our Intelligence Community client.
LMI is a consultancy dedicated to powering a future-ready, high-performing government, drawing from expertise in digital and analytic solutions, logistics, and management advisory services. We deliver integrated capabilities that incorporate emerging technologies and are tailored to customers’ unique mission needs, backed by objective research and data analysis. Founded in 1961 to help the Department of Defense resolve complex logistics management challenges, LMI continues to enable growth and transformation, enhance operational readiness and resiliency, and ensure mission success for federal civilian and defense agencies.
LMI has been named a 2022 #TopWorkplace in the United States by Top Workplaces! We are honored to be recognized as a company that values a people-centered culture, and we are grateful to our employees for making this possible!
Responsibilities
Responsible for providing advanced data analytics and predictive strategic workforce planning via on-demand, intuitive web-based capabilities and reporting.
Collaborate with an integrated team of data scientists, data analysts, and HR functional SMEs to understand business processes, develop analytical requirements, identify key data elements, adhere to data protection guidelines, and interpret and communicate analytic results
Understand and analyze complex and organization-specific datasets.
Define and develop techniques to integrate, consolidate, and structure data for analytical use.
Support the maturation of data quality
Support the transformation of businesses process through automation
Transform data and analysis into informative visualizations and interactive dashboards using open-source and commercially available visualization and dashboard tools
Provide regular detailed reporting to management and customers
Advise on the interpretation and use of data analysis products, dashboards and reports to non-technical customers
Assist the development of junior data scientists through technical mentoring, code review, and other assistance.
The data scientist will need the ability to work in teams and independently
Identify, propose, and assist in the development of future analytics products based on the data pipeline to assist customers in gaining value from the data products.
Required
Qualifications
Bachelor’s degree in science, engineering, mathematics, computer science, economics, data analytics, or other related business or quantitative discipline
7+ years experience in data or business analytics
High level of competency with data science/analysis coding languages like SQL, Python, or R
Experience developing dashboards using Tableau, Qlik, Power BI, RShiny, plotly, or d3.js
Experience with data science methods related to data architecture, data munging, data and feature engineering, and predictive analytics.
Self-starter with the vision to independently identify opportunities for improvement through data analytics
A true team player who maintains a positive attitude in a dynamic environment
Excellent communication skills, written and oral
Proven ability to work with business customers and technical teams
Highly organized and able to manage multiple projects simultaneously
Excellent customer relationship management skills
Innovative problem-solving skills and ability to thrive in a fast-paced environment
Ability to work on client site in the NCR in Washington, D.C.; Reston, VA; or College Park, MD
This position requires an active security clearance at the TS/SCI level with the ability/willingness to receive a polygraph. Current polygraph preferred.
Desired
Masters degree or higher
Experience using python libraries to build machine learning models like regression models and decision trees
Experience with version control software like Git
Management consulting experience desired
Experience working with unstructured text and natural language processing
Experience with deploying and maintaining models in production
Previous experience with people analytics
Show more
Show less","Data Analytics, Predictive Analytics, Data Visualization, Tableau, Qlik, Power BI, RShiny, Plotly, D3.js, SQL, Python, R, Data Architecture, Data Munging, Data Engineering, Feature Engineering, Git, Machine Learning, Regression Models, Decision Trees, Unstructured Text, Natural Language Processing, Model Deployment, People Analytics","data analytics, predictive analytics, data visualization, tableau, qlik, power bi, rshiny, plotly, d3js, sql, python, r, data architecture, data munging, data engineering, feature engineering, git, machine learning, regression models, decision trees, unstructured text, natural language processing, model deployment, people analytics","d3js, data architecture, data engineering, data munging, dataanalytics, decision trees, feature engineering, git, machine learning, model deployment, natural language processing, people analytics, plotly, powerbi, predictive analytics, python, qlik, r, regression models, rshiny, sql, tableau, unstructured text, visualization"
Lead Data Engineer - Active Secret Clearance - Hybrid with Security Clearance,ClearanceJobs,"Crystal City, VA",https://www.linkedin.com/jobs/view/lead-data-engineer-active-secret-clearance-hybrid-with-security-clearance-at-clearancejobs-3770711576,2023-12-17,Dunkirk,United States,Mid senior,Hybrid,"LEAD DATA ENGINEER - ACTIVE SECRET CLEARANCE - HYBRID ALTA IT Services has a direct hire opening for a Lead Data Engineer with an Active Secret clearance to support a leading DOD program. This is a hybrid position, 2-3 days a week onsite in Crystal City, VA. As a Lead Data Engineer, you will lead a team of data engineers and collaborate closely with data scientists, analysts, and other stakeholders to ensure the efficient flow of data through the organization's systems. You will be responsible for designing, building, and maintaining data pipelines, databases, and data warehousing solutions. Your role will involve managing a team, making architectural decisions, and ensuring data quality, security, and scalability. RESPONSIBILITIES
Team Leadership:
Lead and mentor a team of data engineers, providing guidance and support in their daily tasks and career development.
Collaborate with cross-functional teams, including data scientists, analysts, and product managers, to understand data requirements and prioritize projects.
Data Pipeline Development:
Design and develop data pipelines to ingest, transform, and load data from various sources into data warehouses or data lakes.
Ensure data pipelines are reliable, scalable, and maintainable.
Data Warehousing:
Manage and optimize data warehousing solutions, such as data warehouses or data lakes, to store and organize large volumes of data efficiently.
Design and implement schema structures to support analytics and reporting needs.
Data Integration:
Integrate third-party data sources and APIs into the data ecosystem to enrich and enhance internal data.
Ensure data integration processes are robust and handle data discrepancies gracefully.
Data Quality and Governance:
Implement data quality checks, validation, and monitoring processes to ensure data accuracy and consistency.
Establish data governance policies and procedures to maintain data security and compliance.
Performance Tuning:
Monitor system performance and optimize data pipelines and databases for efficiency and speed.
Troubleshoot and resolve performance issues as they arise.
Security and Compliance:
Implement and maintain data security measures, including access controls and encryption, to protect sensitive data.
Ensure compliance with data privacy regulations (e.g., GDPR, HIPAA) and industry best practices.
Documentation and Collaboration:
Create and maintain documentation for data pipelines, processes, and architecture.
Collaborate with data stakeholders to understand their needs and provide data engineering solutions.
Technology Evaluation:
Stay current with data engineering technologies and trends and evaluate new tools and platforms for potential adoption.
Project Management:
Plan and execute data engineering projects, including resource allocation, timelines, and deliverables. QUALIFICATIONS
Bachelor's degree in computer science, data engineering, or a related field or 5-7 years of relevant work experience.
Proven experience in data engineering, with a strong background in data pipeline development and database management.
Proficiency in programming languages like Python, Java, or Scala.
Experience with big data technologies (e.g., Hadoop, Spark, Kafka) and cloud platforms (e.g., AWS, Azure, GCP).
Strong SQL skills and expertise in data modeling and schema design.
Excellent problem-solving skills and the ability to troubleshoot complex data issues.
Leadership and team management skills.
Knowledge of data governance, security, and compliance best practices.
Effective communication and collaboration skills. SALARY: $188,000 Range annually, plus benefits. For consideration, please contact Melissa McNally via
System One, and its subsidiaries including Joulé, ALTA IT Services, CM Access, and MOUNTAIN, LTD., are leaders in delivering outsourced services and workforce solutions across North America. We help clients get work done more efficiently and economically, without compromising quality. System One not only serves as a valued partner for our clients, but we offer eligible employees health and welfare benefits coverage options including medical, dental, vision, spending accounts, life insurance, voluntary plans, as well as participation in a 401(k) plan. System One is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, age, national origin, disability, family care or medical leave status, genetic information, veteran status, marital status, or any other characteristic protected by applicable federal, state, or local law.
Show more
Show less","Data Engineering, Data Pipelines, Data Warehousing, Data Integration, Data Quality, Data Governance, Performance Tuning, Data Security, Python, Java, Scala, Hadoop, Spark, Kafka, SQL, Data Modeling, Leadership, Team Management, Data Governance, Data Compliance, AWS, Azure, GCP","data engineering, data pipelines, data warehousing, data integration, data quality, data governance, performance tuning, data security, python, java, scala, hadoop, spark, kafka, sql, data modeling, leadership, team management, data governance, data compliance, aws, azure, gcp","aws, azure, data compliance, data engineering, data governance, data integration, data quality, data security, datamodeling, datapipeline, datawarehouse, gcp, hadoop, java, kafka, leadership, performance tuning, python, scala, spark, sql, team management"
Specialist Data Developer- EN,CN,"Edmonton, Alberta, Canada",https://ca.linkedin.com/jobs/view/specialist-data-developer-en-at-cn-3780032511,2023-12-17,Edmonton, Canada,Associate,Hybrid,"At CN, we work together to move our company—and North America—forward. Be part of our Information & Technology (I&T) team, a critical piece of the engine that keeps us in motion. From enterprise architecture to operational technology, our teams use the agile methodology to automate and digitize our railroad ensuring our operations run optimally and safely and our employees can focus on value-added tasks. You will be able to develop your skills and career in our close-knit, safety-focused culture working together as ONE TEAM. The careers we offer are meaningful because the work we do matters. Join us!
Job Summary
The Specialist, Data Developer is responsible for building, handling, and optimizing data pipelines. The role moves these data pipelines effectively into production for key data and analytics consumers, shapes the enterprise Data as a Service (DaaS) model and delivers on Information and Technology (I&T) business models. Moreover, the incumbent develops best practices and optimizes data pipelines to deliver products and services aligned with business expectations. The position plays a pivotal role in operationalizing data and analytics initiatives, defining and building CN’s data integration and DaaS platform roadmap.
Main Responsibilities
Data Development
Ensure optimal data delivery architecture and processes are consistent throughout ongoing projects
Optimize CN’s data architecture to support the next generation of products and data initiatives
Build, handle, and optimize data pipelines, moving them effectively into production for key data and analytics consumers
Build data and domain event models, implement business rules, and scalable data pipelines
Ensure compliance with data governance and security requirements while creating, improving, and operationalizing integrated and reusable data pipelines
Enable faster data access, integrate data reuse, and improve time-to-solution for data and analytics initiatives
Integrate analytics and data science results with business processes
Promote effective data management practices
Collaborate with Data Science, Reporting, Analytics and other Development teams to build data pipelines, infrastructure and tooling to support business initiatives
Design and develop Exact, Transfer and Load (ETL) pipelines using multiple sources of data in various formats and deploy them to achieve a high-level of reliability, scalability, and security
Data Design
Collaborate with stakeholders and architects to model data landscape and define secure data exchange approaches
Meet with stakeholders to identify fit-for-purpose within CN’s existing data ecosystem and deliver options and agile solutions
Design and develop processing pipelines that ingest data into Data Hubs
Provide day-to-day support and technical expertise to both technical and
non-technical teams
Participate in building data development expertise and framework
Translate business needs into technical requirements
Data Operations and Quality Assurance
Use Agile methodologies and development practices to streamline project delivery aligned with goals, timelines, and budgets and for code reviews and testing to develop and deliver data pipelines
Build monitoring and debugging tools to analyze data pipelines
Help unify software development and operations seamlessly, efficiently, and cost effectively
Improve software quality, automate processes, and accelerate software releases
Develop and implement test plans and scripts for various data quality processes
Maintain manual and automated test scripts
Working Conditions
Due to the nature of the role, the incumbent must be able to meet tight deadlines, handle pressure, and stress.
Requirements
Experience
Data Development
Minimum 5 years overall work experience
Minimum 3 years of experience in a Data Development role, working in different data management disciplines including data integration, modelling, optimization, and quality
Experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental or multi-departmental data management and analytics initiative
Experience in translating business requirements into advanced data models able to fulfill Analysts and Data Scientists’ requirements
Experience working in an Agile team environment
Education/Certification/Designation
Bachelor's Degree in Computer Science, Electrical Development or Software Development
Google or Azure Data Development certification*
Any designation for these above would be considered as an asset
Competencies
Inspires others with impactful communications and adapts to the audience through speech and writing
Applies analytical thinking
Innovates through problem solving
Knows the business and stays current with industry trends to elevate expertise and work
Demonstrates organizational abilities
Collaborates with key internal stakeholders to enable higher productivity
Works independently with little supervision
Technical Skills/Knowledge
Knowledge of Scala, Java or Python
Knowledge of software development best practices such as code reviews, testing frameworks, maintainability, and readability
Expertise with Databricks Delta Lake
Knowledge of Structured Query Language (SQL) and Non-Structured Query Language (NoSQL) technologies and fluent in writing, executing, and optimizing SQL queries
Knowledge of Big Data technologies and cloud platforms such as Databricks, Apache Spark, Azure Data Factory, Azure Data Explorer, Azure Data Lake, Google BigQuery, Google Dataproc, Google Cloud Data Fusion, Google Dataflow, Google Cloud Composer, Google, Dataprep, Google Dataplex, Google BigLake, Google Vertex
Knowledge of event-driven architecture (e.g., Pub-Sub, Kafka, Message Queuing (MQ), Message Queuing Telemetry Transport (MQTT), Advanced Message Queuing Protocol (AMQP), Event Hub, Logstash)
About CN
CN is a world-class transportation leader and trade-enabler. Essential to the economy, to the customers, and to the communities it serves, CN safely transports more than 300 million tons of natural resources, manufactured products, and finished goods throughout North America every year. As the only railroad connecting Canada’s Eastern and Western coasts with the Southern tip of the U.S. through a 19,500 mile rail network, CN and its affiliates have been contributing to community prosperity and sustainable trade since 1919. CN is committed to programs supporting social responsibility and environmental stewardship. At CN, we work as ONE TEAM, focused on safety, sustainability and our customers, providing operational and supply chain excellence to deliver results.
For internal candidates, note that the grade level of the position will depend on the employee's experience.
CN is an employment equity employer and we encourage all qualified candidates to apply. We thank all applicants for their interest, however, only candidates under consideration will be contacted. Please monitor your email on a regular basis, as communication is primarily made through email.
Show more
Show less","Data Development, Data Design, Data Operations, Quality Assurance, Data Pipelines, Data Modeling, Data Integration, Data Analytics, Data Management, Agile Methodology, Cloud Platforms, Big Data, Machine Learning, Artificial Intelligence, Python, Java, Scala, SQL, NoSQL, Databricks, Apache Spark, Azure Data Factory, Azure Data Explorer, Azure Data Lake, Google BigQuery, Google Dataproc, Google Cloud Data Fusion, Google Dataflow, Google Cloud Composer, Google Dataprep, Google Dataplex, Google BigLake, Google Vertex, PubSub, Kafka, Message Queuing (MQ), Message Queuing Telemetry Transport (MQTT), Advanced Message Queuing Protocol (AMQP), Event Hub, Logstash","data development, data design, data operations, quality assurance, data pipelines, data modeling, data integration, data analytics, data management, agile methodology, cloud platforms, big data, machine learning, artificial intelligence, python, java, scala, sql, nosql, databricks, apache spark, azure data factory, azure data explorer, azure data lake, google bigquery, google dataproc, google cloud data fusion, google dataflow, google cloud composer, google dataprep, google dataplex, google biglake, google vertex, pubsub, kafka, message queuing mq, message queuing telemetry transport mqtt, advanced message queuing protocol amqp, event hub, logstash","advanced message queuing protocol amqp, agile methodology, apache spark, artificial intelligence, azure data explorer, azure data factory, azure data lake, big data, cloud platforms, data design, data development, data integration, data management, data operations, dataanalytics, databricks, datamodeling, datapipeline, event hub, google biglake, google bigquery, google cloud composer, google cloud data fusion, google dataflow, google dataplex, google dataprep, google dataproc, google vertex, java, kafka, logstash, machine learning, message queuing mq, message queuing telemetry transport mqtt, nosql, pubsub, python, quality assurance, scala, sql"
Associate Data Analyst,Tripadvisor,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/associate-data-analyst-at-tripadvisor-3783821446,2023-12-17,Reading, United Kingdom,Associate,Onsite,"Do you have a passion for data and analytics?
Who We Are
Bokun is an innovative travel tech company with headquarters in Reykjavik and Oxford. As we continue to experience a period of exciting growth, we are seeking talented individuals to join our cross-functional team. As part of the Tripadvisor family, we are passionate about helping businesses in the travel industry succeed, and we're looking for a motivated individual who shares our mission.
Who We Are Looking For
We're seeking a self-motivated individual who is enthusiastic about using data to create excellent software and drive business decisions. Our ideal candidate is a fast learner with a curious mindset, who can work independently and collaboratively to analyze data and uncover insights that will help us create more value for our company and our customers. If you're passionate about data analysis and believe in its ability to inform intelligent decision-making you will fit right in at Bokun.
You should currently be present in Reykjavik, Iceland, where our office is at.
What You Will Do
You will join a dynamic and agile team in a quickly changing environment. You will work in a diverse environment with
Product and Engineering on creating outstanding software
Sales and Marketing on optimize and improve business performance
Management team on making better-informed decisions and develop strategies for the company's growth
Some of the tasks that we know you will be doing:
Collaborate with Product and other internal stakeholders to understand and define what data is essential to measure the impact of the work we are doing
Partner with Engineering on implementing the tracking needed to measure impact
Become an expert in translating what Product and Engineering teams are working onto into specific analysis, metrics or reports that yield actionable insights
Investigate data visualization needs of different teams and translate obtained insight into actions and present these to stakeholders
Enable stakeholders to access data and monitor key business metrics by being an expert in building and maintaining dashboards and reports using Mixpanel and Amplitude
Create and present reports and ad-hoc analysis to management
Partner with Product and Engineering teams on A/B test design and analyzing test results to measure the impact of changes, and make recommendations to stakeholders
Education & Experience
An education focused on Statistics, Math, Data Science or similar technical disciplines
Excellent problem-solver & strong analytical skills
Technical aptitude to learn new tools, technologies, and methodologies
Ability to translate complex analysis findings into a clear narrative and actionable insights
Excellent communication skills, with the ability to listen, and collaborate
Product and/or Marketing Analytics experience is a plus
Knowledge of Mixpanel and Amplitude is a plus
Knowledge in Python, R or other analytical programming language is a plus
Knowledge of the SQL language is a plus
Experience with an AB testing tool such as Optimizely, Google Optimize, etc. is a plus
Knowledge of the travel industry is a plus
Show more
Show less","Statistics, Math, Data Science, Problemsolving, Analytical skills, Technical aptitude, Data analysis, Data visualization, Data interpretation, Mixpanel, Amplitude, Python, R, SQL, A/B testing, Optimizely, Google Optimize, Travel industry knowledge","statistics, math, data science, problemsolving, analytical skills, technical aptitude, data analysis, data visualization, data interpretation, mixpanel, amplitude, python, r, sql, ab testing, optimizely, google optimize, travel industry knowledge","ab testing, amplitude, analytical skills, data interpretation, data science, dataanalytics, google optimize, math, mixpanel, optimizely, problemsolving, python, r, sql, statistics, technical aptitude, travel industry knowledge, visualization"
Data Analyst,SmartChoice International Limited,"Newbury, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-smartchoice-international-limited-3785199667,2023-12-17,Reading, United Kingdom,Associate,Onsite,"Job Title : Data Analyst
Job type : Fix Team Or Permanent
Job Location : Newbury, UK
*Active Security Clearance is Mandatory BPSS*
The role is part of the new Digital Operations & Automation organization, aiming to streamline and digitize the company. As a key player, the position focuses on making the organization data-driven by placing telemetry at the core of business processes. The role involves leading a small team of Data Analysts and actively contributing as a senior Data Analyst in one of the squads. The objective is to ensure business functions utilize data for agile decision-making, supporting the elimination and simplification of end-to-end processes across the UK. Collaboration with business experts, collecting and analyzing telemetry/data sets, and deriving insights for business benefits are crucial responsibilities. The role requires strong communication skills, analytics proficiency, and experience with various analytics tools. The position is suited for a self-driven individual passionate about solving complex business problems through data insights, and it offers an opportunity to play a pivotal role in driving the company's strategy and operational transformation in the UK.
Key accountabilities and decision ownership:
· Manipulate, analyze, and interpret complex operational data sets.
· Mine and analyze large datasets, draw valid inferences, and present them successfully to stakeholders.
· Identify areas to increase efficiency and automation of processes.
· Produce and track key performance indicators and reporting processes.
· Assessing the data's performance on an ongoing basis
· Identifying and mitigating risks involved with the transference of data.
· Work with Business Process Architect and Process Owner by in creating data models and interpretation of the data.
· Work with the Benefit tracking manager to ensure the evidence is available to justify the improved efficiency and productivity. Core competencies, knowledge, and experience:
· 3+ years of experience in data modelling, data cleansing and data enrichment techniques
· Excellent numerical and analytical skills
· Strong presentation and communication skills
· Ability to test hypotheses from raw data sets and draw meaningful conclusions.
· Experience in statistical methodologies and data analysis techniques.
· Capacity to develop and document procedures and workflows.
· Ability to produce clear graphical representations and data visualizations.
· Exceptional attention to detail and problem-solving skills
Must have technical / professional qualifications:
· You have studied Statistics, Physics, Maths, Engineering, Econometrics, Macroeconomics or any other relevant field
· Proficient in R, Python and SQL, Excel, SAS, SPSS as well as business intelligence platforms such as Tableau, D3, Qlik Sense
· Proven abilities in mathematics, computer science and analysis to handle vast amounts of data
· Understanding of data modelling and data based solutions
· Certifications – Six Sigma, Lean, ITIL, Agile desirable Location
About Smartchoice International:
Smartchoice International is your premier staffing agency and is committed to assembling the ideal team for your business while seamlessly connecting top-tier talent with exceptional opportunities. Boasting over two decades of profound experience in the staffing domain, we have established an eminent reputation as the go-to agency for enterprises in pursuit of unparalleled service.
At Smartchoice International, we recognize that each company's staffing requisites are unique. With this in mind, we are driven to offer tailored solutions precisely aligned with your distinct demands. Our operational prowess encompasses adaptability, flexibility, and efficiency, empowering us to maintain absolute control throughout the process - streamlining everything from interview preparation to comprehensive post-hire support.
https://www.smartchoice-international.com/
How to Apply:
Please submit your resume and a brief cover letter detailing your relevant experience to j.gurnani@smartchoice-international.com.
Join our dynamic team and contribute to shaping the future of financial technology through your expertise and dedication.
Note: Only shortlisted candidates will be contacted for further steps in the selection process.
Show more
Show less","R, Python, SQL, Excel, SAS, SPSS, Tableau, D3, Qlik Sense, Six Sigma, Lean, ITIL, Agile, Data modeling, Data cleansing, Data enrichment, Data visualization, Statistics, Hypothesis testing, Machine learning, Artificial intelligence, Data analysis, Data interpretation, Business intelligence, Data mining, Data warehousing, Data governance, Data security, Cloud computing, Big data","r, python, sql, excel, sas, spss, tableau, d3, qlik sense, six sigma, lean, itil, agile, data modeling, data cleansing, data enrichment, data visualization, statistics, hypothesis testing, machine learning, artificial intelligence, data analysis, data interpretation, business intelligence, data mining, data warehousing, data governance, data security, cloud computing, big data","agile, artificial intelligence, big data, business intelligence, cloud computing, d3, data enrichment, data governance, data interpretation, data mining, data security, dataanalytics, datacleaning, datamodeling, datawarehouse, excel, hypothesis testing, itil, lean, machine learning, python, qlik sense, r, sas, six sigma, spss, sql, statistics, tableau, visualization"
Data Engineer Consultant,Viable Data,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-consultant-at-viable-data-3772728975,2023-12-17,Reading, United Kingdom,Associate,Remote,"Viable Data is an innovative technology, data and UX consultancy, delivering excellence through our projects and providing our people with a supportive culture and opportunities for growth and continuous learning.
We thrive on the challenge of working across different projects, user needs and technologies and our teams and people live this every day. Our people-first approach and culture is central to our growing success as a consultancy.
We are an all-inclusive equal opportunities employer and proudly celebrate diversity. If you thrive on challenge, have a passion to learn and make a difference, and enjoy being part of a growing multidisciplinary team, look no further and start your Viable career, now.
About The Role
As a Data Engineer at Viable Data, you will ensure that data is in the right state and format, ready to implement data schemas and models. You will make sure that the data is ready and available for data mining purposes.
You will join one of our high-performing consulting teams delivering exceptional digital products and services that directly contribute to the delivery of UK policy, economic growth and security.
This role is largely remote with occasional business essential travel.
Due to the nature of work, you must be willing to undergo and be capable of achieving SC security clearance
Requirements
Key responsibilities:
Develop analysis and design solutions using one or more from: SQL and NoSQL technologies, data streaming, logging and monitoring tools, BI and Data Warehousing solutions and ETL and migration technologies.
Communicate with stakeholders to understand client data and processes to identify solutions.
Create advanced analytics and statistics techniques to business problems.
Effectively communicate insights from a dataset using narratives and visualizations.
Skills and experience needed:
Proven experience of working with large volumes of data, applying tools such as SAS, R, Python, SQL, PowerBI, Tableau, Looker
Ability to work across different data, cloud and messaging technology stacks.
Written and verbal communication abilities
A thorough understanding of one or more from: Master Data Management, Data Lineage analysis, Metadata Management, Data Quality assessment and management, Data Transformation and migration
Proven analysis skills - experience of various data analysis methods
People skills – empathetic, great listener and have a natural curiosity to understand people, their motivations and thought processes.
Benefits
As well as providing a great place to work that has an amazing culture and the opportunity to work on excellent projects where you will really make a difference, we have a whole host of additional employee benefits.
Our benefits package includes:
Employee Assistance Programme (EAP)
Flexible hours and supportive of remote working
A Green Energy incentive
Buy or sell annual leave
Volunteering day
A discount portal through Perkbox
Compensation package:
£45-55k + Company pension contributions
Annual bonus based on company performance
25 days leave + Bank holidays
5 days dedicated training allowance, with individual budget
Personal Development programme, with 6-month review cycles
Choice of company laptop (MacBook or Windows)
Show more
Show less","Data Engineering, SQL, NoSQL, Data Streaming, Logging, Monitoring, BI, Data Warehousing, ETL, Migration, SAS, R, Python, PowerBI, Tableau, Looker, Master Data Management, Data Lineage Analysis, Metadata Management, Data Quality Assessment, Data Quality Management, Data Transformation","data engineering, sql, nosql, data streaming, logging, monitoring, bi, data warehousing, etl, migration, sas, r, python, powerbi, tableau, looker, master data management, data lineage analysis, metadata management, data quality assessment, data quality management, data transformation","bi, data engineering, data lineage analysis, data quality assessment, data quality management, data streaming, data transformation, datawarehouse, etl, logging, looker, master data management, metadata management, migration, monitoring, nosql, powerbi, python, r, sas, sql, tableau"
Data Engineer,Infineum,"Abingdon, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-infineum-3780623027,2023-12-17,Reading, United Kingdom,Associate,Hybrid,"Position Summary:
As part of the global Digital Technology team, the Enterprise Data Governance Specialist plays a crucial role in ensuring the accuracy, consistency, and reliability of data within an organization.
They work closely with data stakeholders, including data engineers, data scientists, business analysts, and various business units, to identify, assess, and rectify data quality issues. This role is pivotal in maintaining data integrity and supporting data-driven decision-making processes.
Enterprise Data Governance Specialist will serve as a bridge between business stakeholders and technology teams: analysing and defining business needs and requirements, identifying root causes related to data quality issues, proposing solutions to close issues, designing new processes with business stakeholders and/or Digital Technology teams.
They support efforts to define data standards, policies, and procedures for overall Enterprise Data Governance and Data Quality Management.
The successful candidate needs to combine a diverse set of technical skills (spanning Enterprise Data Governance, Master Data Management, business processes, SAP applications, and data base architectures) and the ability to work within small virtual teams collaborating with colleagues across the organisation including business data owners and IT global team members.
Key Outputs:
Data Profiling and Assessment:
Analyze and assess the quality of data sources to identify inconsistencies, errors, and anomalies.
Develop and maintain data quality metrics and standards.
Data Cleansing and Enrichment:
Collaborate with data engineers and other stakeholders to cleanse and enrich data to meet quality standards.
Implement data quality rules and transformations.
Quality Assurance:
Create and execute data quality tests and validation procedures.
Monitor data quality on an ongoing basis and report issues to relevant teams.
Business Requirements Gathering:
Collaborate with business stakeholders to elicit, analyze, and document business requirements.
Conduct interviews, surveys, and workshops to gather information.
Data Analysis:
Analyze data to identify trends, patterns, and insights that can inform business decisions.
Create data-driven reports and dashboards.
Business Process Mapping:
Document current and future-state business processes.
Identify opportunities for process improvement and automation.
Stakeholder Communication:
Facilitate communication between business stakeholders and technology teams.
Present findings, data quality reports, and recommendations to stakeholders.
Change Management:
Assist in the development of change management plans for implementing new processes or technologies.
Support training and adoption efforts.
Support training and adoption efforts.
A successful candidate is likely to have skills/experience in some or all the below areas:
Demonstrable exposure to Data Governance & Management practices and tools
Data quality management
Master Data Management
Data catalogs including profiling, modelling, integration, lineage analysis.
Data security, privacy, compliance
Strong knowledge of data management principles and best practices
Business or technical experience with below is a bonus:
SAP ERP and Database environments: SAP ECC, SAP S/4HANA, SAP Datasphere
Data cataloging and analytics: Collibra, Talend, Snowflake, PowerBI
Strong Communication, Collaboration, Analytical and Problem-solving capabilities
Creative decision-making thought process
Excellent written and oral communication skills (English) especially in the context of business process design
Ability to work within or lead small agile global teams with minimal supervision
Ability to adapt to changing business needs and technology landscapes
Good planning and time management
Other:
Change Control Management
Vendor Management
Qualifications:
Bachelor's degree in Information Management, Data Science, Business, or a related field.
Relevant certifications in data management (e.g., CDMP, DAMA) are a plus.
Show more
Show less","Data Governance, Master Data Management, Business Processes, SAP applications, Database Architectures, Data Profiling, Data Assessment, Data Cleansing, Data Enrichment, Data Quality Rules, Data Quality Transformations, Data Quality Tests, Data Validation Procedures, Business Requirements Gathering, Data Analysis, Business Process Mapping, Stakeholder Communication, Change Management, Data Catalogs, Data Profiling, Data Modelling, Data Integration, Data Lineage Analysis, Data Security, Data Privacy, Data Compliance, SAP ERP, SAP ECC, SAP S/4HANA, SAP Datasphere, Collibra, Talend, Snowflake, PowerBI","data governance, master data management, business processes, sap applications, database architectures, data profiling, data assessment, data cleansing, data enrichment, data quality rules, data quality transformations, data quality tests, data validation procedures, business requirements gathering, data analysis, business process mapping, stakeholder communication, change management, data catalogs, data profiling, data modelling, data integration, data lineage analysis, data security, data privacy, data compliance, sap erp, sap ecc, sap s4hana, sap datasphere, collibra, talend, snowflake, powerbi","business process mapping, business processes, business requirements gathering, change management, collibra, data assessment, data catalogs, data compliance, data enrichment, data governance, data integration, data lineage analysis, data modelling, data privacy, data profiling, data quality rules, data quality tests, data quality transformations, data security, data validation procedures, dataanalytics, database architectures, datacleaning, master data management, powerbi, sap applications, sap datasphere, sap ecc, sap erp, sap s4hana, snowflake, stakeholder communication, talend"
Senior Data Engineer (Azure),Edgewell Personal Care,"High Wycombe, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-azure-at-edgewell-personal-care-3781911487,2023-12-17,Reading, United Kingdom,Associate,Hybrid,"Senior Data Engineer (Azure)
Fixed Term Contract (12 months), Full time, Hybrid (two days on site)
Location: UK (High Wycombe), HP13 6DG
Let's Talk About You
You’re ambitious, but you want a career with purpose—we love that. You’re up for a challenge and ready to write the next chapter of your career story with a great group of encouraging teammates. You want to make your mark and let your personality shine. Well, if you’re interested in our Senior Data Engineer (12 month fixed term contract) role, this might be your next step! This is a great choice for an ambitious doer, maker and innovator, especially since it requires someone who enjoys working with data analytics. We’re looking for someone with strong Snowflake/Azure data experience to join Edgewell Personal Care – a global consumer goods business with a portfolio of over 25 brands including Wilkinson Sword, Wet Ones, Hawaiian Tropic, Bulldog Skincare , and Cremo just to name a few.
Let's Get Down To Business
Reporting to the Senior Analyst (based in the US), you will be part of our Information Technology Business Intelligence team and responsible for the technical deployment of our Snowflake/Azure platforms for our customers. There will be lots of variety in this role ranging from data architecture, ETL, security, performance analysis, and data analytics, so that you could have the right insight to make the connection between a customer's specific business problem and a Snowflake/Azure solution. Our aim is to have a satisfied business community with robust BI reporting, effective self-service, stable and secure BI environment based on a modern technology stack. If there’s one thing you’ll get here at Edgewell, it’s a collaborative group full of likeable, diverse and nurturing colleagues. They’re an inspiring bunch. While your experience is of course important, we’re all about making useful things joyful, so we love when our prospective candidates are passionate about this too.
Show Us What You've Got
We are looking for the following skills and experience:
Significant experience in Azure data platform
Deep understanding of data warehouse architecture and ETL tools.
Have worked with BI presentation layer tools, for example Power BI.
Designed and implemented effective practices for business intelligence team including SDLC, data integration, and change control.
Created data tools for analytics and data scientist team members.
Experience improving data governance and data security practices in BI.
Experience with implementing CI/CD.
Ability to work with business and functional teams.
Flexibility to work autonomously across different time zones with diverse teammates
Desirable skills: Machine Learning, Snowflake, Python, R, Java, Scala, BW Hana, SQL and Business Objects
Fluent in English
What We Offer
A diverse and a nurturing ‘can do’ collaborative culture with the following benefits:
Attractive salary and benefits, great learning and career development, meeting free Fridays, onsite car park, dress for the day, and great teammates!
Now Take the Next Step
If you’re looking forward to joining us to Make Useful Things Joyful, then we’re looking forward to hearing from you. Please apply online with an up-to-date CV (in English) along with your salary expectations. Alternatively, feel free to pass onto your teammates who may be interested.
For Information About This Joyful Company, Visit www.edgewell.com
#INT
Edgewell is an equal opportunity employer. We do all we can to create a collaborative and diverse global team, where good ideas can thrive, and our colleagues can learn and lead. We prohibit discrimination based on age, color, disability, marital or parental status, national origin, race, religion, sex, sexual orientation, gender identity, veteran status or any legally protected status in accordance with applicable federal, state and local laws. We listen deeply and speak directly to create an environment that’s open to difference. We aim to bring joy to not only the products we create and the people we serve, but our colleagues across the globe too.
Show more
Show less","Azure, Snowflake, ETL tools, Power BI, SDLC, Data integration, Change control, Data governance, Data security practices, CI/CD, Machine Learning, Python, R, Java, Scala, BW Hana, SQL, Business Objects","azure, snowflake, etl tools, power bi, sdlc, data integration, change control, data governance, data security practices, cicd, machine learning, python, r, java, scala, bw hana, sql, business objects","azure, business objects, bw hana, change control, cicd, data governance, data integration, data security practices, etl tools, java, machine learning, powerbi, python, r, scala, sdlc, snowflake, sql"
Data Analyst,gategroup,"Sunbury-on-Thames, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-gategroup-3769251443,2023-12-17,Reading, United Kingdom,Associate,Hybrid,"Job Overview:
As part of the Operations Planning & Controlling team, the Data Analyst is responsible for ensuring collation, data entry and maintenance of data on the Gate Gourmet database system (SACS). You will support the OP&C team on all the activities involving data gathering, analysis and report creation for the business.
Main Duties & Responsibilities
Support OP&C projects within the business as required
Interpret data, analyze results using statistical techniques and provide ongoing reports
Development, maintenance and improvement of Databases, analytics and reports
Data entry and management into SACS and other tools
Acquire data from data sources and maintain databases
Identify, analyze, and interpret trends or patterns in complex data sets
Data housekeeping, filter and ""clean"" data
Work with management to prioritize business and information needs
Identify and propose new process improvement opportunities
Support reconciliation process
Attend internal and external customer meetings to ensure data accuracy and alignment
Support on the collection of data for OP&C team KPIs
Support CPC calculation and analysis
Qualifications:
Education:
Educated to a degree level preferred
Work Experience:
Relevant experience in a similar role managing large volumes of data and analysis / reporting
Knowledge of Airline catering operations preferred.
Knowledge of SACS or similar ERP preferred
Other Skills: (Certification, Licenses and Registration)
Demonstrated experience in handling large data sets and relational databases
Advanced working knowledge of Microsoft Office Programs is essential.
Effective verbal and written communication skills in English,
Ability to exhibit a high level of accuracy and attention to detail.
Ability to work efficiently using own initiative to resolve issues in a pressurised environment.
Ability to work under pressure and to tight timescales, multi-tasking and prioritising as appropriate.
Must demonstrate integrity, confidentiality, and professionalism always.
Full UK driving license is preferable.
About the Company:
Gate Gourmet is the world's largest independent provider of catering services for airlines and is the core business behind gategroup, whose eleven associated brands offer customers a comprehensive scope of products and services for virtually any on-board need. We provide more than 200 million meals a year to our 270-plus customers at some 120 airport locations around the globe. Working as part of a dedicated and passionate team of people you will be rewarded with free meals whilst on shift and free on-site parking and free uniform. In order to ensure your success in the role, full training will be provided.
All applicants must have the right to work in the UK, undertake a CRC (Criminal Record Check) and provide 5 years of checkable referencing history. If you share our values of excellence, passion, integrity and accountability, don't miss out on this opportunity to join our team. Apply TODAY.
Show more
Show less","Data Analysis, Statistical Analysis, Database Development, Data Entry, Data Management, Data Acquisition, Data Interpretation, Data Cleaning, Data Reconciliation, Data Visualization, Microsoft Office Suite, ERP Systems, SACS System, CPC Calculation, Airline Catering Operations, Data Integrity, Data Confidentiality, Data Professionalism, Communication Skills","data analysis, statistical analysis, database development, data entry, data management, data acquisition, data interpretation, data cleaning, data reconciliation, data visualization, microsoft office suite, erp systems, sacs system, cpc calculation, airline catering operations, data integrity, data confidentiality, data professionalism, communication skills","airline catering operations, communication skills, cpc calculation, data acquisition, data cleaning, data confidentiality, data entry, data integrity, data interpretation, data management, data professionalism, data reconciliation, dataanalytics, database development, erp systems, microsoft office suite, sacs system, statistical analysis, visualization"
Marketing Data Analyst,Westcon-Comstor,"Reading, England, United Kingdom",https://uk.linkedin.com/jobs/view/marketing-data-analyst-at-westcon-comstor-3780708375,2023-12-17,Reading, United Kingdom,Associate,Hybrid,"As a Technology Go-to-Market Data Analyst at Westcon-Comstor, you will play a pivotal role in the planning and execution of data-driven intelligent marketing campaigns to build a pipeline of distribution-led opportunities for vendors and partners.
You will work closely with cross-functional teams, including vendor management, marketing, sales, and data & analytics to ensure the successful design and cross-functional execution of data-driven, intelligent marketing campaigns. You will be responsible for understanding and documenting vendors' go-to-market strategies that encompass product positioning, market analysis, and launch strategies; translating those strategies into specific data-driven campaign criteria; and partnering with local marketing teams to ensure successful campaign execution.
This is a great opportunity to use your IT Channel experience, technical expertise and commercial knowledge you may have already gained at a global distibutor or vendor. Being comfortable with data and GTM strategies in our industry is important to be successful in this role.
We offer a competitive package and great benefits such as 25 days holiday, BUPA Healthcare and many other incentives like EV salary sacrifice scheme.
ABOUT THE ROLE
1. Market Analysis: Conduct in-depth market research to understand industry trends, customer needs, and competitive landscapes for the vendors and products that Westcon distributes (security & networking focus).
2. Product Positioning: Understand, document and communicate the unique value proposition and positioning of our vendors' security and networking technology products, translating those value statements into clear end user (company) personas with the highest propensity to buy.
3. Intelligent Marketing Strategy: Collaborate with vendor management and marketing to articulate a comprehensive go-to-market / marketing strategy, including target end user and partner profiles as well as marketing execution plans and vendor funding to grow a distribution-led business pipeline.
4. Cross-Functional Collaboration: Collaborate with vendor management, marketing, sales, and data & analytics to ensure alignment, execution and attribution.
5. Business Enablement: Develop sales & marketing tools and training materials to equip the sales and marketing team with the knowledge and resources needed to more effectively leverage our intelligent marketing leads.
6. Performance Metrics: Define and track key performance indicators (KPIs) to measure the success of go-to-market initiatives and use data to iterate and improve strategies.
7. Customer Feedback: Gather feedback from customers and other stakeholders to continuously improve intelligent marketing strategies.
ABOUT YOU
* Bachelor's degree in business, marketing, analytics or a related field.
* Demonstrable experience in product marketing, marketing analytics or go-to-market planning within the IT industry.
* High level understanding of technology security areas including network, infrastructure, cloud security.
* Strong analytical and market research skills.
* Excellent communication and interpersonal abilities.
* Ability to work effectively in cross-functional teams.
* Proficiency in project management and budget management.
* Knowledge of industry-standard tools and platforms for market analysis and go-to-market planning.
Join a growing global business
Westcon-Comstor is a leading name in IT distribution with US$4.35 billion in global revenues. Our job is to link technology vendors to our distribution partners who resell software and solutions to business and other customers.
This is an exciting time to join our expanding UK and Ireland operation, which serves the EMEA region. We have ambitious plans and huge future potential - you will be ideally placed to grow your career in a dynamic yet supportive culture.
Show more
Show less","Datadriven marketing, Intelligent marketing campaigns, GotoMarket strategies, Product positioning, Market analysis, Vendor funding, Sales and marketing tools, Training materials, Key performance indicators (KPIs), Customer feedback, Bachelor's degree in business marketing or related field, Product marketing, Marketing analytics, Gotomarket planning, Technology security areas, Network security, Infrastructure security, Cloud security, Analytical and market research skills, Communication and interpersonal abilities, Crossfunctional teams, Project management, Budget management, Industrystandard tools, Platforms for market analysis, Gotomarket planning","datadriven marketing, intelligent marketing campaigns, gotomarket strategies, product positioning, market analysis, vendor funding, sales and marketing tools, training materials, key performance indicators kpis, customer feedback, bachelors degree in business marketing or related field, product marketing, marketing analytics, gotomarket planning, technology security areas, network security, infrastructure security, cloud security, analytical and market research skills, communication and interpersonal abilities, crossfunctional teams, project management, budget management, industrystandard tools, platforms for market analysis, gotomarket planning","analytical and market research skills, bachelors degree in business marketing or related field, budget management, cloud security, communication and interpersonal abilities, crossfunctional teams, customer feedback, datadriven marketing, gotomarket planning, gotomarket strategies, industrystandard tools, infrastructure security, intelligent marketing campaigns, key performance indicators kpis, market analysis, marketing analytics, network security, platforms for market analysis, product marketing, product positioning, project management, sales and marketing tools, technology security areas, training materials, vendor funding"
Data Engineer Contract,Energy Jobline,"Thorpe, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-contract-at-energy-jobline-3773343260,2023-12-17,Reading, United Kingdom,Mid senior,Onsite,"We are excited to present a job opportunity for a data-savvy individual who is passionate about using technology to drive business efficiencies. As a Data Integration Specialist, you will be working alongside the PMO Analyst and Change Manager to develop and realise reporting and integration opportunities from multiple data sources. You will be responsible for optimising existing and developing new and enhanced reports by utilizing API's and Big Data Management.
Additionally, you will be expected to improve manual system workflows by utilizing data automation techniques.
To excel in this role, you must have experience using POST and GET Rest APIs, as well as third-party data integration experience. Knowledge of Microsoft Power Query is also required. Experience using SQL databases is an advantage.
As part of our commitment to our employees' well-being, we offer an Employee Assistance Program and weekly fruit delivery to the office. We also believe in team building and host ad-hoc team events.
Please note that this contract will initially be for 2 months and you must have your own transport due to location.
If you share our values and passion for technology, we encourage you to apply for this exciting opportunity
Show more
Show less","Data Integration, POST, GET, Rest API, Thirdparty data integration, Microsoft Power Query, SQL, API, Big Data Management, Data automation techniques","data integration, post, get, rest api, thirdparty data integration, microsoft power query, sql, api, big data management, data automation techniques","api, big data management, data automation techniques, data integration, get, microsoft power query, post, rest api, sql, thirdparty data integration"
Data Cabling Engineer - Security Cleared,Digital Waffle,"Reading, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-security-cleared-at-digital-waffle-3732300004,2023-12-17,Reading, United Kingdom,Mid senior,Onsite,"Position:
Data Cabling Engineer (Temp/Contract/Freelance)
Department:
Information Technology / Network Infrastructure
Location:
M4 Corridor - Bristol, Swindon, Reading, Heathrow
Job Summary:
We are seeking a skilled and dedicated Data Cabling Engineer to join our Information Technology team. The successful candidate will need to be either be SC or DV security cleared, and will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables within metal trunking and containment to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Hold either a 'Security Cleared' or 'Develop Vetted' level of security clearance
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Security Cleared Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Jake on jake@digitalwaffle.co.uk
Show more
Show less","Data Cabling, Network Infrastructure, Cat6, Cat6a, Copper Cables, Cable installation, Termination, Labeling, Testing, Troubleshooting, TIA/EIA, ISO/IEC, RJ45, Cable Stripping, Cutting Tools, Crimping Tool, Punchdown Tool, Cable Tester, Cable Certifier, Tone Generator, Probe, Cable Ties, Velcro Straps, Cable Clips, Mounts, Tape Measure, Level, Power Drill, Screwdrivers, Wall Anchors, Safety Glasses, Work Gloves, Tool Bag, Pen, Notepad, Mobile Device, Cable Layout, Termination, Labeling, Documentation, Steeltoed Boots, Hard Hat, Cable Fish Tape, Rods, Cable Lubricant, Cable Toner","data cabling, network infrastructure, cat6, cat6a, copper cables, cable installation, termination, labeling, testing, troubleshooting, tiaeia, isoiec, rj45, cable stripping, cutting tools, crimping tool, punchdown tool, cable tester, cable certifier, tone generator, probe, cable ties, velcro straps, cable clips, mounts, tape measure, level, power drill, screwdrivers, wall anchors, safety glasses, work gloves, tool bag, pen, notepad, mobile device, cable layout, termination, labeling, documentation, steeltoed boots, hard hat, cable fish tape, rods, cable lubricant, cable toner","cable certifier, cable clips, cable fish tape, cable installation, cable layout, cable lubricant, cable stripping, cable tester, cable ties, cable toner, cat6, cat6a, copper cables, crimping tool, cutting tools, data cabling, documentation, hard hat, isoiec, labeling, level, mobile device, mounts, network infrastructure, notepad, pen, power drill, probe, punchdown tool, rj45, rods, safety glasses, screwdrivers, steeltoed boots, tape measure, termination, testing, tiaeia, tone generator, tool bag, troubleshooting, velcro straps, wall anchors, work gloves"
Lead Data Engineer (SQL),Energy Jobline,"Camberley, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-sql-at-energy-jobline-3778203507,2023-12-17,Reading, United Kingdom,Mid senior,Onsite,"Lead Data Engineer - SQL
Preston or Frimley - 1 day p/w in the office
£45,000 - £55,000 + bonus
The Data Service team is responsible to source and ingest data into a Shared Data Warehouse, to provide enterprise wide insights that are both operational and strategic and impact across all levels of the business. The team develop and configure re-useable ETL and ELT patterns and packages, following data warehouse principles and best practises to provide data with high quality and integrity, within the security framework and policy's.
In this role you will be a key component of the Data & Analytics Hub, responsible for determining how we operate as a team and as to which technologies are utilised to maximise efficiency and performance across the entire platform.
What You'll Be Doing
Develop, implement and optimise complex T-SQL scripts independently to support data ingestion processes and reporting
Monitoring and do proactive validations and verifications of ETL development and monitoring of data loads into the data warehouse
Maintaining existing ETL solutions and identifying improvements
Analyse existing SQL queries for performance improvements
Conducting peer code reviews and UAT validation and create well defined test cases
Adhering to company source control, validation and promotion policy
Working within change control environment
Writing and Review technical documentation
Your Skills And Experiences
Previous experience in a similar role Excellent in T-SQL programming and experience using SQL Server Management Studio and Microsoft BI stack (SSIS/SSRS)
Excellent understanding of relational databases and normalisation
Solid experience of working with Visual Studio to develop, debug and publish SSIS packages and reports and to proactive provide training to juniors and peers
Desirable
Knowledge of PowerShell scripting
Appreciation of data warehousing architecture techniques, best practised and able to deploy packages for production to be at industrial strength
Show more
Show less","TSQL scripting, SQL Server Management Studio, SSIS, SSRS, Relational databases, Normalisation, Visual Studio, SSIS packages, Data warehousing architecture, PowerShell scripting, ETL, ELT, UAT, Source control, Change control, Technical documentation, Software development, Software testing","tsql scripting, sql server management studio, ssis, ssrs, relational databases, normalisation, visual studio, ssis packages, data warehousing architecture, powershell scripting, etl, elt, uat, source control, change control, technical documentation, software development, software testing","change control, data warehousing architecture, elt, etl, normalisation, powershell scripting, relational databases, software development, software testing, source control, sql server management studio, ssis, ssis packages, ssrs, technical documentation, tsql scripting, uat, visual studio"
Security Cleared Data Cabling Engineer,Digital Waffle,"Reading, England, United Kingdom",https://uk.linkedin.com/jobs/view/security-cleared-data-cabling-engineer-at-digital-waffle-3737918392,2023-12-17,Reading, United Kingdom,Mid senior,Onsite,"Position:
Data Cabling Engineer (Temp/Contract/Freelance)
Department:
Information Technology / Network Infrastructure
Location:
M4 Corridor - Bristol, Swindon, Reading, Heathrow
Job Summary:
We are seeking a skilled and dedicated Data Cabling Engineer to join our Information Technology team. The successful candidate will need to be either be SC or DV security cleared, and will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables within metal trunking and containment to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Hold either a 'Security Cleared' or 'Develop Vetted' level of security clearance
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Security Cleared Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Joe on joe@digitalwaffle.co
Show more
Show less","Data Cabling, Cat6, Cat6a, Copper Cabling, Fiber Optic Cabling, Cable Installation, Cable Routing, Cable Termination, Cable Testing, Cable Certification, Cable Labeling, TIA/EIA Standards, ISO/IEC Standards, Network Infrastructure, Network Expansion, Network Topologies, Network Protocols, Network Equipment, RJ45 Connectors, Patch Panels, Keystone Jacks, Cable Stripping, Cable Cutting, PunchDown Tool, Cable Tester, Cable Certifier, Tone Generator, Probe, Cable Labels, Markers, Label Printer, Measuring Tape, Level, Cable Ties, Velcro Straps, Cable Clips, Mounts, Power Drill, Bits, Screwdrivers, Wall Anchors, Safety Glasses, Work Gloves, Tool Bag, Pen, Notepad, Mobile Device, SteelToed Boots, Hard Hat","data cabling, cat6, cat6a, copper cabling, fiber optic cabling, cable installation, cable routing, cable termination, cable testing, cable certification, cable labeling, tiaeia standards, isoiec standards, network infrastructure, network expansion, network topologies, network protocols, network equipment, rj45 connectors, patch panels, keystone jacks, cable stripping, cable cutting, punchdown tool, cable tester, cable certifier, tone generator, probe, cable labels, markers, label printer, measuring tape, level, cable ties, velcro straps, cable clips, mounts, power drill, bits, screwdrivers, wall anchors, safety glasses, work gloves, tool bag, pen, notepad, mobile device, steeltoed boots, hard hat","bits, cable certification, cable certifier, cable clips, cable cutting, cable installation, cable labeling, cable labels, cable routing, cable stripping, cable termination, cable tester, cable testing, cable ties, cat6, cat6a, copper cabling, data cabling, fiber optic cabling, hard hat, isoiec standards, keystone jacks, label printer, level, markers, measuring tape, mobile device, mounts, network equipment, network expansion, network infrastructure, network protocols, network topologies, notepad, patch panels, pen, power drill, probe, punchdown tool, rj45 connectors, safety glasses, screwdrivers, steeltoed boots, tiaeia standards, tone generator, tool bag, velcro straps, wall anchors, work gloves"
T-SQL Data Engineer,Hydrogen Group,"Frimley, England, United Kingdom",https://uk.linkedin.com/jobs/view/t-sql-data-engineer-at-hydrogen-group-3774842217,2023-12-17,Reading, United Kingdom,Mid senior,Onsite,"Job title:
T-SQL Data Engineer
Location:
Preston or Frimley - Hybrid 2/3 day weekly split.
Salary:
Up to £51K + benefits
What You'll Be Doing
Develop, implement and optimise complex T-SQL scripts independently to support data ingestion processes and reporting
Monitoring and do proactive validations and verifications of ETL development and monitoring of data loads into the data warehouse
Maintaining existing ETL solutions and identifying improvements
Analyse existing SQL queries for performance improvements
Conducting peer code reviews and UAT validation and create well defined test cases
Adhering to company source control, validation and promotion policy
Working within change control environment
Writing and Review technical documentation
Essential
Your skills and experiences:
Previous experience in a similar role Excellent in T-SQL programming and experience using SQL Server Management Studio and Microsoft BI stack (SSIS/SSRS)
Excellent understanding of relational databases and normalisation
Solid experience of working with Visual Studio to develop, debug and publish SSIS packages and reports and to proactive provide training to juniors and peers
Desirable
Knowledge of PowerShell scripting
Appreciation of data warehousing architecture techniques, best practised and able to deploy packages for production to be at industrial strength
Show more
Show less","TSQL, SQL Server Management Studio, Microsoft BI stack (SSIS/SSRS), Relational databases, Normalization, Visual Studio, SSIS packages, Reports, PowerShell scripting, Data warehousing architecture, Best practices, Industrial strength","tsql, sql server management studio, microsoft bi stack ssisssrs, relational databases, normalization, visual studio, ssis packages, reports, powershell scripting, data warehousing architecture, best practices, industrial strength","best practices, data warehousing architecture, industrial strength, microsoft bi stack ssisssrs, normalization, powershell scripting, relational databases, reports, sql server management studio, ssis packages, tsql, visual studio"
Data Engineer - Arcadis Gen,Arcadis,"Guildford, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-arcadis-gen-at-arcadis-3770622531,2023-12-17,Reading, United Kingdom,Mid senior,Onsite,"Role: Data Engineer
Location: UK, Remote
As an Arcadian, you already help us deliver world leading sustainable design, engineering, and consultancy solutions for natural and built assets. You are part of our global business comprising 36,000 people, in over 70 countries, dedicated to improving quality of life.
Everyone has an important role to play. With the power of many curious minds, together we can continue to solve the world’s most complex challenges and deliver more impact together.
Role description:
The Data Engineer is responsible for migrating the data systems, delivery of the technical leadership in data migration, support of data migration strategies, tools, design and build. As well as formulating various methods to transform raw data into useful data systems. Overall, you’ll strive for efficiency by aligning data systems with business goals. This includes data ETL and modelling pipelines, hosting environments, automated testing and deployment, information storage, and retrieval and support enabling infrastructure across multiple client projects.
The Data Engineer will execute migration projects and may be a coach to other Junior Engineers within the Service Engineering Group (SEG). They will work together with Data architects, Data Consultants, and QA Consultants to enable technical solutions throughout the project lifecycle, including assessments, tool evaluations, demonstrations, requirements gathering, results presentation.
Role accountabilities:
Support and own the end-to-end migration journey on cloud-based products.
Technical delivery of ETL data migration with framework agreed.
Execute data migration runs.
Perform data migration using Snowflake, Dbt and Airflow or similar toolset.
Analyse and organize raw data; build data systems and pipelines.
Assess the compatibility with your cloud destination.
Ensure that the design fully meets the client’s needs.
Manage outputs and quality throughout delivery of a project ensuring all work packages align to the project scope.
Qualifications & Experience:
The ideal candidate will have a background in computer science and/or data engineering, ideally within a data migration consultancy or physical asset owning company context. They will have good leadership, communication, and organisational skills. They will have a strong understanding of how to deploy and scale machine learning and advance analytics techniques in a modern cloud environment. A keen interest in the latest technologies is important, to help ensure that the product (as well as Gen in general) stays on the cutting edge.
More importantly than these technical skills, however, we are looking for someone to be a data solution leader, pushing forward the boundaries of what we do, and the standards expected of the wider Gen data architecture. The successful candidate will need to be resilient and maintain a positive attitude when faced with challenges, be an excellent problem-solver, and be happy to be out of their comfort zone. They will need to have excellent communication skills, not just at a technical level, but also with more business-focused colleagues. They will not shy away from a challenge and will stand up and be counted. They will never be satisfied, and always look to find the next improvement in what Gen does, and how we do it.
We are looking for someone who possesses:
Must haves:
Experience in cloud-based data migration tools and validations
Experience using AWS and/or Azure
Experience using the following tools: Snowflake & DBT
Excellent understanding of SQL
Familiarity with DevOps processes/best practices
Part of assessment, tool options and data migration enablement
Experience of delivery of client facing projects, ideally in a data migration consultancy
Knowledge of programming languages (e.g., Java and Python)
You have experience in the use of automated testing, CI/CD, the use and creation of APIs and of virtual package environments to ensure repeatability of code.
Demonstrable skills in database management
Desirable:
Experience using Airflow
Experience using Azure Data Factory
Continue your career journey as an Arcadian.
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You do meaningful work, and no matter where your next role in Arcadis takes you, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark; on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We believe that by working together diverse people with different experiences develop the most innovative ideas. Equality, diversity and inclusion is at the heart of how we improve quality of life and we work closely with our people across six ED&I Workstreams: Age, Disability, Faith, Gender, LGBT+ and Race. A diverse and skilled workforce is essential to our success.
Show more
Show less","Data migration, Cloudbased data migration, ETL, Data modelling, Data pipelines, Snowflake, Dbt, Airflow, AWS, Azure, SQL, DevOps, Java, Python, Automated testing, CI/CD, APIs, Virtual package environments, Database management","data migration, cloudbased data migration, etl, data modelling, data pipelines, snowflake, dbt, airflow, aws, azure, sql, devops, java, python, automated testing, cicd, apis, virtual package environments, database management","airflow, apis, automated testing, aws, azure, cicd, cloudbased data migration, data migration, data modelling, database management, datapipeline, dbt, devops, etl, java, python, snowflake, sql, virtual package environments"
Data Engineer - Arcadis Gen,Arcadis,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-arcadis-gen-at-arcadis-3770619560,2023-12-17,Reading, United Kingdom,Mid senior,Onsite,"Role: Data Engineer
Location: UK, Remote
As an Arcadian, you already help us deliver world leading sustainable design, engineering, and consultancy solutions for natural and built assets. You are part of our global business comprising 36,000 people, in over 70 countries, dedicated to improving quality of life.
Everyone has an important role to play. With the power of many curious minds, together we can continue to solve the world’s most complex challenges and deliver more impact together.
Role description:
The Data Engineer is responsible for migrating the data systems, delivery of the technical leadership in data migration, support of data migration strategies, tools, design and build. As well as formulating various methods to transform raw data into useful data systems. Overall, you’ll strive for efficiency by aligning data systems with business goals. This includes data ETL and modelling pipelines, hosting environments, automated testing and deployment, information storage, and retrieval and support enabling infrastructure across multiple client projects.
The Data Engineer will execute migration projects and may be a coach to other Junior Engineers within the Service Engineering Group (SEG). They will work together with Data architects, Data Consultants, and QA Consultants to enable technical solutions throughout the project lifecycle, including assessments, tool evaluations, demonstrations, requirements gathering, results presentation.
Role accountabilities:
Support and own the end-to-end migration journey on cloud-based products.
Technical delivery of ETL data migration with framework agreed.
Execute data migration runs.
Perform data migration using Snowflake, Dbt and Airflow or similar toolset.
Analyse and organize raw data; build data systems and pipelines.
Assess the compatibility with your cloud destination.
Ensure that the design fully meets the client’s needs.
Manage outputs and quality throughout delivery of a project ensuring all work packages align to the project scope.
Qualifications & Experience:
The ideal candidate will have a background in computer science and/or data engineering, ideally within a data migration consultancy or physical asset owning company context. They will have good leadership, communication, and organisational skills. They will have a strong understanding of how to deploy and scale machine learning and advance analytics techniques in a modern cloud environment. A keen interest in the latest technologies is important, to help ensure that the product (as well as Gen in general) stays on the cutting edge.
More importantly than these technical skills, however, we are looking for someone to be a data solution leader, pushing forward the boundaries of what we do, and the standards expected of the wider Gen data architecture. The successful candidate will need to be resilient and maintain a positive attitude when faced with challenges, be an excellent problem-solver, and be happy to be out of their comfort zone. They will need to have excellent communication skills, not just at a technical level, but also with more business-focused colleagues. They will not shy away from a challenge and will stand up and be counted. They will never be satisfied, and always look to find the next improvement in what Gen does, and how we do it.
We are looking for someone who possesses:
Must haves:
Experience in cloud-based data migration tools and validations
Experience using AWS and/or Azure
Experience using the following tools: Snowflake & DBT
Excellent understanding of SQL
Familiarity with DevOps processes/best practices
Part of assessment, tool options and data migration enablement
Experience of delivery of client facing projects, ideally in a data migration consultancy
Knowledge of programming languages (e.g., Java and Python)
You have experience in the use of automated testing, CI/CD, the use and creation of APIs and of virtual package environments to ensure repeatability of code.
Demonstrable skills in database management
Desirable:
Experience using Airflow
Experience using Azure Data Factory
Continue your career journey as an Arcadian.
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You do meaningful work, and no matter where your next role in Arcadis takes you, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark; on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We believe that by working together diverse people with different experiences develop the most innovative ideas. Equality, diversity and inclusion is at the heart of how we improve quality of life and we work closely with our people across six ED&I Workstreams: Age, Disability, Faith, Gender, LGBT+ and Race. A diverse and skilled workforce is essential to our success.
Show more
Show less","Data Engineer, Cloudbased data migration, AWS, Azure, Snowflake, DBT, SQL, DevOps, Data migration frameworks, Automated testing, CI/CD, APIs, Virtual package environments, Database management, Machine learning, Advance analytics, ETL pipelines, Data modelling, Data storage, Data retrieval, Python, Java","data engineer, cloudbased data migration, aws, azure, snowflake, dbt, sql, devops, data migration frameworks, automated testing, cicd, apis, virtual package environments, database management, machine learning, advance analytics, etl pipelines, data modelling, data storage, data retrieval, python, java","advance analytics, apis, automated testing, aws, azure, cicd, cloudbased data migration, data migration frameworks, data modelling, data retrieval, data storage, database management, dataengineering, dbt, devops, etl pipelines, java, machine learning, python, snowflake, sql, virtual package environments"
Data Center Infrastructure and Automation Engineer,Unity,"Slough, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-center-infrastructure-and-automation-engineer-at-unity-3735184326,2023-12-17,Reading, United Kingdom,Mid senior,Onsite,"The opportunity
Our focus is to increase developer efficiency, ensuring that every second of development time is spent on the things that matter. Our mission includes partnering with our internal development efforts to enable increased velocity and improved productivity.
We are seeking a skilled and dedicated Data Center Infrastructure and Automation Engineer to join our Net and DC team. The primary focus of this role is to ensure stability and high uptime across our infrastructure. Strong Linux skills and proficiency in updating and patching Linux-based systems as well as having a strong understanding of networking will be essential. As a member of the team, you will collaborate closely with our infrastructure and software development teams to maintain and improve our infrastructure. Your expertise in tools like Ansible and your ability to work with Go and Python will also help to ensure your success.
What You’ll Be Doing
Deploy and assist with the rollout of new hardware into our Data Center
Develop automated installation of OS' and infrastructure services utilising tools such as Ansible to ensure fleet-wide consistency of systems. Also through using CI/CD pipelines and related tools (e.g. GitLab CI/CD)
Collaborate with the team to resolve hardware, network, and infrastructure issues promptly, minimising downtime and service disruptions
Collaborate with the Middleware team to support OpenStack services and Ceph on our solution
Document and report on infrastructure changes, configurations, and maintenance steps
What We’re Looking For
Strong proficiency in Linux systems administration, with experience in deploying updates and patches as well as Bash scripting
An ability to display excellent knowledge of networking principles and protocols
Experience working with Ansible for automation tasks as well as scripting and development knowledge.
Experience solving hardware and software issues in a data centre environment. Including using Cumulus Linux, Gigabyte, Mellanox, Dell as well as a wider understanding of consumer grade hardware.
Experience with containerization technologies such as Docker and Kubernetes and familiarity with virtualization technologies.
You might also have
Knowledge of cloud computing platforms, such as AWS, Azure, or Google Cloud
Experience with monitoring and logging like ELK Stack (Elasticsearch, Logstash, Kibana) or Prometheus
Experience with network monitoring tools, such as Wireshark
Life at Unity
Unity (NYSE: U) is the world’s leading platform for creating and operating real-time 3D (RT3D) content. Creators, ranging from game developers to artists, architects, automotive designers, filmmakers, and others, use Unity to make their imaginations come to life. Unity is the foundation upon which the world’s most powerful digital content is created. Specifically, Unity’s platform provides a comprehensive set of software solutions to create, run and monetize interactive, real-time 2D and 3D content for mobile phones, tablets, PCs, consoles, and augmented and virtual reality devices.
In the fourth quarter of 2022, more than 70% of the top 1000 mobile games were made with Unity as derived from a blended number of the top 1000 games in the Google Play Store and Apple App Store. In 2022, Made with Unity Applications had more than 4 billion downloads per month. For more information, please visit www.unity.com.
Unity is a proud equal opportunity employer. We are committed to fostering an inclusive, innovative environment and celebrate our employees across age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law. Our differences are strengths that enable us to support the growing and evolving needs of our customers, partners, and collaborators. If there are preparations or accommodations we can make to help ensure you have a comfortable and positive interview experience, please fill out this form to let us know.
This position requires the incumbent to have a sufficient knowledge of English to have professional verbal and written exchanges in this language since the performance of the duties related to this position requires frequent and regular communication with colleagues and partners located worldwide and whose common language is English.
Headhunters and recruitment agencies may not submit resumes/CVs through this website or directly to managers. Unity does not accept unsolicited headhunter and agency resumes. Unity will not pay fees to any third-party agency or company that does not have a signed agreement with Unity.
Your privacy is important to us. Please take a moment to review our Prospect and Applicant Privacy Policies. Should you have any concerns about your privacy, please contact us at DPO@unity.com.
#SEN
Show more
Show less","Linux, Ansible, Bash, Networking, Go, Python, Cumulus Linux, Gigabyte, Mellanox, Dell, Docker, Kubernetes, Virtualization technologies, AWS, Azure, Google Cloud, ELK Stack, Prometheus, Wireshark","linux, ansible, bash, networking, go, python, cumulus linux, gigabyte, mellanox, dell, docker, kubernetes, virtualization technologies, aws, azure, google cloud, elk stack, prometheus, wireshark","ansible, aws, azure, bash, cumulus linux, dell, docker, elk stack, gigabyte, go, google cloud, kubernetes, linux, mellanox, networking, prometheus, python, virtualization technologies, wireshark"
Staff Data Capture Engineer,Oxa,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/staff-data-capture-engineer-at-oxa-3781533179,2023-12-17,Reading, United Kingdom,Mid senior,Onsite,"Description
Based in Oxford and with offices in Canada and the US, Oxbotica is an international startup founded in 2014 and scaling up rapidly (300+ Oxbots and growing).
As the global leader in our industry, we’re fuelled by a bold purpose: to make the Earth move better. From passenger shuttles to industrial vehicles, our operating system for Universal Autonomy™ is transforming the way people and goods are transported by enabling any vehicle, in any environment, to operate autonomously — safely, securely and efficiently.
Our technology is capturing investors’ imagination. To date we’ve raised $140 million in our Series C investment round. Accommodating growing demand from new and current shareholders, this funding is driving our expansion in North America and EMEA, and accelerating the deployment of our technology in domains where there is both urgent need and potential to scale.
Your Team
Oxa MetaDriver is a suite of tools that combines generative AI, digital twins and simulation to accelerate machine learning and testing of self-driving technology before and during real-world use. The MetaDriver Team is responsible for delivering these technologies to both internal and external companies. It comprises a wide range of disciplines from ML and simulation, to application development, cloud engineering and data capture professionals.
Your role
Autonomous vehicles depend on sensor data, which is central to performance and safety, and is at the very heart of Oxa’s software and platform development. We take a state-of-the-art approach, where data collected in the real world is analysed, decomposed, transformed and multiplied to create novel datasets. This role is responsible for the development lifecycle for data capture operations, including analysing any gaps in coverage, planning novel data capture routes, deploying data capture platforms and post-processing the resulting data into datasets.
Lead the MetaDriver Data Capture Operations team
Develop detailed deployment plans for data capture using data capture platforms, considering different scenarios, environmental conditions and internal requirements.
Oversee the deployment and operation of data collection vehicles, collecting data, and monitoring system performance and data quality.
Ensure adherence to regulations, local laws and bylaws and safety protocols and best practices
Create a framework for storing detailed records of data capture sessions, results, and observations. Log technical issues.
Provide feedback to stakeholders to support continuous improvements across all components involved in the data collection process.
Thoroughly analyse collected data to identify anomalies, and any areas for improvement.
Work with Vehicle Engineering on both the specification for data capture capabilities and scheduling management
Work closely with other engineering teams to improve data collection systems, platforms and protocols
Work with stakeholders across the company to understand requirements and provide tools to support vehicle commissioning, calibration, synchronisation.
Collaborate closely with other teams to ensure alignment on sensor performance characteristics, sensor locations, orientations, field of views etc.
Requirements
What you need to succeed
Experience in a similar data capture role
Hands on experience with solving technical challenges and systems integration with data capture technology (hardware and software)
Have the ability to communicate clearly on technical matters and work well with multiple stakeholders across several teams.
Have an understanding of different sensor modalities and their characteristics, and experience working with them.
Experience of managing multi-disciplinary teams
Global operations management experience
Contract negotiation skills
Familiarity with data requirements for AV and ML
Flexibility in working, being able to thrive with uncertainty
Extra kudos
Experience with time synchronisation and calibration in robotics applications.
Experience in software development for embedded devices
Experience with Automotive CAN
A driving license
Experience in autonomous vehicle testing or regulatory frameworks related to autonomous vehicles
A natural curiosity about the world and a can-do attitude.
A love of collaboration.
Proven agility in fast-changing environments. As a scale-up, we’re constantly evolving so our people need to evolve too for us to succeed together.
An ability to build trust quickly with team members and other stakeholders.
An open mind. We call ourselves ‘Learn it alls’ because we need to remain open to possibilities
A customer-centric outlook. Chances are you won’t be directly customer facing, but we value people who anticipate and prioritise the needs of our customers. We call it ‘inventing on their behalf.’
The Candidate Journey: Multi-Step and Two-Way
No-one wants to feel like a square peg in a round hole, so this process is designed to give you every chance to get the measure of us, and us of you. The various stages give you every opportunity to show your unique strengths and qualities, and enables each of us to establish if we’re a good fit for the other. If the fit is good and you’re selected, you’re then in a position to do great work and thrive, which is what everyone wants.
Benefits
We provide
Competitive salary, benchmarked against the market and reviewed annually
Company share programme
Hybrid and/or flexible work arrangements
Core benefits of market leading private healthcare, life assurance, critical illness cover, income protection, alongside a company paid health cash plan (including gym discounts)
A flexible £2,000 (pro-rata) benefits fund to spend on additional benefits of your choice, including tech scheme and cycle to work benefits
A salary exchange pension plan
25 days’ annual leave plus bank holidays
A pet-friendly office environment
Safe assigned spaces for team members with individual and diverse need
Our Culture
Diversity is a marathon not a sprint! It is a journey with no destination. We are on a mission to unlock the benefits of self-driving technology to every person and organisation on the planet. We are creating an environment where everyone, from any background, can do their best work which put simply is the right thing to do. We hire and nurture those we can learn from, valuing diversity and the innovation that this drives.
We apply a neuro inclusive lens to our recruitment process and want each potential Oxbot to enjoy the best experience possible for them. We promote an open and inclusive culture that empowers our Oxbots to bring their whole, authentic selves to work every day. Oxa is proud to be an inclusive organisation and, as such, we require all team members within our recruitment process to understand and deploy best practices focused on de-biasing the whole recruitment cycle.
Please share with us any individual needs or reasonable adjustments we may need to make in advance of commencing the interview process with us.
Learn more about our culture here.
Why become an Oxbot?
Our team of experts in computer science, AI, robotics and machine learning is world-class, and together they’re solving the most exciting and important technological challenges of our times.
But as well as smarts, Oxbots have heart. Our diverse, multi-cultural crew is guided by a shared vision to bring the myriad benefits of autonomy to our customers and partners. And in a company that celebrates uniqueness as much as skill and experience, they do it with energy, conviction and a healthy dose of excitement, too.
If you are bold, creative and hyper skilled, come and create the future of autonomy with us at Oxbotica.
Show more
Show less","Data capture, Machine learning, Simulation, Software development, Automotive CAN, Time synchronization, Sensor modalities, Sensor characteristics, Embedded devices, Autonomous vehicle testing, Regulatory frameworks, Data requirements, Generative AI, Digital twins","data capture, machine learning, simulation, software development, automotive can, time synchronization, sensor modalities, sensor characteristics, embedded devices, autonomous vehicle testing, regulatory frameworks, data requirements, generative ai, digital twins","automotive can, autonomous vehicle testing, data capture, data requirements, digital twins, embedded devices, generative ai, machine learning, regulatory frameworks, sensor characteristics, sensor modalities, simulation, software development, time synchronization"
Software Engineer for Data Acquisition,Diamond Light Source,"Harwell, England, United Kingdom",https://uk.linkedin.com/jobs/view/software-engineer-for-data-acquisition-at-diamond-light-source-3775396427,2023-12-17,Reading, United Kingdom,Mid senior,Onsite,"Software Engineer for Data Acquisition
£37,087 to £40,754 per annum (Discretionary range to £46,868), dependent upon skills and experience
Harwell Science and Innovation Campus, Oxfordshire – primarily based onsite / some home working available.
Full time / Flexible hours considered.
Reference: 11240
About Us
Diamond Light Source is the UK’s national synchrotron; a huge scientific facility designed to produce very intense beams of X-rays, infrared, and ultraviolet light. Our scientists use the light to study a vast range of subject matter, from new medicines and treatments for disease to innovative engineering and cutting-edge technology.
At Diamond we are currently planning a major upgrade of the facility, known as Diamond-II. Diamond-II will combine a new machine and new experimental labs, known as beamlines, with a comprehensive series of upgrades to optics, detectors, sample environments, sample delivery capabilities and computing, which will ultimately generate an even more brilliant light source at a higher energy.
About the Role
We now have an opportunity for a Software Engineer within the Diamond Data Acquisition (DAQ) group to develop the next generation Data Acquisition software used for orchestrating experiments at Diamond.
The DAQ group develops software to interface with and control the hardware which allows experiments to be performed and monitored as well as guiding the end user to tools for data reduction and analysis. The group works closely with scientists, motion, and controls engineers to continuously develop and improve software solutions for scientific data acquisition and on-the-fly monitoring and data analysis. Typical experiments will generate tens of terabytes of data per day.
Experimental data is currently acquired with our open-source software package, GDA, which is responsible for managing and executing experiments, capture and storage of experimental data and real-time visualisation of collected data. GDA is a client-server application written in Java, making use of the Eclipse RCP framework. The GDA server communicates with beamline hardware via EPICS to provide a control and monitor capability and deliver high-level experimental functions. A GDA client provides both a GUI and a Python scripting capability for users to control experiments locally.
To facilitate the advanced scientific capabilities that are expected from Diamond-II the Data Acquisition Software is being modernised and will result in a more service-based architecture. The new Acquisition Platform, Athena, will be configured to deliver advanced capabilities for a new collection of State-of-the-Art Flagship Beamlines.
As a Software Engineer in the DAQ Group you will work alongside other software engineers contributing to the generic DAQ Software modernisation and developing specialised Diamond-II Flagship Beamline DAQ solutions in an open and collaborative environment. You will be required to interact with scientists and translate their specifications for scientific capabilities into software requirements. You will liaise with controls and motions engineers to put these requirements into action. You will also potentially participate in international collaborations within this domain, sharing best practice and supporting other facilities and synchrotrons within the collaboration.
This role will play a significant part in contributing to the Diamond-II upgrade.
About You
You will be qualified to degree level in a STEM subject, with experience in the full Software Development Lifecycle, using a modern high-level language, and with an understanding of good software design principles and design for usability.
Experience of working with large and complex code bases would be a distinct advantage, as would experience of Python 3, Java, Spring, JMS message-oriented middleware, Java RMI, the HDF5 file format, the Eclipse RCP platform, REST and Kubernetes.
You should display good communication, interpersonal and analytical skills, with personal interest or experience in a science environment.
Benefits
Diamond offers an exceptional benefits package to support staff in achieving a positive work/life balance. This includes 26 days annual leave plus Christmas closure, public holidays, 2 annual volunteering days and flexible working hours. We also offer an excellent defined benefit pension scheme. Staff also have access to a range of amenities on site including a nursery, cafes, a restaurant and sports and leisure facilities. A relocation allowance may also be available where applicable.
To Apply
Please use the online application process to apply and tell us why you believe you are suitable for this role.
The initial closing date for applications is 30th December 2023, however applications will be reviewed and interviewed on an ongoing basis until this vacancy is filled.
Show more
Show less","Java, Python 3, Spring, JMS messageoriented middleware, Java RMI, HDF5 file format, Eclipse RCP platform, REST, Kubernetes, GDA, EPICS, Athena Data Acquisition Platform, Software Development Lifecycle, Software Design Principles, Design for Usability","java, python 3, spring, jms messageoriented middleware, java rmi, hdf5 file format, eclipse rcp platform, rest, kubernetes, gda, epics, athena data acquisition platform, software development lifecycle, software design principles, design for usability","athena data acquisition platform, design for usability, eclipse rcp platform, epics, gda, hdf5 file format, java, java rmi, jms messageoriented middleware, kubernetes, python 3, rest, software design principles, software development lifecycle, spring"
Senior data engineer - Kafka specialist,Aker Systems,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-kafka-specialist-at-aker-systems-3768353242,2023-12-17,Reading, United Kingdom,Mid senior,Onsite,"Aker Systems was founded in 2017 by a team of experienced technology professionals who recognised an opportunity to provide highly secure enterprise data platforms to large organisations. Aker builds and operates ground-breaking, ultra-secure, high-performance, cloud-based data infrastructure for the enterprise. Our proprietary technology solutions drive performance and reduce costs while helping our clients to improve the management and sharing of data across their organisations.
In 2020 Aker Systems was recognised as a One to Watch on the Sunday Times Tech Track. The Company was also recognised at the Thames Valley Tech Awards 2020; winning the Thames Valley Tech Company of the year, the Emerging Tech Company and High Growth Tech Business categories.
Most recently in September 2021, we were successfully acquired by a new investor (Abry Partners) who have a deep industry expertise in our field to support us in the next part of our journey and as we continue to grow and diversify as a business.
We encourage people of all different backgrounds and identities to apply. We are committed to maintaining an inclusive, and supportive place for you to do your very best work.
Duties & Responsibilities
A UK Government Security Check (SC) clearance is required for this role. If you dont hold SC clearance, we will support you to apply assuming you have lived and worked in the UK for a minimum of 5 years.
As a Sr Data Engineer, you will join a technical team extensive knowledge on Kafka streaming. Building large complex solutions on kafka, stateful streaming.
Experience in JDBC, Kotlin
Experience in coding, AWS services, managing Kubernetes clusters.
Experience in Flink in AWS cloud / Spark streaming.
Minimum 3+yrs exp in building streaming applications, experience in functional design patterns, functional style coding, stateful streaming.
Knowledge of BDD/TDD frameworks.
Stakeholder management.
Have a resilient, 'can do attitude', as this is an individual contributor role to begin with.
Eventually, be able to build and mentor teams.
Aker Systems Attributes
At Aker we work as a team, we are collaborative, hardworking, open, and delivery obsessed. There is no blame culture here: try things, and take responsibility for the outcomes. You are always part of the wider Aker. We help out our colleagues and take pride in successfully achieving difficult tasks. We run towards problems and help solve them. Communicate always, do so accurately and in a timely fashion.
In return, we offer a competitive salary, 25 days holiday (excluding bank holidays), Company Paid Medical Insurance, Life Assurance (4x times basic salary), Pension scheme, Perks at Work, Cycle Scheme, Tech Scheme and Season Ticket Loan. Plus, a list of voluntary benefits including Dental Insurance, Critical illness cover, Virtual GP.
Equal Opportunities
Aker Systems fosters a diverse environment that encourages openness in its communications and is committed to providing equal employment opportunity for all people regardless of race, religion, gender or sexual orientation, age, marital status, national origin, citizenship status, disability, veteran status or other personal characteristics. We embrace differences of opinion and diversity because they help challenge us and find new groundbreaking technical solutions.
Show more
Show less","Kafka streaming, JDBC, Kotlin, AWS services, Kubernetes clusters, Flink, Spark streaming, BDD/TDD frameworks, Stakeholder management","kafka streaming, jdbc, kotlin, aws services, kubernetes clusters, flink, spark streaming, bddtdd frameworks, stakeholder management","aws services, bddtdd frameworks, flink, jdbc, kafka streaming, kotlin, kubernetes clusters, spark streaming, stakeholder management"
Data Engineer,TRIA,"Reading, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-tria-3774723351,2023-12-17,Reading, United Kingdom,Mid senior,Remote,"Azure Data Engineer | Azure Data lake | Data Migration | 6 months | Fully remote | Inside IR35
We are looking for an Azure Data Engineer to join a busy not for profit, to support with the merger of two data platforms into a central Data Platform in Azure. You’ll be working closely alongside three other data engineers to help this project complete over the next 6 months.
For this role you will:
Have experience working with Azure + knowledge of Azure Data Lake
Have completed a large-scale data migration previously.
Be able to work quickly to tight deadlines.
Be comfortable working in a close knit team
Any experience of report building with Power BI would be beneficial.
Experience with Python scripting would be a bonus.
If you would like to hear more, please apply below using the links, they are looking for starts before the new year.
Azure Data Engineer | Azure Data Lake | Data Migration | 6 months | Fully remote | Inside IR35
Show more
Show less","Azure, Azure Data Lake, Data Migration, Power BI, Python scripting","azure, azure data lake, data migration, power bi, python scripting","azure, azure data lake, data migration, powerbi, python scripting"
Data Engineer Senior Consultant,Viable Data,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-senior-consultant-at-viable-data-3772735215,2023-12-17,Reading, United Kingdom,Mid senior,Remote,"Viable Data is an innovative technology, data and UX consultancy, delivering excellence through our projects and providing our people with a supportive culture and opportunities for growth and continuous learning.
We thrive on the challenge of working across different projects, user needs and technologies and our teams and people live this every day. Our people-first approach and culture is central to our growing success as a consultancy.
We are an all-inclusive equal opportunities employer and proudly celebrate diversity. If you thrive on challenge, have a passion to learn and make a difference, and enjoy being part of a growing multidisciplinary team, look no further and start your Viable career, now.
About The Role
As a Senior Data Engineer at Viable Data, you will ensure that data is in the right state and format, ready to implement data schemas and models. You will make sure that the data is ready and available for data mining purposes.
You will be someone who can adapt quickly and easily to the flexible needs of our Central Government customers. You will work across our high-performing consulting teams where you will play a pivotal role in designing & delivering exceptional digital products and services that directly contribute to the delivery of UK policy, economic growth and security.
This role is largely remote with occasional business essential travel.
Due to the nature of work, you must be willing to undergo and be capable of achieving SC security clearance
Requirements
Key responsibilities:
Develop analysis and design solutions using SQL and NoSQL technologies, data streaming, logging and monitoring tools, BI and Data Warehousing solutions and ETL and migration technologies.
Lead conversations with stakeholders to understand client data and processes to identify solutions.
Create advanced analytics and statistics techniques to business problems.
Effectively communicate insights from a dataset using narratives and visualizations.
Skills and experience needed:
Proven experience of working with large volumes of data, applying tools such as SAS, R, Python, SQL, PowerBI, Tableau, Looker
Ability to work across different data, cloud and messaging technology stacks.
Excellent written and verbal communication abilities - presenting findings and related design/business recommendations and insights clearly that stakeholders can understand and use.
Experience of pragmatic, hands-on analysis of enterprise-scale data projects across the full SDLC from design to migration, integration and live service in an Agile environment
A thorough understanding of Master Data Management, Data Lineage analysis, Metadata Management, Data Quality assessment and management, Data Transformation and migration
Able to communicate with stakeholders at all levels.
Proven analysis skills - experience of various data analysis methods at enterprise scale
People skills – empathetic, great listener and have a natural curiosity to understand people, their motivations and thought processes.
Experience and understanding of a range of User Centred Design practices.
Excellent organisation, time management and collaboration skills.
Benefits
As well as providing a great place to work that has an amazing culture and the opportunity to work on excellent projects where you will really make a difference, we have a whole host of additional employee benefits.
Our benefits package includes:
Employee Assistance Programme (EAP)
Flexible hours and supportive of remote working
A Green Energy incentive
Buy or sell annual leave
Volunteering day
A discount portal through Perkbox
Compensation package:
£55 - 65k + Company pension contributions
Annual bonus based on company performance
25 days leave + Bank holidays
5 days dedicated training allowance, with individual budget
Personal Development programme, with 6-month review cycles
Choice of company laptop (MacBook or Windows)
Show more
Show less","SAS, R, Python, SQL, PowerBI, Tableau, Looker, Agile, Master Data Management, Data Lineage analysis, Metadata Management, Data Quality assessment and management, Data Transformation, User Centred Design, Employee Assistance Programme (EAP), MacBook","sas, r, python, sql, powerbi, tableau, looker, agile, master data management, data lineage analysis, metadata management, data quality assessment and management, data transformation, user centred design, employee assistance programme eap, macbook","agile, data lineage analysis, data quality assessment and management, data transformation, employee assistance programme eap, looker, macbook, master data management, metadata management, powerbi, python, r, sas, sql, tableau, user centred design"
Azure Data Engineer,Mirai Talent,"Surrey, England, United Kingdom",https://uk.linkedin.com/jobs/view/azure-data-engineer-at-mirai-talent-3784837799,2023-12-17,Reading, United Kingdom,Mid senior,Remote,"We are seeking an experienced Data Engineer Contractor to join a dynamic and rapidly growing company who are at the infancy of their Data Strategy!
You will be a key contributor to data transformation initiatives. Your responsibilities include architecting a brand new Data Platforms, developing Power BI models, managing SQL databases, optimizing data performance, and fostering a culture of innovation and growth.
You’ll play an integral role in shaping the technical direction and implementation of a full data ecosystem.
What you can expect:
Relevant demonstrable experience of deploying Data Lakehouse/Warehouse in the Azure Data Platform including Power BI, Fabric, Data Factory, SQL, Delta Lakes, and PySpark.
Strong understanding of API’s, integration, and CI/CD.
Proficiency in data modelling and performance tuning.
Relevant qualifications in the Microsoft realm are a plus.
Experience delivering enterprise-level data platforms in Azure and surrounding technologies.
Passionate Visionary: Embrace innovation and have a passionate vision for shaping the future of technology.
What you can bring:
Proven Expertise: Demonstrated proficiency in functional or technical areas.
Tenacity: Exhibit tenacity as a problem solver who proactively tackles challenges.
Consultative Partner: Be a consultative partner with strong consulting skills to excel in client-facing roles.
Self-Driven and Motivated: Be self-driven and intrinsically motivated, capable of remote work and self-management.
Location: Remote – Occasional travel to client sites/company days. Based in the Surrey (visa sponsorship not available).
What’s in it for you:
3-6 month contract with opportunity to extend
Remote working model with very limited travel to office
Join a small team with high growth trajectory
Join a dynamic team and contribute to transforming data infrastructure, enabling data-driven decision-making.
If this sounds like you, apply now! We are shortlisting today!
Show more
Show less","Data Engineering, Data Transformation, Data Platforms, Power BI, SQL, Delta Lakes, PySpark, API, CI/CD, Data Modelling, Performance Tuning, Azure Data Platform, Azure, Microsoft","data engineering, data transformation, data platforms, power bi, sql, delta lakes, pyspark, api, cicd, data modelling, performance tuning, azure data platform, azure, microsoft","api, azure, azure data platform, cicd, data engineering, data modelling, data platforms, data transformation, delta lakes, microsoft, performance tuning, powerbi, spark, sql"
BI Data Analyst (90% Remote),This is Alexander Faraday Limited,"Woking, England, United Kingdom",https://uk.linkedin.com/jobs/view/bi-data-analyst-90%25-remote-at-this-is-alexander-faraday-limited-3772074559,2023-12-17,Reading, United Kingdom,Mid senior,Remote,"We are assisting an international specialist in their field who are looking for a BI Data Analyst to join their growing, organic entity. This is an autonomous, remote role that will entail going into their Head Office in Woking from time to time.
Duties entail:
Running reports on BI Tool for the UK and European customer base
Creating documents for New Reports
Analysing data in order to establish up-and-coming, new trends
Working closely with the customer team
Creating reports for governance meetings
SQL query writing
Analysis of sales, digital marketing, campaign activity etc. and reporting to stakeholders
Boards reports
Experience & Skills Required:
MS BI Tool
SQL
Retail environment
Highly analytical and methodical
Excellent communication skills
Minimum 3 years data visualisation and analytics
This is an excellent opportunity to join an ever-growing company in a progressive role with an opportunity to work remotely for majority of the time. Our client is looking to fill this role as soon as possible with an analytical minded, experienced and enthusiastic candidate
Show more
Show less","Microsoft Power BI, SQL, Data visualization, Data analytics, Retail, Analytical thinking, Methodical approach, Communication skills","microsoft power bi, sql, data visualization, data analytics, retail, analytical thinking, methodical approach, communication skills","analytical thinking, communication skills, dataanalytics, methodical approach, microsoft power bi, retail, sql, visualization"
Data Engineer,BJSS,"Reading, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-3288233145,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
About the Role
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","DataOps, Python, Objectoriented programming, CI/CD, Cloud data services, Parallel computing, Workflow programming, SQL, NoSQL, AWS, Azure, GCP, Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion","dataops, python, objectoriented programming, cicd, cloud data services, parallel computing, workflow programming, sql, nosql, aws, azure, gcp, databricks, data factory, synapse, kafka, redshift, glue, athena, bigquery, s3, cloud data fusion","athena, aws, azure, bigquery, cicd, cloud data fusion, cloud data services, data factory, databricks, dataops, gcp, glue, kafka, nosql, objectoriented programming, parallel computing, python, redshift, s3, sql, synapse, workflow programming"
Senior Data Engineer,La Fosse,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-la-fosse-3764307218,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"Job Title: Senior Data Engineer
As a Senior Data Engineer, you have the chance to take a crucial part in shaping, building, maintaining, and enhancing complex data pipeline processes. Your skills are essential for seamlessly integrating a variety of applications, ensuring the effective processing of linguistic data.
Key Responsibilities:
Design and construct robust data pipelines, ensuring the successful delivery of cutting-edge data products.
Collaborate with cross-functional teams, delivering technical support and expertise in Machine Learning workflows and data pipeline orchestration.
Contribute your skills to diverse projects, playing a crucial role in the advancement of language technology.
Your Profile:
We are in search of an individual who thrives in collaborative settings and demonstrates exemplary technical proficiency. Your qualifications should reflect:
A mindset inclined towards collaboration and teamwork.
An orientation towards customer satisfaction.
Expertise in Machine Learning workflows and data pipeline orchestration.
Exceptional communication skills, facilitating effective collaboration with both technical and non-technical stakeholders.
Outstanding problem-solving abilities and a proactive approach to challenges.
Proficient command of Python.
How to Apply:
Interested candidates are invited to apply by sending Current CV's to Samuel.Bywater@lafosse.com.
The organisation is an equal opportunity employer, celebrating diversity, and is committed to creating an inclusive environment for all employees.
Show more
Show less","Data Engineering, Machine Learning, Data Pipeline Orchestration, Python","data engineering, machine learning, data pipeline orchestration, python","data engineering, data pipeline orchestration, machine learning, python"
Senior Data Engineer,ADLIB Recruitment | B Corp™,"Guildford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-adlib-recruitment-b-corp%E2%84%A2-3747588281,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"Global Data Platform Project involving Azure, AWS, Spark, Kafka.
Get involved in shaping the future data vision for a leading FS Firm.
Opportunity to design, build and support cutting-edge data pipelines.
We’re partnering with a with a global financial services institution that are embarking on an ambitious project: to build an industry-leading data platform.
The long-term vision is to migrate all data within the coming years and embed any new products within this platform.
What You’ll Be Doing
You’ll play a pivotal role in designing, developing, and maintaining the company’s data infrastructure.
You’ll collaborate with cross-functional teams to ensure efficient data collection, storage, and retrieval, paving the way for superior analysis and insights.
Your expertise will be crucial in:
Supporting solution design activities and shaping the team’s approach.
Developing, testing, and documenting scalable ETL data pipelines.
Ensuring design aligns with the company’s technology strategy and standards.
Implementing robust data security measures and ensuring compliance.
Becoming a Subject Matter Expert in data technologies and addressing performance bottlenecks.
What You’ll Need To Apply
Minimum of 3 years’ experience in data/software engineering.
Proficiency in Python and Spark/PySpark.
Experience with cloud platforms (e.g., AWS, Azure).
Knowledge in data modelling and ETL frameworks.
Excellent communication and problem-solving skills.
What You’ll Get In Return For Your Experience
A base salary of up to £80k plus bonus and a comprehensive benefits package. This includes an attractive shares program, a matched pension that can reach up to 18%, and substantial discounts on various products. Beyond these, they are firmly committed to the continuous learning and development, offering numerous opportunities to grow and enhance your skills through training courses/days.
They deeply value work-life balance, which is why they’ve adopted a hybrid working model that requires team members to be present at their Guildford office 2 – 4 days per month. These in-office days are primarily for team meet-ups, fostering collaboration and architectural discussions.
What’s next?
Eager to be at the forefront of a transformative data journey? Don’t miss out on this opportunity. Apply now and let’s discuss how you can make an impact.
Show more
Show less","Azure, AWS, Spark, Kafka, Python, Spark/PySpark, ETL, Data modelling, Cloud platforms, Data security, Data engineering, Software engineering","azure, aws, spark, kafka, python, sparkpyspark, etl, data modelling, cloud platforms, data security, data engineering, software engineering","aws, azure, cloud platforms, data engineering, data modelling, data security, etl, kafka, python, software engineering, spark, sparkpyspark"
Lead Data Engineer,Franklin Fitch,"Frimley, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-franklin-fitch-3774476402,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"Lead Data Engineer | Hybrid | Frimley
A chance to become a lead data engineer with one of our global clients has arisen. They need an experienced data engineer, looking to take that next step in their career to come in and hit the ground running.
Responsibilities:
Skilled across developing, implementing and optimising T-SQL scripts
Writing and reviewing technical documentation
Monitor and maintain ETL solutions
Visual Studio experience
What you'll get:
Discretionary bonus
Medical and health insurance
Pension
Tailored development and career programme
If you'd like to be a key member at a global company then feel free to reach out!
Show more
Show less","TSQL, ETL, Visual Studio, Technical Documentation","tsql, etl, visual studio, technical documentation","etl, technical documentation, tsql, visual studio"
Data Analyst,Savant Recruitment Experts,"Newbury, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-savant-recruitment-experts-3783588369,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"Currently seeking a Data Quality Analyst to work on a large digital transformation programme for a global consultancy.
The Applicants must have experience of analysing data from a wide variety of sources having worked with relational data models and low-quality data containing missing or erroneous values and developed methods for correction as well as having prepared and delivered presentations based on data analysis which answers business questions. As well as, developing analysis tools and libraries to improve reliability and efficiency of the data analysis and assisting Data Engineers in maintaining data warehouses by providing insight to the data. Work with Business Process Architect and Process Owner by creating data models and interpretation of the data. Identifying and mitigating risks involved with the transference of data.
You must have experience in statistical methodologies and data analysis techniques, understanding of data modelling and data based solutions. Proficient in R, Python and SQL, Excel, SAS, SPSS as well as business intelligence platforms such as Tableau, D3, Qlik Sense. We are looking for candidates with BPSS certification.
Show more
Show less","Data Analysis, Data Quality, Data Modeling, Data Warehousing, Statistical Analysis, Business Intelligence, R, Python, SQL, Excel, SAS, SPSS, Tableau, D3, Qlik Sense, BPSS","data analysis, data quality, data modeling, data warehousing, statistical analysis, business intelligence, r, python, sql, excel, sas, spss, tableau, d3, qlik sense, bpss","bpss, business intelligence, d3, data quality, dataanalytics, datamodeling, datawarehouse, excel, python, qlik sense, r, sas, spss, sql, statistical analysis, tableau"
Data Analyst Python SQL,Nominet,"Oxford, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-python-sql-at-nominet-3776617802,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"Job description:
This role role is for a 9 month FTC and will be based on-site at our Oxford office 1-2 times per week.
About the role:
This is a data analyst role for a 9 months fixed term contract in our Insights team. As a tech company running .uk domains we have a fascinating and varied dataset that includes data about customers, finance, use of registry systems and the registry itself. You will join the team as we deliver our insights transformation programme to take Nominet on the next stage of our analytics journey and make a step change in how we deliver insight to the business. If you have a natural curiosity, a passion for data and want to work for a purpose driven company this could be the role for you.
You will be working in a small, close-knit team which includes data analysts, data engineers and a data scientist. You will work closely with the business on how we can make best use of our data to deliver value. You can expect to work on activities like:
Answering key operational and strategic business questions using the data - this will involve providing ad-hoc data and analysis as well as developing regular reports that support operations in python and SQL
You will work with the business to understand the insight they need and then use your technical skills to understand where to find that data, understand the data quality and define data requirements for our data engineers
You will work with the business on how best to surface and visualise data in reports and dashboards to give them easy access to the data that they need day to day and bring data driven insights to daily decision making
About you and your experience
You’ll need previous experience as a data analyst to be successful in this role along with the technical skills that support this. We use Python, SQL and PowerBI but we’re also interested in your ability to work with the business to understand the insight that is needed.
A good working knowledge of SQL is a must – we are looking you to have demonstrable experience of using SQL in a commercial role to query and manipulate data to deliver business insights
We also need you to have experience of Python or R in a commercial setting – you will be comfortable reading in, manipulating an visualising data as well as using python to create specific reports
You will have demonstrable experience of data manipulation and data cleansing
You’ll be great at working with different business areas to really understand what they need from the data asking challenging questions to truly understand the problem and proposing solutions which are simple and powerful
We’re looking for someone who is curious and creative you’re someone who is constantly wanting to find out more about a problem and look at it from different angles to help you find a great solution
Show more
Show less","Data analysis, Data wrangling, Python, SQL, PowerBI, R, Data manipulation, Data cleansing, Data visualization, Datadriven decision making, Business intelligence, Data engineering, Data science","data analysis, data wrangling, python, sql, powerbi, r, data manipulation, data cleansing, data visualization, datadriven decision making, business intelligence, data engineering, data science","business intelligence, data engineering, data manipulation, data science, data wrangling, dataanalytics, datacleaning, datadriven decision making, powerbi, python, r, sql, visualization"
Principal Data Engineer / Data Architect,HP,Greater Reading Area,https://uk.linkedin.com/jobs/view/principal-data-engineer-data-architect-at-hp-3769172646,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"HP is a Fortune 100 technology company with $58+ Billion in revenue, with over 50,000 employees operating in more than 170 countries around the world. We provide technology and services that help people and companies address their problems and challenges, and realize their possibilities, aspirations, and dreams. We apply new thinking and ideas to create simpler, more valuable, and trusted experiences with technology, continuously improving the way our customers live and work.
In the GTM advanced analytics COE, our mission is to deliver impact by building machine learning (ML) products to optimize pricing, marketing investments and provide guidance to sales and other HP teams.
We're looking for a principal data engineer / data architect to join our data engineering team.
Qualifications
Typically 5+ years of experience in software or data engineering.
Extensive experience in data modeling, data integration and processing of structured and unstructured data.
Highly proficient in one or more programming languages (Python preferred).
Strong SQL proficiency (experience with NoSQL – advantage).
Highly experienced with Apache Spark / pyspark
Excellent communication skills; mastery in English and local language.
Ability to effectively communicate product architectures, design and change proposals.
Familiar with best practices of the data and software engineering lifecycle and/or best practices of the above platforms and tools.
Additional Preferred Qualifications / Advantages
Databricks
Pandas
Dataiku DSS
About HP
You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.
So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.
HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.
Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.
From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!
Show more
Show less","Data engineering, Machine learning, Python, SQL, NoSQL, Apache Spark, Data modeling, Data integration, Dataiku DSS, Databricks, Pandas","data engineering, machine learning, python, sql, nosql, apache spark, data modeling, data integration, dataiku dss, databricks, pandas","apache spark, data engineering, data integration, databricks, dataiku dss, datamodeling, machine learning, nosql, pandas, python, sql"
Senior Data Engineer - London - GBP65k + bonus,Nigel Frank International,"Reading, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-london-gbp65k-%2B-bonus-at-nigel-frank-international-3723029188,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"Are you an experienced Data Engineer who is looking to work with the latest tech?
My client a leading transport organisation, and they are currently looking for a Senior Data Engineer to join their growing team. Within this role you will be working alongside other Data Engineers and BI professionals, you will be developing Azure Data Factory Pipelines, working with Databricks (Notebooks, pipelines, DLT, Spark, SQL), design solutions and governance and also experience with Data LakeHouse design, build and maintenance will be beneficial as well.
This is a permanent salaried position paying between £60,000 - £65,000 depending on the strength of your skill-set, working 1-2x/week in the office in London.
Minimum qualifications required:
Strong experience with Databricks
Strong experience with SQL and Python (Pyspark)
Experience with Azure DevOps, Snowflake Data Warehouse and CI/CD pipelines
Excellent communication skills
This is a fantastic opportunity to work with an established team and up-skill with exciting technologies. Do not miss this exciting and unique opportunity and reach out to have a chat.
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Data Engineering, Azure Data Factory, Databricks, Azure DevOps, Snowflake Data Warehouse, Data LakeHouse, CI/CD, SQL, Python, Pyspark","data engineering, azure data factory, databricks, azure devops, snowflake data warehouse, data lakehouse, cicd, sql, python, pyspark","azure data factory, azure devops, cicd, data engineering, data lakehouse, databricks, python, snowflake data warehouse, spark, sql"
Senior Data Engineer - London - GBP65k + bonus,Nigel Frank International,"Slough, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-london-gbp65k-%2B-bonus-at-nigel-frank-international-3723023915,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"Are you an experienced Data Engineer who is looking to work with the latest tech?
My client a leading transport organisation, and they are currently looking for a Senior Data Engineer to join their growing team. Within this role you will be working alongside other Data Engineers and BI professionals, you will be developing Azure Data Factory Pipelines, working with Databricks (Notebooks, pipelines, DLT, Spark, SQL), design solutions and governance and also experience with Data LakeHouse design, build and maintenance will be beneficial as well.
This is a permanent salaried position paying between £60,000 - £65,000 depending on the strength of your skill-set, working 1-2x/week in the office in London.
Minimum qualifications required:
Strong experience with Databricks
Strong experience with SQL and Python (Pyspark)
Experience with Azure DevOps, Snowflake Data Warehouse and CI/CD pipelines
Excellent communication skills
This is a fantastic opportunity to work with an established team and up-skill with exciting technologies. Do not miss this exciting and unique opportunity and reach out to have a chat.
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Data Engineering, Azure Data Factory Pipelines, Databricks, SQL, Python, Pyspark, Data Lakehouse design, Azure DevOps, Snowflake Data Warehouse, CI/CD pipelines, Microsoft Business Intelligence, BI professionals","data engineering, azure data factory pipelines, databricks, sql, python, pyspark, data lakehouse design, azure devops, snowflake data warehouse, cicd pipelines, microsoft business intelligence, bi professionals","azure data factory pipelines, azure devops, bi professionals, cicd pipelines, data engineering, data lakehouse design, databricks, microsoft business intelligence, python, snowflake data warehouse, spark, sql"
Senior Data Engineer,SSE plc,"Reading, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-sse-plc-3784566252,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"SSE has big ambitions to be a leading energy company in a low carbon world. Following our commitment to invest £20.5 billion in low carbon projects to 2027, we have significant growth plans and are well on our way to achieving our ambition to build a world that's more sustainable and inclusive for you, your family, the community you live in and for generations to come.
Join us on our journey to net zero and help us power change.
About The Role
Base Location:
Reading or Havant
Salary:
£38,700 - £58,100 + performance related bonus + a range of benefits to support your finances, wellbeing and family.
Working Pattern:
Permanent | Full Time | Flexible First options available
What is the Role?
As Senior Data Engineer your role is to build, optimise and maintain ETL, data applications, systems and services that comply with technical design and security guidelines and guardrails.
You'll provide leadership and accountability for ensuring the ongoing maintenance and developed service improvements to our core applications as well as analysing system requirements and designing, testing, modelling and modifying data pipelines and scripts to agreed company standards. Collaborating with product teams to deliver business outcomes in agile, DevOps environment is key to this role.
What do I need?
To be considered for this role, we would love you to have:
Understanding of the full software development lifecycle (SDLC).
Experience with designing, building, and operating analytics solutions using Azure cloud technologies with extensive experience in Cloud migration methodologies and processes including tools like Azure Data Factory, Event Hub, etc.
Working knowledge of Synapse and ETL technologies.
Solid experience of relational databases and data storage technologies such as: SQL Server, Oracle, MongoDB, MySQL, NoSQL, Data Warehousing and Databricks with experience in interpreting data, analysing results using statistical techniques.
Fully conversant with Agile and DevOps development methodology and concepts as applied to data driven analytics projects including CI/CD Coding, security testing best practice and standards.
Proven knowledge of report building with Power BI skill and experience of Python (including Pandas, Numpy, PySpark, Jupyter, etc) IT.
Microsoft Azure certifications are a plus.
About Our Business
SSE IT underpins the technology needs of all the different businesses that make up the SSE group. From emerging technologies to data and analytics to cyber security - we power SSE's growth and enable it to generate value, while keeping it secure. As a trusted business partner that helps SSE lead in a low carbon world, we are proud of our service. Working for SSE IT is all about equipping SSE for now and the future.
What's in it for you?
We offer an excellent package with 34 days annual leave entitlement. Enhanced maternity/paternity leave, discounted healthcare, salary sacrifice car leasing and much more, view our full benefits package on our careers site.
As an equal opportunity employer we encourage diversity and are committed to creating an inclusive environment for all employees. We encourage applicants from all protected characteristics and commit to providing any reasonable adjustments you need during the application, assessment and upon joining SSE. Search for 'Inclusion & Diversity at SSE' to find out more.
Next Steps
All applications should be made online, and I'll be back in touch after the vacancy closing date to let you know the outcome.
If you would like to discuss any working flexibly requirements or adjustments you may require throughout the recruitment and selection process, please contact David on David.Brickell@sse.com / 01738 275846.
Before commencing your role with SSE, you'll need to complete our pre-employment screening process. This will consist of a criminality and credit check.
Show more
Show less","ETL, Azure cloud technologies, Azure Data Factory, Event Hub, Synapse, SQL Server, Oracle, MongoDB, MySQL, NoSQL, Data Warehousing, Databricks, Power BI, Python, Pandas, Numpy, PySpark, Jupyter, Microsoft Azure, Agile, DevOps, CI/CD Coding, Data driven analytics, SDLC","etl, azure cloud technologies, azure data factory, event hub, synapse, sql server, oracle, mongodb, mysql, nosql, data warehousing, databricks, power bi, python, pandas, numpy, pyspark, jupyter, microsoft azure, agile, devops, cicd coding, data driven analytics, sdlc","agile, azure cloud technologies, azure data factory, cicd coding, data driven analytics, databricks, datawarehouse, devops, etl, event hub, jupyter, microsoft azure, mongodb, mysql, nosql, numpy, oracle, pandas, powerbi, python, sdlc, spark, sql server, synapse"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Reading, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3736353504,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader organisation within the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on the latest technology like Databricks. The organisation has big plans for expansion within the next couple of years, as they are winning more Azure-based projects.
Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too. You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks. There is great room for progression within this role. The team really values the culture, and support and training to further develop your skills-set will be provides..
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, PySpark, Microsoft Azure, Azure Data Factory, Python, SQL","databricks, pyspark, microsoft azure, azure data factory, python, sql","azure data factory, databricks, microsoft azure, python, spark, sql"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Slough, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3736352520,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader organisation within the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on the latest technology like Databricks. The organisation has big plans for expansion within the next couple of years, as they are winning more Azure-based projects.
Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too. You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks. There is great room for progression within this role. The team really values the culture, and support and training to further develop your skills-set will be provides..
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, SQL, Python/PySpark, Azure Data Factory, Azure","databricks, sql, pythonpyspark, azure data factory, azure","azure, azure data factory, databricks, pythonpyspark, sql"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Reading, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3728586039,2023-12-17,Reading, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader in the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on Databricks. Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too.
You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration too. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks.
There is great room for progression within this role, and support and training will be provided as well.
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, SQL, Python, PySpark, Azure Data Factory","databricks, sql, python, pyspark, azure data factory","azure data factory, databricks, python, spark, sql"
Data Engineer,Torch.AI,"Tysons Corner, VA",https://www.linkedin.com/jobs/view/data-engineer-at-torch-ai-3752882698,2023-12-17,Marietta,United States,Associate,Onsite,"Why this Role is Interesting?
There are very few companies in the world with the credentials we have. You’ll have an opportunity to support critically important customer missions and work with paradigm-changing technology.
You’ll be supporting one of the largest programs and customers in the company, offering significant visibility of your impact.
The role is critical in positioning Torch.AI as a leader in data infrastructure AI in the market and helps ensure the company is well-positioning for the next stages of growth.
You’ll have the opportunity to showcase your skills as a seasoned data engineer, while also learning from a team of extremely talented and credentialed individuals.
Torch.AI is in the midst of a pivotal, high-growth period, with several initiatives and investments anticipated to fuel hyper-growth across a diverse customer base.
You’ll play a significant roe in strengthening Torch.AI’s reputation as one of Forbes Best Startup Employers.
A Game-Changing AI Company in the Heart of the Midwest
Torch.AI, the Data Infrastructure AI Pioneers™, are headquartered in Kansas City with offices in Washington, DC. We build AI that makes data easier to use by processing data in-flight and radically evolving analytic and operational capabilities in any IT environment. Our Torch Platform instantly unlocks value from data and provides information needed for humans and machines to be more productive. Partnering with U.S. military forces, our solutions and people support a growing array of national defense capabilities with advanced technology.
At Torch.AI, we’re passionate about building software that solves some of the world’s most challenging problems. We have helped our customers enhance top-secret clearances, stop fraud at massive scales, discover new trends and global events, gain an edge in financial markets, and beyond. We are driven by our mission to unlock human potential and serve our clients in valuable and meaningful ways.
The Role
The U.S. Department of Defense relies on complex data every day to drive decisions, support warfighters, mitigate risks to national security, and ensure our global leadership. Data is being generated at an exponential rate and existing solutions don’t address today’s needs.
This Data Engineer role will work in support of developing and implementing solutions for a major U.S. Department of Defense program. The role will engage with cross-functional teams to discover all existing data types and documentation that are currently in use in multiple data producer environments. The successful candidate will be a creative problem solver, diligent and detail oriented, and will have a sharp focus on value creation and mission importance. They will have a keen eye on understanding customer needs, how they currently leverage Torch.AI software, and will help in driving the future success of the customer program.
What Success Looks Like
Participates in design meetings and consults with other staff to evaluate interface between hardware and software, and operational and performance requirements of overall system.
Develop and perform automated builds, testing, and deployments in support of NiFi development.
Work with the development and services teams to support service artifacts, which could include bug fixes, security improvements, functional extension, performance improvement, refactoring, and/or rewriting.
Experience in building data ingestion workflows/pipeline flows using NiFi, NiFi registry, and other Nifi management tools.
Create Artifact that will primarily be Apache Nifi flows and/or custom processors used in Nifi flows.
Create tables in Trino to deliver data via APIs to package and disseminate data to mission partners.
Build data models and analytics to support mission needs.
Pull or receive command and control files.
Test services artifacts for correctness and/or performance.
What We Value
B.S. degree in related field or equivalent combination of training and experience.
5+ years of proven work experience as a data engineer.
Demonstrable experience with Java, Parquet, Nifi, Kafka.
Experience with ETLs and APIs.
Understanding and Experience with Apache Nifi tool.
Analysis of the product to determine the end points as per the requirements.
Well-versed with docker.
Hands on experience on GIT.
Understanding of security products focusing on IAM.
Knowledge on ETL focusing on mapping.
Good to have experience with NiFi-supported scripting languages (Python) and writing regular expressions.
Experience creating custom NiFi processors.
Full Software Development Life Cycle (SDLC) experience.
Work well within a formal team structure and with minimal supervision.
This position REQUIRES an active TS/SCI. TS/SCI with CI Poly is preferred.
Work Environment & Travel Requirements
This job operates in state-of-the-art, professional office environment.
You will be expected to work out of Torch.AI's Leawood, KS office; limited hybrid/remote days may occasionally be available.
Travel: You will be required to travel to Northern Virginia and/or St. Louis on a periodic basis (10-50%) to work in and attend client meetings in a SCIF environment.
Perks & Compensation
Competitive salary, performance bonus, and benefits package. The salary range for this role is commensurate with experience.
Opportunity to participate in Torch.AI’s employee equity grant program.
Unlimited PTO.
In-Office Catering for Lunch Every Monday.
Access to company suite at the T-Mobile Center, with tickets to all major events and concerts.
Amazing professional growth opportunity at a high growth start-up.
Passionate, smart, and fun people to work with.
Excellent medical, dental, and vision insurance.
Life and disability coverage.
Relocation assistance.
11 paid holidays each year.
Torch.AI is an Equal Opportunity /Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, protected veteran status or status as an individual with a disability.
Show more
Show less","Java, Parquet, NiFi, Kafka, ETLs, APIs, Trino, Apache Nifi tool, Docker, GIT, IAM, ETL, Python, Regular expressions, NiFi processors, SDLC, TS/SCI, CI Poly","java, parquet, nifi, kafka, etls, apis, trino, apache nifi tool, docker, git, iam, etl, python, regular expressions, nifi processors, sdlc, tssci, ci poly","apache nifi tool, apis, ci poly, docker, etl, etls, git, iam, java, kafka, nifi, nifi processors, parquet, python, regular expressions, sdlc, trino, tssci"
Data Center Engineer,Cloudflare,"Raleigh, NC",https://www.linkedin.com/jobs/view/data-center-engineer-at-cloudflare-3732381823,2023-12-17,Marietta,United States,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
Data Center Operations Engineer
About the department
In this role, you will be focused on maintaining the Clou dflare global network. You 'll work closely with Cloudflare’s SRE (Site Reliability Engineering) team, Network Engineering team, Network Deployment Engineering team and with various vendors and partners (including hardware vendors, datacenter and network providers, and ISPs) to maintain and improve our global infrastructure. You will further be responsible for the development and implementation of consistent processes and visibility measurements for consistent and effective management of our infrastructure. This is a highly visible position that requires deep technical understanding of datacenter infrastructure, networking (physical), and basic experience with data analysis and project management.
To be successful in this position, you should have excellent technical skills, communication skills, and be able to navigate a range of challenges and constraints (e.g. schedule adherence, time zones, and cultures). You will have the opportunity to (literally) build a faster, safer Internet for our millions of users and the billions of web surfers that visit their sites each month.
Who You Are
You will thrive in a hypergrowth engineering environment and be self driven with a keen attention to detail. You will come with a deep technical understanding of Data Center colocation environments, network architecture and server technologies. You will be used to working through partners to support infrastructure delivery to a number of remote locations. You will have had experience managing operational environments, and used to developing new approaches to improve delivery efficiency or operational stability.
What You'll Do
Collaborating with internal teams (Infrastructure, Network Engineering and SRE). Create documentation and manage remote contractors to complete datacenter tasks, working with hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 300+ datacenter locations
Maintain Data Center environment operational availability
Creating and maintaining documentation, plans, SOP’s, MOP’s etc.
Support and configure network infrastructure where required
Providing feedback to internal teams to support internal tools and external vendor partnerships
Required Experience
Minimum of 5 yrs of Linux systems administration
Experience with Juniper, Cisco and DWDM network equipment
Experience managing and instructing remote contractors
Familiarity with work required to stand up infrastructure in remote colocation facilities
Experience running and improving operational processes, including automation tooling, in a rapidly changing environment
Familiarity with day-to-day tasks and projects common to Data Center Operations (deployment, migration, decommissioning etc.)
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Incident management
Other Responsibilities May Include
Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure
Assist with the definition, documentation and implementation of consistent processes across all region
Limited travel
Examples Of Desirable Skills, Knowledge And Experience
Bachelor’s degree; technical background in engineering, computer science, or MIS
Direct experience executing on complex data center/infrastructure projects
Previous experience installing / maintaining data center (and other IT) infrastructure and DCIM tools
Experience running and improving operational processes in a rapidly changing environment
Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills
Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously
Ability to manage MS excel and Google spreadsheets
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Must be a team player
Bonus Points
Multi-lingual; experience working with infrastructure in multiple countries
Comfortable with remote “lights-out” and out-of-band access to data center resources
Linux certifications (RHCSA etc.)
Network certifications (CCNA, JNCIA or higher)
Compensation
Compensation may be adjusted depending on work location.
For Colorado-based hires: Estimated annual salary of $ 111,000 - $ 135,000 .
For New York City, Washington, and California (excluding Bay Area) based hires: Estimated annual salary of $ 135,000 - $ 165,000
For Bay Area-based hires: Estimated annual salary of $ 142,000 - $ 174,000 .
Equity
This role is eligible to participate in Cloudflare’s equity plan.
Benefits
Cloudflare offers a complete package of benefits and programs to support you and your family. Our benefits programs can help you pay health care expenses, support caregiving, build capital for the future and make life a little easier and fun! The below is a description of our benefits for employees in the United States, and benefits may vary for employees based outside the U.S.
Health & Welfare Benefits
Medical/Rx Insurance
Dental Insurance
Vision Insurance
Flexible Spending Accounts
Commuter Spending Accounts
Fertility & Family Forming Benefits
On-demand mental health support and Employee Assistance Program
Global Travel Medical Insurance
Financial Benefits
Short and Long Term Disability Insurance
Life & Accident Insurance
401(k) Retirement Savings Plan
Employee Stock Participation Plan
Time Off
Flexible paid time off covering vacation and sick leave
Leave programs, including parental, pregnancy health, medical, and bereavement leave
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Data Center Operations, SRE (Site Reliability Engineering), Network Engineering, Network Deployment Engineering, Linux systems administration, Juniper, Cisco, DWDM, Incident management, Documentation, SOPs (Standard Operating Procedures), MOPs (Maintenance Operating Procedures), Network infrastructure, Automation tooling, Data Center Operations (deployment migration decommissioning), JIRA, MS Excel, Google spreadsheets, Program management, Problemsolving skills, Communication skills, Teamwork","data center operations, sre site reliability engineering, network engineering, network deployment engineering, linux systems administration, juniper, cisco, dwdm, incident management, documentation, sops standard operating procedures, mops maintenance operating procedures, network infrastructure, automation tooling, data center operations deployment migration decommissioning, jira, ms excel, google spreadsheets, program management, problemsolving skills, communication skills, teamwork","automation tooling, cisco, communication skills, data center operations, data center operations deployment migration decommissioning, documentation, dwdm, google spreadsheets, incident management, jira, juniper, linux systems administration, mops maintenance operating procedures, ms excel, network deployment engineering, network engineering, network infrastructure, problemsolving skills, program management, sops standard operating procedures, sre site reliability engineering, teamwork"
Data Movement Engineer,Harvey Nash,United States,https://www.linkedin.com/jobs/view/data-movement-engineer-at-harvey-nash-3780050609,2023-12-17,Marietta,United States,Mid senior,Remote,"""US citizens and Green Card Holders and those authorized to work in the US are encouraged to apply. We are unable to sponsor H1b candidates at this time”
Title: Data Movement Engineer
Location: New Haven CT (FULLY REMOTE)
Job Type: 12+ months contract (Long term)
Must Haves:
- Strong Background in API development
- Strong SQL background
- Strong Data Engineering Background, must understand best practices
- Cloud experience
- Experience with Talend is highly preferred
Job Description
Overview
We are currently seeking a Senior Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will possess experience in the data management domain, and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes.
Skill Qualifications
Required:
Minimum of 8 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 5 years of SQL query development, preferably across multiple database management platforms, and working with dimensional and star schema data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS, Talend, Boomi and PowerCenter preferred)
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g.. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management
Strong project management skills to ensure that projects get done on time and within budget
Effectively participates in teams and moves the team toward completion of goals
Ability to resolve conflicts
Preferred:Some experience with cloud database technologies preferred
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required:
BA or BS in Computer Science, Information Systems or related field.
10+ years experience
A reasonable, good faith estimate of the minimum and maximum hourly rate on this position is $60/hr to $67/hr on W2.
Show more
Show less","API development, SQL, Data engineering, Cloud, Talend, ETL/ELT, Informatica IICS, Boomi, PowerCenter, Data warehousing, Dimensional and star schema data warehouse, AQT, MS Query, Data visualization, Tableau","api development, sql, data engineering, cloud, talend, etlelt, informatica iics, boomi, powercenter, data warehousing, dimensional and star schema data warehouse, aqt, ms query, data visualization, tableau","api development, aqt, boomi, cloud, data engineering, datawarehouse, dimensional and star schema data warehouse, etlelt, informatica iics, ms query, powercenter, sql, tableau, talend, visualization"
Sr. Data Engineer,Aya Healthcare,United States,https://www.linkedin.com/jobs/view/sr-data-engineer-at-aya-healthcare-3777354011,2023-12-17,Marietta,United States,Mid senior,Remote,"Join Aya Healthcare, winner of multiple Top Workplace awards!
We are looking for an experienced Data Engineer to join our Data Operations team! In this hands-on role, the ideal candidate would help design, develop, and implement large scale, high-volume, high-performance, scalable data infrastructure and pipelines for the data warehousing platforms in Azure. Working in Azure Synapse and Data Lake, this person will have strong experience in cloud data design and management on the Azure platform. This person will join a small, nimble, team working in a fast-paced environment for a great company where your opinion and experience matters.
Who We Are:
We’re a $10+ billion, rapidly growing workforce solutions provider in the healthcare industry. We deliver tech-enabled services that help healthcare organizations meet and manage their contingent labor needs. We build and manage tech-enabled marketplaces for national and local healthcare talent and deliver contingent labor management solutions through our proprietary software platform.
At Aya, we’re obsessed with creating exceptional experiences for our clients, clinicians and employees. In fact, we put employee satisfaction above all else. Our team members are responsible for incomparable customer experience and we know that happy employees are critical to maintaining happy clients. We foster an entrepreneurial, high-energy, low-bureaucracy culture and value innovative thinking and creative problem solving. We embrace diversity in thought and backgrounds unified by a commitment to high achievement. When you join Aya, you’ll be surrounded by teammates who care about you as an individual and leaders who will help you grow both personally and professionally.
Responsibilities:
Monitor and troubleshoot data pipeline and warehouse quality and performance issues, ensuring smooth operation and proactively addressing potential bottlenecks.
Develop and maintain data lake architectures on Azure, ensuring efficient storage, organization, and accessibility of both structured and unstructured data across various cloud platforms
Stay updated with industry trends and advancements in data engineering and bring innovative ideas to continuously improve our data processes.
Ensure data security, privacy, and compliance by implementing appropriate Azure policies, procedures, and encryption methods
Monitor and optimize the performance, scalability, and cost-efficiency of data storage and processing solutions on Azure
Required Qualifications:
Bachelor's degree in Computer Science, Information Systems, or a related field
6+ years of experience in cloud data architecture, design and data governance
Strong experience with Azure cloud services, such as Azure Data Lake Storage, Azure Data Factory, and Azure Synapse
Experience with reporting and visualization tools (Tableau, PowerBI & SSRS)
Proficiency in programming languages, such as Python, Scala, or Java
Experience with big data processing frameworks, such as Apache Spark, Hadoop, or Apache Kafka on Azure
Knowledge of various data storage solutions, such as relational databases, etc
Creating & Managing documentation on methodologies, tools and techniques.
Excellent communication and presentation skills, with the ability to convey complex concepts
Willing and able to work irregular hours as needed
What We Offer:
Free premium medical, dental, life and vision insurance
Generous 401(k) match
Aya also offers other benefits to those that are eligible and where required by applicable law, including reimbursements and discretionary bonuses
Aya provides paid sick leave in accordance with all applicable state, federal, and local laws. Aya’s general sick leave policy is that employees accrue one hour of paid sick leave for every 30 hours worked. However, to the extent any provisions of the statement above conflict with any applicable paid sick leave laws, the applicable paid sick leave laws are controlling
Celebrations! We hit our goals and reward ourselves.
Company-sponsored virtual events, happy hours and team-building activities are always on the horizon — plus, you get a special treat on your birthday!
Unlimited DTO — we believe in time off!
Virtual yoga, meditation or boot camp classes offered daily
Compensation:
Aya reasonably anticipates the pay scale for this position to an annual salary of
$140,000 to $165,000
.
The pay scale for this position may vary if applicant possesses experience outside of what Aya reasonably anticipates for this position. Bonuses are subject to the role and your manager’s discretion.
Aya is an Equal Opportunity Employer (EEO), including Disability / Vets, and welcomes all to apply. Please click here for our EEO policy.
Show more
Show less","Cloud data architecture, Data governance, Azure cloud services, Azure Data Lake Storage, Azure Data Factory, Azure Synapse, Tableau, PowerBI, SSRS, Python, Scala, Java, Apache Spark, Hadoop, Apache Kafka, Relational databases, Documentation management, Communication skills, Presentation skills, Irregular work hours","cloud data architecture, data governance, azure cloud services, azure data lake storage, azure data factory, azure synapse, tableau, powerbi, ssrs, python, scala, java, apache spark, hadoop, apache kafka, relational databases, documentation management, communication skills, presentation skills, irregular work hours","apache kafka, apache spark, azure cloud services, azure data factory, azure data lake storage, azure synapse, cloud data architecture, communication skills, data governance, documentation management, hadoop, irregular work hours, java, powerbi, presentation skills, python, relational databases, scala, ssrs, tableau"
Data Engineer (REMOTE),eStaffing Inc.,United States,https://www.linkedin.com/jobs/view/data-engineer-remote-at-estaffing-inc-3645146791,2023-12-17,Marietta,United States,Mid senior,Remote,"Title: Sr. Software Engineer
Duration: full time employment
Location: Remote
Client: Workforce Communication application
Requirement
Job Description
Must-Haves
10+ years of experience developing and delivering scalable, customer-facing enterprise software. Candidates with previous heavy work experience in contract positions are not preferred.
Not married to any technology (has breadth of coding experience & enjoys honing their craft) but also understands how to leverage latest tech to solve problems in a better way. Ruby and Node.js preferred.
5+ years of experience designing, implementing and managing advanced architectures in a SaaS application domain or platform
Track record of delivering timely, high-quality features and functionality within an agile sprint environment.
Experience with large scaling challenges and solving challenging problems.
Nice-To-Haves
1 Good energy and communication skills
2 A leadership mindset
3 Comfort in ambiguous environments
Show more
Show less","Ruby, Node.js, SaaS, Agile, Software Architecture, Scalability, Problem Solving","ruby, nodejs, saas, agile, software architecture, scalability, problem solving","agile, nodejs, problem solving, ruby, saas, scalability, software architecture"
TEST DATA ENGINEER-REMOTE-EST Or CST Only || Remote,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/test-data-engineer-remote-est-or-cst-only-remote-at-steneral-consulting-3673754570,2023-12-17,Marietta,United States,Mid senior,Remote,"GenRocket is MUST have. Additionally, I would like to see minimum of 1+ yr. of implementation experience.
Must required: GenRocket, scripting, PostGres/DB ability, API work
Determine best practice(s) around synthetic, masked data in DCE/Platform
Analyze and understand how data flows in data hub
Determine best way to manage and create synthetic data in Data Hub; deliver solution
Figure out a solution to mock massive accounts coming through APIs
Understanding data models, table structures and dependencies.
Guide teams in generating new synthetic data, creating more, and maintaining over time.
Create PoC for syntheitc data use cases working with Product team
Agile Mindset, familiarity with Azure Cloud
ADO experience, python or powershell scripting experience
Must required: GenRocket, scripting, PostGres/DB ability, API work
Determine best practice(s) around synthetic, masked data in DCE/Platform
Analyze and understand how data flows in data hub
Determine best way to manage and create synthetic data in Data Hub; deliver solution
Figure out a solution to mock massive accounts coming through APIs
Understanding data models, table structures and dependencies.
Guide teams in generating new synthetic data, creating more, and maintaining over time.
Create PoC for syntheitc data use cases working with Product team
Agile Mindset, familiarity with Azure Cloud
ADO experience, python or powershell scripting experience
Show more
Show less","GenRocket, Scripting, PostgreSQL, Database, API, Synthetic data, Masked data, Data Hub, Data modeling, Table structures, Dependencies, Agile, Azure Cloud, ADO, Python, PowerShell","genrocket, scripting, postgresql, database, api, synthetic data, masked data, data hub, data modeling, table structures, dependencies, agile, azure cloud, ado, python, powershell","ado, agile, api, azure cloud, data hub, database, datamodeling, dependencies, genrocket, masked data, postgresql, powershell, python, scripting, synthetic data, table structures"
Data Engineer,Smile ID (formerly Smile Identity),"San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineer-at-smile-id-formerly-smile-identity-3641780519,2023-12-17,Marietta,United States,Mid senior,Remote,"Smile Identity builds trust.
Smile Identity is Africa's leading identity verification, and digital Know Your Customer (KYC) provider. We help companies scale rapidly across Africa by confirming the true identity of their users in real time, using any smartphone or computer. Our technology is powered by proprietary Machine Learning algorithms designed specifically for African faces, devices and network connections.
Our team is a diverse group of hardworking, truth-seeking, and fun-loving
Smilers
spanning 5 offices, 10 countries and 8 time zones. Our products are already making waves across many industries, from Banking to Fintech and Telecoms. We recently announced a $20M Series B raise and are backed by leading global investors, including Norsken22, Costanoa, CRE, Future Africa, Susa Ventures, Commerce Ventures, Courtside Ventures, Two Culture Capital, Latitude, Valuestream Ventures, Intercept Ventures and Vinod Khosla who are supporting us every step of the way.
Do you like working alongside a team of intelligent individuals? Do you want to have fun while making a real difference? Here at Smile Identity, you'll get the freedom and autonomy you need to do your best work, the flexibility to be creative, and the opportunity to grow and put your unique stamp on our mission.
What are you waiting for? Come with us on this amazing journey!
The Role
We are looking for a data engineer who loves bringing order to chaos. The individual in this role will maintain our data warehouse and associated data infrastructure and provide the entire organization with the data they need to be successful. This role is open to candidates across the globe. You will be working with colleagues ranging from the US West Coast to Eastern Africa, with that in mind candidates working in timezones between US Eastern and GMT are preferred.
What You Will Do
Work with our entire organization based in the US, London, Berlin, Lagos, Nairobi, and Cape Town to centralize our data and maintain our data warehouses/lakes.
You will select the right tools and services to bring our data together and provide a solid foundation for all our product and business analytics. Your north star? Empowering the entire organization with data to make the best possible decisions.
Design, build and launch extremely efficient and reliable data pipelines to move data.
Architect, build and launch new data models that provide intuitive analytics.
Manage the delivery of high impact dashboards, tools and data visualizations
Build data expertise and own data quality, including defining and managing SLAs for data sets.
Partner with leadership, engineers, commercial, and data scientists to understand data needs.
Influence short- and long-term strategy with cross-functional teams to drive impact.
Educate your partners: Use your data and analytics experience to discover opportunities, identifying and addressing gaps in existing logging and processes.
Requirements
4+ years experience with data infrastructure, ETL design, data warehousing, schema design and dimensional data modeling
2+ years of experience in SQL, Python, or similar languages
Experience with designing and implementing real-time pipelines
Experience with code management tooling such as Git, Github
Experience with data migrations in production settings
You have a deep understanding of modern data tooling and infrastructure
You are comfortable working independently with periodic guidance from engineering & business teams
You are a strong believer in scale and automation
You are entrepreneurial — you take initiative, solve problems and love to troubleshoot.
You are a great collaborator and can communicate effectively. You enjoy teaching and learning from your colleagues
You are not ideological about programming languages or tools. You have opinions but are open to discussion and tradeoffs
You are a pragmatist
You are a seeker of truth
Preferred Qualifications
Experience querying big data using Spark, Presto, Hive, Impala, etc.
Experience with data quality and validation
Experience with SQL performance tuning and E2E process optimization
Experience creating reports and dashboards with modern business intelligence tools (Tableau, Metabase preferred)
Experience working with Postgres, Hevo, and cloud or on-prem Big Data/MPP analytics platform (i.e. Snowflake, AWS Redshift, Google BigQuery, Azure Data Warehouse, Netezza, Teradata, or similar).
Interest or experience in working in the African Fintech Ecosystem
Experience in a high-growth team and/or startup experience
Ability to communicate and prioritise effectively with a distributed team around the world
Compensation
Salary commensurate with experience
Stock options
Healthcare
Opportunities for travel (Post-Covid19)
Autonomy and a chance to work at a mission-driven company with purpose
What Success Looks Like
Successes in your first 3 months include
Take the time and learn the ins and outs of our data warehouse and dashboards. Investigate how data is being logged in our systems and what existing data pipelines exist across our two data warehouses (Postgres and Redshift).
Evaluate existing dashboards, data quality, and pipelines and identify gaps.
Get introduced to the Product and Engineering team and their bi-weekly sprint processes. At this point you are mostly observing the dynamics and taking on tasks by the team, while building a partnership and exploring support opportunities.
In your first 6 months
Based on your initial exposure to our data stack, you have already built several improvements based on the gaps you've identified. You are able to manage the flow of data across the stack. You have extended our system capabilities as needed and have improved efficiency and simplicity of shared tools and libraries.
You are deeply embedded in how we set up data logging and are able to manage and successfully execute on data requirements from teams across the company (e.g. Engineering, Product, CVML, Data Science, Marketing, Commercial).
You proactively develop technical methodologies or tools which can solve important classes of problems. You can evangelize these methodologies and tools to other data scientists and engineers to scale multiple people.
In your first 12 months
You are the company expert in our data, infrastructure, and technical architecture, and are actively involved in product and business operations to either improve existing data tools or suggest new methodologies to accelerate team execution, including influencing data best practices.
You drive scalable solutions across teams.
You are able to solve challenging technical problems faced by multiple teams and provide significant technical advice to newer or less-technical analysts.
Show more
Show less","Data Infrastructure, ETL design, Data Warehousing, Schema Design, Dimensional Modeling, SQL, Python, Realtime Pipelines, Git, Github, Data Migrations, Data Tooling, Engineering, Automation, Crossfunctional Teams, Data Quality, SLA, Business Intelligence, Reporting, Dashboarding, Postgres, Hevo, Big Data Analytics, Spark, Presto, Hive, Impala, SQL Performance Tuning, E2E Process Optimization, Tableau, Metabase, Snowflake, AWS Redshift, Google BigQuery, Azure Data Warehouse, Netezza, Teradata, Startup Experience, Distributed Team Communication, Data Logging, Data Pipelines, Data Stack, Team Execution, Data Best Practices, Scalable Solutions, Technical Advice","data infrastructure, etl design, data warehousing, schema design, dimensional modeling, sql, python, realtime pipelines, git, github, data migrations, data tooling, engineering, automation, crossfunctional teams, data quality, sla, business intelligence, reporting, dashboarding, postgres, hevo, big data analytics, spark, presto, hive, impala, sql performance tuning, e2e process optimization, tableau, metabase, snowflake, aws redshift, google bigquery, azure data warehouse, netezza, teradata, startup experience, distributed team communication, data logging, data pipelines, data stack, team execution, data best practices, scalable solutions, technical advice","automation, aws redshift, azure data warehouse, big data analytics, business intelligence, crossfunctional teams, dashboard, data best practices, data infrastructure, data logging, data migrations, data quality, data stack, data tooling, datapipeline, datawarehouse, dimensional modeling, distributed team communication, e2e process optimization, engineering, etl design, git, github, google bigquery, hevo, hive, impala, metabase, netezza, postgres, presto, python, realtime pipelines, reporting, scalable solutions, schema design, sla, snowflake, spark, sql, sql performance tuning, startup experience, tableau, team execution, technical advice, teradata"
"Data Engineer, Data Platform",Grammarly,"Maine, United States",https://www.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3689960992,2023-12-17,Marietta,United States,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Data Engineering, Python, Scala, Java, SQL, API design, Microservices, AWS, Azure, GCE, Data lakes, Streaming data processing, System design, Internal tools, Admin sites, Open source software","data engineering, python, scala, java, sql, api design, microservices, aws, azure, gce, data lakes, streaming data processing, system design, internal tools, admin sites, open source software","admin sites, api design, aws, azure, data engineering, data lakes, gce, internal tools, java, microservices, open source software, python, scala, sql, streaming data processing, system design"
Data Analyst/Scientist,VeeAR Projects Inc.,"Cupertino, CA",https://www.linkedin.com/jobs/view/data-analyst-scientist-at-veear-projects-inc-3781753896,2023-12-17,San Jose,United States,Associate,Onsite,"Job Description:
Data analyst/scientist with experience in web analytics using clickstream data analysis.
Tableau and SQL, but they need to
also
have worked with clickstream data in the web analytics space for 2+ years
Show more
Show less","Data Analysis, Data Science, Web Analytics, Clickstream Analysis, Tableau, SQL","data analysis, data science, web analytics, clickstream analysis, tableau, sql","clickstream analysis, data science, dataanalytics, sql, tableau, web analytics"
Associate Data Scientist,Tencent,"Palo Alto, CA",https://www.linkedin.com/jobs/view/associate-data-scientist-at-tencent-3693997100,2023-12-17,San Jose,United States,Associate,Onsite,"Work Mode:
Onsite
Responsibilities:
【About Tencent
】
Tencent is a world-leading internet and technology company that develops innovative products and services to improve the quality of life of people around the world. Founded in 1998 with its headquarters in Shenzhen China, our guiding principle is to use technology for good.
We are not only a major video game publisher in the world, we also produce other high-quality digital content, enriching interactive entertainment experiences for people around the globe. We offer a range of services such as cloud computing, advertising, FinTech, and other enterprise services to support our clients' digital transformation and business growth.
【About Level Infinite
】
Level Infinite is a global gaming brand dedicated to delivering high-quality and engaging interactive entertainment experiences to a worldwide audience, wherever and however they choose to play. It operates from bases in Amsterdam and Singapore with staff around the world.
To learn more about Level Infinite, visit www.levelinfinite.com, and follow on Twitter,Facebook, Instagram and YouTube.
【Responsibilities】
Technically lead one of the following areas (depending on your expertise): Marketing, Machine Learning Engineering, Game Science.
Collaborate closely with product and engineering teams to define and execute data strategies.
Take ownership of the product, identify growth opportunities, and develop hypotheses and data-informed strategies for growth and an enhanced user experience.
Work with large-scale datasets to solve complex business and technical problems through analytics, forecasting, machine learning, recommender system, generative AI, experimentation, causal inference, and econometrics.
Constantly learn the latest technologies in data science, effectively utilize advanced technologies and tools for application attempts, and explore innovation in applied research.
Broadly explore opportunities with an open mindset; flexibly employ various data science methods to identify high-impact opportunities, initiate project ideas, prototype solutions, and drive project completion.
Contribute to data science best practices, processes, training, and sharing; cultivate data science culture and technical advancements.
Demonstrate leadership and participate in team goal setting and planning.
Requirements:
【Requirements】
M.S., or Ph.D. in a quantitative discipline such as Statistics, Math, Economics, Computer Science, Machine Learning, or equivalent practical experience.
Domain expertise and 3+ years of industry experience in one of the following areas: Marketing, Machine Learning Engineering, Game Science.
Excellent communication, collaboration, execution capabilities, and keen business acumen.
Experience programming in SQL, Python, or related languages.
Experience with Big Data technologies such as Hadoop, Hive, Spark, etc.
Experience with building complex data products and machine learning systems on large datasets preferred.
Experience with generative AI applications preferred.
Prior experience in publishing papers at top conferences in the field such as NeurIPS, ICML, AAAI, KDD, WSDM, RecSys, etc. is preferred.
Passionate about games and the gaming industry.
[DEI Statement]
Diversity, Equity & Inclusion at Tencent:
Diversity, equity and inclusion are important, interdependent components of our workplace. As an equal opportunity employer, we firmly believe that diverse voices fuel our innovation and allow us to better serve our users and the community. We foster an environment where every employee of Tencent feels supported and inspired to achieve individual and common goals.
Location State(s)
California
The base pay range for this position in the state(s) above is $72,050 to $144,410 per year. Actual pay is based on market location and may vary depending on job-related knowledge, skills, and experience. A sign on payment, relocation package, and restricted stock units may be provided as part of the compensation package, as well as other medical, financial, and/or other benefits, dependent on the specific position offered.
Show more
Show less","Statistics, Math, Economics, Computer Science, Machine Learning, SQL, Python, Hadoop, Hive, Spark, Data products, Machine learning systems, Generative AI, NeurIPS, ICML, AAAI, KDD, WSDM, RecSys","statistics, math, economics, computer science, machine learning, sql, python, hadoop, hive, spark, data products, machine learning systems, generative ai, neurips, icml, aaai, kdd, wsdm, recsys","aaai, computer science, data products, economics, generative ai, hadoop, hive, icml, kdd, machine learning, machine learning systems, math, neurips, python, recsys, spark, sql, statistics, wsdm"
Data Scientist Associate,Tencent,"Palo Alto, CA",https://www.linkedin.com/jobs/view/data-scientist-associate-at-tencent-3694279863,2023-12-17,San Jose,United States,Associate,Onsite,"Work Mode:
Onsite
Responsibilities:
Join a highly-skilled data science team to mine through data to identify data-driven opportunities in game production, publishing, and live-ops. * Solve complex business problems using advanced methods, including machine learning, deep learning, reinforcement learning, generative AI, experimentation, statistical inference, causal inference, econometrics, operational research, etc. * Establish scalable, efficient, automated processes, and provide end-to-end data products and online optimization systems. * Interact cross-functionally with various teams and provide actionable business insights to stakeholders. * Research and develop novel modeling and inference solutions to support game development and business decision-making. * Continuously improve the accuracy, efficiency, and scalability of existing models and algorithms. * Stay up-to-date with the latest advancements in data science techniques and technologies.
Requirements:
Ph.D. with a specialization in computer science, operational research, mathematics, statistics, economics, or a quantitative field. * Deep and extensive knowledge in one or more of the following areas: generative AI, deep learning, reinforcement learning, causal inference, natural language processing, computer vision/graphics, recommendation systems, econometrics, and other data science domains. * Expert in statistical analysis (probabilities, hypothesis testing, multivariate analysis, time series analysis, generalized linear model, A/B experiments). * Proficient in SQL and Python (or other higher-level programming languages like Java, C++, Scala, etc.). * Experience with Big Data technologies such as Hadoop, Spark, Hive, etc. is preferred. * Ability to understand complex subjects clearly and propose actionable data-driven solutions. Good business acumen and communication skills are a plus.
Location State(s)
California
The base pay range for this position in the state(s) above is $72,050 to $144,410 per year. Actual pay is based on market location and may vary depending on job-related knowledge, skills, and experience. A sign on payment, relocation package, and restricted stock units may be provided as part of the compensation package, as well as other medical, financial, and/or other benefits, dependent on the specific position offered.
Show more
Show less","Data Science, Machine Learning, Deep Learning, Reinforcement Learning, Generative AI, Experimentation, Statistical Inference, Causal Inference, Econometrics, Operational Research, SQL, Python, Hadoop, Spark, Hive, C++, Scala, Java","data science, machine learning, deep learning, reinforcement learning, generative ai, experimentation, statistical inference, causal inference, econometrics, operational research, sql, python, hadoop, spark, hive, c, scala, java","c, causal inference, data science, deep learning, econometrics, experimentation, generative ai, hadoop, hive, java, machine learning, operational research, python, reinforcement learning, scala, spark, sql, statistical inference"
"Senior Software Engineer, Data Platform",Nuro,"Mountain View, CA",https://www.linkedin.com/jobs/view/senior-software-engineer-data-platform-at-nuro-3779945843,2023-12-17,San Jose,United States,Mid senior,Hybrid,"Who We Are
Nuro exists to better everyday life through robotics. The company’s custom electric autonomous vehicles are designed to bring the things you need—from produce to prescriptions—right to your home. Nuro’s autonomous, goods-focused solution can give you valuable time back and more freedom to do what you love. This convenient, eco-friendly alternative to driving has the potential to make streets safer and cities more livable.
About The Role
The Evaluation Infrastructure team is pivotal in the development of autonomous driving systems, as it constructs the essential systems required to assess both the overall system's performance and that of individual components. Collaborating intimately with Machine Learning (ML) engineers, you will be involved in evaluating various ML models integral to the autonomous driving system, including perception, prediction, planning, and more. Furthermore, you will engage closely with system engineers to rigorously validate the autonomous driving system prior to its release.
About The Work
Design and develop unified, introspectable, large-scale batch and streaming data processing systems that can ingest and process data across a wide range of use cases relevant to evaluation.
Create and implement a storage system capable of accommodating both the large volume and diverse range of evaluation and performance metrics.
Construct intuitive dashboards and reports to present evaluation results, facilitating straightforward comparisons that highlight both improvements and regressions.
Design and develop comprehensive end-to-end data pipelines that streamline the flow from data ingestion to final consumption.
Develop and maintain continuous testing and monitoring systems to guarantee the integrity and resilience of our data and associated data pipelines
About You
B.Sc or M.Sc. plus 2+ years of relevant work experience
Strong proficiency in Python, C++, or similar languages
Domain experience: Experience working with large scale data and building scalable & reliable systems / data pipelines; ability to understand and design complex systems
Engineering leadership: Experience setting team or project product and technical vision, timelines and prioritization; formally or informally being a Tech Lead, mentoring and support junior engineers
Technical excellence: Ability and willingness to deep dive into implementation, driving technical standards and best practices across broader software organization
A bachelor's degree in Computer Science, Electrical Engineering, or a closely related field
Bonus Points
Knowledge of data engineering, and its tooling and best practices
Knowledge of batch and streaming data processing, warehousing, and analytics solutions
Experience working with large scale distributed data systems
Experience with system & framework design
Experience with data workflow orchestration platforms
At Nuro, your base pay is one part of your total compensation package. For this position, the reasonably expected pay range is between $167,200 and $250,800/year for the level at which this job has been scoped. Your base pay will depend on several factors, including your experience, qualifications, education, location, and skills. In the event that you are considered for a different level, a higher or lower pay range would apply. This position is also eligible for an annual performance bonus, equity, and a competitive benefits package.
At Nuro, we celebrate differences and are committed to a diverse workplace that fosters inclusion and psychological safety for all employees. Nuro is proud to be an equal opportunity employer and expressly prohibits any form of workplace discrimination based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, veteran status, or any other legally protected characteristics.
Show more
Show less","Machine Learning, Python, C++, Data Engineering, Batch Processing, Streaming Data Processing, Warehousing, Analytics, Data Pipelines, Data Workflow Orchestration, System Design, Framework Design, Distributed Systems, Big Data","machine learning, python, c, data engineering, batch processing, streaming data processing, warehousing, analytics, data pipelines, data workflow orchestration, system design, framework design, distributed systems, big data","analytics, batch processing, big data, c, data engineering, data workflow orchestration, datapipeline, datawarehouse, distributed systems, framework design, machine learning, python, streaming data processing, system design"
"Senior Software Engineer, ML Data Tools and Pipeline",Nuro,"Mountain View, CA",https://www.linkedin.com/jobs/view/senior-software-engineer-ml-data-tools-and-pipeline-at-nuro-3725721549,2023-12-17,San Jose,United States,Mid senior,Hybrid,"Who We Are
Nuro exists to better everyday life through robotics. The company’s custom electric autonomous vehicles are designed to bring the things you need—from produce to prescriptions—right to your home. Nuro’s autonomous, goods-focused solution can give you valuable time back and more freedom to do what you love. This convenient, eco-friendly alternative to driving has the potential to make streets safer and cities more livable.
About The Role
Nuro takes a machine-learning-first approach to autonomous driving technology. In an ML-first system, the overall system performance depends heavily on the quantity and diversity of its training and evaluation data.
The ML Data Tools and Pipeline team develops dataset management and annotation software. We are responsible for developing manual labeling tools and automated labeling pipelines to annotate a variety of sensor data with excellent quality and low cost.
The end result is a scalable suite of tools that enables every ML researcher at Nuro — across every discipline from computer vision to reinforcement learning — to generate exactly the right dataset to improve their models.
About The Work
As a Senior Software Engineer, you’ll build business-critical tools that accelerate the development of Nuro’s autonomous driving technology.
Architect fully automated labeling pipelines that leverage the intelligence of ML models, including both in-house and large-scale pretrained models
Develop manual and ML-assisted labeling tools to annotate image, lidar, audio, text, and other training data with low cost and great accuracy
Engineer annotation processing pipelines that handle petabytes of data with excellent performance and fault tolerance.
Design backend APIs to enable ML developers to perform rich queries and experimentation on their datasets of interest
Collaborate with operations teams (both in-house and outsourced) to understand operation specialists’ needs and deliver easy-to-use products.
Collaborate with ML engineers from mapping, perception, prediction, and planning teams to understand the onboard system architecture and how training data fits in.
About You
You have 3+ years of software development experience in a language such as C++, Rust.
You’re deeply technical with a scrappy mindset. You can jump into any system, quickly understand what’s going on, and make changes to keep the project moving.
You are customer-obsessed: passionate about delivering a great product that solves problems for client teams.
You bring up those around you through mentorship and feedback. You’re always on the lookout for ways to improve our team’s products, people, and processes.
You collaborate effectively with colleagues from a wide range of disciplines and backgrounds. You feel comfortable navigating situations with many stakeholders and requirements to consider.
Bonus Points
Past experience developing internal tools for autonomous driving, robotics, data annotation, crowdsourcing, content moderation, trust & safety, computer-aided design, and/or geospatial information.
Past experience with large scale data pipelines or data migration.
At Nuro, your base pay is one part of your total compensation package. For this position, the reasonably expected pay range is between $167,200 and $250,800/year for the level at which this job has been scoped. Your base pay will depend on several factors, including your experience, qualifications, education, location, and skills. In the event that you are considered for a different level, a higher or lower pay range would apply. This position is also eligible for an annual performance bonus, equity, and a competitive benefits package.
At Nuro, we celebrate differences and are committed to a diverse workplace that fosters inclusion and psychological safety for all employees. Nuro is proud to be an equal opportunity employer and expressly prohibits any form of workplace discrimination based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, veteran status, or any other legally protected characteristics.
Show more
Show less","Machine learning, Data annotation, Computer vision, Reinforcement learning, C++, Rust, Software development, Data pipelines, Data migration, Backend APIs, Largescale data pipelines","machine learning, data annotation, computer vision, reinforcement learning, c, rust, software development, data pipelines, data migration, backend apis, largescale data pipelines","backend apis, c, computer vision, data annotation, data migration, datapipeline, largescale data pipelines, machine learning, reinforcement learning, rust, software development"
Engineer- Data - IV (1011644),The Judge Group,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/engineer-data-iv-1011644-at-the-judge-group-3743032870,2023-12-17,San Jose,United States,Mid senior,Hybrid,"Location:
Sunnyvale, CA
Salary:
$80.00 USD Hourly - $90.00 USD Hourly
Description:
Our client is currently seeking a Engineer- Data - IV (W2 ONLY)
[
Additional Description
]
Job Title: Data Engineer- IV (w2)
Location: San Jose, CA(Remote)
Duration: 12+ Months Contract
Job Description
What you?ll be doing:
The ** Data Breach Investigations Report (DBIR) is the most respected cybersecurity report in the industry, analyzing incident and breach data from dozens of data contributors and presenting them with both academic rigor and a lighthearted and approachable tone. The Data Engineer role in the ** DBIR team is responsible for supporting the tools and infrastructure used to analyze the data for the report. The Data Engineer is responsible for maintaining and improving on R and Python code that makes up the tooling structure for the report?s data pipeline and the monitoring and deployment of such tooling on AWS. Some of the tools developed by the DBIR team are publicly available on GitHub, and as such their documentation and deployment procedures much be kept up to date focusing on ease of use and accessibility. As such, the data engineer should have previous experience in developing and maintaining codebases in Python and R for data pipelines and knowledge on how to automate and increase performance of such tasks. This role can be performed remotely in the US anywhere ** supports remote work.
Responsibilities
Support the development and deployment of analytics (including predictive models, machine learning techniques, and analytical reporting) for projects
Construct, configure, and modify both the components and the code which form the execution environment for the AI/ML models.
Research, engineer and build network automation solutions to drive efficiency of next generation networks.
Be detail oriented, think critically to solve issues in real-time.
MUST HAVE SKILLS (Most Important): - Experience in system development. The data engineer must have 5 to 10 years developing and supporting software in R (bigger focus) and Python (smaller focus). - Experience in AWS DevOps and data pipelines. The data engineer must have at least 4 years of experience in the following technologies: AWS Platform in general, Docker, Airflow, Jupyter notebook deployment. - Experience in instrumenting and monitoring data pipeline workloads, with a focus in improving automation and performance. - Technical writing skills to develop and maintain documentation for the tooling and data pipeline. - Interest in the Cybersecurity field, the different kinds of datasets it can produce and their challenges.
DESIRED SKILLS: - Experience of any kind in the Cybersecurity field - Experience in managing or supporting Open Source projects in R and Python with a focus on data science.
EDUCATION/CERTIFICATIONS: - Computer Science degree from a 4-year University preferred but not required. - AWS DevOps certifications preferred but not required.
Contact:
rsingh08@judge.com
This job and many more are available through The Judge Group. Find us on the web at www.judge.com
Show more
Show less","Data Breach Investigations Report (DBIR), Python, R, AWS, DevOps, Docker, Airflow, Jupyter, Automation, Performance, Writing, Cybersecurity, Open Source, Data Science, Computer Science","data breach investigations report dbir, python, r, aws, devops, docker, airflow, jupyter, automation, performance, writing, cybersecurity, open source, data science, computer science","airflow, automation, aws, computer science, cybersecurity, data breach investigations report dbir, data science, devops, docker, jupyter, open source, performance, python, r, writing"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Waltham, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744391961,2023-12-17,Natick,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pysPark, SQL, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data Warehouses, ETL, Legal compliance, Data management, Data classification, Retention, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, legal compliance, data management, data classification, retention, tdd, pair programming, continuous integration, automated testing, deployment","airflow, automated testing, continuous integration, data classification, data management, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, retention, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Data Analyst & QA Tester,Experfy,"Cambridge, MA",https://www.linkedin.com/jobs/view/data-analyst-qa-tester-at-experfy-3590301300,2023-12-17,Natick,United States,Mid senior,Onsite,"Our client is looking for a Data Analyst to join our digital data team to ensure that the data available as-a-service in global data warehouses meets business requirement, with the right quality for enabling enterprise data and analytics and insights initiatives. Maintains the data catalog and ensure compliance of data manipulation with industry standard practices. The bulk of the work covers partnering with data engineers, data scientists and product owners for data analysis and integration requirements and guiding solution design and architecture.
Tools
Good knowledge of cloud computing. Knowing AWS is a plus
SQL, Excel, PowerPoint, Word
Tableau and Power BI for data exploration is plus
ETL tools knowledge is plus
Snowflake datamart knowledge is plus
Project management & support: JIRA projects & service desk, Confluence, Teams
Skills
Expert in Data manipulation techniques (esp. querying tables with SQL)
Strong capacity to develop consistent, clear analysis and report potential issues
Data quality audit techniques (Electronic Data Discovery)
Expert in applications and product testing
Familiarity with data science methods and scaling data science methods
Soft Skills
Pragmatic and capable of solving complex issues
Ability to understand business needs
Good communication
Service-oriented, flexible, positive team player
Self-motivated, takes initiative
Rigorous, attention to detail to map data systems
Curious to understand what is behind each data field
Experience
3 to 5 years of experience as Business Analyst/QA Tester (performed data manipulation, data analysis, master data management, application/product testing)
Experience in a healthcare company is a strong plus
Preferred Qualifications
Master's Degree in Mathematics, Economics, Computer Science, Information Management or Statistics
Requirements
Responsibilities
Understand business data requirements and translate them into technical needs, and deliver prompt, efficient, quality service
Build and maintain the data model used in global data warehouses and solutions
Define the concepts and attributes of the data and manage the data dictionary of the tables
Keep up to date data catalog with relevant data available and their attributes
Validate quality of data and steer data quality improvement action plans
Ensure and assess data quality on global data warehouses, building KPIs (incident management, completeness of data, etc.) and setting up and automate consistency checks
Identify root causes of potential malfunctions and allocate actions to the right stakeholders
Partner with Product Owners and Data Engineers to prioritize the pipeline implementation plan
Partner with Data Engineers and Data scientists to ensure pipelines meets business requirements
Partner with data domain owner and procurement team in case of need to buy or sell data
Partner with legal team regarding GDPR and other legal requirements
Actively contribute to Data governance community
Guide solution design to meet data and integration requirements
Identify new opportunities to leverage data and to integrate data or applications to enhance business value
Coordinate with data owners to ensure data accessibility and quality
Create detailed, comprehensive, and well-structured test plans and test cases
Collaborate with Digital Quality Operations team to ensure that test plans meet regulatory/compliance requirements where applicable
Estimate, prioritize, plan and coordinate testing activities - Unit, SIT, Regression, and UAT
Design, develop and execute testing
Develop and apply testing processes for new and existing products to meet client needs
Show more
Show less","Data Analysis, Data Manipulation, SQL, Cloud Computing, AWS, Tableau, Power BI, ETL Tools, Snowflake Datamart, JIRA, Confluence, Teams, Data Quality Audit Techniques, Electronic Data Discovery, Data Science Methods, Business Analysis, QA Testing, Master Data Management, Application Testing, Data Modeling, Data Dictionary, Data Catalog, Data Quality Management, KPI, Incident Management, Data Governance, GDPR","data analysis, data manipulation, sql, cloud computing, aws, tableau, power bi, etl tools, snowflake datamart, jira, confluence, teams, data quality audit techniques, electronic data discovery, data science methods, business analysis, qa testing, master data management, application testing, data modeling, data dictionary, data catalog, data quality management, kpi, incident management, data governance, gdpr","application testing, aws, business analysis, cloud computing, confluence, data catalog, data dictionary, data governance, data manipulation, data quality audit techniques, data quality management, data science methods, dataanalytics, datamodeling, electronic data discovery, etl tools, gdpr, incident management, jira, kpi, master data management, powerbi, qa testing, snowflake datamart, sql, tableau, teams"
"Manager, Data Engineering Lead",Pfizer,"Sandwich, England, United Kingdom",https://uk.linkedin.com/jobs/view/manager-data-engineering-lead-at-pfizer-3781120275,2023-12-17,Thanet, United Kingdom,Mid senior,Onsite,"Role Summary
The Enterprise Platforms & Security (EP&S) organization delivers the following capabilities for Pfizer. Business application platforms supporting Pfizer’s enterprise application and critical business processes. Infrastructure allowing business traffic to travel where it needs to go, internally and externally, along with the appropriate access controls. EP&S secures Pfizer's most important information assets through world class controls and protections and enables Pfizer's business results by making security an enabler and not a roadblock to achieving business results.
The Manager Data Engineering Lead is responsible for the development and end-to-end technical hands-on management of the Data Engineering process within the Enterprise Platform and Security Analytics function. This is a hands-on data engineering role which primarily supports our large-scale analytics ecosystem powered by AWS and Splunk. Its scope includes a wide range of highly transactional platforms and cybersecurity data sources with diverse collection points and complicated collection methods. Ensuring data reliability, efficiency, and quality is core to this role.
For Pfizer, we look for candidates that are motivated, self-learning, and team-oriented individuals. From a technical perspective, an ideal candidate would have the skills shown below, but candidates that possess a strong subset and an attitude towards self-development and growth will be considered.
Role Responsibilities
Design, manage, and troubleshoot complex large-scale data engineering methods within a hybrid on premise and cloud hosted environments.
Utilize vendor apps and develop custom app configurations to standardize and normalize diverse data source types.
Develop and maintain service patterns used for the data engineering process and data collection methods.
Partner with internal and external teams to implement solutions which improve data engineering processes and enable automated and/or self-service data onboarding.
Collaborate with the Service Manager, Data Stewards, and customers to support and prioritize business requirements.
Support the Data Management Lifecycle engineering processes from inception and design through deployment, operation, and optimization.
Ensure documentation details the methods to collect, triage and backfill data feeds to support monitoring and data resiliency.
Proactively, continuously assess and identify opportunities to better maintain and reduce ingestion volume.
Manage technical work activities of contingent worker resources to engineer data and ensure procedures and standards are adhered to.
Ensure high data reliability, efficiency, and quality standards are maintained and continuously improve engineering practices and processes.
Support 24x7 oversight of Business as Usual (BAU) operations, along with continual monitoring of the service for quality levels, and response to outages or performance issues with a sense of urgency.
Participate in incident, problem, and change management processes.
Qualifications
Bachelor's degree in Computer Science or physical sciences, Master's degree preferred.
Robust experience in a hands-on data engineering role.
Flexible to changing priorities and comfortable in a fast-passed dynamic environment.
Superior analytical and creative problem-solving skills. Demonstrate successes in analysis, conclusions, and improvement.
Strong problem-solving abilities with an analytic and qualitative eye for reasoning under pressure.
Self-starter with the ability to independently prioritize and complete multiple tasks with little to no supervision.
Experience developing software code in one or more programming languages (Java, JavaScript, Python, etc).
Experience in designing, developing, and implementing advanced data collection methods, sizing for data storage, index strategies, ingesting/indexing processes, transforming/normalizing data to common standards, data enrichment and/or anonymization of data upon ingest.
Can showcase experience with Unix/Linux operating system.
Preferred Qualifications
Robust hands-on experience in implementation and performance tuning of Kinesis, Kafka, Spark or similar implementations.
Strong familiarity with data engineering in Splunk
Experience Implementing AWS automation services and process build to create syslog and HEC ingestion into AWS for processing and optimized data flow.
Extensive experience with design, development and operations that leverages deep knowledge in the use of services like Amazon Kinesis, Apache Kafka, Apache Spark, Amazon Sagemaker, Amazon EMR, NoSQL technologies
Experience with one or more relevant tools (Flink, Spark, Sqoop, Flume, Kafka, Amazon Kinesis)
Experience transforming large datasets into consumable assets for self-service analytics and reporting.
Familiar with Machine learning concepts.
Purpose
Breakthroughs that change patients' lives
... At Pfizer we are a patient centric company, guided by our four values: courage, joy, equity and excellence. Our breakthrough culture lends itself to our dedication to transforming millions of lives.
Digital Transformation Strategy
One bold way we are achieving our purpose is through our company wide digital transformation strategy. We are leading the way in adopting new data, modelling and automated solutions to further digitize and accelerate drug discovery and development with the aim of enhancing health outcomes and the patient experience.
Flexibility
We aim to create a trusting, flexible workplace culture which encourages employees to achieve work life harmony, attracts talent and enables everyone to be their best working self. Let’s start the conversation!
Equal Employment Opportunity
We believe that a diverse and inclusive workforce is crucial to building a successful business. As an employer, Pfizer is committed to celebrating this, in all its forms – allowing for us to be as diverse as the patients and communities we serve. Together, we continue to build a culture that encourages, supports and empowers our employees.
DisAbility Confident
We are proud to be a Disability Confident Employer and we encourage you to put your best self forward with the knowledge and trust that we will make any reasonable adjustments necessary to support your application and future career. Our mission is unleashing the power of our people, especially those with unique superpowers. Your journey with Pfizer starts here!
Information & Business Tech
Show more
Show less","Data Engineering, AWS, Splunk, Java, JavaScript, Python, Unix/Linux, Kinesis, Kafka, Spark, Amazon Sagemaker, Amazon EMR, Flink, Sqoop, Flume, NoSQL, Machine Learning","data engineering, aws, splunk, java, javascript, python, unixlinux, kinesis, kafka, spark, amazon sagemaker, amazon emr, flink, sqoop, flume, nosql, machine learning","amazon emr, amazon sagemaker, aws, data engineering, flink, flume, java, javascript, kafka, kinesis, machine learning, nosql, python, spark, splunk, sqoop, unixlinux"
Sr. Technical Data Analyst,Dice,"Quincy, MA",https://www.linkedin.com/jobs/view/sr-technical-data-analyst-at-dice-3788099591,2023-12-17,Duxbury,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Intellisoft Technologies, is seeking the following. Apply via Dice today!
Role: Sr. Technical Data analyst
Hybrid 2-3 days onsite Quincy, MA per week
Long term Contract
POSITION SUMMARY:
This position is for Senior Technical Data Analyst in Massachusetts. A prior knowledge of Informatica is also preferred. Experience working in government or with governmental agencies is desirable. The ideal candidate will be most comfortable with being hands-on in the latest in data transformation and ETL tools, with the ability to code in Informatica.
DETAILED LIST OF ROLE AND RESPONSIBILITIES:
Responsible for providing technology leadership, guidance and mentoring for Medicaid system with regards to data preparation, ETL, and EDI.
Develop data architecture solutions spanning across business policies, systems, applications and networking environments and BI platforms (e.g. PowerBI or Tableau).
Collect and analyze business and technical requirements and translate them into information technology solutions. Detail and document the logical and code-based designs as per methodologies, policies, standards, and patterns.
Collaborate with the Architecture Teams to assist in defining and managing standards, guidelines, and processes for infrastructure technology and software developments.
Participate in writing of RFPs (request for proposal, quotation, etc.) and SOW (statement of work) for infrastructure solutions and in contract and cost negotiations with vendors for SaaS solutions, infrastructure hardware, software, and professional services in the topic of ETL.
Collaborate with Business Functional Leads, Enterprise Infrastructure, and 3rd parties to define, design implement secure, scalable, extensible, and cost-effective solutions in alignment with business strategy and objectives.
Work in a matrixed environment with infrastructure and applications teams, data warehouse architects, security architects, and strategic partners to create an architecture accommodating the business strategy.
Have working knowledge of systems design using Oracle Suite, Informatica Secure agent, and Informatica Intelligent Cloud Service (IICS).
Provide the best feasible solutions for data analytics applications and services according to specifications and within agreed upon time frame.
Identify and manage technology, architecture and infrastructure risks and lead in developing and implementing appropriate data risk mitigation strategies.
Develop architectural proofs of concept to demonstrate practical business agile solutions and evolve core architectural constructs.
SKILLS:
Thorough understanding of modern enterprise architecture and hands-on experience with data manipulation tools and frameworks (Informatica preferred).
Excellent knowledge of multiple data-centric technologies and current computing trends.
Skilled at translating business strategy and project portfolio into short and long-term data transformation plans, detailed requirements, and models.
Hands-on expertise with cloud-native applications as well as existing mission-critical enterprise applications in the commercial cloud (AWS preferred).
Have a working knowledge of Risk Management, Disaster Recovery, Business Continuity, IT Security Architecture, and IT Regulatory Compliance.
Able to rapidly comprehend the functions and capabilities of new technologies and apply them to improve services or address issues.
Able to create estimates of the time and resource requirements for different activities, prioritize activities and determine which activities can be completed in parallel and in sequence.
Able to handle the most complex issues and possess analytical and problem-solving skills.
Strong written and verbal communication skills, including the ability to facilitate meetings, conduct presentations and effectively lead discussions.
Experience in Healthcare domain/healthcare data is strongly preferred.
Experience in documenting technical artifacts (Environment, TAD, Runbook etc...)
QUALIFICATIONS: (Experience and Education)
Bachelor’s degree or higher in Information Systems, IT with 10+ years’ experience, or equivalent in healthcare administration setting.
Show more
Show less","Informatica, PowerBI, Tableau, ETL, EDI, Oracle Suite, Informatica Secure Agent, Informatica Intelligent Cloud Service (IICS), Cloudnative applications, AWS, Risk Management, Disaster Recovery, Business Continuity, IT Security Architecture, IT Regulatory Compliance","informatica, powerbi, tableau, etl, edi, oracle suite, informatica secure agent, informatica intelligent cloud service iics, cloudnative applications, aws, risk management, disaster recovery, business continuity, it security architecture, it regulatory compliance","aws, business continuity, cloudnative applications, disaster recovery, edi, etl, informatica, informatica intelligent cloud service iics, informatica secure agent, it regulatory compliance, it security architecture, oracle suite, powerbi, risk management, tableau"
"Data Center/Systems Administrator (Iinux, IIS, VMWare , Apache, PDU, IP-KVM) Hillsboro, Oregon -JV",Cube Hub Inc.,"Hillsboro, OR",https://www.linkedin.com/jobs/view/data-center-systems-administrator-iinux-iis-vmware-apache-pdu-ip-kvm-hillsboro-oregon-jv-at-cube-hub-inc-3751440006,2023-12-17,McMinnville,United States,Mid senior,Onsite,"Job Title: Data Center/Systems Administrator
Location - Hillsboro, Oregon
Working hours: Mon-Fri, 8am-5pm
job duration: 12 months
This role requires the candidate to be onsite
Job Description
Key responsibilities include performing general server administration tasks, monitoring and optimizing system performance and reliability, operational workflow development, and managing enhancements/upgrades and providing
Various Ievels of support.
Develops and maintain system documentation for
Iab/Data Center configuration
and customizations.
Designs, develops, and maintains custom templates and reports for performance and proof of concept testing.
Provides application and server Ievel support for server environments and serve as the Subject Matter Expert for asset management system.
Lead Technical efforts for software upgrade, patch updates, data migration, for server software. Apply configuration and tuning standards in accordance with Microsoft and Iinux recommendations and clients requirements.
Develop and maintain system documentation for server/software configuration and customizations.
Conduct system performance analysis and performance improvements in collaboration with Architects, Engineers and Network Engineer to insure system efficiency.
Provide Tier 2 and Tier 3 Ievel support.
Strong knowledge of
Virtualization Technology including
VMWare ESX(i),
Citrix Administration
Skills and Netscaler.
Minimum 3-5 years experience with
Windows and Iinux (UNIX) server administration
including Active Directory, Domain Name Services and
DHCP
.
Expertise in administration of
IIS (Internet Information Server) and Apache
Web Servers including installation, configuration, monitoring, upgrade and Web Application installation. Knowledge of computer diagnostics and installation, to include hardware/software troubleshooting and networking.
Demonstrated experience working with
infrastructure
related
components
such as running
network cables, installing servers
in equipment Hillsboro, Oregon.
Familiarity with the operation and configuration of networking protocols, network interface card installation, switches , routers and similar components
Ability to work in a fast paced environment and offer effective solutions under tight deadlines
Strong problem-solving and root cause analysis skills
Ability to work after hours support on a rotating schedule
Must be capable of lifting 1
U and 2U rack mounted servers up to 40 pounds
and familiar with the use of
Rack Jacks and general data center
safety procedures
Follow written and/or - Verbal instructions for custom operating system and application installs Maintaining and auditing Iab assets and routine inventory control
Additional Skills And Or Education
Capable of Iearning and using custom built asset and inventory tracking software (training will be provided)
Knowledge of data center power system i.e.
PDU, 120/240v
would be helpful.
Knowledge of wire and cable management would be a plus.
Knowledge of
IP-KVM
would be a plus Years of experience required for this positions:
At Ieast five years direct I.T. industry experience
Bachelor's degree required
Show more
Show less","Server Administration, System Performance Monitoring, Reliability Optimization, Operational Workflow Development, Enhancements and Upgrades Management, System Documentation, Lab and Data Center Configuration, Custom Templates and Reports Design, Application and Server Level Support, Asset Management System Expertise, Software Upgrade and Patch Updates, Data Migration, Configuration and Tuning Standards, System Performance Analysis, Virtualization Technology, VMWare ESX(i), Citrix Administration, Skills and Netscaler, Windows Server Administration, Linux Server Administration, Active Directory, Domain Name Services, DHCP, IIS Web Server, Apache Web Server, Hardware and Software Troubleshooting, Networking, Network Cables Installation, Servers Installation, Networking Protocols, Network Interface Cards, Switches, Routers, ProblemSolving, Root Cause Analysis, Rack Jacks, Data Center Safety Procedures, Custom Operating System and Application Installs, Lab Asset Maintenance and Auditing, Inventory Control, Custom Asset and Inventory Tracking Software, Data Center Power Systems, PDU, Wire and Cable Management, IPKVM","server administration, system performance monitoring, reliability optimization, operational workflow development, enhancements and upgrades management, system documentation, lab and data center configuration, custom templates and reports design, application and server level support, asset management system expertise, software upgrade and patch updates, data migration, configuration and tuning standards, system performance analysis, virtualization technology, vmware esxi, citrix administration, skills and netscaler, windows server administration, linux server administration, active directory, domain name services, dhcp, iis web server, apache web server, hardware and software troubleshooting, networking, network cables installation, servers installation, networking protocols, network interface cards, switches, routers, problemsolving, root cause analysis, rack jacks, data center safety procedures, custom operating system and application installs, lab asset maintenance and auditing, inventory control, custom asset and inventory tracking software, data center power systems, pdu, wire and cable management, ipkvm","active directory, apache web server, application and server level support, asset management system expertise, citrix administration, configuration and tuning standards, custom asset and inventory tracking software, custom operating system and application installs, custom templates and reports design, data center power systems, data center safety procedures, data migration, dhcp, domain name services, enhancements and upgrades management, hardware and software troubleshooting, iis web server, inventory control, ipkvm, lab and data center configuration, lab asset maintenance and auditing, linux server administration, network cables installation, network interface cards, networking, networking protocols, operational workflow development, pdu, problemsolving, rack jacks, reliability optimization, root cause analysis, routers, server administration, servers installation, skills and netscaler, software upgrade and patch updates, switches, system documentation, system performance analysis, system performance monitoring, virtualization technology, vmware esxi, windows server administration, wire and cable management"
DMATS JR. Network Data Engineer,"Mission Technologies, a division of HII","Hurlburt Field, FL",https://www.linkedin.com/jobs/view/dmats-jr-network-data-engineer-at-mission-technologies-a-division-of-hii-3777397069,2023-12-17,Fort Walton Beach,United States,Mid senior,Onsite,"Requisition Number: 16622
Required Travel: 0 - 10%
Employment Type: Full Time/Salaried/Exempt
Security Clearance: Secret
Level of Experience: Entry Level
This opportunity resides with
Live, Virtual, Constructive Solutions
, a business group within HII's Mission Technologies division. As a trusted partner to our military customers, we design, develop and operate systems that bring together service members from across the globe to help you train like you fight, because we understand that preparation requires full coordination-not readiness in piece parts.
Meet HII's Mission Technologies Division
Our team of more than 7,000 professionals worldwide delivers all-domain expertise and advanced technologies in service of mission partners across the globe. Mission Technologies is leading the next evolution of national defense - the data evolution - by accelerating a breadth of national security solutions for government and commercial customers. Our capabilities range from C5ISR, AI and Big Data, cyber operations and synthetic training environments to fleet sustainment, environmental remediation and the largest family of unmanned underwater vehicles in every class. Find the role that's right for you. Apply today. We look forward to meeting you.
Summary
HII Mission Technologies is seeking a Junior Network Data Engineer for the Decisive Mission Actions and Technology Services (DMATS) with the US Air Force Special Operations Command (AFSOC) at Hurlburt Field, Florida. The selected candidate must The selected candidate will support the design, implementation and maintenance of DoD computer networks, enabling communication between multiple computer based systems.
What you will do
Assist in the Identification and resolution of network issues, perform routine maintenance and protect data from cyber-attacks.
Supports the design and maintenance of the hardware and software of a network.
Works with the senior Network Engineer to Conduct testing of network systems.
Performs routine network maintenance, including basic troubleshooting and installation of upgrades and service packs.
Consults with network engineering team to suggest network solutions
Test and install new computer systems, hardware, software, and applications as per DoD and customer guidance.
Explore ways to improve network performance or reduce network costs
Maintain analytical systems, verifies the accuracy of the data, and act as liaison.
Manage all aspects of end-to-end data processing utilizing customized report building functions of systems.
Maintain technical expertise in all areas of network and computer hardware and software interconnection and interfacing, such as routers, switchers, firewalls, hubs, bridges, gateways, etc.
What you must have
Must have 0 years experience with Bachelors in Computer Science, Data Scientist, Statistics or related field; High School Diploma or equivalent and 4 years relevant progressive experience.
Must have Knowledge of cloud architectures.
Must have a CompTIA Security + Certificate.
Must be proficient in creating reports and presentations with Microsoft Office 365 products
Must have excellent verbal and written communication skills to present technical information to clients, stakeholders, and team members in a clear and concise manner
Must be a self-starter able to work independently or as part of a larger professional team
Must hold a current or active DoD Secret Security Clearance.
Preferred
Experience in DoD programs
Experience with Air Force programs
Certified Analytics Professional (CAP)
Cloudera Data Platform Generalist Certification
IBM Data Science Professional Certificate
Microsoft Certified: Azure AI Fundamentals
Microsoft Certified: Azure Data Scientist Associate
Open Certified Data Scientist (Open CDS)
SAS Certified AI and Machine Learning Professional
SAS Certified Data Scientist
Tensorflow Developer Certificate
Physical Requirements
May require working in an office, industrial, shipboard, or laboratory environment. Capable of climbing ladders and tolerating confined spaces and extreme temperature variances.
Why HII
We build the world's most powerful, survivable naval ships and defense technology solutions that safeguard our seas, sky, land, space and cyber. Our diverse workforce includes skilled tradespeople; artificial intelligence, machine learning (AI/ML) experts; engineers; technologists; scientists; logistics experts; and business administration professionals.
Recognized as one of America's top large company employers, we are a values and ethics driven organization that puts people's safety and well-being first. Regardless of your role or where you serve, at HII, you'll find a supportive and welcoming environment, competitive benefits, and valuable educational and training programs for continual career growth at every stage of your career.
Together we are working to ensure a future where everyone can be free and thrive.
Today's challenges are bigger than ever, and the nation needs the best of us. It's why we're focused on hiring, developing and nurturing our diversity. We believe that diversity among our workforce strengthens the organization, stimulates creativity, promotes the exchange of ideas and enriches the work lives of all our employees.
All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.
Do You Need Assistance?
If you need a reasonable accommodation for any part of the employment process, please send an e-mail to and let us know the nature of your request and your contact information. Reasonable accommodations are considered on a case-by-case basis. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this email address. Additionally, you may also call
1-
for assistance. Press #3 for HII Technical Solutions. DMATS JR. Network Data Engineer
Show more
Show less","Computer Science, Data Scientist, Statistics, CompTIA Security +, Microsoft Office 365, DoD Secret Security Clearance, Cloud Architectures, Certified Analytics Professional (CAP), Cloudera Data Platform Generalist Certification, IBM Data Science Professional Certificate, Microsoft Certified: Azure AI Fundamentals, Microsoft Certified: Azure Data Scientist Associate, Open Certified Data Scientist (Open CDS), SAS Certified AI and Machine Learning Professional, SAS Certified Data Scientist, Tensorflow Developer Certificate, Network Engineering, Cyber Operations, Data Analytics, Big Data, Synthetic Training Environments, Hardware and Software Interfacing, Routers, Switchers, Firewalls, Hubs, Bridges, Gateways","computer science, data scientist, statistics, comptia security, microsoft office 365, dod secret security clearance, cloud architectures, certified analytics professional cap, cloudera data platform generalist certification, ibm data science professional certificate, microsoft certified azure ai fundamentals, microsoft certified azure data scientist associate, open certified data scientist open cds, sas certified ai and machine learning professional, sas certified data scientist, tensorflow developer certificate, network engineering, cyber operations, data analytics, big data, synthetic training environments, hardware and software interfacing, routers, switchers, firewalls, hubs, bridges, gateways","big data, bridges, certified analytics professional cap, cloud architectures, cloudera data platform generalist certification, comptia security, computer science, cyber operations, data scientist, dataanalytics, dod secret security clearance, firewalls, gateways, hardware and software interfacing, hubs, ibm data science professional certificate, microsoft certified azure ai fundamentals, microsoft certified azure data scientist associate, microsoft office 365, network engineering, open certified data scientist open cds, routers, sas certified ai and machine learning professional, sas certified data scientist, statistics, switchers, synthetic training environments, tensorflow developer certificate"
Power BI Data Analyst,"Canvas, Inc.","Eglin Air Force Base, FL",https://www.linkedin.com/jobs/view/power-bi-data-analyst-at-canvas-inc-3781136399,2023-12-17,Fort Walton Beach,United States,Mid senior,Onsite,"Job Title: Power BI Data Analyst
Location: Eglin AFB, FL (On-Site Work Required)
Canvas is seeking a skilled technical advisory who can serve as a lead system integrator and provide technical management to the tech stack of interest. This individual will use their technical expertise to ensure the tech stack has full spectrum Microsoft DAF365 integration of SharePoint, Teams, PowerBi, Project, and other cloud-based Microsoft applications such as PowerApps and Power Automate. This individual will provide adept data analysis, design, development, implementation, support leveraging SharePoint, Microsoft PowerBi, PowerApps, Power Automate, and MS365 Office capabilities, and complete other duties as assigned. Individual must have experience importing\/manipulating and modeling complex data sets into intuitive user outputs to enhance administrative capabilities within the tech stack.
Education\/Experience:
BS in Computer Science or similar IT field with 3 years of relevant experience required (may be substituted with government approval); MS in Computer Science or similar IT field preferred.
5+ years PowerBi Development & 10+ years experience with data analytics & management required.
8+ years PowerBi Development & 10+ years experience with data analytics & management is preferred.
Ability to communicate effectively with end-users and create meaningful solutions to enduring issues leveraging MS365 cloud-based platforms.
Must be experienced at building table relationships and writing data analysis expressions (DAX) within PowerBi to build intuitive visualizations and reports for end-users.
Microsoft Certified: Power Platform Solution Architect Expert desired.
Previous experience with API authentication key bypasses to leverage real-time reporting through PowerBi desired.
Strong grasp of Air Force admin functions to enhance productivity while reducing organizational waste desired.
Strong experience in utilization of MS365 suite of software tools - predominantly SharePoint, Power Automate, Power Bi and PowerApps. The ability to integrate all three is highly desired for enhanced user experiences is desired. Required Security Level: Must be able to obtain and maintain secret clearance.
About Us
Founded in 2007, Canvas, Inc. connects a passion for going beyond the expected with the knowledge and expertise to deliver what our clients need now and in the future. Canvas has been recognized as a Great Place to Work Certified \u2122, Fortune Best Small Workplace \u2122, Fortune Best Workplace for Millennials \u2122 (2022), Best Place for Working Parents (2022 & 2023), HIRE Vets Gold Medallion Awardee (2021 & 2022), Best Places to Work Awardee, 2019 Government Contracting - Technology Business of the Year, and Woman-Owned Small Business of the Year (2018) by the Huntsville\/Madison Chamber of Commerce.
Benefits of Working with Canvas, Inc.
Benefits
To reflect our company culture, Canvas offers an exciting array of benefits that makes up our employees' total rewards package. Those benefits may include:
Competitive Wages*
Medical, Rx, Dental & Vision Insurance
Generous company-funded Basic Life Insurance
Company-funded Short-Term & Long-Term Disability
11 Paid Federal Holidays
Generous Paid Time Off (PTO)
Dependent Care and Medical Flexible Spending Accounts
401(k) retirement plan with company match and 100% immediate vesting
Tuition Reimbursement for ongoing training, continuing education, or advanced degree programs
Robust Employee Assistance Program
Employee Referral Bonus Program
Corporate Sponsored Events & Community Outreach
Spot Awards for exemplary individual performance
Discretionary performance-based bonuses
And many more!
Final compensation for this position is determined by a variety of factors, such as a candidate's relevant work experience, skills, certifications, and geographic location.
Canvas is an Equal Opportunity Employer
Canvas, Inc has equal employment opportunities that are based upon a candidate's qualifications and capabilities to perform the essential functions of a particular job and are free from discrimination based on race, color, religion, national origin, sex, sexual orientation, gender identity, age, disability, protected veteran status, genetic information, or any other characteristic protected by law. For our complete EEO\/AA and Pay Transparency statement, please visit https:\/\/www.canvas-inc.com\/careers. U.S. citizenship is required for most positions.
Canvas, Inc. is committed to the full inclusion of all qualified individuals. Canvas, Inc. will take the steps to assure that people with disabilities are provided reasonable accommodations. Accordingly, if reasonable accommodation is required to fully participate in the job application or interview process, to perform the essential functions of the position, and\/or to receive all other benefits and privileges of employment, please contact the Human Resources department at (256) 489-2988 or talent@canvas-inc.com.
For further information on Canvas Inc, including more information on employee benefits and our company culture, visit our website at www.canvas-inc.com.
If an offer of employment is extended, applicant must have the ability to pass a background check. Offer of Employment is contingent upon the results.
Show more
Show less","Power BI, SharePoint, Microsoft 365, PowerApps, Power Automate, MS365 Office, Data Analysis, Data Design, Data Development, Data Implementation, Data Support, Complex Datasets, Intuitive User Outputs, Microsoft Certified: Power Platform Solution Architect Expert, API Authentication Key Bypasses, Air Force Admin Functions, MS365 Suite","power bi, sharepoint, microsoft 365, powerapps, power automate, ms365 office, data analysis, data design, data development, data implementation, data support, complex datasets, intuitive user outputs, microsoft certified power platform solution architect expert, api authentication key bypasses, air force admin functions, ms365 suite","air force admin functions, api authentication key bypasses, complex datasets, data design, data development, data implementation, data support, dataanalytics, intuitive user outputs, microsoft 365, microsoft certified power platform solution architect expert, ms365 office, ms365 suite, power automate, powerapps, powerbi, sharepoint"
Mission Data Engineer - Senior - SWW,Astrion,"Shalimar, FL",https://www.linkedin.com/jobs/view/mission-data-engineer-senior-sww-at-astrion-3787326215,2023-12-17,Fort Walton Beach,United States,Mid senior,Onsite,"Mission Data Engineer – Senior
Be the Difference
Astrion offers comprehensive services that boost preparedness, optimize performance, and ensure success across various domains, from Cyber to Digital, Mission and Systems, servicing our nation's Civilian, Defense and Space communities. We support customers with Centers of Excellence in Washington DC, Huntsville, AL and Burlington, MA with an additional 36 locations across the U.S.
Astrion has an exciting opportunity for
Mission Data Engineer
in support of the 36 th Electronic Warfare Squadron (EWS) 350th Spectrum Warfare Wing (SWW), located at
Eglin AFB, FL
, under the Astrion
Air Force Division.
The 350 SWW is the technical focal point for all electronic warfare (EW) support of warfighter systems for the Combat Air Forces (CAF) fighter, bomber, airborne surveillance, and helicopter platforms.
LOCATION:
Eglin AFB, FL
JOB STATUS:
Full Time
TRAVEL:
10% CONUS/OCONUS TDYs
Required Qualifications / Skills
Master’s degree, or additional years of qualifying experience
A minimum of 10 years of work experience, or additional years’ experience for degree substitution
An active Secret security clearance (U.S. Citizenship Required)
Demonstrated ability to recognize and analyze problems, conduct research, summarize results, and make appropriate recommendations
Must have a working knowledge of computer systems and an understanding of Windows-based personal computers and Microsoft Office software and possess the ability to communicate effectively both orally and written
Desired Qualifications / Skills
Knowledge and skills with Electronic Warfare (EW) and the electromagnetic (EM) spectrum, computer software, threat warning, radio frequency (RF) jammers, electro-optical/infrared (EO/IR) jammers, expendables, threat analysis, foreign/US/radar/weapon systems, airborne EW computer software, avionics, systems integration, electronics engineering concepts, principles and practices applicable to a broad range of engineering assignments. Candidates must be analytical, methodical and detail oriented
Highly desire previous experience in RF integration, EM spectrum management, antenna design, and digital signal processing
Desire knowledge of EW weapons systems development, test and evaluation, and systems engineering
Responsibilities
Assist the 350th Spectrum Warfare Wing (350 SWW) at Eglin AFB, Florida, which is the technical focal point for all Electronic Warfare (EW) support of warfighter systems for the Combat Air Forces (CAF). The mission of the 350 SWW is to develop and test Mission Data (MD) to defeat enemy radar and infrared guided missile systems, thus enhancing aircrew and aircraft survivability in combat. This mission includes operational EW testing, MD development/validation/verification, force development evaluation execution and facilitating foreign materiel exploitation
Conduct appropriate EW research, MD development and MD testing. The successful candidate will support EW system programming/reprogramming, work with EW system engineers to coordinate programming/reprogramming requirements, prepare validation and verification test plans, and organize and participate in MD configuration control boards. The successful candidate will assist with the collection, recording, and post-test analysis of data generated during MD testing. The successful candidate may be required to travel
What We Offer
Competitive salaries
Continuing education assistance
Professional development allotment
Multiple healthcare benefits packages
401K with employer matching
Paid time off (PTO) along with a federally recognized holiday schedule
Who We Are
At Astrion, we innovate, elevate, and shape the world of tomorrow. At our core is our purpose to “Be the Difference”. This means we encourage our employees to take action and be the driving force for positive change. We foster an environment where innovative solutions flourish and our company continuously evolves.
We have a culture of care, empathy, and making a tangible difference within our organization and communities. We embrace continuous learning, growth, and innovation, and pushing the boundaries of what’s possible. We promote collaboration and empowering our teams is at the core of our success.
Join Astrion and Be the Difference in your career and the world!
Astrion is an Equal Employment Opportunity/Affirmative Action Employer. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
#CJ
“Air Force Cyber Division”
Show more
Show less","Electronics Engineering, RF Integration, Threat Analysis, Avionics, Microsoft Office, ElectroOptical/Infrared (EO/IR), Defense Systems, Radar Systems, Weapon Systems, Test and Evaluation, Mission Data (MD), EW System Programming, Validation and Verification, Data Analysis","electronics engineering, rf integration, threat analysis, avionics, microsoft office, electroopticalinfrared eoir, defense systems, radar systems, weapon systems, test and evaluation, mission data md, ew system programming, validation and verification, data analysis","avionics, dataanalytics, defense systems, electronics engineering, electroopticalinfrared eoir, ew system programming, microsoft office, mission data md, radar systems, rf integration, test and evaluation, threat analysis, validation and verification, weapon systems"
Data Link Test Engineer - Senior - TGBC,Astrion,"Shalimar, FL",https://www.linkedin.com/jobs/view/data-link-test-engineer-senior-tgbc-at-astrion-3787321685,2023-12-17,Fort Walton Beach,United States,Mid senior,Onsite,"Tactical Datalink Test Engineer - Senior - TGBC
Be the Difference
Astrion offers comprehensive services that boost preparedness, optimize performance, and ensure success across various domains, from Cyber to Digital, Mission and Systems, servicing our nation's Civilian, Defense and Space communities. We support customers with Centers of Excellence in Washington DC, Huntsville, AL and Burlington, MA with an additional 36 locations across the U.S.
Astrion has an exciting opportunity for a
Senior Tactical Datalink Test Engineer
for the
96 Cyberspace Test Group (96 CTG)
contract, supporting the
Astrion
Air Force Division.
We are looking for someone with computer systems design, development, and maintenance knowledge to join the United States Air Force’s premier TDL Developmental Test (DT) Team. If you enjoy working in a fast-paced environment and being on the cutting edge of innovative communications technology, this is the place for you. We provide several opportunities to learn and grow, ranging from on-the-job training with other team members to formal courses for unique technology areas.
JOB DETAILS
LOCATION:
Eglin AFB, FL
JOB STATUS:
Full Time
TRAVEL:
25% CONUS / OCONUS
Required Qualifications / Skills
Master’s Degree in a technical field and a minimum of 10 years of relevant technical experience is required. Twelve years of relevant technical experience may be substituted for a Master’s Degree (Total of 22 years) or eight years of relevant technical experience and a technical BS Degree (Total of 18 Years)
An active Secret security clearance is required with eligibility to qualify for a Top Secret clearance for consideration (U.S. Citizenship Required)
Working knowledge of DOD tactical data links
Ability to communicate and present technical details both written and verballyAble to travel to contractors' facilities, test sites, and other locations, CONUS and OCONUS up to 25% of the time
Strong work ethic; Comfortable working in a team environment with minimal supervision
Desired Qualifications / Skills
Understanding of basic computer networking
Technical experience with Link 16, SADL, and VMF
Technical experience in the areas related to data link radios
Experience and knowledge of tactical datalinks mission threads
Proficiency with the Microsoft and Linux Operating Systems
Understanding of DT&E processes, specifically acquisition program development and detailed knowledge of Air Force Command and Control (C2)
Knowledge of systems analysis and application software development concepts, functions, and techniques
Able to evaluate tactical communications emerging technologies
Security+ certification
Responsibilities
Perform as a member of the 46 TS Gateways Element Test team
Interface directly with System Program Offices, other services, Joint Battle Labs as well as product development contractors.
Plan, execute, and report on test events for datalink gateway systems hosting the following data links: Joint Range Extension, Link 16, Situation Awareness Data Link (SADL), Variable Message Format (VMF) and emerging data links
Operate datalink systems and equipment
Proficient/familiar with MIL-STD-6016 J-message formats
As a technical leader in an engineering role, review customer requirements and participate in test planning meetings
Develop test plans and procedures, schedules, and execute a test within the constraints of customer requirements and produce timely test reports
Capable of performing as a team leader in directing and executing system testing
Author technical documents and briefings and conduct formal presentations
Travel to locations, as required, to conduct tests, collect and reduce data
What We Offer
Competitive salaries
Continuing education assistance
Professional development allotment
Multiple healthcare benefits packages
401K with employer matching
Paid time off (PTO) along with a federally recognized holiday schedule
Who We Are
At Astrion, we innovate, elevate, and shape the world of tomorrow. At our core is our purpose to “Be the Difference”. This means we encourage our employees to take action and be the driving force for positive change. We foster an environment where innovative solutions flourish and our company continuously evolves.
We have a culture of care, empathy, and making a tangible difference within our organization and communities. We embrace continuous learning, growth, and innovation, and pushing the boundaries of what’s possible. We promote collaboration and empowering our teams is at the core of our success.
Join Astrion and Be the Difference in your career and the world!
Astrion is an Equal Employment Opportunity/Affirmative Action Employer. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
""Air Force Cyber Division""
Show more
Show less","Master Degree, Technical Field, Relevant Technical Experience, Secret Security Clearance, Top Secret Clearance, DOD Tactical Data Links, Link 16, SADL, VMF, Data Link Radios, Tactical Datalinks Mission Threads, Microsoft Operating Systems, Linux Operating Systems, DT&E Processes, Acquisition Program Development, Air Force Command and Control (C2), Systems Analysis, Application Software Development, Tactical Communications Emerging Technologies, Security+ Certification, MILSTD6016 Jmessage Formats, Test Plans and Procedures, Schedules, Test Execution, Team Leader, Technical Documents, Briefings, Formal Presentations, Travel, Data Collection, Data Reduction","master degree, technical field, relevant technical experience, secret security clearance, top secret clearance, dod tactical data links, link 16, sadl, vmf, data link radios, tactical datalinks mission threads, microsoft operating systems, linux operating systems, dte processes, acquisition program development, air force command and control c2, systems analysis, application software development, tactical communications emerging technologies, security certification, milstd6016 jmessage formats, test plans and procedures, schedules, test execution, team leader, technical documents, briefings, formal presentations, travel, data collection, data reduction","acquisition program development, air force command and control c2, application software development, briefings, data collection, data link radios, data reduction, dod tactical data links, dte processes, formal presentations, link 16, linux operating systems, master degree, microsoft operating systems, milstd6016 jmessage formats, relevant technical experience, sadl, schedules, secret security clearance, security certification, systems analysis, tactical communications emerging technologies, tactical datalinks mission threads, team leader, technical documents, technical field, test execution, test plans and procedures, top secret clearance, travel, vmf"
DMATS JR. Network Data Engineer - 16622 with Security Clearance,ClearanceJobs,"Hurlburt Field, FL",https://www.linkedin.com/jobs/view/dmats-jr-network-data-engineer-16622-with-security-clearance-at-clearancejobs-3761065854,2023-12-17,Fort Walton Beach,United States,Mid senior,Onsite,"Requisition Number: 16622 Required Travel: 0 - 10% Employment Type: Full Time/Salaried/Exempt Security Clearance: Secret Level of Experience: Entry Level This opportunity resides with Live, Virtual, Constructive Solutions, a business group within HII's Mission Technologies division. As a trusted partner to our military customers, we design, develop and operate systems that bring together service members from across the globe to help you train like you fight, because we understand that preparation requires full coordination-not readiness in piece parts. Meet HII's Mission Technologies Division Our team of more than 7,000 professionals worldwide delivers all-domain expertise and advanced technologies in service of mission partners across the globe. Mission Technologies is leading the next evolution of national defense - the data evolution - by accelerating a breadth of national security solutions for government and commercial customers. Our capabilities range from C5ISR, AI and Big Data, cyber operations and synthetic training environments to fleet sustainment, environmental remediation and the largest family of unmanned underwater vehicles in every class. Find the role that's right for you. Apply today. We look forward to meeting you. Summary HII Mission Technologies is seeking a Junior Network Data Engineer for the Decisive Mission Actions and Technology Services (DMATS) with the US Air Force Special Operations Command (AFSOC) at Hurlburt Field, Florida. The selected candidate must The selected candidate will support the design, implementation and maintenance of DoD computer networks, enabling communication between multiple computer based systems. What you will do * Assist in the Identification and resolution of network issues, perform routine maintenance and protect data from cyber-attacks.
Supports the design and maintenance of the hardware and software of a network.
Works with the senior Network Engineer to Conduct testing of network systems. * Performs routine network maintenance, including basic troubleshooting and installation of upgrades and service packs.
Consults with network engineering team to suggest network solutions
Test and install new computer systems, hardware, software, and applications as per DoD and customer guidance.
Explore ways to improve network performance or reduce network costs
Maintain analytical systems, verifies the accuracy of the data, and act as liaison.
Manage all aspects of end-to-end data processing utilizing customized report building functions of systems. * Maintain technical expertise in all areas of network and computer hardware and software interconnection and interfacing, such as routers, switchers, firewalls, hubs, bridges, gateways, etc.
What you must have * Must have 0 years experience with Bachelors in Computer Science, Data Scientist, Statistics or related field; High School Diploma or equivalent and 4 years relevant progressive experience.
Must have Knowledge of cloud architectures.
Must have a CompTIA Security + Certificate. * Must be proficient in creating reports and presentations with Microsoft Office 365 products * Must have excellent verbal and written communication skills to present technical information to clients, stakeholders, and team members in a clear and concise manner * Must be a self-starter able to work independently or as part of a larger professional team
Must hold a current or active DoD Secret Security Clearance. * Must be a U.S. Citizen. Preferred * Experience in DoD programs * Experience with Air Force programs * Certified Analytics Professional (CAP)
Cloudera Data Platform Generalist Certification
IBM Data Science Professional Certificate
Microsoft Certified: Azure AI Fundamentals
Microsoft Certified: Azure Data Scientist Associate
Open Certified Data Scientist (Open CDS)
SAS Certified AI and Machine Learning Professional
SAS Certified Data Scientist
Tensorflow Developer Certificate
Physical Requirements May require working in an office, industrial, shipboard, or laboratory environment. Capable of climbing ladders and tolerating confined spaces and extreme temperature variances. Why HII We build the world's most powerful, survivable naval ships and defense technology solutions that safeguard our seas, sky, land, space and cyber. Our diverse workforce includes skilled tradespeople; artificial intelligence, machine learning (AI/ML) experts; engineers; technologists; scientists; logistics experts; and business administration professionals. Recognized as one of America's top large company employers, we are a values and ethics driven organization that puts people's safety and well-being first. Regardless of your role or where you serve, at HII, you'll find a supportive and welcoming environment, competitive benefits, and valuable educational and training programs for continual career growth at every stage of your career. Together we are working to ensure a future where everyone can be free and thrive. Today's challenges are bigger than ever, and the nation needs the best of us. It's why we're focused on hiring, developing and nurturing our diversity. We believe that diversity among our workforce strengthens the organization, stimulates creativity, promotes the exchange of ideas and enriches the work lives of all our employees. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law. Do You Need Assistance? If you need a reasonable accommodation for any part of the employment process, please send an e-mail to and let us know the nature of your request and your contact information. Reasonable accommodations are considered on a case-by-case basis. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this email address. Additionally, you may also call 1-844-849-8463 for assistance. Press #3 for HII Technical Solutions.
Show more
Show less","Computer Networks, Cloud Architectures, DoD Computer Networks, Microsoft Office 365, DoD Programs, Air Force Programs, Analytics, Data Science, Machine Learning, Artificial Intelligence, DoD Secret Security Clearance, CompTIA Security + Certificate, Tensorflow, Microsoft Certified: Azure AI Fundamentals, Open Certified Data Scientist (Open CDS), IBM Data Science Professional Certificate, Microsoft Certified: Azure Data Scientist Associate, Cloudera Data Platform Generalist Certification, SAS Certified AI and Machine Learning Professional, SAS Certified Data Scientist","computer networks, cloud architectures, dod computer networks, microsoft office 365, dod programs, air force programs, analytics, data science, machine learning, artificial intelligence, dod secret security clearance, comptia security certificate, tensorflow, microsoft certified azure ai fundamentals, open certified data scientist open cds, ibm data science professional certificate, microsoft certified azure data scientist associate, cloudera data platform generalist certification, sas certified ai and machine learning professional, sas certified data scientist","air force programs, analytics, artificial intelligence, cloud architectures, cloudera data platform generalist certification, comptia security certificate, computer networks, data science, dod computer networks, dod programs, dod secret security clearance, ibm data science professional certificate, machine learning, microsoft certified azure ai fundamentals, microsoft certified azure data scientist associate, microsoft office 365, open certified data scientist open cds, sas certified ai and machine learning professional, sas certified data scientist, tensorflow"
Mission Data Engineer - Senior - SWW with Security Clearance,ClearanceJobs,"Shalimar, FL",https://www.linkedin.com/jobs/view/mission-data-engineer-senior-sww-with-security-clearance-at-clearancejobs-3781964890,2023-12-17,Fort Walton Beach,United States,Mid senior,Onsite,"Overview Mission Data Engineer - Senior Be the Difference Astrion offers comprehensive services that boost preparedness, optimize performance, and ensure success across various domains, from Cyber to Digital, Mission and Systems, servicing our nation's Civilian, Defense and Space communities. We support customers with Centers of Excellence in Washington DC, Huntsville, AL and Burlington, MA with an additional 36 locations across the U.S. Astrion has an exciting opportunity for Mission Data Engineer in support of the 36 th Electronic Warfare Squadron (EWS) 350th Spectrum Warfare Wing (SWW), located at Eglin AFB, FL , under the Astrion Air Force Division. The 350 SWW is the technical focal point for all electronic warfare (EW) support of warfighter systems for the Combat Air Forces (CAF) fighter, bomber, airborne surveillance, and helicopter platforms. LOCATION: Eglin AFB, FL JOB STATUS: Full Time TRAVEL: 10% CONUS/OCONUS TDYs REQUIRED QUALIFICATIONS / SKILLS
Master's degree, or additional years of qualifying experience
A minimum of 10 years of work experience, or additional years' experience for degree substitution
An active Secret security clearance (U.S. Citizenship Required)
Demonstrated ability to recognize and analyze problems, conduct research, summarize results, and make appropriate recommendations
Must have a working knowledge of computer systems and an understanding of Windows-based personal computers and Microsoft Office software and possess the ability to communicate effectively both orally and written DESIRED QUALIFICATIONS / SKILLS
Knowledge and skills with Electronic Warfare (EW) and the electromagnetic (EM) spectrum, computer software, threat warning, radio frequency (RF) jammers, electro-optical/infrared (EO/IR) jammers, expendables, threat analysis, foreign/US/radar/weapon systems, airborne EW computer software, avionics, systems integration, electronics engineering concepts, principles and practices applicable to a broad range of engineering assignments. Candidates must be analytical, methodical and detail oriented
Highly desire previous experience in RF integration, EM spectrum management, antenna design, and digital signal processing
Desire knowledge of EW weapons systems development, test and evaluation, and systems engineering RESPONSIBILITIES
Assist the 350th Spectrum Warfare Wing (350 SWW) at Eglin AFB, Florida, which is the technical focal point for all Electronic Warfare (EW) support of warfighter systems for the Combat Air Forces (CAF). The mission of the 350 SWW is to develop and test Mission Data (MD) to defeat enemy radar and infrared guided missile systems, thus enhancing aircrew and aircraft survivability in combat. This mission includes operational EW testing, MD development/validation/verification, force development evaluation execution and facilitating foreign materiel exploitation
Conduct appropriate EW research, MD development and MD testing. The successful candidate will support EW system programming/reprogramming, work with EW system engineers to coordinate programming/reprogramming requirements, prepare validation and verification test plans, and organize and participate in MD configuration control boards. The successful candidate will assist with the collection, recording, and post-test analysis of data generated during MD testing. The successful candidate may be required to travel What We Offer
Competitive salaries
Continuing education assistance
Professional development allotment
Multiple healthcare benefits packages
401K with employer matching
Paid time off (PTO) along with a federally recognized holiday schedule Who We Are At Astrion, we innovate, elevate, and shape the world of tomorrow. At our core is our purpose to ""Be the Difference"". This means we encourage our employees to take action and be the driving force for positive change. We foster an environment where innovative solutions flourish and our company continuously evolves. We have a culture of care, empathy, and making a tangible difference within our organization and communities. We embrace continuous learning, growth, and innovation, and pushing the boundaries of what's possible. We promote collaboration and empowering our teams is at the core of our success. Join Astrion and Be the Difference in your career and the world! Astrion is an Equal Employment Opportunity/Affirmative Action Employer. We provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. #CJ ""Air Force Cyber Division""
Show more
Show less","Electronic Warfare (EW), electromagnetic spectrum, computer software, threat warning, radio frequency (RF) jammers, electrooptical/infrared (EO/IR) jammers, expendables, threat analysis, foreign/US/radar/weapon systems, airborne EW computer software, avionics, systems integration, electronics engineering, RF integration, EM spectrum management, antenna design, digital signal processing, EW weapons systems development, test and evaluation, systems engineering, Mission Data (MD), Combat Air Forces (CAF)","electronic warfare ew, electromagnetic spectrum, computer software, threat warning, radio frequency rf jammers, electroopticalinfrared eoir jammers, expendables, threat analysis, foreignusradarweapon systems, airborne ew computer software, avionics, systems integration, electronics engineering, rf integration, em spectrum management, antenna design, digital signal processing, ew weapons systems development, test and evaluation, systems engineering, mission data md, combat air forces caf","airborne ew computer software, antenna design, avionics, combat air forces caf, computer software, digital signal processing, electromagnetic spectrum, electronic warfare ew, electronics engineering, electroopticalinfrared eoir jammers, em spectrum management, ew weapons systems development, expendables, foreignusradarweapon systems, mission data md, radio frequency rf jammers, rf integration, systems engineering, systems integration, test and evaluation, threat analysis, threat warning"
513th EWS USRL Fleet Support Mission Data Analyst,DCS Corp,"Niceville, FL",https://www.linkedin.com/jobs/view/513th-ews-usrl-fleet-support-mission-data-analyst-at-dcs-corp-3731277703,2023-12-17,Fort Walton Beach,United States,Mid senior,Onsite,"DCS Corp has an immediate opening for a Fleet Support Mission Data (MD) Analyst supporting the USRL and 513th Electronic Warfare Squadron on Eglin AFB.
Essential Job Functions
Support Fleet Operational Mission Data requirements to ensure the 513th EWS maintains perspective of current operational priorities.
Provide outreach and assistance to U.S. Navy, Marine Corps, & Air Force F-35 units and EW community.
Provide feedback to F-35 fleet and test community and satisfy Requests for Information (RFI).
Support MD Academics, Roadshows, F-35 Capability Briefings, and provide MD expertise at tactics conferences.
Manage the USRL online websites and databases, monitoring readiness and ensuring/enabling capability for rapid reprogramming.
Serve as the USRL Technical Lead for MD Fielding.
Ensure all MD release approvals are received, and documentation is complete and accurate prior to fielding.
Verify all MD products are loaded to both MSDDS and NDDS and can be accessed by operational squadrons.
Assist in development, documentation, and improvement of MD Fielding process.
The Fleet Support MD Analyst will work with other members of the MD team to develop state-of-the-art mission data for advanced fifth generation fighter aircraft.
Evaluate data from specified intelligence sources and program parametric data into SPECTRE database.
Maintain accurate MD according to new intelligence, database changes, test results and JSF updates.
Provide data quality feedback to intelligence communities and SPECTRE database managers.
Assist in development, documentation, and improvement of Mission Data File (MDF) and load generation processes.
Required Skills
Due to the sensitivity of customer related requirements, U.S. Citizenship is required.
A Master's degree with five years of relevant experience.
Must currently possess or have eligibility for a TS/SCI security clearance.
Ability to speak to aircrew in an instructional format covering MD programming updates/highlights.
Ability to travel domestically and internationally, approximately 0-25%.
Desired Skills
U.S. Military Flight Experience as a Pilot, Electronic Warfare Officer (EWO), or Combat Systems Officer (CSO).
Electronic Warfare experience and/or working knowledge of EW principles.
Experience programming Mission Data for 4th/5th generation fighter aircraft.
Experience facilitating changes to MD supporting operational and engineering requirements.
Experience programming MD and providing training/instruction on MD updates.
Familiar with MD Verification, Validation, and testing in a lab.
Experience with MATLAB.
Experience with Intel sources such as CED, EWIRDB, and NGES.
Show more
Show less","Fleet Support Mission Data (MD) Analyst, Mission Data Academics, Roadshows, F35 Capability Briefings, USRL online websites, MSDDS, NDDS, SPECTRE database, Mission Data File (MDF), TS/SCI security clearance, Electronic Warfare (EW), MATLAB, CED, EWIRDB, NGES","fleet support mission data md analyst, mission data academics, roadshows, f35 capability briefings, usrl online websites, msdds, ndds, spectre database, mission data file mdf, tssci security clearance, electronic warfare ew, matlab, ced, ewirdb, nges","ced, electronic warfare ew, ewirdb, f35 capability briefings, fleet support mission data md analyst, matlab, mission data academics, mission data file mdf, msdds, ndds, nges, roadshows, spectre database, tssci security clearance, usrl online websites"
Sr. Cloud Data Infrastructure Engineer,Intellectt Inc,"Abbott, TX",https://www.linkedin.com/jobs/view/sr-cloud-data-infrastructure-engineer-at-intellectt-inc-3707549053,2023-12-17,Waco,United States,Mid senior,Onsite,"This is Tanmai from Intellect Inc. Please find the job description and reach out to me with an updated copy of your resume. You can send it to tanmai@intellectt.com or call me at +1(907) 802-6640
Role
: Sr. Cloud Data Infrastructure Engineer
Location
: Abbott Park, IL - 60064
Duration
: 12 Months on W2
Shift Timings:
8 AM to 5 PM
100% Onsite role.
No option for hybrid or remote.
Job Summary
Implements cloud data infrastructure technology stack deployments and evolution for Abbott Enterprise cloudservices.
Follows a DevSecOps operating model to continuously integrate, deploy and scale our cloud data
infrastructure services. Accountable for delivering a comprehensive set of cloud data infrastructure services including requirements definition, design, integration with infrastructure as code approach and handling operations.
Participates in proof-of-concept analysis and vendor solution evaluations. Effectively communicates
and partners with IS cross-functional teams, IT divisional customers and stakeholders in support of cloud
initiatives.
Maintain Cloud data Infrastructure architecture best practices with robust design & automation.
Job Responsibilities
Serve as a Tech lead for Cloud Data infrastructure services/platforms managed by Abbott BTS cloud services
DevSecOps, Agile Delivery and SLC methodologies adjusted for cloud data infrastructure service delivery
Leverage Site Reliability Engineering (SRE) principles to build highly reliable cloud data infrastructure,
manage and operate at scale, solve technical problems, and automate operational tasks
Use Terraform modular approach to deploy all data infrastructure as code (IaC) based XaaS solutions
Establishes build-and-destroy models for cloud data infrastructure technologies/platforms with blue/green, canary deployment patterns.
Implement CI/CD pipelines for the deployment of cloud data infrastructure stacks for applications.
Drive cloud data infrastructure service performance health, troubleshooting, operational metrics &capacity management
Manage cloud data protection and restoration including data backups, Site recovery & Replication
Evolve Infrastructure as Code frameworks for data infra / platforms scaling adjustments and optimizations
Optimize cloud data infra workloads for security, cost, performance, latency, reliability, and availability aspects
Automate cloud data infra Day2Ops tasks leveraging fully managed data platforms and orchestration tools
Support IT audits and compliance, adhering to applicable Corporate and Divisional Policies and procedures.
Implement effective standards, methodologies, and processes, driving change proactively asappropriate for continuous improvement.
Oversee overall cloud infrastructure estate operational governance & framework for promotingoperational efficiencies
Must Have Skills
Experience with AWS RDS, Azure SQL, PostgreSQL, MongoDB Atlas, Cosmos DB database solutions
Experience with Managed Kafka Data Streaming, ETL (Informatica, Azure Data Factory) platforms
Experience with Datawarehouse (RedShift), Analytics (Synapse/Databricks/Snowflake) platforms
Experience with Cloud Storage platforms (Managed Disks Azure Blob /Files, AWS S3, EBS, EFS)
Experience with Azure/AWS native Data Protection & Backup/Recovery, Site recovery solutions
Experience with Terraform (IaC), Ansible (Configuration management), Packer (Gold images)
Experience with Git (GitHub) and CI/CD Pipelines (Jenkins, CI/CD Argo, or Azure DevOps)
Experience with any of the Observability/Monitoring tools like Data Dog, New Relic, Dynatrace, LogicMonitor
Experience with ServiceNow (ITSM) Ticketing Support and Process integrations
Education And Experience
A bachelor's Degree is preferred in technology or management of information systems discipline.
Overall 10 years of IT experience including cloud experience for at least 4 years potentially in a lead SME type of role.
Show more
Show less","AWS, Azure, RDS, SQL, PostgreSQL, MongoDB Atlas, Cosmos DB, Kafka, ETL, Informatica, Azure Data Factory, Redshift, Analytics, Synapse, Databricks, Snowflake, Cloud Storage, Managed Disks, Azure Blob, Files, AWS S3, EBS, EFS, Git, GitHub, CI/CD, Jenkins, Argo, Azure DevOps, ServiceNow, Terraform, Ansible, Packer, Data Dog, New Relic, Dynatrace, LogicMonitor","aws, azure, rds, sql, postgresql, mongodb atlas, cosmos db, kafka, etl, informatica, azure data factory, redshift, analytics, synapse, databricks, snowflake, cloud storage, managed disks, azure blob, files, aws s3, ebs, efs, git, github, cicd, jenkins, argo, azure devops, servicenow, terraform, ansible, packer, data dog, new relic, dynatrace, logicmonitor","analytics, ansible, argo, aws, aws s3, azure, azure blob, azure data factory, azure devops, cicd, cloud storage, cosmos db, data dog, databricks, dynatrace, ebs, efs, etl, files, git, github, informatica, jenkins, kafka, logicmonitor, managed disks, mongodb atlas, new relic, packer, postgresql, rds, redshift, servicenow, snowflake, sql, synapse, terraform"
Principal/Sr. Engineer Systems Test (Data Acquisition Engineer),Northrop Grumman,"Palmdale, CA",https://www.linkedin.com/jobs/view/principal-sr-engineer-systems-test-data-acquisition-engineer-at-northrop-grumman-3757771244,2023-12-17,Lancaster,United States,Associate,Onsite,"At Northrop Grumman, our employees have incredible opportunities to work on revolutionary systems that impact people's lives around the world today, and for generations to come. Our pioneering and inventive spirit has enabled us to be at the forefront of many technological advancements in our nation's history - from the first flight across the Atlantic Ocean, to stealth bombers, to landing on the moon. We look for people who have bold new ideas, courage and a pioneering spirit to join forces to invent the future, and have fun along the way. Our culture thrives on intellectual curiosity, cognitive diversity and bringing your whole self to work - and we have an insatiable drive to do what others think is impossible. Our employees are not only part of history, they're making history.
This role may be selected at the higher grade based on requirements listed below.
Northrop Grumman Aeronautics Systems has an opening for a Principal or Senior Principal Electronics Engineer (Data Acquisition) to join our team of qualified, diverse individuals within our Test and Evaluation organization. This role is located in Palmdale.
The primary job responsibilities are, but not limited, to the following:
Responsible for turning engineer requirement into data acquisition.
Provide mission load files to Curtiss Wright KSM-500, Omega Next, System 550, MCS, and IADS.
Develop software application for instrumentation.
Performs a variety of duties, including development, testing, and procedure writing.
This role may be selected at the higher grade based on requirements listed below.
Essential Functions
Responsible for turning engineer requirement into data acquisition.
Provide mission load files to Curtiss Wright KSM-500, Omega Next, System 550, MCS, and IADS.
Develop software application for instrumentation.
Performs a variety of duties, including development, testing, and procedure writing.
Basic Qualifications for a Principal level 3:
BS in STEM (Science, Technology, Engineering or Mathematics) and 5 years equivalent experience In Instrumentation / Data Acquisition career field or 3 years with a Master's degree and 0 years with a Ph D.
DOD Secret clearance is required to be considered
Ability to obtain Special Program Access prior to start
Familiar with ACRA Airborne Data Acquisition System KSM-500 series or industry equivalent.
Experience with ground telemetry frontends and IADS
IT working experience, candidate must have solid understanding of client and server applications
Experience with software development and maintenance
Overtime, odd shifts, and weekend work will occasionally be required
Basic Qualifications for a Sr. Principal level 4:
BS in STEM (Science, Technology, Engineering or Mathematics) and 9 years equivalent experience In Instrumentation / Data Acquisition career field or 7 years with a Master's degree and 4 years with a Ph D
DOD Secret clearance is required to be considered
Ability to obtain Special Program Access prior to start
Familiar with ACRA Airborne Data Acquisition System KSM-500 series or industry equivalent.
Experience with ground telemetry frontends and IADS
IT working experience, candidate must have solid understanding of client and server applications
Experience with software development and maintenance
Overtime, odd shifts, and weekend work will occasionally be required
Preferred Qualifications:
Active DOD Top Secret
Experience working with IRIG-106 standard, particularly chapters 4, 7, and 10
Experience working with SQL, XML, and C#, and is willing to adapt to special software projects
Active Security+ certification
9+ years of data acquisition experience working with over-the-air RF telemetry transmission and onboard recording systems
9+ experience working with ground station mission control room and test ranges
9+ years of experience working with various ICDs written for various digital bus mediums such as Ethernet, Mil-Std-1553, Arinc-429, Serial, 1394, etc
Ideal candidate must have an ability to digest different data formats and apply best filtering criteria for capturing data
5+ years of experience working with RT Station IENA data format
Experience in supporting test activities, anomaly resolution and process improvement initiatives
Familiar with the engineering development cycle, along with engineering configuration management concepts
The selected candidate will work in a dynamic people-focused environment while interacting with customers and other design engineers.
The position will be based out of Palmdale, CA.
Salary Range:
$95,000 - $142,400
Salary Range 2:
$117,700 - $176,500
The above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions.
Employees may be eligible for a discretionary bonus in addition to base pay. Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results. Employees in Vice President or Director positions may be eligible for Long Term Incentives. In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business.
Northrop Grumman is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO/AA and Pay Transparency statement, please visit http://www.northropgrumman.com/EEO . U.S. Citizenship is required for most positions.
Show more
Show less","Data Acquisition, Curtiss Wright KSM500, Omega Next, System 550, MCS, IADS, Airborne Data Acquisition System, Ground Telemetry Frontends, Client and Server Applications, Software Development, IRIG106 Standard, SQL, XML, C#, Active Security+ Certification, RT Station IENA Data Format, Engineering Development Cycle, Engineering Configuration Management","data acquisition, curtiss wright ksm500, omega next, system 550, mcs, iads, airborne data acquisition system, ground telemetry frontends, client and server applications, software development, irig106 standard, sql, xml, c, active security certification, rt station iena data format, engineering development cycle, engineering configuration management","active security certification, airborne data acquisition system, c, client and server applications, curtiss wright ksm500, data acquisition, engineering configuration management, engineering development cycle, ground telemetry frontends, iads, irig106 standard, mcs, omega next, rt station iena data format, software development, sql, system 550, xml"
Principal / Sr Principal Instrumentation and Data Acquisition Hardware Engineer,Northrop Grumman,"Palmdale, CA",https://www.linkedin.com/jobs/view/principal-sr-principal-instrumentation-and-data-acquisition-hardware-engineer-at-northrop-grumman-3757767975,2023-12-17,Lancaster,United States,Associate,Onsite,"At Northrop Grumman, our employees have incredible opportunities to work on revolutionary systems that impact people's lives around the world today, and for generations to come. Our pioneering and inventive spirit has enabled us to be at the forefront of many technological advancements in our nation's history - from the first flight across the Atlantic Ocean, to stealth bombers, to landing on the moon. We look for people who have bold new ideas, courage and a pioneering spirit to join forces to invent the future, and have fun along the way. Our culture thrives on intellectual curiosity, cognitive diversity and bringing your whole self to work - and we have an insatiable drive to do what others think is impossible. Our employees are not only part of history, they're making history.
Northrop Grumman Aeronautics Systems has an opening for an Instrumentation and Data Acquisition Hardware Engineer to join our team of qualified, diverse individuals. This position is located in Palmdale, CA.
As an Instrumentation and Data Acquisition Engineer you'll be part of the design, development, integration, configuration and test of a real-time, hardware in the loop (HITL), Data Acquisition Systems (DAS). Tasking will include design, development, verification and acceptance testing of data builds and real time Mission Control Room configuration files using Symvionics IADS software.
This position will require occasionally work shift work and overtime and travel.
""This requisition may be filled at a higher grade based on qualifications listed below.""
Basic Qualifications
""
This requisition may be filled at either a principal or a Sr. Principal Level)
Basic Qualifications For Principal Instrumentation Data Hardware Engineer
Bachelor's degree within a science, technical, engineering or mathematics (STEM Degree) discipline with 5 years of test experience, Masters with (STEM Degree) 0 years of experience.
Basic understanding of troubleshooting systems to identify and assist in the correct the root cause.
Assist with creating, editing and executing procedures.
Assist in data acquisition and data collection.
Experience with Microsoft Window Family, AutoCAD, MatLab.
Requires active DoD Secret Clearance
Ability to obtain and maintain PAR (Program Special Access)
Basic Qualification for Sr Principal Instrumentation Data Hardware Engineer:
Bachelor's degree within a science, technical, engineering or mathematics (STEM Degree) discipline with 9 years of test experience, master's with (STEM Degree) 7 years of experience.
Basic understanding of troubleshooting systems to identify and assist in the correct the root cause.
Assist with creating, editing and executing procedures.
Assist in data acquisition and data collection.
Experience with Microsoft Window Family, AutoCAD, MatLab.
Requires active DoD Secret Clearance
Ability to obtain and maintain PAR (Program Special Access)
Preferred Qualifications:
Electronics, Analog and Digital Communications, Digital Signal Processes, Computer Communication Networks, Embedded Systems
Experience working in Flight Test or Lab Test environments
Experience using oscilloscopes and waveform generators.
Experience working with Strain Gages, Accelerometers, Pressure Transducers and other commonly used instrumentation sensors
Basic knowledge of data bus architecture MIL-STD-1553, IEE-1394, ARINC-429, RS232 an RS422.
Experience using Symvionics IADS software and working in a Mission Control Room
Experience post-test processing data from a IRIG-106 CH10 recording
Experience using TTCWare Application Software
Experience working Curtiss-Wright TTC Data Acquisitions Systems
Ability to write scripts GUIs or small scale applications a plus.
Degree in Electrical or a Bachelor's of Science degree with Instrumentation data experience.
Salary Range:
$95,000 - $142,000
Salary Range 2:
$117,700 - $176,500
The above salary range represents a general guideline; however, Northrop Grumman considers a number of factors when determining base salary offers such as the scope and responsibilities of the position and the candidate's experience, education, skills and current market conditions.
Employees may be eligible for a discretionary bonus in addition to base pay. Annual bonuses are designed to reward individual contributions as well as allow employees to share in company results. Employees in Vice President or Director positions may be eligible for Long Term Incentives. In addition, Northrop Grumman provides a variety of benefits including health insurance coverage, life and disability insurance, savings plan, Company paid holidays and paid time off (PTO) for vacation and/or personal business.
Northrop Grumman is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO/AA and Pay Transparency statement, please visit http://www.northropgrumman.com/EEO . U.S. Citizenship is required for most positions.
Show more
Show less","Hardware in the loop (HITL), Data Acquisition Systems (DAS), Symvionics IADS software, Microsoft Window Family, AutoCAD, MatLab, DoD Secret Clearance, PAR (Program Special Access), Electronics, Analog and Digital Communications, Digital Signal Processes, Computer Communication Networks, Embedded Systems, IRIG106 CH10 recording, TTCWare Application Software, CurtissWright TTC Data Acquisitions Systems, GUIs, MILSTD1553, IEE1394, ARINC429, RS232, RS422","hardware in the loop hitl, data acquisition systems das, symvionics iads software, microsoft window family, autocad, matlab, dod secret clearance, par program special access, electronics, analog and digital communications, digital signal processes, computer communication networks, embedded systems, irig106 ch10 recording, ttcware application software, curtisswright ttc data acquisitions systems, guis, milstd1553, iee1394, arinc429, rs232, rs422","analog and digital communications, arinc429, autocad, computer communication networks, curtisswright ttc data acquisitions systems, data acquisition systems das, digital signal processes, dod secret clearance, electronics, embedded systems, guis, hardware in the loop hitl, iee1394, irig106 ch10 recording, matlab, microsoft window family, milstd1553, par program special access, rs232, rs422, symvionics iads software, ttcware application software"
Data Analytics Engineer,MicroStrategy,"Tysons Corner, VA",https://www.linkedin.com/jobs/view/data-analytics-engineer-at-microstrategy-3725620132,2023-12-17,Borden, Canada,Associate,Onsite,"Company Description
4000+ customers. Direct operations in 27+ countries. 5 global development centers. MicroStrategy is the largest independent publicly traded analytics and business intelligence company in the world. And we are all about innovation.
Since our inception in 1989, we have continuously pushed boundaries and transformed the landscape of enterprise analytics. In 2010, it was with MicroStrategy Mobile, one of the first mobile analytics platforms for enterprises. In 2019, it was with HyperIntelligence, the most comprehensive embedded BI tool. And we are doing it again with MicroStrategy ONE, an AI-integrated Business Intelligence platform that will transform how organizations approach analytics.
We are consistently ranked as the best Enterprise Analytics platform. And we drive value all around – for organizations around the world with innovative data analytics solutions; for investors with our sound financial strategies; and for our employees by giving them an agile working environment where they can grow.
Job Description
We are seeking a highly motivated Data Analytics Engineer to join our team. As a Data Analytics Engineer, you will be responsible for designing, developing, and implementing natural language processing (NLP) and AI-based solutions to enhance our products and services. You will work collaboratively with cross-functional teams to identify and solve complex problems using NLP and AI techniques.
Responsibilities:
Design, develop, and implement NLP and AI-based solutions to enhance our products and services.
Collaborate with cross-functional teams, including data scientists, software engineers, customer support specialists and product managers, to define and deliver NLP and AI solutions.
Conduct data analysis and exploratory research to identify potential areas for improvement.
Build and maintain large-scale data pipelines and ETL processes to support NLP and AI workflows.
Utilize programming languages such as Python, R, and SQL to develop and deploy NLP and AI models. Leverage and integrate with GPT models.
Implement best practices for NLP and AI model training, validation, and deployment to ensure accuracy, scalability, and maintainability.
Monitor and evaluate the performance of NLP and AI models, and refine them to improve accuracy and efficiency.
Keep up-to-date with the latest NLP and AI research and techniques, and share your knowledge with the team.
Qualifications
Required Experience and Skills:
Motivation, Innovation, Passion, Integrity, Teamwork, Customer-Focus.
Qualifications:
Bachelor's or Master's degree in Computer Science or a related field.
Proven experience in developing and deploying NLP and AI-based solutions.
Strong programming skills in Python, R, and SQL.
Experience with NLP and AI frameworks such as NLTK, Spacy, TensorFlow, or PyTorch.
Familiarity with data visualization tools such as MicroStrategy preferred.
Strong analytical and problem-solving skills.
If you are a passionate and driven Data Analytics Engineer who wants to make an impact with cutting-edge technologies, we would love to hear from you!
Additional Information
MicroStrategy is an Equal Employment and Affirmative Action employer F/M/Disability/Vet/Sexual Orientation/Gender Identity. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, national origin, sexual orientation, gender identity, disability, veteran status, sex age, genetic information, or any other legally-protected basis.
MicroStrategy is an Equal Employment /Affirmative Action employer and provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need an accommodation due to a disability, you may contact us about your interest in employment at 703-848-8600.
Show more
Show less","Data Analytics, Natural Language Processing, AI, Python, R, SQL, GPT, NLTK, Spacy, TensorFlow, PyTorch, MicroStrategy, Data Visualization","data analytics, natural language processing, ai, python, r, sql, gpt, nltk, spacy, tensorflow, pytorch, microstrategy, data visualization","ai, dataanalytics, gpt, microstrategy, natural language processing, nltk, python, pytorch, r, spacy, sql, tensorflow, visualization"
Data Engineer,Nexus Staff Inc.,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-at-nexus-staff-inc-3767588625,2023-12-17,Borden, Canada,Associate,Onsite,"Job Title:
Data Engineer
Employment Type: Full Time
Work Hours: 40
+
hrs./week
Work site: Hybrid
Location: Midtown Manhattan
Salary: 80k-180k
Overview
At our technology subsidiary, we are dedicated technology practitioners who work with a diverse range of industries and geographies to solve complex problems. Our team leverages cutting-edge tools to transform our clients' portfolios and create value by using data to create transparency and promote collaboration.
Responsibilities
As a Data Engineer, you will be responsible for creating solutions that extract value from rich datasets, implementing technology transformations across industries such as healthcare, retail/consumer, telecom, and industrial.
Minimum Education
Bachelor's degree in Computer Science, Software Engineering, or a related field.
Work Experience
Proven track record as a Data Engineer, designing, building, and implementing batch, event-driven, and real-time data pipelines.
Experience With
SQL Server, MySQL, or Postgres databases.
Azure tools, OAuth 2.0, and Key Vault for security.
Azure DevOps, Jenkins, Jira, and Git for DevOps.
C#, NodeJS, Java, and Scala programming languages.
Synapse, Snowflake, Redshift, and BigQuery warehousing.
Google Cloud Platform (GCP) and Amazon Web Services (AWS) cloud technologies.
Spark Streaming, Pub/Sub, and Kafka streaming.
Kubernetes, Docker, GKE, AKS, EKS for microservices.
GraphQL, OpenAPI, Swagger, SOAP, and EDI APIs.
OpenID for security.
Power BI and Tableau for BI.
Job Duties
Design, build, and implement batch, event-driven and real-time data pipelines using Microsoft Azure, GCP, and AWS technologies.
Implement large-scale data platforms to meet the analytical and operational needs across various organizations.
Build products and frameworks that can be re-used across different use-cases to increase efficiency in coding and agility in the implementation of solutions.
Build streaming ingestion processes to efficiently read, process, analyze, and publish data for the real-time needs of applications and data science models.
Perform analyses of large structured and unstructured data to solve multiple and complex business problems.
Investigate and prototype different task dependency frameworks to understand the most appropriate design for a given use case to assess and advise.
Understand business use cases to design engineering routines to affect the outcomes.
Review and assess data frameworks and technology platforms with the goal of suggesting and implementing improvements to the existing frameworks and platforms.
Understand the quality of data used in existing use cases to suggest process improvements and implement data quality routines.
Preferred Skills
Proficient in Python, SQL, and REST API.
Ability to work in a fast-paced, dynamic environment with changing priorities.
Excellent communication and interpersonal skills.
Strong problem-solving and critical thinking skills.
Ability to work independently as well as part of a team.
Ability to learn new tools and technologies quickly.
Understanding of Monolithic, SOA, Microservice, Serverless Architectures.
Understanding of authentication methods, data collection, data parsing, data validation, and data presentation.
Misc. Information
Schedule: Monday-Friday | 9 AM - 5 PM.
Interviews will be conducted In-Person or virtually.
Show more
Show less","Data Engineering, Python, SQL, REST API, Azure tools, OAuth 2.0, Key Vault, Azure DevOps, Jenkins, Jira, Git, C#, NodeJS, Java, Scala, Synapse, Snowflake, Redshift, BigQuery, Google Cloud Platform (GCP), Amazon Web Services (AWS), Spark Streaming, Pub/Sub, Kafka, Kubernetes, Docker, GKE, AKS, EKS, GraphQL, OpenAPI, Swagger, SOAP, EDI, OpenID, Power BI, Tableau, Monolithic Architectures, SOA Architectures, Microservice Architectures, Serverless Architectures, Authentication methods, Data collection, Data parsing, Data validation, Data presentation","data engineering, python, sql, rest api, azure tools, oauth 20, key vault, azure devops, jenkins, jira, git, c, nodejs, java, scala, synapse, snowflake, redshift, bigquery, google cloud platform gcp, amazon web services aws, spark streaming, pubsub, kafka, kubernetes, docker, gke, aks, eks, graphql, openapi, swagger, soap, edi, openid, power bi, tableau, monolithic architectures, soa architectures, microservice architectures, serverless architectures, authentication methods, data collection, data parsing, data validation, data presentation","aks, amazon web services aws, authentication methods, azure devops, azure tools, bigquery, c, data collection, data engineering, data parsing, data presentation, data validation, docker, edi, eks, git, gke, google cloud platform gcp, graphql, java, jenkins, jira, kafka, key vault, kubernetes, microservice architectures, monolithic architectures, nodejs, oauth 20, openapi, openid, powerbi, pubsub, python, redshift, rest api, scala, serverless architectures, snowflake, soa architectures, soap, spark streaming, sql, swagger, synapse, tableau"
Data (Databricks) Engineer,Prismagic Solutions Inc.,"New York, NY",https://www.linkedin.com/jobs/view/data-databricks-engineer-at-prismagic-solutions-inc-3768025990,2023-12-17,Borden, Canada,Associate,Onsite,"Elite Databricks team within one of the world's most fascinating organizations is looking to scale and add a Senior Databricks Engineer. This role is for someone who highly skilled in engineering and development with the Databricks platform across multiple cloud (AWS, Azure, GCP) environments. You will design, build and maintain data pipelines using Databricks extensively and must be able to adapt to rapidly changing environments. Responsibilities: -Design, build and maintain data pipelines using Databricks -Must be able to fully envision and understand requirements and build/design solutions accordingly. -Monitor, troubleshoot and maintain data pipelines -Improve/optimize data pipelines for efficiency and performance Qualifications: -Must have extensive experience working with Databricks -Data Engineering and Data pipeline experience required -SQL, Python development -Spark, Hadoop and Big Data tech experience -Multi Cloud Experience (AWS, GCP, Azure) This opportunity is perfect for someone who is looking for a team of elite Databricks specialists. Room for growth and rapid advancement is NEEDED for this person to be successful- sky is the limit.
Show more
Show less","Databricks, Cloud, Data Pipelines, Data Engineering, Data Pipeline Engineering, SQL, Python, Apache Spark, Hadoop, Big Data, AWS, Azure, GCP","databricks, cloud, data pipelines, data engineering, data pipeline engineering, sql, python, apache spark, hadoop, big data, aws, azure, gcp","apache spark, aws, azure, big data, cloud, data engineering, data pipeline engineering, databricks, datapipeline, gcp, hadoop, python, sql"
Data Engineer,Nexus Staff Inc.,"Floral Park, NY",https://www.linkedin.com/jobs/view/data-engineer-at-nexus-staff-inc-3552188324,2023-12-17,Borden, Canada,Associate,Onsite,"Job Title:
Data Engineer
Employment Type: Full Time
Work Hours: 40
+
hrs./week
Work site: Hybrid
Location: Midtown Manhattan
Salary: 80k-180k
Overview:
At our technology subsidiary, we are dedicated technology practitioners who work with a diverse range of industries and geographies to solve complex problems. Our team leverages cutting-edge tools to transform our clients' portfolios and create value by using data to create transparency and promote collaboration.
Responsibilities:
As a Data Engineer, you will be responsible for creating solutions that extract value from rich datasets, implementing technology transformations across industries such as healthcare, retail/consumer, telecom, and industrial.
Minimum Education:
Bachelor's degree in Computer Science, Software Engineering, or a related field.
Work Experience:
Proven track record as a Data Engineer, designing, building, and implementing batch, event-driven, and real-time data pipelines.
Experience with:
SQL Server, MySQL, or Postgres databases.
Azure tools, OAuth 2.0, and Key Vault for security.
Azure DevOps, Jenkins, Jira, and Git for DevOps.
C#, NodeJS, Java, and Scala programming languages.
Synapse, Snowflake, Redshift, and BigQuery warehousing.
Google Cloud Platform (GCP) and Amazon Web Services (AWS) cloud technologies.
Spark Streaming, Pub/Sub, and Kafka streaming.
Kubernetes, Docker, GKE, AKS, EKS for microservices.
GraphQL, OpenAPI, Swagger, SOAP, and EDI APIs.
OpenID for security.
Power BI and Tableau for BI.
Job Duties:
Design, build, and implement batch, event-driven and real-time data pipelines using Microsoft Azure, GCP, and AWS technologies.
Implement large-scale data platforms to meet the analytical and operational needs across various organizations.
Build products and frameworks that can be re-used across different use-cases to increase efficiency in coding and agility in the implementation of solutions.
Build streaming ingestion processes to efficiently read, process, analyze, and publish data for the real-time needs of applications and data science models.
Perform analyses of large structured and unstructured data to solve multiple and complex business problems.
Investigate and prototype different task dependency frameworks to understand the most appropriate design for a given use case to assess and advise.
Understand business use cases to design engineering routines to affect the outcomes.
Review and assess data frameworks and technology platforms with the goal of suggesting and implementing improvements to the existing frameworks and platforms.
Understand the quality of data used in existing use cases to suggest process improvements and implement data quality routines.
Preferred Skills:
Proficient in Python, SQL, and REST API.
Ability to work in a fast-paced, dynamic environment with changing priorities.
Excellent communication and interpersonal skills.
Strong problem-solving and critical thinking skills.
Ability to work independently as well as part of a team.
Ability to learn new tools and technologies quickly.
Understanding of Monolithic, SOA, Microservice, Serverless Architectures.
Understanding of authentication methods, data collection, data parsing, data validation, and data presentation.
Misc. Information:
Schedule: Monday-Friday | 9 AM - 5 PM.
Interviews will be conducted In-Person or virtually.
Show more
Show less","Data Engineering, SQL Server, MySQL, Postgres, Azure tools, OAuth 2.0, Key Vault, Azure DevOps, Jenkins, Jira, Git, C#, NodeJS, Java, Scala, Synapse, Snowflake, Redshift, BigQuery, Google Cloud Platform (GCP), Amazon Web Services (AWS), Spark Streaming, Pub/Sub, Kafka, Kubernetes, Docker, GKE, AKS, EKS, GraphQL, OpenAPI, Swagger, SOAP, EDI, OpenID, Power BI, Tableau, Python, SQL, REST API, Monolithic, SOA, Microservice, Serverless Architectures, Authentication, Data collection, Data parsing, Data validation, Data presentation","data engineering, sql server, mysql, postgres, azure tools, oauth 20, key vault, azure devops, jenkins, jira, git, c, nodejs, java, scala, synapse, snowflake, redshift, bigquery, google cloud platform gcp, amazon web services aws, spark streaming, pubsub, kafka, kubernetes, docker, gke, aks, eks, graphql, openapi, swagger, soap, edi, openid, power bi, tableau, python, sql, rest api, monolithic, soa, microservice, serverless architectures, authentication, data collection, data parsing, data validation, data presentation","aks, amazon web services aws, authentication, azure devops, azure tools, bigquery, c, data collection, data engineering, data parsing, data presentation, data validation, docker, edi, eks, git, gke, google cloud platform gcp, graphql, java, jenkins, jira, kafka, key vault, kubernetes, microservice, monolithic, mysql, nodejs, oauth 20, openapi, openid, postgres, powerbi, pubsub, python, redshift, rest api, scala, serverless architectures, snowflake, soa, soap, spark streaming, sql, sql server, swagger, synapse, tableau"
Data Engineer/Analyst,SQUAD Techlab,"Fremont, CA",https://www.linkedin.com/jobs/view/data-engineer-analyst-at-squad-techlab-3768734796,2023-12-17,Borden, Canada,Associate,Onsite,"Title: Data Engineer/Analyst – 12 Months Contract
Location: Fremont, CA (Hybrid Mode) – 3 Days Onsite in a week
Required skills/experience:
PowerBI, SAP, SharePoint
Strongly Preferred:
SQL, Synapse, Snowflake
Responsibilities
Data mining in PLM/SAP
Ensure data quality and integrity by performing data cleansing, validation, and error handling.
Optimize data infrastructure to support data processing, storage, and retrieval efficiently.
Perform exploratory data analysis to uncover trends, patterns, and insights from the data.
Conduct statistical analysis and data modeling to support business decision-making.
Develop and maintain data documentation, including data dictionaries and data lineage.
Create visually appealing and interactive dashboards that effectively communicate complex data insights to stakeholders.
Leverage best practices to present data in a clear, concise, and compelling manner.
Collaborate with business users & cross functional teams to understand their data requirements, analytical needs and translate them into actionable visualizations.
Monitor and maintain existing dashboards, making improvements and updates as needed
Minimum Qualifications
5+ years of related experience with a bachelor’s degree in Computer Science, Data Science, Statistics, Supply Chain Management, Operations Research, or a related field.
Expertise with SharePoint, Power BI
Prefer experience in ENOVIA PLM and SAP
Proven experience in data analysis, data modeling, and statistical analysis.
Proficient in data visualization tools such as Power BI
Experience managing IT programs
Strong analytical and problem-solving skills
Ability to develop front end applications with custom workflows and automations. (PowerApps, SharePoint Classic and Online, QuickBase Applications)
Ability to integrate front end applications with databases (SAP/SILK) to show real time data (Custom APIs, Webservices)
Additional Qualifications
Experience with programming languages such as SQL.
Positive attitude in a rapidly changing environment with ambiguous information.
Concise communication skills.
Intellectual curiosity with an ability to learn quickly.
Passion for technology and operations.
Experience With Databricks, Snowflake, AWS, And Other Cloud Systems
Experience with Excel and Gsheets
Strong understanding of supply chain processes and terminology.
Strong analytical and problem-solving skills.
Excellent communication and presentation skills.
Ability to work independently and as part of a team.
Show more
Show less","PowerBI, SAP, SharePoint, SQL, Synapse, Snowflake, PLM, Data cleansing, Data validation, Error handling, Data infrastructure optimization, Data processing, Data storage, Data retrieval, Exploratory data analysis, Data modeling, Statistical analysis, Data visualization, Data dictionaries, Data lineage, Dashboards, Data presentation, Business intelligence, Data requirements, Analytical needs, Actionable visualizations, IT program management, Analytical skills, Problemsolving skills, Front end development, Custom workflows, Automations, PowerApps, SharePoint Classic, SharePoint Online, QuickBase Applications, Database integration, Custom APIs, Webservices, Databricks, AWS, Cloud systems, Excel, Gsheets, Supply chain processes, Supply chain terminology","powerbi, sap, sharepoint, sql, synapse, snowflake, plm, data cleansing, data validation, error handling, data infrastructure optimization, data processing, data storage, data retrieval, exploratory data analysis, data modeling, statistical analysis, data visualization, data dictionaries, data lineage, dashboards, data presentation, business intelligence, data requirements, analytical needs, actionable visualizations, it program management, analytical skills, problemsolving skills, front end development, custom workflows, automations, powerapps, sharepoint classic, sharepoint online, quickbase applications, database integration, custom apis, webservices, databricks, aws, cloud systems, excel, gsheets, supply chain processes, supply chain terminology","actionable visualizations, analytical needs, analytical skills, automations, aws, business intelligence, cloud systems, custom apis, custom workflows, dashboard, data dictionaries, data infrastructure optimization, data lineage, data presentation, data processing, data requirements, data retrieval, data storage, data validation, database integration, databricks, datacleaning, datamodeling, error handling, excel, exploratory data analysis, front end development, gsheets, it program management, plm, powerapps, powerbi, problemsolving skills, quickbase applications, sap, sharepoint, sharepoint classic, sharepoint online, snowflake, sql, statistical analysis, supply chain processes, supply chain terminology, synapse, visualization, webservices"
Data Center Engineer - Las Vegas,DeRisk Technologies,"Las Vegas, NV",https://www.linkedin.com/jobs/view/data-center-engineer-las-vegas-at-derisk-technologies-3766678838,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers, Storage, Backup, Hyper Converged Infrastructure, Tape Storage, Power Distribution Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Diagnostics Commands, Unit Replacement, Media Insertion/Removal, Component Replacement, Fault Diagnosis, Remote Access, Storage Array Configuration, Cable Replacement, Tape Management, Device Rebooting, IT Ticket Management, Stakeholder Coordination, Phone/Remote/Onsite Support, Installation, Infrastructure Support, Asset Tagging, Task Instruction Following, Networking, Hardware, Domains, Data Center, Active Directory, Infrastructure Equipment, Rack and Stack, IMAC, Troubleshooting, Spare Identification, TCP/IP, Tape Management, Server Infrastructure, Backup/Recovery, English, Client Communication, Analytical Thinking, Record Keeping, Unsupervised Work, Time Management, Quality Focus, Productivity, Efficiency, Engineering, Technology, Science","servers, storage, backup, hyper converged infrastructure, tape storage, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, diagnostics commands, unit replacement, media insertionremoval, component replacement, fault diagnosis, remote access, storage array configuration, cable replacement, tape management, device rebooting, it ticket management, stakeholder coordination, phoneremoteonsite support, installation, infrastructure support, asset tagging, task instruction following, networking, hardware, domains, data center, active directory, infrastructure equipment, rack and stack, imac, troubleshooting, spare identification, tcpip, tape management, server infrastructure, backuprecovery, english, client communication, analytical thinking, record keeping, unsupervised work, time management, quality focus, productivity, efficiency, engineering, technology, science","access points, active directory, analytical thinking, asset tagging, backup, backuprecovery, cable management, cable replacement, client communication, component replacement, data center, device rebooting, diagnostics commands, domains, efficiency, engineering, english, fault diagnosis, firewalls, hardware, hyper converged infrastructure, imac, infrastructure equipment, infrastructure support, installation, it ticket management, kvm units, media insertionremoval, network switches, networking, phoneremoteonsite support, physical cabling, power distribution units, productivity, quality focus, rack and stack, record keeping, remote access, routers, san fabric switches, science, server infrastructure, servers, spare identification, stakeholder coordination, storage, storage array configuration, tape management, tape storage, task instruction following, tcpip, technology, time management, troubleshooting, unit replacement, unsupervised work, wan optimization devices"
Data Center Engineer - Jacksonville,DeRisk Technologies,"Jacksonville, FL",https://www.linkedin.com/jobs/view/data-center-engineer-jacksonville-at-derisk-technologies-3766680761,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers (Virtual & Physical), Storage, Backup Devices, Server Appliances, Hyper Converged Infrastructure, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Power Cycling, Diagnostics Commands, Unit Replacement, Media Handling, Component Replacement, Fault Diagnosis, Remote Access Configuration, Storage Array Configuration, Cable Replacement, Tape Management, Equipment Rebooting, IT Ticket Management, Stakeholder Coordination, Onsite Support, Installation, Asset Tagging, Task Instructions, IT Principles, Networks, Hardware, Domains, Infrastructure Architecture, Active Directory, Server/Client Operations, Rack and Stack, IMAC, Troubleshooting, Spare Identification, TCP/IP, Networking, Tape Management, Server Infrastructure Management, Backup and Recovery, English, Communication, Logical Thinking, Record Keeping, Unsupervised Work, Time Management, Quality Focus, Productivity, Efficiency, Engineering, Technology, Science","servers virtual physical, storage, backup devices, server appliances, hyper converged infrastructure, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, power cycling, diagnostics commands, unit replacement, media handling, component replacement, fault diagnosis, remote access configuration, storage array configuration, cable replacement, tape management, equipment rebooting, it ticket management, stakeholder coordination, onsite support, installation, asset tagging, task instructions, it principles, networks, hardware, domains, infrastructure architecture, active directory, serverclient operations, rack and stack, imac, troubleshooting, spare identification, tcpip, networking, tape management, server infrastructure management, backup and recovery, english, communication, logical thinking, record keeping, unsupervised work, time management, quality focus, productivity, efficiency, engineering, technology, science","access points, active directory, asset tagging, backup and recovery, backup devices, cable management, cable replacement, communication, component replacement, diagnostics commands, domains, efficiency, engineering, english, equipment rebooting, fault diagnosis, firewalls, hardware, hyper converged infrastructure, imac, infrastructure architecture, installation, it principles, it ticket management, kvm units, logical thinking, media handling, network switches, networking, networks, onsite support, physical cabling, power cycling, productivity, quality focus, rack and stack, record keeping, remote access configuration, routers, san fabric switches, science, server appliances, server infrastructure management, serverclient operations, servers virtual physical, spare identification, stakeholder coordination, storage, storage array configuration, tape management, task instructions, tcpip, technology, time management, troubleshooting, unit replacement, unsupervised work, wan optimization devices"
Data Center Engineer - Memphis,DeRisk Technologies,"Memphis, TN",https://www.linkedin.com/jobs/view/data-center-engineer-memphis-at-derisk-technologies-3766686007,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","IT principles, Networking, Hardware, Domains, Infrastructure architecture, Troubleshooting, Active Directory, Server operations, Client operations, IMAC, Breakfix, TCP/IP standards, Tape Management, Backup software, Recovery methodologies, English, Customer facing skills, Communication skills, Logical thinking, Analytical thinking, Record keeping, Time management, Quality work, Productivity, Efficiency","it principles, networking, hardware, domains, infrastructure architecture, troubleshooting, active directory, server operations, client operations, imac, breakfix, tcpip standards, tape management, backup software, recovery methodologies, english, customer facing skills, communication skills, logical thinking, analytical thinking, record keeping, time management, quality work, productivity, efficiency","active directory, analytical thinking, backup software, breakfix, client operations, communication skills, customer facing skills, domains, efficiency, english, hardware, imac, infrastructure architecture, it principles, logical thinking, networking, productivity, quality work, record keeping, recovery methodologies, server operations, tape management, tcpip standards, time management, troubleshooting"
Data Center Engineer - Boston,DeRisk Technologies,"Boston, MA",https://www.linkedin.com/jobs/view/data-center-engineer-boston-at-derisk-technologies-3766676864,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers, Storage, Backup, Hyper Converged Infrastructure, Tape Storage, Power Distribution Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Power Cycling, Diagnostics Commands, Whole Unit Replacement, Media Insertion/Removal, Component Replacement, Fault Diagnosis, Remote Access Configuration, Storage Array Configuration, Faulty Cable Replacement, Tape Management/Maintenance, Router/Server/Storage Device Rebooting, IT Ticket Management, Stakeholder Coordination, Phone/Remote/Onsite Support, Installation, Tool Carrying, Labelling, Patching, Asset Tagging, Task Instruction Following, Reporting, Network Principles, Hardware Principles, Domain Principles, Infrastructure Architecture, Server/Client Operations, Active Directory, Hardware Infrastructure Platforms, Infrastructure Equipment Installation/Troubleshooting, Rack and Stack, IMAC Experience, BreakFix Activities, Defective Spare Identification, Spare Replacement, TCP/IP Standards, Networking, Tape Management, Server Infrastructure Management/Control/Monitoring, Backup and Recovery Software/Methodologies, English, Customer Facing Skills, Clear Communication, Logical Approach, Analytical Approach, Accurate Record Keeping, Unsupervised Work, Timekeeping, Quality Focus, Productivity, Efficiency, Engineering/Technology/Science Degree","servers, storage, backup, hyper converged infrastructure, tape storage, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, power cycling, diagnostics commands, whole unit replacement, media insertionremoval, component replacement, fault diagnosis, remote access configuration, storage array configuration, faulty cable replacement, tape managementmaintenance, routerserverstorage device rebooting, it ticket management, stakeholder coordination, phoneremoteonsite support, installation, tool carrying, labelling, patching, asset tagging, task instruction following, reporting, network principles, hardware principles, domain principles, infrastructure architecture, serverclient operations, active directory, hardware infrastructure platforms, infrastructure equipment installationtroubleshooting, rack and stack, imac experience, breakfix activities, defective spare identification, spare replacement, tcpip standards, networking, tape management, server infrastructure managementcontrolmonitoring, backup and recovery softwaremethodologies, english, customer facing skills, clear communication, logical approach, analytical approach, accurate record keeping, unsupervised work, timekeeping, quality focus, productivity, efficiency, engineeringtechnologyscience degree","access points, accurate record keeping, active directory, analytical approach, asset tagging, backup, backup and recovery softwaremethodologies, breakfix activities, cable management, clear communication, component replacement, customer facing skills, defective spare identification, diagnostics commands, domain principles, efficiency, engineeringtechnologyscience degree, english, fault diagnosis, faulty cable replacement, firewalls, hardware infrastructure platforms, hardware principles, hyper converged infrastructure, imac experience, infrastructure architecture, infrastructure equipment installationtroubleshooting, installation, it ticket management, kvm units, labelling, logical approach, media insertionremoval, network principles, network switches, networking, patching, phoneremoteonsite support, physical cabling, power cycling, power distribution units, productivity, quality focus, rack and stack, remote access configuration, reporting, routers, routerserverstorage device rebooting, san fabric switches, server infrastructure managementcontrolmonitoring, serverclient operations, servers, spare replacement, stakeholder coordination, storage, storage array configuration, tape management, tape managementmaintenance, tape storage, task instruction following, tcpip standards, timekeeping, tool carrying, unsupervised work, wan optimization devices, whole unit replacement"
Data Center Engineer - Pittsburgh,DeRisk Technologies,"Pittsburgh, PA",https://www.linkedin.com/jobs/view/data-center-engineer-pittsburgh-at-derisk-technologies-3766681723,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","IT principles, Networks, Hardware, Domains, Data Center, Network, Active Directory, IMAC, TCP/IP, Backup, Recovery, English, Customer service, Problem solving, Time management, Quality control, Communication, Technical writing","it principles, networks, hardware, domains, data center, network, active directory, imac, tcpip, backup, recovery, english, customer service, problem solving, time management, quality control, communication, technical writing","active directory, backup, communication, customer service, data center, domains, english, hardware, imac, it principles, network, networks, problem solving, quality control, recovery, tcpip, technical writing, time management"
Data Center Engineer - Upper Arlington,DeRisk Technologies,"Upper Arlington, OH",https://www.linkedin.com/jobs/view/data-center-engineer-upper-arlington-at-derisk-technologies-3759946946,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","IT Principles, Networking, Hardware, Domains, Infrastructure Architecture, Active Directory, Server/Client Operations, Infrastructure Equipment, Rack and Stack, IMAC, Breakfix, TCP/IP Standards, Tape Management, Server Infrastructure Management, Backup and Recovery Software, English, Customer Facing Skills, Communication Skills, Logical and Analytical Thinking, Record Keeping, Unsupervised Work, Time Management, Quality Focus, Productivity, Efficiency","it principles, networking, hardware, domains, infrastructure architecture, active directory, serverclient operations, infrastructure equipment, rack and stack, imac, breakfix, tcpip standards, tape management, server infrastructure management, backup and recovery software, english, customer facing skills, communication skills, logical and analytical thinking, record keeping, unsupervised work, time management, quality focus, productivity, efficiency","active directory, backup and recovery software, breakfix, communication skills, customer facing skills, domains, efficiency, english, hardware, imac, infrastructure architecture, infrastructure equipment, it principles, logical and analytical thinking, networking, productivity, quality focus, rack and stack, record keeping, server infrastructure management, serverclient operations, tape management, tcpip standards, time management, unsupervised work"
Data Center Engineer - Tampa,DeRisk Technologies,"Tampa, FL",https://www.linkedin.com/jobs/view/data-center-engineer-tampa-at-derisk-technologies-3766678859,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Server Management, Storage Management, Backup and Recovery, Networking, Troubleshooting, Installation, Configuration, Data Center Hardware Architecture, Active Directory, IMAC, Breakfix, TCP/IP, Tape Management, Server Infrastructure Management, Backup and Recovery Software, English, Customer Service, Communication, Analytical Thinking, Record Keeping, Time Management, Quality Assurance, Productivity, Efficiency, Bachelor's in Engineering/Technology/Science","server management, storage management, backup and recovery, networking, troubleshooting, installation, configuration, data center hardware architecture, active directory, imac, breakfix, tcpip, tape management, server infrastructure management, backup and recovery software, english, customer service, communication, analytical thinking, record keeping, time management, quality assurance, productivity, efficiency, bachelors in engineeringtechnologyscience","active directory, analytical thinking, bachelors in engineeringtechnologyscience, backup and recovery, backup and recovery software, breakfix, communication, configuration, customer service, data center hardware architecture, efficiency, english, imac, installation, networking, productivity, quality assurance, record keeping, server infrastructure management, server management, storage management, tape management, tcpip, time management, troubleshooting"
Data Center Engineer - Miami,DeRisk Technologies,"Miami, FL",https://www.linkedin.com/jobs/view/data-center-engineer-miami-at-derisk-technologies-3766680759,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Nutanix, Cisco UCS, Tape Storage Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Power Cycling, Diagnostics Commands, Whole Unit Replacement, Inserting/Removing Media, Replacing Defective Components, Fault Diagnosis, Investigation, Configuring Remote Access, Basic Storage Array Configuration, Replacing Faulty Cables, Tape Management, Tape Maintenance, Rebooting Devices, Updating and Recording Activities, IT Ticket Management System, Coordinating Attendance, Phone Support, Remote Tools, Onsite Support, Installation, Labelling, Patching, Asset Tagging, Task Instructions, Reporting, Networks, Hardware, Domains, Infrastructure Architecture, Troubleshooting, H&E Support, Server/Client Operations, Domain Environment, Active Directory, Hardware Infrastructure Platforms, Installation, Troubleshooting, Infrastructure Equipment, Rack and Stack, IMAC, Breakfix Activities, Infrastructure Environment, Identifying Defective Spares, Replacing Defective Spares, TCP/IP Standards, Networking, Tape Management, Best Practices, Management, Control, Monitoring, Server Infrastructure, Backup Software, Recovery Software, Methodologies, English, Customer Facing Skills, Communication, Logical Thinking, Analytical Approach, Record Keeping, Unsupervised Work, Timekeeping, Quality Focus, Productivity, Efficiency, Engineering, Technology, Science","nutanix, cisco ucs, tape storage units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, power cycling, diagnostics commands, whole unit replacement, insertingremoving media, replacing defective components, fault diagnosis, investigation, configuring remote access, basic storage array configuration, replacing faulty cables, tape management, tape maintenance, rebooting devices, updating and recording activities, it ticket management system, coordinating attendance, phone support, remote tools, onsite support, installation, labelling, patching, asset tagging, task instructions, reporting, networks, hardware, domains, infrastructure architecture, troubleshooting, he support, serverclient operations, domain environment, active directory, hardware infrastructure platforms, installation, troubleshooting, infrastructure equipment, rack and stack, imac, breakfix activities, infrastructure environment, identifying defective spares, replacing defective spares, tcpip standards, networking, tape management, best practices, management, control, monitoring, server infrastructure, backup software, recovery software, methodologies, english, customer facing skills, communication, logical thinking, analytical approach, record keeping, unsupervised work, timekeeping, quality focus, productivity, efficiency, engineering, technology, science","access points, active directory, analytical approach, asset tagging, backup software, basic storage array configuration, best practices, breakfix activities, cable management, cisco ucs, communication, configuring remote access, control, coordinating attendance, customer facing skills, diagnostics commands, domain environment, domains, efficiency, engineering, english, fault diagnosis, firewalls, hardware, hardware infrastructure platforms, he support, identifying defective spares, imac, infrastructure architecture, infrastructure environment, infrastructure equipment, insertingremoving media, installation, investigation, it ticket management system, kvm units, labelling, logical thinking, management, methodologies, monitoring, network switches, networking, networks, nutanix, onsite support, patching, phone support, physical cabling, power cycling, productivity, quality focus, rack and stack, rebooting devices, record keeping, recovery software, remote tools, replacing defective components, replacing defective spares, replacing faulty cables, reporting, routers, san fabric switches, science, server infrastructure, serverclient operations, tape maintenance, tape management, tape storage units, task instructions, tcpip standards, technology, timekeeping, troubleshooting, unsupervised work, updating and recording activities, wan optimization devices, whole unit replacement"
Data Center Engineer - Phoenix,DeRisk Technologies,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-center-engineer-phoenix-at-derisk-technologies-3766681729,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers (Virtual & Physical), Storage & Backup Devices, Server Appliances, Hyper Converged Infrastructure, Tape Storage Units, Power Distribution Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Power Cycling, Running Diagnostics Commands, Whole Unit Replacement, Inserting/Removing Media, Replacing Defective Components, Fault Diagnosis and Investigation, Configuring Remote Access, Basic Storage Array Configuration, Replacing Faulty Cables, Tape Management and Maintenance, Rebooting Routers Servers Storage Devices, Updating and Recording IT Ticket Management System, Coordinating Attendance Time and Date, Provide Support (Phone Remote Onsite), Installation (Physical or Network Medium), Coordinate Activity Date and Timings, Carry Necessary Tools and Laptops, Labelling Patching Asset Tagging Activities, Task Instructions and Reporting, Networks, Hardware, Domains, Infrastructure (Data Center and Network) Hardware Architecture, Server/Client Operations, Active Directory, Current and Legacy Hardware Infrastructure Platforms, Installation and Troubleshooting Infrastructure Equipment, Rack and Stack Equipment/Cable, IMAC and Breakfix Activities, Identifying Defective Spares, Replacing Defective Spares, TCP/IP Standards, Networking, Tape Management Activities, Backup and Recovery Software, Methodologies, English, Exceptional Customer Facing Skills, Communication (Client and Customer), Logical and Analytical Approach to Work, Accurate Record Keeping, Unsupervised Work, Timekeeping, Focus on Quality Work, Productivity, Efficiency, Bachelor of Engineering / Technology / Science, 5  7 Years Work Experience","servers virtual physical, storage backup devices, server appliances, hyper converged infrastructure, tape storage units, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, power cycling, running diagnostics commands, whole unit replacement, insertingremoving media, replacing defective components, fault diagnosis and investigation, configuring remote access, basic storage array configuration, replacing faulty cables, tape management and maintenance, rebooting routers servers storage devices, updating and recording it ticket management system, coordinating attendance time and date, provide support phone remote onsite, installation physical or network medium, coordinate activity date and timings, carry necessary tools and laptops, labelling patching asset tagging activities, task instructions and reporting, networks, hardware, domains, infrastructure data center and network hardware architecture, serverclient operations, active directory, current and legacy hardware infrastructure platforms, installation and troubleshooting infrastructure equipment, rack and stack equipmentcable, imac and breakfix activities, identifying defective spares, replacing defective spares, tcpip standards, networking, tape management activities, backup and recovery software, methodologies, english, exceptional customer facing skills, communication client and customer, logical and analytical approach to work, accurate record keeping, unsupervised work, timekeeping, focus on quality work, productivity, efficiency, bachelor of engineering technology science, 5 7 years work experience","5 7 years work experience, access points, accurate record keeping, active directory, bachelor of engineering technology science, backup and recovery software, basic storage array configuration, cable management, carry necessary tools and laptops, communication client and customer, configuring remote access, coordinate activity date and timings, coordinating attendance time and date, current and legacy hardware infrastructure platforms, domains, efficiency, english, exceptional customer facing skills, fault diagnosis and investigation, firewalls, focus on quality work, hardware, hyper converged infrastructure, identifying defective spares, imac and breakfix activities, infrastructure data center and network hardware architecture, insertingremoving media, installation and troubleshooting infrastructure equipment, installation physical or network medium, kvm units, labelling patching asset tagging activities, logical and analytical approach to work, methodologies, network switches, networking, networks, physical cabling, power cycling, power distribution units, productivity, provide support phone remote onsite, rack and stack equipmentcable, rebooting routers servers storage devices, replacing defective components, replacing defective spares, replacing faulty cables, routers, running diagnostics commands, san fabric switches, server appliances, serverclient operations, servers virtual physical, storage backup devices, tape management activities, tape management and maintenance, tape storage units, task instructions and reporting, tcpip standards, timekeeping, unsupervised work, updating and recording it ticket management system, wan optimization devices, whole unit replacement"
Data Center Engineer - Omaha,DeRisk Technologies,"Omaha, NE",https://www.linkedin.com/jobs/view/data-center-engineer-omaha-at-derisk-technologies-3766680770,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Active Directory, Backup and recovery software, Cable Management, Cisco UCS, Data Center, Domains, English, Firewalls, Hardware, Hyper Converged Infrastructure, IMAC, IT Ticket Management System, KVM Units, Networks, Nutanix, Power Distribution Units, Rack and Stack, SAN Fabric Switches, Servers, Storage & Backup Devices, Storage Array Configuration, Tape Management, TCP/IP, Virtual machines","active directory, backup and recovery software, cable management, cisco ucs, data center, domains, english, firewalls, hardware, hyper converged infrastructure, imac, it ticket management system, kvm units, networks, nutanix, power distribution units, rack and stack, san fabric switches, servers, storage backup devices, storage array configuration, tape management, tcpip, virtual machines","active directory, backup and recovery software, cable management, cisco ucs, data center, domains, english, firewalls, hardware, hyper converged infrastructure, imac, it ticket management system, kvm units, networks, nutanix, power distribution units, rack and stack, san fabric switches, servers, storage array configuration, storage backup devices, tape management, tcpip, virtual machines"
"Data Engineer, LLM",Genentech,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-llm-at-genentech-3772239502,2023-12-17,Borden, Canada,Associate,Onsite,"The Position
The Position
Prescient Design is developing cutting-edge machine learning for drug discovery, building new methods, techniques, and systems to fundamentally transform how drugs are designed.
The Engineering team within the Prescient Design group is looking for exceptional data engineers in order to design and build the data infrastructure foundation for our molecule design system. The data platform will play a key role in Prescient Design’s success. We are looking for someone with a background in software engineering, a passion for technical problem-solving, and a proven ability to build data infrastructure and pipelines for modeling.
The Data Engineer will work closely with ML researchers to enable end-to-end data machine learning workflows on various modeling efforts including supporting training protein language models, as well as interfacing with other teams within Genentech Research and Early Development (gRED).
The Role
Enable cutting-edge research in machine learning and applications to drug discovery, design, and development through the collection, design, and management of data pipelines and infrastructure.
You will collaborate closely with cross-functional teams across both Prescient Design and gRED to solve complex problems in the life sciences, including understanding and analyzing algorithm issues.
You will interface with other teams at gRED in developing a common data architecture and model and in formalizing best practices.
You will be expected to help develop, manage, and scale data pipelines and infrastructure for analysis and modeling in production.
You will be expected to solve core engineering challenges including the design and implementation of stable data architecture and models.
You will be expected to serve as an expert and resource for multiple, diverse groups at Prescient Design and gRED.
Qualifications
B.S., M.S., or Ph.D. in Computer Science, Statistics, Applied Mathematics, Computational Biology, Physics, related technical field, or equivalent practical experience.
At least one year relevant work experience.
Advanced programming skills in languages like C++, Python, Java, Scala, or SQL.
Preferred Qualifications
Experience with cloud computing and infrastructure including Amazon Web Services (AWS) and distributed computing libraries like Spark, Hive, Impala, and Kafka.
Experience with data modeling and schema design, including databases and file systems for scientific data
Experience with containerization and orchestration tools like Docker, Singularity, Airflow, Luigi, and Kubernetes.
Experience developing and maintaining codebases and software libraries, following industry best practices.
Experience with CI/CD and automation tools like Terraform, CloudFormation, Jenkins, and Ansible.
Experience with tools and platforms for MLOps like Weights & Biases.
Intense curiosity about the biology of disease and eagerness to contribute to scientific and computational efforts.
#gCS
Who We Are
Genentech, a member of the Roche group and founder of the biotechnology industry, is dedicated to pursuing groundbreaking science to discover and develop medicines for people with serious and life-threatening diseases. To solve the world's most complex health challenges, we ask bigger questions that challenge our industry and the boundaries of science to transform society. Our transformational discoveries include the first targeted antibody for cancer and the first medicine for primary progressive multiple sclerosis.
Genentech is an equal opportunity employer & prohibits unlawful discrimination based on race, color, religion, gender, sexual orientation, gender identity/expression, national origin/ancestry, age, disability, marital & veteran status. For more information about equal employment opportunity, visit our Genentech Careers page.
The expected salary range for this position based on the primary location of New York City is $125,000 - 232,100 of hiring range. Actual pay will be determined based on experience, qualifications, geographic location, and other job-related factors permitted by law. A discretionary annual bonus may be available based on individual and Company performance. This position also qualifies for the benefits detailed at the link provided.
Genentech is an equal opportunity employer, and we embrace the increasingly diverse world around us. Genentech prohibits unlawful discrimination based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin or ancestry, age, disability, marital status and veteran status.
Show more
Show less","Machine Learning, Data Infrastructure, Python, C++, SQL, Cloud Computing, AWS, Data Modeling, Schema Design, Containerization, Docker, Kubernetes, MLOps, Terraform, CloudFormation, Jenkins","machine learning, data infrastructure, python, c, sql, cloud computing, aws, data modeling, schema design, containerization, docker, kubernetes, mlops, terraform, cloudformation, jenkins","aws, c, cloud computing, cloudformation, containerization, data infrastructure, datamodeling, docker, jenkins, kubernetes, machine learning, mlops, python, schema design, sql, terraform"
Data Center Engineer - Austin,DeRisk Technologies,"Austin, TX",https://www.linkedin.com/jobs/view/data-center-engineer-austin-at-derisk-technologies-3766676856,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers, Storage, Backup devices, Server appliances, Hyperconverged Infrastructure, Tape storage, Power distribution units, SAN fabric switches, Network switches, KVM Units, WAN optimization devices, Firewalls, Access points, Routers, Physical cabling, Cable management, Power cycling, Diagnostics commands, Whole unit replacement, Media insertion/removal, Component replacement, Fault diagnosis and investigation, Remote access configuration, Storage array configuration, Faulty cable replacement, Tape management and maintenance, Equipment rebooting, IT ticket management system, Infrastructure environment support, Labelling, Patching, Asset tagging, Task instruction following, Reporting, Networks, Hardware, Domains, Infrastructure architecture, Server/client operations, Active Directory, Infrastructure equipment installation and troubleshooting, Rack and stack, IMAC, Breakfix activities, Defective spare identification and replacement, TCP/IP standards, Networking, Tape management, Server infrastructure management control and monitoring, Backup and recovery software and methodologies, English, Customer service skills, Clear and effective communication, Logical and analytical approach, Accurate record keeping, Unsupervised work ability, Timekeeping skills, Quality focus, Productivity and efficiency, Engineering technology or science degree, 57 years of experience","servers, storage, backup devices, server appliances, hyperconverged infrastructure, tape storage, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, power cycling, diagnostics commands, whole unit replacement, media insertionremoval, component replacement, fault diagnosis and investigation, remote access configuration, storage array configuration, faulty cable replacement, tape management and maintenance, equipment rebooting, it ticket management system, infrastructure environment support, labelling, patching, asset tagging, task instruction following, reporting, networks, hardware, domains, infrastructure architecture, serverclient operations, active directory, infrastructure equipment installation and troubleshooting, rack and stack, imac, breakfix activities, defective spare identification and replacement, tcpip standards, networking, tape management, server infrastructure management control and monitoring, backup and recovery software and methodologies, english, customer service skills, clear and effective communication, logical and analytical approach, accurate record keeping, unsupervised work ability, timekeeping skills, quality focus, productivity and efficiency, engineering technology or science degree, 57 years of experience","57 years of experience, access points, accurate record keeping, active directory, asset tagging, backup and recovery software and methodologies, backup devices, breakfix activities, cable management, clear and effective communication, component replacement, customer service skills, defective spare identification and replacement, diagnostics commands, domains, engineering technology or science degree, english, equipment rebooting, fault diagnosis and investigation, faulty cable replacement, firewalls, hardware, hyperconverged infrastructure, imac, infrastructure architecture, infrastructure environment support, infrastructure equipment installation and troubleshooting, it ticket management system, kvm units, labelling, logical and analytical approach, media insertionremoval, network switches, networking, networks, patching, physical cabling, power cycling, power distribution units, productivity and efficiency, quality focus, rack and stack, remote access configuration, reporting, routers, san fabric switches, server appliances, server infrastructure management control and monitoring, serverclient operations, servers, storage, storage array configuration, tape management, tape management and maintenance, tape storage, task instruction following, tcpip standards, timekeeping skills, unsupervised work ability, wan optimization devices, whole unit replacement"
Data Center Engineer - Seattle,DeRisk Technologies,"Seattle, WA",https://www.linkedin.com/jobs/view/data-center-engineer-seattle-at-derisk-technologies-3766682212,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers, Storage, Backup, Hyper Converged Infrastructure, Tape Storage, Power Distribution Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Power Cycling, Diagnostics Commands, Whole Unit Replacement, Media Insertion/Removal, Component Replacement, Fault Diagnosis/Investigation, Remote Access Configuration, Storage Array Configuration, Faulty Cable Replacement, Tape Maintenance, Router/Server/Storage Rebooting, Ticketing System Utilization, Stakeholder Coordination, Phone/Remote/Onsite Support, Installation, Tool/Laptop Deployment, Labeling/Patching/Asset Tagging, Task Instructions/Reporting, Networking, Hardware, Domains, Infrastructure Architecture, Active Directory, Legacy Hardware Infrastructure, Equipment Installation/Troubleshooting, Rack and Stack, IMAC Activities, Troubleshooting, Spare Identification/Replacement, TCP/IP Standards, Tape Management, Server Infrastructure Management, Backup/Recovery Software, English, Customer Service, Communication, Logical/Analytical Thinking, Record Keeping, Unsupervised Work, Time Management, Quality Control, Efficiency","servers, storage, backup, hyper converged infrastructure, tape storage, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, power cycling, diagnostics commands, whole unit replacement, media insertionremoval, component replacement, fault diagnosisinvestigation, remote access configuration, storage array configuration, faulty cable replacement, tape maintenance, routerserverstorage rebooting, ticketing system utilization, stakeholder coordination, phoneremoteonsite support, installation, toollaptop deployment, labelingpatchingasset tagging, task instructionsreporting, networking, hardware, domains, infrastructure architecture, active directory, legacy hardware infrastructure, equipment installationtroubleshooting, rack and stack, imac activities, troubleshooting, spare identificationreplacement, tcpip standards, tape management, server infrastructure management, backuprecovery software, english, customer service, communication, logicalanalytical thinking, record keeping, unsupervised work, time management, quality control, efficiency","access points, active directory, backup, backuprecovery software, cable management, communication, component replacement, customer service, diagnostics commands, domains, efficiency, english, equipment installationtroubleshooting, fault diagnosisinvestigation, faulty cable replacement, firewalls, hardware, hyper converged infrastructure, imac activities, infrastructure architecture, installation, kvm units, labelingpatchingasset tagging, legacy hardware infrastructure, logicalanalytical thinking, media insertionremoval, network switches, networking, phoneremoteonsite support, physical cabling, power cycling, power distribution units, quality control, rack and stack, record keeping, remote access configuration, routers, routerserverstorage rebooting, san fabric switches, server infrastructure management, servers, spare identificationreplacement, stakeholder coordination, storage, storage array configuration, tape maintenance, tape management, tape storage, task instructionsreporting, tcpip standards, ticketing system utilization, time management, toollaptop deployment, troubleshooting, unsupervised work, wan optimization devices, whole unit replacement"
Data Center Engineer - San Diego,DeRisk Technologies,"San Diego, CA",https://www.linkedin.com/jobs/view/data-center-engineer-san-diego-at-derisk-technologies-3766684315,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers, Virtualization, Storage, Backup, Hyperconverged infrastructure, Tape storage, Power distribution units, SAN switches, Network switches, KVM, WAN optimization devices, Firewalls, Access points, Routers, Cabling, Cable management, Diagnostics, Breakfix, Configuration, Troubleshooting, Active Directory, TCP/IP, Tape management, Backup and recovery, English, Communication, Analytical thinking, Record keeping, Time management, Quality control","servers, virtualization, storage, backup, hyperconverged infrastructure, tape storage, power distribution units, san switches, network switches, kvm, wan optimization devices, firewalls, access points, routers, cabling, cable management, diagnostics, breakfix, configuration, troubleshooting, active directory, tcpip, tape management, backup and recovery, english, communication, analytical thinking, record keeping, time management, quality control","access points, active directory, analytical thinking, backup, backup and recovery, breakfix, cable management, cabling, communication, configuration, diagnostics, english, firewalls, hyperconverged infrastructure, kvm, network switches, power distribution units, quality control, record keeping, routers, san switches, servers, storage, tape management, tape storage, tcpip, time management, troubleshooting, virtualization, wan optimization devices"
Data Center Engineer - Dallas,DeRisk Technologies,"Dallas, TX",https://www.linkedin.com/jobs/view/data-center-engineer-dallas-at-derisk-technologies-3766685012,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers, Storage, Backup Devices, Hyper Converged Infrastructure, Tape Storage Units, Power Distribution Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Power Cycling, Diagnostics Commands, Whole Unit Replacement, Media Insertion/Removal, Defective Component Replacement, Fault Diagnosis and Investigation, Remote Access Configuration, Storage Array Configuration, Faulty Cable Replacement, Tape Management and Maintenance, Router/Server/Storage Rebooting, IT Ticket Management, Stakeholder Coordination, Phone/Remote/Onsite Support, Installation (Physical/Network), Activity Date/Time Coordination, Tool Carrying and Usage, Labeling Patching Asset Tagging, Task Instruction Following, Reporting, Networking, Hardware, Domains, Infrastructure, Data Center, Active Directory, Server/Client Operations, Rack and Stack, IMAC, Breakfix, TCP/IP Standards, Tape Management, Server Infrastructure, Backup and Recovery Software, English, Customer Service, Communication, Logical Thinking, Analytical Thinking, Record Keeping, Unsupervised Work, Time Management, Quality Focus, Productivity, Bachelor's Degree (Engineering/Technology/Science), 57 Years Experience","servers, storage, backup devices, hyper converged infrastructure, tape storage units, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, power cycling, diagnostics commands, whole unit replacement, media insertionremoval, defective component replacement, fault diagnosis and investigation, remote access configuration, storage array configuration, faulty cable replacement, tape management and maintenance, routerserverstorage rebooting, it ticket management, stakeholder coordination, phoneremoteonsite support, installation physicalnetwork, activity datetime coordination, tool carrying and usage, labeling patching asset tagging, task instruction following, reporting, networking, hardware, domains, infrastructure, data center, active directory, serverclient operations, rack and stack, imac, breakfix, tcpip standards, tape management, server infrastructure, backup and recovery software, english, customer service, communication, logical thinking, analytical thinking, record keeping, unsupervised work, time management, quality focus, productivity, bachelors degree engineeringtechnologyscience, 57 years experience","57 years experience, access points, active directory, activity datetime coordination, analytical thinking, bachelors degree engineeringtechnologyscience, backup and recovery software, backup devices, breakfix, cable management, communication, customer service, data center, defective component replacement, diagnostics commands, domains, english, fault diagnosis and investigation, faulty cable replacement, firewalls, hardware, hyper converged infrastructure, imac, infrastructure, installation physicalnetwork, it ticket management, kvm units, labeling patching asset tagging, logical thinking, media insertionremoval, network switches, networking, phoneremoteonsite support, physical cabling, power cycling, power distribution units, productivity, quality focus, rack and stack, record keeping, remote access configuration, reporting, routers, routerserverstorage rebooting, san fabric switches, server infrastructure, serverclient operations, servers, stakeholder coordination, storage, storage array configuration, tape management, tape management and maintenance, tape storage units, task instruction following, tcpip standards, time management, tool carrying and usage, unsupervised work, wan optimization devices, whole unit replacement"
Data Center Engineer - St. Louis,DeRisk Technologies,"St Louis, MO",https://www.linkedin.com/jobs/view/data-center-engineer-st-louis-at-derisk-technologies-3766684640,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Network, Hardware, Servers, Storage, Backup, Virtualization, Hyper Converged Infrastructure, SAN Fabric, KVM, Firewalls, Access Points, Routers, Cabling, Troubleshooting, Diagnostics, Breakfix, Configuration, Tape Management, RAID, Active Directory, Domains, TCP/IP, Backup and Recovery, English, Customer Service, Communication, Problem Solving, Time Management, Quality Control","network, hardware, servers, storage, backup, virtualization, hyper converged infrastructure, san fabric, kvm, firewalls, access points, routers, cabling, troubleshooting, diagnostics, breakfix, configuration, tape management, raid, active directory, domains, tcpip, backup and recovery, english, customer service, communication, problem solving, time management, quality control","access points, active directory, backup, backup and recovery, breakfix, cabling, communication, configuration, customer service, diagnostics, domains, english, firewalls, hardware, hyper converged infrastructure, kvm, network, problem solving, quality control, raid, routers, san fabric, servers, storage, tape management, tcpip, time management, troubleshooting, virtualization"
Data Center Engineer - Nashville,DeRisk Technologies,"Nashville, TN",https://www.linkedin.com/jobs/view/data-center-engineer-nashville-at-derisk-technologies-3766680788,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers, Storage, Backup, Hyper Converged Infrastructure, Tape Storage, Power Distribution Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Remote Access, Storage Array Configuration, Tape Management, IT Ticket Management System, Active Directory, TCP/IP, Best Practices, Backup and Recovery Software, English, Customer Service, Communication, Logical Thinking, Analytical Thinking, Record Keeping, Time Management, Quality Control, Productivity, Efficiency, Bachelor's Degree, Engineering, Technology, Science","servers, storage, backup, hyper converged infrastructure, tape storage, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, remote access, storage array configuration, tape management, it ticket management system, active directory, tcpip, best practices, backup and recovery software, english, customer service, communication, logical thinking, analytical thinking, record keeping, time management, quality control, productivity, efficiency, bachelors degree, engineering, technology, science","access points, active directory, analytical thinking, bachelors degree, backup, backup and recovery software, best practices, cable management, communication, customer service, efficiency, engineering, english, firewalls, hyper converged infrastructure, it ticket management system, kvm units, logical thinking, network switches, physical cabling, power distribution units, productivity, quality control, record keeping, remote access, routers, san fabric switches, science, servers, storage, storage array configuration, tape management, tape storage, tcpip, technology, time management, wan optimization devices"
Data Center Engineer - Kansas City,DeRisk Technologies,"Kansas City, MO",https://www.linkedin.com/jobs/view/data-center-engineer-kansas-city-at-derisk-technologies-3766680683,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Networking, Hardware, Active Directory, Server Infrastructure, IMAC, Breakfix, TCP/IP, Tape Management, Backup and Recovery","networking, hardware, active directory, server infrastructure, imac, breakfix, tcpip, tape management, backup and recovery","active directory, backup and recovery, breakfix, hardware, imac, networking, server infrastructure, tape management, tcpip"
Data Center Engineer - Portland,DeRisk Technologies,"Portland, OR",https://www.linkedin.com/jobs/view/data-center-engineer-portland-at-derisk-technologies-3766685481,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Server, Storage, Backup, Hyperconverged Infrastructure, Tape Storage, Power Distribution Units, KVM, WAN Optimization, Firewall, Access Point, Router, Network Switch, Cable Management, Network, Hardware, Domain, Active Directory, TCP/IP, Backup, Recovery, English, Customer Service","server, storage, backup, hyperconverged infrastructure, tape storage, power distribution units, kvm, wan optimization, firewall, access point, router, network switch, cable management, network, hardware, domain, active directory, tcpip, backup, recovery, english, customer service","access point, active directory, backup, cable management, customer service, domain, english, firewall, hardware, hyperconverged infrastructure, kvm, network, network switch, power distribution units, recovery, router, server, storage, tape storage, tcpip, wan optimization"
Data Center Engineer - Indianapolis,DeRisk Technologies,"Indianapolis, IN",https://www.linkedin.com/jobs/view/data-center-engineer-indianapolis-at-derisk-technologies-3766678923,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","IT principles, Networking, Hardware, Domains, Infrastructure architecture, Active Directory, Server/client operations, Rack and stack, IMAC, Breakfix, Tape Management, TCP/IP, Backup and recovery software, English, Customer service, Communication, Logical thinking, Analytical thinking, Record keeping, Unsupervised work, Time management, Quality work, Productivity, Efficiency, Bachelor's degree in Engineering / Technology / Science","it principles, networking, hardware, domains, infrastructure architecture, active directory, serverclient operations, rack and stack, imac, breakfix, tape management, tcpip, backup and recovery software, english, customer service, communication, logical thinking, analytical thinking, record keeping, unsupervised work, time management, quality work, productivity, efficiency, bachelors degree in engineering technology science","active directory, analytical thinking, bachelors degree in engineering technology science, backup and recovery software, breakfix, communication, customer service, domains, efficiency, english, hardware, imac, infrastructure architecture, it principles, logical thinking, networking, productivity, quality work, rack and stack, record keeping, serverclient operations, tape management, tcpip, time management, unsupervised work"
Data Center Engineer - San Jose,DeRisk Technologies,"San Jose, CA",https://www.linkedin.com/jobs/view/data-center-engineer-san-jose-at-derisk-technologies-3766680835,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers, Storage, Backup, Hyper Converged Infrastructure, Tape Storage, Power Distribution Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Power Cycling, Diagnostics Commands, Whole Unit Replacement, Media Insertion/Removal, Component Replacement, Fault Diagnosis, Remote Access Configuration, Basic Storage Array Configuration, Faulty Cable Replacement, Tape Management, Equipment Rebooting, IT Ticket Management, Stakeholder Coordination, Phone Support, Remote Support, Onsite Support, Installation, Tool Management, Labeling, Patching, Asset Tagging, Task Instructions, Reporting, Network Principles, Hardware Principles, Domain Principles, Infrastructure Architecture, Server/Client Operations, Active Directory, Hardware Platforms, Equipment Installation, Troubleshooting, Racks, Stacks, IMAC, BreakFix, Defective Spare Identification, TCP/IP Standards, Backup Software, Recovery Software, English, Customer Service, Communication, Logical Thinking, Analytical Thinking, Record Keeping, Unsupervised Work, Time Management, Quality Focus, Productivity, Efficiency, Engineering, Technology, Science","servers, storage, backup, hyper converged infrastructure, tape storage, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, power cycling, diagnostics commands, whole unit replacement, media insertionremoval, component replacement, fault diagnosis, remote access configuration, basic storage array configuration, faulty cable replacement, tape management, equipment rebooting, it ticket management, stakeholder coordination, phone support, remote support, onsite support, installation, tool management, labeling, patching, asset tagging, task instructions, reporting, network principles, hardware principles, domain principles, infrastructure architecture, serverclient operations, active directory, hardware platforms, equipment installation, troubleshooting, racks, stacks, imac, breakfix, defective spare identification, tcpip standards, backup software, recovery software, english, customer service, communication, logical thinking, analytical thinking, record keeping, unsupervised work, time management, quality focus, productivity, efficiency, engineering, technology, science","access points, active directory, analytical thinking, asset tagging, backup, backup software, basic storage array configuration, breakfix, cable management, communication, component replacement, customer service, defective spare identification, diagnostics commands, domain principles, efficiency, engineering, english, equipment installation, equipment rebooting, fault diagnosis, faulty cable replacement, firewalls, hardware platforms, hardware principles, hyper converged infrastructure, imac, infrastructure architecture, installation, it ticket management, kvm units, labeling, logical thinking, media insertionremoval, network principles, network switches, onsite support, patching, phone support, physical cabling, power cycling, power distribution units, productivity, quality focus, racks, record keeping, recovery software, remote access configuration, remote support, reporting, routers, san fabric switches, science, serverclient operations, servers, stacks, stakeholder coordination, storage, tape management, tape storage, task instructions, tcpip standards, technology, time management, tool management, troubleshooting, unsupervised work, wan optimization devices, whole unit replacement"
"Senior Engineer, Data Management",Vuori,"Carlsbad, CA",https://www.linkedin.com/jobs/view/senior-engineer-data-management-at-vuori-3779277178,2023-12-17,Borden, Canada,Associate,Onsite,"Vuori is re-defining what athletic apparel looks like: built to move and sweat in but designed with a casual aesthetic to transition into everyday life. We draw inspiration from an active coastal California lifestyle; an integration of fitness, creative expression and life. Our high energy fast paced office environment is reflected in the clothes we make. We aim to inspire others to take on all aspects of their lives with clarity, enthusiasm and purpose…while having a lot of fun along the way. We are proud to be an outlet for opportunity and for personal growth and success.
Job Description
As a Senior Engineer – Data Management, you will be responsible for designing and implementing the data architecture, infrastructure, and tools to support the organization's data processing and analysis needs. You will work closely with data scientists, analysts, and other stakeholders to understand data requirements and ensure the efficient flow of data from various sources to data repositories. Your role will involve optimizing data systems and building scalable solutions to enable advanced analytics and business intelligence.
Responsibilities include but are not limited to:
Data Architecture Design:
Design and implement scalable, high-performance data pipelines and data architecture.
Collaborate with data scientists and analysts to understand data requirements and design solutions accordingly.
Ensure data architecture meets the organization's security, compliance, and performance standards.
Data Integration and Processing:
Develop and maintain ETL (Extract, Transform, Load) processes for ingesting and processing structured and unstructured data.
Integrate data from various sources, ensuring data quality and accuracy.
Optimize data processing systems for performance and reliability.
Database Management:
Manage and maintain databases, ensuring data integrity, security, and performance.
Implement data partitioning, indexing, and optimization strategies.
Troubleshoot and resolve database-related issues.
Data Technologies:
Work with data technologies such as Spark, Snowflake, Azure Data Factory etc
Implement and optimize distributed computing and storage solutions.
Support:
Enable best in class, reliable and quality driven solutions that are automated and monitored
Provide support for issues with data pipes, restart-ability and data issues from sources
Work with Analytics and Application teams to ensure end to end data integrity & reliability
Collaboration:
Collaborate with cross-functional teams, including data scientists, analysts, and software engineers.
Provide technical leadership and mentorship to junior team members.
Documentation:
Document data engineering processes, architectures, and solutions.
Create and maintain documentation for data workflows and systems.
Qualifications
Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field
Preferred knowledge of retail and wholesale in apparel or similar businesses
Proven 7+ years experience as a Data Engineer or a similar role.
Strong programming skills in languages such as Python
Experience with big data technologies (Hadoop, Spark) and cloud platforms (Azure & Snowflake)
Proficiency in SQL and database management systems (Snowflake, Synapse).
Knowledge of data modeling, data warehousing, and data integration techniques.
Familiarity with containerization and orchestration tools (e.g., ADF).
Excellent problem-solving and communication skills.
Knowledge of machine learning frameworks and analytics tools.
Certifications in relevant technologies (e.g., Snowflake, ADF, Synapse).
Additional Information
Pay Range:
From $136,000-$160,000/yr
Benefits:
Health Insurance
Paid Time Off
Employee Discount
401(k)
All your information will be kept confidential according to EEO guidelines.
Show more
Show less","Data Architecture Design, Data Integration and Processing, Database Management, Data Technologies, Data Engineering, Spark, Snowflake, Azure Data Factory, Hadoop, Python, SQL, Snowflake, Synapse, Data Modeling, Data Warehousing, Data Integration, Containerization, Orchestration Tools, ADF, Machine Learning Frameworks, Analytics Tools, Snowflake Certification, ADF Certification, Synapse Certification","data architecture design, data integration and processing, database management, data technologies, data engineering, spark, snowflake, azure data factory, hadoop, python, sql, snowflake, synapse, data modeling, data warehousing, data integration, containerization, orchestration tools, adf, machine learning frameworks, analytics tools, snowflake certification, adf certification, synapse certification","adf, adf certification, analytics tools, azure data factory, containerization, data architecture design, data engineering, data integration, data integration and processing, data technologies, database management, datamodeling, datawarehouse, hadoop, machine learning frameworks, orchestration tools, python, snowflake, snowflake certification, spark, sql, synapse, synapse certification"
Data Center Engineer - Denver,DeRisk Technologies,"Denver, CO",https://www.linkedin.com/jobs/view/data-center-engineer-denver-at-derisk-technologies-3766680388,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers, Storage, Backup Devices, Server Appliances, Hyper Converged Infrastructure, Tape Storage Units, Power Distribution Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Power Cycling, Diagnostics Commands, Whole Unit Replacement, Media Insertion/Removal, Defective Component Replacement, Fault Diagnosis and Investigation, Remote Access Configuration, Basic Storage Array Configuration, Faulty Cable Replacement, Tape Management and Maintenance, Router Server Storage Device and Equipment Rebooting, IT Ticket Management System, Stakeholder Attendance Coordination, Phone Remote Tools and Onsite Support, Physical and Network Medium Installation, Arrival and Departure Time Coordination, Infrastructure Environment Support Tools, Labelling Patching and Asset Tagging, Task Instruction Following, Reporting, Networks, Hardware, Domains, Infrastructure (Data Center and Network) Hardware Architecture, Server/Client Operations, Active Directory, Current and Legacy Hardware Infrastructure Platforms, Infrastructure (DC & Network) Equipment Installation and Troubleshooting, Rack and Stack of Equipment/Cable, IMAC and Breakfix Activities, Defective Spare Identification and Replacement, TCP/IP Standards, Networking, Tape Management, Server Infrastructure Management Control and Monitoring, Backup and Recovery Software and Methodologies, English, Exceptional Customer Facing Skills, Clear and Effective Communication, Logical and Analytical Approach, Accurate Record Keeping, Unsupervised Work Ability, Good Timekeeping, Intense Focus on Quality Work, Productive and Efficient, Bachelor of Engineering / Technology / Science or Equivalent, 5  7 Years Work Experience","servers, storage, backup devices, server appliances, hyper converged infrastructure, tape storage units, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, power cycling, diagnostics commands, whole unit replacement, media insertionremoval, defective component replacement, fault diagnosis and investigation, remote access configuration, basic storage array configuration, faulty cable replacement, tape management and maintenance, router server storage device and equipment rebooting, it ticket management system, stakeholder attendance coordination, phone remote tools and onsite support, physical and network medium installation, arrival and departure time coordination, infrastructure environment support tools, labelling patching and asset tagging, task instruction following, reporting, networks, hardware, domains, infrastructure data center and network hardware architecture, serverclient operations, active directory, current and legacy hardware infrastructure platforms, infrastructure dc network equipment installation and troubleshooting, rack and stack of equipmentcable, imac and breakfix activities, defective spare identification and replacement, tcpip standards, networking, tape management, server infrastructure management control and monitoring, backup and recovery software and methodologies, english, exceptional customer facing skills, clear and effective communication, logical and analytical approach, accurate record keeping, unsupervised work ability, good timekeeping, intense focus on quality work, productive and efficient, bachelor of engineering technology science or equivalent, 5 7 years work experience","5 7 years work experience, access points, accurate record keeping, active directory, arrival and departure time coordination, bachelor of engineering technology science or equivalent, backup and recovery software and methodologies, backup devices, basic storage array configuration, cable management, clear and effective communication, current and legacy hardware infrastructure platforms, defective component replacement, defective spare identification and replacement, diagnostics commands, domains, english, exceptional customer facing skills, fault diagnosis and investigation, faulty cable replacement, firewalls, good timekeeping, hardware, hyper converged infrastructure, imac and breakfix activities, infrastructure data center and network hardware architecture, infrastructure dc network equipment installation and troubleshooting, infrastructure environment support tools, intense focus on quality work, it ticket management system, kvm units, labelling patching and asset tagging, logical and analytical approach, media insertionremoval, network switches, networking, networks, phone remote tools and onsite support, physical and network medium installation, physical cabling, power cycling, power distribution units, productive and efficient, rack and stack of equipmentcable, remote access configuration, reporting, router server storage device and equipment rebooting, routers, san fabric switches, server appliances, server infrastructure management control and monitoring, serverclient operations, servers, stakeholder attendance coordination, storage, tape management, tape management and maintenance, tape storage units, task instruction following, tcpip standards, unsupervised work ability, wan optimization devices, whole unit replacement"
Data Center Engineer - Houston,DeRisk Technologies,"Houston, TX",https://www.linkedin.com/jobs/view/data-center-engineer-houston-at-derisk-technologies-3766680531,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Server management, Storage management, Hyperconverged infrastructure, Network switches, KVM units, WAN optimization, Firewalls, Access points, Routers, Cabling and cable management, Breakfix and technical tasks, Fault diagnosis and investigation, Remote access configuration, Storage array configuration, Tape management and maintenance, Operations tasks, IT ticket management, Stakeholder coordination, Onsite and remote support, Installation and troubleshooting, Asset tagging and labeling, Reporting, Server/client operations, Networking, Hardware architecture, Active Directory, TCP/IP standards, Tape management, Infrastructure management, Backup and recovery, Customer service, Communication skills, Problemsolving skills, Recordkeeping skills, Time management skills, Attention to detail, English language skills","server management, storage management, hyperconverged infrastructure, network switches, kvm units, wan optimization, firewalls, access points, routers, cabling and cable management, breakfix and technical tasks, fault diagnosis and investigation, remote access configuration, storage array configuration, tape management and maintenance, operations tasks, it ticket management, stakeholder coordination, onsite and remote support, installation and troubleshooting, asset tagging and labeling, reporting, serverclient operations, networking, hardware architecture, active directory, tcpip standards, tape management, infrastructure management, backup and recovery, customer service, communication skills, problemsolving skills, recordkeeping skills, time management skills, attention to detail, english language skills","access points, active directory, asset tagging and labeling, attention to detail, backup and recovery, breakfix and technical tasks, cabling and cable management, communication skills, customer service, english language skills, fault diagnosis and investigation, firewalls, hardware architecture, hyperconverged infrastructure, infrastructure management, installation and troubleshooting, it ticket management, kvm units, network switches, networking, onsite and remote support, operations tasks, problemsolving skills, recordkeeping skills, remote access configuration, reporting, routers, server management, serverclient operations, stakeholder coordination, storage array configuration, storage management, tape management, tape management and maintenance, tcpip standards, time management skills, wan optimization"
Data Center Engineer - Los Angeles,DeRisk Technologies,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-center-engineer-los-angeles-at-derisk-technologies-3766686017,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Nutanix, Cisco UCS, Active Directory, TCP/IP, Infrastructure architecture, Server/client operations, IMAC, Breakfix activities, Tape Management, Backup and recovery software, English, Analytical approach, Record keeping, Timekeeper, Quality work, Engineering, Technology, Science","nutanix, cisco ucs, active directory, tcpip, infrastructure architecture, serverclient operations, imac, breakfix activities, tape management, backup and recovery software, english, analytical approach, record keeping, timekeeper, quality work, engineering, technology, science","active directory, analytical approach, backup and recovery software, breakfix activities, cisco ucs, engineering, english, imac, infrastructure architecture, nutanix, quality work, record keeping, science, serverclient operations, tape management, tcpip, technology, timekeeper"
Data Center Engineer - Atlanta,DeRisk Technologies,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-center-engineer-atlanta-at-derisk-technologies-3766681049,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Nutanix, Cisco UCS, Active Directory, IMAC, TCP/IP, Tape Management","nutanix, cisco ucs, active directory, imac, tcpip, tape management","active directory, cisco ucs, imac, nutanix, tape management, tcpip"
Data Center Engineer - Tallahassee,DeRisk Technologies,"Tallahassee, FL",https://www.linkedin.com/jobs/view/data-center-engineer-tallahassee-at-derisk-technologies-3766681772,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","IT principles, Networks, Hardware, Domains, Infrastructure architecture, Active Directory, Server/client operations, Hardware Infrastructure platforms, Cable management, IMAC, Breakfix activities, Troubleshooting, TCP/IP standards, Networking, Tape management, Backup and recovery software, English, Customer service skills, Communication skills, Logical thinking, Analytical skills, Record keeping, Time management, Quality control, Productivity, Efficiency, Engineering, Technology, Science","it principles, networks, hardware, domains, infrastructure architecture, active directory, serverclient operations, hardware infrastructure platforms, cable management, imac, breakfix activities, troubleshooting, tcpip standards, networking, tape management, backup and recovery software, english, customer service skills, communication skills, logical thinking, analytical skills, record keeping, time management, quality control, productivity, efficiency, engineering, technology, science","active directory, analytical skills, backup and recovery software, breakfix activities, cable management, communication skills, customer service skills, domains, efficiency, engineering, english, hardware, hardware infrastructure platforms, imac, infrastructure architecture, it principles, logical thinking, networking, networks, productivity, quality control, record keeping, science, serverclient operations, tape management, tcpip standards, technology, time management, troubleshooting"
Data Center Engineer - Richmond,DeRisk Technologies,"Richmond, VA",https://www.linkedin.com/jobs/view/data-center-engineer-richmond-at-derisk-technologies-3766679940,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","IT principles, Networks, Hardware, Domains, Infrastructure (Data Center and Network), Server/client operations, Active Directory, Server/client operations, Installation, Troubleshooting, IMAC, Breakfix, TCP/IP, Networking, Tape Management, Server infrastructure management, Backup and recovery software, English, Exceptional customer facing skills, Communication, Logical thinking, Analytical approach, Accurate record keeping, Unsupervised work, Time management, Focus on quality work, Productivity, Efficiency, Bachelor of Engineering / Technology / Science, 57 years of experience","it principles, networks, hardware, domains, infrastructure data center and network, serverclient operations, active directory, serverclient operations, installation, troubleshooting, imac, breakfix, tcpip, networking, tape management, server infrastructure management, backup and recovery software, english, exceptional customer facing skills, communication, logical thinking, analytical approach, accurate record keeping, unsupervised work, time management, focus on quality work, productivity, efficiency, bachelor of engineering technology science, 57 years of experience","57 years of experience, accurate record keeping, active directory, analytical approach, bachelor of engineering technology science, backup and recovery software, breakfix, communication, domains, efficiency, english, exceptional customer facing skills, focus on quality work, hardware, imac, infrastructure data center and network, installation, it principles, logical thinking, networking, networks, productivity, server infrastructure management, serverclient operations, tape management, tcpip, time management, troubleshooting, unsupervised work"
Data Center Engineer - Newark,DeRisk Technologies,"Newark, NJ",https://www.linkedin.com/jobs/view/data-center-engineer-newark-at-derisk-technologies-3766679823,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Networking, Hardware, Domains, Rack and Stack, Data Center, Network, Active Directory, Server Appliances, Virtual Machines, Power Distribution Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Power Cycling, Diagnostics Commands, Whole Unit Replacement, Media Insertion/Removal, Defective Component Replacement, Fault Diagnosis and Investigation, Remote Access Configuration, Basic Storage Array Configuration, Faulty Cable Replacement, Tape Management and Maintenance, Router Server and Storage Device Rebooting, IT Ticket Management System, Stakeholder Coordination, Phone Remote Tool and Onsite Support, Installation, Physical and Network Medium, Activity Date and Time Coordination, Tool and Laptop Carrying, Labelling, Patching, Asset Tagging, Task Instruction Following, Reporting, Networks, Hardware, Domains, Data Center, Network, Active Directory, Server Appliances, Virtual Machines, Power Distribution Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Power Cycling, Diagnostics Commands, Whole Unit Replacement, Media Insertion/Removal, Defective Component Replacement, Fault Diagnosis and Investigation, Remote Access Configuration, Basic Storage Array Configuration, Faulty Cable Replacement, Tape Management and Maintenance, Router Server and Storage Device Rebooting, IT Ticket Management System, Stakeholder Coordination, Phone Remote Tool and Onsite Support, Installation, Physical and Network Medium, Activity Date and Time Coordination, Tool and Laptop Carrying, Labelling, Patching, Asset Tagging, Task Instruction Following, Reporting","networking, hardware, domains, rack and stack, data center, network, active directory, server appliances, virtual machines, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, power cycling, diagnostics commands, whole unit replacement, media insertionremoval, defective component replacement, fault diagnosis and investigation, remote access configuration, basic storage array configuration, faulty cable replacement, tape management and maintenance, router server and storage device rebooting, it ticket management system, stakeholder coordination, phone remote tool and onsite support, installation, physical and network medium, activity date and time coordination, tool and laptop carrying, labelling, patching, asset tagging, task instruction following, reporting, networks, hardware, domains, data center, network, active directory, server appliances, virtual machines, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, power cycling, diagnostics commands, whole unit replacement, media insertionremoval, defective component replacement, fault diagnosis and investigation, remote access configuration, basic storage array configuration, faulty cable replacement, tape management and maintenance, router server and storage device rebooting, it ticket management system, stakeholder coordination, phone remote tool and onsite support, installation, physical and network medium, activity date and time coordination, tool and laptop carrying, labelling, patching, asset tagging, task instruction following, reporting","access points, active directory, activity date and time coordination, asset tagging, basic storage array configuration, cable management, data center, defective component replacement, diagnostics commands, domains, fault diagnosis and investigation, faulty cable replacement, firewalls, hardware, installation, it ticket management system, kvm units, labelling, media insertionremoval, network, network switches, networking, networks, patching, phone remote tool and onsite support, physical and network medium, physical cabling, power cycling, power distribution units, rack and stack, remote access configuration, reporting, router server and storage device rebooting, routers, san fabric switches, server appliances, stakeholder coordination, tape management and maintenance, task instruction following, tool and laptop carrying, virtual machines, wan optimization devices, whole unit replacement"
Data Engineer,iTech Solutions,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineer-at-itech-solutions-3714393746,2023-12-17,Borden, Canada,Associate,Onsite,"Position - Data Engineer
Location - San Francisco, CA
Duration : Long Term
Job Description:
Must Have :
·         Scala. experience
·         Spark, Must have worked in Hadoop Environment.
·         Experience with Hive would be helpful.
Desired:
·         Retail Industry Experience.
·         Any experience with GCP (Google Cloud Platform) would be a BIG PLUS
Certifications:
·         None required
Show more
Show less","Scala, Spark, Hadoop, Hive, GCP","scala, spark, hadoop, hive, gcp","gcp, hadoop, hive, scala, spark"
Data Engineer,VeeAR Projects Inc.,"Mountain View, CA",https://www.linkedin.com/jobs/view/data-engineer-at-veear-projects-inc-3774026278,2023-12-17,Borden, Canada,Associate,Onsite,"Job Description:
Full Stack Engineer capable of designing solutions and writing and testing code.
Able to research and learn new methodologies and technologies and bring knowledge to the team
Preferred Qualifications:
5+ years of python development experience
3+ years of experience with Angular.js
SQL Experience/knowledge good to have.
Show more
Show less","Python, Angular.js, SQL","python, angularjs, sql","angularjs, python, sql"
Data Center Engineer - Philadelphia,DeRisk Technologies,"Philadelphia, PA",https://www.linkedin.com/jobs/view/data-center-engineer-philadelphia-at-derisk-technologies-3766685495,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","IT principles, Network protocols, Serverclient operations, Active Directory, Infrastructure hardware platforms, Troubleshooting, IMAC, Breakfix activities, TCP/IP standards, Backup and recovery software, Tape Management, English, Customerfacing skills, Communication skills, Logical and analytical approach, Recordkeeping, Unsupervised work, Time management, Quality focus, Productivity, Efficiency","it principles, network protocols, serverclient operations, active directory, infrastructure hardware platforms, troubleshooting, imac, breakfix activities, tcpip standards, backup and recovery software, tape management, english, customerfacing skills, communication skills, logical and analytical approach, recordkeeping, unsupervised work, time management, quality focus, productivity, efficiency","active directory, backup and recovery software, breakfix activities, communication skills, customerfacing skills, efficiency, english, imac, infrastructure hardware platforms, it principles, logical and analytical approach, network protocols, productivity, quality focus, recordkeeping, serverclient operations, tape management, tcpip standards, time management, troubleshooting, unsupervised work"
Data Center Engineer - Ashburn,DeRisk Technologies,"Ashburn, VA",https://www.linkedin.com/jobs/view/data-center-engineer-ashburn-at-derisk-technologies-3766677771,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Server Configuration, Storage and Backup, Hyper Converged Infrastructure, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization, Firewalls, Access Points, Routers, Cable Management, Power Cycling, Diagnostics, Unit Replacement, Media Management, Remote Access, Storage Array Configuration, Active Directory, Hardware Infrastructure, Infrastructure Troubleshooting, IMAC, Breakfix, TCP/IP Networking, Tape Management, Server Infrastructure Management, Backup and Recovery, English, Customer Service Skills, Communication Skills, Analytical Thinking, Record Keeping, Unsupervised Work, Time Management, Quality Focus, Productivity, Bachelor's Degree in Engineering Technology or Science","server configuration, storage and backup, hyper converged infrastructure, san fabric switches, network switches, kvm units, wan optimization, firewalls, access points, routers, cable management, power cycling, diagnostics, unit replacement, media management, remote access, storage array configuration, active directory, hardware infrastructure, infrastructure troubleshooting, imac, breakfix, tcpip networking, tape management, server infrastructure management, backup and recovery, english, customer service skills, communication skills, analytical thinking, record keeping, unsupervised work, time management, quality focus, productivity, bachelors degree in engineering technology or science","access points, active directory, analytical thinking, bachelors degree in engineering technology or science, backup and recovery, breakfix, cable management, communication skills, customer service skills, diagnostics, english, firewalls, hardware infrastructure, hyper converged infrastructure, imac, infrastructure troubleshooting, kvm units, media management, network switches, power cycling, productivity, quality focus, record keeping, remote access, routers, san fabric switches, server configuration, server infrastructure management, storage and backup, storage array configuration, tape management, tcpip networking, time management, unit replacement, unsupervised work, wan optimization"
"Data Analyst-SQL, Python, Visualization tool",Zortech Solutions,"Mountain View, CA",https://www.linkedin.com/jobs/view/data-analyst-sql-python-visualization-tool-at-zortech-solutions-3779230363,2023-12-17,Borden, Canada,Associate,Onsite,"Role: Data Analyst-SQL, Python, Visualization tool
Location: Mountain View CA (day one onsite)
Duration: 6+ Months
Job Description
Must have:
Strong SQL
Python
Any Visualization Tool – Tableau / Qlik sense / Power BI
Qualifications
5+ years of experience in generating and sharing insights from in-product/CRM/billing-system data.
Strong SQL with ability to handle large volumes of data.
Experience with one or more scripting languages (Python preferred, R, Scala, etc. or shell scripting).
Experience with one or more data visualization tools such as Tableau, Qlik sense, Alteryx, etc.
Fluent with Google suite (spreadsheets, documents, and slides).
Take responsibility and ownership like no other.
Entrepreneurial and curious spirit - passionate about business and data.
Able to work with technical and business stakeholders and comfortable working with a global cross-functional team.
Detail-oriented with superior project management and organizational skills. Able to navigate between multiple concurrent threads and meet deadlines.
Strong communication skills with the ability to interact with business leaders to make business recommendations.
Master’s degree in mathematics, statistics, engineering or related field (preferred).
Responsibilities
Build data/analytics solutions with ‘Data as a product’ mindset leveraging data across all business groups within Intuit.
Build deep subject matter expertise to be able to voice the nuances of how customer experience translates from our data.
Define key business indicators and success metrics/KPIs and develop ways to track them automatically across time.
Provide guidance to Business and Tech leaders and stakeholders on how best to harness available data, extract meaning, reconcile assumptions, and identify logical path for action.
Collaborate with business stakeholders as well as with product development and data engineering teams to ensure we capture the right data to drive insights.
Proactively uncover product data gaps that will impact customers as well as lead and manage in-product fixes across product development teams to resolve these.
Leverage knowledge of data analytics principles and communication skills to develop a data driven business case needed for proposal sign off from sponsors
Ability to dissect opportunity size and determine financial benefit as well as to map out effort/ROI for developing the solution
Uses considerable expertise and independent judgment in collaborating with peers, product managers, developers, and data engineers, in designing and implementing the research strategy needed to methodically and iteratively structure, extract, cleanse, sample, test, validate, and communicate data-driven insights from complex sources and significant volumes of data.
Be the point of contact for investigative analytics questions supporting our business stakeholders as well as MSE PMs
Show more
Show less","SQL, Python, Tableau, Qlik, Power BI, R, Scala, Alteryx, Shell scripting, Google Suite, Data visualization, Data analysis, Data mining, Data warehousing, Business intelligence, Machine learning, Data Integration, Data Engineering, Data Extraction, Data Cleansing, Data Sampling, Testing, Validation, Multivariate Analysis","sql, python, tableau, qlik, power bi, r, scala, alteryx, shell scripting, google suite, data visualization, data analysis, data mining, data warehousing, business intelligence, machine learning, data integration, data engineering, data extraction, data cleansing, data sampling, testing, validation, multivariate analysis","alteryx, business intelligence, data engineering, data extraction, data integration, data mining, data sampling, dataanalytics, datacleaning, datawarehouse, google suite, machine learning, multivariate analysis, powerbi, python, qlik, r, scala, shell scripting, sql, tableau, testing, validation, visualization"
Data Center Engineer - Columbus,DeRisk Technologies,"Columbus, OH",https://www.linkedin.com/jobs/view/data-center-engineer-columbus-at-derisk-technologies-3759951573,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers, Storage, Backup, Hyperconverged Infrastructure, Tape Storage, Power Distribution Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Power Cycling, Diagnostics Commands, Unit Replacement, Media Insertion/Removal, Component Replacement, Fault Diagnosis, Remote Access Configuration, Storage Array Configuration, Cable Replacement, Tape Management, Router/Server/Storage Rebooting, IT Ticket Management, Stakeholder Coordination, Phone/Remote/Onsite Support, Installation, Tool Carrying, Labelling, Patching, Asset Tagging, Task Instruction Following, Reporting, Networking, Hardware, Domains, Active Directory, Infrastructure Platforms, Installation/Troubleshooting, Rack and Stack, IMAC, Breakfix, TCP/IP, Tape Management, Server Infrastructure Management, Backup/Recovery Software, English, Customer Service, Communication, Logical Thinking, Analytical Thinking, Record Keeping, Unsupervised Work, Time Management, Quality Focus, Productivity, Efficiency, Engineering, Technology, Science","servers, storage, backup, hyperconverged infrastructure, tape storage, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, power cycling, diagnostics commands, unit replacement, media insertionremoval, component replacement, fault diagnosis, remote access configuration, storage array configuration, cable replacement, tape management, routerserverstorage rebooting, it ticket management, stakeholder coordination, phoneremoteonsite support, installation, tool carrying, labelling, patching, asset tagging, task instruction following, reporting, networking, hardware, domains, active directory, infrastructure platforms, installationtroubleshooting, rack and stack, imac, breakfix, tcpip, tape management, server infrastructure management, backuprecovery software, english, customer service, communication, logical thinking, analytical thinking, record keeping, unsupervised work, time management, quality focus, productivity, efficiency, engineering, technology, science","access points, active directory, analytical thinking, asset tagging, backup, backuprecovery software, breakfix, cable management, cable replacement, communication, component replacement, customer service, diagnostics commands, domains, efficiency, engineering, english, fault diagnosis, firewalls, hardware, hyperconverged infrastructure, imac, infrastructure platforms, installation, installationtroubleshooting, it ticket management, kvm units, labelling, logical thinking, media insertionremoval, network switches, networking, patching, phoneremoteonsite support, physical cabling, power cycling, power distribution units, productivity, quality focus, rack and stack, record keeping, remote access configuration, reporting, routers, routerserverstorage rebooting, san fabric switches, science, server infrastructure management, servers, stakeholder coordination, storage, storage array configuration, tape management, tape storage, task instruction following, tcpip, technology, time management, tool carrying, unit replacement, unsupervised work, wan optimization devices"
Data Center Engineer,Cloudflare,"Kansas City, MO",https://www.linkedin.com/jobs/view/data-center-engineer-at-cloudflare-3732383583,2023-12-17,Borden, Canada,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
Data Center Operations Engineer
About the department
In this role, you will be focused on maintaining the Clou dflare global network. You 'll work closely with Cloudflare’s SRE (Site Reliability Engineering) team, Network Engineering team, Network Deployment Engineering team and with various vendors and partners (including hardware vendors, datacenter and network providers, and ISPs) to maintain and improve our global infrastructure. You will further be responsible for the development and implementation of consistent processes and visibility measurements for consistent and effective management of our infrastructure. This is a highly visible position that requires deep technical understanding of datacenter infrastructure, networking (physical), and basic experience with data analysis and project management.
To be successful in this position, you should have excellent technical skills, communication skills, and be able to navigate a range of challenges and constraints (e.g. schedule adherence, time zones, and cultures). You will have the opportunity to (literally) build a faster, safer Internet for our millions of users and the billions of web surfers that visit their sites each month.
Who You Are
You will thrive in a hypergrowth engineering environment and be self driven with a keen attention to detail. You will come with a deep technical understanding of Data Center colocation environments, network architecture and server technologies. You will be used to working through partners to support infrastructure delivery to a number of remote locations. You will have had experience managing operational environments, and used to developing new approaches to improve delivery efficiency or operational stability.
What You'll Do
Collaborating with internal teams (Infrastructure, Network Engineering and SRE). Create documentation and manage remote contractors to complete datacenter tasks, working with hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 300+ datacenter locations
Maintain Data Center environment operational availability
Creating and maintaining documentation, plans, SOP’s, MOP’s etc.
Support and configure network infrastructure where required
Providing feedback to internal teams to support internal tools and external vendor partnerships
Required Experience
Minimum of 5 yrs of Linux systems administration
Experience with Juniper, Cisco and DWDM network equipment
Experience managing and instructing remote contractors
Familiarity with work required to stand up infrastructure in remote colocation facilities
Experience running and improving operational processes, including automation tooling, in a rapidly changing environment
Familiarity with day-to-day tasks and projects common to Data Center Operations (deployment, migration, decommissioning etc.)
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Incident management
Other Responsibilities May Include
Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure
Assist with the definition, documentation and implementation of consistent processes across all region
Limited travel
Examples Of Desirable Skills, Knowledge And Experience
Bachelor’s degree; technical background in engineering, computer science, or MIS
Direct experience executing on complex data center/infrastructure projects
Previous experience installing / maintaining data center (and other IT) infrastructure and DCIM tools
Experience running and improving operational processes in a rapidly changing environment
Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills
Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously
Ability to manage MS excel and Google spreadsheets
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Must be a team player
Bonus Points
Multi-lingual; experience working with infrastructure in multiple countries
Comfortable with remote “lights-out” and out-of-band access to data center resources
Linux certifications (RHCSA etc.)
Network certifications (CCNA, JNCIA or higher)
Compensation
Compensation may be adjusted depending on work location.
For Colorado-based hires: Estimated annual salary of $ 111,000 - $ 135,000 .
For New York City, Washington, and California (excluding Bay Area) based hires: Estimated annual salary of $ 135,000 - $ 165,000
For Bay Area-based hires: Estimated annual salary of $ 142,000 - $ 174,000 .
Equity
This role is eligible to participate in Cloudflare’s equity plan.
Benefits
Cloudflare offers a complete package of benefits and programs to support you and your family. Our benefits programs can help you pay health care expenses, support caregiving, build capital for the future and make life a little easier and fun! The below is a description of our benefits for employees in the United States, and benefits may vary for employees based outside the U.S.
Health & Welfare Benefits
Medical/Rx Insurance
Dental Insurance
Vision Insurance
Flexible Spending Accounts
Commuter Spending Accounts
Fertility & Family Forming Benefits
On-demand mental health support and Employee Assistance Program
Global Travel Medical Insurance
Financial Benefits
Short and Long Term Disability Insurance
Life & Accident Insurance
401(k) Retirement Savings Plan
Employee Stock Participation Plan
Time Off
Flexible paid time off covering vacation and sick leave
Leave programs, including parental, pregnancy health, medical, and bereavement leave
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Linux, Juniper, Cisco, DWDM, Data Center Operations, Network architecture, Remote contractors, Data Center colocation, Automation tooling, MS Excel, Google Spreadsheets, Project management, Incident management, RHCSA, CCNA, JNCIA","linux, juniper, cisco, dwdm, data center operations, network architecture, remote contractors, data center colocation, automation tooling, ms excel, google spreadsheets, project management, incident management, rhcsa, ccna, jncia","automation tooling, ccna, cisco, data center colocation, data center operations, dwdm, google spreadsheets, incident management, jncia, juniper, linux, ms excel, network architecture, project management, remote contractors, rhcsa"
Sr. Data Engineer,Abbott,"Lake Forest, IL",https://www.linkedin.com/jobs/view/sr-data-engineer-at-abbott-3697563814,2023-12-17,Borden, Canada,Associate,Onsite,"Abbott is a global healthcare leader that helps people live more fully at all stages of life. Our portfolio of life-changing technologies spans the spectrum of healthcare, with leading businesses and products in diagnostics, medical devices, nutritionals and branded generic medicines. Our 115,000 colleagues serve people in more than 160 countries.
Working at Abbott
At Abbott, You Can Do Work That Matters, Grow, And Learn, Care For Yourself And Family, Be Your True Self And Live a Full Life. You’ll Also Have Access To
Career development with an international company where you can grow the career you dream of.
Free medical coverage for employees* via the Health Investment Plan (HIP) PPO
An excellent retirement savings plan with high employer contribution
Tuition reimbursement, the Freedom 2 Save student debt program and FreeU education benefit - an affordable and convenient path to getting a bachelor’s degree.
A company recognized as a great place to work in dozens of countries around the world and named one of the most admired companies in the world by Fortune.
A company that is recognized as one of the best big companies to work for as well as a best place to work for diversity, working mothers, female executives, and scientists.
The Opportunity
This position will work out of one of our two offices in the office of either site:
Lake Forest J55 in IL or St. Paul in MN
within the BI & DA organization.
The
Sr. Data Engineer
is responsible for designing, building, and maintaining pipelines and reusable components to support reporting and analytics data products. This position will be responsible for partnering with team members to implement the best technical solution with performance, governance, scalability, security, and maintainability in mind. The person hired in this role will also have the opportunity to participate in solution architecture with senior IT staff.
What You’ll Work On
If you enjoy organizing raw data, then this is a great job for you! The data that this team sees and organize in data bricks will then go to multiple groups in the company. This team has high exposure to projects companywide and worldwide at Abbott. If making a difference with data extraction and loading the data using Azure Cloud is your “superpower”, then please apply!
What Your Responsibilities Would Be If Hired
Create and maintain an optimal data pipeline architecture by assisting with the designing and implementation of data ingestion solutions on Azure using DataBricks and/or Datafactory.
Writing complex queries to transform raw data sources into accessible models.
Clean, prepare, transform, and optimize data at scale.
Assist with designing and optimizing data models on Azure cloud using Azure Analysis Services.
Read, extract, transform, stage and load data to selected tools and frameworks as required and requested.
Ensure your work remains backed-up and readily accessible to relevant co-workers using GIT or Azure Cloud for Doc Control or (other programs the team uses for this purpose).
Providing system support to end users and administrators to resolve business and technical problems. Including possible rotation on call on a third tier level on occasion at most.
Using/improving existing standards, methodologies, and processes and understanding other systems/business processes related to each other. In addition, you will understand SDLC in Waterfall or Agile methodologies in your current or past roles.
Working with CI/CD and version control tools such as GIT.
You will have knowledge of working with healthcare data for HIPPA Privacy and International Data Privacy Agreement Laws.
Competencies
Strong problem-solving skills, attention to detail and organization / documentation skills
Ability to prioritize and triage deadline-driven tasks in a high-pressure environment.
Required
Bachelor’s degree (± 16 years) in any of the following – Math, Physics, Computer Science, Statistics, Economics, Quantitative Sciences.
Minimum 7 years of experience in IT as a Data Engineer
At least one year of experience with developing ETL pipelines in one or more of the following tools: Azure Data Factory, Azure functions, Data Flow, Event hubs, Event grids, Informatica
At least one year of experience with Databricks and/or Spark
At least two years of experience with SQL and data modeling
At least two years of experience with Python and some ETL libraries like Pandas.
Preferred
Degree in Data Science
Experience with CosmoDB, AzureSQL, Synapse
Experience with SCALA
Participants who complete a short wellness assessment qualify for FREE coverage in our HIP PPO medical plan. Free coverage applies in the next calendar year.
Learn more about our health and wellness benefits, which provide the security to help you and your family live full lives:
www.abbottbenefits.com
Follow your career aspirations to Abbott for diverse opportunities with a company that can help you build your future and live your best life. Abbott is an Equal Opportunity Employer, committed to employee diversity.
Connect with us at www.abbott.com, on Facebook at www.facebook.com/Abbott and on Twitter @AbbottNews and @AbbottGlobal.
The base pay for this position is $80,700.00 – $161,300.00. In specific locations, the pay range may vary from the range posted.
Show more
Show less","Azure Synapse, Azure Data Factory, Azure Functions, Azure Data Lake Analytics, Azure SQL Database, Azure Cosmos DB, Spark, Databricks, SQL, Apache Spark, ETL, Python, Pandas, GIT, CI/CD, Docker, Scala, Data modeling, Data pipelines, Data ingestion, Data transformation, Data optimization, Data governance, Data security, Data privacy, Data architecture, Solution architecture, Cloud computing, Big data, Machine learning, Artificial intelligence, Healthcare data, HIPPA, International Data Privacy Agreement Laws","azure synapse, azure data factory, azure functions, azure data lake analytics, azure sql database, azure cosmos db, spark, databricks, sql, apache spark, etl, python, pandas, git, cicd, docker, scala, data modeling, data pipelines, data ingestion, data transformation, data optimization, data governance, data security, data privacy, data architecture, solution architecture, cloud computing, big data, machine learning, artificial intelligence, healthcare data, hippa, international data privacy agreement laws","apache spark, artificial intelligence, azure cosmos db, azure data factory, azure data lake analytics, azure functions, azure sql database, azure synapse, big data, cicd, cloud computing, data architecture, data governance, data ingestion, data optimization, data privacy, data security, data transformation, databricks, datamodeling, datapipeline, docker, etl, git, healthcare data, hippa, international data privacy agreement laws, machine learning, pandas, python, scala, solution architecture, spark, sql"
Data Center Engineer - Montgomery,DeRisk Technologies,"Montgomery, AL",https://www.linkedin.com/jobs/view/data-center-engineer-montgomery-at-derisk-technologies-3766686038,2023-12-17,Borden, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Networking, Hardware, Domains, Data center, Server, Client operations, Active Directory, Infrastructure hardware platforms, Installation, Troubleshooting, Rack and Stack, IMAC, Breakfix, TCP/IP, Backup, Recovery, English, Customer service, Communication, Logical thinking, Analytical thinking, Record keeping, Unsupervised work, Time management, Quality assurance, Productivity, Efficiency","networking, hardware, domains, data center, server, client operations, active directory, infrastructure hardware platforms, installation, troubleshooting, rack and stack, imac, breakfix, tcpip, backup, recovery, english, customer service, communication, logical thinking, analytical thinking, record keeping, unsupervised work, time management, quality assurance, productivity, efficiency","active directory, analytical thinking, backup, breakfix, client operations, communication, customer service, data center, domains, efficiency, english, hardware, imac, infrastructure hardware platforms, installation, logical thinking, networking, productivity, quality assurance, rack and stack, record keeping, recovery, server, tcpip, time management, troubleshooting, unsupervised work"
Senior Data Engineer,"The Computer Merchant, LTD (TCM)","Waltham, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-the-computer-merchant-ltd-tcm-3768058588,2023-12-17,Borden, Canada,Associate,Onsite,"JOB TITLE: Senior Data Engineer
JOB LOCATION: Waltham, MA Hybrid
WAGE RANGE*: 75-85/hr.
JOB NUMBER: NGGJP00013916
Required Experience
Strong Experience in Python, PySpark and SQL
Strong Experience in Databricks
Experience with cloud-based databases, specifically Azure technologies (e.g., Azure data lake, ADF, Azure DevOps and Azure Functions)
Experience using SQL queries as well as writing and perfecting SQL queries in a business environment with large-scale, complex datasets
Experience with data warehouse technologies. Experience creating ETL and/or ELT jobs
Experience in Kafka, Flink ,Fivetran and Matillion is nice to have
Job Description
As a Senior Data Engineer, you will be part of a cross-functional development team that is focused on creating a forecasting platform at Client available to our business teams. Using the agile framework, you will build end-to-end pipelines based on rigorous engineering standards and coding practices to deliver data that is accessible and of the highest quality. A Senior Data Engineer will also contribute to the modernization of our architecture and tools to help increase our output, scalability, and speed.
What you'll do
Design and develop highly scalable and extensible data pipelines which enable collection, storage, distribution, modeling, and analysis of large data sets from many channels.
Function as an innovative software engineer who is passionate about data & data quality
The ideal candidate will have strong data warehousing and API integration experience and the ability to develop scalable data pipelines that make data management and analytics/reporting faster, more insightful, and more efficient
Establish and follow data governance processes and guidelines to ensure data availability, usability, consistency, integrity, and security
Collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making across the organization
Design, implement, and automate deployment of our distributed system for collecting and processing streaming events from multiple sources
Guide and mentor junior engineers on coding best practices and optimization
What you'll need
Education: 4-year college degree or equivalent combination of education and experience. Prefer an academic background in Computer Science, Mathematics, Statistics, or related technical field
5+ years of relevant work experience in analytics, data engineering, business intelligence or related field
Skilled in object-oriented programming (Python in particular)
Strong Experience in Python, PySpark and SQL
Strong Experience in Databricks
Experience with cloud-based databases, specifically Azure technologies (e.g., Azure data lake, ADF, Azure DevOps and Azure Functions)
Experience using SQL queries as well as writing and perfecting SQL queries in a business environment with large-scale, complex datasets
Experience with data warehouse technologies. Experience creating ETL and/or ELT jobs
Experience in Kafka, Flink ,Fivetran and Matillion is nice to have
More Information
Our organization follows a hybrid work structure in our service territory (NY & MA and adjacent states) where employees can work remotely or from the office, as needed. Working from the office is encouraged when working on tasks that require a high degree of collaboration. We work with our employees to foster a work schedule that fits your flexible schedule.
Equal Opportunity Employer Veterans/Disabled
While an hourly range is posted for this position, an eventual hourly rate is determined by a comprehensive salary analysis which considers multiple factors including but not limited to: job-related knowledge, skills and qualifications, education and experience as compared to others in the organization doing substantially similar work, if applicable, and market and business considerations. Benefits offered include medical, dental and vision benefits; dependent care flexible spending account; 401(k) plan; voluntary life/short term disability/whole life/term life/accident and critical illness coverage; employee assistance program; sick leave in accordance with regulation. Benefits may be subject to generally applicable eligibility, waiting period, contribution, and other requirements and conditions.
Show more
Show less","Python, PySpark, SQL, Databricks, Azure data lake, Azure DevOps, Azure Functions, Kafka, Flink, Fivetran, Matillion, Agile framework, ETL, ELT, Data governance, Data modeling, Data analytics, Data visualization, Data warehousing, Objectoriented programming, Cloudbased databases","python, pyspark, sql, databricks, azure data lake, azure devops, azure functions, kafka, flink, fivetran, matillion, agile framework, etl, elt, data governance, data modeling, data analytics, data visualization, data warehousing, objectoriented programming, cloudbased databases","agile framework, azure data lake, azure devops, azure functions, cloudbased databases, data governance, dataanalytics, databricks, datamodeling, datawarehouse, elt, etl, fivetran, flink, kafka, matillion, objectoriented programming, python, spark, sql, visualization"
Data Analyst 3 (Need local candidate only),Alrek Business Solutions Inc,"Iowa, IA",https://www.linkedin.com/jobs/view/data-analyst-3-need-local-candidate-only-at-alrek-business-solutions-inc-3760860866,2023-12-17,Iowa,United States,Associate,Onsite,"Client is seeking a Data Analyst
Required/Desired Skills
Skill Required /Desired Amount of Experience Experience with Data Analytics Required 3 Years Experience in SQL Server Reporting Services (SSRS) / Microsoft Report Builder or similar reporting software Required 3 Years Experience with Data Analytic Software (PowerBI, Tableau, etc) Required 3 Years Experience with relational database queries, specifically writing subqueries, versioning and/or temp tables. SSMS experience a plus. Required 3 Years Experience working with clinical data Desired 0
Show more
Show less","Data Analytics, SQL Server Reporting Services (SSRS), Microsoft Report Builder, PowerBI, Tableau, Relational databases, Subqueries, Versioning, Temp tables, SSMS, Clinical data","data analytics, sql server reporting services ssrs, microsoft report builder, powerbi, tableau, relational databases, subqueries, versioning, temp tables, ssms, clinical data","clinical data, dataanalytics, microsoft report builder, powerbi, relational databases, sql server reporting services ssrs, ssms, subqueries, tableau, temp tables, versioning"
Technical Data Analyst,System One,"Des Moines, IA",https://www.linkedin.com/jobs/view/technical-data-analyst-at-system-one-3781198905,2023-12-17,Iowa,United States,Associate,Onsite,"What To Expect
Technical Data analyst responsibilities include conducting full lifecycle data analysis to include requirements, activities, and design.
Complete data profiling and define data transformations for use by the ETL Developers
Acquire data from primary or secondary data sources and maintain databases/data systems
Identify, analyze, and interpret trends or patterns in complex data sets
Locate and define new process improvement opportunities
Qualifications:
Qualifications:
Strong knowledge of Advanced SQL
Strong knowledge of and experience with databases, programming (XML, Python, Json, Javascript, or ETL frameworks)
Previous experience in working with Informatica/other ETL environments preferred
Previous experience or knowledge of mulesoft preferred
Previous experience working with Cloud Data Warehouse/Data Lake preferred(Eg: Snowflake)
Understanding of Data projects and Data Design Documents
Previous experience with creating data mapping document, data requirements, translating business requirements to transformation rules
Previous experience with Data Modeling and exposure to at least Dimensional and 3NF modeling preferred(Exposure to Data Vault 2.0 an additional advantage).
Previous experience in an agile analytics environment preferred. Previous experience working in FAST application preferred
Proactive, problem solver and self-motivated.
Strong analytical abilities
Bachelor's Degree business or information technology related major preferred
Minimum 6 years' quality assurance experience relative to the development and execution of test cases and analysis of testing results preferred
Proven experience with test automation tools preferred
Proven experience working in Agile teams preferred
Demonstrated leadership experience
Strong mathematical aptitude and analytical skills
Strong Microsoft Excel skills and familiarity with other Microsoft tools such as Word and Access. Ability to understand and create complex SQL queries
Understanding of complex life or annuity products and processes
Strong communication skills including verbal, written, and presentation skills
Strong interpersonal skills and proven ability to work and communicate with a variety of personalities, across locations and varying organizational levels
Proven ability to research, analyze, and resolve problem situations using strong judgment and high level decision making skills
Strong initiative, self-motivation, and ability to work independently and oversee work of others
College Degree in the field of computer science, information science, management information systems preferred
Strong knowledge in Insurance industry preferably Life insurance
Problem solving skills sufficient to perform research and recommend a proposed solution to problems
Able to work on multiple tasks and meet established deadlines
Able to effectively direct and coordinate the work of other team members on a project without having HR management responsibility for them
Knowledge of computer programming languages as required for the system
Criminal background check required.
Show more
Show less","Advanced SQL, Databases, Programming, XML, Python, JSON, JavaScript, ETL frameworks, Informatica, Mulesoft, Cloud Data Warehouse, Data Lake, Snowflake, Data projects, Data Design Documents, Data mapping documents, Data requirements, Data Modeling, Dimensional modeling, 3NF modeling, Data Vault 2.0, Agile analytics, FAST application, Test cases, Test automation tools, Agile teams, Mathematical aptitude, Analytical skills, Microsoft Excel, Microsoft Word, Microsoft Access, SQL queries, Complex life products, Annuity products, Communication skills, Verbal communication, Written communication, Presentation skills, Interpersonal skills, Problem solving, Decision making skills, Initiative, Selfmotivation, Teamwork, Leadership experience, Computer programming languages","advanced sql, databases, programming, xml, python, json, javascript, etl frameworks, informatica, mulesoft, cloud data warehouse, data lake, snowflake, data projects, data design documents, data mapping documents, data requirements, data modeling, dimensional modeling, 3nf modeling, data vault 20, agile analytics, fast application, test cases, test automation tools, agile teams, mathematical aptitude, analytical skills, microsoft excel, microsoft word, microsoft access, sql queries, complex life products, annuity products, communication skills, verbal communication, written communication, presentation skills, interpersonal skills, problem solving, decision making skills, initiative, selfmotivation, teamwork, leadership experience, computer programming languages","3nf modeling, advanced sql, agile analytics, agile teams, analytical skills, annuity products, cloud data warehouse, communication skills, complex life products, computer programming languages, data design documents, data lake, data mapping documents, data projects, data requirements, data vault 20, databases, datamodeling, decision making skills, dimensional modeling, etl frameworks, fast application, informatica, initiative, interpersonal skills, javascript, json, leadership experience, mathematical aptitude, microsoft access, microsoft excel, microsoft word, mulesoft, presentation skills, problem solving, programming, python, selfmotivation, snowflake, sql queries, teamwork, test automation tools, test cases, verbal communication, written communication, xml"
Data Engineer,"Oreva Technologies, Inc.","Des Moines, IA",https://www.linkedin.com/jobs/view/data-engineer-at-oreva-technologies-inc-3783118978,2023-12-17,Iowa,United States,Mid senior,Onsite,"Role - Data Engineer – ETL - SNOWFLAKE
Location: Des Moines, IA – MUST BE IN IOWA!
Contract Duration: End of 2024 minimum
Job Description:
We are seeking a highly skilled and motivated Data Engineer/ETL Developer with proven expertise in working with the Snowflake platform. As a key member of our data team, you will play a crucial role in designing, implementing, and optimizing data pipelines, ensuring efficient extraction, transformation, and loading of data into Snowflake.
Desired Skill
• Key Skills · ETL Development: Design, develop, and maintain robust ETL processes to extract, transform, and load data into Snowflake from various sources, ensuring data quality, integrity, and reliability.
• Snowflake Optimization: Utilize in-depth knowledge of Snowflake architecture, features, and best practices to optimize performance, scalability, and efficiency of data pipelines and warehouse operations.
• Data Modeling: Collaborate with data architects to design and implement data models in Snowflake, ensuring alignment with business requirements and scalability for future growth.
• Monitoring and Maintenance: Implement monitoring solutions to proactively identify and address issues related to data quality, pipeline performance, and system health. Conduct regular maintenance and troubleshooting of ETL processes.
• Proficiency in SQL and scripting languages (Javascript, Python, etc.) for data manipulation and automation.
• Strong understanding of data warehousing concepts, ETL principles, and data modeling.
• Experience working with various data integration tools and technologies.
• Excellent problem-solving skills and the ability to troubleshoot complex issues.
• Strong communication skills and ability to work effectively in a collaborative team environment.
• Experience with Informatica or Microsoft PowerBI a plus.
Years of Experience
• 7 years IT development experience
• 5 years designing and implementing enterprise data solutions
For more details follow me on Linkedin
https://www.linkedin.com/in/tyagi14
/
Show more
Show less","Data Engineering, ETL Development, Snowflake, Data Modeling, Monitoring and Maintenance, SQL, Javascript, Python, Data Warehousing, Informatica, Microsoft PowerBI","data engineering, etl development, snowflake, data modeling, monitoring and maintenance, sql, javascript, python, data warehousing, informatica, microsoft powerbi","data engineering, datamodeling, datawarehouse, etl development, informatica, javascript, microsoft powerbi, monitoring and maintenance, python, snowflake, sql"
Data Engineer 2/3 (Database Administrator),MidAmerican Energy Company,Des Moines Metropolitan Area,https://www.linkedin.com/jobs/view/data-engineer-2-3-database-administrator-at-midamerican-energy-company-3763368818,2023-12-17,Iowa,United States,Mid senior,Onsite,"This is a multi-level posting. Candidates may be considered for any of the posted levels, depending on their level of experience and depth of expertise.
Purpose of Position
Responsible for the design, implementation, maintenance and monitoring of database technologies and related software. Conducts analysis, creates system specifications, develops, tests and implements engineering, scientific and business applications, operating systems, and file/database servers. Utilizes existing or new technology in the automation of processes. Evaluates software packages and provides recommendations to management. Provides actionable recommendations for improvements in application performance after analyzing underlying database-related infrastructure components.
Primary Job Duties And Responsibilities (Essential Job Functions)
Designs, codes, tests documents, implements and maintains database utilities. (25%)
Performs top expert-level database analysis and design reviews. (20%)
Research and fact-finding to develop or modify database structures. (10%)
Works with vendors to review quality, adaptability and compatibility of their products. (10%)
Provides support for vendor-developed database implementation and periodic maintenance releases as required. (10%)
Provides technical input to assist in development of project planning. (10%)
Performs additional responsibilities as requested or assigned. (5%)
Maintains RDBMS software at appropriate vendor-supported levels. (10%)
Job
Information Technology
Primary Location
IA-Des Moines
Other Locations
NV-Las Vegas, OR-Portland
Organization
Corp - Information Technology
Schedule
Full time
Hiring Range
$87,600-$118,400
Position Requirements
Bachelor’s degree in computer science, information technology or related field; or equivalent work experience. (Typically six years of related, progressive work experience would be needed for candidates applying for this position who do not possess a bachelor’s degree.)
A minimum of two years of additional directly-related experience is required.
Educational experience in database administration or systems analysis on one of the following or any combination: Oracle and Oracle Cloud products.
A strong understanding of database fundamentals
Experience on Maximum Availability Architecture components like Oracle RAC, Oracle ASM, Oracle Grid Control, Oracle Data Guard & Oracle Goldengate
Hand on experience with Database Upgrade/Migration
Preferred hands-on experience with CDB/PDB multitenant architecture (create, cloning, dropping, CDBs and PDBs),
Preferred hands-on experience with Database migration from conventional database to CDB/PDB multitenant architecture
Preferred Experience multiple Unix Flavors like AIX/SOLARIS/LINUX
A knowledge of RMAN backup and recovery processes
Proficiency in using SRVCTL and CRSCTL for listeners, adding databases, shutdown/startup
Experience using Oracle Enterprise Manager
Oracle Fleet Maintenance experience a plus
Experience on Exadata a plus
Position requires 24/7 production on call in a group rotation.
Night /weekend work required periodically for patching activities or similar.
Employees must be able to perform the essential functions of the position, with or without an accommodation. We celebrate diversity, equity and inclusion, and we are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion or religious creed, age, national origin, ancestry, citizenship status (except as required by law), gender (including gender identity and expression), sex (including pregnancy), sexual orientation, genetic information, physical or mental disability, veteran or military status, familial or parental status, marital status or any other category protected by applicable local, state or U.S. federal law.
Show more
Show less","Oracle, SQL, Unix, Linux, RMAN, SRVCTL, CRSCTL, Oracle Enterprise Manager, Oracle Fleet Maintenance, Exadata, RDBMS, Database Administration, Systems Analysis, Oracle Cloud, Oracle RAC, Oracle ASM, Oracle Grid Control, Oracle Data Guard, Oracle Goldengate, CDB/PDB, Multitenant Architecture","oracle, sql, unix, linux, rman, srvctl, crsctl, oracle enterprise manager, oracle fleet maintenance, exadata, rdbms, database administration, systems analysis, oracle cloud, oracle rac, oracle asm, oracle grid control, oracle data guard, oracle goldengate, cdbpdb, multitenant architecture","cdbpdb, crsctl, database administration, exadata, linux, multitenant architecture, oracle, oracle asm, oracle cloud, oracle data guard, oracle enterprise manager, oracle fleet maintenance, oracle goldengate, oracle grid control, oracle rac, rdbms, rman, sql, srvctl, systems analysis, unix"
Senior NoSQL Developer - Data Modeling and Design Expert (719936),Laksan Technologies,"Walnut, IA",https://www.linkedin.com/jobs/view/senior-nosql-developer-data-modeling-and-design-expert-719936-at-laksan-technologies-3727566441,2023-12-17,Iowa,United States,Mid senior,Onsite,"Searching for a NoSQL data modeling and design expert.
The Iowa Department of Health and Human Services is seeking a highly skilled and experienced Senior NoSQL Developer with a deep understanding of data modeling, design, and a specialization in transactions and analytical data. The ideal candidate will possess a profound expertise in representing relational models using graph databases, especially for large clusters of nodes. Additionally, experience with Microsoft Azure Data Platform elements is essential for this role.
Key Responsibilities
General understanding of Entity Resolution techniques
Data Modeling and Design: Develop and maintain efficient data models for transactional and analytical data systems.
Graph Database Expertise: Design and optimize graph database structures for large clusters of nodes, ensuring optimal performance and scalability.
NoSQL Development: Develop and implement NoSQL database solutions tailored to specific project requirements.
Optimizing graph database queries for performance and scalability.
Microsoft Azure Integration: Utilize Azure Data Platform elements to build and optimize data solutions on the Microsoft cloud ecosystem.
Performance Optimization: Identify and address performance bottlenecks, ensuring data systems are optimized for speed and efficiency.
Data Security: Implement robust security measures to safeguard sensitive data within the NoSQL environment.
Collaboration: Collaborate with cross-functional teams including data engineers, developers, data scientists, and business analysts to ensure data solutions meet business needs.
Documentation: Create and maintain comprehensive documentation for data models, design decisions, and system configurations.
Show more
Show less","NoSQL, Data Modeling, Data Design, Graph Databases, Entity Resolution, Microsoft Azure Data Platform, Performance Optimization, Data Security, Collaboration, Documentation","nosql, data modeling, data design, graph databases, entity resolution, microsoft azure data platform, performance optimization, data security, collaboration, documentation","collaboration, data design, data security, datamodeling, documentation, entity resolution, graph databases, microsoft azure data platform, nosql, performance optimization"
"Senior Data Engineer - Data Bricks, ADLS",Compunnel Inc.,"Des Moines, IA",https://www.linkedin.com/jobs/view/senior-data-engineer-data-bricks-adls-at-compunnel-inc-3786827979,2023-12-17,Iowa,United States,Mid senior,Onsite,"Description
Primary Skills:
5+ Years of Data Engineering experience with Data Bricks and ADLS 5+ years of Azure Cloud and Microsoft Experience
Summary
A resource building data pipelines using Databricks and ADLS based on a defined set of requirements. Deliver outputs in collaboration with the product team, platform and data engineering, and business partners.
Experience Level
Engineer or Senior Engineer level experience is needed.
Required
5+ Years of Data Engineering experience with Data Bricks and ADLS
5+ years of Azure Cloud and Microsoft Experience
Education:
Bachelors Degree
Show more
Show less","Data Engineering, Data Bricks, ADLS, Azure Cloud, Microsoft, Product Team, Platform, Data Engineering, Business Partners, Bachelor's Degree","data engineering, data bricks, adls, azure cloud, microsoft, product team, platform, data engineering, business partners, bachelors degree","adls, azure cloud, bachelors degree, business partners, data bricks, data engineering, microsoft, platform, product team"
Business/Data analyst,Stellar Professionals,"Des Moines, IA",https://www.linkedin.com/jobs/view/business-data-analyst-at-stellar-professionals-3637871510,2023-12-17,Iowa,United States,Mid senior,Onsite,"Mode of interview: In Person Only
Work arrangement: Onsite
Applicant must have 4 years of relevant experience with the following:
Moderate to expert understanding of data systems, data integrity, and data validation
Proficient in Microsoft Excel with the ability to do advanced data manipulation.
Strong organizational skills including attention to detail and multi-tasking skills
Experience in developing data for analysis
Good Communication Skills
Show more
Show less","Data systems, Data integrity, Data validation, Microsoft Excel, Data manipulation, Data analysis, Communication skills, Organizational skills, Attention to detail, Multitasking","data systems, data integrity, data validation, microsoft excel, data manipulation, data analysis, communication skills, organizational skills, attention to detail, multitasking","attention to detail, communication skills, data integrity, data manipulation, data systems, data validation, dataanalytics, microsoft excel, multitasking, organizational skills"
Grants manager/Data analyst,Stellar Professionals,"Des Moines, IA",https://www.linkedin.com/jobs/view/grants-manager-data-analyst-at-stellar-professionals-3723911112,2023-12-17,Iowa,United States,Mid senior,Onsite,"Mode of interview: Either Web Cam or In Person Interview
Work arrangement: Hybrid
Applicant must have 6 years of relevant experience with the following:
Strong organizational skills including attention to detail and multi-tasking skills
Proficient in Microsoft Excel with the ability to do advanced data manipulation
Good Communication Skills
Experience in developing data for analysis
Show more
Show less","Microsoft Excel, Data Manipulation, Communication Skills, Data Analysis","microsoft excel, data manipulation, communication skills, data analysis","communication skills, data manipulation, dataanalytics, microsoft excel"
IA-DOM-HHS-DBA4-IBHRS Data Analyst,HexaQuEST Global,"Des Moines, IA",https://www.linkedin.com/jobs/view/ia-dom-hhs-dba4-ibhrs-data-analyst-at-hexaquest-global-3782826442,2023-12-17,Iowa,United States,Mid senior,Onsite,"Job Description
The HHS Division of Behavioral Health is seeking a Data Analyst
This position is for a Data Analyst for the Iowa Behavioral Health Reporting System (IBHRS) within the Bureau of Services, Planning and Performance in the Office of Data Analytics and Reporting. This position will be responsible for data quality of the Iowa Behavioral Health System (IBHRS) database.
The main duties will consist of analyzing trends in data reporting, working with providers on the integrity of the data submitted, and working with stakeholders to understand the data. In addition, this position will be responsible for preparing SSRS reports for use by internal and external stakeholders, reviewing existing reports for integrity, streamlining and combining reports for ease of access and use, and preparing dashboards using various data visualization tools.
Skills
We are looking for a self-starter who is willing to learn. An ideal candidate will have a positive attitude and strong communication skills.
The Needs Of This Position Are As Follows
Works as the Data Analyst for the Iowa Behavioral Health Reporting System (IBHRS) within the Bureau of Services, Planning and Performance in the Office of Data Analytics and Reporting.
Coordinate people and resources required to accomplish objectives without the use of supervisory authority.
Responsible for data quality of the substance use, problem gambling and mental health data reported into the IBHRS system.
Possess the skills required to provide actionable recommendations based on data analysis.
Able to understand and assess the quality of source data; Gain knowledge of the system to validate the data being reported by providers and prepared for use by stakeholders. Collect metrics for trends in data reported into IBHRS. Review data reported for outliers.
Provide technical assistance and guidance to providers to clean data and prepare it for analysis.
Able to use a relational database, reporting software, and data analytic software to create simple to complex ad hoc queries, write reports and create dashboards against the data reported from IBHRS.
Uses data visualization tools such as Microsoft Excel and Power BI or Tableau. Set up and maintain executive dashboards so all stakeholders (internal and external) view the same data in the same way.
Reviews existing reports for integrity.
Works with stakeholders to explain the data being reported. Provide guidance to programs on how to access available reports.
Works with business user, data stewards to review data with the business to evaluate data quality and trace data lineage from source systems.
Provide data expertise to other Bureau staff and acts as a liaison between business users and technical staff.
Daily/Weekly Time Allocation
25% - Develop new reports in IBHRS, respond to ad hoc requests for data, check integrity of existing reports, review data submitted by providers into IBHRS.
20% - Work with reporting providers to correct data reporting issues.
15% - Meet with agency programs on a regular basis (monthly to quarterly) to review data trends and assess ongoing needs for reports from the programs.
15% - Create and maintain dashboards for internal and external stakeholders.
15% - Participate in division meetings related to data.
10% - Technical testing of the Iowa Behavioral Health Reporting System.
Questions
INTERVIEW DATES: Interviews will be conducted on [Nov 16, 17, 20, 21] only to submit candidates available for interviews on the date(s) provided.
Skills & Experience
Skills Required / Desired Required Years of Experience Actual Years of Experience
Required
Experience with Data Analytics Required 3 Years
Experience in SQL Server Reporting Services (SSRS) / Microsoft Report Builder or similar reporting software Required 3 Years
Experience with Data Analytic Software (PowerBI, Tableau, etc) Required 3 Years
Experience with relational database queries, specifically writing subqueries, versioning and/or temp tables. SSMS experience a plus. Required 3 Years
Experience working with clinical data Desired
Show more
Show less","Data Analytics, SQL Server Reporting Services (SSRS), Microsoft Report Builder, Data Analytic Software (PowerBI Tableau), Relational database queries, Subqueries, Versioning, Temp tables, SSMS, Clinical data, Microsoft Excel","data analytics, sql server reporting services ssrs, microsoft report builder, data analytic software powerbi tableau, relational database queries, subqueries, versioning, temp tables, ssms, clinical data, microsoft excel","clinical data, data analytic software powerbi tableau, dataanalytics, microsoft excel, microsoft report builder, relational database queries, sql server reporting services ssrs, ssms, subqueries, temp tables, versioning"
Data Scientist,Intellectt Inc,"Urbandale, IA",https://www.linkedin.com/jobs/view/data-scientist-at-intellectt-inc-3750868851,2023-12-17,Iowa,United States,Mid senior,Onsite,"Job Title
: Data Scientist I
Location :
Urbandale, Iowa
Please find the below immediate Full-Time Position.
Note: Would prefer candidates that can work onsite in Urbandale, but will consider remote candidates if needed.
Job Responsibilities
Communicate with impact your findings and methodologies to stakeholders with a variety of backgrounds.
Work with high resolution machine and agronomic data in the development and testing of predictive models.
Develop and deliver production-ready machine learning approaches to yield insights and recommendations from precision agriculture data
Define, quantify, and analyze Key Performance Indicators that define successful customer outcomes.
Work closely with the Data Engineering teams to ensure data is stored efficiently and can support the required analytics.
Relevant Skills Include
Demonstrated competency in developing production-ready models in an Object-Oriented Prog language such as Python.
Demonstrated competency in using data-access technologies such as SQL, Spark, Databricks, BigQuery, MongoDB, etc.
Experience with Visualization tools such as Tableau, PowerBI, DataStudio, etc.
Experience with Data Modeling techniques such as Normalization, data quality and coverage assessment, attribute analysis, performance management, etc.
Experience building machine learning models such as Regression, supervised learning, unsupervised learning, probabilistic inference, natural language modeling, etc.
Excellent communication skills. Able to effectively lead meetings, to document work for reproduction, to write persuasively, to communicate proof-of-concepts, and to effectively take notes.
What Makes Candidates Stand-out Are Skills Such As
Additional experience with other languages such as R, JavaScript, Scala, etc.
Examples of professional work such as publications, patents, a portfolio of relevant project-work, etc.
Familiarity with Distributed Datasets
Experienced with a variety of data structures such as time-series, geo-tagged, text, structured, and unstructured.
Experience with simulations such as Monte Carlo simulation, Gibbs sampling, etc.
Experience with model validation, measuring model bias, measuring model drift, etc.
Experience collaborating with stakeholders from disciplines such as Product, Sales, Finance, etc.
Ability to communicate complex analytical insights in a manner which is clearly understandable by nontechnical audiences.
Major Purpose
Collaborates with business and analytics leaders to generate insights and answer business questions by using analytics techniques such as advanced data visualization, statistical analysis, randomized testing, predictive modeling, forecasting, optimization, and/or machine learning.
Proposes innovative ways to look at problems by using these approaches on available enterprise data as well as customer third party data and information.
Validates findings using experimental and iterative approaches.
This level performs basic statistical analysis of low to moderately complex data from a single source, with heavy reliance on industry/standardized tools and existing models.
Output is reviewed by higher-level Data Scientists or Analytics Manager for execution.
Major Duties
Works with data sets and performs appropriate analytical methodology to provide insights and decision modeling for the business.
Creates and implements algorithms which manage the data to enable it to be analyzed in an efficient manner.
Supports the communication of derived insights, especially through appropriate visualization techniques.
Supports the identification of the required data sources and works with Data Wranglers and/or IT to implement methodologies to retrieve and use this data.
Stays abreast of the latest appropriate analytical techniques and recommends these where needed.
Explains implementation and usage in business terms.
Applies analytical techniques to prospect for business insights and find patterns in data which could be valuable for the business Skills, Abilities.
Knowledge
Quantitative analytical skills
Knowledge of appropriate industry
Good interpersonal, negotiation and conflict resolution skills.
Excellence in verbal and written communication forms with emphasis on persuasive communication, tact and negotiation.
Business process knowledge of assigned area(s) and/or function(s).
Knowledge of advanced data gathering and analysis techniques, including statistical analysis.
Education
Degree in a Math discipline or equivalent experience. - University Degree (4 years or equivalent)
Economics - University Degree (4 years or equivalent)
Statistics - University Degree (4 years or equivalent)
Work Experience
Internal or external industry specific experience in relevant discipline. (1 - 3 years)
Data analytics experience. (1 - 3 years)
Background or proven experience in mining data for analytics insights. (1 - 3 years)
Good exposure to enterprise statistical tools like SAS, Statistica, SPSS or SAS E Miner (1 - 3 years)
Varun
Senior IT Recruiter
varun@intellectt.com| www.intellectt.com
Desk: 7323986579;Ex:261
Address: 517 Route 1 S Suite 1115, Iselin, NJ 08830
Show more
Show less","Python, SQL, Spark, Databricks, BigQuery, MongoDB, Tableau, PowerBI, DataStudio, Normalization, Regression, Supervised learning, Unsupervised learning, Probabilistic inference, Natural language modeling, R, JavaScript, Scala, Timeseries, Geotagged, Text, Structured, Unstructured, Monte Carlo simulation, Gibbs sampling, Model validation, Model bias, Model drift, Advanced data visualization, Statistical analysis, Randomized testing, Predictive modeling, Forecasting, Optimization, Machine learning, Business process knowledge, Advanced data gathering, Analysis techniques, SAS, Statistica, SPSS, SAS E Miner","python, sql, spark, databricks, bigquery, mongodb, tableau, powerbi, datastudio, normalization, regression, supervised learning, unsupervised learning, probabilistic inference, natural language modeling, r, javascript, scala, timeseries, geotagged, text, structured, unstructured, monte carlo simulation, gibbs sampling, model validation, model bias, model drift, advanced data visualization, statistical analysis, randomized testing, predictive modeling, forecasting, optimization, machine learning, business process knowledge, advanced data gathering, analysis techniques, sas, statistica, spss, sas e miner","advanced data gathering, advanced data visualization, analysis techniques, bigquery, business process knowledge, databricks, datastudio, forecasting, geotagged, gibbs sampling, javascript, machine learning, model bias, model drift, model validation, mongodb, monte carlo simulation, natural language modeling, normalization, optimization, powerbi, predictive modeling, probabilistic inference, python, r, randomized testing, regression, sas, sas e miner, scala, spark, spss, sql, statistica, statistical analysis, structured, supervised learning, tableau, text, timeseries, unstructured, unsupervised learning"
Principal Software Engineer - Air Combat Datalinks (Onsite),Collins Aerospace,"Cedar Rapids, IA",https://www.linkedin.com/jobs/view/principal-software-engineer-air-combat-datalinks-onsite-at-collins-aerospace-3767481763,2023-12-17,Iowa,United States,Mid senior,Onsite,"Date Posted:
2023-10-30
Country:
United States of America
Location:
HIA32: Cedar Rapids, IA 400 Collins Rd NE , Cedar Rapids, IA, 52498-0505 USA
Position Role Type:
Onsite
The Air Combat Test and Training (ACTT) engineering department is responsible for the design, analysis, test and certification of Air Combat Test and Training systems. We solve the challenges of next-generation pilot proficiency training so they can “Train as they Fight.” We field Test and Training range systems with state-of-the-art technology in cryptography, datalinks, navigation, communication, networking and command and control. We specialize in highly accurate, highly secure solutions for U.S. Navy and Air Force customers. We are a system of systems development team that designs, integrates, and supports some of the largest, most complex solutions offered by Collins Aerospace.
The Datalinks team within the ACTT department is responsible for the development of the mesh networking waveform that links together the training participants with the mission commanders. The team is looking for an experienced
Principal Software Engineer
to support the expansion of our Air Combat system capabilities. This position will be located onsite at our Cedar Rapids, IA site.
Eligible for relocation.
What you will do:
Actively participate as a member of an agile development team.
Effectively communicate with internal teams and leadership
Design, Develop, Review, and Test new software features.
Package software builds and deploy to an embedded target environment.
Support integration and system test activities in a lab environment.
Education and experience:
Typically requires a degree in Science, Technology, Engineering or Mathematics (STEM) unless prohibited by local laws/regulations and minimum 8 years prior relevant experience or an Advanced Degree in a related field and minimum 5 years of experience or in absence of a degree, 12 years of relevant experience
Active and transferable U.S. government issued security clearance is required prior to start date
U.S. citizenship is required, as only U.S. citizens are eligible for a security clearance.
Skills you must have:
Experience coding in C++ and Python
Experience with Windows and Linux Operating Systems
Experience investigating problems to root cause and championing corrective actions
Experience with Object Oriented architectures and design patterns
Experience with software configuration management (Git, SVN)
Experience with networking standards and communication protocol design
Skills we value:
Experience with Agile development methodology
Experience working in Atlassian tools (JIRA, Bitbucket, Confluence, Bamboo)
Experience with CI/CD technologies
Experience with LVC communication standards (DIS, TENA, HLA)
Motivated self-starter with strong analytical skills
Collins Aerospace, a Raytheon Technologies company, is a leader in technologically advanced and intelligent solutions for the global aerospace and defense industry. Collins Aerospace has the capabilities, comprehensive portfolio and expertise to solve customers’ toughest challenges and to meet the demands of a rapidly evolving global market.
Do you want to be a part of something bigger? A team whose impact stretches across the world, and even beyond? At Collins Aerospace, our Mission Systems team helps civilian, military and government customers complete their most complex missions — whatever and wherever they may be. Our customers depend on us for intelligent and secure communications, missionized systems for specialized aircraft and spacecraft and collaborative space solutions. By joining our team, you’ll have your own critical part to play in ensuring our customer succeeds today while anticipating their needs for tomorrow. Are you up for the challenge? Join our mission today.
#reempowerprogram
This role is also eligible for the Re-Empower Program. The Re-Empower Program helps support talented and committed professionals as they rebuild their capabilities, enhance leadership skills, and continue their professional journey. Over the course of the 14-week program, experienced professionals will gain paid, on-the-job experience, have an opportunity to participate in sessions with leadership, develop personalized plans for success and receive coaching to guide their return-to-work experience. Upon completion of the program, based on performance and contributions participants will be eligible for a career at RTX.
Minimum Program Qualifications
Be on a career break of one or more year at time of application
Have prior experience in functional area of interest
Have interest in returning in either a full-time or part-time position
Diversity drives innovation; inclusion drives success
. We believe a multitude of approaches and ideas enable us to deliver the best results for our workforce, workplace, and customers. We are committed to fostering a culture where all employees can share their passions and ideas so we can tackle the toughest challenges in our industry and pave new paths to limitless possibility.
WE ARE REDEFINING AEROSPACE.
Please consider the following role type definitions as you apply for this role.
Onsite:
Employees who are working in Onsite roles will work primarily onsite. This includes all production and maintenance employees, as they are essential to the development of our products.
Regardless of your role type, collaboration and innovation are critical to our business and all employees will have access to digital tools so they can work with colleagues around the world – and access to Collins sites when their work requires in-person meetings.
Some of our competitive benefits package includes:
Medical, dental, and vision insurance
Three weeks of vacation for newly hired employees
Generous 401(k) plan that includes employer matching funds and separate employer retirement contribution, including a Lifetime Income Strategy option
Tuition reimbursement program
Student Loan Repayment Program
Life insurance and disability coverage
Optional coverages you can buy: pet insurance, home and auto insurance, additional life and accident insurance, critical illness insurance, group legal, ID theft protection
Birth, adoption, parental leave benefits
Ovia Health, fertility, and family planning
Adoption Assistance
Autism Benefit
Employee Assistance Plan, including up to 10 free counseling sessions
Healthy You Incentives, wellness rewards program
Doctor on Demand, virtual doctor visits
Bright Horizons, child and elder care services
Teladoc Medical Experts, second opinion program
And more!
At Collins, the paths we pave together lead to limitless possibility. And the bonds we form – with our customers and with each other -- propel us all higher, again and again.
Apply now and be part of the team that’s redefining aerospace, every day.
RTX is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.
Privacy Policy and Terms:
Click on this link to read the Policy and Terms
01659824
Show more
Show less","C++, Python, Windows, Linux, Git, SVN, Agile, Atlassian tools, JIRA, Bitbucket, Confluence, Bamboo, CI/CD, LVC communication standards, DIS, TENA, HLA, Object Oriented architectures, Software configuration management","c, python, windows, linux, git, svn, agile, atlassian tools, jira, bitbucket, confluence, bamboo, cicd, lvc communication standards, dis, tena, hla, object oriented architectures, software configuration management","agile, atlassian tools, bamboo, bitbucket, c, cicd, confluence, dis, git, hla, jira, linux, lvc communication standards, object oriented architectures, python, software configuration management, svn, tena, windows"
Senior Cassandra Database Engineer,Wells Fargo,"West Des Moines, IA",https://www.linkedin.com/jobs/view/senior-cassandra-database-engineer-at-wells-fargo-3766518279,2023-12-17,Iowa,United States,Mid senior,Onsite,"At Wells Fargo, we are looking for talented people who will put our customers at the center of everything we do. We are seeking candidates who embrace diversity, equity and inclusion in a workplace where everyone feels valued and inspired. Help us build a better Wells Fargo. It all begins with outstanding talent. It all begins with you.
About This Role
Wells Fargo is seeking a Senior Cassandra Database Engineer to administer Cassandra database technology. This role is part of the Digital Technology and Innovation group in our Technology Organization. Learn more about the career areas and lines of business at wellsfargojobs.com .
In This Role, You Will
Installation and adminstration of Cassandra databases
Documentation of guides for production database administrators
Defining standards for installation, deployment, security, authentication and authorization, management policies and best practices
Defining and implementing backup and restoration strategies
Defining and implementing monitoring and alarming strategies
Perform the planning, research, design, implementation, maintenance, and control of server class databases
Consult with and advise management and multiple clients on high impact data or database management issues, influencing strategic direction.
Required Qualifications, US:
4+ years of experience with implementing and administrating Cassandra database
4+ years experience with Datastax Cassandra
4+ years of experience with Datastax Ops Centre backup/restoration and monitoring/alarm implementation
4+ years of experience with Ansible automation tool or equivalent
4+ years of Shell or Python or Perl experience
Desired Certifications and Qualifications:
Datastax Certified Cassandra Administrator
Datastax Certified Cassandra Developer
Proven experience with Mongodb or other SQL and NoSQL databases a plus
Experience with Agile methodology
Demonstrated experience with UNIX and Shell Scripting
Demonstrated experience in designing for high volume OLTP applications
Demonstrated experience in Change Management and SDLC
Experience with Version Control System such as Git
Experience with Issue and Tracking software such as Jira
Pay Range
$84,000.00 - $179,200.00
Benefits
Wells Fargo provides all eligible full- and part-time employees with a comprehensive set of benefits designed to protect their physical and financial health and to help them make the most of their financial future. Visit Benefits - Wells Fargo Careers for an overview of the following benefit plans and programs offered to employees.
401(k) Plan
Paid Time Off
Parental Leave
Critical Caregiving Leave
Discounts and Savings
Health Benefits
Commuter Benefits
Tuition Reimbursement
Scholarships for dependent children
Adoption Reimbursement
We Value Diversity
At Wells Fargo, we believe in diversity, equity and inclusion in the workplace; accordingly, we welcome applications for employment from all qualified candidates, regardless of race, color, gender, national origin, religion, age, sexual orientation, gender identity, gender expression, genetic information, individuals with disabilities, pregnancy, marital status, status as a protected veteran or any other status protected by applicable law.
Employees support our focus on building strong customer relationships balanced with a strong risk mitigating and compliance-driven culture which firmly establishes those disciplines as critical to the success of our customers and company. They are accountable for execution of all applicable risk programs (Credit, Market, Financial Crimes, Operational, Regulatory Compliance), which includes effectively following and adhering to applicable Wells Fargo policies and procedures, appropriately fulfilling risk and compliance obligations, timely and effective escalation and remediation of issues, and making sound risk decisions. There is emphasis on proactive monitoring, governance, risk identification and escalation, as well as making sound risk decisions commensurate with the business unit's risk appetite and all risk and compliance program requirements.
Candidates applying to job openings posted in US: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other legally protected characteristic.
Drug and Alcohol Policy
Wells Fargo maintains a drug free workplace. Please see our Drug and Alcohol Policy to learn more.
Reference Number
R-276818-8
Show more
Show less","Cassandra, Datastax Cassandra, Datastax Ops Centre, Ansible, Shell, Python, Perl, UNIX, Shell Scripting, SQL, NoSQL, Agile, Version Control System, Git, Jira, OLTP, SDLC","cassandra, datastax cassandra, datastax ops centre, ansible, shell, python, perl, unix, shell scripting, sql, nosql, agile, version control system, git, jira, oltp, sdlc","agile, ansible, cassandra, datastax cassandra, datastax ops centre, git, jira, nosql, oltp, perl, python, sdlc, shell, shell scripting, sql, unix, version control system"
Data Analyst (Business Intelligence),A-Line Staffing Solutions,"West Des Moines, IA",https://www.linkedin.com/jobs/view/data-analyst-business-intelligence-at-a-line-staffing-solutions-3680700079,2023-12-17,Iowa,United States,Mid senior,Onsite,"Hybrid
Grimes, IA
Direct Hire Role
General Function
An developing data professional contributing to the design of data models and databases, data quality, and the effective acquisition, integration, management and communication of data and metadata.
Core Competencies
Partnerships
Growth mindset
Results oriented
Customer focused
Professionalism
Reporting Relations
Accountable and Reports to: TBD
Positions that Report to you: TBD
Primary Duties And Responsibilities
Is able to pull pre-defined data sources together for common problems. More complex data needs will be a challenge within domain.
Learning and able to interact with modeling techniques. May not yet be master of any one, but is on their way to be. Is able to understand and communicate simple results, but needs direction on where next to take the model(s).
Works with superiors to understand evaluation metrics. Collaborates with superiors to plan out next action steps in the modeling process.
Works with superiors on deployment.
Familiarity with statistical or technical software. Varying levels of code efficiency. Focus on key areas of need to upskill code writing.
Able to create basic visuals.
Develops, configures, and tests data and metadata flows from specifications
Performs data profiling and builds Data Quality rules from specifications
Prepares data policies, standards, and procedures
Performs on-boarding and curation of data in enterprise data catalog
Knowledge, Skills, Abilities, And Worker Characteristics
Has a basic understanding of their team's domain, and how it contributes to overall business strategy. Is able to identify more complex KPI's or inner-workings within their focus area.
Builds and maintains working knowledge of multiple business areas and the data and applications that support them.
Demonstrates working knowledge of multiple data management and analytics tools and technologies.
Exhibits an understanding of the technical architecture, data landscape and IT environment, as well as tools used in development, deployment, and testing.
Experience and Education
Bachelor Degree Preferred, Or Relevant Experience.
Supervisory Responsibilities (Direct Reports)
Operates effectively as part of a team and manages own work, including estimating own work effort for input to project planning.
Escalates delays, issues, risks and highlights to managers and/or project leads.
Physical Requirements
Visual requirements include: ability to see detail at near range with or without correction.
Must be physically able to perform sedentary work: operating a computer, occasionally lifting or carrying objects of no more than 10 pounds, and occasionally standing or walking.
Must be able to perform the following physical activities: meeting with customers, kneeling, reaching, handling, grasping, feeling, talking, hearing, and repetitive motions.
If you think this position is a good fit for you, please reach out to me - feel free to call, text, e-mail, or apply to this posting!
Brett Middleton
586-574-5681
bmiddleton@alinestaffing.com
Show more
Show less","Data modeling, Databases, Data quality, Data integration, Data management, Data communication, Metadata, Partnerships, Growth mindset, Results oriented, Customer focused, Professionalism, Statistical software, Technical software, Code efficiency, Data visualization, Data profiling, Data Quality rules, Data policies, Data standards, Data procedures, Data catalog, KPI, Data management tools, Analytics tools, Technical architecture, Data landscape, IT environment, Development tools, Deployment tools, Testing tools","data modeling, databases, data quality, data integration, data management, data communication, metadata, partnerships, growth mindset, results oriented, customer focused, professionalism, statistical software, technical software, code efficiency, data visualization, data profiling, data quality rules, data policies, data standards, data procedures, data catalog, kpi, data management tools, analytics tools, technical architecture, data landscape, it environment, development tools, deployment tools, testing tools","analytics tools, code efficiency, customer focused, data catalog, data communication, data integration, data landscape, data management, data management tools, data policies, data procedures, data profiling, data quality, data quality rules, data standards, databases, datamodeling, deployment tools, development tools, growth mindset, it environment, kpi, metadata, partnerships, professionalism, results oriented, statistical software, technical architecture, technical software, testing tools, visualization"
Data Center Engineer,Park Place Technologies,"Des Moines, IA",https://www.linkedin.com/jobs/view/data-center-engineer-at-park-place-technologies-3765295148,2023-12-17,Iowa,United States,Mid senior,Remote,"Field Service Engineer
The Field Service Engineer is responsible for providing onsite system diagnostic and analytical support to customers within a geographic territory, or assigned to a specific account, supporting the customer per the terms of the SLA (Service Level Agreement) with Park Place Technologies. Specifically, the FSE responds to customers’ systems failures by way of computer hardware service, testing, diagnostic analysis, and systems analysis of hardware, storage area networks, and system configurations. An FSE must be available to respond to customer issues 24/7 and service all equipment regardless of product training within the designated service area. They also must have a thorough and broad knowledge of system configuration.
Who We Are:
As the global leader in third party maintenance, our 2500 Park Place Associates provide support to 21,000+ customers in more than 154+ countries. We are proud to service 90% of Fortune 500 companies and 40% of Forbes 100 clients.
Our company’s strength and success are a credit to our Associates, and Park Place Life is how we communicate and deliver our culture internally. We have been awarded as a NorthCoast 99 “Best Workplace” winner for 13 consecutive years in recognition of our employee commitment. Park Place Life is about collaboration, responsiveness, diversity, and integrity, and represents everything that makes our company great and our culture unique.
Top Rated Benefits We Offer:
We cover 100% of your Healthcare benefits!
Profit Sharing!
401K matching contributions and earnings are always 100% vested.
Flexible Vacation.
Overtime Pay.
Mileage Reimbursement.
Plus much more!!!
What you’ll be doing:
Provides onsite technical customer support:
Ensures timely, professional, and effective response to customer service needs to maintain a high level of customer satisfaction.
Must be able to be scheduled for work on shifts occurring at any time of day.
Provides effective problem analysis and identification remotely before arrival at customer site; determines needed parts and documentation to minimize down time and multiple trips
Performs service in a cost effective manner.
Displays professional attitude and courtesy while on site.
Maintains effective communication with customer and our customer support center during repair process and any projected delay.
Analyzes software and hardware error logs, utilizes diagnostic and troubleshooting techniques and operating system analysis to ensure timely and effective repair.
Analyzes, diagnoses, troubleshoots and repairs hardware, storage area network and systems configuration and compatibility problems.
Utilizes multiple tools for remote system connection to perform remote diagnosis, repair or configuration changes.
Follows customer specific repair procedures.
Assesses current and future customer needs based on usage of the system.
Inventory / Parts Management:
Determines needed parts and quantities based on contracts in service area.
Returns bad or excess parts in a timely manner.
Manages accurate inventory count, daily and as required and performs bi-yearly physical count inventory.
Administrative:
Accurately completes and timely returns audit forms, email replies, timesheets and expense reports.
Attends and participates in regularly scheduled team meetings.
Accounts for all activities correctly using Field Point time reporting utility.
What we’re looking for:
Minimum seven (7) years specific experience working with OEM data center hardware
Successful Prior Field experience
Solid technical aptitude
Understanding of different OEM equipment with the ability to grasp new products/concepts
Ability to work long and/or unusual hours while maintaining effectiveness (manage being on call 24/7)
Demonstrated experience providing Customer Service (may be internal to organization) and ability to put the Customer Needs first
Proven ability to work independently, while exhibiting leadership and collaboration, when working with others
CompTIA A+ and Server + preferred
Must possess a valid driver’s license and an appropriate driving record based on the position travel requirements.
Must be able to assume an on-call status position and respond based on service level agreements within assigned territory.
Must be able to maintain regular working hours assigned, if site specific assignment.
Must be able to lift 50 pounds, stand and walk for extended periods of time, pull, lift, squat, reach, bend, and stoop to equipment, parts, and supplies. Again, this is an extremely active position that requires walking, standing, squatting, bending, and driving for over 50% of the workday.
Must be able to pass customer background screenings in addition to Park Place Technologies pre-employment screenings.
Bonus Points:
Certification and other OEM computer hardware certifications from major vendors such as Dell, IBM, HP, etc. are helpful
Bachelor's degree in related field (i.e. Computer Science, Engineering) or equivalent experience preferred
Education:
High School Degree required
Travel:
Must be able to travel to client sites up to 75% of the time within a predetermined territory radius (up to 4+ hours/one way by car).
Show more
Show less","Field service, Computer hardware, Testing, Diagnostics, Systems analysis, Storage area networks, System configurations, Customer service, Problem analysis, Troubleshooting, Operating systems, Remote system connection, Inventory management, Parts management, Audit forms, Timesheets, Expense reports, Team meetings, Time reporting, Data center hardware, OEM equipment, Long hours, Unusual hours, Customer needs, Independent work, Leadership, Collaboration, CompTIA A+, Server +, Driver's license, Oncall status, Regular working hours, Physical strength, Background screenings, Certifications, Dell, IBM, HP, Bachelor's degree, Computer science, Engineering","field service, computer hardware, testing, diagnostics, systems analysis, storage area networks, system configurations, customer service, problem analysis, troubleshooting, operating systems, remote system connection, inventory management, parts management, audit forms, timesheets, expense reports, team meetings, time reporting, data center hardware, oem equipment, long hours, unusual hours, customer needs, independent work, leadership, collaboration, comptia a, server, drivers license, oncall status, regular working hours, physical strength, background screenings, certifications, dell, ibm, hp, bachelors degree, computer science, engineering","audit forms, bachelors degree, background screenings, certifications, collaboration, comptia a, computer hardware, computer science, customer needs, customer service, data center hardware, dell, diagnostics, drivers license, engineering, expense reports, field service, hp, ibm, independent work, inventory management, leadership, long hours, oem equipment, oncall status, operating systems, parts management, physical strength, problem analysis, regular working hours, remote system connection, server, storage area networks, system configurations, systems analysis, team meetings, testing, time reporting, timesheets, troubleshooting, unusual hours"
"Experienced Software Engineer, Enterprise Data and Applications",Principal Financial Group,"Des Moines, IA",https://www.linkedin.com/jobs/view/experienced-software-engineer-enterprise-data-and-applications-at-principal-financial-group-3783938712,2023-12-17,Iowa,United States,Mid senior,Remote,"What You'll Do
We're looking for a Software Engineer to join our Enterprise Data and Analytics (EDA) team. In this role, you’ll join a group of engineers in our data space focusing on our Master Data Management platform. Here in EDA, we are at the intersection of many strategic and digital initiatives that impact the enterprise! This provides a unique opportunity to provide highly valuable and critically important solutions to Principal as an enterprise while gaining exposure to cross functional engineers and teams.
If you like working with data and understanding how data flows through our various systems, then this is just the team for you. If you don't know the specific tools that we use, that's fine as we are happy to mentor and train someone who knows a thing or two about object-oriented development and has an innate curiosity and eagerness to learn and grow. As a software engineer in our data space, you'll be performing many functions, but not limited to eliciting requirements, developing software and integrating data for new vendor based and in-house systems, and providing operational support.
You'll have the opportunity to:
Embrace a Product Mentality by focusing on outcomes over outputs, pursue fast feedback loops, and deliver solutions iteratively with low risk and low cost
Grow our DevOps approach and culture by continuously maturing and optimizing our SDLC through automation and safe/frequent delivery
Engage in all facets of software engineering from understanding the problem, evaluating designs, creating the solution, validating the outcome, etc.
Understand and continue driving our journey to a cloud-first technology community
Appreciate and promote Cloud Engineering – AWS PaaS, cloud integration patterns, cloud security, cloud operations, etc.
Network and communicate with cross-functional teams and collaborate with both IT and non-IT partners
Learn new technology and continuously grow through creative solutioning
Operating at the intersection of financial services and technology, Principal builds financial tools that help our customers live better lives. We take pride in being a purpose-led firm, motivated by our mission to make financial security accessible to all. Our mission, integrity, and customer focus have made us a trusted leader for more than 140 years.
As Principal continues to modernize its systems, this role will offer you an exciting opportunity to build solutions that will directly impact our long-term strategy and tech stack, all while ensuring that our products are robust, scalable, and secure!
Who You Are
Associate's or bachelor's degree (preference in a computer science, technology, engineering or math-related field) or equivalent work experience
3+ years of engineering experience in modern object oriented technologies
Experience/exposure to data and/or data technologies (more specifically, how that data moves from system to system)
Strong motivation for continuous learning, mentoring, problem solving, analytical thinking, and helping others grow along with you
Passion for working in a highly collaborative environment to solve problems and deliver customer value
Rotational on-call support is required
Skills That Will Help You Stand Out
Cloud technologies (AWS)
Enterprise level environment experience
Experience with the following technologies and tools
Programming (preference Java)
Master Data Management (Informatica)
ETL (PowerCenter)
Data engineering (data management, data transformation, data modeling, SQL, data lake, data warehousing)
DevOps practices (TDD, CI/CD, etc.)
Salary Range Information
Salary ranges below reflect targeted base salaries. Non-sales positions have the opportunity to participate in a bonus program. Sales positions are eligible for sales incentives, and in some instances a bonus plan, whereby total compensation may far exceed base salary depending on individual performance. Actual compensation for all roles will be based upon geographic location, work experience, education, licensure requirements and/or skill level and will be finalized at the time of offer.
Salary Range (Non-Exempt expressed as hourly; Exempt expressed as yearly)
$77350 - $182400 / year
Additional Information
Our Engineering Culture
Through our product-driven Agile/Lean DevOps environment, we’ve fostered a culture of innovation and experimentation across our development teams. As a customer-focused organization, we work closely with our end users and product owners to understand and rapidly respond to emerging business needs.
Collaboration is embedded into everything we do – from the products we develop to the quality service we provide. We’re driven by the belief that diversity of thought, background, and perspective is critical to creating the best products and experiences for our customers.
Job level
We’ll consider talent at the next levels with the right experiences and skills.
Hours
This team has Tuesday-Thursday meetings starting at 7:30am CST.
Work Environments
This role offers in-office, hybrid (blending at least three office days in a typical workweek), and remote work arrangements (only if residing more than 30 miles from Des Moines, IA, or Charlotte, NC). You’ll work with your leader to figure out which option may align best based on several factors.
Work Authorization/Sponsorship
At this time, we're not considering applicants that need any type of immigration sponsorship (additional work authorization or permanent work authorization) now or in the future to work in the United States. This includes, but IS NOT LIMITED TO: F1-OPT, F1-CPT, H-1B, TN, L-1, J-1, etc. For additional information around work authorization needs please use the following links.
Nonimmigrant Workers and Green Card for Employment-Based Immigrants
Investment Code of Ethics
For Principal Asset Management positions, you’ll need to follow an Investment Code of Ethics related to personal and business conduct as well as personal trading activities for you and members of your household. These same requirements may also apply to other positions across the organization.
Experience Principal
At Principal, we value connecting on both a personal and professional level. Together, we’re imagining a more purpose-led future for financial services – and that starts with you. Our success depends on the unique experiences, backgrounds, and talents of our employees. And we support our employees the same way we support our customers: with comprehensive, competitive benefit offerings crafted to protect their physical, financial, and social well-being. Check out our careers site to learn more about our purpose, values and benefits.
Principal is an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
Posting Window
We will be accepting applications for at least 3 days from when the job was originally posted, after which we may keep open or remove the posting based upon applications we receive. Please submit applications in a timely manner as there is no guarantee the posting will be available beyond 3 days of the original posting date.
Original Posting Date
11/20/2023
LinkedIn Remote Hashtag
LinkedIn Hashtag
Show more
Show less","Java, Informatica, PowerCenter, SQL, Data engineering, Data management, DevOps, TDD, CI/CD, Cloud technologies, AWS, PaaS, Cloud security, Cloud operations, Enterprise level environment","java, informatica, powercenter, sql, data engineering, data management, devops, tdd, cicd, cloud technologies, aws, paas, cloud security, cloud operations, enterprise level environment","aws, cicd, cloud operations, cloud security, cloud technologies, data engineering, data management, devops, enterprise level environment, informatica, java, paas, powercenter, sql, tdd"
"Data Analyst at Des Moines area, IA",IVY TECH SOLUTIONS INC,Des Moines Metropolitan Area,https://www.linkedin.com/jobs/view/data-analyst-at-des-moines-area-ia-at-ivy-tech-solutions-inc-3787778187,2023-12-17,Iowa,United States,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
Data Analyst
Duration: long-term contract engagement
Location: Des Moines area, IA
Only w2 or 1099
Please send the resume to
or 847- 350-1008
a financial institution in the with onboarding a for a long-term contract engagement. The resource will be required to work onsite for at least three days weekly.
POSITION PURPOSE
The position's primary purpose is to produce timely, accurate, and reliable enterprise procurement and vendor-related information to various company stakeholders. The Reporting Analyst will oversee and actively engage in the development and review of diverse spending, OMWI reporting requirements, vendor diversity assessments, and associated metrics. In addition, she/he will contribute to process reengineering initiatives associated with Enterprise Procurement and Vendor Management.
Responsibilities/Duties/Function/Tasks
Manage departmental reporting and support outreach initiatives
Ensure accurate and timely report development and delivery
Evaluate regulatory requirements and design solutions to comply
Develop departmental metrics and associated reporting solutions
Understand and work within enterprise applications (e.g., Workday, ProcessUnity, etc.)
Perform other duties and projects as assigned
Qualifications
Bachelor’s degree in business or related field desired
Between 3 and 7 years of related experience
Familiar with regulatory environment
Strong written and verbal communication skills
Advanced Excel, Word, Power Point, and SQL reporting skills
High attention to detail
Advanced analytical skills
Works well independently and within a team
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
PH.
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
Twl57NmDVQ
Show more
Show less","SQL, Data Analysis, Reporting, Process Reengineering, Data Management, Excel, Word, Power Point, Advanced Analytics, Regulatory Compliance","sql, data analysis, reporting, process reengineering, data management, excel, word, power point, advanced analytics, regulatory compliance","advanced analytics, data management, dataanalytics, excel, power point, process reengineering, regulatory compliance, reporting, sql, word"
SA3 Data Engineer-locals,Steneral Consulting,"Des Moines, IA",https://www.linkedin.com/jobs/view/sa3-data-engineer-locals-at-steneral-consulting-3779237355,2023-12-17,Iowa,United States,Mid senior,Hybrid,"SA3 Data Engineer
6-12M Contract
Hybrid out of Des Moines, IA
Job Description
, a leader in the financial services industry, is actively seeking a skilled and motivated Data Engineer/ETL Developer for a contract position. As a crucial member of our data team, you will be instrumental in the design, implementation, and optimization of data pipelines, with a primary focus on leveraging the Snowflake platform. This position offers a hybrid work model, allowing you to collaborate from our Des Moines office or work remotely.
Key Responsibilities
ETL Development:
Design, develop, and maintain robust ETL processes, ensuring efficient extraction, transformation, and loading of data into Snowflake from diverse sources.
Uphold data quality, integrity, and reliability standards throughout the ETL lifecycle.
Snowflake Optimization:
Leverage in-depth knowledge of Snowflake architecture, features, and best practices to optimize performance, scalability, and efficiency of data pipelines and warehouse operations.
Data Modeling:
Collaborate with data architects to design and implement data models in Snowflake, aligning with business requirements and ensuring scalability for future growth.
Monitoring and Maintenance:
Implement proactive monitoring solutions to identify and address issues related to data quality, pipeline performance, and system health.
Conduct regular maintenance and troubleshooting of ETL processes.
Data Manipulation and Automation:
Demonstrate proficiency in SQL and scripting languages (Javascript, Python, etc.) for data manipulation and automation tasks.
Skills and Experience:
7 years of IT development experience.
5 years of experience in designing and implementing enterprise data solutions.
Strong understanding of data warehousing concepts, ETL principles, and data modeling.
Experience with various data integration tools and technologies.
Excellent problem-solving skills and the ability to troubleshoot complex issues.
Strong communication skills with the ability to work effectively in a collaborative team environment.
Preferred Qualifications
Experience with Informatica or Microsoft PowerBI is a plus.
Desired Office Location:
Des Moines, IA (Hybrid work model)
Show more
Show less","Data Engineer, ETL Developer, Snowflake, ETL, Data Warehousing, Data Modeling, SQL, Scripting Languages (Javascript Python), Informatica, Microsoft PowerBI","data engineer, etl developer, snowflake, etl, data warehousing, data modeling, sql, scripting languages javascript python, informatica, microsoft powerbi","dataengineering, datamodeling, datawarehouse, etl, etl developer, informatica, microsoft powerbi, scripting languages javascript python, snowflake, sql"
Hybrid Work - Need Senior Data Platform Engineer-Azure in Des Moines IA,Steneral Consulting,"Des Moines, IA",https://www.linkedin.com/jobs/view/hybrid-work-need-senior-data-platform-engineer-azure-in-des-moines-ia-at-steneral-consulting-3750853112,2023-12-17,Iowa,United States,Mid senior,Hybrid,"Senior Data Platform Engineer-Azure
Des Moines, IA - Hybrid 3 days a week - locals are highly preferred but will consider someone from Midwest who will relocate from day one to work onsite.
Communication and collaboration are key. They must be easily understood and able to speak well to their projects and technical experience.
Must have valid LinkedIn and Photo ID required with submission
Must Have’s: Must have everything or please do not send them to me.
Azure Microsoft Fabric -- end to end lifecycle.
Azure Azure SQL Database: Provisioning, performance tuning, scaling, and security.
Azure Cosmos DB: Understanding of NoSQL databases, partitioning, consistency models.
Azure Data Factory: Data integration and ETL processes.
Azure Blob Storage and Data Lake Storage: Management, performance, security, and data lifecycle.
Azure Stream Analytics: Real-time data streaming and analytics.
Azure Databricks & HDInsight: Big data analytics solutions. (lower priority)
Azure Synapse Analytics: Knowledge of data warehousing, data integration, and analytics.
SQL: Writing, optimizing, and debugging SQL queries.
Data modeling: Normalization, star schema, snowflake schema
Familiarity with SDKs and APIs associated with Azure data services.
Integration with other Azure services or third-party applications.
Experience in one or more programming languages like C#, Python, or Java can be beneficial.
Azure Monitor, Azure Log Analytics, and Application Insights.
DP-203 certification
Financial/Investment industry experience.
Job Description
Here are the skills sets for building out the Microsoft Azure Data Platform.
Azure Fundamentals:
Understanding of Azure subscriptions, resources, and resource groups.
Familiarity with Azure regions, availability zones, and the Azure portal.
Azure Data Services Knowledge of tool set:
Azure Microsoft Fabric -- end to end lifecycle.
Azure Azure SQL Database: Provisioning, performance tuning, scaling, and security.
Azure Cosmos DB: Understanding of NoSQL databases, partitioning, consistency models.
Azure Data Factory: Data integration and ETL processes.
Azure Blob Storage and Data Lake Storage: Management, performance, security, and data lifecycle.
Azure Stream Analytics: Real-time data streaming and analytics.
Azure Databricks & HDInsight: Big data analytics solutions. (lower priority)
Azure Synapse Analytics: Knowledge of data warehousing, data integration, and analytics.
Skills:
SQL: Writing, optimizing, and debugging SQL queries.
Data modeling: Normalization, star schema, snowflake schema
Familiarity with SDKs and APIs associated with Azure data services.
Integration with other Azure services or third-party applications.
Experience in one or more programming languages like C#, Python, or Java can be beneficial.
Azure Monitor, Azure Log Analytics, and Application Insights.
DP-203 certification
Optional but helpful:
Azure Active Directory and role-based access control (RBAC)
Tools like Azure Data Migration Service, SSIS (SQL Server Integration Services).
Strategies for migrating data from on-premises or other clouds to Azure.
Show more
Show less","Azure, Azure SQL Database, Azure Cosmos DB, Azure Data Factory, Azure Blob Storage, Azure Data Lake Storage, Azure Stream Analytics, Azure Databricks, Azure HDInsight, Azure Synapse Analytics, SQL, Data modeling, SDKs, APIs, C#, Python, Java, Azure Monitor, Azure Log Analytics, Application Insights, DP203 certification, Azure Active Directory, RBAC, Azure Data Migration Service, SSIS","azure, azure sql database, azure cosmos db, azure data factory, azure blob storage, azure data lake storage, azure stream analytics, azure databricks, azure hdinsight, azure synapse analytics, sql, data modeling, sdks, apis, c, python, java, azure monitor, azure log analytics, application insights, dp203 certification, azure active directory, rbac, azure data migration service, ssis","apis, application insights, azure, azure active directory, azure blob storage, azure cosmos db, azure data factory, azure data lake storage, azure data migration service, azure databricks, azure hdinsight, azure log analytics, azure monitor, azure sql database, azure stream analytics, azure synapse analytics, c, datamodeling, dp203 certification, java, python, rbac, sdks, sql, ssis"
Hybrid Work - Need Senior Data Platform Engineer-Azure in Des Moines IA,Steneral Consulting,"Des Moines, IA",https://www.linkedin.com/jobs/view/hybrid-work-need-senior-data-platform-engineer-azure-in-des-moines-ia-at-steneral-consulting-3758725468,2023-12-17,Iowa,United States,Mid senior,Hybrid,"Senior Data Platform Engineer-Azure
Des Moines, IA - Hybrid 3 days a week - locals are highly preferred but will consider someone from Midwest who will relocate from day one to work onsite.
Communication and collaboration are key. They must be easily understood and able to speak well to their projects and technical experience.
Must have valid LinkedIn and Photo ID required with submission
Must Have’s: Must have everything or please do not send them to me.
Azure Microsoft Fabric -- end to end lifecycle.
Azure Azure SQL Database: Provisioning, performance tuning, scaling, and security.
Azure Cosmos DB: Understanding of NoSQL databases, partitioning, consistency models.
Azure Data Factory: Data integration and ETL processes.
Azure Blob Storage and Data Lake Storage: Management, performance, security, and data lifecycle.
Azure Stream Analytics: Real-time data streaming and analytics.
Azure Databricks & HDInsight: Big data analytics solutions. (lower priority)
Azure Synapse Analytics: Knowledge of data warehousing, data integration, and analytics.
SQL: Writing, optimizing, and debugging SQL queries.
Data modeling: Normalization, star schema, snowflake schema
Familiarity with SDKs and APIs associated with Azure data services.
Integration with other Azure services or third-party applications.
Experience in one or more programming languages like C#, Python, or Java can be beneficial.
Azure Monitor, Azure Log Analytics, and Application Insights.
DP-203 certification
Financial/Investment industry experience.
Job Description
Here are the skills sets for building out the Microsoft Azure Data Platform.
Azure Fundamentals:
Understanding of Azure subscriptions, resources, and resource groups.
Familiarity with Azure regions, availability zones, and the Azure portal.
Azure Data Services Knowledge of tool set:
Azure Microsoft Fabric -- end to end lifecycle.
Azure Azure SQL Database: Provisioning, performance tuning, scaling, and security.
Azure Cosmos DB: Understanding of NoSQL databases, partitioning, consistency models.
Azure Data Factory: Data integration and ETL processes.
Azure Blob Storage and Data Lake Storage: Management, performance, security, and data lifecycle.
Azure Stream Analytics: Real-time data streaming and analytics.
Azure Databricks & HDInsight: Big data analytics solutions. (lower priority)
Azure Synapse Analytics: Knowledge of data warehousing, data integration, and analytics.
Skills:
SQL: Writing, optimizing, and debugging SQL queries.
Data modeling: Normalization, star schema, snowflake schema
Familiarity with SDKs and APIs associated with Azure data services.
Integration with other Azure services or third-party applications.
Experience in one or more programming languages like C#, Python, or Java can be beneficial.
Azure Monitor, Azure Log Analytics, and Application Insights.
DP-203 certification
Optional but helpful:
Azure Active Directory and role-based access control (RBAC)
Tools like Azure Data Migration Service, SSIS (SQL Server Integration Services).
Strategies for migrating data from on-premises or other clouds to Azure.
Show more
Show less","Azure, Microsoft Fabric, Azure SQL Database, Azure Cosmos DB, Azure Data Factory, Azure Blob Storage, Azure Data Lake Storage, Azure Stream Analytics, Azure Databricks, Azure HDInsight, Azure Synapse Analytics, SQL, Data modeling, SDKs, APIs, Azure services, Thirdparty applications, C#, Python, Java, Azure Monitor, Azure Log Analytics, Application Insights, DP203 certification, Azure Active Directory, RBAC, Azure Data Migration Service, SSIS, Data migration","azure, microsoft fabric, azure sql database, azure cosmos db, azure data factory, azure blob storage, azure data lake storage, azure stream analytics, azure databricks, azure hdinsight, azure synapse analytics, sql, data modeling, sdks, apis, azure services, thirdparty applications, c, python, java, azure monitor, azure log analytics, application insights, dp203 certification, azure active directory, rbac, azure data migration service, ssis, data migration","apis, application insights, azure, azure active directory, azure blob storage, azure cosmos db, azure data factory, azure data lake storage, azure data migration service, azure databricks, azure hdinsight, azure log analytics, azure monitor, azure services, azure sql database, azure stream analytics, azure synapse analytics, c, data migration, datamodeling, dp203 certification, java, microsoft fabric, python, rbac, sdks, sql, ssis, thirdparty applications"
Hybrid Work - Need Senior Data Platform Engineer-Azure in Des Moines IA,Steneral Consulting,"Des Moines, IA",https://www.linkedin.com/jobs/view/hybrid-work-need-senior-data-platform-engineer-azure-in-des-moines-ia-at-steneral-consulting-3751750122,2023-12-17,Iowa,United States,Mid senior,Hybrid,"Senior Data Platform Engineer-Azure
Des Moines, IA - Hybrid 3 days a week - locals are highly preferred but will consider someone from Midwest who will relocate from day one to work onsite.
Communication and collaboration are key. They must be easily understood and able to speak well to their projects and technical experience.
Must have valid LinkedIn and Photo ID required with submission
Must Have’s: Must have everything or please do not send them to me.
Azure Microsoft Fabric -- end to end lifecycle.
Azure Azure SQL Database: Provisioning, performance tuning, scaling, and security.
Azure Cosmos DB: Understanding of NoSQL databases, partitioning, consistency models.
Azure Data Factory: Data integration and ETL processes.
Azure Blob Storage and Data Lake Storage: Management, performance, security, and data lifecycle.
Azure Stream Analytics: Real-time data streaming and analytics.
Azure Databricks & HDInsight: Big data analytics solutions. (lower priority)
Azure Synapse Analytics: Knowledge of data warehousing, data integration, and analytics.
SQL: Writing, optimizing, and debugging SQL queries.
Data modeling: Normalization, star schema, snowflake schema
Familiarity with SDKs and APIs associated with Azure data services.
Integration with other Azure services or third-party applications.
Experience in one or more programming languages like C#, Python, or Java can be beneficial.
Azure Monitor, Azure Log Analytics, and Application Insights.
DP-203 certification
Financial/Investment industry experience.
Job Description
Here are the skills sets for building out the Microsoft Azure Data Platform.
Azure Fundamentals:
Understanding of Azure subscriptions, resources, and resource groups.
Familiarity with Azure regions, availability zones, and the Azure portal.
Azure Data Services Knowledge of tool set:
Azure Microsoft Fabric -- end to end lifecycle.
Azure Azure SQL Database: Provisioning, performance tuning, scaling, and security.
Azure Cosmos DB: Understanding of NoSQL databases, partitioning, consistency models.
Azure Data Factory: Data integration and ETL processes.
Azure Blob Storage and Data Lake Storage: Management, performance, security, and data lifecycle.
Azure Stream Analytics: Real-time data streaming and analytics.
Azure Databricks & HDInsight: Big data analytics solutions. (lower priority)
Azure Synapse Analytics: Knowledge of data warehousing, data integration, and analytics.
Skills:
SQL: Writing, optimizing, and debugging SQL queries.
Data modeling: Normalization, star schema, snowflake schema
Familiarity with SDKs and APIs associated with Azure data services.
Integration with other Azure services or third-party applications.
Experience in one or more programming languages like C#, Python, or Java can be beneficial.
Azure Monitor, Azure Log Analytics, and Application Insights.
DP-203 certification
Optional but helpful:
Azure Active Directory and role-based access control (RBAC)
Tools like Azure Data Migration Service, SSIS (SQL Server Integration Services).
Strategies for migrating data from on-premises or other clouds to Azure.
Show more
Show less","Azure, Microsoft Fabric, Azure SQL Database, Azure Cosmos DB, Azure Data Factory, Azure Blob Storage, Azure Data Lake Storage, Azure Stream Analytics, Azure Databricks, Azure HDInsight, Azure Synapse Analytics, SQL, Data modeling, SDKs, APIs, Azure Active Directory, Azure rolebased access control (RBAC), Azure Data Migration Service, SSIS (SQL Server Integration Services), C#, Python, Java, Azure Monitor, Azure Log Analytics, Application Insights, DP203 certification","azure, microsoft fabric, azure sql database, azure cosmos db, azure data factory, azure blob storage, azure data lake storage, azure stream analytics, azure databricks, azure hdinsight, azure synapse analytics, sql, data modeling, sdks, apis, azure active directory, azure rolebased access control rbac, azure data migration service, ssis sql server integration services, c, python, java, azure monitor, azure log analytics, application insights, dp203 certification","apis, application insights, azure, azure active directory, azure blob storage, azure cosmos db, azure data factory, azure data lake storage, azure data migration service, azure databricks, azure hdinsight, azure log analytics, azure monitor, azure rolebased access control rbac, azure sql database, azure stream analytics, azure synapse analytics, c, datamodeling, dp203 certification, java, microsoft fabric, python, sdks, sql, ssis sql server integration services"
Sr Data Engineer,Pella Corporation,"Johnston, IA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-pella-corporation-3724372294,2023-12-17,Iowa,United States,Mid senior,Hybrid,"JOB SUMMARY:
This role will be embedded on a cross-functional product Agile scrum team as the primary data engineer. Work with Data Architect(s) and other DAI personnel and takes the lead to develop and maintain data flows, data workflows and other code/logic to gather, create and deliver high quality reliable data to meet the needs of Pella’s business. These efforts will support Pella’s analytics and transactional needs. Additionally, this position will support Pella’s Data Enablement Technologies and Data Users with on-call responsibilities, direct end-user tickets and addressing performance or quality issues.
ESSENTIAL RESPONSIBILITIES:
Design and develop data pipelines to manage how data flows between disparate systems
Build data pipelines to feed analytics use cases, KPI or enterprise apps.
Develop data quality metrics and performs QC tests (system and visual) to verify data integrity.
Interface with architects, product managers/SMEs and product analysts to understand data needs and support the implementation of the business rules into transformation.
Document the data blending process along with the specifications and workflow/data lineage.
Perform continuous integration to ensure that every step of the pipeline is testable and automated
Lead cloud data migration, transformation and modeling projects, developing project plans and communicating project status through Agile process and in Jira for the cross-functional team
Collaborate with the business to understand backlog and refine use cases related to data management, BI reporting and data science deliverables. Research source system data, architecture and transactions.
Takes lead to perform detailed design (the Physical Data Model and transformations), based on understanding of the Logical Data Model (the business requirements)
Create design documents for data integration or data reporting projects
Develop new and improve existing processes to ensure service levels are being met
Support development of new or modify existing analytical reports
Analyze data integration problems, provide solutions and recommend corrective actions.
Analyze source system data structures and map them to target data warehouse schemas.
Must have excellent skills in requirements analysis, logical/physical modeling, data transformation and data modeling and technical governance design concepts.
Serve as a technical expert to data warehouse project teams and key business individuals for support of applications, tools, data integration, and ad-hoc analytics.
Participate in design and code reviews, documentation of design, and implementation of methodologies to ensure high quality deployments
Analyze application and data integration problems, provide solutions and recommend corrective actions.
Education/Experience
BS degree in Computer Science, Data Engineering, Software Engineering, or a related field. MBA beneficial.
5+ years' experience in data engineering / software development
QUALIFICATIONS:
Technical / functional skills (includes computer skills):
Expertise in Azure cloud technologies specifically Synapse, ADF, Delta Lake, Databricks or comparable technology experience within AWS, Snowflake and/or GCP
Experience working with architectural fabric of Salesforce or comparable CRM applications
Understanding of architecture of Data Quality, Metadata Management and Master Data Management
Understanding of Data Governance and Data Stewardship concepts
Understanding of dimensional data modeling and design as well as data population techniques for target structures such as Star Schemas.
Skilled in Python, SPARK and SQL to build production-grade data pipelines and tools
Experience navigating a modern data environment and working between on-prem & cloud technologies
Knowledge of the data science process and understanding of/experience with Data Engineering support for Data Science
Strong grasp of CI/CD operating practices
Experience operating within a Product Scrum Agile team
Experience with MS Office, Outlook, Jira
Leadership Skills:
Strong communication and collaboration skills, good project management methodology
Certifications or licenses: None
Travel Expected:
5-10% of time
Show more
Show less","Agile, Data Engineering, Data Pipelines, Data Quality, Data Integration, Data Modeling, Data Governance, Data Stewardship, Dimensional Data Modeling, Python, SPARK, SQL, Azure, Synapse, ADF, Delta Lake, Databricks, AWS, Snowflake, GCP, Salesforce, Data Quality, Metadata Management, Master Data Management, Star Schemas, CI/CD, Product Scrum Agile, MS Office, Outlook, Jira","agile, data engineering, data pipelines, data quality, data integration, data modeling, data governance, data stewardship, dimensional data modeling, python, spark, sql, azure, synapse, adf, delta lake, databricks, aws, snowflake, gcp, salesforce, data quality, metadata management, master data management, star schemas, cicd, product scrum agile, ms office, outlook, jira","adf, agile, aws, azure, cicd, data engineering, data governance, data integration, data quality, data stewardship, databricks, datamodeling, datapipeline, delta lake, dimensional data modeling, gcp, jira, master data management, metadata management, ms office, outlook, product scrum agile, python, salesforce, snowflake, spark, sql, star schemas, synapse"
"Data Engineer with Life Insurance industry experience || Des Moines, IA - Hybrid onsite from Day One",Steneral Consulting,"Des Moines, IA",https://www.linkedin.com/jobs/view/data-engineer-with-life-insurance-industry-experience-des-moines-ia-hybrid-onsite-from-day-one-at-steneral-consulting-3670708050,2023-12-17,Iowa,United States,Mid senior,Hybrid,"Data Engineer With Life Insurance Industry Experience.
Des Moines, IA - Hybrid onsite from Day One - Local candidates will be given priority.
Candidates must be able to work onsite/hybrid in Des Moines, IA from day one. Prefer candidates local to Iowa.
Please screen your candidates well to ensure they are an excellent technical fit, can speak to their experience well and are excellent communicators and collaborators that can be easily understood. This client is picky about that.
Must Have's: (Please only send me candidates that have all the Must Have's.)
Experience in API Development and Data Governance tools like EDC.
Proficient in developing APIs - using MuleSoft, to retrieve data from and send to cloud based applications
Able to understand and modify C# code mainly for administration and configuration of tools
Expertise in using and administering Data Governance tools like Axon, EDC, IDQ, MDM
Good communication skills Written and Verbal
Preferably in Des Moines, open for remote if the candidate is extremely good
Experience working in Azure DevOps
Life Insurance industry experience
Job Description
IT Contractor Requisition
Job Title
Data Engineer
Job Description: Preferred Office
Location: Des Moines
We are looking for a Data Engineer with experience in API Development and Data Governance tools like EDC. This team member will work with our business stakeholders and other data team members to create APIs to provide data to and from Data Governance tools like EDC and Profisee.
Desired Skill Years of Experience 5+ years
Proficient in developing APIs - using MuleSoft, to retrieve data from and send to cloud based applications
Able to understand and modify C# code mainly for administration and configuration of tools
Expertise in using and administering Data Governance tools like Axon, EDC, IDQ, MDM
Good communication skills Written and Verbal
Preferably in Des Moines, open for remote if the candidate is extremely good
Additional Preferred Experience
Technical background/aptitude to learn now tools and technologies
Experience working in Azure DevOps
Facilitating clear and effective communications across technical and non-technical individuals and teams at all levels of the organization.
Show more
Show less","Data governance tools, API Development, MuleSoft, C#, Axon, EDC, IDQ, MDM, Azure DevOps, Life Insurance, Cloudbased applications","data governance tools, api development, mulesoft, c, axon, edc, idq, mdm, azure devops, life insurance, cloudbased applications","api development, axon, azure devops, c, cloudbased applications, data governance tools, edc, idq, life insurance, mdm, mulesoft"
Data Engineer – Cloud Data Integration,Sammons Financial Group Companies,"Des Moines, IA",https://www.linkedin.com/jobs/view/data-engineer-%E2%80%93-cloud-data-integration-at-sammons-financial-group-companies-3778879320,2023-12-17,Iowa,United States,Mid senior,Hybrid,"Title:
Data Engineer - Cloud Data Integration
Location:
Chicago, IL; Des Moines, IA; Sioux Falls, SD; Fargo, ND; Remote
Position Overview:
We are seeking a highly skilled and motivated Data Engineer with expertise in cloud-based data integration to join our dynamic team. The ideal candidate will play a pivotal role in designing, developing, and maintaining robust data pipelines and systems to support our insurance and annuity products.
Responsibilities:
Design, implement, and manage scalable data pipelines for ingesting, transforming, and storing diverse data sets from various sources into our cloud-based data infrastructure.
Develops, tests, and deploys code using internal software development toolsets, including the code for deploying infrastructure and solutions for secure data storage, ETL pipelines, data catalogs and data queries.
Collaborate with cross-functional teams to understand data requirements and develop solutions that meet business objectives.
Optimize data workflows and processes for efficiency, reliability, cost and performance.
Ensure data quality, integrity, and security throughout the data lifecycle.
Stay updated with emerging technologies and best practices in data engineering, cloud services, and integration methodologies to drive continuous improvement.
Requirements:
Bachelor’s degree in Computer Science, Engineering, or a related field.
5+ years of IT development experience.
3+ years of experience in designing and implementing enterprise data solutions.
Proficiency in cloud data platforms such as AWS, Azure, or Google Cloud Platform.
Proven experience publishing and managing Kafka topics.
Proven experience as a Data Engineer, preferably in the insurance or financial services industry.
Strong expertise in data modeling, ETL/ELT processes, and data warehousing concepts.
Hands-on experience with programming languages (Python, Java, or Scala) and SQL.
Strong understanding of data governance, metadata management, data quality controls, data lineage and data security.
Experience working in an Agile environment with strong Scrum framework knowledge.
Excellent problem-solving skills and ability to work in a fast-paced, collaborative environment.
Preferred Qualifications:
Experience with Jupyter Notebooks.
Experience with API management.
Advanced degree in a relevant field.
Industry certifications related to cloud services or data engineering.
Experience with specific tools or technologies relevant to insurance data analysis or actuarial science.
Other Details:
$78,672 - $163,899 - Range includes data points from multiple labor markets. Specific range is dependent on the labor market where the incumbent will be hired to perform the position. Starting salary is dependent on candidate qualifications and experience. For a narrower salary range specific to your labor market, please inquire
Sammons Financial Group offers incentive programs for defined goals subject to eligibility and performance. Monetary rewards are based on individual and/or overall company performance
Our competitive benefit package includes: Health, Dental, Vision, Company Paid Retirement, PTO and Holiday Pay
Criminal background check required
Show more
Show less","Cloud Data Integration, Data Pipelines, ETL/ELT, Data Warehousing, Data Modeling, Data Governance, Metadata Management, Data Quality Control, Data Lineage, Data Security, Agile, Scrum, Kubernetes, Python, Java, Scala, SQL, Kafka, AWS, Azure, Google Cloud Platform, Jupyter Notebooks, API Management","cloud data integration, data pipelines, etlelt, data warehousing, data modeling, data governance, metadata management, data quality control, data lineage, data security, agile, scrum, kubernetes, python, java, scala, sql, kafka, aws, azure, google cloud platform, jupyter notebooks, api management","agile, api management, aws, azure, cloud data integration, data governance, data lineage, data quality control, data security, datamodeling, datapipeline, datawarehouse, etlelt, google cloud platform, java, jupyter notebooks, kafka, kubernetes, metadata management, python, scala, scrum, sql"
Senior Data Engineer,StoneX Group Inc.,"West Des Moines, IA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-stonex-group-inc-3752009089,2023-12-17,Iowa,United States,Mid senior,Hybrid,"Overview
The Data Platform Team looks to raise the level and productivity of data engineering and data science by building, scaling, and supporting our big data infrastructure with an emphasis on simple and efficient solutions on top of complex distributed data stores. As a contributing senior data engineer, you will assist in architecting, designing, and implementing components within our cloud data platform expanding our data assets while continuously improving the architecture and processes around our daily operations.
Responsibilities
Create cloud and big data technical design recommendations for developing and integrating new software and system technologies – from the physical layer through to the virtual layer – per written specifications; test, evaluate, engineer, implement and support said technologies.
Review, influence and contribute to new and evolving design, architecture, standards, and methods for operating and contributing to services within our big data ecosystem.
Add to our existing business and data models. Reviews existing designs and processes to highlight more efficient ways to complete existing workload more effectively through industry perspectives.
Drive technical innovation and efficiency in infrastructure operations through automation by assisting in improvements to continuous integration, continuous deployment and
Create cloud and big data technical design recommendations for developing and integrating new software and system technologies – from the physical layer through to the virtual layer – per written specifications; test, evaluate, engineer, implement and support those technologies
Collaborates with technical teams and utilizes system expertise to deliver technical solutions, continuously learning and evolving big data skillsets.
Monitors and evaluates overall strategic data infrastructure; tracks system efficiency and reliability; identifies and recommends efficiency improvements and mitigates operational vulnerabilities. Respond to and resolve emergent service problems. Design solutions using automation and self-repair rather than relying on alarming and human intervention
Qualifications
Pursuing a Bachelor’s degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline.
5-7 years experience developing software in a professional environment (preferably financial services but not required)
Exposure to Docker/Containers, microservices, distributed systems architecture, Kubernetes, and cloud computing preferably Azure.
Comfortable with core programming concepts and techniques (e.g. concurrency, memory management)
ETL tooling like Airflow and Databricks.
Experience in supporting API Gateways and building and consuming REST APIs along with other distribution technologies.
Familiarity with Financial Systems architecture/ecosystems, Real Time Market Data messaging and FIX Protocol a huge plus.
Foundational knowledge of data structures, algorithms, and designing for performance.
Competent in one of the following programming languages: Java, C# or Python (preferred) and willingness to learn and adopt new languages as necessary
Experience in database technology like MSSQL and one of key value and document databases like MongoDb, Redis, Dynamo Db, Casandra.
Monitoring/Observability concepts and tooling: APM, Distributed Tracing, Grafana, Splunk, Prometheus.
Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts.
Ability to work and potentially lead in an Agile methodology environment.
Show more
Show less","Data Engineering, Cloud Architecture, Big Data, Data Infrastructure, Automation, Continuous Integration, Continuous Deployment, Microservices, Distributed Systems, Kubernetes, Cloud Computing, Azure, Docker, Containers, Concurrency, Memory Management, Airflow, Databricks, REST APIs, FIX Protocol, Data Structures, Algorithms, Performance Optimization, Java, C#, Python, MSSQL, MongoDB, Redis, DynamoDB, Cassandra, APM, Distributed Tracing, Grafana, Splunk, Prometheus, Agile Methodology","data engineering, cloud architecture, big data, data infrastructure, automation, continuous integration, continuous deployment, microservices, distributed systems, kubernetes, cloud computing, azure, docker, containers, concurrency, memory management, airflow, databricks, rest apis, fix protocol, data structures, algorithms, performance optimization, java, c, python, mssql, mongodb, redis, dynamodb, cassandra, apm, distributed tracing, grafana, splunk, prometheus, agile methodology","agile methodology, airflow, algorithms, apm, automation, azure, big data, c, cassandra, cloud architecture, cloud computing, concurrency, containers, continuous deployment, continuous integration, data engineering, data infrastructure, data structures, databricks, distributed systems, distributed tracing, docker, dynamodb, fix protocol, grafana, java, kubernetes, memory management, microservices, mongodb, mssql, performance optimization, prometheus, python, redis, rest apis, splunk"
Senior Data Engineer - Cloud Data Integration,Sammons Financial Group Companies,"Des Moines, IA",https://www.linkedin.com/jobs/view/senior-data-engineer-cloud-data-integration-at-sammons-financial-group-companies-3784922737,2023-12-17,Iowa,United States,Mid senior,Hybrid,"Title:
Senior Data Engineer - Cloud Data Integration
Location
:  Chicago, IL; Des Moines, IA; Sioux Falls, SD or Remote
Position Overview:
We are seeking a highly skilled and motivated Data Engineer with expertise in cloud-based data integration to join our dynamic team. The ideal candidate will play a pivotal role in designing, developing, and maintaining robust data pipelines and systems to support our insurance and annuity products.
Responsibilities:
Design, implement, and manage scalable data pipelines for ingesting, transforming, and storing diverse data sets from various sources into our cloud-based data infrastructure.
Develops, tests, and deploys code using internal software development toolsets, including the code for deploying infrastructure and solutions for secure data storage, ETL pipelines, data catalogs and data queries.
Collaborate with cross-functional teams to understand data requirements and develop solutions that meet business objectives.
Optimize data workflows and processes for efficiency, reliability, cost and performance.
Ensure data quality, integrity, and security throughout the data lifecycle.
Stay updated with emerging technologies and best practices in data engineering, cloud services, and integration methodologies to drive continuous improvement.
Requirements:
Bachelor’s degree in Computer Science, Engineering, or a related field.
10+ years of IT development experience.
5+ years of experience in designing and implementing enterprise data solutions.
Proficiency in cloud data platforms such as AWS, Azure, or Google Cloud Platform.
Proven experience publishing and managing Kafka topics.
Proven experience as a Data Engineer, preferably in the insurance or financial services industry.
Strong expertise in data modeling, ETL/ELT processes, and data warehousing concepts.
Hands-on experience with programming languages (Python, Java, or Scala) and SQL.
Strong understanding of data governance, metadata management, data quality controls, data lineage and data security.
Experience working in an Agile environment with strong Scrum framework knowledge.
Excellent problem-solving skills and ability to work in a fast-paced, collaborative environment.
Preferred Qualifications:
Experience with Jupyter Notebooks.
Experience with API management.
Advanced degree in a relevant field.
Industry certifications related to cloud services or data engineering.
Experience with specific tools or technologies relevant to insurance data analysis or actuarial science.
Other Details:
$87,412 - $182,109 - Range includes data points from multiple labor markets. Specific range is dependent on the labor market where the incumbent will be hired to perform the position. Starting salary is dependent on candidate qualifications and experience. For a narrower salary range specific to your labor market, please inquire
Sammons Financial Group offers incentive programs for defined goals subject to eligibility and performance. Monetary rewards are based on individual and/or overall company performance
Our competitive benefit package includes: Health, Dental, Vision, Company Paid Retirement, PTO and Holiday Pay
Criminal background check required
Show more
Show less","Cloud Data Platforms, Kafka, Data Modeling, ETL/ELT, Data Warehousing, Python, Java, Scala, SQL, Data Governance, Metadata Management, Data Quality Control, Data Lineage, Data Security, Agile, Scrum, Jupyter Notebooks, API Management, Insurance Data Analysis, Actuarial Science","cloud data platforms, kafka, data modeling, etlelt, data warehousing, python, java, scala, sql, data governance, metadata management, data quality control, data lineage, data security, agile, scrum, jupyter notebooks, api management, insurance data analysis, actuarial science","actuarial science, agile, api management, cloud data platforms, data governance, data lineage, data quality control, data security, datamodeling, datawarehouse, etlelt, insurance data analysis, java, jupyter notebooks, kafka, metadata management, python, scala, scrum, sql"
Senior Azure Data Engineer,Global Atlantic Financial Group,"Des Moines, IA",https://www.linkedin.com/jobs/view/senior-azure-data-engineer-at-global-atlantic-financial-group-3768486697,2023-12-17,Iowa,United States,Mid senior,Hybrid,"All offices are currently open, and our employees are back 4 or 5 days a week in Hudson Yards, NY and 3 days a week in all other offices. If you have questions on this policy or the application process, please contact recruiting@gafg.com .
Company Overview
Global Atlantic Financial Group is a leader in the U.S. life insurance and annuity industry, serving the needs of individuals and institutions. Global Atlantic is a majority-owned subsidiary of KKR, a leading global investment firm that offers alternative asset management across multiple strategies and capital markets solutions.
Global Atlantic is looking for a diverse team of talented individuals who reinforce our culture of collaboration and innovation. We are dedicated to the career development of our people because we know they are critical to our long-term success. Join our team and come grow with us.
We use Greenhouse as our scheduling tool and communicate through their systems. At times, your email may block our communications. Please be sure to check your SPAM so that you do not miss critical information about our process, including scheduling.
Summary
The Finance Platform Developer role partners with the Actuary team, IT data team and primary vendor to provide data development, support, and administration for the Actuarial Modeling/Data Management Platform. This position will be accountable for managing the data lifecycle and development throughout the Actuarial Modeling process.
This position can sit in our Boston or Des Moines offices.
Key Responsibilities Will Include The Following
New Requests/Builds
Collaborate with Actuary Model Development to support any data needs for models
Collaborate with Actuary Valuation to support model analytics and approval workflow
Define backlog items and requirements for IT Data team and other supporting IT areas
Participate in testing and verification of technical solutions
General Support
Become a subject matter expert in Actuarial Modeling Platform Data Management capabilities
Is primary administrator of Actuarial Modeling Platform and appropriately manages required administrative tasks if needed
Responsible for the regular maintenance, updates, fixes and upgrades. This could include testing after upgrade or facilitating UAT
Ensures platform is running at optimal performance and levels
Maintain application run books to ensure appropriate documentation exists
Provide training to new and existing users on current and future functionality
Production Process
Manage and facilitate all incidents from Priority 1 to Priority 4, which includes, but not limited to, providing support to users of applications, responding to user-based questions, tickets and assists with troubleshooting issues within platform.
Manage day-to-day operational and tactical aspects of multiple projects or requests
Provides on-call assistance as requested, for after-hours issues and support needs
Resolve incidents if able to, otherwise escalate as needed (provide tier-2 incident response)
Skills
General development skills with proven experience across 2+ programming languages
Understanding of Life and Annuity insurance a plus
Experience working with technology vendors & Actuaries
Demonstrates excellent interpersonal communication skills and documentation skills
Proven Data management skills with understanding of Data flows across different platforms
Able to manage work across a development team
Work cross-functionally within IT, with an emphasis working with a data team
Self-disciplined, able to work independently, but also able to take direction when necessary
Self-motivated to identity and resolve issues and in advancing personal knowledge
Hands on experience with GITLAB
Qualifications
An undergraduate degree in an IT related field or similar combination or education and experience
Experience working on Life and Annuity products
2+ years’ experience in Data Integration and Data Management
Experience working in cloud environments
Hands on experience with SAS programming
Must have
Experience in the following technologies –Azure Data Factory and PowerBI
Experience in Data Integration and Data Management
Experience working with and actuarial or finance
Global Atlantic’s
base salary range
is determined through an analysis of similar positions in the external labor market. The annual base salary range provided in this posting for this position is a nationwide market range and represents a broad range of salaries for this role across the country. Base pay is just one component of Global Atlantic’s total compensation package for employees and at times we hire outside the boundaries of the salary range. Other rewards may include annual cash bonuses, long-term incentives (equity), generous benefits (including immediate vesting on employee contributions to a 401(k), as well as a company match on your contributions), and sales incentives. Actual compensation for all roles will be based upon geographic location, work experience, education, licensure requirements and/or skill level and will be finalized at the time of offer. Compensation for our more senior positions have a larger component of short-term cash bonus and long-term incentives. The
base salary
range for this role is $84,800 to $155,000.
TOTAL REWARDS STATEMENT
Global Atlantic’s total rewards package is reflective of our corporate values, particularly diversity, excellence and innovation, with a focus on inclusion, pay equity, and flexibility. We are proud to support your personal and professional growth and well-being through programs such as educational assistance, virtual physical therapy, remote/onsite fitness reimbursement, a medical second opinion program, pet insurance, military leave, parental leave, adoption assistance, fertility and family planning coverage. We strive to foster a culture of total well-being through community outreach and charitable giving programs.
We Are Active In Our Communities
New York: Red Hook Conservancy, Girls Who Invest, Outward Bound, Teach for America, StreetWise Partners,
Boston: Catie’s Closet, Project Bread, Thompson Island Outward Bound Education Center, Cradles to Crayons, and many others
Hartford: Braids and Company, Junior Achievement
Indianapolis: Elevate Indianapolis, Gleaners Food Bank and the Juvenile Diabetes Research Foundation
Batesville: So Loved Ripley County Foster Closet, Southeastern Indiana YMCA, Batesville Community Education Foundation, Southeastern Indiana Voices for Children, local area youth sports, as well as many others
Des Moines: United Way of Central Iowa, Meals from the Heartland, Oakridge Neighborhood, Community Support Advocates, and many others
Wayne: For Pete’s Sake, Chester County Food Bank, Habitat for Humanity Chester County, Brandywine SPCA, as well as others
Bermuda: Transformational Living Centre for Families
Social platforms provide an environment to collaborate with others and participate in friendly competitions towards achieving physical, emotional and financial well-being. Our highly competitive health, retirement, life and disability plans can be tailored to best suit your needs and those of your whole family.
Global Atlantic is committed to creating an inclusive environment where everyone can meaningfully contribute to our success. We are proud to be an equal opportunity employer and we do not discriminate in employment on any basis that is prohibited by federal, state or local laws. More than that, we strive to be inclusive of all backgrounds and experiences, which we feel gives us a competitive advantage in the market and within our firm. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, or veteran status.
Employees who require an accommodation to perform the essential functions of their job will participate in an interactive process which may include providing documentation. If you are hired and require an accommodation for any protected status, please email benefits@gafg.com .
Please click on the below links to learn more about Global Atlantic.
Global Atlantic Privacy Statement
Show more
Show less","General development skills, Programming languages, Life and Annuity insurance, Technology vendors, Actuarial Modeling Platform Data Management, GITLAB, SAS programming, Azure Data Factory, PowerBI, Data Integration, Data Management, Cloud environments","general development skills, programming languages, life and annuity insurance, technology vendors, actuarial modeling platform data management, gitlab, sas programming, azure data factory, powerbi, data integration, data management, cloud environments","actuarial modeling platform data management, azure data factory, cloud environments, data integration, data management, general development skills, gitlab, life and annuity insurance, powerbi, programming languages, sas programming, technology vendors"
Senior Software Engineer (Data),FalconX,Elmira-Corning Area,https://www.linkedin.com/jobs/view/senior-software-engineer-data-at-falconx-3774355249,2023-12-17,Corning,United States,Mid senior,Hybrid,"Who are we?
FalconX is the most advanced digital asset platform for institutions. We provide trade execution, credit & treasury management, prime offering and market making services. Given our global operations, industry-leading technology and deep liquidity, we have facilitated client transactions of $1 trillion in volume. Our products & services are regulated, compliant and trusted.
We are a team of engineers, product builders, institutional sales and trading leaders, operations experts, and business strategists. Our teammates have entrepreneurial experience and come from companies such as Google, Apple, Paypal, Citadel, Bridgewater, and Goldman Sachs. And, we embody our values: Think big; Drive bold outcomes; Be one team; Iterate with speed; and be an entrepreneur.
We prioritize learning. Outcomes are mission-critical, but we also believe that learning in success and in failure will drive our continued success. Our industry is emergent - there’s no shortage of experiments to get involved with and to continue growing and learning together.
Impact
As a Senior Software Engineer (Data) at our organization, you will play a pivotal role in building and optimizing cutting-edge Vertical AI solutions for the financial services sector. Your contributions will directly influence the innovation and efficiency within the financial industry by leveraging data to create intelligent, automated, and predictive systems. Through the use of state-of-the-art technologies like Databricks and OpenSearch, you will be instrumental in driving the transformation of financial services, making them more accessible, secure, and efficient.
Responsibilities
Design and Develop Scalable Solutions: Lead the design and development of scalable data pipelines and AI models, ensuring the integration of complex data sets that drive intelligent decision-making in the financial sector.
Collaboration and Leadership: Collaborate with cross-functional teams and stakeholders to identify business needs, and translate them into technical requirements, while fostering a culture of knowledge sharing and mentorship.
Data Management and Optimization: Utilize OpenSearch for efficient data management and optimization, ensuring data quality and consistency across various data sources.
Innovation and Research: Conduct research on emerging technologies and methodologies in the field of AI and data science, and propose innovative solutions to enhance the capabilities of the vertical AI platform.
Performance Monitoring and Optimization: Monitor the performance of data pipelines and AI models, implementing optimizations and enhancements to ensure high performance and reliability.
Security and Compliance: Ensure the security and privacy of sensitive financial data, adhering to regulatory requirements and implementing best practices in data governance and compliance.
Required Qualifications
Bachelor’s Degree in Computer Science or Related Field A bachelor’s degree in Computer Science, Information Technology, or a related field, with a strong background in data science and artificial intelligence.
Experience with Databricks: Hands-on experience with Databricks for big data analytics, including the development and deployment of machine learning models.
Expertise in OpenSearch: Proficiency in using OpenSearch for data indexing, search, and analytics, with the ability to optimize data storage and retrieval processes.
Software Development Skill** Strong software development skills with experience in languages such as Python, with a demonstrated ability to develop scalable and robust data solutions.
Industry Experience: At least 5-7 years of experience in the financial services sector, with a deep understanding of industry-specific challenges and opportunities.
Excellent Communication and Teamwork Skills: Excellent communication and teamwork skills, with the ability to work effectively in a collaborative and fast-paced environment.
Tech Stack:
Databricks
OpenSearch
Python
Big Data Technologies (Spark)
Cloud Platforms (AWS, Google Cloud)
Base pay for this role is expected to be between $190,000 and $265,000 USD. This expected base pay range is based on information at the time this post was generated. This role will also be eligible for other forms of compensation such as a performance linked bonus, equity, and a competitive benefits package. Actual compensation for a successful candidate will be determined based on a number of factors such as skillset, experience, and qualifications.
Inclusivity Statement
FalconX is committed to building a diverse, inclusive, equitable, and safe workspace for all people. Our roles are intended for people from all walks of life. We encourage all those interested in applying to our organization to submit an application regardless if you are missing some of the listed background requirements, skills, or experiences!
As part of our commitment to inclusivity, FalconX would like to acknowledge that the EEOC survey has limited potential responses that you can select. For legal reasons, FalconX must use this language to align with federal requirements, however, we want to ensure that you are able to provide a response to our own voluntary survey questions about your identity that best aligns with your most true self.
Show more
Show less","Databricks, OpenSearch, Python, Big Data Technologies, Spark, AWS, Google Cloud, Data Science, Artificial Intelligence, Software Development, Machine Learning, Cloud Platforms, Scalable Solutions, Data Pipelines, AI Models, Data Management, Data Optimization, Data Quality, Data Governance, Data Compliance, Financial Services","databricks, opensearch, python, big data technologies, spark, aws, google cloud, data science, artificial intelligence, software development, machine learning, cloud platforms, scalable solutions, data pipelines, ai models, data management, data optimization, data quality, data governance, data compliance, financial services","ai models, artificial intelligence, aws, big data technologies, cloud platforms, data compliance, data governance, data management, data optimization, data quality, data science, databricks, datapipeline, financial services, google cloud, machine learning, opensearch, python, scalable solutions, software development, spark"
"Data Analyst IN Summit, NJ",Collab Infotech,"Summit, NJ",https://www.linkedin.com/jobs/view/data-analyst-in-summit-nj-at-collab-infotech-3707518639,2023-12-17,Newark,United States,Associate,Onsite,"Our client, a Top Pharmaceutical company needs a
""Data Analyst
"" in Summit, NJ
ASAP
.
Job Description
Job Title: Data Analyst
Location: Summit, NJ
Duration: 12 Months
Pay Rate: $41 - $43/hr on W2
Job Description
50% onsite
Purpose And Scope Of Position
The DM analyst is responsible for evaluating Deviations to identify patterns and trends, monitor trend investigations E2E including CAPAS and EC to eliminate trends, perform analysis of deviation data and summarize information for Annual Product Quality Review and other required reports. Notification to management of identified trends.
REQUIRED COMPETENCIES: Knowledge, Skills, and Abilities:
Must have knowledge and experience with cGMP manufacturing, Quality, and compliance.
Must have experience with Deviations, Root Cause Analysis, and CAPAs.
Must have knowledge of data trending and tracking, including use of statistical analysis software a plus.
Proficient in Excel data mining and report creation.
Ability to use electronic Quality systems such as Infinity.
Junior to intermediate ability to interpret results and situations and articulate recommendations for resolution.
Knowledge of US and global cGMP requirements.
Excellent verbal and written communication skills.
Requires moderate direction to complete more complex tasks; completes routine tasks with little or no supervision.
Work is self-directed.
Confident in making decisions for non-routine issues.
Able to prepare written communications and communicate problems to management with clarity and accuracy.
Able to effectively multi-task.
Education And Experience
Associate degree or higher required, minimum of three years of experience in the pharmaceutical or related industry. Equivalent combination of education and experience acceptable.
Duties And Responsibilities
Responsible for trending of deviations at S12.
Facilitates Trending Governance meetings. Issues minutes and notifies senior leadership of any risks or delays.
Reviews all deviations and notifies management of any trends.
Must have ability to author reports, interpret results, and generate conclusions consistent with Quality risk management principles.
Knowledge of quality processes, including investigations, and CAPA management.
Able to effectively multi-task.
Show more
Show less","Data Analysis, Data Mining, Statistical Analysis Software, Excel, Infinity, Quality Systems, cGMP, Deviations, Root Cause Analysis, CAPAs, Quality Risk Management, Investigations","data analysis, data mining, statistical analysis software, excel, infinity, quality systems, cgmp, deviations, root cause analysis, capas, quality risk management, investigations","capas, cgmp, data mining, dataanalytics, deviations, excel, infinity, investigations, quality risk management, quality systems, root cause analysis, statistical analysis software"
Research Data Analyst,"RecruitTalent, LLC.","Brooklyn, NY",https://www.linkedin.com/jobs/view/research-data-analyst-at-recruittalent-llc-3727055796,2023-12-17,Newark,United States,Associate,Onsite,"Tasks & Duties
Create and implement data mining and matching program to proactively monitor and enhance the integrity of the Agency’s records.
Perform regular data mining of records in the systems. Data mine records for duplicates, mismatches, inconsistent data, and other types of outliers and anomalies. Monitor, identify, and correct gaps, inconsistencies, and anomalies in mapping among the various systems.
Match record extracts to extracts from other internal and external systems to correct information.
Leverage geocoding and address validation tools, including the NYC Department of City Planning (DCP) Geographic Online Address Translator (GOAT), to correct and/or normalize address information.
Track and document systematic issues affecting data integrity. Develop and propose enhancements to the back-end design of relevant applications to enhance these systems to prevent issues from recurring where possible. Provide data expertise and guidance to other programs and partners.
Provide guidance and training to the team on correcting records through the user interface. Use ad-hoc queries to achieve objectives and to assist other program areas to address individual high priority cases.
Required Skills
Minimum 4 Years Experience managing the extraction, manipulation, and analysis of data from large databases
Minimum 4 Years Advanced Microsoft Excel and Access
Minimum 4 Years SQL programming experience
Minimum 4 Years Proficiency with a statistical software package, such as R, SAS, Stata, SPSS, or MATLAB
Minimum 4 Years Python experience
Minimum 4 Years Able to work in a fast-paced environment and respond to multiple stakeholder groups
Minimum 4 Years Experience with database design and development
Minimum 4 Years Experience with administrative data and producing concise reports for decision makers
Minimum 4 Years Excellent organizational, interpersonal, and communication skills
Minimum 4 Years Ability to be creative, flexible, strategic, collaborative, and innovative
Show more
Show less","Data mining, Data matching, Data integrity, Data validation, Data normalization, Geocoding, Address validation, Data analysis, Statistical software, SQL, Python, R, SAS, Stata, SPSS, MATLAB, Microsoft Excel, Microsoft Access, Database design, Database development, Administrative data, Report writing, Organizational skills, Interpersonal skills, Communication skills, Creativity, Flexibility, Strategic thinking, Collaboration, Innovation","data mining, data matching, data integrity, data validation, data normalization, geocoding, address validation, data analysis, statistical software, sql, python, r, sas, stata, spss, matlab, microsoft excel, microsoft access, database design, database development, administrative data, report writing, organizational skills, interpersonal skills, communication skills, creativity, flexibility, strategic thinking, collaboration, innovation","address validation, administrative data, collaboration, communication skills, creativity, data integrity, data matching, data mining, data normalization, data validation, dataanalytics, database design, database development, flexibility, geocoding, innovation, interpersonal skills, matlab, microsoft access, microsoft excel, organizational skills, python, r, report writing, sas, spss, sql, stata, statistical software, strategic thinking"
Technology & Data Analyst,Greystone Monticello LLC,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/technology-data-analyst-at-greystone-monticello-llc-3747773807,2023-12-17,Newark,United States,Associate,Onsite,"Greystone Monticello LLC (together with its affiliates, “Greystone Monticello”) is a leading real estate lender and asset manager providing comprehensive capital solutions for healthcare, multifamily, and commercial real estate assets throughout the US. Greystone Monticello is seeking leaders and team players who can work in a collaborative environment and possess drive, integrity, creativity, compassion, and a strong work ethic.
We are looking for a Technology and Data Analytics Analyst for our New York City office to support the Head of Technology and Data Analytics, investment management teams such as originations, underwriting and asset management as well as finance, accounting, compliance, investor relations, and human resources. The Data Analytics and Technology Analyst’s primary responsibilities are:
Proactively analyze investment related data to answer key questions from internal and external stakeholders including executive management, investment team members, and investors
Develop custom investment related models & analysis reports across healthcare and multi-family real estate debt and equity for ongoing reporting and ad-hoc projects
Perform qualitative and quantitative research on public and proprietary data sets to develop insights, create presentations, and make actionable business recommendations
Evaluate individual investment and portfolio performance across asset class, geography, and other segmentations to identify key trends
Find, explore, and integrate new data sets from vendors and public sources while maintaining high standard for data integrity and normalization
Establish and execute procedures and controls for gathering data while also ensuring the excellence and reliability of imported data through quality control and quality assurance measures.
Break complex processes down into their individual components and identify areas where data and technology can increase efficiencies, effectiveness, and scalability
Assist with the implementation large data assimilation projects, such as portfolio data transfers
Sustain and oversee the data management systems critical to the firm's success.
Process confidential data and information according to internal guidelines
Job Requirements:
Bachelor’s Degree
Finance, accounting, credit, legal, real estate and/or business background
Established organizational skills and ability to simultaneously handle multiple projects
Extensive technical skills including, iLevel, Power BI, Tableau, SQL, Python
Experience sourcing and analyzing data through APIs, data scraping, and database querying
Ability to quickly learn new tools and technologies
Experience implementing and optimizing workflow management tools like Monday.com or JIRA preferred but not required
Interest in financing healthcare, senior housing, multi-family housing and/or renewable energy preferred
Effective oral and written communication and interpersonal skills to liaise with borrowers, financing counterparties, and other external parties
Advanced financial analytical proficiency along with the ability to “see the big picture”
Strong grasp of logic and data analytics
Passion for the firm and passion for what we do
Intellectual curiosity and a desire to understand the purpose behind their work
At Greystone Monticello, we believe that finding creative solutions for our clients comes from the collaboration of people with diverse backgrounds and perspectives. We strive to build an inclusive work environment that celebrates differences and empowers all individuals with opportunities to channel their entrepreneurial spirit. Greystone Monticello is an EEO employer.
Show more
Show less","iLevel, Power BI, Tableau, SQL, Python, Data scraping, API, Database querying, Monday.com, JIRA, Financial analytics, Data analytics, Logic","ilevel, power bi, tableau, sql, python, data scraping, api, database querying, mondaycom, jira, financial analytics, data analytics, logic","api, data scraping, dataanalytics, database querying, financial analytics, ilevel, jira, logic, mondaycom, powerbi, python, sql, tableau"
Data Center Operations Engineer,"Liberty Personnel Services, Inc.","Secaucus, NJ",https://www.linkedin.com/jobs/view/data-center-operations-engineer-at-liberty-personnel-services-inc-3698219138,2023-12-17,Newark,United States,Associate,Onsite,"Job Details:
Data Center Operations Engineer
My client is looking to hire a Data Center Operations Engineer for its Secaucus NJ location. In this role you will support linux and windows, telecom and security. Shift work is required. This is a permanent position.
If you are interested please forward your resume in word format to kevin@libertyjobs.com
Kevin McCarthy
#associate
#mid-senior
Show more
Show less","Linux, Windows, Telecom, Security","linux, windows, telecom, security","linux, security, telecom, windows"
Data Analyst,Motion Recruitment,"Woodcliff Lake, NJ",https://www.linkedin.com/jobs/view/data-analyst-at-motion-recruitment-3762684541,2023-12-17,Newark,United States,Associate,Onsite,"Our client is looking to hire for an
Ecommerce
Data Analyst
to join their team
onsite
, in
Woodcliff Lake, NJ!
Responsibilities:
Provide support in managing and maintaining digital analytics, opportunity identification and prioritization, hypothesis generation, test setup and reporting
Responsible for reporting and analysis of digital platforms and marketing campaigns against a holistic set of KPIs and benchmarks
Collecting and analyzing data from multiple sources such as customer databases, web analytics, market research, and surveys.
Creating reports and dashboards to track key performance indicators.
Identifying trends and recommending strategies to improve ecommerce performance.
Building and monitoring predictive models to optimize pricing and promotional decisions.
Enhancing customer segmentation and targeted marketing campaigns.
Understanding user behavior and leveraging insights to drive product design, development, and optimization.
Collaborating with teams across departments to ensure data accuracy and integrity.
Qualifications:
2+ years of direct and hands-on experience providing solutions.
Excellent analytical skills, be detail-oriented, and work well within a small, dynamic and data-driven team.
Solid understanding of database technologies, data analytics, and reporting tools.
Analytical skills to interpret data and draw meaningful insights.
Good communication and presentation skills.
Knowledge of machine learning and predictive modeling techniques is plus but not required.
Understanding of web analytics and customer segmentation techniques.
Hands-on experience with statistical platform such as Google Analytics, Ads
Benefits:
A competitive starting salary based on experience, with achievement based opportunities for annual bonuses and increases.
Opportunities for professional advancement. We value big thinking tied to practical, collaborative execution in a structured and growth oriented company.
Ongoing mentoring from senior staff and periodic opportunities to attend industry seminars and workshops.
Starting on the first day of hire, all employees can begin participating in our excellent Major Medical, Dental, Vision and Life Insurance plans.
Paid holiday and vacation time, which starts in the first year of employment and increases with tenure.
A modern, professional, suburban office space, concentrated work day (8:30-5:30) and business-attire environment.
We enjoy a professional, collegial and positive work atmosphere, sharing camaraderie and rooting for individual and collective success.
Show more
Show less","Data Analytics, Digital Analytics, Reporting, KPIs, Data Analysis, Data Mining, Data Visualization, Machine Learning, Predictive Modeling, Customer Segmentation, Web Analytics, Google Analytics, Google Ads, Statistical Analysis, SQL, Excel, Tableau, Power BI","data analytics, digital analytics, reporting, kpis, data analysis, data mining, data visualization, machine learning, predictive modeling, customer segmentation, web analytics, google analytics, google ads, statistical analysis, sql, excel, tableau, power bi","customer segmentation, data mining, dataanalytics, digital analytics, excel, google ads, google analytics, kpis, machine learning, powerbi, predictive modeling, reporting, sql, statistical analysis, tableau, visualization, web analytics"
"Staff Engineer, Data Infrastructure",Ro,"New York, NY",https://www.linkedin.com/jobs/view/staff-engineer-data-infrastructure-at-ro-3682689871,2023-12-17,Newark,United States,Associate,Remote,"Who We Are
Ro is a direct-to-patient healthcare company with a mission of helping as many patients as possible achieve their health goals. Ro is the only company to offer telehealth care, at-home diagnostic testing, labs, and pharmacy services nationwide. This is enabled by Ro's vertically integrated platform that helps patients achieve their goals through a convenient end-to-end healthcare experience spanning from diagnosis, to delivery of medication, to ongoing care. Since 2017, Ro has helped millions of patients in nearly every single county in the United States, including 98% of primary care deserts.
Ro was recognized as a CNBC Disruptor 50 in 2022, listed by Inc. Magazine as a Best Place to Work in 2022 for our third consecutive year, and named one of FORTUNE's 2022 Best Medium Workplaces.
We are looking for an experienced Staff Software Engineer to join our Data Infrastructure team. As an engineer on this team, you will be responsible for designing, developing, and maintaining the data infrastructure platform that empowers data analysts, data scientists, and software engineers to build data intelligence and data-powered applications. You take pride in working with stakeholders across the organization to understand their pain points and aspirations and use that information to build solutions to make working with data more convenient and secure, driving the vision for our platform.
What You'll Do
Design, develop, and maintain data infrastructure capabilities that enable users to effectively and securely produce, consume, and share data generated by systems and third-party tools.
Design, develop, and maintain new functionalities and improvements for our Kafka-based event streaming platform used to enable complex workflows involving several different microservices.
Collaborate with cross-functional teams to understand pain points and ways they can be more effective, translating those into new capabilities or improvements for the data infrastructure.
Support our users by providing technical guidance and support in utilizing our tooling.
Monitor and troubleshoot our systems to proactively identify and resolve production issues.
Stay current with industry best practices and emerging technologies related to data infrastructure and provide recommendations for improvement within the platform.
What You'll Bring
6+ years of full-time working experience building production-ready systems, ideally with experience building platforms.
Expertise in Kafka, including experience with stream processing and event-driven architectures.
Experience in infrastructure as code tools like Terraform and Pulumi.
Strong Python programming skills.
Excellent communication and interpersonal skills, with the ability to work effectively with cross-functional teams.
Bonus Points
Go programing skills
Knowledge of containerization technologies like Docker and orchestration frameworks like Kubernetes.
Experience with AWS services, including EKS, S3, and IAM.
Experience managing and optimizing Snowflake data warehouse.
Knowledge of data modeling, data warehousing concepts, and SQL using dbt.
Experience automating workflows with GitHub Actions, Webhooks, etc.
Understanding of data governance best practices.
Experience with Looker and Metabase for data analytics and visualization.
We've Got You Covered
Full medical, dental, and vision insurance + OneMedical membership
Healthcare and Dependent Care FSA
401(k) with company match
Flexible PTO
Wellbeing + Learning & Growth reimbursements
Paid parental leave + Fertility benefits
Pet insurance
Student loan refinancing
Virtual resources for mindfulness, counseling, fitness, and physical therapy
The target base salary for this position ranges from $194,200 to $234,000, in addition to a competitive equity and benefits package (as applicable). When determining compensation, we analyze and carefully consider several factors, including location, job-related knowledge, skills and experience. These considerations may cause your compensation to vary.
Ro recognizes the power of in-person collaboration, while supporting the flexibility to work anywhere in the United States. For our Ro’ers in the tri-state (NY) area, you will join us at HQ on Tuesdays and Thursdays. For those outside of the tri-state area, you will be able to join in-person collaborations throughout the year (i.e., during team on-sites).
At Ro, we believe that our diverse perspectives are our biggest strengths — and that embracing them will create real change in healthcare. As an equal opportunity employer, we provide equal opportunity in all aspects of employment, including recruiting, hiring, compensation, training and promotion, termination, and any other terms and conditions of employment without regard to race, ethnicity, color, religion, sex, sexual orientation, gender identity, gender expression, familial status, age, disability and/or any other legally protected classification protected by federal, state, or local law.
See our California Privacy Policy here .
Show more
Show less","Kafka, Python, Terraform, Pulumi, Go, Docker, Kubernetes, AWS, EKS, S3, IAM, Snowflake, SQL, dbt, GitHub Actions, Webhooks, Looker, Metabase","kafka, python, terraform, pulumi, go, docker, kubernetes, aws, eks, s3, iam, snowflake, sql, dbt, github actions, webhooks, looker, metabase","aws, dbt, docker, eks, github actions, go, iam, kafka, kubernetes, looker, metabase, pulumi, python, s3, snowflake, sql, terraform, webhooks"
"Senior Software Engineer, Orders Data Platform",Square,"New York, NY",https://www.linkedin.com/jobs/view/senior-software-engineer-orders-data-platform-at-square-3785217074,2023-12-17,Newark,United States,Associate,Remote,"Company Description
Since we opened our doors in 2009, the world of commerce has evolved immensely, and so has Square. After enabling anyone to take payments and never miss a sale, we saw sellers stymied by disparate, outmoded products and tools that wouldn’t work together.
So we expanded into software and started building integrated, omnichannel solutions – to help sellers sell online, manage inventory, offer buy now, pay later functionality through Afterpay, book appointments, engage loyal buyers, and hire and pay staff. Across it all, we’ve embedded financial services tools at the point of sale, so merchants can access a business loan and manage their cash flow in one place. Afterpay furthers our goal to provide omnichannel tools that unlock meaningful value and growth, enabling sellers to capture the next generation shopper, increase order sizes, and compete at a larger scale.
Today, we are a partner to sellers of all sizes – large, enterprise-scale businesses with complex operations, sellers just starting, as well as merchants who began selling with Square and have grown larger over time. As our sellers grow, so do our solutions. There is a massive opportunity in front of us. We’re building a significant, meaningful, and lasting business, and we are helping sellers worldwide do the same.
Job Description
We are the Orders Data Platform team, a team whose mission is to help merchants of all sizes gain insights from their sales data through a variety of interfaces. We sit at the center of critical domains and data flows, and we’re building a multi-layered platform to achieve our goals with Square-wide impact. We’re looking for a senior engineer who can help us build new platform features to dramatically improve user-facing search and reporting experiences at Square.
We provide a unified view of Orders/Orders-adjacent data at Square, and power a variety of interfaces for our teams and third-party developers to access that data. You’ll work with teams to understand requirements, and ensure the platform we’re building works across several distinct use cases.
To power that view of Orders, we’re also building a general-purpose Elasticsearch- and GraphQL-based search and reporting platform that can operate at Square scale. This general-purpose platform already powers search and reporting for a variety of teams at Square in production, and we’re continuing to evolve it for our collective use cases. We build in the open, and look forward to open-sourcing this platform in 2024 for even wider impact.
You will:
Work with Product and partners across Square to identify platform requirements, and work within the engineering team to develop the corresponding features
Provide high-quality hands-on contributions across multiple code bases
Identify technical and architectural end states for the project, and influence/evolve the code case in those directions
Play a key role in choosing technical investments for the team
Qualifications
You Have:
5+ years of software development experience
Familiarity with architecting/implementing Java-/Kotlin-based backend services
Strong product intuition and interest, with platform-building experience
Strategic leadership experience on medium/large-scale software projects
Interest and experience in mentoring other engineers
Even better:
Experience with real-time data streaming platforms such as Kafka and Kinesis
Experience and familiarity with Kotlin, GraphQL, Elasticsearch, and AWS technologies
Proficiency in large-scale Ruby projects, or a history and interest in quickly learning new technologies and stacks
History of contributions to open-source projects
Familiarity with the payments-processing domain
Technologies we use within Orders Data Platform:
Java, Kotlin, Ruby
GraphQL
Elasticsearch, DynamoDb
Terraform, AWS Lambda, SQS, Cloudwatch
Additional Information
Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.
Zone A: USD $167,300 - USD $204,500
Zone B: USD $158,900 - USD $194,300
Zone C: USD $150,600 - USD $184,000
Zone D: USD $142,200 - USD $173,800
To find a location’s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information.
Full-time employee benefits include the following:
Healthcare coverage (Medical, Vision and Dental insurance)
Health Savings Account and Flexible Spending Account
Retirement Plans including company match
Employee Stock Purchase Program
Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance
Paid parental and caregiving leave
Paid time off (including 12 paid holidays)
Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees)
Learning and Development resources
Paid Life insurance, AD&D, and disability benefits
These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.
US and Canada EEOC Statement
We’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.
We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.
Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our
I+D page
.
Additionally, we consider qualified applicants with criminal histories for employment on our team, and always assess candidates on an individualized basis.
We’ve noticed a rise in recruiting impersonations across the industry, where individuals are sending fake job offer emails. Contact from any of our recruiters or employees will always come from an email address ending with @block.xyz, @squareup.com, @tidal.com, or @afterpay.com, @clearpay.co.uk.
Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.
Show more
Show less","Java, Kotlin, Ruby, GraphQL, Elasticsearch, DynamoDb, Terraform, AWS Lambda, SQS, Cloudwatch, Kafka, Kinesis","java, kotlin, ruby, graphql, elasticsearch, dynamodb, terraform, aws lambda, sqs, cloudwatch, kafka, kinesis","aws lambda, cloudwatch, dynamodb, elasticsearch, graphql, java, kafka, kinesis, kotlin, ruby, sqs, terraform"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Elizabeth, NJ",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783184540,2023-12-17,Newark,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Elizabet-DataScientist.012
Show more
Show less","Python, JavaScript, JSON, R, OOP, Generative AI, Technology, Data Science, Product Development, Research, Communication, Stakeholder Management, Agile, Scrum, SQL, Machine Learning, Deep Learning, Natural Language Processing, Cloud Computing, DevOps, Automation","python, javascript, json, r, oop, generative ai, technology, data science, product development, research, communication, stakeholder management, agile, scrum, sql, machine learning, deep learning, natural language processing, cloud computing, devops, automation","agile, automation, cloud computing, communication, data science, deep learning, devops, generative ai, javascript, json, machine learning, natural language processing, oop, product development, python, r, research, scrum, sql, stakeholder management, technology"
Staff Python + Data Engineer (AI SaaS),EvolutionIQ,"New York, NY",https://www.linkedin.com/jobs/view/staff-python-%2B-data-engineer-ai-saas-at-evolutioniq-3743013716,2023-12-17,Newark,United States,Mid senior,Onsite,"About Us:
EvolutionIQ’s mission is to improve the lives of injured and disabled workers and enable them to return to the workforce, saving billions of dollars in avoidable costs and lost productivity to the US and global economies and make insurance more affordable for everyone. We are currently experiencing massive growth and to accomplish our goals, we are hiring world-class talent who want to help build and scale internally, and transform the insurance space. We’re backed by First Round Capital, FirstMark Capital, Foundation Capital, Brewer Lane Ventures, and have been named as Inc.’s top places to work!
Our Team:
We are founded by a senior Google AI expert and a Bridgewater Associates Algorithmic Investor & Stanford MBA. We’re not looking for employees. We’re looking for partners in work, partners in culture-building, and partners in the future of data-driven insurance. The development team consists of world class engineers and leaders from companies like Google and Bloomberg. Each individual has had great success building large scale enterprise software and is now excited to try their hand at transforming the insurance industry.
Job Summary:
We are looking for a Lead Engineer for our Workers Comp Engineering team who will play an integral role in securing, architecting, and managing our highly sensitive insurance data. This position is tasked with overseeing our foundational datasets, data models, general software architecture, and analytics. The ideal candidate will have considerable experience in creating and managing secure data platforms, a strong engineering background, and a demonstrated record of technical leadership and effective communication.
In this critical role, you will not only ensure the robustness and reliability of our data systems, but also their security and compliance with stringent industry regulations. You will navigate the complexities of insurance data, bringing technical excellence and a security-first approach to safeguard our information assets. Your keen eye for security will be instrumental in protecting our company, customers, and stakeholders, while your technical expertise will shape the future of our data platform architecture.
Key Responsibilities:
Architect, design, and implement robust, secure, scalable, and high-quality data platforms, ensuring the availability, integrity, and confidentiality of the information.
Lead the development and maintenance of data pipelines, including personally coding and building the most critical components.
Work closely with product engineers, data scientists, analysts, and other stakeholders to understand data needs and deliver on those needs.
Define, design, and improve foundational data models to be used across the company to enable feature development and analytics.
Continuously improve our data quality toolkit
Provide guidance and technical leadership to the data engineering team, promoting continual team growth and individual team member skill development.
Be a role model for all engineers and provide mentorship as needed
Drive proof of concepts and experiments to explore new technologies that can level up the entire organization
Requirements:
7+ years of industry experience, holding staff/principal/lead level roles in Software Engineer or Data Engineer, with a focus in building scalable, mission critical, data platforms
Strong written and verbal communication skills
Extensive Python development experience
Experience with distributed data/computing tools, such as: Spark, Airflow, dbt
Proven track record of establishing engineering best practices for both coding and architecture
Experience building out systems and processes to enable secure handling of highly sensitive data
Experience using modern big data storage technologies such as Apache Parquet or Avro
Strong familiarity with modern data warehouse such as BigQuery or Snowflake
Ambitious, collaborative, and empathetic values
Even Better if You Have:
You have at least 3+ years experience in deploying systems on GCP or AWS
Experience with MLOps, such as feature engineering and model serving
You have worked with Dagster/Airflow, BigQuery, GCP, Terraform, Kubernetes, sklearn, keras/TensorFlow/pytorch, dbt, data modeling, Python/Pandas data frameworks, and scalable technical concepts/solutions
Work-life, Culture & Perks:
Compensation: The range is $210-240K depending on a candidate’s background and experience.
Well-Being: Full medical, dental, vision, short- & long-term disability, 401k matching. 100% of the employee contribution up to 3% and 50% of the next 2%
Work/Life Balance: For this role we are hoping this person can work out of the NYC office regularly with much of our leadership with flexibility. We also have a flexible vacation policy.
Home & Family: 100% paid parental leave (4 months for primary caregivers and 3 months for secondary caregivers), sick days, paid time off. For new parents returning to work we offer a flexible schedule. We also offer sleep training to help you and your family navigate life schedules with a newborn
Office Life: Catered lunches, happy hours, and pet-friendly office space. $500 for your in home office setup and $200/year for upgrades every year after your initial setup
Growth & Training: $1,000/year for each employee for professional development, as well as upskilling opportunities internally
Sponsorship: We are open to sponsoring candidates currently in the U.S. who need to transfer their active H1-B visa
EvolutionIQ appreciates your interest in our company as a place of employment. EvolutionIQ is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees
Show more
Show less","Python, Spark, Airflow, dbt, Apache Parquet, Avro, BigQuery, Snowflake, GCP, AWS, MLOps, Feature engineering, Model serving, Dagster, Terraform, Kubernetes, Sklearn, Keras, TensorFlow, Pytorch, Data modeling, Data frameworks, Scalable technical concepts/solutions","python, spark, airflow, dbt, apache parquet, avro, bigquery, snowflake, gcp, aws, mlops, feature engineering, model serving, dagster, terraform, kubernetes, sklearn, keras, tensorflow, pytorch, data modeling, data frameworks, scalable technical conceptssolutions","airflow, apache parquet, avro, aws, bigquery, dagster, data frameworks, datamodeling, dbt, feature engineering, gcp, keras, kubernetes, mlops, model serving, python, pytorch, scalable technical conceptssolutions, sklearn, snowflake, spark, tensorflow, terraform"
Senior Azure Data Engineer (Hybrid),"The Dignify Solutions, LLC","Jersey City, NJ",https://www.linkedin.com/jobs/view/senior-azure-data-engineer-hybrid-at-the-dignify-solutions-llc-3769523367,2023-12-17,Newark,United States,Mid senior,Onsite,"8+ yrs of Experience Java, Azure Databricks, Azure Data Factory (ADF), Cosmos DB, Kafka messaging, Azure SQL
10+ yrs of Experience in architecting data integration workflows in Azure Stack Key technologies: Azure Databricks, Azure Data Factory (ADF), Cosmos DB, Kafka messaging, Azure SQL Banking experience is a big plus
Show more
Show less","Java, Azure Databricks, Azure Data Factory (ADF), Cosmos DB, Kafka, Azure SQL, Data integration, Azure Stack","java, azure databricks, azure data factory adf, cosmos db, kafka, azure sql, data integration, azure stack","azure data factory adf, azure databricks, azure sql, azure stack, cosmos db, data integration, java, kafka"
Big Data Engineer,Barclays Corporate & Investment Bank,"Whippany, NJ",https://www.linkedin.com/jobs/view/big-data-engineer-at-barclays-corporate-investment-bank-3768526224,2023-12-17,Newark,United States,Mid senior,Onsite,"Whippany, New Jersey
Barclays Services Corp.
What will you be doing?
Barclays Services Corp seeks Big Data Engineer in Whippany, New Jersey (multiple positions available):
Perform data collection and data modeling with business stockholders to prepare business requirement documents and develop generic data extraction pipelines for unstructured data from various sources
Participate in the release process to ensure the latest deployments are operational with the latest features
Works as Technology Lead with the Product owner and Scrum Leads to execute the agile sprints, owns and runs with release tasks and collaborates with release management and BA/QA teams to make sure production releases are performed with high levels of accuracy
Provide end-to-end ownership of micro-services including developing, testing, and deploying in production
Work with other data engineers and analysts to build new and existing data structures to support sales and client analytics
Research and troubleshoot data quality issues, provide fixes and propose both short-term and long-term solutions by monitoring and improving the front-end performance
Contributes to team review and product demos to provide valuable insights and improvements for future development
Provide production support of projects when receiving incidents and requests from end users, then analyze and respond to the end user with a solution
Deliver high quality code deliverables
Lead the team by supporting peers and juniors on specific technical competencies required to deliver on assigned tasks
Develop and provide solutions for real time streaming solutions using Spark and Cassandra
May telecommute
What We’re Looking For
Employer will accept a Bachelor's degree in Computer Science, Engineering, Electronics Engineering, Computer Information Systems, or a related field
5 years of progressive post baccalaureate experience in the job offered or in a technology-related occupation
Alternatively, employer will accept a Master’s degree in Computer Science, Engineering, Electronics Engineering, Computer Information Systems, or a related field
Two (2) years of experience in the job offered or in a technology-related occupation
Where will you be working?
You will be working at our Whippany, New Jersey location at 300 Jefferson Park. At Barclays, we are proud to be redefining the future of finance and here at Whippany we are defining the future of the workplace and the future of the way we work and live. We are creating a unique community, one of four strategic tech-enabled hubs that will redefine opportunity for everyone who works here. Whatever you do at Whippany, you’ll have every chance to build a world-class career in this world-class environment.
Interested and want to know more about Barclays? Visit home.barclays/who-we-are/ for more details.
Our Values
Everything we do is shaped by the five values of Respect, Integrity, Service, Excellence and Stewardship. Our values inform the foundations of our relationships with customers and clients, but they also shape how we measure and reward the performance of our colleagues. Simply put, success is not just about what you achieve, but about how you achieve it.
Our Diversity
We aim to foster a culture where individuals of all backgrounds feel confident in bringing their whole selves to work, feel included and their talents are nurtured, empowering them to contribute fully to our vision and goals.
It is the policy of Barclays to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, creed, religion, national origin, alienage or citizenship status, age, sex, sexual orientation, gender identity or expression, marital or domestic/civil partnership status, disability, veteran status, genetic information, or any other basis protected by law.
Dynamic working gives everyone at Barclays the opportunity to integrate professional and personal lives, if you have a need for flexibility then please discuss this with the hiring manager.
Our Benefits
Our customers are unique. The same goes for our colleagues. That's why at Barclays we offer a range of benefits, allowing every colleague to choose the best options for their personal circumstances. These include a competitive salary and pension, health care and all the tools, technology and support to help you become the very best you can be. We are proud of our dynamic working options for colleagues. If you have a need for flexibility then please discuss this with us.
This position is eligible for incentives pursuant to Barclays Employee Referral Program.
Show more
Show less","Big Data, Data Modeling, Data Extraction, Data Pipelines, Agile, Scrum, Microservices, Data Structures, Data Quality, Spark, Cassandra, Realtime Streaming, Computer Science, Engineering, Electronics Engineering, Computer Information Systems","big data, data modeling, data extraction, data pipelines, agile, scrum, microservices, data structures, data quality, spark, cassandra, realtime streaming, computer science, engineering, electronics engineering, computer information systems","agile, big data, cassandra, computer information systems, computer science, data extraction, data quality, data structures, datamodeling, datapipeline, electronics engineering, engineering, microservices, realtime streaming, scrum, spark"
Senior Software Engineer (Data Strategy),SAM NETWORK SYSTEMS,"New York, NY",https://www.linkedin.com/jobs/view/senior-software-engineer-data-strategy-at-sam-network-systems-3782876644,2023-12-17,Newark,United States,Mid senior,Onsite,"Company Description
SAM NETWORK SYSTEMS is a IL based IT services company specializing in Enterprise, Project, On-Demand, Full Cycle, and Partial Cycles. Our consulting solutions and outsourcing services meet our clients' needs with flexibility and recruitment personnel who boast expertise in IT and Non-IT. Our unique framework helps businesses make strategic improvements, and our proactive services exponentially increase productivity levels.
Role Description
This is a full-time on-site role for a Senior Software Engineer (Data Strategy) located in New York, NY. The Senior Software Engineer (Data Strategy) will lead the company's data strategy and solutions to ensure delivery of high-quality software products that meet our clients' needs. This role will manage and analyze complex and large-scale data, design and deploy scalable and maintainable data pipelines, and implement new data-driven features.
Qualifications
Master’s Degree or Ph.D. in data science, computer science, or related quantitative field such as applied mathematics, statistics, engineering or operations research, or equivalent experience.
8+ years of software development experience with a focus on Data Engineering, Data Analytics, and/or Machine Learning.
Strong proficiency in Python, Java, and SQL.
Experience with big data technologies such as Hadoop, Spark, and Hive.
Expertise in designing and managing large and complex data systems and data pipelines.
Experience with data analytics tools and technologies such as Tableau, Power BI, and D3.js
Ability to lead and mentor junior team member(s).
Excellent analytical skills and problem-solving ability.
Experience working in a fast-paced Agile development environment.
Strong communication and collaboration skills.
Required Skills:
Python, FastAPI, Django, SQL, CI/CD, Azure DevOps, GitLab, Travis, Jenkins, Data Analysis, Data Science, AI, (py)Spark, ELT/ETL Frameworks, Web Scraping, Crowdsourcing, Databricks, Optimizing Spark Clusters, PowerBI, Tableau
Show more
Show less","Python, Java, SQL, Hadoop, Spark, Hive, Tableau, Power BI, D3.js, Django, FastAPI, CI/CD, Azure DevOps, GitLab, Travis, Jenkins, Data Analysis, Data Science, AI, (py)Spark, ELT/ETL Frameworks, Web Scraping, Crowdsourcing, Databricks, Optimizing Spark Clusters, PowerBI","python, java, sql, hadoop, spark, hive, tableau, power bi, d3js, django, fastapi, cicd, azure devops, gitlab, travis, jenkins, data analysis, data science, ai, pyspark, eltetl frameworks, web scraping, crowdsourcing, databricks, optimizing spark clusters, powerbi","ai, azure devops, cicd, crowdsourcing, d3js, data science, dataanalytics, databricks, django, eltetl frameworks, fastapi, gitlab, hadoop, hive, java, jenkins, optimizing spark clusters, powerbi, python, spark, sql, tableau, travis, web scraping"
"Data Analyst _ HYBRID- New York, NY 10020 (3 in-office/2 remote)",Ekodus INC.,"New York, NY",https://www.linkedin.com/jobs/view/data-analyst-hybrid-new-york-ny-10020-3-in-office-2-remote-at-ekodus-inc-3691416126,2023-12-17,Newark,United States,Mid senior,Onsite,"Title: Data Analyst
Location: HYBRID- New York, NY 10020 (3 in-office/2 remote)
Duration: 12+ Months with the possibility of an Extension
Onsite interview
Key Responsibilities
Strong understanding of data concepts (Modeling/Design, -Warehousing, ETL Development)
Sound knowledge in RDBMS, SQL Queries, Indexes, Keys and Tables considering platforms, such as Apache Hadoop, Oracle, MySQL, HiveQL, and Microsoft SQL
Experience in data integration, conversion and migration
Knowledge of Microsoft Office applications (Visio, PowerPoint, Excel, Word)
Experience with ETL tools (preferably Informatica) with the ability to develop ETL design specifications and understand from code what existing ETLs do
Knowledge of domain specific data and enterprise data
Knowledge of Autosys, Unix commands and scripting
Knowledge of Data Quality controls & implementation of DQ
Knowledge And Experience In Agile Development Methodology (scrum Preferred)
Extensive experience documenting processes, workflows and technical specifications
Skills Required
Bachelor's Degree in Computer Science or similar
Minimum 9 years of experience in global organizations as a Data Analyst
Proficiency in Data Analysis, with working experience in SQL and relational databases
Experience using API and SoupUI
Strong communication skills, both verbal and written; as well as work in an international team setting
Have strong reasoning skills, logical deduction and apply to data analysis
Collaborate effectively across a variety of IT and Business groups, across regions and roles
Present and be able to tell a story with the data analysis/reporting
Self-motivated individual and creative thinker who will take ownership of tasks assigned
Ability to problem solve and have creative solutions in challenging environments
Able to thrive in a fast-paced, high energy, demanding and team-oriented environment
Good customer service skills. Ability to deal with difficult situations gracefully
SQL querying building, Word, Excel, and PowerPoint
Please share your resume to career@ekodusinc.com
Show more
Show less","Data Modeling, Data Warehousing, ETL Development, RDBMS, SQL, Hadoop, Oracle, MySQL, HiveQL, Microsoft SQL, Data Integration, Data Conversion, Data Migration, Microsoft Office, Visio, PowerPoint, Excel, Word, ETL Tools, Informatica, ETL Design, Data Quality, Agile Development, Scrum, Process Documentation, Workflow Documentation, Technical Specifications, Data Analysis, SQL, Relational Databases, API, SoupUI, Communication Skills, Reasoning Skills, Logical Deduction, Collaboration, Data Storytelling, Problem Solving, Creative Thinking, Customer Service, Word Processing, Spreadsheets, Presentation Software","data modeling, data warehousing, etl development, rdbms, sql, hadoop, oracle, mysql, hiveql, microsoft sql, data integration, data conversion, data migration, microsoft office, visio, powerpoint, excel, word, etl tools, informatica, etl design, data quality, agile development, scrum, process documentation, workflow documentation, technical specifications, data analysis, sql, relational databases, api, soupui, communication skills, reasoning skills, logical deduction, collaboration, data storytelling, problem solving, creative thinking, customer service, word processing, spreadsheets, presentation software","agile development, api, collaboration, communication skills, creative thinking, customer service, data conversion, data integration, data migration, data quality, data storytelling, dataanalytics, datamodeling, datawarehouse, etl design, etl development, etl tools, excel, hadoop, hiveql, informatica, logical deduction, microsoft office, microsoft sql, mysql, oracle, powerpoint, presentation software, problem solving, process documentation, rdbms, reasoning skills, relational databases, scrum, soupui, spreadsheets, sql, technical specifications, visio, word, word processing, workflow documentation"
Clinical Data Analyst,Columbia University Irving Medical Center,"New York, NY",https://www.linkedin.com/jobs/view/clinical-data-analyst-at-columbia-university-irving-medical-center-3766133733,2023-12-17,Newark,United States,Mid senior,Onsite,"Grade 104
Job Type: Officer of Administration
Bargaining Unit:
Regular/Temporary: Regular
End Date if Temporary:
Hours Per Week: 35
Standard Work Schedule:
Building:
Salary Range: $65,000 - $85,000
The salary of the finalist selected for this role will be set based on a variety of factors, including but not limited to departmental budgets, qualifications, experience, education, licenses, specialty, and training. The above hiring range represents the University's good faith and reasonable estimate of the range of possible compensation at the time of posting.
Position Summary
The Clinical Database Analyst will maintain data storage; assess database design; and gather, organize, and interpret information based on the clinical data in the database. The analyst will also support the regulatory requirements for multiple government- and industry-funded research studies.
Responsibilities
Gathers, organizes, and interprets data from the database to provide reports to, and answer questions from, upper management (40%).
Oversees database development and modiﬁcation eﬀorts (10%).
Designs and implements secure, eﬃcient, and accurate databases (10%).
Maintains current and accurate knowledge of data storage and management best practices (10%).
Develops and maintains documentation, standards, and regulatory compliance (10%).
Ensures that databases are designed to meet speciﬁcations and requirements (5%).
Ensures that database projects are completed on time (5%).
Assists Manager and identiﬁes and resolves production and/or applications development problems related to the use of the database management system software or utilities (5%).
Performs other related duties as assigned (5%).
Minimum Qualifications
Requires a bachelor's degree or equivalent in education and experience, plus three years of related experience.
Preferred Qualifications
Bachelors degree in Business Administration, Statistics, Mathematics, Accounting, or Computer Science or equivalent work experience required
Excellent verbal and written communication skills.
Proﬁcient in Microsoft Oﬃce Suite or related software.
Excellent organizational skills and attention to detail.
Understanding of computer languages used within databases.
Understanding of database design and construction.
Equal Opportunity Employer / Disability / Veteran
Columbia University is committed to the hiring of qualified local residents.
Show more
Show less","Data storage, Database design, Data interpretation, Database development, Database modification, Database security, Database accuracy, Data storage best practices, Data management best practices, Documentation, Standards, Regulatory compliance, Database specifications, Database requirements, Database project management, Production problem resolution, Applications development problem resolution, Database management system software, Microsoft Office Suite, Verbal communication, Written communication, Organizational skills, Attention to detail, Computer languages, Database design, Database construction","data storage, database design, data interpretation, database development, database modification, database security, database accuracy, data storage best practices, data management best practices, documentation, standards, regulatory compliance, database specifications, database requirements, database project management, production problem resolution, applications development problem resolution, database management system software, microsoft office suite, verbal communication, written communication, organizational skills, attention to detail, computer languages, database design, database construction","applications development problem resolution, attention to detail, computer languages, data interpretation, data management best practices, data storage, data storage best practices, database accuracy, database construction, database design, database development, database management system software, database modification, database project management, database requirements, database security, database specifications, documentation, microsoft office suite, organizational skills, production problem resolution, regulatory compliance, standards, verbal communication, written communication"
GCP Lead Data Engineer- REMOTE,PSRTEK,"New York, NY",https://www.linkedin.com/jobs/view/gcp-lead-data-engineer-remote-at-psrtek-3659213273,2023-12-17,Newark,United States,Mid senior,Onsite,"Position:- GCP Lead Data Engineer
Type:- Remote
Hands on experience working in GCP services like Big Query, Cloud Storage (GCS), cloud function, cloud dataflow, Pub/sub, Cloud Shell, GSUTIL, Big Query, Data Proc, Operations Suite (Stack driver).
Performed in-memory data processing for batch, real-time, and advanced analytics
PSRTEK is a reputed technology recruitment and IT staffing brand with a global footprint and an admired client base. As an ideas and innovation powerhouse with a culture of excellence, we bring remarkable expertise and deliver powerfully transformative results.
Show more
Show less","GCP, BigQuery, Cloud Storage (GCS), Cloud Function, Cloud Dataflow, Pub/Sub, Cloud Shell, GSUTIL, Data Proc, Operations Suite (Stackdriver), Inmemory data processing, Batch processing, Realtime analytics, Advanced analytics","gcp, bigquery, cloud storage gcs, cloud function, cloud dataflow, pubsub, cloud shell, gsutil, data proc, operations suite stackdriver, inmemory data processing, batch processing, realtime analytics, advanced analytics","advanced analytics, batch processing, bigquery, cloud dataflow, cloud function, cloud shell, cloud storage gcs, data proc, gcp, gsutil, inmemory data processing, operations suite stackdriver, pubsub, realtime analytics"
Senior Data Engineer,City of New York,"Manhattan, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-city-of-new-york-3728292950,2023-12-17,Newark,United States,Mid senior,Onsite,"The New York City Department of Transportation’s (NYC DOT) provides safe, efficient, and environmentally responsible movement of pedestrians, goods, and vehicular traffic on the streets, highways, bridges, and waterways of the City's transportation network. NYC DOT rehabilitates and maintains the City's infrastructure, including bridges, tunnels, streets, sidewalks, and highways.
The NYC DOT IT & Telecom division is an award-winning team leading business transformation through technology innovation to enhance the agency’s ability in fulfilling its mission. IT & Telecom acts as a strategic partner with all business units to promote technology initiatives by delivering quality service, and secure solutions that provide new and improved capabilities for the Agency. We work hard to embrace diversity and inclusion and encourage everyone in the division to bring their authentic selves to work every day.
We offer our full-time employee’s competitive salaries, excellent benefit options and perks that include:
Pension and Retirement Plans- Upon retirement, qualified members of the City’s generous pension program receive a guaranteed salary and health benefits for the rest of their lives. There are also 401K and 457 plans available to supplement your retirement needs.
Health Coverage- We offer health coverage to meet your needs and your family.
Dental and Vision Coverage- We offer a wide variety of excellent civil service title-based union dental and vision coverage to meet the needs of you and your family.
Paid Time Off- We offer paid vacation and sick time off which accrues over time based on your years of service.
Professional Development - We offer numerous training programs, leadership development opportunities, and career coaching services, while encouraging employees to attend approved off and on-site trainings and seminars.
Additional Perks - Our employees are eligible for discounts on top theme parks, hotels, shows, events, movies and more.
The IT & Telecom division is seeking a Senior Data Engineer to be a member of the Enterprise Data Science and Engineering Unit (EDSE). EDSE provides the Agency with custom architected data storage and processing solutions that can handle very large datasets, or “Big Data”. In addition to managing Big Data operations, we work alongside Agency analysts and data scientists to help the Agency’s leadership and planners make data-driven decisions for critical projects and strategic planning initiatives. The candidate will work with the Director to support our Big Data analytics platform and develop new data systems and services.
Major Responsibilities
Data platform and Linux administration/maintenance.
ETL maintenance and development.
Platform and query performance tuning.
Ensure proper classification and handling of sensitive data.
Diagnose and resolve end-user issues regarding server performance, database access issues, ETL and query performance.
Writing performant, reusable, and maintainable code.
Perform exploratory data analyses as needed.
Develop data dashboards and reports for end-users.
Assist in the ongoing deployment of server hardware, software, and applications; provide technical support, troubleshooting, diagnosis and problem resolution and maintenance.
Assist in planning and migration of on-premise big data environment to public cloud.
Minimum Qualifications
A baccalaureate degree from an accredited college in computer science, engineering or a related field and four years of satisfactory full-time experience related to datacenter engineering and operations, cloud engineering and operations, complex IT infrastructure engineering; or,
A baccalaureate degree from an accredited college and eight years of satisfactory full-time experience related to datacenter engineering and operations, cloud engineering and operations, complex IT infrastructure engineering; or,
Education and/or experience which is equivalent to ""1"" or ""2"" above.
Preferred Skills
Professional experience working with hundreds of billions of records in a distributed compute environment. - Minimum of (3) years’ professional experience in Hortonworks Data Platform (HDP), Hortonworks Data Flow (HDF), and/or Cloudera Data Platform (CDP)/Cloudera Data Flow (CDF). - 3-5 years of experience with Kafka, HBase, Hive, Nifi, Spark. - 3-5 years of data lake and data warehouse design. - Experience working with geospatial data at scale. - Fluency in Python, Scala, Java, and SQL. - Public cloud experience (AWS/GCP/Azure), certification a plus. - Proven professional experience migrating on-premise big data infrastructure to cloud based solutions. - Excellent organizational and interpersonal skills. - Ability to effectively prioritize and execute tasks in a high-pressure environment. - Ability to understand and gather client requirements as well as provide a solution that meets or exceeds client expectations.
Public Service Loan Forgiveness
As a prospective employee of the City of New York, you may be eligible for federal loan forgiveness programs and state repayment assistance programs. For more information, please visit the U.S. Department of Education’s website at https://studentaid.gov/pslf/
Residency Requirement
New York City Residency is not required for this position
Additional Information
The City of New York is an inclusive equal opportunity employer committed to recruiting and retaining a diverse workforce and providing a work environment that is free from discrimination and harassment based upon any legally protected status or protected characteristic, including but not limited to an individual's sex, race, color, ethnicity, national origin, age, religion, disability, sexual orientation, veteran status, gender identity, or pregnancy.
$38,333.00 – $140,000.00
Show more
Show less","AWS, Azure, Big Data, Kafka, Cloudera, Python, Linux, Hadoop, Scala, GCP, ETL, HDFS, Hive, SQL, HBase, Public Cloud, Nifi, Java, Data Lake, Hadoop, Hortonworks Data Platform, Cloudera Data Platform, CDP, Spark, Data Warehouse, ETL","aws, azure, big data, kafka, cloudera, python, linux, hadoop, scala, gcp, etl, hdfs, hive, sql, hbase, public cloud, nifi, java, data lake, hadoop, hortonworks data platform, cloudera data platform, cdp, spark, data warehouse, etl","aws, azure, big data, cdp, cloudera, cloudera data platform, data lake, datawarehouse, etl, gcp, hadoop, hbase, hdfs, hive, hortonworks data platform, java, kafka, linux, nifi, public cloud, python, scala, spark, sql"
Senior Data Engineer (Stream Sets),Saransh Inc,"Berkeley Heights, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-stream-sets-at-saransh-inc-3712853324,2023-12-17,Newark,United States,Mid senior,Onsite,"Roles & Responsibilities
Skills required - SQL, Stream Sets, Snowflake
7+ years of experience with ETL
3+ year's hands-on experience with Stream Sets data collector is a must.
Experienced in Data Analysis and expert level in SQL.
Exposure to Snowflake is a plus.
Hands on experience in developing data pipelines to load files into Snowflake RAW zone using Stream sets data collector.
Show more
Show less","SQL, Stream Sets, Snowflake, ETL, Data Analysis, Data Pipelines, Data Collector","sql, stream sets, snowflake, etl, data analysis, data pipelines, data collector","data collector, dataanalytics, datapipeline, etl, snowflake, sql, stream sets"
"Data Engineer (Java, Spark, Kafka)",Saransh Inc,"Berkeley Heights, NJ",https://www.linkedin.com/jobs/view/data-engineer-java-spark-kafka-at-saransh-inc-3723219655,2023-12-17,Newark,United States,Mid senior,Onsite,"Basic Qualifications For Consideration
10+ Overall industry experience
7+ years' experience with building large scale big data applications development
Bachelors in computer science or related field
Provide technical leadership in developing data solutions and building frameworks.
Expertise in solutions for processing large volumes of data, using data processing tools and Big Data platforms.
Experience building Data Lake, EDW and data applications using Azure, AWS and
Hands-on experience in cloud Data stack (preference is Azure)
Understanding of cluster and parallel architecture as well as high-scale or distributed RDBMS, SQL experience
Hands on experience on major programming/scripting languages like Java
Java experience with OOPS concepts, multithreading
Nice to have experience deploying code on containers.
Conduct code reviews and strive for improvement in software engineering quality.
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience of successfully delivering big data projects using Kafka, Spark
Exposure working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus.
In depth knowledge of design principles and patterns
Able to tune big data solutions to improve performance.
Preferred Skills, Experience And Education
Exposure to Big Data tools and solutions a strong plus
Exposure to Relational Modeling, Dimensional Modeling, and Modeling of Unstructured Data
Bachelors in computer science or related field
Experience in Design and architecture review
Experience in Banking, Financial domain
Show more
Show less","Big data, Data processing, Big data platforms, Data lake, EDW, Data applications, Azure, AWS, Cloud data stack, Cluster architecture, Parallel architecture, Distributed RDBMS, SQL, Java, OOPS, Multithreading, Containers, Code reviews, Infrastructure configuration, Kafka, Spark, NoSQL, Cassandra, HBase, DynamoDB, Elastic Search, PCI, Data science, Design principles, Design patterns, Performance tuning","big data, data processing, big data platforms, data lake, edw, data applications, azure, aws, cloud data stack, cluster architecture, parallel architecture, distributed rdbms, sql, java, oops, multithreading, containers, code reviews, infrastructure configuration, kafka, spark, nosql, cassandra, hbase, dynamodb, elastic search, pci, data science, design principles, design patterns, performance tuning","aws, azure, big data, big data platforms, cassandra, cloud data stack, cluster architecture, code reviews, containers, data applications, data lake, data processing, data science, design patterns, design principles, distributed rdbms, dynamodb, edw, elastic search, hbase, infrastructure configuration, java, kafka, multithreading, nosql, oops, parallel architecture, pci, performance tuning, spark, sql"
Senior Data Engineer,Apexon,"Berkeley Heights, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-apexon-3764557762,2023-12-17,Newark,United States,Mid senior,Onsite,"Description
Position at Apexon
About Apexon
Apexon is a digital-first technology services firm specializing in accelerating business transformation and delivering human-centric digital experiences. We have been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
Apexon brings together distinct core competencies – in AI, analytics, app development, cloud, commerce, CX, data, DevOps, IoT, mobile, quality engineering and UX, and our deep expertise in BFSI, healthcare, and life sciences – to help businesses capitalize on the unlimited opportunities digital offers. Our reputation is built on a comprehensive suite of engineering services, a dedication to solving clients’ toughest technology problems, and a commitment to continuous improvement.
Backed by Goldman Sachs Asset Management and Everstone Capital, Apexon now has a global presence of 15 offices (and 10 delivery centers) across four continents.
We enable #HumanFirstDIGITAL
Responsibilities
5+ years of hands-on software engineering experience.
5+ years of experience integrating technical processes and business outcomes – specifically: data and process analysis, data quality metrics/monitoring, data architecture, developing policies/standards & supporting processes.
Designing and building data pipeline (batch & streaming), extensive experience in Spark with Java.
Strong database fundamentals including SQL, performance, and schema design.
Good experience on Java.
Good experience on Kafka
Excellent communications skills, both written and verbal.
Experience working in an offshore/onshore team model.
Design and implement Data security and privacy controls.
Experience with Git or equivalent source code control software.
Experience in designing solutions for large data warehouses with a good understanding of cluster and parallel architecture as well as high-scale or distributed RDBMS and/or knowledge on NoSQL platforms.
Qualifications
Bachelor's Degree or master’s degree in Computer Science, Mathematics, Statistics or equivalent.
Don’t worry if you don’t check all the boxes; we’d still love to hear from you.
Our Commitment To Diversity & Inclusion
Did you know that Apexon has been Certified™ by Great Place To Work®, the global authority on workplace culture, in each of the three regions in which it operates: USA (for the fourth time in 2023), India (seven consecutive certifications as of 2023), and the UK.
Apexon is committed to being an equal opportunity employer and promoting diversity in the workplace. We take affirmative action to ensure equal employment opportunity for all qualified individuals. Apexon strictly prohibits discrimination and harassment of any kind and provides equal employment opportunities to employees and applicants without regard to gender, race, color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law.
You can read about our Job Applicant Privacy policy here Job Applicant Privacy Policy (apexon.com)
Our Perks And Benefits
Our benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones.
As an Apexer, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.
We Also Offer
Health Insurance with Dental & Vision
401K Plan
Life Insurance, STD & LTD
Paid Vacations & Holidays
Paid Parental Leave
FSA Dependent & Limited Purpose care
Learning & Development
Show more
Show less","Software engineering, Data analysis, Process analysis, Data quality metrics, Data architecture, Data pipeline design, Spark, Java, SQL, Kafka, Git, Data security, Data privacy, RDBMS, NoSQL, Distributed systems, Cluster computing, Parallel computing","software engineering, data analysis, process analysis, data quality metrics, data architecture, data pipeline design, spark, java, sql, kafka, git, data security, data privacy, rdbms, nosql, distributed systems, cluster computing, parallel computing","cluster computing, data architecture, data pipeline design, data privacy, data quality metrics, data security, dataanalytics, distributed systems, git, java, kafka, nosql, parallel computing, process analysis, rdbms, software engineering, spark, sql"
Senior Data Engineer,MoneyLion,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-moneylion-3739442714,2023-12-17,Newark,United States,Mid senior,Remote,"W
HAT
YOU
’
LL
DO
Skills
The Senior Data Engineer at Engine by MoneyLion must be hands on with a broad and deep data skillset who can spearhead multiple concurrent data initiatives to meet business goals and improve existing data processing pipelines. Efforts will mainly be focused around:
Advising on, designing and developing product enabling data solutions
Drive product critical, fast cycle POC work
Oversee productionisation of POC work as needed
Gain additional insights from available data sets, working with Product/Data Science when appropriate
Work closely with Product to set requirements as well as extract insights based on business requirements
Find improvements and help keep the data pipelines in efficient, resilient and well tuned state with the goal of reducing processing related infrastructure efforts and expenses, saving both time and money for the company
Be a steward of the data ecosystem, getting buy-ins from stakeholders for enhancements and cleanup/organizational improvements of the data throughout the company
Be the technical partner for non-data-engineering teams (product, data science, platform and the rest of engineering) with their data needs/requirements, actively participating in all phases, from inception of the idea to production release
Derive efficient approaches to meet business/product goals for data as well as general engineering efforts
Participate in the conversion of POC work into scalable, maintainable and performant production ready data pipelines. This includes designing architecture, implementation as well as providing important updates throughout the project lifecycle
W
HO
YOU
ARE
Experience wrangling terabytes of big, complicated, imperfect data using scalable data processing frameworks such as Redshift, Snowflake, DBT, Amundsen, Apache Spark, Kafka, Debezium, AWS products (EMR, S3, Glue, IAM, RDS, etc)
A strong understanding of data structures, algorithms, and effective software design
Significant development experience with a major modern language (e.g. Python, Scala, Ruby, C/C++, etc.)
Significant experience working with structured and unstructured data at scale and comfort with a variety of different stores (key-value, document, columnar, etc.) as well as traditional RDBMS’s and data warehouses
Experience writing unit, functional and end to end tests
Comfort with version control systems (e.g. Git, SVN)
Excellent verbal and written communication skills; must work well in an agile, collaborative team environment
What We Value
We value
growth-minded
and
collaborative
people with high
learning agility
who embody our core values of
teamwork, customer-first
and
innovation
. Every member of the MoneyLion Pride is passionate about fintech and ready to give 100% in helping us achieve our mission.
Working At MoneyLion
At MoneyLion, we want you to be well and thrive. Our generous benefits package includes:
Competitive salary packages
Comprehensive medical, dental, vision and life insurance benefits
Wellness perks
Paid parental leave
Generous Paid Time Off
Learning and Development resources
Flexible working hours
MoneyLion is committed to equal employment opportunities for all employees. Inside our company, every decision we make regarding our employees is based on merit, competence, and performance, completely free of discrimination. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. Within that team, no one will feel more “other” than anyone else. We realize the full promise of diversity and want you to bring your whole self to work every single day.
Show more
Show less","Data Engineering, Data Processing, Data Pipelines, Data Analytics, Data Science, Product Development, POC Work, Data Structures, Algorithms, Software Design, Python, Scala, Ruby, C/C++, Structured Data, Unstructured Data, KeyValue Stores, Document Stores, Columnar Stores, RDBMS, Data Warehouses, Unit Testing, Functional Testing, EndtoEnd Testing, Version Control, Git, SVN, Communication Skills, Teamwork, CustomerFirst Mentality, Innovation, Learning Agility, Competitive Salary, Comprehensive Benefits, Wellness Perks, Paid Parental Leave, Paid Time Off, Learning and Development Resources, Flexible Working Hours, Equal Employment Opportunities, Diversity and Inclusion","data engineering, data processing, data pipelines, data analytics, data science, product development, poc work, data structures, algorithms, software design, python, scala, ruby, cc, structured data, unstructured data, keyvalue stores, document stores, columnar stores, rdbms, data warehouses, unit testing, functional testing, endtoend testing, version control, git, svn, communication skills, teamwork, customerfirst mentality, innovation, learning agility, competitive salary, comprehensive benefits, wellness perks, paid parental leave, paid time off, learning and development resources, flexible working hours, equal employment opportunities, diversity and inclusion","algorithms, cc, columnar stores, communication skills, competitive salary, comprehensive benefits, customerfirst mentality, data engineering, data processing, data science, data structures, data warehouses, dataanalytics, datapipeline, diversity and inclusion, document stores, endtoend testing, equal employment opportunities, flexible working hours, functional testing, git, innovation, keyvalue stores, learning agility, learning and development resources, paid parental leave, paid time off, poc work, product development, python, rdbms, ruby, scala, software design, structured data, svn, teamwork, unit testing, unstructured data, version control, wellness perks"
"Senior Data Engineer (NYC) | Top Performing AI Startup (Series A, $3m+ ARR)",Intelletec,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-nyc-top-performing-ai-startup-series-a-%243m%2B-arr-at-intelletec-3773596912,2023-12-17,Newark,United States,Mid senior,Hybrid,"I have partnered w/ a well-funded, diverse, and unique startup, based out of NYC. They are series A, having raised almost $20m, with more than $3m in ARR and growing.
They have developed a new type of tool that brings together some of the most powerful capabilities of spreadsheets, data enrichment, and workflow automation to build the first spreadsheet that fills itself.
They are flying financially with over $3mm ARR and growing. 99% of startups never hit this milestone, to do it in 2 years is very impressive!
I am seeking a
Senior Data Engineer
to come in on a hybrid model in their
Manhattan
office. They're focused on building a high-trust, ownership-focused environment where everyone aims to define what they think is most important to work on and drive initiatives forward.
What You'll Do:
Build, scale and maintain robust and scalable data pipelines, monitoring and optimizing data pipeline performance
Take ownership of designing, implementing, and maintaining scalable and efficient data models for both structured and unstructured data
Collaborate with your teammates and external stakeholders to ensure the data you’re providing the customers is functional and beneficial
Develop and optimize ETL processes to load data into our robust data stores, then implement efficient testing and validation processes to ensure the accuracy and integrity of the data
Evaluation: You’ll be the decision maker on how the data is presented in our product to our end customers
What You'll Bring:
6+ years of production-level engineering experience in a Data Engineering, DevOps, or Data Science role
Expert in in a language that will allow you to process data at scale (
Python, Javascript/Node, etc
.)
BS or MS in Computer Science, engineering, or a related field
You're able to navigate large quantities of data and surface statistics and recommendations
You're comfortable interacting with data stores (
SQL
, etc.) and know or can learn new APIs with ease (elastic search,
AWS S3
files, etc.)
You're experienced in setting up data pipelines (cron jobs, queues, serverless architectures, etc.) according to the dataset needs
You’ve designed and implemented scalable data architectures with experience building large-scale high-performance data pipelines and event ingestion systems
Nice to Have:
You have previous experience as the sole data engineer at a fast-growing startup
You have familiarity with the GTM tech stack and data providers and have opinions on what would be impactful to integrate
On Offer:
Base salary $175-200k
Cash bonus
Significant equity piece
Tier-1 benefits package
Friendly, inviting, diverse culture, rich with people from all over the world
We are able to transfer H-1b visas (with I-140) - please don't hesitate to reach out if you have quesitons.
Show more
Show less","Data Engineering, DevOps, Data Science, Python, JavaScript/Node, SQL, Elastic Search, AWS S3, Data Pipelines, Cron Jobs, Queues, Serverless Architectures, GTM Tech Stack, Data Providers","data engineering, devops, data science, python, javascriptnode, sql, elastic search, aws s3, data pipelines, cron jobs, queues, serverless architectures, gtm tech stack, data providers","aws s3, cron jobs, data engineering, data providers, data science, datapipeline, devops, elastic search, gtm tech stack, javascriptnode, python, queues, serverless architectures, sql"
Senior Data Engineer Consultant (Remote),ClinChoice,"Greater Toronto Area, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-consultant-remote-at-clinchoice-3766025328,2023-12-17,Brampton, Canada,Associate,Remote,"JOB DESCRIPTION
Job Title:
Senior Data Engineer Consultant
Employment Type : 6 months Contract with Possible Extension.
Experience:
2.5 – 5 years
Location:
Mexico (Remote)
About ClinChoice
ClinChoice is a global contract research organization supporting data management, biostatistics, statistical programming, clinical operations, regulatory affairs, safety, pharmacovigilance, toxicology, medical affairs, medical writing, quality, risk, and compliance services to the pharmaceutical, biotechnology, and medical device industries worldwide. We continuously strive to raise the standard of excellence through accuracy and efficiency to achieve the highest quality output for our Customers. ClinChoice was established in the US in 1995, and has grown to more than 3,000 employees worldwide, with key offices and delivery centers in the US, Canada, UK, China, India, Armenia, Japan and throughout the Asia-Pacific region. Please visit our website ClinChoice.com for our company information.
Job Description:
We are seeking a highly skilled and motivated Senior Data Engineer to join our dynamic team. The ideal candidate will possess expertise in data engineering, programming, cloud technologies, and a strong background in pharmaceutical or healthcare industries.
Responsibilities:
Design, develop, and maintain robust data pipelines using programming languages such as Python, R, and Spark.
Utilize cloud technologies (AWS, Azure, Google Cloud Platform) to architect scalable and efficient data solutions.
Implement and manage ETL processes using tools such as AWS Glue, AWS EMR, GCP Dataproc, GCP Datafusion, Azure Databricks, Azure Data Flow, AWS Lambda, AWS EC2, Azure Logic Apps, and GCP Cloud functions.
Apply data modeling techniques, including Kimball dimensional model and database schema design, to ensure effective and optimized data storage and retrieval.
Work with various databases, including Oracle, DeltaLake, BigQuery, and Azure Synapse.
Leverage Big Data technologies such as HDFS, Oozie, Hive, Hadoop, Kafka, and REST API for efficient data processing and storage.
Develop insightful and interactive dashboards using tools like PowerBI, Tableau, and Qlik for data visualization.
Contribute to machine learning modeling efforts, specializing in natural language processing, large language models, regression, feature store, and model monitoring & feedback.
Implement data privacy and security principles to ensure compliance with industry standards and regulations.
Collaborate with cross-functional teams, providing technical expertise in Jira project management.
Utilize healthcare or pharmaceutical industry knowledge to understand and address specific data engineering challenges in the domain.
Qualifications:
Bachelor’s or master’s degree in a relevant field.
Proven experience of 3 – 5 years in data engineering roles.
Proficient in programming languages, cloud technologies, ETL tools, data modeling, databases, Big Data technologies, dashboard development, and machine learning modeling.
Strong understanding of data privacy and security principles.
Experience with Jira project management.
Previous exposure to healthcare or pharmaceutical industries is highly desirable.
ClinChoice is an Equal Opportunity Employer / Committed to Diversity
Show more
Show less","Python, R, Spark, AWS, Azure, Google Cloud Platform, AWS Glue, AWS EMR, GCP Dataproc, GCP Datafusion, Azure Databricks, Azure Data Flow, AWS Lambda, AWS EC2, Azure Logic Apps, GCP Cloud functions, Kimball dimensional model, DeltaLake, BigQuery, Azure Synapse, HDFS, Oozie, Hive, Hadoop, Kafka, REST API, PowerBI, Tableau, Qlik, Natural language processing, Large language models, Regression, Feature store, Model monitoring & feedback, Jira project management, Healthcare or pharmaceutical industry knowledge","python, r, spark, aws, azure, google cloud platform, aws glue, aws emr, gcp dataproc, gcp datafusion, azure databricks, azure data flow, aws lambda, aws ec2, azure logic apps, gcp cloud functions, kimball dimensional model, deltalake, bigquery, azure synapse, hdfs, oozie, hive, hadoop, kafka, rest api, powerbi, tableau, qlik, natural language processing, large language models, regression, feature store, model monitoring feedback, jira project management, healthcare or pharmaceutical industry knowledge","aws, aws ec2, aws emr, aws glue, aws lambda, azure, azure data flow, azure databricks, azure logic apps, azure synapse, bigquery, deltalake, feature store, gcp cloud functions, gcp datafusion, gcp dataproc, google cloud platform, hadoop, hdfs, healthcare or pharmaceutical industry knowledge, hive, jira project management, kafka, kimball dimensional model, large language models, model monitoring feedback, natural language processing, oozie, powerbi, python, qlik, r, regression, rest api, spark, tableau"
Senior Data Engineer,Mozilla,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-mozilla-3787099239,2023-12-17,Brampton, Canada,Associate,Remote,"Why Mozilla?
Mozilla Corporation is the non-profit-backed technology company that has shaped the internet for the better over the last 25 years. We make pioneering brands like Firefox, the privacy-minded web browser, and Pocket, a service for keeping up with the best content online. Now, with more than
225
million people around the world using our products each month, we’re shaping the next 25 years of technology. Our work focuses on diverse areas including AI, social media, security and more. And we’re doing this while never losing our focus on our core mission – to make the internet better for everyone.
The Mozilla Corporation is wholly owned by the non-profit 501(c) Mozilla Foundation. This means we aren’t beholden to any shareholders — only to our mission. Along with
60,000
+ volunteer contributors and collaborators all over the world, Mozillians design, build and distribute
open-source
software that enables people to enjoy the internet on their terms.
About The Team & Role
Now more than ever, the Internet is a utility that facilitates modern life. At Mozilla, we take this to heart, striving to build products that keep the Internet open, accessible, and secure for everyone. We handle terabytes of data every day from millions of users to guide our decision-making processes. We need your help to enable the future of Mozilla in a way that makes us proud!
As a Data Engineer At Mozilla, Your Primary Area Of Focus Will Be On Our Analytics Engineering Team. This Team Focuses On Modeling Our Data So That The Rest Of Mozilla Has Access To It, In The Appropriate Format, When They Need It, To Help Them Make Data Informed Decisions. This Team Is Also Tasked With Helping To Maintain And Make Improvements To Our Data Platform. Some Recent Improvements Include Introducing a Data Catalog, Building In Data Quality Checks Among Others. Check Out The Data@Mozilla Blog For More Details On Some Of Our Work. You Will
work with other data engineers to design and maintain scalable data models and ETL pipelines.
help design, build, and improve the infrastructure for ingesting, storing, and transforming data at a scale of tens of terabytes per day.
help design and build systems to monitor and analyze data from Mozilla’s products.
work with data scientists to answer questions and guide product decisions.
General Professional Requirements
Proficiency with one or more of the programming languages used by our teams (SQL and Python).
Strong software engineering fundamentals: modularity, abstraction, data structures, and algorithms.
Ability to work collaboratively with a distributed team.
Specific Skills/Experience
Our team requires skills in a variety of domains. You should have proficiency in one or more of the areas listed below, and be interested in learning about the others.
You have used data to answer specific questions and guide company decisions.
You have experience building modular and reusable ETL/ELT pipelines in distributed databases
You are opinionated about data models and how they should be implemented. You partner with others to map out a business process, profile available data, design and build flexible data models for analysis.
You have experience recommending / implementing new data collection to help improve the quality of data models.
You have experience with data infrastructure: databases, message queues, batch and stream processing
You have experience with highly scalable distributed systems hosted on cloud providers (e.g. Google Cloud Platform)
About Mozilla
Mozilla exists to build the Internet as a public resource accessible to all because we believe that open and free is better than closed and controlled. When you work at Mozilla, you give yourself a chance to make a difference in the lives of Web users everywhere. And you give us a chance to make a difference in your life every single day. Join us to work on the Web as the platform and help create more opportunity and innovation for everyone online.
Commitment to diversity, equity, inclusion, and belonging
Mozilla understands that valuing diverse creative practices and forms of knowledge are crucial to and enrich the company’s core mission. We encourage applications from everyone, including members of all equity-seeking communities, such as (but certainly not limited to) women, racialized and Indigenous persons, persons with disabilities, persons of all sexual orientations, gender identities, and expressions.
We will ensure that qualified individuals with disabilities are provided reasonable accommodations to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment, as appropriate. Please contact us at hiringaccommodation@mozilla.com to request accommodation.
We are an equal opportunity employer. We do not discriminate on the basis of race (including hairstyle and texture), religion (including religious grooming and dress practices), gender, gender identity, gender expression, color, national origin, pregnancy, ancestry, domestic partner status, disability, sexual orientation, age, genetic predisposition, medical condition, marital status, citizenship status, military or veteran status, or any other basis covered by applicable laws. Mozilla will not tolerate discrimination or harassment based on any of these characteristics or any other unlawful behavior, conduct, or purpose.
Group: D
Req ID: R2413
To learn more about our Hiring Range System, please click this link.
Hiring Ranges
Canada Tier 1 Locations
$104,000—$151,000 CAD
Canada Tier 2 Locations
$95,000—$137,000 CAD
Show more
Show less","SQL, Python, ETL/ELT pipelines, Data models, Data infrastructure, Databases, Message queues, Batch and stream processing, Distributed systems, Cloud providers, Google Cloud Platform","sql, python, etlelt pipelines, data models, data infrastructure, databases, message queues, batch and stream processing, distributed systems, cloud providers, google cloud platform","batch and stream processing, cloud providers, data infrastructure, data models, databases, distributed systems, etlelt pipelines, google cloud platform, message queues, python, sql"
Data Governance Analyst,Supply Ontario,"Greater Toronto Area, Canada",https://ca.linkedin.com/jobs/view/data-governance-analyst-at-supply-ontario-3778820719,2023-12-17,Brampton, Canada,Associate,Hybrid,"Supply Ontario’s Data Strategy and Analytics team is seeking a Data Governance Analyst to be accountable for capturing business needs and processes, as well as creating and maintaining data governance processes that will advance Supply Ontario’s data and analytics capabilities.
Reporting to the Director, Data Strategy and Analytics, you will develop an understanding of the current status of business processes in the public sector related to supply chain management and work with the digital team to identify and analyze opportunities for improvement.
In this role, your key responsibilities will include:
Upkeeping metadata in our data governance solution and creating and maintaining documentation around how and what data is collected in business processes.
Developing and managing data glossaries, data lineage and data owner matrices to establish enterprise data standards.
Supporting engagements with key business stakeholders to assist with establishing fundamental data governance processes.
Identifying and escalating data risks and issues and providing support to data stewards in the development and implementation of action plans for issue resolution as directed by Data Governance Policy Lead.
Providing analysis for systems development and business requirements.
Supporting with the identification of training needs for employees involved in data management and governance.
Gathering updates from diverse teams to maintain the data collection tracker and ensure alignment across teams.
Assisting in data access management and audits on a regular basis.
Successful candidates will demonstrate the following:
Degree in Business, Finance, or a related field.
1-2 years of experience in Data discovery, Data Quality Management, Data Lineage, Metadata management, Data Access management.
1-2 years of experience identifying business needs and managing strategic plans driven by qualitative/quantitative analysis.
Experience with any Data Governance tools preferably MS Azure Purview.
Knowledge of data governance principles and practices as well as D&A governance technical solutions and standard processes.
Solid understanding of data platforms and fundamental architecture principles.
Demonstrated knowledge of business analysis, planning and improvement principles and methodologies as well as performance measurement and benchmarking techniques.
Proficient at identifying and articulating problems that have not previously been recognized and propose relevant solutions.
Ability to set priorities and manage multiple initiatives independently while leveraging cross-functional resources.
Effective interpersonal and communication skills (verbal and written)
Supply Ontario offers a competitive compensation package including benefits and defined benefit pension plan.
HOW TO APPLY
: Please apply by
December 29, 2023
.
We thank all applicants for their interest, however, only those selected for further consideration will be contacted.
Supply Ontario is an inclusive employer which respects equity, inclusion, diversity and anti-racism. Accommodation, if required, will be provided throughout the hiring process in accordance with the
Ontario Human Rights Code
.
Show more
Show less","Data Governance, Data Analysis, Data Quality Management, Data Lineage, Metadata Management, Data Access Management, Data Governance tools, MS Azure Purview, Data Governance principles, Data Governance practices, Data Governance technical solutions, Data Platforms, Architecture Principles, Business Analysis, Performance Measurement, Benchmarking, Problem Solving, Initiative Management, CrossFunctional Resources, Interpersonal Skills, Communication Skills, Data Stewards","data governance, data analysis, data quality management, data lineage, metadata management, data access management, data governance tools, ms azure purview, data governance principles, data governance practices, data governance technical solutions, data platforms, architecture principles, business analysis, performance measurement, benchmarking, problem solving, initiative management, crossfunctional resources, interpersonal skills, communication skills, data stewards","architecture principles, benchmarking, business analysis, communication skills, crossfunctional resources, data access management, data governance, data governance practices, data governance principles, data governance technical solutions, data governance tools, data lineage, data platforms, data quality management, data stewards, dataanalytics, initiative management, interpersonal skills, metadata management, ms azure purview, performance measurement, problem solving"
Energy Analyst (Data Analyst),Toronto Community Housing,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/energy-analyst-data-analyst-at-toronto-community-housing-3787331626,2023-12-17,Brampton, Canada,Associate,Hybrid,"What we offer
In addition to competitive wages and a rewarding career where you can truly make a difference, we offer a comprehensive package that meets the various needs of our diverse employees, including:
Ability to participate in inclusive employee-led networks to educate, inspire, amplify voices, build relationships and provide development opportunities;
Three weeks paid annual vacation days, increasing with years of service;
Four (4) paid personal days;
Defined benefit pension plan with OMERS, includes 100-per-cent employer matching;
Health and dental benefits;
Employee and family assistance program;
Maternity and parental leave top up (93% of base salary);
Training and development programs including tuition reimbursement of $1500 per calendar year;
Fitness membership discount;
This job offers the opportunity to work from home as part of a hybrid work arrangement. This arrangement will allow you to work some days at a TCHC work location and the rest of the time from home. The amount of time required to work at a TCHC work location is flexible, while considering operational and service delivery requirements.
Make a difference
We are currently searching for a dedicated and passionate individual to join our family at Toronto Community Housing! As an Energy Analyst, you’ll utilize your expertise and knowledge of energy management to contribute to the ongoing efforts of Toronto Community Housing to improve the neighborhoods and lives of residents of Toronto.
The Energy Analyst will report to the Manager, Business Analysis and is responsible for the collection and monitoring of energy and water consumption data. The Energy Analyst will also perform energy, cost /benefit and life-cycle cost analysis in an effort to reduce operating costs and assess energy and conservation related initiatives.
What you’ll do
Verify utility bills and work with accounts payable staff to ensure correct and timely payments
Manage utility data through Yardi energy modules (i.e. energy management software used for utility bill & greenhouse gas tracking, analysis, reporting, auditing and benchmarking) to assure accurate utility data
Analyze weather impacts on utility costs and consumption
Identify utility bills with potential errors and conduct investigation if needed,
Prepare monthly utility reports (including consumption, prices, costs, variances against budget and last year actuals)
Assess and verify projected energy savings provided by energy programs,
Assist Measurement and Verification (M&V) staff to evaluate energy savings for completed energy programs
Support energy program managers for utility/energy data analysis, etc.
Maintain Yardi as a central utility database for consumption tracking, monitoring, and verification
Ensure utility data is maintained and up-to-date and expanded to include all buildings in the housing portfolio
Work with accounts payable to close utility accounts for the buildings that have been demolished
Ensure all utility bills are entered in Yardi. Contact Yardi Service if missing bills
Continuously check Yardi data to identify and correct input errors
Verify utility consumption data, and correct data input errors if needed. Also, ensure utility consumption data are accurate and investigation if needed. Contact utility companies to correct errors on utility bills
Provide analysis support on annual utility budget forecasts and budget
Produce benchmarking reports for different types of buildings, not only for TCH buildings but also against other similar buildings across GTA
Collect utility data for similar groups of buildings
Identify buildings that are served by building bulk meters
Ensure the accuracy of utility data
Analyze trends in utility consumption and rate changes
Provide analysis on utility cost variance
Keep track of Heating Degree Day (HDD) and Cooling Degree Day (CDD) for the GTA
Analyze utility cost savings for the M&V of energy efficiency retrofit projects
Assess energy saving projection provided by energy program managers
Other duties assigned by the Manager, Business Analysis
What you’ll need
University degree in energy management, environmental management, engineering, accounting, or related field
Minimum of 3 years of energy or financial analysis experience within energy management, environmental management, engineering, finance, or related fields. Accounting/Finance experience is preferred.
Possess strong problem-solving skills
Demonstrated analytical skills with ability to be detail oriented.
Demonstrated written and verbal communication skills
Strong proficiency with MS Office, particularly Excel and Word
Demonstrated time management skills with ability to work on multiple tasks
Possess excellent teamwork skills
What’s next
Once you apply, we’ll review your resume and contact you if your skills and experience match the qualifications for the role. If you are selected to move forward, the process will include one or more interviews and/or assessments and reference checks.
Candidates for unionized positions must score a minimum of 70% to pass any interview or assessment and be considered for the next stage of the recruitment process. Successful candidates will be determined based on score and where applicable, union seniority. Not all candidates who score 70% or higher will automatically proceed to the next stage of the recruitment process as this will be dependent on the number of candidates and number of available roles.
Show more
Show less","Energy management, Environmental management, Engineering, Accounting, Energy analysis, Financial analysis, Problemsolving, Analytical skills, Attention to detail, Written communication, Verbal communication, Microsoft Office, Excel, Word, Time management, Teamwork, Yardi, Utility bill verification, Utility data management, Weather impact analysis, Energy savings assessment, Energy program evaluation, Utility budget forecasting, Benchmarking, Utility data collection, Data accuracy verification, Trend analysis, Cost variance analysis, Heating Degree Day (HDD), Cooling Degree Day (CDD), Measurement and Verification (M&V), Energy efficiency retrofit projects","energy management, environmental management, engineering, accounting, energy analysis, financial analysis, problemsolving, analytical skills, attention to detail, written communication, verbal communication, microsoft office, excel, word, time management, teamwork, yardi, utility bill verification, utility data management, weather impact analysis, energy savings assessment, energy program evaluation, utility budget forecasting, benchmarking, utility data collection, data accuracy verification, trend analysis, cost variance analysis, heating degree day hdd, cooling degree day cdd, measurement and verification mv, energy efficiency retrofit projects","accounting, analytical skills, attention to detail, benchmarking, cooling degree day cdd, cost variance analysis, data accuracy verification, energy analysis, energy efficiency retrofit projects, energy management, energy program evaluation, energy savings assessment, engineering, environmental management, excel, financial analysis, heating degree day hdd, measurement and verification mv, microsoft office, problemsolving, teamwork, time management, trend analysis, utility bill verification, utility budget forecasting, utility data collection, utility data management, verbal communication, weather impact analysis, word, written communication, yardi"
Data Platform Engineer,Sanofi,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-platform-engineer-at-sanofi-3730733271,2023-12-17,Brampton, Canada,Mid senior,Onsite,"Reference No.
R2715512
Position Title:
Data Platform Engineer Specialist
Department:
Global Data Platform
Location
: Toronto, Ontario
At Sanofi, we chase the miracles of science to improve people’s lives. We believe our cutting-edge science and manufacturing, fueled by data and digital technologies, have the potential to transform the practice of medicine, turning the impossible into possible for millions of people.
As one of Canada’s leading investors in life sciences, manufacturing and research and development, we focus on delivering new and better ways to address unmet medical needs. Our life-changing and lifesaving products are grounded in science that Canadians can trust. They empower self-care, prevent and treat diseases, and help people live better.
Our vision for digital, data analytics and AI
Sanofi has embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions. This has enabled us, to accelerate R&D, improve manufacturing and commercial performance, and bring novel drugs and vaccines to patients faster, all in order to improve health and save lives.
The Digital Team at Sanofi is a unique data-driven team. We pride ourselves on being data obsessed and highly focused on using state of the art processes along with global technologies to drive impact to our solutions. We measure our insights and products based on how they perform across the globe and hold ourselves to the highest regard as our solutions can impact millions of lives. When tackling a problem, we do not just ask how we will create a solution, but how we will create a solution that reaches across the world with the best possible societal outcome.
If you are passionate about improving the health and wellness of people across the globe using Data as your means, then you should look no farther than the Digital Team here at Sanofi. Join us on our journey in enabling Sanofi’s Digital Transformation through becoming an AI first organization.
AI Factory - Versatile Teams Operating in Cross Functional Pods: Utilizing digital and data resources to develop AI products, bringing data management, AI and product development skills to products, programs and projects to create an agile, fulfilling and meaningful work environment.
Leading Edge Tech Stack: Experience build products that will be deployed globally on a leading-edge tech stack.
World Class Mentorship and Training: Working with renowned, published leaders and academics in machine learning to further develop your skillsets.
Who You Are:
You are an experienced data engineer who actively engages in designing and developing comprehensive solutions to support and enhance business operations. You have a strong understanding of back-end and front-end technologies and have experience implementing highly functional solutions that can scale. You have a passion for good user experience and know how to ensure user needs are met through agile development and stakeholder engagement. You are an excellent communicator who enjoys collaborating with cross functional teams, works with subject matter experts, mentor and support teams and manages to deliver professional grade solutions.
We are seeking an highly skilled Data Engineer with at least 4-6 years of diverse data engineering experience who will play vital role in implementation of Data platform for Sanofi's advanced analytic, AI, and ML initiatives. As a seasoned Data Engineer, you will play a critical role in designing and implementing globally scalable data engineering platform to make it easier for internal teams to start implementation of their user case and to enhance the lives of our global patients and customers. Your expertise will be instrumental in shaping the future of data-driven healthcare while providing technical leadership and mentorship to the data engineering team.
In your role, you will be reporting to Senior Data Platform Manager, working closely with leadership to build data platform with inputs and feedback received from various teams across data foundations and business to address their immediate and long-term needs. You will be responsible for mentoring data platform engineers and have opportunity to learn from technical teams, SME's and business leaders about available platforms, tools and business processes.
Key Responsibilities:
Collaborate with experienced data engineers, data scientists, and other stakeholders to understand intricate data requirements, especially in areas like bioinformatics, omics, clinical data, and more.
Design, implement, and maintain data infrastructure on AWS Cloud to support our data mesh architecture.
Develop, automate, optimize, and fine-tune data platform provisioning, scaling, and maintenance tasks to improve operational efficiency, performance, scalability, and cost-effectiveness.
Lead data pipeline design, development, and optimization, drawing on your expertise in data integration, ETL/ELT, modern tools, and AWS Cloud, to ensure efficient data processing and cutting-edge solutions.
Implement data monitoring and alerting solutions while collaborating with DevOps teams to proactively identify and address data issues.
Ensure data security, compliance, and governance standards are met throughout the data platform, adhering to global data engineering standards and principles.
Establish and enforce global data engineering standards, ensuring strict adherence to data architecture, platform, quality, and governance principles.
Demonstrate your expertise in implementing data warehouse/lake solutions, data mesh architectures, and distributed processing technologies (e.g., Spark, Hadoop, Kafka) for production environments.
Showcase your advanced proficiency in SQL (preferably in Snowflake) and relational/non-relational databases to optimize complex data queries and manipulations.
Exhibit mastery in programming languages such as Python, Shell scripting, and Scala/Java, leveraging them to develop sophisticated data engineering solutions.
Work hand-in-hand with cross-functional agile teams to architect and implement hybrid-cloud solutions with automated pipelines, ensuring seamless and high-performance data processing.
Act as a mentor and leader, providing guidance and mentorship to junior data engineers, fostering a collaborative and growth-oriented team culture.
Engage actively with the data engineering community, sharing insights, best practices, and innovative ideas to contribute to the growth of the industry.
Document data infrastructure design, configuration, and processes for reference and training purposes.
Key Requirements:
Bachelor's/Master's degree in Computer Science, Engineering, Mathematics, or a related field. 4-6 years of proven and progressive experience in data engineering, with a strong preference for experience in the life sciences/pharmaceutical industry.
Extensive background in designing, developing, and optimizing data solutions, including data pipelines, architectures, and data sets.
Proven expertise in data integration technologies, ETL / ELT processes, and modern data engineering tools, with an emphasis on Informatica/IICS.
Experience with multimodal data systems and architectures, including batch, near real-time, and streaming data.
Demonstrated success in developing distributed architectures and processing technologies (e.g., Spark, Hadoop, Kafka) for large-scale data processing.
Expertise in developing cloud-native data platforms on AWS, ensuring high performance, scalability, and fault tolerance.
Advanced knowledge of SQL, relational/non-relational databases, and data query optimization.
Proficiency in programming languages such as Python, Shell scripting, and Scala/Java.
Expertise in managing cloud-native systems following IaC and DataOps principles (terraform, CI/CD, Orchestration, Actions)
Extensive experience with agile development processes and concepts.
Exceptional problem-solving skills and attention to detail.
Excellent communication, presentation, and interpersonal skills.
Ability to lead teams effectively and collaborate with stakeholders at all levels.
Pursue Progress
Discover Extraordinary
Better is out there. Better medications, better outcomes, better science. But progress doesn’t happen without people – people from different backgrounds, in different locations, doing different roles, all united by one thing: a desire to make miracles happen. So, let’s be those people.
Watch our ALL IN video and check out our Diversity, Equity and Inclusion actions at sanofi.com!
Sanofi is an equal opportunity employer committed to diversity and inclusion. Our goal is to attract, develop and retain highly talented employees from diverse backgrounds, allowing us to benefit from a wide variety of experiences and perspectives. We welcome and encourage applications from all qualified applicants. Accommodations for persons with disabilities required during the recruitment process are available upon request.
Thank you in advance for your interest.
Only those candidates selected for interviews will be contacted.
Follow Sanofi on Twitter: @SanofiCanada and on LinkedIn: https://www.linkedin.com/company/sanofi
#DBBCA #DDB
At Sanofi diversity and inclusion is foundational to how we operate and embedded in our Core Values. We recognize to truly tap into the richness diversity brings we must lead with inclusion and have a workplace where those differences can thrive and be leveraged to empower the lives of our colleagues, patients and customers. We respect and celebrate the diversity of our people, their backgrounds and experiences and provide equal opportunity for all.
Show more
Show less","AI, Machine learning, Natural language processing, Data mining, Data analytics, Data visualization, Data science, Data engineering, DataOps, Cloud computing, AWS, Snowflake, Informatica/IICS, Python, SQL, Shell scripting, Scala, Java, Git, Jenkins, Terraform, Agile development, DevOps, IaC","ai, machine learning, natural language processing, data mining, data analytics, data visualization, data science, data engineering, dataops, cloud computing, aws, snowflake, informaticaiics, python, sql, shell scripting, scala, java, git, jenkins, terraform, agile development, devops, iac","agile development, ai, aws, cloud computing, data engineering, data mining, data science, dataanalytics, dataops, devops, git, iac, informaticaiics, java, jenkins, machine learning, natural language processing, python, scala, shell scripting, snowflake, sql, terraform, visualization"
Senior Data Engineer,TalentWorld,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-talentworld-3785119528,2023-12-17,Brampton, Canada,Mid senior,Hybrid,"Are you a skilled and experienced Data Engineer looking for a new and exciting opportunity? We are seeking a dynamic individual to join our team and contribute to the success of our data analytics initiatives.
TalentWorld
is recruiting a
Senior Data Engineer
in
Toronto, Ontario
for one of our clients.
Responsibilities
Assist with the design and configuration of the Amazon Redshift cluster to ensure optimal cost/performance.
Implement and utilize advanced features such as auto-scaling and workload management.
Fine-tune data models, including Redshift and Data Lake (S3) integration.
Convert Oracle PL/SQL code to Amazon Redshift PL/pgSQL.
Troubleshoot any data migration issues.
Act as the main support point for team members, providing advanced Amazon Redshift knowledge.
Optimize data pipelines loading into Amazon Redshift.
Provide knowledge transfer to other team members as needed
Qualifications
Bachelor’s degree in computer science, Information Technology, or a related field.
Minimum of 5 years of hands-on experience in software development.
Proficiency in programming languages such as Java, Python, or C++.
Solid understanding of database management systems and SQL.
Experience with front-end and back-end development frameworks.
Strong problem-solving skills and attention to detail.
Excellent communication and collaboration skills.
Ability to work independently and as part of a team.
Proven track record of delivering high-quality software solutions.
Familiarity with agile development methodologies.
Continuous learning mindset to stay updated with industry trends and technologies.
Certification in relevant technologies or frameworks.
Experience with cloud computing platforms such as AWS or Azure.
Knowledge of DevOps practices and tools.
Employment Details
Salary: $65.00HR - $70.00HR
Benefits: 37.5 Hours Weekly, Contract, Hybrid
Schedule: 8:00AM to 4:00PM, Monday to Friday
Other: 6 months contract, possibility of extension.
Interested in
Senior Data Engineer
role in
Toronto, Ontario
?
#TWONTEMP
Show more
Show less","Data Engineering, Data Analytics, Amazon Redshift, Data Modeling, Data Migration, Oracle PL/SQL, Amazon Redshift PL/pgSQL, Java, Python, C++, SQL, Frontend Development Frameworks, Backend Development Frameworks, Agile Development Methodologies, AWS, Azure, DevOps Practices","data engineering, data analytics, amazon redshift, data modeling, data migration, oracle plsql, amazon redshift plpgsql, java, python, c, sql, frontend development frameworks, backend development frameworks, agile development methodologies, aws, azure, devops practices","agile development methodologies, amazon redshift, amazon redshift plpgsql, aws, azure, backend development frameworks, c, data engineering, data migration, dataanalytics, datamodeling, devops practices, frontend development frameworks, java, oracle plsql, python, sql"
Data Engineer/ Solution Architect,Matrix-IFS,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-solution-architect-at-matrix-ifs-3758977129,2023-12-17,Brampton, Canada,Mid senior,Hybrid,"Job Summary
Duties & Responsibilities
Design and architect cloud solution while using data bricks/and snowflake
Led and managed the data engineering team to deliver projects on time and within budget.
Collaborate with the Senior Data Solution Architect to translate client requirements into technical specifications.
Oversee the development and implementation of data pipelines, workflows, and ETL processes.
Technical Expertise: Utilize expertise in DataBricks and Snowflake to design and implement scalable, reliable, and efficient data solutions.
Conduct code reviews, ensuring best practices and adherence to coding standards.
Troubleshoot and resolve technical issues, supporting the team as needed.
Team Leadership & Collaboration: Mentor and guide team members, fostering a culture of continuous learning and improvement.
Facilitate cross-functional collaboration, ensuring smooth communication between
engineering, sales, and other departments.
Participate in new team members' recruitment, onboarding, and training.
Client Engagement: Serve as a technical liaison between clients and the
engineering team, ensuring alignment and satisfaction.
Assist with pre-sales support in collaboration with the Senior Data Solution
Architect.
Continuous Improvement:
Stay abreast of industry trends and emerging technologies, identifying
opportunities for innovation and growth.
Contribute to internal documentation, process improvement, and knowledge
sharing.
Desired Qualifications
Bachelor’s or master’s degree in computer science, Engineering, or related field.
Minimum of 4+ years of experience in data engineering, focusing on DataBricks and Snowflake.
Proven experience with any of the following related technologies:
Big Data Technologies - Apache Spark, Apache Kafka, Apache Flink
Cloud Platforms – AWS, GCP, Azure
DevOps and CI/CD Tools
Proven experience developing in one or more of the following programming
languages: Scala, Java, Python, SQL
Proven experience leading and managing technical teams.
Strong knowledge of data warehousing, big data technologies, cloud
platforms, and ETL processes.
Excellent communication, problem-solving, and organizational skills.
Preferred Qualifications:
Relevant certifications in Data Bricks, Snowflake, or related technologies.
Experience working in an agile development environment.
Matrix is a global, dynamic, fast-growing technical consultancy leading technology services company with 13000 employees worldwide. Since its foundation in 2001, Matrix has made more travelers and acquisitions and has executed some of the largest, most significant. The company specializes in implementing and developing leading technologies, software solutions, and products. It provides its customers with infrastructure and consulting services, IT outsourcing, offshore, training and assimilation, and Ves as representatives for the world's leading software vendors. With vast experience in private and public sectors, ranging from Finance, Telecom, Health, Hi-Tech, Education, Defense, and Secu city, Matrix's customer base includes guest organizations in Israel and a steadily growing client base worldwide. We are comprised of talented, creative, and dedicated individuals passionate about delivering innovative solutions to the market. We source and foster the best talent and recognize that all employee's contributions are integral to our company's future. Matrix- success is based on a challenging work environment, competitive compensation and benefits, and rewarding career opportunities. We encourage a diverse work environment of sharing, learning, and ceding together. Come and join the winning team! You'll be challenged and have fun in a highly respected organization.
To Learn More, Visit: www.matrix-ifs.com ,Jobs at Matrix | Join Us | Worldwide openings (matrix-globalservices.com)
EQUAL OPPORTUNITY EMPLOYER: Matrix is an Equal Opportunity Employer and Prohibits Discrimination and Harassment of Any Kind. Matrix is committed to the principle of equal employment opportunity for all employees, providing employees with a work environment free of discrimination and harassment. All employment decisions at Matrix are based on business needs, job requirements, and individual qualifications, regardless of race, color, religion or belief, family or parental status, or any other status protected by the laws or regulations in our locations. Matrix will not tolerate discrimination or harassment based on any of these characteristics. Matrix encourages applicants of all ages
.
Show more
Show less","Data engineering, Data warehousing, Data pipelines, ETL, DataBricks, Snowflake, Apache Spark, Apache Kafka, Apache Flink, AWS, GCP, Azure, DevOps, CI/CD, Scala, Java, Python, SQL","data engineering, data warehousing, data pipelines, etl, databricks, snowflake, apache spark, apache kafka, apache flink, aws, gcp, azure, devops, cicd, scala, java, python, sql","apache flink, apache kafka, apache spark, aws, azure, cicd, data engineering, databricks, datapipeline, datawarehouse, devops, etl, gcp, java, python, scala, snowflake, sql"
Lead Data Engineer - Quality,CPP Investments | Investissements RPC,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/lead-data-engineer-quality-at-cpp-investments-investissements-rpc-3733821770,2023-12-17,Brampton, Canada,Mid senior,Hybrid,"Company Description
Make an impact at a global and dynamic investment organization
When you invest your career in CPP Investments, you join one of the most respected and fastest growing institutional investors in the world. With current assets under management valued in excess of $500 billion, CPP Investments is a professional investment management organization that globally invests the funds of the Canada Pension Plan (CPP) to help ensure long-term sustainability. The CPP Fund is projected to reach $3 trillion by 2050. CPP Investments invests in all major asset classes, including public equity, private equity, real estate, infrastructure and fixed-income instruments, and is headquartered in Toronto with offices in Hong Kong, London, Luxembourg, Mumbai, New York City, San Francisco, São Paulo and Sydney.
CPP Investments attracts and selects high-calibre individuals from top-tier institutions around the globe. Join our team and look forward to:
Diverse and inspiring colleagues and approachable leaders
Stimulating work in a fast-paced, intellectually challenging environment
Accelerated exposure and responsibility
Global career development opportunities
Being motivated every day by CPP Investments’ important social purpose and unshakable principles
A flexible/hybrid work environment combining in office collaboration and remote working
A deeply rooted culture of Integrity, Partnership and High Performance
If you share a passion for performance, value a collegial and collaborative culture, and approach everything with the highest integrity, here’s an opportunity for you to invest your career at CPP Investments.
Job Description
We are looking for a hands-on Lead Data Engineer to plan/develop and lead quality engineering and automation strategy for data products and platforms on AWS. The ideal candidate should have extensive hands-on experience in building quality engineering standards and strategy across the team by defining and implementing detailed, comprehensive automated test strategy and leading QE strategy. Ensuring that our company's products and services meet or exceed customer expectations for quality. This includes developing and implementing quality assurance processes, conducting audits and inspections, and working closely with other departments to ensure that quality is maintained throughout the product lifecycle.
Job Responsibilities:
Lead data engineers responsible for product quality and verifying software products or services.
Plan, build and implement resilient systems/products which follows the chaos engineering and fault injection practices.
Build and maintain automated test scripts to improve resiliency, efficiency, and test coverage.
Develop and implement quality assurance strategies, processes, and procedures to ensure that products or services meet or exceed customer expectations.
Collaborate with product management, and other cross-functional teams to identify and prioritize quality issues and drive resolution.
Define and execute test plans, test cases, and test scripts to ensure that products or services meet QE standards
Analyze test results, identify defects, and work to resolve issues.
Monitor and report on product or service quality metrics to drive continuous improvement.
Mentor and coach team members to improve their technical skills and knowledge of quality engineering best practices.
Stay up to date with industry trends and advancements in quality engineering and apply them to improve product or service quality.
Conduct disaster recovery, audits, and inspections to ensure product compliance with quality standards and regulatory requirements.
Develop and maintain product quality metrics and KPI’s to track performance and identify areas for improvement.
Participate in product design and development to ensure that quality is built in from the beginning.
Participate in end-to-end development life cycle – requirements, analysis, development, testing, and deployment.
Qualifications
University degree in Engineering or Computer Science preferred.
7+ years relevant experience in Data/Infrastructure/Quality Engineering.
Hand-on experience building resilient systems/products which follows the chaos engineering and fault injection practices.
Hands-on experience writing clean, reusable, and efficient terraform, python code on AWS.
Experience in big data technologies specially AWS EMR, SPARK, HIVE, HUDI, JupyterHub, Livy
Proficiency in Python libraries like pyspark, pandas, Boto3, matplotlib, Plotly/Dash, etc.
Hands-on experience in test planning – identifying test objectives, test cases, test data and test environment.
Hands-on experience with test automation tools like Playwright, Selenium, Appium, TestComplete, Robot Framework, etc.
Hand-on experience with test management tools like Zephyr, qTest, TestRail, etc.
Hands-on experience in test automation, test execution of test plans, text cases and reporting test results.
Proficient in defect management must be able to identify, report and track defects effectively.
Experience with core AWS technologies such as EMR, EKS, Lakeformation, Athena, Glue, EC2, ELB, NLB, Auto Scaling, S3, EFS, Lambda, API Gateway, Step Functions, Cloudwatch, VPC, Route 53, ACM.
Demonstrated ability to easily deal with both abstract and concrete concepts and be able to reconcile them for the appropriate audience and context preferred.
Quickly understand organizational dynamics and management priorities, and to be able to work effectively in a fast paced, results driven company.
Demonstrate strong facilitation, negotiation, interpersonal, and collaboration skills
Strong problem-solving skills with excellent written and verbal communication skills.
Ability to work independently and collaboratively in a team environment.
Preferred Qualifications:
AWS Certifications.
Experience with BI tools like Tableau, AWS QuickSight.
Experience with other cloud platforms such as GCP, Azure.
Experience with Cloud based data and analytics platforms, warehouses (Redshift/Spectrum, Databricks, Snowflake), OLAP systems (Clickhouse, Druid), including a mix of relational, non-relational, streaming and event-based architectures.
Understanding of web development technologies including HTML, CSS, and JavaScript.
CI/CD pipelines using Terraform, Jenkins, Sonarcube, Github actions, Gitflow
Understanding of agile development methodologies.
Additional Information
Visit our
LinkedIn Career Page
or
Follow us
on
LinkedIn.
At CPP Investments, we are committed to diversity and equitable access to employment opportunities based on ability.
We thank all applicants for their interest but will only contact candidates selected to advance in the hiring process.
Our Commitment to Inclusion and Diversity:
In addition to being dedicated to building a workforce that reflects diverse talent, we are committed to fostering an inclusive and accessible experience. If you require an accommodation for any part of the recruitment process (including alternate formats of materials, accessible meeting rooms, etc.), please let us know and we will work with you to meet your needs.
Disclaimer:
CPP Investments does not accept resumes from employment placement agencies, head-hunters or recruitment suppliers that are not in a formal contractual arrangement with us. Our recruitment supplier arrangements are restricted to specific hiring needs and do not include this or other web-site job postings. Any resume or other information received from a supplier not approved by CPP Investments to provide resumes to this posting or web-site will be considered unsolicited and will not be considered. CPP Investments will not pay any referral, placement or other fee for the supply of such unsolicited resumes or information.
Mandatory Vaccine Policy
All employees in the Toronto, Sydney and Hong Kong offices will be required to be vaccinated against COVID-19. Accommodations to this policy will be made for medical or other protected grounds. Please contact us to discuss any accommodation needs.
Show more
Show less","Quality engineering, Fault injection, Chaos engineering, Unit testing, Functional testing, Integration testing, Regression testing, Performance testing, Scalability testing, Security testing, Test automation, Test planning, Test design, Test execution, Test reporting, Defect management, Jira, Confluence, Python, Pyspark, Pandas, Boto3, Matplotlib, Plotly, Dash, Terraform, AWS, EMR, EKS, Lakeformation, Athena, Glue, EC2, ELB, NLB, Auto Scaling, S3, EFS, Lambda, API Gateway, Step Functions, Cloudwatch, VPC, Route 53, ACM, Tableau, AWS QuickSight, GCP, Azure, Redshift, Spectrum, Databricks, Snowflake, Clickhouse, Druid, HTML, CSS, JavaScript, Terraform, Jenkins, Sonarcube, Github actions, Gitflow, Agile development","quality engineering, fault injection, chaos engineering, unit testing, functional testing, integration testing, regression testing, performance testing, scalability testing, security testing, test automation, test planning, test design, test execution, test reporting, defect management, jira, confluence, python, pyspark, pandas, boto3, matplotlib, plotly, dash, terraform, aws, emr, eks, lakeformation, athena, glue, ec2, elb, nlb, auto scaling, s3, efs, lambda, api gateway, step functions, cloudwatch, vpc, route 53, acm, tableau, aws quicksight, gcp, azure, redshift, spectrum, databricks, snowflake, clickhouse, druid, html, css, javascript, terraform, jenkins, sonarcube, github actions, gitflow, agile development","acm, agile development, api gateway, athena, auto scaling, aws, aws quicksight, azure, boto3, chaos engineering, clickhouse, cloudwatch, confluence, css, dash, databricks, defect management, druid, ec2, efs, eks, elb, emr, fault injection, functional testing, gcp, gitflow, github actions, glue, html, integration testing, javascript, jenkins, jira, lakeformation, lambda, matplotlib, nlb, pandas, performance testing, plotly, python, quality engineering, redshift, regression testing, route 53, s3, scalability testing, security testing, snowflake, sonarcube, spark, spectrum, step functions, tableau, terraform, test automation, test design, test execution, test planning, test reporting, unit testing, vpc"
"Lead Full Stack Data Engineer, Data Foundation, Data Platforms",CPP Investments | Investissements RPC,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/lead-full-stack-data-engineer-data-foundation-data-platforms-at-cpp-investments-investissements-rpc-3574567945,2023-12-17,Brampton, Canada,Mid senior,Hybrid,"Company Description
Make an impact at a global and dynamic investment organization
When you invest your career in CPP Investments, you join one of the most respected and fastest growing institutional investors in the world. With current assets under management valued in excess of $500 billion, CPP Investments is a professional investment management organization that globally invests the funds of the Canada Pension Plan (CPP) to help ensure long-term sustainability. The CPP Fund is projected to reach $3 trillion by 2050. CPP Investments invests in all major asset classes, including public equity, private equity, real estate, infrastructure and fixed-income instruments, and is headquartered in Toronto with offices in Hong Kong, London, Luxembourg, Mumbai, New York City, San Francisco, São Paulo and Sydney.
CPP Investments attracts and selects high-calibre individuals from top-tier institutions around the globe. Join our team and look forward to:
Diverse and inspiring colleagues and approachable leaders
Stimulating work in a fast-paced, intellectually challenging environment
Accelerated exposure and responsibility
Global career development opportunities
Being motivated every day by CPP Investments’ important social purpose and unshakable principles
A flexible/hybrid work environment combining in office collaboration and remote working
A deeply rooted culture of Integrity, Partnership and High Performance
If you share a passion for performance, value a collegial and collaborative culture, and approach everything with the highest integrity, here’s an opportunity for you to invest your career at CPP Investments.
Job Description
We are looking for an experienced Full Stack Data Engineer to join our team for building a next generation data platform built on Data Mesh architecture/principles. The ideal candidate should have extensive hands-on experience of building a big data platform, Big Data Technologies, Data Pipelines, backend development using Python, BI/Analytics tools as well as experience with DevOps, AWS, and UI Development in Angular JS. You will be responsible for designing, developing, and maintaining our web applications and data pipelines, as well as implementing CI/CD best practices.
Responsibilities:
Design, build, and maintain scalable and efficient data platform using data engineering technologies such as Glue, EMR, Athena, Redshift, Lake Formation, Apache Spark, Hive, HDFS and Trino.
Build/manage data pipelines, and common data related cross cutting concerns like data catalog, data lineage, data quality, data profiling, data discovery, metadata management
Develop and maintain web applications using AngularJS and Python.
Build/manage BI/Analytical dashboard reducing time to insight for the business stakeholders.
Implement CI/CD pipelines using Terraform, Jenkins, Github actions, Gitflow.
Collaborate with cross-functional teams to develop and implement new features.
Write clean, reusable, and efficient code.
Participate in code reviews and ensure code quality.
Develop and maintain APIs using Python and ensure API security and best practices are implemented.
Implement SSO integration with Microsoft Azure AD using oAuth, OIDC, and SAML.
Implement integration with AWS Cognito for user authentication and authorization.
Ensure the application is optimized for maximum speed and scalability.
Troubleshoot and debug issues as they arise.
Implement DevOps best practices to ensure efficient application deployment and management.
Collaborate with data scientists and analysts to integrate data analytics solutions with web applications.
Stay up to date with emerging trends and technologies.
Qualifications
Hands on Experience with data engineering technologies such as AWS Glue, EMR, Athena, Redshift, Lake Formation, Apache Spark, Apache Hive, Apache Airflow, S3FS, Apache Hudi and Trino.
Extensive experience of building data pipelines using orchestration tool like Apache Airflow. Hands on experience on building cross cutting concerns like data catalog, data lineage, data quality, data profiling, data discovery, metadata management
Proven experience as a Full Stack Developer with AngularJS and Python.
Strong understanding of web development technologies including HTML, CSS, and JavaScript.
Experience working with RESTful APIs and JSON. Familiarity with microservices architecture.
Additional Information
Visit our
LinkedIn Career Page
or
Follow us
on
LinkedIn.
At CPP Investments, we are committed to diversity and equitable access to employment opportunities based on ability.
We thank all applicants for their interest but will only contact candidates selected to advance in the hiring process.
Our Commitment to Inclusion and Diversity:
In addition to being dedicated to building a workforce that reflects diverse talent, we are committed to fostering an inclusive and accessible experience. If you require an accommodation for any part of the recruitment process (including alternate formats of materials, accessible meeting rooms, etc.), please let us know and we will work with you to meet your needs.
Disclaimer:
CPP Investments does not accept resumes from employment placement agencies, head-hunters or recruitment suppliers that are not in a formal contractual arrangement with us. Our recruitment supplier arrangements are restricted to specific hiring needs and do not include this or other web-site job postings. Any resume or other information received from a supplier not approved by CPP Investments to provide resumes to this posting or web-site will be considered unsolicited and will not be considered. CPP Investments will not pay any referral, placement or other fee for the supply of such unsolicited resumes or information.
Mandatory Vaccine Policy
All employees in the Toronto, New York, San Francisco, Mumbai, Sydney, Hong Kong, and Sao Paulo offices will be required to be vaccinated against COVID-19. Accommodations to this policy will be made for medical or other protected grounds. Please contact us to discuss any accommodation needs.
Show more
Show less","Data Engineering, Data Pipelines, Apache Spark, Python, AngularJS, AWS, DevOps, Jenkins, Microsoft Azure AD, RESTful APIs, JSON, HTML, CSS, JavaScript, Data Cataloging, Data Discovery, Data Lineage, Data Profiling, Data Quality, Metadata Management, Microservices, OWAA, OIDC, SAML, AWS Cognito, API Security, Gitflow","data engineering, data pipelines, apache spark, python, angularjs, aws, devops, jenkins, microsoft azure ad, restful apis, json, html, css, javascript, data cataloging, data discovery, data lineage, data profiling, data quality, metadata management, microservices, owaa, oidc, saml, aws cognito, api security, gitflow","angularjs, apache spark, api security, aws, aws cognito, css, data cataloging, data discovery, data engineering, data lineage, data profiling, data quality, datapipeline, devops, gitflow, html, javascript, jenkins, json, metadata management, microservices, microsoft azure ad, oidc, owaa, python, restful apis, saml"
Senior Data Engineer,Viral Nation,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-viral-nation-3720875741,2023-12-17,Brampton, Canada,Mid senior,Hybrid,"At
Viral Nation,
we specialize in building social-first ecosystems for brands to connect with the modern consumer journey. Our integrated solutions align strategy, talent, media, and technology with culturally relevant creativity to scale the world’s fastest-growing digital brands. Viral Nation offers a fluid, creative, and growth-oriented environment that will support your ambitions to apply your talents in an open, collaborative, and fast-paced culture. Our ability to stay at the forefront of the industry has fueled our success and will guide us in paving the path forward. We’re driven to push boundaries and think beyond today to deliver strategies, and we’re just getting started.
While we continuously exceed our goals, we need your help – our success is only as great as our people. Strong performance leads to high expectations, and we must keep raising the bar!
What you’ll do here:
Designing, building and maintaining efficient, reusable, and reliable architecture and code.
Lead a team: task delegation, organization, motivation and feedback
Conduct code reviews and deployment upon task completion
Participate in the architecture and system design discussions
Participate in developing data mesh architectures and building big data pipelines
Independently perform hands on development/coding and unit testing of the applications
Collaborate with the development and AI teams and build individual components into complex enterprise web systems
Work in a team environment with product, frontend design, production operation, QE/QA and cross functional teams to deliver a project throughout the whole software development cycle
Architect and implement CI/CD strategy for EDP
Implement high velocity streaming solutions using Kafka (preferred), Azure Stream Analytics, and Event Hubs
Ensure the best possible performance and quality of high scale web applications and services
To identify and resolve any performance issues
Keep up to date with new technology development and implementation
Participate in code review to make sure standards and best practices are met
Migrate data from traditional relational database systems, file systems, NAS shares to Azure relational databases such as Azure SQL Database or Azure Synapse Analytics
Migrate data from APIs to Azure data lake and relational databases such as PostgreSQL to Synapse
Work closely with the Data Scientist leads, CTO, Product, Engineering, DevOps and other members of the AI Data Science teams
Collaborate with the product team, share feedback from project implementations and influence the product roadmap.
Be comfortable in a highly dynamic, agile environment without sacrificing the quality of work products.
What you need to have:
Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience
5+ years of experience as a Data application developer
2+ years of experience with Apache Kafka or Confluent Kafka
Primary responsibilities are focused on the Analysis, Design, Development, and Testing of features utilizing Confluent Kafka.
Maintain and enhance Confluent Kafka architecture, Confluent Kafka design principles, and CI/CD Deploåyment procedures.
Experience with building streaming applications with Confluent Kafka
Experience leading a small to medium-sized team
Proficiency in Azure services including but not limited to Azure Data Lake Storage, Azure Synapse Analytics, Azure Databricks, Azure Cosmos DB, and Azure Machine Learning.
Experience in developing Azure-based data lakes, data warehouses, and business analytics solutions for large enterprises.
Strong programming skills in Python, Shell scripting, and SQL with experience in Azure-based application development.
Expertise in building Azure-based solutions encompassing data ingestion, storage, integration, processing, and access.
Solid understanding and implementation experience with NoSQL/SQL databases and object stores within Azure.
Hands-on experience with Azure services such as Azure Functions, Azure Logic Apps, Azure SQL Database, Azure Cosmos DB, etc.
Knowledge of Data Science models and Azure AI solutions including Azure Cognitive Services such as Azure Computer Vision, Azure Speech Services, and Azure Natural Language Processing.
Experience with Azure Data services
Experience in building MLOps Pipelines
Proficient in Python, NodeJS, RestAPI, Microservices, Postman, GraphQL, MongoDB
Viral Nation is committed to diversity, equity and inclusion in our agency. Viral Nation welcomes applications from people with visible and non-visible disabilities. Accommodations are available on request for candidates taking part in all aspects of the recruiting and selection process.
Show more
Show less","Data Application Development, Apache Kafka, Confluent Kafka, Azure Services, Azure Data Lake Storage, Azure Synapse Analytics, Azure Databricks, Azure Cosmos DB, Azure Machine Learning, Python, Shell Scripting, SQL, Azurebased Application Development, NoSQL/SQL Databases, Azure Functions, Azure Logic Apps, Azure SQL Database, Data Science Models, Azure AI Solutions, Azure Cognitive Services, MLOps Pipelines, NodeJS, RestAPI, Microservices, Postman, GraphQL, MongoDB","data application development, apache kafka, confluent kafka, azure services, azure data lake storage, azure synapse analytics, azure databricks, azure cosmos db, azure machine learning, python, shell scripting, sql, azurebased application development, nosqlsql databases, azure functions, azure logic apps, azure sql database, data science models, azure ai solutions, azure cognitive services, mlops pipelines, nodejs, restapi, microservices, postman, graphql, mongodb","apache kafka, azure ai solutions, azure cognitive services, azure cosmos db, azure data lake storage, azure databricks, azure functions, azure logic apps, azure machine learning, azure services, azure sql database, azure synapse analytics, azurebased application development, confluent kafka, data application development, data science models, graphql, microservices, mlops pipelines, mongodb, nodejs, nosqlsql databases, postman, python, restapi, shell scripting, sql"
Data Architect with Azure,Zortech Solutions,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-architect-with-azure-at-zortech-solutions-3667480053,2023-12-17,Brampton, Canada,Mid senior,Hybrid,"Role: Data Architect with Azure
Location: Remote/Canada
Duration: Contract
Job Description
Product centric mindset and business focused outcomes
Ability to span across multiple platform engineering tracks
Ability to assess the current state of platform and recommend future state
Ability to work with cross functional teams
Responsibilities
The ideal candidate will have a proven track record in leading and delivering Azure Data Analytics solutions with Enterprise level organizations. We are seeking someone who can create and deliver a BI vision with customers across many sectors. You will be highly proficient working with business stakeholders and directing project teams and will have direct experience of a range of BI Tools and Platforms.
You will have excellent experience and a proven track record architecting enterprise-level data platforms. You will manage your own and your team's performance to deliver high quality, highly perform an solutions to our customer.
Your Main Responsibilities Will Be
Provide technical and engineering development oversight over one or more engineering teams
Engineer and automate platform solutions to meet and exceed expectations of Product Leadership
Designing and delivering Azure Data Analytics solutions
Requirements Analysis and Solution Architecture Design.
Data Modelling and data architecture design.
Show more
Show less","Azure, Azure Data Analytics, BI, Data Analytics, Business Intelligence, Data Architect, Data Architecture, Data Modelling, Requirements Analysis, Solution Architecture","azure, azure data analytics, bi, data analytics, business intelligence, data architect, data architecture, data modelling, requirements analysis, solution architecture","azure, azure data analytics, bi, business intelligence, data architect, data architecture, data modelling, dataanalytics, requirements analysis, solution architecture"
Research Data Systems Analyst,The State University of New York,"Stony Brook, NY",https://www.linkedin.com/jobs/view/research-data-systems-analyst-at-the-state-university-of-new-york-3762293414,2023-12-17,Medford,United States,Mid senior,Onsite,"Required Qualifications (as Evidenced By An Attached Resume)
Bachelor's degree (foreign equivalent or higher). Four (4) years of full-time experience in clinical research informatics or information technology. Experience working in a hospital, medical center, or research setting. Experience with data collection and analysis. Experience working in consulting, training, and/or supporting end-users.
Preferred Qualifications
Five (5) years of full-time experience in clinical research informatics or information technology. Experience in REDCap or OnCore. Experience with MySQL or other RDBMS. Experience in data modeling, data architecture, data integration technologies, and/or complex data environments. Experience in JavaScript, Python, PHP, or another modern scripting language.
Brief Description Of Duties
The Research Data Systems Analyst leads efforts to provide technical and information systems-related development and support services to research teams and other initiatives as part of Stony Brook University's Clinical and Translational Science Center (CTSC). Working with diverse, multidisciplinary teams, the incumbent will utilize information architecture and software development best practices to document, analyze, and translate research workflow needs into various technical designs and informatics solutions. These activities will assist in driving the CTSC’s core mission of accelerating clinical research discoveries and their application to patient care, “from bench to bedside.” The incumbent will apply biomedical informatics (BMI) technical standards, methodologies, and principles in such areas as data capture, data integration, and data analytics and visualization to advance program needs, objectives, and outcomes. Solutions may include but are not limited to building complex data capture workflows, designing data monitoring dashboards, and implementing interoperability interfaces, both within and outside of existing development platforms. This position involves interfacing with multiple levels of the Stony Brook University Enterprise, including faculty, research support staff, and senior leaders, and will work within the Research Computing, Informatics, & Innovation team under the Chief Research Information Officer.
Responsibilities of the Research Data Systems Analyst may include the following but are not limited to:
Work closely with diverse stakeholders (including clinicians, investigators, leadership, and research support staff) to define requirements and devise database technology solutions that meet client needs, follow best practices, and comply with institutional policies and regulations.
Exercise technical leadership to plan and carry out successful development projects, adhering to coding standards and validation practices to ensure high-quality solutions.
Utilize technical and problem-solving skill sets to troubleshoot database design issues and coordinate enhancements, both individually and in collaboration with other team members.
Automate repetitive tasks to achieve operational efficiency through the use of scripting, routines, stored procedures, etc. Build automated workflows to support the extract-transform-load (ETL), cleaning, and validation of data as needed.
Work with counterparts at surveillance sites to coordinate continuous data quality processes, provide access to the central data repositories, troubleshoot data quality issues, and develop visualization and dashboards for meeting operational and site performance monitoring requirements.
Collaborate with team members on database modeling and other technical functions to support and advance the objectives of the CTSC.
Collaborate with team members to create and maintain documentation, including changelogs, technical specifications, and SOPs.
Provide consultation, training, and support to users of clinical research information systems as needed.
Assist in the administration of clinical research information systems to ensure stability, security, and performance.
Identify workflow and technical issues for process improvement and change management. Scope out new technologies as needed, perform proof of concept testing, and make recommendations to the team.
Maintain list of open queries and communicate list to group in weekly meetings. Contribute to presentations for both internal and external audiences. May provide oversight, mentorship, and cross-training to other analysts on the team.
Non-essential: Stay informed about new technologies, practices, and industry trends through community lists, publications, and conferences. Other duties or projects as appropriate to rank and departmental mission.
Special Notes
The Research Foundation of SUNY is a private educational corporation. Employment is subject to the Research Foundation policies and procedures, sponsor guidelines and the availability of funding. FLSA Exempt position, not eligible for the overtime provisions of the FLSA. Minimum salary threshold must be met to maintain FLSA exemption.
Resume/CV and cover letter should be included with the online application.
Stony Brook University is committed to excellence in diversity and the creation of an inclusive learning, and working environment. All qualified applicants will receive consideration for employment without regard to race, color, national origin, religion, sex, pregnancy, familial status, sexual orientation, gender identity or expression, age, disability, genetic information, veteran status and all other protected classes under federal or state laws.
If you need a disability-related accommodation, please call the university Office of Equity and Access (OEA) at (631) 632-6280 or visit OEA .
In accordance with the Title II Crime Awareness and Security Act a copy of our crime statistics can be viewed here .
Visit our
WHY WORK HERE
page to learn about the
total rewards
we offer.
SUNY Research Foundation: A Great Place to Work.
Job Number:
2301570
Official Job Title : Senior Programmer/Analyst (Project)
Job Field
: Information Technology
Primary Location
: US-NY-Stony Brook
Department/Hiring Area : SBM IT
Schedule
: Full-time
Shift
: Day Shift Shift Hours : 8:00 a.m. - 4:00 p.m.
Posting Start Date
: Nov 9, 2023
Posting End Date
: Dec 18, 2023, 4:59:00 AM
Salary : $88,000-$110,000
Appointment Type : Regular
Salary Grade : E79
SBU Area : The Research Foundation for The State University of New York at Stony Brook
Show more
Show less","Bachelor's degree, Clinical research informatics, Information technology, Data collection, Data analysis, Consulting, Training, User support, REDCap, OnCore, MySQL, RDBMS, Data modeling, Data architecture, Data integration, Complex data environments, JavaScript, Python, PHP, Scripting languages, Information architecture, Software development, Biomedical informatics, Data capture, Data integration, Data analytics, Data visualization, ETL, Data cleaning, Data validation, Troubleshooting, Automation, Scripting, Routines, Stored procedures, Workflow automation, Documentation, Change logs, Technical specifications, SOPs, User training, User support, Clinical research information systems, Stability, Security, Performance, Process improvement, Change management, New technologies, Proof of concept testing, Presentations, Mentorship, Crosstraining, Diversity, Inclusion, Equity, Access, Crime Awareness and Security Act","bachelors degree, clinical research informatics, information technology, data collection, data analysis, consulting, training, user support, redcap, oncore, mysql, rdbms, data modeling, data architecture, data integration, complex data environments, javascript, python, php, scripting languages, information architecture, software development, biomedical informatics, data capture, data integration, data analytics, data visualization, etl, data cleaning, data validation, troubleshooting, automation, scripting, routines, stored procedures, workflow automation, documentation, change logs, technical specifications, sops, user training, user support, clinical research information systems, stability, security, performance, process improvement, change management, new technologies, proof of concept testing, presentations, mentorship, crosstraining, diversity, inclusion, equity, access, crime awareness and security act","access, automation, bachelors degree, biomedical informatics, change logs, change management, clinical research informatics, clinical research information systems, complex data environments, consulting, crime awareness and security act, crosstraining, data architecture, data capture, data cleaning, data collection, data integration, data validation, dataanalytics, datamodeling, diversity, documentation, equity, etl, inclusion, information architecture, information technology, javascript, mentorship, mysql, new technologies, oncore, performance, php, presentations, process improvement, proof of concept testing, python, rdbms, redcap, routines, scripting, scripting languages, security, software development, sops, stability, stored procedures, technical specifications, training, troubleshooting, user support, user training, visualization, workflow automation"
Global HR Data Analyst,Incitec Pivot Limited,"Murarrie, Queensland, Australia",https://au.linkedin.com/jobs/view/global-hr-data-analyst-at-incitec-pivot-limited-3756063491,2023-12-17,Brisbane, Australia,Associate,Hybrid,"The role
Implementing standardised people metrics, analytics & reporting that support the strategic and operational needs of the business. You’ll also maintain our data integrity through SAP and develop a global auditing process.
Global accountability for IPL’s People metrics, reporting, analytics & insight including:
Define and produce people metrics in support of the business’ strategic (e.g. talent), operational (e.g. labour productivity) and transactional (e.g. data required by third parties) requirements
Develop a process for extracting and collating the required data for the defined metrics
Develop a process/system/tool to translate that data into metrics in appropriate reports in support of the business’s strategic, operational, and transactional requirements
Produce reports that meet the quality and time requirements of the business
Working closely with the HR community and business leaders to understand operational challenges and provide additional data-driven insight
Lead the interface with IT and other stakeholders to ensure values within our systems are configured and maintained to meet HR requirements
About you:
Ideally you will have formal qualifications in a related field with hands on experience within an enterprise HR Data, Systems or Analytics environment. We work in a SAP environment, hence individuals with prior SAP experience are preferred.
By leveraging your advanced Microsoft Excel skills, you will be able to easily interpret data requests, extract information from diverse sources and formats, and present the data in a manner aligned to the specified requirements.
You should possess experience in constructing HR dashboards, advanced in excel, and experience reporting on Workplace Gender Equality, and DJ Sustainability Index.
Why join us?
We’re a warm, inclusive, and supportive team who work well together (flexibly) in support of delivering an excellent service to the business. This opportunity provides the foundations to take on a wide range of career paths and we continually invest in the development of our people.
About us
Incitec Pivot Limited (IPL) and its subsidiary company Dyno Nobel, is a ASX 100 global leader in the resources and agricultural sectors with an unrelenting focus on Zero Harm. With a diverse leadership, we add value for our customers through manufacturing excellence, innovation, and world class services. We provide ground-breaking solutions through practical innovation in the mining services and agriculture industry. The group comprises more than 5,000 people across the sales, commercial operations, and manufacturing footprint globally, including the key mining markets in North America, Asia Pacific, South Africa and South America.
We regularly have exciting opportunities come up within the business and can offer a rewarding career for the long term
Show more
Show less","SAP, Data Analytics, Reporting, Metrics, Microsoft Excel, Data Visualization, HR Dashboards, Workplace Gender Equality, DJ Sustainability Index","sap, data analytics, reporting, metrics, microsoft excel, data visualization, hr dashboards, workplace gender equality, dj sustainability index","dataanalytics, dj sustainability index, hr dashboards, metrics, microsoft excel, reporting, sap, visualization, workplace gender equality"
Data Analyst x 2,Cross River Rail,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-analyst-x-2-at-cross-river-rail-3779513214,2023-12-17,Brisbane, Australia,Mid senior,Onsite,"Cross River Rail Delivery Authority (CRRDA)…Your chance to work with the best!
Cross River Rail will transform the way we travel to, from and through Brisbane in the future. It will be a catalyst for 10 years of wider rail transformation across the whole of South East Queensland and it is already playing a key role in how we prepare for the 2032 Olympic and Paralympic Games.
Cross River Rail is a new 10.2km rail line including 5.9km of twin tunnels running under the Brisbane River and CBD. It includes the delivery of four new underground stations, two new above-ground stations, accessibility rebuilds for six stations, the construction of three new stations on the Gold Coast, and the introduction of a new world-class ‘ETCS' signalling system.
This is a once in a lifetime opportunity to work on one of Australia's largest infrastructure projects and to be a part of something that will leave a legacy for generations to come.
About The Role
As the Data Analyst within the Data and Analytics team at the Cross River Rail Delivery Authority, you will play a crucial role in shaping the organisation's data strategy and analytics capabilities. Reporting to the Manager Data and Analytics, you will lead and mentor junior resources, ensuring the team aligns with the broader business’ goals.
Key Responsibilities
BI Development:
Collaborate with stakeholders to translate business requirements into BI solutions.
Import, store, and display historical and dynamic data within the Integrated Data Dashboard (IDD).
Monitor and enhance Power BI reporting solutions.
Data Analysis
Utilise advanced analytics to identify trends, patterns, and anomalies for effective decision-making.
Data Integration
Integrate data from various sources to provide a unified view for reporting and analysis.
Reporting Design
Design visually appealing reports, dashboards, and visualisations using Power BI.
Stakeholder Engagement
Collaborate with diverse stakeholders across the organisation and project teams.
Lead meetings to provide insights and align solutions with business goals.
Quality Assurance
Implement data quality checks and validation processes to ensure accuracy within the IDD environment.
Documentation
Maintain clear documentation of data sources, transformations, and analytical methodologies.
Training And Support
Provide end-user training on tools, reports, and dashboards.
Train and coach Data and Analytics team members.
Innovation
Stay updated with industry trends and propose innovative solutions to enhance IDD capabilities.
About You
Bachelor’s degree in data science, computer science, information technology, or related field.
Proven experience (5+ years) in data analysis, business intelligence, and data visualization.
Solid understanding of data modelling concepts and ETL processes.
Proficiency in Power BI and data manipulation.
Excellent communication skills with the ability to convey complex technical concepts to non-technical stakeholders and develop collaborative relationships.
Experience in large-scale infrastructure projects is highly desirable.
Demonstrated leadership in reporting and business intelligence functions.
Why work with CRRDA?
On Cross River Rail you will not only work with the best but become your best. And as we enter a new phase of delivery, our project offers a rare career opportunity for top tier professionals to have played their part on something that is genuinely city shaping and transformative.
Our people are supported with flexible work options, career development through the Australian Institute of Management, salary packaging and access to world class employee assistance provider Benestar.
At Cross River Rail we are an equal opportunity employer and strive to build balanced teams from all walks of life.
How To Apply
Please submit your updated CV by clicking on the apply link before the closing date of 2nd January 2024. Please note that shortlisting may occur prior to close date if suitable candidates are identified, so please do not delay in submitting your application!
Aboriginal and Torres Strait Islander people are strongly encouraged to apply for all Cross River Rail Delivery Authority vacancies.
Show more
Show less","Data Science, Computer Science, Information Technology, Data Analysis, Business Intelligence, Data Modeling, ETL, Power BI, Data Manipulation, Data Visualization, Reporting, Stakeholder Engagement, Data Quality, Documentation, Training, Leadership, Agile, Scrum, Jira, Confluence, AWS, Azure, Google Cloud, SQL, Python, R, Java, Scala","data science, computer science, information technology, data analysis, business intelligence, data modeling, etl, power bi, data manipulation, data visualization, reporting, stakeholder engagement, data quality, documentation, training, leadership, agile, scrum, jira, confluence, aws, azure, google cloud, sql, python, r, java, scala","agile, aws, azure, business intelligence, computer science, confluence, data manipulation, data quality, data science, dataanalytics, datamodeling, documentation, etl, google cloud, information technology, java, jira, leadership, powerbi, python, r, reporting, scala, scrum, sql, stakeholder engagement, training, visualization"
Senior Data Engineer,Talenza,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-talenza-3785568997,2023-12-17,Brisbane, Australia,Mid senior,Onsite,"Our client is looking for a Senior PowerBI Developer to join their dynamic team - renowned organisation!
(Please note that this role is only open to Brisbane based candidates who are Aus citizens or PR holders.
Position Overview
As a Senior PowerBI Developer, you will play a crucial role in designing, developing, and maintaining business intelligence solutions using PowerBI. The ideal candidate will have a strong background in SQL and PowerBI, with desirable experience in Snowflake and Azure. You will collaborate with cross-functional teams to gather requirements, design data models, and create visually compelling reports and dashboards.
Responsibilities
Develop and maintain PowerBI reports and dashboards to meet business requirements.
Design and implement data models to support business intelligence solutions.
Collaborate with business stakeholders to gather and understand requirements.
Optimize and troubleshoot existing PowerBI solutions for performance and usability.
Utilize strong SQL skills to extract, transform, and load data from various sources.
Work with data engineers to integrate PowerBI with Snowflake and Azure environments.
Stay updated on industry best practices and emerging trends in PowerBI and related technologies.
Qualifications
Bachelor's degree in Computer Science, Information Systems, or a related field.
Proven experience as a PowerBI Developer in a senior role.
Strong proficiency in SQL and database design.
Experience with Snowflake and Azure is highly desirable.
Solid understanding of data modeling and ETL processes.
Excellent analytical and problem-solving skills.
Strong communication and collaboration skills.
Ability to work independently and as part of a team in a fast-paced environment.
Benefits
Competitive salary commensurate with experience.
Structured career progression
Flexible working arrangements
Professional development opportunities.
For a faster response please reach out to Tamie Tran on linkedin directly or via tamie@talenza.com.au
Show more
Show less","PowerBI, SQL, Snowflake, Azure, Data modeling, ETL, Data analysis, Problem solving, Communication, Collaboration, Team work, Fastpaced environment","powerbi, sql, snowflake, azure, data modeling, etl, data analysis, problem solving, communication, collaboration, team work, fastpaced environment","azure, collaboration, communication, dataanalytics, datamodeling, etl, fastpaced environment, powerbi, problem solving, snowflake, sql, team work"
Data Governance Lead,Talent,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/data-governance-lead-at-talent-3779063049,2023-12-17,Brisbane, Australia,Mid senior,Onsite,"Talent International
is searching for a
Data Governance Lead
to join our
Client
, a
State Government Client.
Initial contract until June 2025 and starting in January
Hourly rate up to $137.90 p/h inc super depending on experience
Based in Brisbane CBD with flexible WFH (Not fully remote)
About The Role
As the Data Governance Lead, you will be responsible for establishing key data standards, policies, and frameworks that will lead to successful outcomes on the program. Providing advisory services for both structured and unstructured data governance, you will guide the project team towards optimal data management practices and compliance.
Your Key Responsibilities
Lead the development, implementation and governance of data standards, policies, and frameworks for the entire data lifecycle within the Program and associated systems.
Direct the successful delivery of the Project by supporting the establishment of a fit-for-purpose data management and governance framework while guiding the program towards best practices in data governance.
Collaborate with the project team to establish uniform solutions for data definition, transformation, classification, archiving, disposal, and obfuscation. Ensure alignment with the broader data management and governance framework.
Serve as the primary contact in the Program for data governance, ensuring adherence to established data standards, policies, and frameworks. Address issues related to governance, data quality, and definition, acting as a key point of escalation.
Build program and organisational understanding of the importance and role of data governance, data ownership and data enablement. Facilitate stakeholder and user adoption of the new framework, standards and policies implemented by the project. Share knowledge, best practices and usage guidelines to enable effective utilisation by the program.
Role Requirements
3+ years’ experience in similar role such as a Data Manager, Data Engineer or Data Governance Manager.
Knowledge and experience in data modelling, gathering and data analysis and business intelligence design and solutions.
Knowledge and/or experience in technical environments, analytical tools and programs (e.g. SQL, Power Bi, MS Office 365 applications).
Knowledge and understanding of project management methodologies.
Certified Data Management Professional (CDMP) or similar qualification highly desirable.
Desirable experience in implementing metadata, business glossary, and data quality tools, with exposure to cloud environments and APIs.
Excellent communication and interpersonal skills, conveying complex technical concepts to non-technical stakeholders.
To find out more, please “Apply for this job” or contact David Meiring / Tom Circosta on (07) 3221 3333 or david.meiring@talentinternational.com
For a list of all vacant positions, please see our website www.talentinternational.com
Show more
Show less","Data Governance, Data Standards, Data Policies, Data Frameworks, Data Management, Data Quality, Data Analysis, Data Modeling, Business Intelligence, Data Lifecycle, Project Management, Metadata, Business Glossary, Data Quality Tools, Cloud Environments, APIs, SQL, Power BI, MS Office 365","data governance, data standards, data policies, data frameworks, data management, data quality, data analysis, data modeling, business intelligence, data lifecycle, project management, metadata, business glossary, data quality tools, cloud environments, apis, sql, power bi, ms office 365","apis, business glossary, business intelligence, cloud environments, data frameworks, data governance, data lifecycle, data management, data policies, data quality, data quality tools, data standards, dataanalytics, datamodeling, metadata, ms office 365, powerbi, project management, sql"
Lead Data Engineer - Databricks,Mantel Group,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/lead-data-engineer-databricks-at-mantel-group-3752445110,2023-12-17,Brisbane, Australia,Mid senior,Hybrid,"About Us
Mantel Group is an Australian-owned technology consulting business with capabilities across Cloud, Digital, Data & Security. Since our inception in November 2017, we have experienced remarkable growth across Australia & New Zealand and are honoured to be recognised as a Great Place to Work for 4 years in a row!
We hire smart and talented people and get out of their way. As a principle-based organisation we have a flat structure with no hierarchy. By focusing on our five principles and not getting caught up in red tape, we trust you to get the job done!
Data
Working in data at Mantel Group means there’s opportunities to align to a particular cloud platform or remain cloud agnostic. Our teams are made up of awesome people who specialise in numerous areas of data so there’s endless opportunities for you including data engineering, machine learning, data analytics and data science.
So you’re ready for something new?
As a
Lead Data Engineer - Databricks,
you will work closely with our clients to help them unlock value from their data using the Databricks platform. Your work might range from building a full data platform; building data pipelines to load large volumes of data using data devops principles; or helping with data architecture and strategy.
Here’s a snapshot of what’s in store:
Responsible for defining the technical direction of a data project, while also providing mentorship to team members
Help build trusted relationships and take a leadership role in discussions with C-level stakeholders
Crafting and presenting a clearly defined architecture that aligns with the intended purpose
Providing guidance on enterprise cloud considerations, including network security and access control
Represent Mantel Group within the data engineering community through active participation in conferences and meetups
Work on new Databricks consulting & solution offerings that address unmet needs in the market
Be a part of pre-sales conversations and provide technical advice in early stages of projects
Establish and maintain relationships with clients as a source of informed, reliable, professional advice about solution options
We want to hear from you if you have:
Experience with leading end-to-end data projects in complex enterprise environments
Background in a reputable consulting organisation, with a focus on the technology and data domains
Strong expertise in data engineering and development, particularly with Databricks and Spark
Modern Big Data experience with main focus on working in a cloud based environment (Azure, AWS or GCP)
Excellent SQL and/or Python skills, expertise in native development tools
Experience in designing and presenting complex architectures to stakeholders
Previous experience with mentoring and coaching team members in your area of expertise
A keen interest in continuous learning, including upskilling in modern technologies and obtaining new certifications
A genuine passion for and connection to the data community
What you can expect from us:
We know you won’t have one job for life. At Mantel Group we believe in supporting our team to take their career in a direction that aligns with their passions. We have internal opportunities across Cloud, Data, Security and Digital.
You’ll get all the tools you need to hit the ground running including a new phone, laptop & swag.
We believe in unique experiences for all. Our My Deal program allows you to tailor your yearly plan, with the support of your Leader, to decide on what’s most important to you. That might be extra professional development, extra annual or parental leave, time to work on your side hustle, or something else completely different! One size does not fit all.
You’ll be genuinely supported by an organisation that cares about not only you but your family as well, Mantel Group offers Flexible Personal Leave options for those unplanned moments in life.
We support a flexible hybrid approach to working which is guided by our principles; we trust each other to “make good choices” about the best workplace locations for the requirements of the project, role and client. This can change based on our client needs.
We value a diverse workplace and strongly encourage people from all backgrounds and minority groups to apply.
Check out ‘how we hire’ to find out what’s in store if you’re successful and get to know us better by visiting our website and following Mantel Group on LinkedIn.
#EAST
Show more
Show less","Data engineering, Machine learning, Data analytics, Data science, Databricks, Spark, Azure, AWS, GCP, SQL, Python, Cloud computing, Data architecture, Data pipelines, Enterprise cloud considerations, Network security, Access control, Mentorship, Consulting, Software development, Presales conversations, Client relationships, Continuous learning, Upskilling, Certifications","data engineering, machine learning, data analytics, data science, databricks, spark, azure, aws, gcp, sql, python, cloud computing, data architecture, data pipelines, enterprise cloud considerations, network security, access control, mentorship, consulting, software development, presales conversations, client relationships, continuous learning, upskilling, certifications","access control, aws, azure, certifications, client relationships, cloud computing, consulting, continuous learning, data architecture, data engineering, data science, dataanalytics, databricks, datapipeline, enterprise cloud considerations, gcp, machine learning, mentorship, network security, presales conversations, python, software development, spark, sql, upskilling"
Senior Data Engineer - AWS,Mantel Group,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-aws-at-mantel-group-3785139064,2023-12-17,Brisbane, Australia,Mid senior,Hybrid,"About Us
Mantel Group is an Australian-owned technology consulting business with capabilities across Cloud, Digital, Data & Security. Since our inception in November 2017, we have experienced remarkable growth across Australia & New Zealand and are honoured to be recognised as a Great Place to Work for 4 years in a row!
We hire smart and talented people and get out of their way. As a principle-based organisation we have a flat structure with no hierarchy. By focusing on our five principles and not getting caught up in red tape, we trust you to get the job done!
Cloud
Mantel Group’s dedicated cloud capability means you are able to work with a team of passionate people and all the latest tech in your cloud of choice whether you want to help businesses reach their full potential with Microsoft products, be part of the full cloud transformation on AWS or an expert in Google cloud - there’s an opportunity here for you.
About The Role
As a key member of our AWS Data Engineering team, you’ll be responsible for designing and deploying client environments through code, developing data pipelines, building data models and advanced queries to help our clients harness the power of data powered by AWS.
As a Senior Data Engineer - AWS you'll be:
Solution designing and delivering using AWS tools within the Data and Analytics domain.
Building strong trusted relationships with new and existing clients through thought leadership, strong delivery and great relationships which align to Mantel Group’s principles
Building and deploying data lakes using AWS and AWS data-focused services such as Kinesis, Glue, Athena, S3 and RedShift
Developing complex data pipelines using Python, EMR and Lambda functions
Solving complex data engineering issues to help our clients advance their data and analytics solution
Providing guidance and mentorship to others within the team through on-the-job teaching, hands-on training and other similar activities
Representing Mantel Group within the data engineering community
We want to hear from you if you have:
Previous experience in a data engineering role at a consulting organisation
AWS data services experience in particular CloudFormation and/or Terraform), EMR, Glue RDS, S3, EC2, Redshift and other data infrastructure automation services
Infrastructure automation through DevOps scripting using - Python, R, SparkQL or PowerShell
Querying and managing data - SQL
Data modelling - Relational, Dimensional, NoSQL, Snowflake
Code Management System - GitHub, BitBucket or Gitlab
Experience with CI/CD practices & tools - Gitlab, Jenkins
Ability to engage with senior level stakeholders and having a broad influence in the internal and client’s teams
Experience supporting technical projects and project teams varying in size
What you can expect from us:
We know you won’t have one job for life. At Mantel Group we believe in supporting our team to take their career in a direction that aligns with their passions. We have internal opportunities across Cloud, Data, Security and Digital.
You’ll get all the tools you need to hit the ground running including a new phone, laptop & swag.
We believe in unique experiences for all. Our My Deal program allows you to tailor your yearly plan, with the support of your Leader, to decide on what’s most important to you. That might be extra professional development, extra annual or parental leave, time to work on your side hustle, or something else completely different! One size does not fit all.
You’ll be genuinely supported by an organisation that cares about not only you but your family as well, Mantel Group offers Flexible Personal Leave options for those unplanned moments in life.
We support a flexible hybrid approach to working which is guided by our principles; we trust each other to “make good choices” about the best workplace locations for the requirements of the project, role and client. This can change based on our client needs.
Click ‘Apply’ to be considered for this role and our Talent team will be in touch.
We value a diverse workplace and strongly encourage people from all backgrounds and minority groups to apply.
#EAST
Show more
Show less","Cloud, Digital, Data, Security, AWS, Microsoft, Google Cloud, Data Engineering, Solution Designing, Data Pipelines, Data Models, Advanced Queries, Kinesis, Glue, Athena, S3, RedShift, Python, EMR, Lambda Functions, DevOps Scripting, SQL, Relational, Dimensional, NoSQL, Snowflake, GitHub, BitBucket, Gitlab, CI/CD, Gitlab, Jenkins, Terraform, RDS, EC2, R, SparkQL, PowerShell","cloud, digital, data, security, aws, microsoft, google cloud, data engineering, solution designing, data pipelines, data models, advanced queries, kinesis, glue, athena, s3, redshift, python, emr, lambda functions, devops scripting, sql, relational, dimensional, nosql, snowflake, github, bitbucket, gitlab, cicd, gitlab, jenkins, terraform, rds, ec2, r, sparkql, powershell","advanced queries, athena, aws, bitbucket, cicd, cloud, data, data engineering, data models, datapipeline, devops scripting, digital, dimensional, ec2, emr, github, gitlab, glue, google cloud, jenkins, kinesis, lambda functions, microsoft, nosql, powershell, python, r, rds, redshift, relational, s3, security, snowflake, solution designing, sparkql, sql, terraform"
Business Analyst - Data Services,Retained,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/business-analyst-data-services-at-retained-3778750486,2023-12-17,Brisbane, Australia,Mid senior,Hybrid,"Job Title: Business Analyst – Data Services (6-Month Contract, Potential for Extension)
Location:
Brisbane CBD (80% Remote)
Overview:
Join a significant transformation project with our client in Brisbane! We're seeking a skilled Business Analyst for the Data Services Workstream. This role is crucial in reshaping data processes, contributing to a new platform vision, and improving customer outcomes.
Key Responsibilities:
As a Business Analyst in the Data Services Workstream, you will:
Collaborate on a greenfield project to reset the data strategy and implement a new platform approach.
Lead initial requirements gathering aligned with the new data strategy.
Identify data requirements, sources, and document the data model.
Evaluate security, compliance, and governance across the data lifecycle.
Identify data for analytics, reporting, and dashboards.
Your Experience:
Proven experience in data projects, including analysis, warehousing, modeling, governance, and mapping.
Expertise in Data Management practices (cataloging, quality, lineage, metadata).
Familiarity with operational reporting and analytics.
Ideal Previous Project Experience:
Preferably within a Data transformation or Business Intelligence program.
Track record with Greenfields projects or large early-stage initiatives.
Exposure to a platform approach or B2C analytics for insights.
Additional Specifics:
Analytical Skills:
Ability to analyze complex data sets and derive meaningful insights.
Data Visualization:
Proficiency in tools like Tableau or Power BI for visually appealing dashboards.
SQL Knowledge:
Understanding of SQL for querying databases.
Statistical Knowledge:
Basic statistical knowledge for interpreting data trends.
Business Acumen:
Understanding of the business domain to align data analysis with goals.
Communication Skills:
Clear communication for translating data findings to non-technical stakeholders.
Problem-Solving:
Ability to identify business problems and propose data-driven solutions.
Data Modeling:
Understanding how to create data models.
Technical Proficiency:
Familiarity with BI tools, data warehouses, and ETL processes.
Attention to Detail:
Precision in handling data for reliable results.
Adaptability:
Ability to adapt to technological advancements and changing business needs.
Team Collaboration:
Excellent collaboration and interpersonal skills.
For more information, contact
Michael Tran
on
0448 879 648
or email
michael@retained.com.au
.
Show more
Show less","Business Analysis, Data Strategy, Data Modeling, Data Governance, Data Warehousing, Data Analytics, Tableau, Power BI, SQL, Statistics, Business Acumen, Communication, ProblemSolving, Data Visualization, BI Tools, ETL Processes, Attention to Detail, Adaptability, Team Collaboration","business analysis, data strategy, data modeling, data governance, data warehousing, data analytics, tableau, power bi, sql, statistics, business acumen, communication, problemsolving, data visualization, bi tools, etl processes, attention to detail, adaptability, team collaboration","adaptability, attention to detail, bi tools, business acumen, business analysis, communication, data governance, data strategy, dataanalytics, datamodeling, datawarehouse, etl, powerbi, problemsolving, sql, statistics, tableau, team collaboration, visualization"
Senior Data Analyst,Troocoo,"Brisbane, Queensland, Australia",https://au.linkedin.com/jobs/view/senior-data-analyst-at-troocoo-3767565410,2023-12-17,Brisbane, Australia,Mid senior,Hybrid,"A new 12 month contract opportunity is currently on offer for a Senior Data Analyst (Reporting & Insights) to join this highly reputable, and well recognised Statutory Authority, based in Fortitude Valley.
The role of the Senior Data Analyst will be accountable for analysing processes and data, defining critical performance reporting requirements and for the development of optimised and actionable reporting which provide quality insights and intelligence.
This supports the vertical integration to department, group & corporate KPI’s as well horizontal integration into associated strategic KRA’s to ensure short- & long-term department performance targets are achieved.
Key Responsibilities of the Senior Data Analyst will be as below;
Maintain dynamic prioritisation of a small sized team responsibilities and drive for faster execution on emerging business opportunities.
Run advanced and predictive analyses and perform model assessments, validation, and enhancement activities, using predictive analytics' software tools and functionalities.
Develop personal capabilities using existing formal and informal training opportunities, while also coaching others as required.
Collect business requirements using a variety of methods such as interviews, document analysis, workshops, and workflow analysis to express the requirements in terms of target user roles and goals.
Run advanced and predictive analyses and perform model assessments, validation, and enhancement activities, using predictive analytics' software tools and functionalities.
Conduct research and select relevant information to enable analysis of key themes and trends using primary data sources and business intelligence tools.
Prepare and coordinate the completion of various data and analytics reports.
To be considered for this role, we are seeking Senior Level Candidates demonstrating the following key technical experience;
Experience
Advanced knowledge and experience in using data analysing & visualisation tools such as tableau or power BI.
Demonstrated experience in the design or development of performance reports for complex systems.
Demonstrated experience in SQL, DAX or equivalent languages.
Demonstrated ability to simplify complexity in order to generate insight.
Interested applicants are urged to apply online ASAP or reach out to shameem.rpinsloo@troocoo.com to discuss this opportunity further.
Show more
Show less","Data Analysis, Reporting, Tableau, Power BI, SQL, DAX, Predictive Analytics, Model Assessment, Model Validation, Model Enhancement, Data Visualization, Business Intelligence Tools, Performance Reporting, Data Interpretation, Business Requirements Gathering, Interviews, Document Analysis, Workshops, Workflow Analysis, Target User Roles, Goals","data analysis, reporting, tableau, power bi, sql, dax, predictive analytics, model assessment, model validation, model enhancement, data visualization, business intelligence tools, performance reporting, data interpretation, business requirements gathering, interviews, document analysis, workshops, workflow analysis, target user roles, goals","business intelligence tools, business requirements gathering, data interpretation, dataanalytics, dax, document analysis, goals, interviews, model assessment, model enhancement, model validation, performance reporting, powerbi, predictive analytics, reporting, sql, tableau, target user roles, visualization, workflow analysis, workshops"
Senior Data Engineer,Harnham,"West Yorkshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-harnham-3781939814,2023-12-17,West Yorkshire, United Kingdom,Mid senior,Onsite,"To Apply for this Job Click Here
Senior Data Engineer
Hybrid - 3x per week, Manchester
£65, 000
Permanent
An opportunity for a Senior Data Engineer to join a financial services company and implement the data warehouse.
The Company
The company is an award-winning secured loans provider specialising in homeowner and broker lending.
They are looking for a Senior Data Engineer to work with the Lead Data Engineer and the rest of the team to build out the data warehouse and reporting solutions to enable decision making, optimise efficiency, and identify new opportunities.
The Role And Responsibilities
Defining and building data warehouse and reporting solutions
Providing technical and consultancy reports to stakeholders
Develop and carry out architectural blueprints and best practices
Technical leadership for junior team members
Your Skills And Experience
Experience with Azure and Azure DevOps
Experience with data modelling
Experience building data lakes and/or data warehouses
Experience with Big Data technologies
Experience with Power BI
Benefits
Up to £65, 000
Healthcare plan
Childcare vouchers
To Apply for this Job Click Here
Show more
Show less","Azure, Azure DevOps, Data Modelling, Data Lakes, Data Warehouses, Big Data Technologies, Power BI","azure, azure devops, data modelling, data lakes, data warehouses, big data technologies, power bi","azure, azure devops, big data technologies, data lakes, data modelling, data warehouses, powerbi"
Lead Data Engineer / Data Architect,Enexus Global Inc.,United States,https://www.linkedin.com/jobs/view/lead-data-engineer-data-architect-at-enexus-global-inc-3766088969,2023-12-17,Dalton,United States,Mid senior,Remote,"Job Title - Lead Data Engineer / Data Architect (12 Years)
Location - Remote
Contract Type - W2/C2C/1099
Experience - 11+ Years minimum
Skills - Pyspark, Azure, Matillion, Python
Job Description :-
Overall 10+ years of experience in Data Engineering, Data Modelling.
Excellent (Hands-On) in Azure, ADF, Databricks (PySpark), Python, SQL, Unix Shell scripting. Good experience in Snowflake Datawarehouse.
Experience in ETL, tools like Matillion, QLIK, DataStage, and performance tuning / optimization.
Experience in building dimensional data models.
Experience in working with ML teams to coordinate and enable the promotion of ML models to a governed production environment to bring stability and robustness to be supported by AMS (Data Ops and ML Ops team) teams.
Coordinating with onshore and offshore cross-functional teams to deliver concurrent projects.
Strong ETL experience in handling large volumes of data in the complex heterogeneous data warehouses and processing high volume jobs.
Experience in building data ingestion pipeline and data replication into cloud environments hosted on Azure platform.
Ability to architect scalable data pipelines, following the best practices.
Show more
Show less","Data Engineering, Data Modelling, Azure, Azure Data Factory (ADF), Databricks, PySpark, Python, SQL, Unix Shell scripting, Snowflake, ETL, Matillion, QLIK, DataStage, Data Ingestion, Data Replication, Cloud Architecture, Scalable Data Pipelines","data engineering, data modelling, azure, azure data factory adf, databricks, pyspark, python, sql, unix shell scripting, snowflake, etl, matillion, qlik, datastage, data ingestion, data replication, cloud architecture, scalable data pipelines","azure, azure data factory adf, cloud architecture, data engineering, data ingestion, data modelling, data replication, databricks, datastage, etl, matillion, python, qlik, scalable data pipelines, snowflake, spark, sql, unix shell scripting"
Lead Data Engineer - Remote,Enexus Global Inc.,United States,https://www.linkedin.com/jobs/view/lead-data-engineer-remote-at-enexus-global-inc-3695496591,2023-12-17,Dalton,United States,Mid senior,Remote,"Location - Remote
Contract Type - W2/C2C/1099
Minimum Experience - 12+ Years
Responsibilities
Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation.
Collaborate with product and technology teams to design and validate the capabilities of the data platform
Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability
Provide technical support and usage guidance to the users of our platform’s services.
Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services.
Qualifications
Experience building and optimizing data pipelines in a distributed environment
Experience supporting and working with cross-functional teams
Proficiency working in Linux environment
8+ years of advanced working knowledge of SQL, Python, and PySpark
5+ years of experience with using a broad range of AWS technologies
Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline
Experience with platform monitoring and alerts tools
Show more
Show less","Python, PySpark, SQL, AWS, GitLab automation, Jenkins, CodeBuild, CodePipeline, Linux","python, pyspark, sql, aws, gitlab automation, jenkins, codebuild, codepipeline, linux","aws, codebuild, codepipeline, gitlab automation, jenkins, linux, python, spark, sql"
Network Data Engineer,Danta Technologies,"Ashburn, VA",https://www.linkedin.com/jobs/view/network-data-engineer-at-danta-technologies-3768023020,2023-12-17,Harpers Ferry,United States,Mid senior,Onsite,"Job Location - Ashburn or Reston, VA (Day 1 onsite)
Requirement -
8 years in network domain
Expert level of knowledge in minimum of the 2 domain below (Build/operation experience)
Cisco ACI
VMware NSX
F5 (LTM/GTM)
Infoblox
Citrix Netscalar
SDWAN (Versa/Silpverpeak/Viptella/Fortinet)
CCNA/CCNP; CCIP will be added Advantage
Knowledge on MPLS , BGP, EIGRP, OSPF.
Ready to work on different technologies
Excellent on verbal and written English communication
Excellent Customer handling skills
Candidate should be open for 24*7 environments.
Notes
:- All qualified applicants will receive consideration for employment without regard to race, color, religion, religious creed, sex, national origin, ancestry, age, physical or mental disability, medical condition, genetic information, military and veteran status, marital status, pregnancy, gender, gender expression, gender identity, sexual orientation, or any other characteristic protected by local law, regulation, or ordinance.
Benefits
: Danta offers a compensation package to all W2 employees that are competitive in the industry. It consists of competitive pay, the option to elect healthcare insurance (Dental, Medical, Vision), Major holidays and Paid sick leave as per state law.
The rate/ Salary range is dependent on numerous factors including Qualification, Experience and Location.
Show more
Show less","Cisco ACI, VMware NSX, F5 (LTM/GTM), Infoblox, Citrix Netscalar, SDWAN (Versa/Silpverpeak/Viptella/Fortinet), CCNA, CCNP, CCIP, MPLS, BGP, EIGRP, OSPF","cisco aci, vmware nsx, f5 ltmgtm, infoblox, citrix netscalar, sdwan versasilpverpeakviptellafortinet, ccna, ccnp, ccip, mpls, bgp, eigrp, ospf","bgp, ccip, ccna, ccnp, cisco aci, citrix netscalar, eigrp, f5 ltmgtm, infoblox, mpls, ospf, sdwan versasilpverpeakviptellafortinet, vmware nsx"
Senior Site Reliability Engineer - Data Management (SQL Server),Visa,"Ashburn, VA",https://www.linkedin.com/jobs/view/senior-site-reliability-engineer-data-management-sql-server-at-visa-3785220112,2023-12-17,Harpers Ferry,United States,Mid senior,Onsite,"Company Description
Visa is a world leader in digital payments, facilitating more than 215 billion payments transactions between consumers, merchants, financial institutions and government entities across more than 200 countries and territories each year. Our mission is to connect the world through the most innovative, convenient, reliable and secure payments network, enabling individuals, businesses and economies to thrive.
When you join Visa, you join a culture of purpose and belonging – where your growth is priority, your identity is embraced, and the work you do matters. We believe that economies that include everyone everywhere, uplift everyone everywhere. Your work will have a direct impact on billions of people around the world – helping unlock financial access to enable the future of money movement.
Join Visa: A Network Working for Everyone.
Job Description
What the Data Services Team does at Visa:
At Visa, the Data Services Team plays a diverse and dynamic role with a strong emphasis on data management. The primary responsibility is to ensure the reliability and consistency of both inbound and outbound data. This involves proactive file queue management and technical analysis of any exceptions.
This role also prioritises the proactive management and resolution of customer and issuer incidents, upholding our team's reputation for quick and effective incident resolution, in accordance with Service Level Agreement (SLA) standards. Issues that affect the timeliness and accuracy of inbound or outbound transactional data loading are identified and proactively managed until resolution.
Effective Change Management is another key component of this role. The responsibility here is to facilitate seamless transitions, including the onboarding and offboarding of customers, issuers, and data feeds. The role requires frequent communication with customers, banks, and other transactional data issuers, managing the incoming data from these entities and the outgoing distribution of data. The objective is to build robust working relationships with these entities to ensure smooth transition processes.
While this summary outlines the main responsibilities, this role may also be asked to perform other specific tasks as needed. The ultimate goal is to cultivate a spirit of teamwork and collaboration, establishing an effective and efficient departmental operation in line with our team's mission. Occasionally, this position may require work outside of standard hours.
Why this is important to Visa
The Data Services team holds accountability for initiating data loading and delivery incidents with partners, and proactively notifying them about any failures. The team also manages operational relationships with data issuers and collaborates with them on behalf of our customers to fulfill requests.
As a core team in the business with expertise in data management, Data Services is involved in many internal exercises and projects. We play a critical role in configuring data loading components in production/UAT environments and transition for any new partner.
Given the service levels we need to meet with multiple partners, we have proactively created critical file failure processes to minimise the impact on our customers. While we work on Service Requests to add revenue to the business, we also generate billing opportunities that allow the Client Relationship teams to invoice partners for any requested data corrections.
This is a hybrid position. Hybrid employees can alternate time between both remote and office. Employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs.
Qualifications
Basic Qualifications
2+ years of relevant work experience and a Bachelors degree, OR 5+ years of relevant work experience
Preferred Qualifications
3 or more years of work experience with a Bachelor’s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)
Proven track record in providing technical service and support.
Hands-on experience with web protocols and infrastructure, including HTTP, Firewalls, Public Key Infrastructure, and SFTP (SSH File Transfer Protocol)
Experience with data management principles and methodologies, including ETL processes.
Proficiency in SQL, particularly in troubleshooting stored procedures.
Experience with transactional systems in sectors such as banking, finance, or telecommunications.
Experience using software tools such as MS SQL to investigate and diagnose issues.
Experience working in Cloud/SaaS environments.
Knowledge of ITIL practices.
Prior client-facing experience, demonstrating excellent communication and interpersonal skills.
Experience collaborating with global teams across multiple time zones.
Ideally a tertiary qualification with appropriate Microsoft or other relevant certifications.
Ability to work independently, take initiative, and manage time-critical tasks effectively.
Ability to identify and resolve data-related issues, ensuring data quality, reliability, and integrity.
Ability to comprehend complex information and communicate it effectively, both verbally and in writing.
Understanding of commonly used data file formats in the industry, as well as data privacy standards and regulations.
Flexible and ability to adapt to a changing environment.
Strong accuracy and meticulous attention to detail.
Ability to prioritise customer needs and communicate professionally and effectively with internal and external partners.
Ability to develop and maintain strong relationships both internally and externally.
Ability to evaluate situations, gather and analyse facts and determine critical issues.
Additional Information
Work Hours:
Varies upon the needs of the department.
Travel Requirements:
This position requires travel 5-10% of the time.
Mental/Physical Requirements:
This position will be performed in an office setting. The position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers.
Visa is an EEO Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Visa will also consider for employment qualified applicants with criminal histories in a manner consistent with EEOC guidelines and applicable local law.
Visa will consider for employment qualified applicants with criminal histories in a manner consistent with applicable local law, including the requirements of Article 49 of the San Francisco Police Code.
U.S. APPLICANTS ONLY: The estimated salary range for a new hire into this position is 102,300.00 to 130,500.00 USD, which may include potential sales incentive payments (if applicable). Salary may vary depending on job-related factors which may include knowledge, skills, experience, and location. In addition, this position may be eligible for bonus and equity. Visa has a comprehensive benefits package for which this position may be eligible that includes Medical, Dental, Vision, 401 (k), FSA/HSA, Life Insurance, Paid Time Off, and Wellness Program.
Show more
Show less","Data management, ETL processes, SQL, Stored procedures, Transactional systems, Banking, Finance, Telecommunications, MS SQL, Cloud/SaaS, ITIL, Data privacy, Data regulations, Microsoft certifications","data management, etl processes, sql, stored procedures, transactional systems, banking, finance, telecommunications, ms sql, cloudsaas, itil, data privacy, data regulations, microsoft certifications","banking, cloudsaas, data management, data privacy, data regulations, etl, finance, itil, microsoft certifications, ms sql, sql, stored procedures, telecommunications, transactional systems"
Lead Data Center Facility Engineer,Pkaza - Critical Facilities Recruiting,"Ashburn, VA",https://www.linkedin.com/jobs/view/lead-data-center-facility-engineer-at-pkaza-critical-facilities-recruiting-3763103933,2023-12-17,Harpers Ferry,United States,Mid senior,Onsite,"This opportunity is working directly with a leading mission-critical data center developer / wholesaler / colo provider. This firm provides data center solutions custom-fit to the requirements of their client's mission-critical operational facilities. They provide reliability of mission-critical facilities for many of the world's largest organizations and government facilities supporting Enterprise Clients, Colocation Providers and Hyperscale Companies. This opportunity provides a career-growth minded role with exciting projects with leading-edge technology and innovation as well as competitive salaries and benefits.
Lead Data Center Facility Engineer - Ashburn, VA
We are looking for a Lead Data Center Facility Engineer to support a critical data center. The Lead Critical Facility Engineer will be responsible for monitoring compliance with contractual commitments, directing technical and administrative tasks related to the operation of the critical facility infrastructure and assuring compliance with company’s policies and procedures. The Data Center is a 7x24x365 a year operation.
Responsibilities
Assist in managing both routine and emergency service on a variety of state-of-the-art critical systems such as: medium voltage switchgear, diesel generators, UPS systems, power distribution equipment, chillers, cooling towers, CRAC / CRAH, computer room air handlers, fire detection / suppression; building monitoring systems – BAS and BMS; etc.
Troubleshoot and run diagnostics, conduct preventive maintenance and emergency repairs on critical facilities equipment within the Data Center as specified and on schedule
Supervise the on-site management of sub-contractors and vendors, ensuring that all work is performed according to established practices and procedures
Assist with the preparation of analyses and reports on the data center infrastructure, develops project plans and conducts briefings and updates the Critical Facilities Manager
Manage local client relationship and act as the point of contact for the company at this site
Establish performance benchmarks, conduct analyses and prepare reports on all aspects of the critical facility operations and maintenance
Work with IT managers and other business leaders to coordinate projects, manage capacity and optimize plant safety, performance, reliability and efficiency
Create, utilize and administer MOPs and SOPs for all work on critical facilities / data center facility equipment
Schedule work activities, within specified change control / management protocol
Maintain a constant state of readiness in support of the mission goal of 99.999% uptime
Assist with CMMS database management
Qualifications:
7-10+ years of data center / critical facility operations and maintenance experience
Solid Understanding of Critical Power, Electrical Power Systems and Equipment - UPS, Generators, Switchgears, PDU's, and Mechanical equipment - Air handling units, CRAC, CRAH, Chillers, HVAC, computer grade cooling systems, DC Power Systems, and building monitoring / control systems – BAS / BMS
Experience supervising or managing a technical staff (Critical facilities technicians)
Certifications / training in either Electrical or Mechanical Designations
Experience working with Electrical or Mechanical Trades a plus
Previous experience interfacing and supporting Customers
Strong verbal and written communications skills
Previous experience in the Military / Military veterans in all branches that have experience with Electrical / Mechanical is a huge plus (Navy nukes- EMN, ETN, MMNs, SeaBees, Army - Power Generation, Air Force – Power Production, Generator Techs, Maritime, Coast Guard, etc.)
Experience working with various computer systems and applications, CMMS Inventory tracking
Submittal Instructions:
Please send resumes to resume@pkaza.com (resume at pkaza dot com) with 16331526 in the subject. After applying, if you have further questions, you may call 973-895-5200 and ask for Iggy. You can also submit via our career portal and take a look at other Critical Facility openings we are working on at, https://www.pkaza.com/jobs/
If you are in Data Center Facilities but this opportunity is not what you are looking for let’s still talk. New openings are coming in weekly and others we are not allowed to post. We also not only keep leads confidential but pay for referrals as well.
Company offers competitive salaries and benefits package including medical insurance, a 401(k) plan.
EOE/AA Employer M/F/D/V
Pkaza, LLC is a third party employment firm. All fees assessed by Pkaza will be paid by our employer that we represent and not by the candidate.
Show more
Show less","Data Center Operations, Critical Facility Maintenance, Power Systems, UPS Systems, Generators, Switchgears, PDU's, Mechanical Equipment, Air Handling Units, CRAC, CRAH, Chillers, HVAC, Computer Grade Cooling Systems, DC Power Systems, Building Monitoring Systems, BAS, BMS, Electrical Designations, Mechanical Designations, CMMS Inventory Tracking, Verbal Communication, Written Communication","data center operations, critical facility maintenance, power systems, ups systems, generators, switchgears, pdus, mechanical equipment, air handling units, crac, crah, chillers, hvac, computer grade cooling systems, dc power systems, building monitoring systems, bas, bms, electrical designations, mechanical designations, cmms inventory tracking, verbal communication, written communication","air handling units, bas, bms, building monitoring systems, chillers, cmms inventory tracking, computer grade cooling systems, crac, crah, critical facility maintenance, data center operations, dc power systems, electrical designations, generators, hvac, mechanical designations, mechanical equipment, pdus, power systems, switchgears, ups systems, verbal communication, written communication"
Principal Data Scientist,Verizon,"Ashburn, VA",https://www.linkedin.com/jobs/view/principal-data-scientist-at-verizon-3779750118,2023-12-17,Harpers Ferry,United States,Mid senior,Onsite,"When you join Verizon
Verizon is one of the world's leading providers of technology and communications services, transforming the way we connect around the world. We're a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together-lifting up our communities and striving to make an impact to move the world forward. If you're fueled by purpose, and powered by persistence, explore a career with us. Here, you'll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.
What you'll be doing...
We use digital data as the fuel to propel us beyond our competition. As a Senior Data Scientist, you'll lead projects that develop and perform complex analyses using Big Data technologies. You'll be constantly on the lookout for new modeling techniques, evolving technologies, and emerging industry trends so that we can stay ahead of the game. Your work will help us meet our customers' needs and make it even easier for them to do business with us. Responsibilities include:
Design, develop, and test custom analytic software systems for telecommunications and data communications applications.
Analyze customer requirements and develop concepts for new applications.
Effectively work in an inter-disciplinary team environment.
Coordinate with project management, software architects, other engineering and data science teams in determining overall system solutions.
Support the scoping and implementation of technical solutions: estimate, prioritize, and coordinate development activities.
Apply both procedural and object oriented techniques and Agile methodologies.
Author technical documentation as needed.
Support QA team in developing test plans.
Where you'll be working...
In this worksite-based role, you'll work onsite at a defined location, Ashburn, VA.
What we're looking for...
With an eye towards improving performance and predictability, you like the science of analytics. Developing resolutions to complex problems, using your sharp judgment to develop methods, techniques, and evaluation criteria allows you to deliver solutions that make a huge impact. You're able to communicate technical information to non-technical audiences, and you take pride in your ability to share your considerable knowledge with your peers.
You'll need to have:
Bachelor's or four or more years of work experience
Six or more years of relevant work experience
Experience in IT software development with some Big Data software development
Predictive Analytics model implementation experience in production environments using ML/DL libraries like TensorFlow, H20, Pytorch, Sci-kit Learn.
Experiences in designing, developing, optimizing, and troubleshooting complex data analytic pipelines and ML model applications using big data related technologies such as Spark or Hive
Must be able to pass an extensive background investigation as a condition of employment.
Even better if you have one or more of the following:
Bachelor's or advanced degree in computer science, applied math, statistics or other relevant quantitative discipline, or equivalent industry experience
Four or more years of relevant work experience as a data scientist, analyst, or statistical modeler.
Master's/Ph.D in Computer Science or relevant technology field.
Experience in using NLP, Bi/Visual analytics, Graph Databases like Neo4j/OrientDB/Neptune
Programming in Python and R using distributed frameworks like PySpark, Spark, SparkR
Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, etc. and their real-world advantages/drawbacks
Rigorous understanding of statistics and ability to discern appropriate statistical techniques to problem-solve
Proven expertise optimizing extraction, transformation and loading of data from a wide variety of data sources using Apache NiFi
Familiarity with virtualization/containerization, DevOps and CI/CD tools for automation of build, packaging, deployment, and testing
Experience with Atlassian's agile development tools including Bitbucket, Jira and Confluence.
Experience with programming languages, like Java, Python, or Scala.
Excellent written and verbal communication skills.
Good soft skills in working with other engineering and analytical teams to arrive at optimal solutions to technical challenges.
High degree of initiative with a passion for learning technology.
Why Verizon?
Verizon is committed to maintaining a Total Rewards package which is competitive, valued by our employees, and differentiates us as an Employer of Choice.
We are a 'pay for performance' company and your contribution is rewarded through competitive salaries, performance-based incentives and an employee Stock Program. We create an opportunity for us all to share in the success of Verizon and the value we help to create through this broad-based discretionary equity award program.
Your benefits are market competitive and delivered by some of the best providers.
You are provided with a full spectrum of health and wellbeing resources, including a first in-class Employee Assistance Program, to empower you to make positive health decisions.
We offer generous paid time off benefits.
Verizon provides training and development for all levels, to help you enhance your skills and develop your career, from funding towards education assistance, award-winning training, online development tools and access to industry research.
You will be able to take part in volunteering opportunities as part of our environmental, community and sustainability commitment.
If Verizon and this role sound like a fit for you, we encourage you to apply even if you don't meet every ""even better"" qualification listed above.
#STSERP22
Where you'll be working
In this worksite-based role, you'll work onsite at a defined location(s).
Scheduled Weekly Hours
40
Equal Employment Opportunity
We're proud to be an equal opportunity employer - and celebrate our employees' differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.
Show more
Show less","Agile, Apache NiFi, Atlassian, Big Data, Bitbucket, Confluence, DevOps, H20, Hive, Java, Jira, ML/DL, Neo4j, NLP, OrientDB, PySpark, Python, R, Scala, Scikit Learn, Spark, SparkR, TensorFlow","agile, apache nifi, atlassian, big data, bitbucket, confluence, devops, h20, hive, java, jira, mldl, neo4j, nlp, orientdb, pyspark, python, r, scala, scikit learn, spark, sparkr, tensorflow","agile, apache nifi, atlassian, big data, bitbucket, confluence, devops, h20, hive, java, jira, mldl, neo4j, nlp, orientdb, python, r, scala, scikit learn, spark, sparkr, tensorflow"
Data Center Operation Engineer,HTC Global Services,"Ashburn, VA",https://www.linkedin.com/jobs/view/data-center-operation-engineer-at-htc-global-services-3743875557,2023-12-17,Harpers Ferry,United States,Mid senior,Hybrid,"HTC Global Services wants you. Come build new things with us and advance your career. At HTC Global you'll collaborate with experts. You'll join successful teams contributing to our clients' success. You'll work side by side with our clients and have long-term opportunities to advance your career with the latest emerging technologies.
Data Center Operations designs, installs & maintains the world's largest Cloud Computing Infrastructure. to help us expand AWS Cloud offerings to the next level. To keep up with this demand on both disk and network capacity, Provide Professional Services including but not limited to implementation, audit and design. Ensure automation, scalability and availability of AWS infrastructure.
Minimum of 8 years of experience in architecture, design, implementation, and operational expertise in infrastructure/applications hosted in on-premises and cloud Develop and manage the data center equipment placement process for multiple data centers in compliance with policies and procedures.
Experience in Manage AWS Security tools (such as Guard Duty, Trusted Advisor, Secret Manager, Parameter Store, Inspector).
Experience in Creating AWS CloudFormation and managing AWS Security Groups.
Implement Python code and AWS Lambda function to automate security.
At HTC Global Services our consultants have access to a comprehensive benefits package. Benefits can include Paid-Time-Off, Paid Holidays, 401K matching, Life and Accidental Death Insurance, Short & Long Term Disability Insurance, and a variety of other perks.
EEO/M/F/V/H
Show more
Show less","Cloud Computing Infrastructure, AWS Cloud, AWS Lambda, Python, AWS Security Groups, AWS CloudFormation, AWS Guard Duty, AWS Trusted Advisor, AWS Secret Manager, AWS Parameter Store, AWS Inspector","cloud computing infrastructure, aws cloud, aws lambda, python, aws security groups, aws cloudformation, aws guard duty, aws trusted advisor, aws secret manager, aws parameter store, aws inspector","aws cloud, aws cloudformation, aws guard duty, aws inspector, aws lambda, aws parameter store, aws secret manager, aws security groups, aws trusted advisor, cloud computing infrastructure, python"
Senior Data Analyst,Jobs for Humanity,"Cincinnati, OH",https://www.linkedin.com/jobs/view/senior-data-analyst-at-jobs-for-humanity-3770548229,2023-12-17,Norwood,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with FIS Global to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: FIS Global
Job Description
Job Opportunity: Senior Data Analyst
Position Type:
Full-time
Type of Hire:
Experienced (relevant combination of work and education)
Education Desired:
Bachelor of Computer Science
Travel Percentage:
1 - 5%
Job Description:
Are you curious, motivated, and forward-thinking? At FIS, we offer an opportunity to work on some of the most challenging and relevant issues in financial services and technology. We believe in being part of a team that is open, collaborative, entrepreneurial, passionate, and, above all, fun!
About the Team:
As a key member of the Data Analytics & Strategic Insights team, you will contribute to Worldpay's Global reporting function. Together with the Global Financial Operations group, we strive to drive change in FIS's Global Merchant solutions through the delivery of insights. Join us and be at the center of the rapidly changing and growing payments industry, utilizing leading cloud-based technologies. The position is based in Cincinnati, with global analytics teams in the UK.
What You Will Be Doing:
No sponsorship available*
Working as a Senior Data Analyst in a diverse team to create a single source of truth for informed company decisions
Interpreting data, identifying patterns and trends, and delivering timely and impactful insights
Creating and managing complex data models and mining large datasets for opportunities
Shaping our new cloud-based data warehouse and reporting capabilities
Understanding stakeholder needs for self-service data delivery
Key Responsibilities:
Utilizing a hypothesis-driven problem-solving approach for exploratory data-mining analysis
Interpreting and communicating insights to multiple business teams
Collaborating with business leaders to develop meaningful metrics and visualizations
Developing in-depth understanding of data structures and business use
Gathering and documenting business requirements for data modeling decisions
Compiling data from different sources into a single, usable dataset
Identifying and implementing data-driven process optimization
Working in a fast-paced and dynamic environment
What You Bring:
No sponsorship available*
Bachelor's degree in Engineering, Math, Data Analytics, or related field
4+ years of relevant experience
Experience in data mining, root cause analysis, and querying large datasets
Proficiency with relational databases, OLAP Cubes, and data warehousing
Strong analytical acumen, ability to integrate various data sources, and interpret insights
Ability to work with large, complex datasets and design efficient processes
Proficiency in analytics tools (e.g., R, SQL) and visualization tools (Tableau)
Passionate about data and driving insight
Agile and solution-focused team player
Ability to work in a fast-paced and dynamic environment
What We Offer You:
A multifaceted job with a high degree of responsibility and opportunities
A modern, international work environment with a motivated team
The chance to work on challenging issues in financial services and technology
Professional education and personal development opportunities
Varied and challenging work to help grow your technical skillset
Privacy Statement:
FIS is committed to protecting the privacy and security of all personal information. For specific information on how FIS protects personal information online, please see the Online Privacy Notice.
EEOC Statement:
FIS is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, marital status, genetic information, national origin, disability, veteran status, and other protected characteristics. The EEO is the Law poster is available here, and the supplement document is available here.
Sourcing Model:
FIS primarily employs a direct sourcing model for recruitment. We do not accept resumes from recruitment agencies unless they are on our preferred supplier list. FIS does not take responsibility for any related fees for resumes submitted to job postings.
#pridepass
Response:
You will work full-time as a Senior Data Analyst at FIS
Prior relevant experience and education are required
You should have a bachelor's degree in Computer Science
There may be occasional travel, approximately 1-5% of the time
We embrace curiosity, motivation, and forward-thinking
FIS fosters an open, collaborative, entrepreneurial, passionate, and fun team culture
You will be part of the Data Analytics & Strategic Insights team, driving change in the global reporting function
This role is based in Cincinnati, with global analytics teams in the UK
Your responsibilities include interpreting data, identifying patterns and trends, and delivering impactful insights
You will shape our new cloud-based data warehouse and reporting capabilities
We value your ability to work with large, complex datasets and design efficient processes for accurate and timely information
Proficiency in analytics tools (e.g., R, SQL) and visualization tools (Tableau) is required
We offer a modern work environment, professional development opportunities, and the chance to work on challenging financial and technological issues
FIS is committed to protecting your privacy and upholding equal opportunity for all applicants
Recruitment at FIS is primarily done through direct sourcing, with limited involvement from recruitment agencies
Show more
Show less","Data Mining, Root Cause Analysis, Relational Databases, OLAP Cubes, Data Warehousing, SQL, R, Tableau, Data Modeling, Data Visualization, Analytical Acumen, Data Interpretation, Data Integration, Process Optimization, Agile, HypothesisDriven ProblemSolving, Stakeholder Collaboration, Metrics and Visualization Development, Business Use Understanding, DataDriven Process Optimization","data mining, root cause analysis, relational databases, olap cubes, data warehousing, sql, r, tableau, data modeling, data visualization, analytical acumen, data interpretation, data integration, process optimization, agile, hypothesisdriven problemsolving, stakeholder collaboration, metrics and visualization development, business use understanding, datadriven process optimization","agile, analytical acumen, business use understanding, data integration, data interpretation, data mining, datadriven process optimization, datamodeling, datawarehouse, hypothesisdriven problemsolving, metrics and visualization development, olap cubes, process optimization, r, relational databases, root cause analysis, sql, stakeholder collaboration, tableau, visualization"
"Junior Data Analyst | Bristol/Hybrid | £25,000 – £30,000 |",Opus Recruitment Solutions,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-analyst-bristol-hybrid-%C2%A325-000-%E2%80%93-%C2%A330-000-at-opus-recruitment-solutions-3774809810,2023-12-17,Bristol, United Kingdom,Associate,Remote,"Junior Data Analyst | Bristol/Hybrid | £25,000 – £30,000 |
Power BI | SQL | Business Intelligence | Data Engineering | Data Analyst | Dashboards | Visualisation
Are you looking to join a company where you will have huge growth potential? Or maybe you are looking to join a company that offers fantastic training and development. If so I have a role for you.
I have teamed up with a leading industrial services provider that provide a range of solutions for various sectors mostly for the renewable energy and construction sector.
They are looking for a Junior Data Analyst to come on board and produce dashboards, interact with internal and external stakeholders and ensure that data is correctly uploaded into management programs.
What are they after?
Power BI
SQL
If you also have experience working at a civil engineering company this will be a big bonus.
What is it in for you?
Matched pension
25 days holiday + bank
Private healthcare
Half a day on a Friday
And more
If you are anyone in your network are interested in this role send your CV to adam.holmes@opurs.com to apply.
Sponsorship is not available.
Power BI | SQL | Business Intelligence | Data Engineering | Data Analyst | Dashboards | Visualisation
Show more
Show less","Power BI, SQL, Business Intelligence, Data Engineering, Data Analytics, Dashboards, Visualisation","power bi, sql, business intelligence, data engineering, data analytics, dashboards, visualisation","business intelligence, dashboard, data engineering, dataanalytics, powerbi, sql, visualisation"
Data Analyst,Propel Finance,"Newport, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-propel-finance-3779431022,2023-12-17,Bristol, United Kingdom,Associate,Hybrid,"We are seeking a talented and motivated mid-level data analyst to join our team in the asset finance sector. As a data analyst, you will play a key role in analysing and visualising data, contributing to critical decision-making processes, and driving business growth. Proficiency in SQL and Tableau is required, and a desire to further develop coding and visualisation skills is highly valued. Whilst experience in the finance sector is preferred, it is not a prerequisite.
Key Responsibilities:
Gather, clean, and transform data from various sources to perform in depth analysis.
Develop and maintain data models, databases, and data systems for efficient data retrieval and reporting
Analyse complex data sets to identify trends, patterns, and insights to drive strategic decision making.
Prepare regular reports and dashboards to provide actionable insights to stakeholders.
SQL development and optimization:
Utilise SQL to query and manipulate large datasets, ensuring data integrity and accuracy.
Develop and optimise SQL queries and stored procedures for efficient data retrieval and analysis.
Collaborate with cross functional teams to understand data requirements and provide solutions.
Tableau Visualisation:
Design and develop interactive dashboards, visualisations, and reports using Tableau
Translate complex data into meaningful and visually appealing insights for stakeholders.
Enhance existing Tableau visualisations to improve usability and functionality.
Data quality assurance:
Perform data validation and quality checks to ensure accuracy and consistency of data.
Identify and resolve data inconsistencies, anomalies, and errors.
Implement data governance best practises to maintain data integrity.
Continuous learning and development:
Stay up to date with industry trends, emerging technologies, and best practises in data analysis and visualisation.
Actively seek opportunities to enhance coding skills, expand knowledge of asset finance, and acquire new tools and techniques.
Role Requirements:
Strong proficiency in SQL for data manipulation, extraction and analysis
Proficiency in Tableu for data visualisation and dashboard development
Familiarity with data modelling, data warehousing, and database design principals
Ability to analyse and interpret complex data sets to extract meaningful insights
Strong communication skills to collaborate with cross functional teams
Any experience with programming languages, such as Python, would be a bonus.
This is a hybrid role with an expectation of a one day every couple of months in the office.
Show more
Show less","SQL, Tableau, Data analysis, Data visualization, Data warehousing, Data modeling, Data governance, Data quality assurance, Python, Programming languages, Statistical analysis, Machine learning, Data storytelling, Business intelligence, Data mining, Data integration, Data architecture","sql, tableau, data analysis, data visualization, data warehousing, data modeling, data governance, data quality assurance, python, programming languages, statistical analysis, machine learning, data storytelling, business intelligence, data mining, data integration, data architecture","business intelligence, data architecture, data governance, data integration, data mining, data quality assurance, data storytelling, dataanalytics, datamodeling, datawarehouse, machine learning, programming languages, python, sql, statistical analysis, tableau, visualization"
Sustainability Data Analyst,Renishaw,"Wotton under Edge, England, United Kingdom",https://uk.linkedin.com/jobs/view/sustainability-data-analyst-at-renishaw-3773516153,2023-12-17,Bristol, United Kingdom,Associate,Hybrid,"Salary:
£33,000 - £43,000 (depending on experience)
Hybrid working:
2 days per week on site, 3 from home
Working within the Sustainability Team at Renishaw offers an exciting opportunity to be part of a rapidly growing field that addresses the global challenges of climate change and human rights. Renishaw has set ambitious sustainability targets to reduce our environmental footprint and enhance our social impact.
As a Sustainability Data Analyst, you will play a pivotal role in driving sustainability initiatives and helping Renishaw to meet its targets, as well as contributing to a low carbon future. If you have a passion for making a difference, then working in the Sustainability Team at Renishaw offers a unique and rewarding career.
Responsibilities
- Data collection and analysis:
Lead on the collection and maintenance of sustainability data from various sources, including internal records, third-party databases, and industry benchmarks.
Responsible for ensuring data accuracy and integrity through regular data validation and quality control processes.
Assist in the completion of the quarterly and annual reporting.
- Reporting and audits:
Responsible for administering and maintaining Renishaw’s GHG software system (SCCS)
Support our global subsidiaries with data collection through our audit processes
Responsible for monitoring group wide data collated and entered in the SCCS system by the Sustainability Team
Lead on the development and delivery of appropriate training to SCCS users
Produce reports and dashboards on key ESG metrics
- Continuous improvement:
Lead on the identification of opportunities to optimise data collection processes and improve sustainability practices within the organization.
Support global subsidiary and divisional activities
Horizon scanning for forthcoming data requirements
Key requirements
Be conversant with relevant software or able to learn quickly (SCCS, Teamcenter, Syteline, Microsoft Office 365 Suite, SharePoint),
An advanced standard in Excel is needed for this role
Be a natural problem solver with numerate and analytical skills
Working knowledge of latest sustainability standards, tools, and processes:
Basic understanding of sustainability, GHG reporting standards(GHG Protocol, ISSB, ISO14074, Life Cycle Analysis) and corporate regulatory reporting requirements
Desirable requirements
Hold or willing to work towards Affiliate IEMA membership
Knowledge of environmental management systems, i.e., ISO 14001, ISO 50001
Experience working in an engineering and/or manufacturing sector
Experience in database/system management
Person specification
As a Sustainability Data Analyst, you’ll be a tenacious, organised, and proactive individual who shows the ability to understand how their work fits with the bigger picture at Renishaw. You’ll have an eye for detail, be a confident communicator and someone can communicate complex information in a simple way, through presentations and public speaking. You’ll also thrive on building strong relationships with key stakeholders to build and develop reports to influence decision making.
Benefits
When you join Renishaw, we're committing to your future career. That's because we believe in developing our people's skills and promoting them internally. We also offer a benefits package that's highly desirable; including a 9% non-contributory pension, discretionary annual bonus, subsidised onsite restaurants and coffee shops, free parking, car sharing scheme and 24 hour fitness centres.
We also want to promote a healthy work-life balance as much as possible, so we have introduced a Homeworking policy which allows for a combination of home and office based working depending on the nature of your role. We also offer a variable working programme, 25 days holiday plus bank holidays, Life Assurance policy of 12 times annual salary, Cycle to Work scheme, enhanced maternity pay subject to qualifying criteria, the option to join BUPA Renishaw health trust and an Employee Assistance Programme for employees and family.
At Renishaw we believe that our success is powered by welcoming a workforce of diverse and talented people. Through encouraging an inclusive culture, where all our employees are free to be themselves, we can achieve our core values: Innovation, Inspiration, Integrity, and Involvement.
If you are excited about the role but feel as though you don’t meet all the requirements, we would encourage you still to apply. You might just be the right person for this role or another opportunity at Renishaw.
We are committed to providing reasonable adjustments to make interviews and jobs more accessible. Should you have any difficulty during the recruitment process, or require any reasonable adjustments please contact the recruitment team.
Show more
Show less","SCCS, Teamcenter, Syteline, Microsoft Office 365 Suite, SharePoint, Advanced standard in Excel, Sustainability standards, GHG Protocol, ISSB, ISO14074, Life Cycle Analysis, ISO 14001, ISO 50001, Database management, System management","sccs, teamcenter, syteline, microsoft office 365 suite, sharepoint, advanced standard in excel, sustainability standards, ghg protocol, issb, iso14074, life cycle analysis, iso 14001, iso 50001, database management, system management","advanced standard in excel, database management, ghg protocol, iso 14001, iso 50001, iso14074, issb, life cycle analysis, microsoft office 365 suite, sccs, sharepoint, sustainability standards, system management, syteline, teamcenter"
Junior Data Engineer,Vaarst,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-at-vaarst-3747490185,2023-12-17,Bristol, United Kingdom,Mid senior,Onsite,"Our love for the natural world drives us to innovate. Join Vaarst to work on technology that respects and supports the environment.
Our actions and innovations play an active role in the energy transition making the world cleaner and greener.
Location: Bristol, UK - Hybrid
Salary: £30,000 - £40,000
About Vaarst
Vaarst
focuses on making the world a cleaner, greener and safer place by deploying technology for good. We’re using techniques in artificial intelligence, autonomy, computer vision and marine robotics to create a difference in the world with underwater technology for our clients in offshore wind and the subsea industry.
Take a look at one of our products in action: SubSLAM X2 - Unlocking the Future of Marine Robotics
Founded in 2021 by our sister company Rovco, we’ve announced funding of more than £15 million, been voted one of the most innovative and sustainable companies working towards net zero and won awards for Best use of AI and Best technology innovation.
Discover your place in a hybrid / remote team of 50+ people that celebrates diversity knowing that every perspective is a valuable part of our success and that empowers you to achieve a fulfilling work-life balance.
Junior Data Engineer
This is a new role due to growth, you will be working to develop Machine Learning Operations (MLOps), geospatial data processing pipelines and related systems to provide the backbone for our geospatial and ML platform and model training capabilities.
As part of a team of 5, you will be responsible for development, support and maintenance of these pipelines and systems for very large datasets including imagery, video, bathymetry and 3D models.
Other companies may refer to this role as: Graduate Data Engineer, Data Engineer, MLOps Engineer, ML Operations, Machine learning operations
If you have expertise working with big data sets and building data pipelines and an interest in geospatial datasets this is a great opportunity to join Vaarst as we grow.
In addition, this is an incredible opportunity to help further the transition to renewable energy.
Objectives & Responsibilities:
Vaarst is an exciting and dynamic environment meaning these are likely to change as we grow, upon joining your objectives and responsibilities will include:
Assist in streamlining and automating the flow of large geospatial datasets around the organisation.
Develop tools to maximise the use that can be made of the company data assets, including in the development of AI capability.
Develop your skills in the field of geospatial big data.
You should apply if you have
.
we know it’s tough, but please try to avoid the confidence gap. You don’t have to match all the listed requirements exactly to be considered for this role:
Familiar with Linux operating system
Developing and testing software in Python
Working with databases
Knowledge of good practice and principles in software development including agile working
Experience in CI/CD toolchains (including git) and containers (Docker)
Grow together with Vaarst, you may have some knowledge of the following, if you don't these are areas you'll develop in.
Work with big datasets and building data pipelines
Cloud based development in AWS
3D data including mesh and point cloud
Geospatial domain and working with geospatial data formats
Working with Kubernetes
Distributed computing
Developing secure software
Benefits
At Vaarst, we’re committed to creating a diverse and inclusive workplace where everyone can thrive
. Our hybrid-remote approach and state-of-the-art Bristol Office Hub provide a welcoming space designed to nurture your creativity, productivity and well-being.
In addition you’ll get an extensive range of benefits so you can focus on doing your very best work:
Flexible, hybrid working and a 35 hour work week so you can work when is best for you
25 days annual leave to start with increasing to 35 days after 6 years + bank holidays
Sabbaticals at 4 and 6 years and a £500 holiday voucher at 2 years
Private medical insurance, including Dental & optical
Career and learning development through paid courses, conferences and events
Curiosity fund – up to £500 to spend on learning which is not role related
Up to 10% company bonus
Pension up to 6% company contribution
Volunteering day, to give back to your local community
Enhanced maternity and adoption leave
Cycle to work scheme
Recognition & rewards for doing great work and living our values and behaviours
Flexible working options including, reduced hours, part time, job share, phased return to work, term time working, compressed hours
We’re a sociable, tight-knit team with monthly socials
Hybrid working, Most teams work in our offices 2+ days a week to collaborate and be hands on with our technology. When you do visit our Bristol office is 10 minute walk from the train station, with its own balcony and Fresh fruit & snacks and drinks in the office
Join Vaarst in our mission to make the world a cleaner, greener and safer place by deploying technology for good.
Interview Process
At Vaarst, we've designed a straightforward interview process to ensure the best fit for both you and the company. We have adopted anonymised recruitment. This means that your name, date of birth and other personal details will not be seen by the hiring team.
Application: Begin by submitting your application with your CV, highlighting your skills and experience relevant to the job. Answer key questions on elements that are important to the role.
Talent Partner Interview: We will tell you more about the role, the team and Vaarst’s mission. This is a two-way conversation; we want to learn about your motivation, what you can bring to Vaarst, and provide answers to your questions.
Aptitude Test: Demonstrate your critical thinking, problem-solving abilities, and workplace personality through an aptitude test. It includes a timed cognitive exercise and a workplace personality questionnaire.
Team Interview: Engage in a 1-hour interview with a few team members. Experience the role firsthand and share your skills and experience in. We'll discuss our technologies, key skills, and team dynamics. As always, feel free to ask any questions you may have.
CTO Conversation: Meet with our CTO for a 30-minute discussion. This is your opportunity to express your thoughts on the role and ask any final questions. We'll clarify expectations and ensure this role aligns with your aspirations.
Offer! If you are successful in the process , you'll receive an offer to join Vaarst and become part of our team.
We value the diversity of our teams and are committed to supporting and welcoming individuals from all backgrounds, knowing that every perspective is a valuable part of our success. Should you require any reasonable adjustment throughout the recruitment process, please do not hesitate to let a member of the Talent team know.
Apply now and help deploy technology
for good.
Vaarst is a cloud robotics business, focused on the energy transition space. We are developing a platform for mobile autonomous robotics, starting with the offshore renewables sector, where our aim is to solve robotic challenges of today and the future.
We produce state of the art computer vision coupled with the latest in machine learning techniques. We use this to deploy underwater robots on offshore wind farms where they perform visual surveys and output detailed 3D models.
Our team have won numerous awards and have been recognised for their contribution to advancements in vision, data and autonomous technology.
Show more
Show less","Linux, Python, Databases, Agile, CI/CD, Git, Docker, Kubernetes, Distributed computing, Cloud, AWS, Geospatial data, 3D data, Secure software development, Machine Learning, Artificial Intelligence, Computer Vision, Marine robotics","linux, python, databases, agile, cicd, git, docker, kubernetes, distributed computing, cloud, aws, geospatial data, 3d data, secure software development, machine learning, artificial intelligence, computer vision, marine robotics","3d data, agile, artificial intelligence, aws, cicd, cloud, computer vision, databases, distributed computing, docker, geospatial data, git, kubernetes, linux, machine learning, marine robotics, python, secure software development"
Data Cabling Engineer (SC Cleared),Digital Waffle,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-sc-cleared-at-digital-waffle-3748517589,2023-12-17,Bristol, United Kingdom,Mid senior,Onsite,"Position:
Data Cabling Engineer- SC Cleared (Temp/Contract/Freelance)
Department:
Information Technology / Network Infrastructure
Location:
M4 Corridor - Bristol, Swindon, Reading, Heathrow
Job Summary:
We are seeking a skilled and dedicated Data Cabling Engineer to join our Information Technology team. The successful candidate will need to be either be SC or DV security cleared, and will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables within metal trunking and containment to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Hold either a 'Security Cleared' or 'Develop Vetted' level of security clearance
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Security Cleared Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Joe on joe@digitalwaffle.co.uk
Show more
Show less","Data Cabling, Cat6, Cat6a, Copper Cables, Metal Trunking, Containment, Network Infrastructure, IT Team, Cabling Installation, Cable Pathways, Cable Trays, Conduits, Wall Penetrations, Cable Connectors, Outlets, Termination Panels, Termination, Labeling, Cable Testing, Signal Quality, Continuity, Performance, Troubleshooting, Connectivity Issues, Signal Degradation, CableRelated Problems, Cable Testers, Certification Tools, Quality Assurance, Network Expansion, New Cabling, Safety Protocols, Safety Guidelines, TIA/EIA, ISO/IEC, RJ45 Crimping Tool, PunchDown Tool, Cable Stripper, Cable Cutter, Cable Tester, Cable Certifier, Tone Generator, Probe, Cable Labels, Markers, Label Printer, Tape Measure, Ruler, Level, Cable Ties, Velcro Straps, Cable Clips, Mounts, Power Drill, Bits, Screwdrivers, Wall Anchors, Safety Glasses, Work Gloves, Tool Bag, Pouch, Pen, Notepad, Mobile Device, SteelToed Boots, Hard Hat, Cable Fish Tape, Cable Rods, Cable Lubricant, Cable Toner","data cabling, cat6, cat6a, copper cables, metal trunking, containment, network infrastructure, it team, cabling installation, cable pathways, cable trays, conduits, wall penetrations, cable connectors, outlets, termination panels, termination, labeling, cable testing, signal quality, continuity, performance, troubleshooting, connectivity issues, signal degradation, cablerelated problems, cable testers, certification tools, quality assurance, network expansion, new cabling, safety protocols, safety guidelines, tiaeia, isoiec, rj45 crimping tool, punchdown tool, cable stripper, cable cutter, cable tester, cable certifier, tone generator, probe, cable labels, markers, label printer, tape measure, ruler, level, cable ties, velcro straps, cable clips, mounts, power drill, bits, screwdrivers, wall anchors, safety glasses, work gloves, tool bag, pouch, pen, notepad, mobile device, steeltoed boots, hard hat, cable fish tape, cable rods, cable lubricant, cable toner","bits, cable certifier, cable clips, cable connectors, cable cutter, cable fish tape, cable labels, cable lubricant, cable pathways, cable rods, cable stripper, cable tester, cable testers, cable testing, cable ties, cable toner, cable trays, cablerelated problems, cabling installation, cat6, cat6a, certification tools, conduits, connectivity issues, containment, continuity, copper cables, data cabling, hard hat, isoiec, it team, label printer, labeling, level, markers, metal trunking, mobile device, mounts, network expansion, network infrastructure, new cabling, notepad, outlets, pen, performance, pouch, power drill, probe, punchdown tool, quality assurance, rj45 crimping tool, ruler, safety glasses, safety guidelines, safety protocols, screwdrivers, signal degradation, signal quality, steeltoed boots, tape measure, termination, termination panels, tiaeia, tone generator, tool bag, troubleshooting, velcro straps, wall anchors, wall penetrations, work gloves"
Data Cabling Engineer (SC Cleared),Digital Waffle,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-sc-cleared-at-digital-waffle-3746745140,2023-12-17,Bristol, United Kingdom,Mid senior,Onsite,"Position:
Data Cabling Engineer- SC Cleared (Temp/Contract/Freelance)
Department:
Information Technology / Network Infrastructure
Location:
M4 Corridor - Bristol, Swindon, Reading, Heathrow
Job Summary:
We are seeking a skilled and dedicated Data Cabling Engineer to join our Information Technology team. The successful candidate will need to be either be SC or DV security cleared, and will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables within metal trunking and containment to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Hold either a 'Security Cleared' or 'Develop Vetted' level of security clearance
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Security Cleared Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Joe on joe@digitalwaffle.co.uk
Show more
Show less","Cat6, Cat6a, Copper cables, Metal trunking, Containment, Cable trays, Conduits, Wall penetrations, RJ45 connectors, Patch panels, Termination panels, Cable testing, Cable certification, TIA/EIA, ISO/IEC, Network topologies, Protocols, Network equipment, Technical drawings, Schematics, Cabling diagrams, Cable stripping, Cutting tools, RJ45 crimping, Punchdown tool, Cable labels, Markers, Label printer, Tape measure, Level, Cable ties, Velcro straps, Cable clips, Mounts, Power drill, Bits, Screwdrivers, Wall anchors, Safety glasses, Work gloves, Tool bag, Pen, Notepad, Mobile device, Steeltoed boots, Hard hat, Cable fish tape, Rods, Cable lubricant, Cable toner, Probe","cat6, cat6a, copper cables, metal trunking, containment, cable trays, conduits, wall penetrations, rj45 connectors, patch panels, termination panels, cable testing, cable certification, tiaeia, isoiec, network topologies, protocols, network equipment, technical drawings, schematics, cabling diagrams, cable stripping, cutting tools, rj45 crimping, punchdown tool, cable labels, markers, label printer, tape measure, level, cable ties, velcro straps, cable clips, mounts, power drill, bits, screwdrivers, wall anchors, safety glasses, work gloves, tool bag, pen, notepad, mobile device, steeltoed boots, hard hat, cable fish tape, rods, cable lubricant, cable toner, probe","bits, cable certification, cable clips, cable fish tape, cable labels, cable lubricant, cable stripping, cable testing, cable ties, cable toner, cable trays, cabling diagrams, cat6, cat6a, conduits, containment, copper cables, cutting tools, hard hat, isoiec, label printer, level, markers, metal trunking, mobile device, mounts, network equipment, network topologies, notepad, patch panels, pen, power drill, probe, protocols, punchdown tool, rj45 connectors, rj45 crimping, rods, safety glasses, schematics, screwdrivers, steeltoed boots, tape measure, technical drawings, termination panels, tiaeia, tool bag, velcro straps, wall anchors, wall penetrations, work gloves"
Senior Data Engineer,Vaarst,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-vaarst-3747484841,2023-12-17,Bristol, United Kingdom,Mid senior,Onsite,"Our love for the natural world drives us to innovate. Join Vaarst to work on technology that respects and supports the environment.
Our actions and innovations play an active role in the energy transition making the world cleaner and greener.
Location: Bristol, UK - Hybrid
Salary: £65,000 - £85,000
About Vaarst
Vaarst
focuses on making the world a cleaner, greener and safer place by deploying technology for good. We’re using techniques in artificial intelligence, autonomy, computer vision and marine robotics to create a difference in the world with underwater technology for our clients in offshore wind and the subsea industry.
Take a look at one of our products in action: SubSLAM X2 - Unlocking the Future of Marine Robotics
Founded in 2021 by our sister company Rovco, we’ve announced funding of more than £15 million, been voted one of the most innovative and sustainable companies working towards net zero and won awards for Best use of AI and Best technology innovation.
Discover your place in a hybrid / remote team of 50+ people that celebrates diversity knowing that every perspective is a valuable part of our success and that empowers you to achieve a fulfilling work-life balance.
Senior Data Engineer
This is a new role due to growth, you will be working to develop Machine Learning Operations (MLOps), geospatial data processing pipelines and related systems to provide the backbone for our geospatial and ML platform and model training capabilities.
As part of a team of 5, you will be responsible for design, development, and implementation of these pipelines and systems for very large datasets including imagery, video, bathymetry and 3D models.
Other companies may refer to this role as: MLOps Engineer or Python Software Engineer
If you have expertise working with big data sets and building data pipelines and an interest in geospatial datasets this is a great opportunity to join Vaarst as we grow.
In addition, this is an incredible opportunity to help further the transition to renewable energy.
Objectives & Responsibilities:
Vaarst is an exciting and dynamic environment meaning these are likely to change as we grow, upon joining your objectives and responsibilities will include:
Streamline and automate the flow of large geospatial datasets around the organisation.
Design and develop the tools that will maximise the use that can be made of the company data assets, including in the development of AI capability.
Curate and administer Vaarst's data holdings.
You should apply if you have
.
we know it’s tough, but please try to avoid the confidence gap. You don’t have to match all the listed requirements exactly to be considered for this role:
Expertise with Linux operating system
Developing and testing complex solutions in Python
Cloud-based development in AWS
Working with databases, big data sets and building data pipelines
Writing and maintaining production-level code, with knowledge of good practice and principles including agile working
Experience in CI/CD toolchains (including git) and containers (Docker)
Grow together with Vaarst, you may have some knowledge of the following, if you don't these are areas you'll develop in.
3D data including mesh and point cloud
Geospatial domain and working with geospatial data formats
Kubernetes
Distributed computing
Developing secure software
Benefits
At Vaarst, we’re committed to creating a diverse and inclusive workplace where everyone can thrive
. Our hybrid-remote approach and state-of-the-art Bristol Office Hub provide a welcoming space designed to nurture your creativity, productivity and well-being.
In addition you’ll get an extensive range of benefits so you can focus on doing your very best work:
Flexible, hybrid working and a 35 hour work week so you can work when is best for you
25 days annual leave to start with increasing to 35 days after 6 years + bank holidays
Sabbaticals at 4 and 6 years and a £500 holiday voucher at 2 years
Private medical insurance, including Dental & optical
Career and learning development through paid courses, conferences and events
Curiosity fund – up to £500 to spend on learning which is not role related
Up to 10% company bonus
Pension up to 6% company contribution
Volunteering day, to give back to your local community
Enhanced maternity and adoption leave
Cycle to work scheme
Recognition & rewards for doing great work and living our values and behaviours
Flexible working options including, reduced hours, part time, job share, phased return to work, term time working, compressed hours
We’re a sociable, tight-knit team with monthly socials
Hybrid working, Most teams work in our offices 2+ days a week to collaborate and be hands on with our technology. When you do visit our Bristol office is 10 minute walk from the train station, with its own balcony and Fresh fruit & snacks and drinks in the office
Join Vaarst in our mission to make the world a cleaner, greener and safer place by deploying technology for good.
Interview Process
At Vaarst, we've designed a straightforward interview process to ensure the best fit for both you and the company. We have adopted anonymised recruitment. This means that your name, date of birth and other personal details will not be seen by the hiring team.
Application: Begin by submitting your application with your CV, highlighting your skills and experience relevant to the job. Answer key questions on elements that are important to the role.
Talent Partner Interview: We will tell you more about the role, the team and Vaarst’s mission. This is a two-way conversation; we want to learn about your motivation, what you can bring to Vaarst, and provide answers to your questions.
Aptitude Test: Demonstrate your critical thinking, problem-solving abilities, and workplace personality through an aptitude test. It includes a timed cognitive exercise and a workplace personality questionnaire.
Team Interview: Engage in a 1-hour interview with a few team members. Experience the role firsthand and share your skills and experience in. We'll discuss our technologies, key skills, and team dynamics. As always, feel free to ask any questions you may have.
CTO Conversation: Meet with our CTO for a 30-minute discussion. This is your opportunity to express your thoughts on the role and ask any final questions. We'll clarify expectations and ensure this role aligns with your aspirations.
Offer! If you are successful in the process , you'll receive an offer to join Vaarst and become part of our team.
We value the diversity of our teams and are committed to supporting and welcoming individuals from all backgrounds, knowing that every perspective is a valuable part of our success. Should you require any reasonable adjustment throughout the recruitment process, please do not hesitate to let a member of the Talent team know.
Apply now and help deploy technology
for good.
Vaarst is a cloud robotics business, focused on the energy transition space. We are developing a platform for mobile autonomous robotics, starting with the offshore renewables sector, where our aim is to solve robotic challenges of today and the future.
We produce state of the art computer vision coupled with the latest in machine learning techniques. We use this to deploy underwater robots on offshore wind farms where they perform visual surveys and output detailed 3D models.
Our team have won numerous awards and have been recognised for their contribution to advancements in vision, data and autonomous technology.
Show more
Show less","Linux, Python, AWS, Databases, Big Data, Data Pipelines, Agile, CI/CD, Git, Docker, 3D Data, Geospatial, Kubernetes, Distributed Computing, Secure Software, Machine Learning Operations (MLOps), Geospatial Data Processing, Computer Vision, Marine Robotics","linux, python, aws, databases, big data, data pipelines, agile, cicd, git, docker, 3d data, geospatial, kubernetes, distributed computing, secure software, machine learning operations mlops, geospatial data processing, computer vision, marine robotics","3d data, agile, aws, big data, cicd, computer vision, databases, datapipeline, distributed computing, docker, geospatial, geospatial data processing, git, kubernetes, linux, machine learning operations mlops, marine robotics, python, secure software"
Principal Data Engineer,Energy Jobline,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/principal-data-engineer-at-energy-jobline-3779116783,2023-12-17,Bristol, United Kingdom,Mid senior,Onsite,"Principal Data Engineer ( Databricks / Snowflake )
Hybrid / Bristol
£90,000 - £110,000 per annum depending on experience
A Princicapl Data Engineer with experience working with Snowflake or Databricks is needed to join a industry-leading consultancy.
Working within an agile development team, the Principal Data Engineer will be responsible for delivering client data engineering projects, enhancing data engineering capacities, fostering the growth of their technology professionals, and shaping the approach to data, both internally and for their clients.
The Principal Data Engineer holds the highest position for data expertise in the team. While not directly responsible for managerial roles, you will serve as a leader and expert in the field, actively participating in coaching and mentoring. Additionally, you will be contributing to broader operational and strategic responsibilities.
Education and Experience
As the Principal Data Engineer , you will demonstrate leadership experience with teams and problem-solving on an enterprise scale within the realms of Data Architecture, Science, or Analytics.
Proficiency in hands-on technical project delivery for client-facing data initiatives, spanning the entire Software Development Life Cycle (SDLC) from design through migration, integration, and live service, all within a multi-vendor environment.
Essential Skills Include Expertise In
Python and SQL, with knowledge of R and Scala as advantageous.
Strong proficiency in cloud-native data engineering, encompassing Azure (e.g., CosmosDB, DataFactory, DataWarehouse, SQL DB, Snowflake, DataBricks).
Experience in applying AI/ML in data engineering is a valuable addition.
Proven track record in quality assurance for data programs, encompassing non-functional testing and performance optimization.
Hands-on involvement in modern software delivery, including Continuous Integration/Continuous Delivery (CI/CD) and DevOps practices, with a deep understanding and extensive history of delivering complex data solutions using Agile methodologies like Scrum and SAFe.
A solid grasp of industry best practices related to standards, quality, and continuous improvement in the field of Data Engineering.
Proficiency in handling large-scale enterprise batch and real-time processing from diverse data sources.
Experience in designing and building scalable enterprise data architectures.
Why join?
Opportunities to engage in projects for prominent clients and the chance to contribute to impactful work that positively influences people's lives.
A ""Blended Working"" approach, granting you the flexibility to work from various locations, including your home, your local office, client sites.
A dedicated career scrum team is in place to support your professional growth, helping you achieve your career objectives and acquire the necessary skills for personal development.
An annual budget allocated for training and skill enhancement, which includes designated days off, allowing you to focus on learning without sacrificing your free time.
Enjoy monthly and quarterly team social events, funded by us, spanning from informal after-work gatherings to exciting driving experience days with your fellow team members.
A nurturing and inclusive environment where you can be your authentic self and take on challenges with confidence.
What’s on offer?
£90,000 - £110,000 salary
26 days holiday allowance + bank holidays
13 days per year for innovation or upskilling
2 days per year for volunteering
Share scheme
A £1000 flexi-fund to use on a personalised list of benefits such Gym membership, Cycle to Work Scheme, Health, dental and optical cash plan
PLUS, many more
Apply
To apply for the Principal Data Engineer role, please click apply
Show more
Show less","Python, SQL, R, Scala, Azure, CosmosDB, DataFactory, DataWarehouse, SQL DB, Snowflake, DataBricks, AI/ML, Continuous Integration/Continuous Delivery (CI/CD), DevOps, Agile, Scrum, SAFe, Continuous Improvement, Enterprise Batch Processing, Realtime Processing, Scalable Enterprise Data Architectures","python, sql, r, scala, azure, cosmosdb, datafactory, datawarehouse, sql db, snowflake, databricks, aiml, continuous integrationcontinuous delivery cicd, devops, agile, scrum, safe, continuous improvement, enterprise batch processing, realtime processing, scalable enterprise data architectures","agile, aiml, azure, continuous improvement, continuous integrationcontinuous delivery cicd, cosmosdb, databricks, datafactory, datawarehouse, devops, enterprise batch processing, python, r, realtime processing, safe, scala, scalable enterprise data architectures, scrum, snowflake, sql, sql db"
Lead Data Engineer,Energy Jobline,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-energy-jobline-3774725060,2023-12-17,Bristol, United Kingdom,Mid senior,Onsite,"I am currently recruiting for multiple roles within the data space for my awesome client at senior and Lead data engineer level! The successful candidates will be individuals with robust commercial experience in a Senior/Lead Data Engineering role and well-versed in a similar tech stack. Your background should encompass mid-large sized organizations and be adept at handling extensive datasets.
Data is key to my clients continued success as they look to harness data engineering, analysis, science, machine learning, and interactive visualizations to extract insights, crafting superior products and fostering innovation.
These technologies extend to shaping our internal product strategy. Managing diverse technical platforms and back-office systems from acquisitions necessitates creating a unified view of the internal processes and customer lifecycle.
Key Tech Used Within This Area Are
Cassandra
Apache Spark
ActiveMQ
Python
Pandas
AWS (S3, Redshift, Lambda)
Tableau
Key Aspects Of The Day-to-day Role
Crafting data pipelines for innovative products
Maintaining existing pipelines and jobs
Scaling the data platform to accommodate our expanding data footprint
Assessing new data sources and integrating them into our existing data lake
Applying machine learning for new data products and dashboards
Conducting purpose-driven data analysis to guide business decisions
This opportunity will see you working on an awesome team within a highly successful global organisation doing incredibly well in multiple specialist domains. Interested? Apply ASAP to avoid disappointment
Show more
Show less","Senior Data Engineer, Lead Data Engineer, Data Engineering, Data Analysis, Data Science, Machine Learning, Interactive Visualization, Big Data, Apache Spark, Cassandra, ActiveMQ, Python, Pandas, AWS (S3 Redshift Lambda), Tableau, Data Pipelines, Data Lakes, Data Integration, Machine Learning Models, Data Products, Dashboards, DataDriven Decision Making","senior data engineer, lead data engineer, data engineering, data analysis, data science, machine learning, interactive visualization, big data, apache spark, cassandra, activemq, python, pandas, aws s3 redshift lambda, tableau, data pipelines, data lakes, data integration, machine learning models, data products, dashboards, datadriven decision making","activemq, apache spark, aws s3 redshift lambda, big data, cassandra, dashboard, data engineering, data integration, data lakes, data products, data science, dataanalytics, datadriven decision making, datapipeline, interactive visualization, lead data engineer, machine learning, machine learning models, pandas, python, senior data engineer, tableau"
Data Analyst - SQL & Power BI - UK-wide,Nigel Frank International,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-sql-power-bi-uk-wide-at-nigel-frank-international-3780312516,2023-12-17,Bristol, United Kingdom,Mid senior,Onsite,"A healthcare provider are looking for an Information Analyst with skills in SQL and either SSRS or Power BI to join their established Management Information team, where you'll be responsible for delivering timely and accurate data and reporting solutions for users across the organisation.
This organisation are headquartered in Hertfordshire, although this is a home-based role, so you can be based anywhere in the UK. It's likely you will be asked to travel to their Head Office around twice per year for full IT team meetings, and this will be fully expensed.
We are looking for an individual with strong SQL and T-SQL skills in order to effectively work with the data in their SQL and Snowflake database, as well as experience with either SSRS or Power BI to produce insightful reports and visualisations - currently they're on a journey to migrate most of their SSRS reports over to Power BI, with any new reports being created in Power BI.
This is a really exciting opportunity for an ambitious data professional to progress their career with an organisation who will be committed to your ongoing professional development, with the chance to work towards your Microsoft certifications - you'll have the opportunity to complete your Power BI and Snowflake certifications, and get your ITIL qualifications too.
Requirements:
Strong SQL and T-SQL skills - including complex queries, stored procedures, tables, views etc.
SSRS or Power BI Report builder experience (including DAX)
Excellent communication, problem solving and stakeholder management skills
Benefits:
Salary of £25-32,000, depending upon experience
25 days' annual leave plus bank holidays plus day off for your birthday (increasing by 1 day for every 2 years' service up to 28 days)
Pension with 5% employer contribution and 3% employee contribution
Enhanced maternity package
Death in service (2X salary)
Learning and development opportunities
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","SQL, TSQL, SSRS, Power BI, DAX, Snowflake, ITIL, Microsoft Certifications, Complex queries, Stored procedures, Tables, Views","sql, tsql, ssrs, power bi, dax, snowflake, itil, microsoft certifications, complex queries, stored procedures, tables, views","complex queries, dax, itil, microsoft certifications, powerbi, snowflake, sql, ssrs, stored procedures, tables, tsql, views"
Senior Data Analyst,Dyson,"Malmesbury, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-dyson-3722169786,2023-12-17,Bristol, United Kingdom,Mid senior,Onsite,"About Us
We are responsible for assessing the success of Dyson’s products in market and identifying the drivers of reliability and satisfaction in our technologies. Using data from a wide range of sources and business domains, we unlock valuable insights that can be fed back into the business to ensure that we continue to delight Dyson owners with innovative and ground-breaking technology.
We push the boundaries of what others have defined as possible. Our approach starts with an engineering mindset and different thinking. Then we continuously refine our ideas – unwilling to compromise and driven by an obsession for finding a better way.
About The Role
We are currently seeking Data Analysts to join the Business Intelligence Team embedded within the Quality function. We are interested in speaking to people at both junior and senior levels who have relevant industry experience. This role is essential to This role is essential to keeping Dyson ahead of the competition delivering products that are reliable and deeply satisfying to our customers.
You Will:
Develop and maintain scalable data models within our DAG to support the analytics and reporting needs for the Quality BI team.
Collaborate with business stakeholders to understand data requirements and implement complex data enrichment logic.
Ensure data accuracy, integrity, and security through robust testing and validation.
Identify opportunities for data quality and performance improvement.
Create and maintain documentation for data processes and systems.
About You
Confident and competent with SQL
Relevant experience in project and stakeholder management
Forward thinking and thrives on change.
Interested in how business works and desire to improve our performance.
Self-motivated, dynamic, and results-driven
Team player, comfortable dealing with all business levels
Ethical, personal integrity
Willing to be flexible in tasks and activities taken on.
Attention to detail without losing bigger picture.
You Will:
Be highly proficient in SQL
Have Experience with data warehousing solutions within cloud environments
Have Git version control experience
Python scripting is preferable
DBT is preferable
CI/CD using tools such as Jenkins would be preferred, as would experience analysing data from CRM systems.
Benefits
Financial:
Performance related bonus
Company paid Life Assurance
Discounts on Dyson machines
Competitive pension scheme
Purchase additional holidays
Lifestyle:
27 days holiday plus statutory bank holidays
Free bus travel to and from Malmesbury campus from Bristol, Chippenham and Swindon
On-site parking
Endless free coffee and tea, and a free lunch
Free on-site hair salon
On-site lifestyle Assist
Electric vehicle scheme
Health:
Private Medical insurance for all employees
Employee Assistance Program for employee and dependents
Digital GP and prescription service
Fertility treatment support
On-site gym and sports centre
Dyson is an equal opportunity employer. We know that great minds don’t think alike, and it takes all kinds of minds to make our technology so unique. We welcome applications from all backgrounds and employment decisions are made without regard to race, colour, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other any other dimension of diversity.
Show more
Show less","SQL, Data warehousing, Git, Python, DBT, CI/CD tools, Jenkins, CRM systems","sql, data warehousing, git, python, dbt, cicd tools, jenkins, crm systems","cicd tools, crm systems, datawarehouse, dbt, git, jenkins, python, sql"
Senior Data Analyst,Babcock International Group,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-babcock-international-group-3724250136,2023-12-17,Bristol, United Kingdom,Mid senior,Onsite,"Job Title: Senior Data Analyst
Location: Stoke Gifford, Bristol
Compensation: £46,589 - £56,906 dependent upon experience
Role Type: Full time
Role ID: SF52612
At Babcock we’re working to create a safe and secure world, together, and if you join us, you can play your part as a Senior Data Analyst at our Bristol site.
The role
As a Senior Data Analyst, you’ll have a role that’s out of the ordinary. An exciting opportunity exists for someone with modelling and data analytics experience to work at the heart of Babcock’s Nuclear Submarine Operations delivering strategic analysis of the submarine programme.
Day to day, you’ll be leading and developing a small team and presenting to wider stakeholders to deliver submarine programme analysis, recommendations, and conclusions. You’re key responsibilities will included;
Leading a team to develop and implement data analytics and modelling concept and capabilities undertake resulting analysis for the business executive
Developing optimal solutions to visualise and report analytic outputs to a diverse and senior level of stakeholders
Proactively leading engagement and facilitation across the business and client organisations to ensure organisational collaboration and contribution
Developing knowledge of submarine maintenance and disposal operations and using this to intelligently develop analysis outputs
Developing, collating, and maintaining coherent and validated master data and information that underpins analytic outputs.
This is a full-time position, 36 hours per week. As a Senior Data Analyst you'll be based primarily out of North Bristol for a minimum 3/4 days a week. Occasional travel to HMNB Devonport and HMNB Clyde may be required, from time to time.
Essential Experience Of The Senior Data Analyst
Experienced in complex modelling, data analysis and associated development of tools and techniques to solve real-world problems
A strong sense of ownership and responsibility for outputs, including timeliness, quality, and accuracy
Experience managing diverse teams to deliver outputs to senior stakeholders
Excellent team working and willingness to proactively engage people and teams to deliver objectives
Experience and modelling software languages such as Python or R
Skills utilising the power of MS Excel with Visual Basic and Power BI
Qualifications For The Senior Data Analyst
Degree level qualification in data analytics, maths, statistics, software or similar subject
The successful candidate will be a sole UK National who holds, or is eligible to achieve SC security clearance for this role.
National security vetting: clearance levels - GOV.UK (www.gov.uk)
Our Benefits
Generous holiday allowance
Matched contribution pension scheme up to 8% of salary, with life assurance
Employee share scheme
Employee shopping savings portal
Payment of Professional Fees
Reservists in the armed forces receive 10-days special paid leave
Holiday Trading is a benefit that allows the majority of employees to buy additional leave or to sell up to one working week of annual leave from their annual entitlement
‘Be Kind Day’ enables employees to take one working day's paid leave a year (or equivalent hours) to undertake volunteering work with their chosen organisation or registered charity
Babcock International
For over 130 years Babcock International have helped to defend nations, protect communities and build a better world. To continue, we must adapt, advance and be a sustainable business with a shared goal.
If you have a disability or need any reasonable adjustments during the application and selection stages, please let us know. We’re committed to building an inclusive culture where everyone’s free to thrive. We are happy to talk about flexible working - please ask about alternative patterns of work at interview.
Closing date: 08/01/2024
Show more
Show less","Data Analytics, Modelling, Python, R, Visual Basic, Power BI, Data Analysis, Development, MS Excel, Data Visualization, Team Management, Communication, Problem Solving, Software Development, Software Languages, Submarine Maintenance, Disposal Operations, Data Collection, Data Validation, Security Clearance, Statistics, Mathematics, Software, Share Scheme, Pension Scheme, Holiday Allowance, Professional Fees, Volunteering","data analytics, modelling, python, r, visual basic, power bi, data analysis, development, ms excel, data visualization, team management, communication, problem solving, software development, software languages, submarine maintenance, disposal operations, data collection, data validation, security clearance, statistics, mathematics, software, share scheme, pension scheme, holiday allowance, professional fees, volunteering","communication, data collection, data validation, dataanalytics, development, disposal operations, holiday allowance, mathematics, modelling, ms excel, pension scheme, powerbi, problem solving, professional fees, python, r, security clearance, share scheme, software, software development, software languages, statistics, submarine maintenance, team management, visual basic, visualization, volunteering"
Data Engineering Consultant,Nigel Frank International,"Bath, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineering-consultant-at-nigel-frank-international-3740001859,2023-12-17,Bristol, United Kingdom,Mid senior,Onsite,"I am working with a Microsoft Partnered consultancy who are looking for a Data Engineering Consultant to join their growing team. You will have the opportunity to work on a variety of client projects across a number of different sectors such as retail, finance and the public sector.
In this role you will utilise your experience with the latest Azure technologies to provide guidance to clients. As a hands-on consultant you use your experience as a data engineer to ensure the timely delivery on clients data driven projects.
You will be joining a people centred business, who invest heavily in its team. You will be given the opportunity to explore other areas of technologies that may be of interest to you, including data science, machine learning and AI. You will have time designated purely to training and development, with internal development schemes as well as funded Microsoft learning, training courses and certifications.
This is an exciting time to join a hugely successful and growing business who have a number of exciting roadmap of projects planned all the way into 2024!
As part of this role, you will be responsible for some of the following areas.
Offer guidance to clients on data driven projects
Take the lead on data engineering projects focused around the Microsoft and Azure tech stack
Deliver on all aspects of the project including requirements gathering, testing, implementation and maintenance
This is a salaried role paying up to £55,000 per annum depending on experience and a company benefits package. This is a home based role with occasional, fully expensed visits to client sites as and when required. You will also need to commute to company retreats once per quarter to meet other members of the team.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Coding experience with languages such as SQL and Python.
Strong knowledge of Databricks for data ingestion and transformation would be beneficial
Excellent communication skills
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure, Azure Data Factory, Synapse, Azure Data Lake, ETL, SQL, Python, Databricks, Data ingestion, Data transformation, Communication skills","azure, azure data factory, synapse, azure data lake, etl, sql, python, databricks, data ingestion, data transformation, communication skills","azure, azure data factory, azure data lake, communication skills, data ingestion, data transformation, databricks, etl, python, sql, synapse"
Data Analyst- SC Cleared - Infrastructure Digital - Bristol,Turner & Townsend,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-sc-cleared-infrastructure-digital-bristol-at-turner-townsend-3756392682,2023-12-17,Bristol, United Kingdom,Mid senior,Onsite,"Company Description
Every day we help our global clients deliver ambitious and highly technical projects in over 130 countries worldwide. If you’re looking to take your career to the next level, there’s room for you to grow at Turner & Townsend.
At Turner & Townsend we’re passionate about making the difference. That means delivering better outcomes for our clients, helping our people to realize their potential, and doing our part to create a prosperous society.
Every day we help our major global clients deliver ambitious and highly technical projects, in over 110 offices worldwide. As part of our 2025 Vision, we are putting Sustainability and Net Zero at the heart of our business. https://www.turnerandtownsend.com/en/about-us/our-purpose-and-values/
Our team is dynamic, innovative, and client-focused, supported by an inclusive and fun company culture. Our clients value our proactive approach, depth of expertise, integrity and the quality we deliver. As a result, our people get to enjoy working on some of the most exciting projects in the world.
We are recruiting for a variety of skills and specialisms owing to an increase in work across our National Security accounts. A successful candidate must hold a valid security clearance (SC) as a minimum, through UKSV. Therefore, you would need to be a British National and hold a UK Passport. Your clearance status will be verified prior to interview.
Background
Job Description
Due to growing demand for our services we are seeking to recruit SC Cleared experienced Data Analysts to join our Bristol Infrastructure Digital team. The role involves supporting a wide range of Defence projects of all sizes and dimensions and we are seeking individuals with a demonstrable track record of delivery in a consultancy environment.
The Opportunity
The responsibilities of the Data Analyst will include, but are not limited to:
Providing technical expertise and support to non-experts
Developing multi-faceted reports with recommendations to senior management
Development, analysis and maintenance of corporate datasets
Provide guidance and advice on data quality issues to a wide variety of data providers
Develop / use, the mechanisms employed by clients to recharge manpower against projects
Responsible for visibility, accuracy and transparency of the project critical path
Preparing presentation and other materials for client pitches.
Inputting key information into the Turner & Townsend internal database tools.
Qualifications
Required Skills
Must have active Security Clearance or, be eligible to receive Security Clearance if needed.
Minimum 2 years’ experience working as a Data Analyst
Strong analytic and numeracy skills with the ability to understand and present data in different representations
Demonstrable track record in Data Analysis; identifying requirements and documenting these using the appropriate types of tools and documentation
Knowledge of data warehousing and ETL processes
End to end experience from project launch to ongoing operation of data, analytics and reporting projects
Ability to identify and run different business and model scenarios
Additional Information
Turner & Townsend work with the large clients across all areas of the transport and utilities sector (including Rail, Water, Airports, Ports and Roads). This provides a range of exciting opportunities for candidates to support their development and growth across all project controls disciplines. Our large team of UK and International Project Controls professionals across all sectors ensures a strong network to help us bring out the best in all our team.
Our inspired people share our vision and mission. We provide a great place to work, where each person has the opportunity and voice to affect change.
We want our people to succeed both in work and life. To support this we promote a healthy, productive, and flexible working environment that respects work-life balance.
We are an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees and actively encourage applications from all sectors of the community.
Please find out more about us at www.turnerandtownsend.com/
SOX control responsibilities may be part of this role, which are to be adhered to where applicable.
SOX control responsibilities may be part of this role, which are to be adhered to where applicable.
Join our social media conversations for more information about Turner & Townsend and our exciting future projects:
Twitter
Instagram
LinkedIn
It is strictly against Turner & Townsend policy for candidates to pay any fee in relation to our recruitment process. No recruitment agency working with Turner & Townsend will ask candidates to pay a fee at any time.
Any unsolicited resumes/CVs submitted through our website or to Turner & Townsend personal e-mail accounts, are considered property of Turner & Townsend and are not subject to payment of agency fees. In order to be an authorised Recruitment Agency/Search Firm for Turner & Townsend, there must be a formal written agreement in place and the agency must be invited, by the Recruitment Team, to submit candidates for review.
Show more
Show less","Data Analyst, Security Clearance, Numeracy, Data Analysis, Data Representation, ETL Processes, Data Warehousing, Business Scenarios, Project Controls, Presentation, Data Quality, Data Sets, Analytic Skills, Reporting Projects, Dashboards, Change Management","data analyst, security clearance, numeracy, data analysis, data representation, etl processes, data warehousing, business scenarios, project controls, presentation, data quality, data sets, analytic skills, reporting projects, dashboards, change management","analytic skills, business scenarios, change management, dashboard, data quality, data representation, data sets, dataanalytics, datawarehouse, etl, numeracy, presentation, project controls, reporting projects, security clearance"
Bootcamp Data Analyst Trainer (FTC),Cambridge Spark,"Cambridge, England, United Kingdom",https://uk.linkedin.com/jobs/view/bootcamp-data-analyst-trainer-ftc-at-cambridge-spark-3787770293,2023-12-17,Bristol, United Kingdom,Mid senior,Remote,"Department
: Delivery
Location
: Home based
Reports to
: Bootcamp Programme Manager
Contract
: 6 & 12 month contracts available
Hours
: 37.5
Salary
: Depending on industry and technical experience
Role Overview
The role is responsible for workshop delivery on Cambridge Spark’s best-in-class educational programmes, primarily this would be on our funded Bootcamp programmes, with some additional delivery on Corporate Training where required. Trainers will provide technical support and feedback to all learners, ensuring that teaching, learning and assessment is of a gold standard and that retention, achievement and outcomes are at or above national standards and learner progression is robustly monitored.
This is your opportunity to be directly involved with improving career prospects, opening the door into the world of data and making an impact by sharing your expert knowledge.
Key Responsibilities
Deliver our existing data level 3 and 4 courses to students. (Our cohorts run both daytime and evening/weekend, delivery times are completed on a rotational basis to ensure a fair split is provided.)
Design Innovative and engaging teaching and learning strategies
Support learners with developing technical skills
Provide Ad-hoc technical support and check-in sessions
Provide regular feedback and updates on learner progress to the Employability Coach & Bootcamp Programme Manager
Attend regular team meetings and contribute to conversations and actions which will contribute to team impact
Support the Quality Assurance process, attending standardisations and commitment to own CPD
Extend the English and mathematical skills of learners through the ongoing delivery of technical teaching
To safeguard learners from harm and to report concerns in accordance with the Cambridge Spark process
Support with any ad-hoc duties or requests that may be reasonably required to successfully carry out the role or to support the business
Candidate Specification
Essential Skills
Relevant Technical Qualifications
Teaching or training experience
A minimum level 4 qualification within the subject area
Relevant industry experience
The ability to work independently and as part of a team
Excellent verbal, written and interpersonal communication skills
Ability to work under pressure
Experience of prioritising workload, working to strict deadlines and following governing procedures and controls
Committed to student success and supporting students to fulfil their potential
Commitment to continuously developing teaching and technical knowledge and skills
Technical Knowledge and Skills
Microsoft Excel
Python & Pandas
Data privacy, ethics and regulations in AI
Data Visualisation
Power BI
Tableau
Databases and SQL
Maths for Data Analysis
Time Series Analysis
Machine Learning
Text Mining
Web Scraping and APIs
Desirable Skills
Ability to differentiate and create enjoyable learning opportunities
Experience in a role of a similar nature within formal teaching or funded education
Company Benefits:
Remote first company providing flexibility to work from home
Pension with up to 5% matched contributions
25 days holiday + 8 bank holidays + 1 day off on your birthday
Pre-tax gym allowance of £30 per month
Annual Summer and Xmas events
Company socials including everything from Cambridge College formals, pub nights to team building events
Contribution towards books and training courses that help you learn and grow in your role with us
Private medical insurance and cash plan
Holiday buy back scheme (up to 10 days p/a)
EAP with 24 hour confidential support line
Background to the Organisation
We are an education technology company that enables corporate and government organisations to achieve their business goals by educating their workforce with critical digital transformation skills to succeed in the AI era.
We deliver unique and innovative professional education that is accelerating the digital transformation of our clients, advancing the careers of their employees, helping people get into work and closing the digital skills gap. We are in a sector that is crucial to the economy and workforce, with a lot of opportunity for change and innovation. We are at the cutting edge of teaching applied data and digital skills, with our unique patented learning platform EDUKATE.AI offering our clients and learners a unique learning experience. EDUKATE.AI was developed with support from Innovate UK and provides all of our learners with 24/7 immediate feedback on their work, helping accelerate the learning process and providing a sandbox environment to experiment on real world datasets.
Since 2016, we have supported more than 15,000 learners across four continents with nearly 550,000 pieces of code submitted for feedback on EDUKATE.AI. We are trusted by some of the most recognisable brands in the world to educate their workforce, including Microsoft, the NHS, GSK, easyJet, the BBC and John Lewis. Our focus on applied learning to create business impact sets us apart - individual learners have reported applying their skills at work to generate recorded value of up to £40m.
Values
At the centre of the way we work together and inspire each other to achieve success are these core values:
Entrepreneurial
We take initiative and show entrepreneurial spirit which fuels innovation at Cambridge Spark. This includes identifying opportunities for improvement, taking ownership for implementing solutions effectively and driving improvement by using proof of concepts to demonstrate the feasibility and value of their work.
Team Spirit
Everyone is part of building an open and transparent culture, communicating effectively to raise issues, discuss improvements and share the evidence used to make decisions.
Customer-focused
Our customers are at the centre of everything we do, inspiring us to create great work. We strive to build friendly, professional and lasting relationships with them to better understand and anticipate their needs.
Gold Standard
We are experts in our field and are constantly developing our technology and offering. We set the benchmark in our industry: both in what we offer customers and in how we deliver it.
____________________________________________________________________________
Cambridge Spark is an Equal Opportunities Employer and prohibits discrimination and harassment of any kind. Cambridge Spark is committed to the principle of equal employment opportunities for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at Cambridge Spark are based on business needs, job requirements and individual qualifications, without regard to race, colour or ethnicity, ability or disability, gender or gender reassignment, sexual orientation, marital status, religion, age or any other status protected by the laws or regulations in the locations where we operate. Cambridge Spark will not tolerate discrimination or harassment based on any of these characteristics. Cambridge Spark encourages applicants of all ages.
Powered by JazzHR
DRilYdrRAr
Show more
Show less","Data Privacy, Ethics, Regulations in AI, Data Visualization, Power BI, Tableau, Databases, SQL, Maths for Data Analysis, Time Series Analysis, Machine Learning, Text Mining, Web Scraping, APIs, Microsoft Excel, Python, Pandas","data privacy, ethics, regulations in ai, data visualization, power bi, tableau, databases, sql, maths for data analysis, time series analysis, machine learning, text mining, web scraping, apis, microsoft excel, python, pandas","apis, data privacy, databases, ethics, machine learning, maths for data analysis, microsoft excel, pandas, powerbi, python, regulations in ai, sql, tableau, text mining, time series analysis, visualization, web scraping"
Senior Data Engineer,KDR Talent,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-kdr-talent-3780663925,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer | Hybrid working with base locations in London, Bristol, Birmingham, and Manchester | up to £65,000 base + package
If you’re looking to take the next step in your career as a Senior Data Engineer working for a world-leading design, engineering, and project management consultancy, delivering on some of the world’s most complex projects then please read this advert.
This is the role for you if you are passionate about using data to solve real-world problems, seeing the direct results of your work, and utilising your expertise to support clients on multiple projects in the Aerospace, Defence & Infrastructure Sectors.
What's on offer:
Join a prestigious business, with over 80 years of design & engineering expertise
Opportunity to work technology agnositc - utilising modern cloud and big data technology
Be a part of a recognised organisation, ranked as one of LinkedIn’s top 25 companies, and rated by the Financial Times as the UK’s leading management consultancies
An opportunity to work in a fun, flexible, and collaborative working environment
Work with a business who are dedicated to the growth and development of their staff, with up to 30% of your time dedicated to upskilling and training.
A role offering a generous salary along with a tailored total rewards package and flexible hybrid working!
The experience you'll need
Experience working in a Data Warehousing environment, building complex ETL routines
Experience working with large data sets, data pipeline and workflow management tools, Azure cloud services, and stream-processing system
Deep knowledge of SQL
Security Clearance - some experience working i nthe Defence, Security or Government sector
As an employer and service provider, we are committed to Equity, Equality, Diversity and Inclusion. Please feel comfortable to let us know if you have a difference or disability that would require us or our clients to make any helpful adjustments for you.
Show more
Show less","Data Warehousing, ETL Routines, Data Pipeline, Workflow Management, Azure Cloud Services, StreamProcessing Systems, SQL, Data Analytics, Big Data, Cloud Computing, Agile Development, DevOps, AWS, GCP","data warehousing, etl routines, data pipeline, workflow management, azure cloud services, streamprocessing systems, sql, data analytics, big data, cloud computing, agile development, devops, aws, gcp","agile development, aws, azure cloud services, big data, cloud computing, data pipeline, dataanalytics, datawarehouse, devops, etl routines, gcp, sql, streamprocessing systems, workflow management"
Data Engineer,BJSS,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-3591708801,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
About the Role
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","Python, CI/CD, Cloud Data Services, Parallel Computing, Relational and NonRelational Data Stores, AWS, Azure, GCP, Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion, O'Reilly","python, cicd, cloud data services, parallel computing, relational and nonrelational data stores, aws, azure, gcp, databricks, data factory, synapse, kafka, redshift, glue, athena, bigquery, s3, cloud data fusion, oreilly","athena, aws, azure, bigquery, cicd, cloud data fusion, cloud data services, data factory, databricks, gcp, glue, kafka, oreilly, parallel computing, python, redshift, relational and nonrelational data stores, s3, synapse"
Senior Data Engineer,Yolk Recruitment Ltd,"Newport, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-yolk-recruitment-ltd-3763364803,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer – up to £51,000 – Hybrid (Newport 1 day a week)
The Opportunity
Yolk Recruitment Public Sector & Not-for-Profit team has partnered with a specialised Civil Service Organisation who are presently undergoing an interesting transformation into the cloud space.
We are supporting them to recruit a Senior Data Engineer. This role is ideal for any data engineer who has extensive experience with Azure Data Factory and Python.
What the Senior Data Engineer will be doing
You will be supporting the Data Engineering Lead and working closely with the Data Management and Business Intelligence teams in order to build solutions, pipelines and plans using the current frameworks and toolkits.
Help develop world class data engineering capabilities
Own data engineering artefacts for data pipelines you will build
Prioritise data enhancements and plans alongside the IPO teams
Ensure the ease of data movement internally and externally
What the successful Senior Data Engineer will bring to the team
You will have actively used Azure Data Factory (ADF) and DataBricks as well as being proficient in Python.
Evidence of designing coding testing and correcting simple programs and scripts.
Ability to design solutions that are scalable and future-proof data services.
Experience cleansing data sets then formatting and preparing them
Experience of Data Modelling and Data Governance
Here’s What You’ll Get in Return
Pension scheme up to 27.9%
Salary of up to £51,000
Full Flexi-Time – Work your 37.5 hours on any schedule between 5am and 10pm
Accrued Time – If you work 37.5 hours in 4 days, you can have the fifth day off
Unlimited Pluralsight learning videos
25 Days annual leave PLUS Bank Holidays (Increasing to 30 days)
Think this one’s for you
If you think this Senior Data Engineer opportunity is for you then please apply online.
Yolk Public Sector & Not-for-Profit team works with organisations across the UK to fulfil their recruitment needs and to achieve their D&I objectives. We recruit temporary, contract and permanent hires for 1 off specialist needs or for volume campaigns. We support our applicants to navigate the public sector recruitment processes and secure their dream jobs.
Yolk Recruitment is an equal opportunities employer and embraces diversity in our workforce. We employ the best people for the job at hand and actively encourage applications from all qualified candidates, regardless of gender, age, race, religion, sexual orientation, disability, educational background, parental status, gender identity or any other protected characteristic. We champion and celebrate diversity at Yolk allowing our team to bring their whole selves to work.
Show more
Show less","Azure Data Factory, Python, Data engineering, Data pipelines, Data cleansing, Data modeling, Data governance, DataBricks, Pluralsight, SQL Server","azure data factory, python, data engineering, data pipelines, data cleansing, data modeling, data governance, databricks, pluralsight, sql server","azure data factory, data engineering, data governance, databricks, datacleaning, datamodeling, datapipeline, pluralsight, python, sql server"
Data Engineer,E-Solutions,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-e-solutions-3784395730,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"Role: Data Engineer (Oracle Fusion)
Location: Bristol, UK
Type: Contract (Inside IR35)
Job Description:
• Cloud platform technologies covering GCP / Oracle Cloud.
• Extensive experience of data product development including BIAN architecture.
• Appreciation of sophisticated enterprise strategies covering services being delivered across a hybrid hosting architecture of cloud and tradition platforms.
• API architecture standards including OpenID Connect profiles, API gateways, microservices including event driven architectures, and domain driven design methodology.
• Data repositories including relational and NoSQL databases, data warehouses and data lakes.
• Broad appreciation of cyber security (e.g., zero trust) and the architectural implications it has on application design.
• Modern progressive technologies – e.g., machine learning, artificial intelligence, block chain etc.
Show more
Show less","GCP, Oracle Cloud, BIAN architecture, OpenID Connect, API gateways, Microservices, Event driven architectures, Domain driven design, Relational databases, NoSQL databases, Data warehouses, Data lakes, Cyber security, Zero trust, Machine learning, Artificial intelligence, Blockchain","gcp, oracle cloud, bian architecture, openid connect, api gateways, microservices, event driven architectures, domain driven design, relational databases, nosql databases, data warehouses, data lakes, cyber security, zero trust, machine learning, artificial intelligence, blockchain","api gateways, artificial intelligence, bian architecture, blockchain, cyber security, data lakes, data warehouses, domain driven design, event driven architectures, gcp, machine learning, microservices, nosql databases, openid connect, oracle cloud, relational databases, zero trust"
Senior Data Engineer,SR2 | Socially Responsible Recruitment | Certified B Corporation™,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-sr2-socially-responsible-recruitment-certified-b-corporation%E2%84%A2-3773321426,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer
Up to £75,00
Tech4Good
Bristol
Chance to join an environmentally focussed B Corp
Be the strategic driving force of the company’s Azure data platform
Highly autonomous role responsible for maturing the data engineering function
Chane to work under one of the best mentors in the business so the career progression opportunities are massive
If you’re an environmentally conscious Data Engineer who is looking for a role where you can make a tangible impact on our planet whilst being pivotal to the success of the data engineering functions maturity, then this is for you!
I placed the Director of Data with this company almost two years ago and the data function has gone from strength to strength. They’ve increased headcount, adding an unbelievable Data Analytics Manager and they’re now in need of a technically astute, Azure-based Senior Data Engineer to really drive the adoption of their Azure data platform.
As this is a senior position it would be ideal if Azure was your bread and butter, however, I'm still keen to speak to those from AWS and GCP backgrounds.
The most important part is your ability to work strategically and not be exclusively hands-on, although being hands-on will still take up the majority of your time. Essentially, you will be the business SME when it comes to the data platform ensuring that the performance, data quality and governance are all taken care making this a multifaceted and genuinely exciting opportunity.
Cultural fit and behaviours are equally as important as technical ability. You need to genuinely care about the company mission and have the ability and desire to wear many hats. If you’re someone who needs strict processes in place or you’re unwilling to pick up responsibility outside of your remit, you won’t enjoy it here. Collaboration is key!
Tech Skills:
Azure - In-depth knowledge of the Azure environment is essential.
Snowflake - don’t need to have extensive knowledge but an understanding would be good.
Python, SQL, PowerBI
Understanding of governance and principles of security
This is a Bristol based role with 1-2 days a week in the office required.
Please note that sadly there is no sponsorship on offer for this role.
Speak to Jamie Forgan at SR2 to find out more.
Show more
Show less","Azure, Snowflake, Python, SQL, PowerBI, Data governance, Data security","azure, snowflake, python, sql, powerbi, data governance, data security","azure, data governance, data security, powerbi, python, snowflake, sql"
Senior Data Engineer,ADLIB Recruitment | B Corp™,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-adlib-recruitment-b-corp%E2%84%A2-3747049755,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"Join a high performing team supporting BI and data science functions.
Work with leading cloud and data technologies to shape the data landscape.
Lots of growth opportunities, with the team set to double in size.
Senior Data Engineer
Industry Leader in Insurance
We’re on the lookout for a seasoned Senior Data Engineer to join a leading figure in the insurance domain.
Be an influential force by sculpting the data infrastructure in an organisation where innovation is the focus. Your contribution will be pivotal in propelling them toward their goal of offering unparalleled solutions, all while harnessing the most advanced technologies in cloud-based and open-source data engineering.
What You’ll Be Doing
In your role as a Senior Data Engineer, your key responsibilities will be to:
Engineer, enhance and safeguard sophisticated data architectures, including databases and vast processing systems.
Construct intricate data streams to bolster analytical capabilities.
Construct efficient data frameworks for the extraction, transformation and loading of data.
Offer crucial data and analytic support to facilitate superior decision-making.
Embrace and drive automation.
With the recent integration of Snowflake, their ambition extends beyond data management; they’re set on harnessing data to innovate, evolve solutions and advance their mission.
What Experience You’ll Need To Apply
A rich background in Data Engineering.
Profound expertise in programming (Python, Java, etc.), SQL/NoSQL databases.
Skilled in data warehousing concepts.
Familiarity with data pipeline and workflow management systems such as Azkaban, Luigi, Airflow.
Experienced in utilizing cloud services.
Exceptional communicative clarity, especially in translating complex technical concepts to a non-technical audience.
What You’ll Get In Return For Your Experience
For your valuable experience, you’ll be rewarded with a base salary of up to £85,000 plus bonus and benefits. They also believe in a flexible work-life balance, offering a largely remote working model (1 day bi-weekly). For your career development, you’ll have access to top-tier learning and training programs, a variety of benefits, and exciting opportunities for growth.
If you value a culture that encourages individual input while achieving collective results, this is the place for you.
What’s next?
Feel like you’re the one? Click ‘Apply’ and let us know why you are the perfect fit for this role.
Show more
Show less","Data Engineering, Python, Java, SQL, NoSQL, Data Warehousing, Azkaban, Luigi, Airflow, Cloud Services, Snowflake","data engineering, python, java, sql, nosql, data warehousing, azkaban, luigi, airflow, cloud services, snowflake","airflow, azkaban, cloud services, data engineering, datawarehouse, java, luigi, nosql, python, snowflake, sql"
Lead Data Engineer at Revolutionary Aviation Tech Organisation - Bristol,Energy Jobline,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-revolutionary-aviation-tech-organisation-bristol-at-energy-jobline-3772981180,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"Are you a senior data professional with a passion for aviation and technology? Do you want to work on cutting-edge projects that will help to shape the future of flight? If so, I have the perfect opportunity for you.
As the Lead Data Engineer at a revolutionary aviation tech startup, you will be responsible for delivering data solutions that help to improve operations and optimize aircraft. You will also be responsible for leading and developing the data team, and helping us to build a world-class data culture.
This Bristol organisation are looking for a candidate with strong technical skills in Python, SQL, PostgreSQL, InfluxDB, Kafka, and Azure DevOps. You should also have experience working on real-time data pipelines and building data-driven dashboards and reporting systems
This company have a great benefits and is reviewing C.Vs this week and next, and I have interview availability over the next 2 weeks, to be considered please get in touch.
Rory Dare - (url removed)
Show more
Show less","Python, SQL, PostgreSQL, InfluxDB, Kafka, Azure DevOps, Realtime data pipelines, Datadriven dashboards, Reporting systems","python, sql, postgresql, influxdb, kafka, azure devops, realtime data pipelines, datadriven dashboards, reporting systems","azure devops, datadriven dashboards, influxdb, kafka, postgresql, python, realtime data pipelines, reporting systems, sql"
Data Engineer (Oracle Fusion),Insight International (UK) Ltd,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-oracle-fusion-at-insight-international-uk-ltd-3761454128,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"Job spec:
Cloud platform technologies covering GCP / Oracle Cloud.
Extensive experience of data product development including BIAN architecture.
Appreciation of sophisticated enterprise strategies covering services being delivered across a hybrid hosting architecture of cloud and tradition platforms.
API architecture standards including OpenID Connect profiles, API gateways, microservices including event driven architectures, and domain driven design methodology.
Data repositories including relational and NoSQL databases, data warehouses and data lakes.
Broad appreciation of cyber security (e.g., zero trust) and the architectural implications it has on application design.
Modern progressive technologies – e.g., machine learning, artificial intelligence, block chain etc.
Show more
Show less","GCP, Oracle Cloud, Data product development, BIAN architecture, Hybrid hosting architecture, Cloud platforms, API architecture standards, OpenID Connect profiles, API gateways, Microservices, Event driven architectures, Domain driven design, Relational databases, NoSQL databases, Data warehouses, Data lakes, Cyber security, Zero trust, Machine learning, Artificial intelligence, Blockchain","gcp, oracle cloud, data product development, bian architecture, hybrid hosting architecture, cloud platforms, api architecture standards, openid connect profiles, api gateways, microservices, event driven architectures, domain driven design, relational databases, nosql databases, data warehouses, data lakes, cyber security, zero trust, machine learning, artificial intelligence, blockchain","api architecture standards, api gateways, artificial intelligence, bian architecture, blockchain, cloud platforms, cyber security, data lakes, data product development, data warehouses, domain driven design, event driven architectures, gcp, hybrid hosting architecture, machine learning, microservices, nosql databases, openid connect profiles, oracle cloud, relational databases, zero trust"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Bath, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3728581669,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader in the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on Databricks. Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too.
You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration too. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks.
There is great room for progression within this role, and support and training will be provided as well.
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Data Engineering, Databricks, Azure Data Factory, SQL, Python, PySpark","data engineering, databricks, azure data factory, sql, python, pyspark","azure data factory, data engineering, databricks, python, spark, sql"
Data Analytics Engineer,Peaple Talent,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analytics-engineer-at-peaple-talent-3781849158,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"⚽ Data Analytics Engineer | £40,000-£50,000 | Bristol (Hybrid) ⚽
Peaple Talent are very excited to have partnered with a fantastic Bristol based Sports Information company, who provide services for sports and entertainment companies who are tasked to quantify 360-degree fan engagement.
After a very successful 2023, covering the biggest sporting spectacles including the FIFA Men's & Women's World Cup - We are now looking for a number of Data Analytics Engineers to help their clients make sense of their performance, through a blend of data engineering, data science & analytics.
The Role:
Daily tasks are rich and challenging, one minute you will be stitching data from multiple sources, the next visualising data in Power BI.
You will be setting up and maintaining ETL pipelines & utilising AI/ML for the analysis of the biggest sporting events in the world.
We welcome applications from candidates, from a wide range of backgrounds, who can demonstrate most of the following skills and qualifications:
We are looking for experience in the following areas:
Proficient in SQL & Python
Data visualisation skills in Power BI/Tableau/Looker
ETL pipeline development
Ability to work under pressure, with tight deadlines in a fast-paced and dynamic environment
Bonus points for:
Experience of AWS or Azure
Datawarehouse experience such as Databricks or Snowflake
If you'd like to work in an ambitious, fast-paced environment with access to work on some of the biggest sporting spectacles in the world.. apply below!
Show more
Show less","SQL, Python, Power BI, Tableau, Looker, ETL pipelines, AI, ML, AWS, Azure, Databricks, Snowflake","sql, python, power bi, tableau, looker, etl pipelines, ai, ml, aws, azure, databricks, snowflake","ai, aws, azure, databricks, etl pipelines, looker, ml, powerbi, python, snowflake, sql, tableau"
Data Engineer,Confidential,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-confidential-3786241254,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"Job Description:
Qualifications:
· 10+ years experience with Python and React, along with proficiency in TypeScript and JavaScript.
· 3+ years experience working with Java applications
· Strong knowledge of web development technologies, including HTML and CSS.
· Experience with Pytest and unit testing.
· Expertise in building RESTful APIs with Django and FastAPI.
· Familiarity with cloud services, particularly AWS (Lambda, EC2, Kubernetes, SQS) and Azure (Event Hubs).
· Proficiency in using monitoring and logging tools such as Sentry, Mixpanel, and Papertrail.
· Solid understanding of event-driven architecture, Design Patterns, and microservices.
· Experience with PostgreSQL and AWS S3.
· Experience in NLP and Machine Learning, with the ability to apply these skills to enhance applications and features.
· Knowledge of libraries and frameworks commonly used in NLP and Machine Learning projects.
· Terraform experience for infrastructure management.
· Excellent communication skills and the ability to work effectively in a collaborative team environment.
· A proven track record of successfully delivering complex software solutions.
· Experience with automation testing frameworks such as Playwright and Selenium.
· Experienced working in Agile environments within a Scrum team
Initiatives
· Support and maintenance of existing complex Java application
· Greenfield Microservices that utilize modern Architecture to migrate from monolithic applications that incorporate NLP
Regular Activities:
· Design, develop, test, and maintain Python and React-based applications.
· Develop, test, and maintain Java based application.
· Collaborate with cross-functional teams to design and implement solutions that align with business goals.
· Work on event-driven architectures and microservices to create scalable and efficient systems.
· Write unit tests, conduct code reviews, and ensure code quality through best practices.
· Develop RESTful APIs and integrate third-party services when necessary.
· Apply NLP and Machine Learning techniques to develop intelligent features and enhance user experiences.
· Troubleshoot and resolve software defects and performance issues.
· Utilize Terraform for infrastructure as code and manage cloud resources on AWS and Azure.
· Work with technologies like AWS EC2, AWS Kubernetes, AWS SQS, and Azure Event Hubs.
· Implement monitoring and logging solutions using Sentry, Mixpanel, Papertrail, and other relevant tools.
· Manage databases, including PostgreSQL, and work with AWS S3 for data storage.
· Implement and maintain CI/CD pipelines using GitHub Actions for automated testing, building, and deployment processes.
· Configure and manage automation testing frameworks such as Playwright and Selenium to ensure the reliability and quality of applications.
· Contribute to the evolution of our monolithic Python/Django application into a more modular and scalable architecture.
· Participate in production support and maintenance
· Participate in problem solving and troubleshooting with the development teams.
· Write clear, concise, well organized user documentation to maintain standards and procedures
· Participate in Scrum ceremonies
Show more
Show less","Python, React, TypeScript, JavaScript, Java, HTML, CSS, Pytest, Django, FastAPI, AWS, Azure, Sentry, Mixpanel, Papertrail, PostgreSQL, AWS S3, NLP, Machine Learning, Terraform, Playwright, Selenium, Scrum, Agile, GitHub Actions","python, react, typescript, javascript, java, html, css, pytest, django, fastapi, aws, azure, sentry, mixpanel, papertrail, postgresql, aws s3, nlp, machine learning, terraform, playwright, selenium, scrum, agile, github actions","agile, aws, aws s3, azure, css, django, fastapi, github actions, html, java, javascript, machine learning, mixpanel, nlp, papertrail, playwright, postgresql, pytest, python, react, scrum, selenium, sentry, terraform, typescript"
Senior/Lead Data Engineer,HomeLINK,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-lead-data-engineer-at-homelink-3759204824,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"Company Description
HomeLINK is a multi-award-winning high-tech business that leverages cutting-edge smart home integration and analytics technologies to address the needs of social landlords and their residents. HomeLINK has been recognised as one of the most innovative businesses in the SW region. Their team has industry-leading expertise in artificial intelligence and IoT as well as software development and integration. They have a track record of helping landlords reduce operating costs and carbon emissions whilst improving their residents' wellbeing and safety. HomeLINK is located in Bristol and offers some flexibility for remote work.
Role Description
HomeLINK are looking to bring a Data Engineer to the data team to lead the effort at designing data infrastructure for scale. As a new role, there is significant green field project opportunity for an experienced engineer to implement a vision for a modern, meaningful data engineering stack.
With over 10,000 new sensor readings ingested every second, current engineering efforts at HomeLINK have seen us adopt cutting edge column oriented database solutions, as well as streaming and many AWS services. This role would suit someone with advanced SQL skills (in particular with experience or knowledge about ClickHouse), someone experienced with RabbitMQ (and ideally one of Kafka or Kinesis) and someone with a strong grasp of the data engineering landscape. We’re looking for a self starter with a passion for data engineering and best practice.
Key responsibilities
﻿
Manage ClickHouse clusters to handle large-scale data storage and analytics
Manage MySQL database performance, including design of data models, query optimization and indexing
Advise and implement elegant data models, working with product managers and software engineers to bring customers more insight from our data
Establish and develop data contracts with upstream data providers
Share best practice/skills with the team through documentation, coaching etc.
Coordinate data ingestion processes
Help manage cloud architecture services
Introduce new software and technologies to the HomeLINK stack wherever this adds value to our data.
Preferred skills
Experience with cloud platforms such as AWS, particularly services such as RDS, S3, EC2, Lambda, Kinesis, EventBridge
Scripting and programming skills in languages like Python, Typescript
Experience conducting load testing
Experience building monitoring dashboards and developing system performance alarms e.g. with Kibana and Cloudwatch Alarms
Proficiency in (or decent knowledge of) ClickHouse or other column oriented databases
Strong SQL skills and understanding of database design principles
Experience with NoSQL databases like ElasticSearch, Dynamo DB
Experience with ETL tools and data integration processes
Familiarity with data pipeline orchestration and data streaming technologies such as RabbitMQ, Kinesis, Kafka.
What we look for
A passion for improving skills - we provide everyone with a Udemy subscription and book allowance
A self starter. Many projects will be green field or use new technologies like ClickHouse
Willingness to travel occasionally - we sometimes visit our sister company in Oswestry for team training
Someone who could be in our Bristol office a few days a week
Someone interested in being part of a high responsibility, close knit (and growing) team
Show more
Show less","Data Engineering, SQL, ClickHouse, RabbitMQ, Kafka, AWS, RDS, S3, EC2, Lambda, Kinesis, EventBridge, Python, Typescript, Load Testing, Kibana, Cloudwatch Alarms, Column Oriented Databases, Database Design Principles, NoSQL Databases, ElasticSearch, Dynamo DB, ETL Tools, Data Integration Processes, Data Pipeline Orchestration, Data Streaming Technologies","data engineering, sql, clickhouse, rabbitmq, kafka, aws, rds, s3, ec2, lambda, kinesis, eventbridge, python, typescript, load testing, kibana, cloudwatch alarms, column oriented databases, database design principles, nosql databases, elasticsearch, dynamo db, etl tools, data integration processes, data pipeline orchestration, data streaming technologies","aws, clickhouse, cloudwatch alarms, column oriented databases, data engineering, data integration processes, data pipeline orchestration, data streaming technologies, database design principles, dynamo db, ec2, elasticsearch, etl tools, eventbridge, kafka, kibana, kinesis, lambda, load testing, nosql databases, python, rabbitmq, rds, s3, sql, typescript"
Senior Azure Data Developer – UK Wide,WSP in the UK,"Bristol, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-azure-data-developer-%E2%80%93-uk-wide-at-wsp-in-the-uk-3780776586,2023-12-17,Bristol, United Kingdom,Mid senior,Hybrid,"We are WSP - Join us and make your career future ready!
In today’s world it’s important to work for a company that has clear purpose, giving back to communities and supporting what is truly important in the world.
When considering a career move it’s vital to work for a business that is aligned to your values and goals, a place where you can belong. See what WSP stands for in 2024 and beyond…
To find out more about our
Intelligent Transportation Services (ITS)
business click on the following link and discover what awaits you at WSP: Intelligent transportation services (ITS) | WSP
Your new role, what's involved?
Join our Intelligent Infrastructure discipline and make a real-world difference by helping to deliver crucial projects for public sector clients, which include examples from using big data to help the DfT provide better rail services, to helping road safety officers visualise accident data geospatially, or gathering vital transport planning data using AI-enabled traffic cameras that distinguish and count the different modes of transport.
Work on amazing projects, large and small, to deliver tangible value for our clients through technology enabled change
Engage with clients and technical stakeholders to understand requirements and solve problems
Estimate, Design, Build and Deliver digital solutions using industry best practice
Carry out peer reviews and provide direction and guidance to other team members
Input to the continuous improvement of practice and standards within the team
Provide contributions to the future technical strategy of the team
Help to support and maintain new and legacy software systems, ensuring timely and effective resolutions while maintaining clear communication with stakeholders.
You will benefit from Personal Development Reviews that will align your personal development plan to your own career goals, and those of the business, to help grow and develop your skillset and elevate your position within the business over time.
YOUR TEAM
This role sits within the Intelligent Infrastructure business and will form a key part of the growing Data and Architecture Capability, which is our centre of excellence for digital skills. The Capability team provides a community of colleagues working in and around technology to help you deliver effectively and support your professional development through training, mentoring and opportunities to really stretch your capabilities.
You’ll work as part of an Agile team with specialist developers, testers and analysts to design, develop, deliver and maintain software applications and systems using the latest technologies, helping to make people’s journeys greener, safer, cheaper, and faster to deliver innovative solutions across the property and infrastructure sectors.
We operate a hybrid working policy and expect colleagues to spend time on client sites and collaborating with colleagues face-to-face, as well as working from a local office. We have offices across the UK.
We'd love to hear from you if you have:
Excellent analytical skills and a proven ability to think creatively to solve problems.
Significant experience in the development of data solutions within Azure, utilising functionality such as Azure Data Factory / Lake, Synapse Analytics, Logic and Function Apps and Azure SQL
In depth knowledge of data transformations, pipelines, structured / unstructured data and data ingestion from various sources such as blob, Excel, API.
Strong experience of working in an Agile delivery team and producing and deploying applications in a production environment through a CI/CD pipeline.
Experience with building enterprise-level web applications or services, predominantly using ASP .Net and Azure.
Strong leadership skills, demonstrating the ability to effectively lead and collaborate with other developers to achieve project goals and deliver high-quality solutions.
Experience of working within a DevOps environment
Experience working with clients and customers and excellent communications skills.
A degree in an analytical discipline such as engineering, mathematics, computer science, economics, social sciences, or equivalent industry experience.
Desirable Skills And Experience
Non-Azure cloud platforms (AWS / Google)
Wider programming experience, such a Python, R, PHP, etc
Associated Microsoft (or other) certificates and qualifications
Data Analytics using AI/ML
Visual analytics tools, e.g. Power BI / Tableau / etc.
Good general understanding of data governance/quality/privacy practices
Experience of NOSQL data stores such as MongoDB / CosmosDB
Experience of applying Software Architecture Patterns.
Infrastructure as code technologies and cloud technologies and distributed systems
Business Analysis, Solutions Architecture
Experience in both Cloud and On-Premise server infrastructure
Experience of applying Design Patterns to complex problems and architecting solutions
What's in it for you?
Work-life balance?
WSP recognises that work is only one part of your life and making time for other things is important – whether that’s for your families, friends, or yourself.
Our hybrid working policy allows the flexibility to work from the comfort of your own home as well as collaborating in our contemporary offices across the UK.
Inclusivity & Diversity?
We want our people to achieve rewarding careers, bringing their whole selves to work. We celebrate integrity and treat people with respect, supporting each other and embracing diversity to create a culture of inclusion and belonging at WSP.
Our employee resource groups VIBE (LGBTQ+ employees), CREED (Championing Racial Equality and Ethnic Diversity) and our Gender Balance Group, in tandem with WSP’s Neurodiverse Community Group, WSP Connect Group (visible and non-visible disabilities) help us promote the right environment for you to reach your full potential.
Health & Wellbeing?
We are committed to supporting our people, giving you the tools to make improvements to your health and wellbeing through our Thrive programme.
Med24 gives you and your family unrestricted telephone access to an NHS doctor where you can call day or night or have a face-to-face video consultation.
Flex your time?
For improved work life balance, WSP offers the “WSP Hour” which enables you to take one hour per day to do as you wish and make up the time earlier or later that day. We also offer part time and flexible working arrangements plus the option to flex your bank holiday entitlement to suit you.
Your development?
We appreciate that development and training is important to you and that’s why we have a supportive environment that invests in your development, whether that’s chartership, training or mentoring.
Apply now and be the future of WSP!
#WeAreWSP
Here at WSP we positively encourage applications from suitably qualified and eligible candidates regardless of sex, race, disability, age, sexual orientation, gender reassignment, religion or belief, marital status, pregnancy or maternity/paternity. As a Disability Confident leader, we will interview all disabled applicants who meet the essential criteria, please let us know if you require any workplace adjustments in support of your application.
Please note WSP reserves the right to close the vacancy before the advertised closing date.
Show more
Show less","Azure, Azure Data Factory, Azure Lake, Synapse Analytics, Logic Apps, Function Apps, Azure SQL, Data transformations, Data pipelines, Structured data, Unstructured data, Data ingestion, Blob, Excel, API, DevOps, ASP .Net, Python, R, PHP, Microsoft certificates, Data Analytics, AI/ML, Power BI, Tableau, Data governance, Data quality, Data privacy, MongoDB, CosmosDB, Software Architecture Patterns, Infrastructure as code, Distributed systems, Business Analysis, Solutions Architecture, Cloud infrastructure, Onpremise server infrastructure, Design Patterns","azure, azure data factory, azure lake, synapse analytics, logic apps, function apps, azure sql, data transformations, data pipelines, structured data, unstructured data, data ingestion, blob, excel, api, devops, asp net, python, r, php, microsoft certificates, data analytics, aiml, power bi, tableau, data governance, data quality, data privacy, mongodb, cosmosdb, software architecture patterns, infrastructure as code, distributed systems, business analysis, solutions architecture, cloud infrastructure, onpremise server infrastructure, design patterns","aiml, api, asp net, azure, azure data factory, azure lake, azure sql, blob, business analysis, cloud infrastructure, cosmosdb, data governance, data ingestion, data privacy, data quality, data transformations, dataanalytics, datapipeline, design patterns, devops, distributed systems, excel, function apps, infrastructure as code, logic apps, microsoft certificates, mongodb, onpremise server infrastructure, php, powerbi, python, r, software architecture patterns, solutions architecture, structured data, synapse analytics, tableau, unstructured data"
"Data Analyst, Senior",Southern Farm Bureau Casualty Insurance Company,"Ridgeland, MS",https://www.linkedin.com/jobs/view/data-analyst-senior-at-southern-farm-bureau-casualty-insurance-company-3783064797,2023-12-17,Magnolia,United States,Mid senior,Onsite,"This position will direct and perform logical data modeling, identify patterns in data, design, analyze and document the enterprise data warehouse and ETL processes supporting it.
This team member actively participates in Data Governance initiatives to support the Enterprise Data Strategy across operational and analytical database environments. This position will work closely with the Data Analyst Lead working close to support the Enterprise Data Warehouse.
Show more
Show less","Data Modeling, Data Analysis, Data Warehousing, ETL Processes, Data Governance, Enterprise Data Strategy, Data Analyst","data modeling, data analysis, data warehousing, etl processes, data governance, enterprise data strategy, data analyst","data governance, dataanalytics, datamodeling, datawarehouse, enterprise data strategy, etl"
Business Data Analyst,MAHLE,"Olive Branch, MS",https://www.linkedin.com/jobs/view/business-data-analyst-at-mahle-3776289177,2023-12-17,Magnolia,United States,Mid senior,Onsite,"We move the world – together. As an international leading development partner and supplier of the automotive industry, we at MAHLE are passionate about developing innovative solutions for climate-neutral mobility of tomorrow. Our focus is on e-mobility and thermal management as well as further technology fields to reduce CO2 emissions, such as fuel cells and hydrogen motors.
In 2021, MAHLE with its more than 71,000 employees, generated sales of around EUR 11 billion and with its 160 locations, including 12 large research and development centers, is represented in more than 30 countries. Come join our team at MAHLE! #StrongerTogether
SUMMARY
This position will fill a key support role within the Controlling Department and will be a business partner to operations. The position requires strong analytical skills and information systems knowledge. The successful candidate will be well organized, self-starting, and energetic, with demonstrated presentations skills and an outstanding work ethic.
ESSENTIAL DUTIES AND RESPONSIBILITIES
Actively support the Business Unit period end closing controlling activities.
Analyze, interpret, and comment on operating performance.
Identify improvement potentials and support the definition of respective measures by partnering with management on financial performance.
Drive the ME performance management processes (planning, budgeting, forecasting, reporting) and partner with management on respective financial aspects.
Use available business information tools including Microsoft Excel, Celonis, and SAP analytics Cloud to analyze, interpret, and create reports which summarize large quantities of statistical and account information into actionable management information.
Ability to work with a team to create presentations containing charts and graphs.
Perform general accounts analysis and reconciliations, including fixed assets, employer’s benefit costs, accruals, prepaid expenses, and recording of recurring, adjusting, and reclassification journal entries, when necessary.
Work heavily within the SAP ERP system and use it identify, analyze, and solve problems.
QUALIFICATIONS
Bachelor’s degree in accounting, finance, or other related field.
Up to 5 years post graduate experience.
High energy and ambitious with a demonstrated history to show ambition.
Professional and non-professional examples of making a difference in the company and/or community.
Must be able to work in fast paced environment.
Have strong/advanced Microsoft Excel and Powerpoint skills.
IT capability must be demonstrated. Prior knowledge of SAP is a plus but must demonstrate an aptitude to learn the ERP system and use it to solve problems.
Detailed oriented with an excellent ability to analyze data and circumstances.
Strong problem-solving techniques and decision-making skills.
Ability to perform multiple tasks simultaneously, change direction at a moment’s notice, while still meeting deadlines.
Strong Critical thinking skills and the ability to proffer quality ideas to improving processes.
Good at working both in a team environment and independently.
Show more
Show less","Analytical skills, Microsoft Excel, Celonis, SAP analytics Cloud, SAP ERP, Accounting, Finance, PowerPoint, IT capability","analytical skills, microsoft excel, celonis, sap analytics cloud, sap erp, accounting, finance, powerpoint, it capability","accounting, analytical skills, celonis, finance, it capability, microsoft excel, powerpoint, sap analytics cloud, sap erp"
"Data Analyst, CSB",C Spire,"Ridgeland, MS",https://www.linkedin.com/jobs/view/data-analyst-csb-at-c-spire-3769023639,2023-12-17,Magnolia,United States,Mid senior,Onsite,"The Data Analyst, CSB is expected to work with and support the demand for data and information by business users, business analysts, and decision makers. Responsibilities include data analysis, generating dashboards, data visualizations, reports, datasets, and information for end-users using SQL or other data extraction tools, reporting tools, or data visualization tools. Analyze data to aid in addressing complex business problems and issues. Construct models, strategic and tactical plans based on data analysis.
Responsibilities
Support internal customers performing a variety of complex queries or analytical tasks.
Integrate data from multiple sources to produce required results.
Work closely with internal customers providing data analysis support or instruction.
Test complex queries and audit data extracts to ensure the data meets requirements.
Comply with policies, including but not limited to, defining scope, coding and query standards, quality assurance processes, and data verification policies.
Work within Salesforce to support teams within CSB.
Perform additional duties as assigned.
Qualifications
Baccalaureate degree in Mathematics, Statistics, Business Administration, Information Systems, Computer Science, Economics, Engineering, or a related discipline, or +4 years of data-focused work experience required.
Ability to use software tools to gather and extract data for review and analysis is required.
Excellent oral and written communication skills are required, including the ability to communicate clearly and succinctly required.
Strong organization and presentation skills required.
Excellent planning and execution, with attention to detail; ability to support multiple projects simultaneously is required.
Proven experience using SQL required.
An understanding of business terms, business processes, and data systems required.
Ability to perform statistical analysis using Excel or similar applications required.
Experience working with Salesforce preferred.
Show more
Show less","Data analysis, Data visualization, Dashboards, Reporting, Data extraction, SQL, Data modeling, Strategic planning, Tactical planning, Complex queries, Data integration, Data support, Data instruction, Data quality assurance, Data verification, Salesforce, Mathematics, Statistics, Business administration, Information systems, Computer science, Economics, Engineering, Excel","data analysis, data visualization, dashboards, reporting, data extraction, sql, data modeling, strategic planning, tactical planning, complex queries, data integration, data support, data instruction, data quality assurance, data verification, salesforce, mathematics, statistics, business administration, information systems, computer science, economics, engineering, excel","business administration, complex queries, computer science, dashboard, data extraction, data instruction, data integration, data quality assurance, data support, data verification, dataanalytics, datamodeling, economics, engineering, excel, information systems, mathematics, reporting, salesforce, sql, statistics, strategic planning, tactical planning, visualization"
"Direct client: Data Analyst - Pathways: Position@ Jackson, MS",Infobahn Softworld Inc,"Jackson, MS",https://www.linkedin.com/jobs/view/direct-client-data-analyst-pathways-position%40-jackson-ms-at-infobahn-softworld-inc-3713255182,2023-12-17,Magnolia,United States,Mid senior,Onsite,"Job Title - Data Analyst - Pathways
Location: Jackson, MS, 39211 (Onsite)
Duration: 12 months
Job Description
MDCPS seeks a professional to participate in planning and execution of data cleansing and data migration initiatives and concurrently participate in development of data models, data stack architecture, technical implementation and other critical functions of data management that will enable evidence-based Child Welfare Practice at MDCPS.
Position Summary
This position is full-time (40 hours weekly) and requires an individual to participate as a contributing team member on the MDCPS Data Management Team (DMT).
The DMT is an emergent capability within MDCPS.
The near-term scope of this team and this position includes participating in data related migration and design activities on the replacement of legacy case management system with the new CCWIS Cloud-Based hosted solution, and additional to that, implementation of cloud-based data stack to enable robust and actionable analytics capabilities at MDCPS.
This individual will be reporting to the lead architect on the DMT.
Required Skills
Four years of experience with Multidimensional Model Data Base Design.
ETL design and development.
Four years of experience with calculating data error rates and data cleansing on source databases.
Four years of experience with data quality and master data management integration.
Four years of experience with ERD's, business vocabulary, data dictionary and database schema mapping.
Four years of data analytic implementation projects.
Two years of experience with cloud-based data stack tool and platforms.
Excellent English-speaking skills, written communication skills, and knowledge transfer skills
Show more
Show less","Data Migration, Data Cleansing, Data Modeling, Data Stack Architecture, ETL Design and Development, Data Quality Management, Master Data Management, ERD, Business Vocabulary, Data Dictionary, Database Schema Mapping, Data Analytic Implementation, CloudBased Data Stack Tools and Platforms","data migration, data cleansing, data modeling, data stack architecture, etl design and development, data quality management, master data management, erd, business vocabulary, data dictionary, database schema mapping, data analytic implementation, cloudbased data stack tools and platforms","business vocabulary, cloudbased data stack tools and platforms, data analytic implementation, data dictionary, data migration, data quality management, data stack architecture, database schema mapping, datacleaning, datamodeling, erd, etl design and development, master data management"
Lead Data Engineer,Encore Technologies,"Ohio, United States",https://www.linkedin.com/jobs/view/lead-data-engineer-at-encore-technologies-3720913089,2023-12-17,Genoa,United States,Mid senior,Onsite,"Must Have
Database Configuration
Data Field Mapping
Data input/output integrations
Data Strategy (import or export based on the use case)
Data Visualization into BI Tools
PPRC (Taxonomy) build out and design.
Reporting Capabilities
Nice To Have
Financial Industry Knowledge
RSA Archer
Understanding of governance, risk, and compliance programs and processes
Minimum Knowledge, Skills And Abilities Required
Bachelor's degree in computer science/Information Systems or equivalent combination of education and experience.
Must be able to communicate ideas both verbally and in writing to management, business and IT sponsors, and technical resources in language that is appropriate for each group.
At least 6 years of related experience, including at least 4 years in a hands-on software development role.
Significant experience with at least one major RDBMS product.
Experience working with and supporting Unix/Linux and Windows systems.
Proficient in relational database modeling concepts and techniques.
Solid conceptual understanding of distributed computing principles.
Working knowledge of application and data security concepts, best practices, and common vulnerabilities.
Experience in one or more of the following disciplines preferred: big data technologies and distributions, metadata management products, commercial ETL tools, Bi and reporting tools, messaging systems, data warehousing, Java (language and run time environment), major version control systems, continuous integration/delivery tools, infrastructure automation and virtualization tools, major cloud, or rest API design and development.
Financial industry experience is a plus.
Essential Duties And Responsibilities
Responsible for design, Development, and Support of data solutions, APIs, tools, and processes to enable rapid delivery of business capabilities.
Work closely with IT application teams, Enterprise architecture, infrastructure, information security, and LOB stakeholders to translate business and technical strategies into data-driven solutions for the Bank.
Act as a technical Expert addressing problems related to system and application design, performance, integration, security, etc.
Conduct research and Development based on current trends and technologies related to the banking industry, data engineering and architecture, data security, and related topics.
Work with developers to Build CI/CD pipelines, Self-service Build tools, and automated deployment processes.
Evaluate software products and provide documented recommendations as needed.
Provide Support and troubleshooting for data platforms. Must be willing to Provide escalated on-call Support for complicated and/or critical incidents.
Participate in the planning process for hardware and software.
Plan and work on internal projects as needed, including legacy system replacement, Monitoring and analytics improvements, tool Development, and technical documentation.
Provide technical guidance and mentoring for other team members.
Manage and prioritize multiple assignments.
Encore Technologies is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills, and experiences within our workforce.
Show more
Show less","Database Configuration, Data Field Mapping, Data input/output integrations, Data Strategy, Data Visualization, PPRC Design, Reporting Capabilities, Unix/Linux, Windows, Relational Database Modeling, Distributed Computing Principles, Application and Data Security Concepts, Big Data Technologies, Metadata Management, ETL Tools, BI and Reporting Tools, Messaging Systems, Data Warehousing, Java, Version Control Systems, Continuous Integration/Delivery Tools, Infrastructure Automation, Virtualization Tools, Cloud, REST API Design/Development, System Design, Application Design, Performance, Integration, Security, CI/CD Pipelines, SelfService Build Tools, Automated Deployment Processes, Software Evaluation, Platform Support, Troubleshooting, Hardware Planning, Software Planning, Legacy System Replacement, Monitoring and Analytics, Tool Development, Technical Documentation, Technical Guidance, Mentoring","database configuration, data field mapping, data inputoutput integrations, data strategy, data visualization, pprc design, reporting capabilities, unixlinux, windows, relational database modeling, distributed computing principles, application and data security concepts, big data technologies, metadata management, etl tools, bi and reporting tools, messaging systems, data warehousing, java, version control systems, continuous integrationdelivery tools, infrastructure automation, virtualization tools, cloud, rest api designdevelopment, system design, application design, performance, integration, security, cicd pipelines, selfservice build tools, automated deployment processes, software evaluation, platform support, troubleshooting, hardware planning, software planning, legacy system replacement, monitoring and analytics, tool development, technical documentation, technical guidance, mentoring","application and data security concepts, application design, automated deployment processes, bi and reporting tools, big data technologies, cicd pipelines, cloud, continuous integrationdelivery tools, data field mapping, data inputoutput integrations, data strategy, database configuration, datawarehouse, distributed computing principles, etl tools, hardware planning, infrastructure automation, integration, java, legacy system replacement, mentoring, messaging systems, metadata management, monitoring and analytics, performance, platform support, pprc design, relational database modeling, reporting capabilities, rest api designdevelopment, security, selfservice build tools, software evaluation, software planning, system design, technical documentation, technical guidance, tool development, troubleshooting, unixlinux, version control systems, virtualization tools, visualization, windows"
Data Analyst - Sr,Trew,"Ohio, United States",https://www.linkedin.com/jobs/view/data-analyst-sr-at-trew-3694948957,2023-12-17,Genoa,United States,Mid senior,Remote,"TREW’s Story
Business gets done working together. Successful business happens when trusted partners work together, to win together. At TREW we know that our customers buy solutions and technology built by people. With over 400 team members, we work fearlessly every day to do the right thing, even when no one is watching. From seasoned professionals to undergraduate co-ops, our team members enjoy seeing the impact of their contributions every day.
Analytics Duties
Design and build scalable models and reports to create data driven answers to business questions across the value stream from sales to customer aftermarket support.
Monitor, maintain, and support analytics tools (Power BI, Power Query enabled Excel books, Microsoft Power Apps)
Work with ERP and IT teams to understand the structure of our data systems.
Data Stewardship / Information Strategy
Work across the organization to understand business data needs and advise on information strategy, contributing to the overall data roadmap for the organization. Verify the business is capturing the right data, the right way, for our future business needs while maintaining a business appropriate data collection workload for the organization.
Skills / Education / Experience
5+ years experience in analytics development and support
Ability to work cross functionally within the organization with minimal oversight.
Bachelor's Degree in relevant discipline: Software, Engineering, Statistics
Strong skills in Excel, Power BI (including Power Query, DAX, M)
Working knowledge of MS SharePoint / Teams.
Strong written and verbal communication skills.
Background in a project-based business, and/or a highly configured product industry a plus.
Other Skills That Would Be Helpful But Not Required
Netsuite ERP
MS Power Apps
Azure Storage (data warehousing such as synapse)
C# Programming
VBA (Supporting Excel legacy applications)
Trew/Hilmot/TKO is an equal opportunity employer. Applicants will be considered for employment without regard to age, race, creed, color, national origin, ancestry, marital status, affectional or sexual orientation, gender identity or expression, disability, nationality, sex, or veteran status.
Show more
Show less","Data Analytics, Power BI, Power Query, DAX, M, Excel, Power Apps, SQL, Azure, Netsuite ERP, C#, VBA, SharePoint, Teams, Data Stewardship, Information Strategy, Data Warehousing","data analytics, power bi, power query, dax, m, excel, power apps, sql, azure, netsuite erp, c, vba, sharepoint, teams, data stewardship, information strategy, data warehousing","azure, c, data stewardship, dataanalytics, datawarehouse, dax, excel, information strategy, m, netsuite erp, power apps, power query, powerbi, sharepoint, sql, teams, vba"
Staff Cybersecurity Data Platform Engineer,Adobe,"Ohio, United States",https://www.linkedin.com/jobs/view/staff-cybersecurity-data-platform-engineer-at-adobe-3767921516,2023-12-17,Genoa,United States,Mid senior,Remote,"Our Company
Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.
We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!
As a Staff Cybersecurity Data Platform Engineer at Adobe, you will be joining a team responsible for enhancing the organization's security data platform. Our focus is on contributing to the organization by emphasizing real-world security and embracing automation, as well as optimizing data flow and consumption of the data to multiple teams within Security.
The ideal candidate will help develop security-focused data platform needs and the scaling of a data lake spanning petabytes of data. The job involves system architecture, design, hands-on development, optimization, setting expectations and SLAs with a focus on reliability, availability, and performance.
They must be self-directed and comfortable supporting the data needs of multiple security teams. The right candidate will be excited by the prospect of optimizing our company’s data platform to support our next generation of security products and data initiatives.
Key Responsibilities:
Collaborate with partners, cybersecurity engineers, and operations teams across the security organization to drive the development of enterprise-scale security solutions.
Work with enterprise architects team to ensure alignment with strategic objectives, and leverage design principles while providing input into program direction.
Translate business needs into technical requirements, capabilities, and formulation of solutions, while identifying risks, dependencies, financial impacts, and the risk profile in the technical solution.
Take a hands-on approach to driving proof-of-concept, design, and implementation activities from an architectural perspective.
Develop and maintain solution architecture documents and other artifacts to guide the planning, design, and implementation of the proposed solution.
Required Skills to be Successful:
7 years of experience in designing and building solutions in Databricks, across multiple clouds, using orchestration, data ingestion, medallion architectures, unity catalog, autoloader jobs, and Delta Lakehouse concepts.
10 years of experience working as either: Software Engineer/Data Engineer: query tuning, performance tuning, troubleshooting, and implementing/debugging Spark/PySpark and/or other big data solutions.
10 years of proficiency in developing or architecting modern distributed cloud architectures using AWS tools and technology.
Design end-to-end robust, scalable, real-time data streaming (Kafka / Flink) and data platform architecture that will support the analytical and reporting needs of the entire organization.
Offer technical expertise by collaborating with analysts and business users to translate diverse and intricate functional specifications into technical designs.
Build data models and improve standard schemas across different data sources and normalize data.
Significant experience in security, encompassing threat management, incident response, and enterprise security.
Strong understanding of Security Operations Center (SOC) operations and security management workflows within large organizations.
Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $146,400 -- $275,000 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.
At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).
In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.
Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.
Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.
Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.
Show more
Show less","Databricks, Cloud Architecture, AWS, Kafka, Flink, Spark, PySpark, Big Data Solutions, Data Models, Schemas, Security Operations Center (SOC), Threat Management, Incident Response, Enterprise Security","databricks, cloud architecture, aws, kafka, flink, spark, pyspark, big data solutions, data models, schemas, security operations center soc, threat management, incident response, enterprise security","aws, big data solutions, cloud architecture, data models, databricks, enterprise security, flink, incident response, kafka, schemas, security operations center soc, spark, threat management"
Sr. Land Data Analyst,"Endeavor Energy Resources, LP","Midland, TX",https://www.linkedin.com/jobs/view/sr-land-data-analyst-at-endeavor-energy-resources-lp-3688410478,2023-12-17,Midland,United States,Mid senior,Onsite,"As one of the largest employers in the Permian, we’re committed to the success of our employees and providing an environment that fosters people and teamwork, continuous improvement, HSE excellence, technical and financial discipline, and integrity. Poised for sustainable growth, we look forward to executing our horizontal program with a world class land position and valued employees whose knowledge and expertise drive the success of this company. Interested in joining our team?
Land Data Analyst are responsible for researching, identifying, and analyzing land data within Company land systems and delivering the data to stakeholders within the enterprise in an accurate, streamlined, and consumable format.
Essential Duties And Responsibilities
The following represents many of the duties performed by the position, but is not meant to be all-inclusive nor prevent other duties from being assigned when necessary:
Knowledgeable of land specific data, including tracts, depths, ownership and terms of oil and gas leases and/or assignments and input data into land database
Review, identify, analyze, and update large amount of data associated with oil and gas leases, contracts, surface, right-of-way, mineral deeds, title opinions and division orders in Company land system
Build simple to use tools, dashboards, and reports that are capable of handling large amounts of data to drive decisions which enhance Company performance and drive value
Use of data analytics to provide quick reports, visual aids and statistical analysis facilitating information sharing and process improvement
Extract, Transform and Load (ETL) data from acquired asset data rooms into core Company systems
Communicate technical requirements of the Company Land Department to Company IT Department and/or 3rd party software vendors (i.e. Enertia)
Assist with special projects such as audits, acquisitions, and divestitures
Other tasks as assigned
Skills And Experience
Bachelor’s degree required (Data Science, Energy Management or Business Degree preferred) The combination of education and relevant experience may substitute for degree
Minimum of 3 years of experience in the oil and gas industry; Data Science, Land or Land Administration preferred
Exhibit a high aptitude in analytical thinking
Experience with data extraction, data automation, artificial intelligence and machine learning preferred
Proven proficiency with data analytics software (Spotfire and/or PowerBi preferred)
Established proficiency in Microsoft Office required, with emphasis in Excel, Word, and Teams.
Experience with Land Data Management Software (Enertia preferred)
Experience with Document Management Systems (M-Files and/or Thomson Reuters Document Intelligence preferred)
Ability to effectively communicate and build relationships with various levels of company personnel
Demonstrate time management and organizational skills approaching projects with a sense of urgency
Detail oriented with the ability to multi-task with attention to accuracy
Recognizes value in, and seeks opportunities for, continued development and improvement
Ability to seek out, implement and adapt to new technologies, workflows, and processes
Team player who possesses the ability to adapt to a changing and fast paced work environment
Endeavor Energy Resources, LP is an Equal Opportunity Employer and does not discriminate in regard to race, color, creed, age, religion, ancestry, national origin, sex, genetics, marital status or disability. Endeavor Energy Resources, L.P. complies with all local, state, and federal laws pertaining to employment, and discrimination will not be tolerated.
Show more
Show less","Data Science, Energy Management, Business Degree, Oil and Gas Industry, Analytical Thinking, Data Extraction, Data Automation, Artificial Intelligence, Machine Learning, Data Analytics Software, Spotfire, PowerBI, Microsoft Office, Excel, Word, Teams, Land Data Management Software, Enertia, Document Management Systems, MFiles, Thomson Reuters Document Intelligence, Effective Communication, Relationship Building, Time Management, Organizational Skills, Detail Orientation, MultiTasking, Accuracy, Continuous Development, Adaptability, New Technologies, Workflows, Processes, Team Player, FastPaced Work Environment","data science, energy management, business degree, oil and gas industry, analytical thinking, data extraction, data automation, artificial intelligence, machine learning, data analytics software, spotfire, powerbi, microsoft office, excel, word, teams, land data management software, enertia, document management systems, mfiles, thomson reuters document intelligence, effective communication, relationship building, time management, organizational skills, detail orientation, multitasking, accuracy, continuous development, adaptability, new technologies, workflows, processes, team player, fastpaced work environment","accuracy, adaptability, analytical thinking, artificial intelligence, business degree, continuous development, data analytics software, data automation, data extraction, data science, detail orientation, document management systems, effective communication, energy management, enertia, excel, fastpaced work environment, land data management software, machine learning, mfiles, microsoft office, multitasking, new technologies, oil and gas industry, organizational skills, powerbi, processes, relationship building, spotfire, team player, teams, thomson reuters document intelligence, time management, word, workflows"
Data Center Building Operating Engineer (2nd shift),JLL,"Richardson, TX",https://www.linkedin.com/jobs/view/data-center-building-operating-engineer-2nd-shift-at-jll-3770695700,2023-12-17,McKinney,United States,Associate,Onsite,"Data Center Operating Engineer
General Description:
The Data Center Building Engineer is responsible for delivery of best practice systems and problem resolution on all data center electrical and mechanical infrastructure (UPS, MV electrical systems, generators, cooling systems etc.)
Location:
Principal Duties and Responsibilities
Task will include but not be limited to:
Responsible for maintaining, monitoring, and performing preventive maintenance and continuous operation of all building systems to maintain 100% Up-time including: fire/life safety, mechanical systems such as (HVAC, chillers, crac, crah, plumbing, controls), electrical including emergency backup systems such as (lighting, UPS, ATS, STS, PDU, generators, primary switchgear, power distribution, transformers), and hot water systems. Monitors operation, adjusts, and maintains refrigeration, chilled water, and air conditioning equipment; boilers, and ventilating and water heaters; pumps, valves, piping, and filters; other mechanical and electrical equipment. Must record readings and make and adjust where necessary to ensure proper operation of equipment.
Requires the ability to analyze the operation of various systems, determine the cause of any problems/malfunctions and take corrective action as required.
Comply with departmental policy for the safe storage, usage, and disposal of hazardous materials. Maintains a clean and safe workplace.
Learn and understand the data center site in-order to manage incidents and events that put the critical systems at risk.
Work order management, including CMMS, Vendor Management, and Customer Facing Tickets.
Understanding and complying with emergency escalation procedures.
Perform additional job duties as required.
Minimum Requirements:
Preferred to have hands-on experience working in a data center/critical facility, including UPS.
Systems, emergency generators, and switchgears.
High School diploma or GED equivalent
2+ years related work experience.
Working knowledge of computer applications including Word and Excel.
Demonstrated verbal/written communication skills.
Preferred Requirements:
Corrigo Experience.
MCIM / Salesforce Experience.
Zendesk Experience.
Service Now Experience.
Received EPA 608.
Trained in NFPA70E.
Show more
Show less","UPS, Electrical Systems, Generators, Cooling Systems, HVAC, Chillers, CRAC, CRAH, Plumbing, Controls, Lighting, ATS, STS, PDU, Power Distribution, Transformers, Hot Water Systems, CMMS, Vendor Management, Customer Facing Tickets, Corrigo, MCIM, Salesforce, Zendesk, Service Now, EPA 608, NFPA70E","ups, electrical systems, generators, cooling systems, hvac, chillers, crac, crah, plumbing, controls, lighting, ats, sts, pdu, power distribution, transformers, hot water systems, cmms, vendor management, customer facing tickets, corrigo, mcim, salesforce, zendesk, service now, epa 608, nfpa70e","ats, chillers, cmms, controls, cooling systems, corrigo, crac, crah, customer facing tickets, electrical systems, epa 608, generators, hot water systems, hvac, lighting, mcim, nfpa70e, pdu, plumbing, power distribution, salesforce, service now, sts, transformers, ups, vendor management, zendesk"
"Asset & Wealth Management Engineering, Data Engineer-Associate-Richardson",Goldman Sachs,"Richardson, TX",https://www.linkedin.com/jobs/view/asset-wealth-management-engineering-data-engineer-associate-richardson-at-goldman-sachs-3725151840,2023-12-17,McKinney,United States,Associate,Onsite,"Job Description
Associate, Sr Data Engineering Engineer (SDE)
Asset & Wealth Management
Across Asset & Wealth Management, Goldman Sachs helps empower clients and customers around the world to reach their financial goals. Our advisor-led wealth management businesses provide financial planning, investment management, banking and comprehensive advice to a wide range of clients, including ultra-high net worth and high net worth individuals, as well as family offices, foundations and endowments, and corporations and their employees. Our direct-to-consumer business provides digital solutions that help customers save and invest. Across Wealth Management, our growth is driven by a relentless focus on our people, our clients and customers, and leading-edge technology, data and design.
Marcus by Goldman Sachs
The firm’s direct-to-consumer business, Marcus by Goldman Sachs, combines the entrepreneurial spirit of a start-up with more than 150 years of experience. Today, we serve millions of customers across multiple products, leveraging innovative design, data, engineering and other core capabilities to provide customers with powerful tools and products that are grounded in value, transparency and simplicity.
Role Overview
This role offers the opportunity to work in a competitive & nimble team of engineers building and supporting the firm's direct to consumer deposits business. Candidates will collaborate extensively with Engineering, Analytics and Product teams to support functional use-cases and take data driven decisions to build and enhance the state of the art, industry leading, & best in class online and phone banking platform.
This person will be responsible for expanding and optimizing our cloud based data pipeline architecture. The ideal candidate has experience building robust data pipelines and reporting tools. However, someone with a strong application development background with keen interest in data engineering would be considered.
Technical Skills And Qualifications
4+ years of experience in data processing & software engineering and can build high-quality, scalable data oriented products
Experience on distributed data technologies (e.g. Hadoop, MapReduce, Spark, EMR, etc..) for building efficient, large-scale data pipelines
Strong Software Engineering experience with in-depth understanding of Python, Scala, Java or equivalent
Strong understanding of data architecture, modeling and infrastructure
Experience with building workflows (ETL pipelines)
Experience with SQL and optimizing queries
Problem solver with attention to detail who can see complex problems in the data space through end to end
Willingness to work in a fast paced environment
MS/BS in Computer Science or relevant industry experience
Strongly recommended (but optional)
Experience building scalable applications on the Cloud (Amazon AWS, Google Cloud, etc..)
Experience building stream-processing applications (Spark streaming, Apache-Flink, Kafka, etc..)
Experience with ETL tools like Informatica Power Center
About Goldman Sachs
At Goldman Sachs, we commit our people, capital and ideas to help our clients, shareholders and the communities we serve to grow. Founded in 1869, we are a leading global investment banking, securities and investment management firm. Headquartered in New York, we maintain offices around the world.
We believe who you are makes you better at what you do. We're committed to fostering and advancing diversity and inclusion in our own workplace and beyond by ensuring every individual within our firm has a number of opportunities to grow professionally and personally, from our training and development opportunities and firmwide networks to benefits, wellness and personal finance offerings and mindfulness programs. Learn more about our culture, benefits, and people at GS.com/careers.
We’re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https://www.goldmansachs.com/careers/footer/disability-statement.html
© The Goldman Sachs Group, Inc., 2023. All rights reserved.
Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Veteran/Sexual Orientation/Gender Identity
Show more
Show less","Data Engineering, Software Engineering, Python, Hadoop, MapReduce, Spark, EMR, SQL, Kafka, Scala, Java, ETL, AWS, Google Cloud, Informatica Power Center, ApacheFlink","data engineering, software engineering, python, hadoop, mapreduce, spark, emr, sql, kafka, scala, java, etl, aws, google cloud, informatica power center, apacheflink","apacheflink, aws, data engineering, emr, etl, google cloud, hadoop, informatica power center, java, kafka, mapreduce, python, scala, software engineering, spark, sql"
Database Engineer - TS/SCI (Relo Assistance Available),CyberCoders,"Richardson, TX",https://www.linkedin.com/jobs/view/database-engineer-ts-sci-relo-assistance-available-at-cybercoders-3786097726,2023-12-17,McKinney,United States,Mid senior,Onsite,"If you are a Database Engineer with an active TS/SCI Security Clearance, please read on!
Job Title:
Database Engineer
Location:
Richardson, TX or Dulles, VA **Relocation assistance is available**
Salary:
$150,000 - $250,000 DOE + extensive benefits, PTO, 401(k) with a company match, and more
Requirements:
6+ years of experience as a Database Engineer, willing to obtain a CompTIA Security+ certification, and an active TS/SCI with Poly Security Clearance
This would be an amazing opportunity for you if you are looking for a role that will provide professional growth and new challenges yet balanced with great culture and quality of life!
Top Reasons to Work with Us
Certified 8a and verified Service-Disabled Veteran-Owned Small Business
Government service partner that delivers innovative and high-value solutions to unique customer problems & mission critical operations
Have the opportunity to work in a variety of different areas including Military Intelligence & Cyber Ops, Cyber Security, Enterprise IT Management, SaaS, Software Development, and System Engineering
Full-time employment with a long-term contract opportunity with a globally recognized company
What You Will Be Doing
Work with the whole database lifecycle
Support System Engineers, Software Engineers, Testers, and Operations staff
Remain proactive in troubleshooting database issues which includes performance tuning
Perform backup & recovery
Maintain database deployments
Installation, configuration, and maintenance of databases
Additional tasks/duties as assigned
What You Need for this Position
Required
Active TS/SCI Security Clearance
CompTIA Security+ Certification (or willing to obtain one within a month of joining the program)
Oracle/PostgreSQL
6+ years of experience working as a Database Engineer
History of working in a software development environment and following software processes (SCM/Etc.)
History of working with the whole database life cycle
Experience with backup, recovery, and database deployments
Strong Linux skills which includes scripting
Procedural SQL scripting skills (plsql/pgsql)
Preferred
MongoDB
Oracle Spatial
AWS RDS
Ansible
Nagios
What's In It for You
Competitive salary ($150,000 - $250,000 DOE)
Extensive benefits (medical, dental, vision)
Relocation assistance
PTO
Federal holidays and birthday PTO
401(k) with a company match
We are actively interviewing so APPLY TODAY!
Email Your Resume In Word To
Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:
Gage.Wolinski@CyberCoders.com
Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : GW1-1778712 -- in the email subject line for your application to be considered.***
Gage Wolinski - Recruiter - CyberCoders
Applicants must be authorized to work in the U.S.
CyberCoders is proud to be an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work
– In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.
Show more
Show less","Oracle, PostgreSQL, MongoDB, Oracle Spatial, AWS RDS, Ansible, Nagios, Linux, Scripting, SQL, PlSQL, PgSQL, Software Development, Troubleshooting, Performance Tuning, Backup, Recovery, Database Deployments, Installation, Configuration, Maintenance","oracle, postgresql, mongodb, oracle spatial, aws rds, ansible, nagios, linux, scripting, sql, plsql, pgsql, software development, troubleshooting, performance tuning, backup, recovery, database deployments, installation, configuration, maintenance","ansible, aws rds, backup, configuration, database deployments, installation, linux, maintenance, mongodb, nagios, oracle, oracle spatial, performance tuning, pgsql, plsql, postgresql, recovery, scripting, software development, sql, troubleshooting"
Senior Data Analyst,Brooksource,"Plano, TX",https://www.linkedin.com/jobs/view/senior-data-analyst-at-brooksource-3705699979,2023-12-17,McKinney,United States,Mid senior,Hybrid,"Data Analyst
Plano, TX
6+ Month CTH
As a Senior Data Analyst in Data and Analytics team, you will be a crucial member of our data engineering team, serving as the key domain expert overseeing our PepsiCo’s business processes. You will play a pivotal role in driving discussions around understanding Supply Chain, Financial, Consumer, Channel, and Category. This will involve close collaboration with business SME’s, the data science team, and the data engineering team. You will be responsible for developing an in-depth understanding of our business processes and translating business requirements into technical specifications for the data engineering team.
MINIMUM QUALIFICATIONS
Bachelor’s degree in Supply Chain and/or Operations Management
5+ years of experience with data analysis & data profiling in project, business requirements definition or data engineering in CPG or Manufacturing Industry
5+ years’ work experience in the areas of Distribution Network Analysis, Manufacturing, Production Network Optimization, Transportation, Demand Planning, or other areas related to Supply Chain or other domains such as Financial, Consumer, Channel, Category etc.
4+ years of strong Data Profiling experience & ability to identify trends and anomalies in the data to in-form data model build out
Experience in working with Datasets from POS Aggregators such as IRI, Nielsen, Kantar, Fetch, 8451, Luminate etc.
Experience working with structured/unstructured datasets, ability to clearly document and communicate requirement to technical team members
Experience with Business Intelligence tools, SQLTools
Experience leading projects working with large data sets, finding insights, and telling stories using data
RESPONSIBILITES:
Demonstrated high curiosity and ownership to learn and deep dive in problems
Demonstrated proficient in data mining principles: predictive analytics, mapping, collecting data from multiple data systems on premises and cloud-based data sources
Have knowledge on business process and SAP ERP Modules, such as, order to Cash, Account Payables, Finance (GL), Purchase Order, etc
Develop metrics, storyboards, and dashboards by gathering data from various sources to tell a story about the data
Analyze complex data to identify patterns, detect anomalies in data using statistical concepts and tools
Gather and analyze data pertaining to various business processes such as sales, forecasts, capital requirements, inventory, logistic, manufacturing and production capacity to develop supply chain models. geographics, POS, pricing and promotion, store profile, e-commerce data to develop channel models
Perform in-depth data analysis, including data cleansing, transformation, and validation, to ensure the accuracy and reliability of data used for reporting and analysis
Develop and apply statistical models, including logistic regression, linear regression, and other methods, to analyze data trends, identify patterns, and make predictions that inform business strategies and decisions
Hands on experience in data visualizations and dashboards using tools like OpenRefine, Tableau, Power BI, SAP Business Objects, Databricks, or other data visualization platforms to communicate insights effectively to stakeholders
Interpret and explain complex data sets to non-technical stakeholders, translating data findings into actionable recommendations
Collaborate with data engineering teams to ensure data quality and consistency, including identifying and resolving data anomalies or discrepancies
Work closely with cross-functional teams to uncover actionable insights and trends that drive business growth and customer satisfaction in the CPG industry
Adhere to data governance policies and best practices, ensuring compliance with data privacy and security regulations
Expertise with Database systems, SAP, SAP BW, SAP BO, RDBMS (Oracle, SQL Server, MySQL) with an excellent understanding of transaction management
Ability to learn and adapt to new technologies, passion for continuous improvement
Create high-level process models (system interface diagrams, workflow & swim lane diagrams, data flow diagrams) to represent processes for the area under analysis
Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.
Show more
Show less","Data Analysis, Data Profiling, Data Mining, Predictive Analytics, Mapping, Data Visualization, Data Cleansing, Data Transformation, Data Validation, Statistical Models, Logistic Regression, Linear Regression, Tableau, Power BI, SAP Business Objects, Databricks, OpenRefine, SAP, SAP BW, SAP BO, RDBMS, Oracle, SQL Server, MySQL, Transaction Management, Database Systems","data analysis, data profiling, data mining, predictive analytics, mapping, data visualization, data cleansing, data transformation, data validation, statistical models, logistic regression, linear regression, tableau, power bi, sap business objects, databricks, openrefine, sap, sap bw, sap bo, rdbms, oracle, sql server, mysql, transaction management, database systems","data mining, data profiling, data transformation, data validation, dataanalytics, database systems, databricks, datacleaning, linear regression, logistic regression, mapping, mysql, openrefine, oracle, powerbi, predictive analytics, rdbms, sap, sap bo, sap business objects, sap bw, sql server, statistical models, tableau, transaction management, visualization"
Staff Data Engineer,Recruiting from Scratch,"Colorado Springs, CO",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744392471,2023-12-17,Colorado Springs,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data management tools, Data classification","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, data warehouses, etl, data management tools, data classification","airflow, business intelligence, continuous integration, data classification, data engineering, data management tools, data science, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Staff Data Engineer,Recruiting from Scratch,"Colorado Springs, CO",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744398155,2023-12-17,Colorado Springs,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Dimensional data modeling, ETL pipelines, Kafka, Storm, SparkStreaming, Data Warehouses","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, dimensional data modeling, etl pipelines, kafka, storm, sparkstreaming, data warehouses","airflow, business intelligence, continuous integration, data engineering, data science, data warehouses, dimensional data modeling, docker, etl pipelines, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Colorado Springs, CO",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748833039,2023-12-17,Colorado Springs,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, ETL, Kafka, Storm, SparkStreaming, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Data Warehouses, Data Classification, Data Retention, Data Governance, Data Security, Data Scalability","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, etl, kafka, storm, sparkstreaming, sql, tdd, pair programming, continuous integration, automated testing, data warehouses, data classification, data retention, data governance, data security, data scalability","airflow, automated testing, continuous integration, data classification, data governance, data retention, data scalability, data security, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Colorado Springs, CO",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748832216,2023-12-17,Colorado Springs,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, SQL, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, ETL, Data Warehouses, Data Management, Data Classification, Data Retention","data engineering, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, sql, pair programming, continuous integration, kafka, storm, sparkstreaming, dimensional data modeling, schema design, etl, data warehouses, data management, data classification, data retention","airflow, continuous integration, data classification, data engineering, data management, data retention, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Colorado Springs, CO",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744397107,2023-12-17,Colorado Springs,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Science, Business Intelligence, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, Pyspark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data Compliance, Data Classification, Data Retention","data engineering, data science, business intelligence, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, data warehouses, etl, data compliance, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data compliance, data engineering, data retention, data science, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Colorado Springs, CO",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744390967,2023-12-17,Colorado Springs,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Kafka, Storm, Spark Streaming, Data Warehouses, ETL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Stream Processing, Dimensional Data Modeling, Schema Design","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, spark streaming, data warehouses, etl, tdd, pair programming, continuous integration, automated testing, deployment, stream processing, dimensional data modeling, schema design","airflow, automated testing, continuous integration, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, spark streaming, sql, storm, stream processing, tdd"
Data Engineer,"KBR, Inc.","Colorado Springs, CO",https://www.linkedin.com/jobs/view/data-engineer-at-kbr-inc-3770446183,2023-12-17,Colorado Springs,United States,Mid senior,Onsite,"Title
Data Engineer
The KBR team in Colorado Springs, CO is seeking a highly motivated Data Engineer to join us in supporting our great nation, specifically on the Air Force Tactical Exploitation of National Capabilities (TENCAP) HOPE contract.
Air Force TENCAP exploits National space, cyber and intelligence capabilities to deliver rapid, cost-dominant and innovative warfighting solutions across the full spectrum of Air Force and Joint military missions. AF TENCAP is a Congressionally-mandated rapid acquisition organization supporting tactical and operational warfighters, and serves as the Air Force executive agent for the Military Exploitation of Reconnaissance and Intelligence Technologies (MERIT) program. As the Air Force's leader for National-to-Tactical capability development, AF TENCAP collaborates with leading experts across the Intelligence Community, Department of Defense, National Laboratories, industry, inter-agency, academia, and partner nations.
We are looking for innovative, creative, and highly motivated individuals to support the strategic planning and tactical execution of Big Data Analytics. Specifically, the full lifecycle of data onboarding, ETL and analysis in support of wide ranging projects within AF TENCAP. The ideal candidate will bring a passion for learning, the drive to innovate and a humble attitude. You will work cutting edge projects, solving problems that most aren't even aware exist.
Work Location:
Colorado Springs, CO - 100% On Site
Security Clearance
Active TS/SCI or Active TS with T5/SSBI within last two years.
Required Experience And Education
BS degree in Computer Science, Mathematics, Statistics or Data Science. Other degrees will be considered with relevant work experience.
Required Skills
Strong understanding of data engineering principles, data modeling, and data architecture
Proficiency in designing and implementing data flows using Apache NiFi, including creating and configuring processors, connecting to databases, and integrating with other tools and technologies.
Experience with developing scalable ETL and ELT workflows for reporting and analytics
Experience with designing, developing, operationalizing, and maintaining complex data applications at enterprise scale
Experience with programming (scripting) languages (i.e. Bash, Python, .Net or Java)
Experience as a Data Engineer supporting single or multiple domain areas
Experience with Apache open-source products (i.e. Kafka, Zookeeper, Spark)
A drive to learn and master new technologies
Develop and implement methods of automation and optimization of data and products to present to upper management
Create data packages in the form of databases (DBs), reports, and interactive visualizations
Demonstrate effective communication skills to relay technical findings, and data products for both technical and non-technical customers
Ability to combine a diverse set of data sources containing multiple forms of information including, but not limited to, Open Source, Publicly Available Information (PAI), Commercially Available Information (CAI), and intelligence records to provide technical information to produce data packages
Demonstrated ability to use technical and analytic skills to solve complex problems
Experience with developing scripts and programs for converting various types of data into usable formats and supporting project team to scale, monitor, and operate data platforms
Desired Skills
DoD 8140 Cyber Workforce Basic Qualification (CompTIA Security+ CE or equivalent)
Understanding of real-time data streaming concepts and experience with stream processing frameworks like Apache Kafka Streams or Apache Flink
Experience with cloud platforms like AWS, Azure, or GCP, including knowledge of cloud-based data storage, compute, and analytics services
Experience working in an Agile development environment, using tools like Jira or Confluence, and following Agile methodologies like Scrum or Kanban
Understanding of Continuous Integration and Deployment (CI/CD) pipelines and experience with tools like Git, Jenkins, or Bamboo for version control, automated testing, and deployment
Working knowledge of computer networking concepts
Experience as a formal instructor, teacher or tutor
Experience in Space, ISR, Cyber or SOF
Experience leading projects w/ up to 5 team members
Linux System Administrator
Algorithm development experience
Software development experience (i.e. C++, Java, C)
Experience With IC, USSF, USAF Or Other DOD Entities
Experience working with any intelligence disciplines (i.e. SIGINT, OSINT, MASINT, GEOINT, HUMINT)
Experience with Kubernetes / Docker
Experience with Apache Accumulo
Experience with XML Schemas
Hands-on experience using commercially available data exploitation and visualization tools for analysis (e.g., MS Excel, Tableau, Power BI, Wireshark)
Knowledge of National Intelligence Agencies and Department of Defense elements
Experience in analytical tool development, identification, and integration
Basic Compensation: $120k - $179k
The offered rate will be based on the selected candidate’s knowledge, skills, abilities and/or experience and in consideration of internal parity.
Additional Compensation
KBR may offer bonuses, commissions, or other forms of compensation to certain job titles or levels, per internal policy or contractual designation. Additional compensation may be in the form of sign on bonus, relocation benefits, short term incentives, long term incentives, or discretionary payments for exceptional performance.
KBR is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, disability, sex, sexual orientation, gender identity or expression, age, national origin, veteran status, genetic information, union status and/or beliefs, or any other characteristic protected by federal, state, or local law.
R2063937
Show more
Show less","Apache NiFi, ETL, Apache Kafka, Zookeeper, Apache Spark, Bash, Python, .Net, Java, Databases, Data packages, Apache Kafka Streams, Apache Flink, AWS, Azure, GCP, Jira, Confluence, Scrum, Kanban, Git, Jenkins, Bamboo, Linux, C++, Kubernetes, Docker, Apache Accumulo, XML Schemas, MS Excel, Tableau, Power BI, Wireshark","apache nifi, etl, apache kafka, zookeeper, apache spark, bash, python, net, java, databases, data packages, apache kafka streams, apache flink, aws, azure, gcp, jira, confluence, scrum, kanban, git, jenkins, bamboo, linux, c, kubernetes, docker, apache accumulo, xml schemas, ms excel, tableau, power bi, wireshark","apache accumulo, apache flink, apache kafka, apache kafka streams, apache nifi, apache spark, aws, azure, bamboo, bash, c, confluence, data packages, databases, docker, etl, gcp, git, java, jenkins, jira, kanban, kubernetes, linux, ms excel, net, powerbi, python, scrum, tableau, wireshark, xml schemas, zookeeper"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Colorado Springs, CO",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709532,2023-12-17,Colorado Springs,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Pandas, Python, Java, bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming, Applied machine learning, Data management tools, Data classification, Retention","pandas, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, retention","airflow, applied machine learning, aws, azure, bash, data classification, data management tools, docker, dynamodb, gcp, git, helm, java, kafka, kubernetes, pandas, python, retention, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Colorado Springs, CO",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087728,2023-12-17,Colorado Springs,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Python, SQL, NoSQL, Airflow, Kubernetes, Docker, Spark, Hadoop, AWS, GCP, Azure, Kafka, Storm, SparkStreaming, Natural Language Processing","data engineering, machine learning, python, sql, nosql, airflow, kubernetes, docker, spark, hadoop, aws, gcp, azure, kafka, storm, sparkstreaming, natural language processing","airflow, aws, azure, data engineering, docker, gcp, hadoop, kafka, kubernetes, machine learning, natural language processing, nosql, python, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Colorado Springs, CO",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087714,2023-12-17,Colorado Springs,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Machine Learning, Data Engineering, Data Science, NLP, Data Mining, Data Visualization, Data Pipelines, Airflow, KubeFlow, Git, Python, Java, bash, SQL, NoSQL, DynamoDB, Snowflake, Kubernetes, Docker, Spark, pySpark, Kafka, Storm, SparkStreaming","machine learning, data engineering, data science, nlp, data mining, data visualization, data pipelines, airflow, kubeflow, git, python, java, bash, sql, nosql, dynamodb, snowflake, kubernetes, docker, spark, pyspark, kafka, storm, sparkstreaming","airflow, bash, data engineering, data mining, data science, datapipeline, docker, dynamodb, git, java, kafka, kubeflow, kubernetes, machine learning, nlp, nosql, python, snowflake, spark, sparkstreaming, sql, storm, visualization"
Senior Data Engineer,Kellton,"Charlotte, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kellton-3666789937,2023-12-17,Denver,United States,Mid senior,Onsite,"Job Title: Senior Data Engineer
Assignment Type: 12-month contract
Work Location: Charlotte, NC In office Tue/Wed/Thurs
Pay Rate: $70/HR on W2
Primary Responsibilities
This position will be focused on unattended retail solutions for clients and consumers across the US. The role will primarily be focused on providing expertise with development and management of our analytical data and reporting platforms.
The Senior Data Engineer Role Is Responsible For
Creates and supports the ETL in order to facilitate the accommodation of data into the warehouse. In this capacity,
Designs and develops systems for the maintenance of the business's data warehouse, ETL processes, and business intelligence.
Manage and elevate other engineers (both full-time, contractor, and/or third-party resources) while remaining hands-on
Maintain and build on our data warehouse and analytics environment utilizing Python, Snowflake, and AWS
Collaborate with the data services team and data architect to develop a strategy for long-term data platform architecture
Assist application teams with the collection of transactional and master data from source systems
Design and implement data movement and transformation pipelines (e.g., AWS Glue, Apache AirFlow, dbt, Snowflake)
Design processes and algorithms to enhance data quality and reliability
Provide availability and access to consume analytical data through a wide range of Business Intelligence and Reporting toolsets (e.g., Microsoft Power BI, Google Looker)
Implement proactive monitoring and alerting to ensure operational stability and supportability
Collaborate with data analysts, data scientists, security engineers, and architects to achieve the best possible technical solutions that address our business needs
Performs analysis and critical thinking required to troubleshoot data-related issues and assist in the resolution
Coordinate change and release management across technical teams inside and outside the department
Minimize operational impact while achieving the best possible performance and system health.
Assist with production issues in Data Warehouses like reloading data and transformations
Qualifications
Applicants must have a proven work experience in a data-related role and the following qualifications:
Minimum of 1 year of experience implementing a full-scale data warehouse solution using Snowflake Cloud.
Expertise and excellent proficiency with Snowflake internals and integration of Snowflake with other technologies for data processing and reporting.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
5+ years in Data Warehousing, Big Data, or ETL development (preferably using AWS S3, Glue, Apache AirFlow)
5+ years of experience with a programming language (preferably Python)
Experience designing, building, and maintaining data processing and transformation pipelines (AWS Glue, Apache AirFlow, dbt, Snowflake)
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
Technical expertise in data models, data mining, and data segmentation techniques
Knowledge of CI/CD deployment practices
Strong skills with Python and SQL with the ability to write efficient queries
Excellent analytical and organizational skills
Excellent critical thinking and problem-solving skills
Proven success in communicating with colleagues, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
The ability to work independently and in a team environment
Special Skills And/or Training Desired
Experience working in an Agile / Scrum environment
Strong experience with AWS data services (e.g., S3, DynamoDB, Glue, Athena, Redshift, Lambda, SQS, SNS, and API Gateway). Experience with AWS DMS a-plus
AWS Solutions Architect Associate or AWS Developer Certification (desired)
Strong experience with the Snowflake cloud data platform
SnowPro certification (preferred)
Strong experience with a programming language (preferably Python)
Experience With Dbt Data Transformation Framework Preferred
Experience utilizing Apache AirFlow to manage data pipelines and workflows
Experience with Microsoft Azure DevOps for work item management and CI/CD deployment pipelines
Show more
Show less","Snowflake, AWS, Python, SQL, Data Warehousing, ETL, Machine Learning, Data Mining, Data Segmentation, Big Data, Data Analysis, Data Science, Data Quality, Relational Databases, AWS S3, AWS Glue, Apache AirFlow, dbt, CI/CD, Agile, Scrum, AWS DMS, AWS Solutions Architect Associate, AWS Developer Certification, SnowPro certification","snowflake, aws, python, sql, data warehousing, etl, machine learning, data mining, data segmentation, big data, data analysis, data science, data quality, relational databases, aws s3, aws glue, apache airflow, dbt, cicd, agile, scrum, aws dms, aws solutions architect associate, aws developer certification, snowpro certification","agile, apache airflow, aws, aws developer certification, aws dms, aws glue, aws s3, aws solutions architect associate, big data, cicd, data mining, data quality, data science, data segmentation, dataanalytics, datawarehouse, dbt, etl, machine learning, python, relational databases, scrum, snowflake, snowpro certification, sql"
Big data developer,Lorven Technologies Inc.,"Charlotte, NC",https://www.linkedin.com/jobs/view/big-data-developer-at-lorven-technologies-inc-3737118585,2023-12-17,Denver,United States,Mid senior,Onsite,"Experience with Big Data, Hadoop, Hortonworks Data Platform/CDP, Hortonworks Data Flow, and YARN – 7 Yrs
Experience working in Linux environments – 7 Yrs
Knowledge of industry standard Incident, Problem, Release, and Change Management processes
Experience troubleshooting issues related to Hadoop Ecosystem components like Hive, Spark, Hbase
Experience installing and maintaining Python Libraries
Experience with environment and infrastructure integration
Experience with Ezmeral Data Fabric cluster administration would be additional advantage.
Capacity management experience
Show more
Show less","Big Data, Hadoop, Hortonworks Data Platform/CDP, Hortonworks Data Flow, YARN, Linux, Incident Management, Problem Management, Release Management, Change Management, Hive, Spark, Hbase, Python Libraries, Ezmeral Data Fabric, Capacity Management","big data, hadoop, hortonworks data platformcdp, hortonworks data flow, yarn, linux, incident management, problem management, release management, change management, hive, spark, hbase, python libraries, ezmeral data fabric, capacity management","big data, capacity management, change management, ezmeral data fabric, hadoop, hbase, hive, hortonworks data flow, hortonworks data platformcdp, incident management, linux, problem management, python libraries, release management, spark, yarn"
"Python Data Engineer with Ansible / Charlotte, NC",Lorven Technologies Inc.,"Charlotte, NC",https://www.linkedin.com/jobs/view/python-data-engineer-with-ansible-charlotte-nc-at-lorven-technologies-inc-3747468644,2023-12-17,Denver,United States,Mid senior,Onsite,"Job Description
Bachelor’s degree or equivalent working experience with minimum of 9+ years of IT experience.
Required Skills
Develop functional and technical specifications from business requirements for Platform automation
5+ years’ experience working with Ansible.
5+ years’ experience working on Python Development.
2+ years of Data Engineering Experience.
Strong knowledge on Unix Shell scripting and Data Engineering best practices.
Ability to develop Python Script to integrate with Service Now or Ansible using APIs.
Connect to various databases through Python to load data & generate reports, based on Client/Business requirement.
Ability to manage and create Ansible playbooks for platform automation.
Show more
Show less","Python Development, Data Engineering, Unix Shell scripting, Ansible, Service Now, SQL, Ansible playbooks","python development, data engineering, unix shell scripting, ansible, service now, sql, ansible playbooks","ansible, ansible playbooks, data engineering, python development, service now, sql, unix shell scripting"
Data Developer - Senior,Atrium Health,"Charlotte, NC",https://www.linkedin.com/jobs/view/data-developer-senior-at-atrium-health-3759848150,2023-12-17,Denver,United States,Mid senior,Onsite,"Overview
Job Summary
Data Developer – Senior is a role in the Center for Health System Sciences (CHASSIS) at Atrium Health. The role is responsible for providing custom data solutions for a wide range of medical research projects. Eligible candidates must be able to translate requirements into an actionable data management plan. This includes, but is not limited to designing, coding, debugging, testing, deploying code/processes, and communicating results. Regular interaction with clients. Candidates will be successful in this role if they become a subject matter expert with internal data warehouses, are proficient in SQL to source and manipulate data, have a customer centric mindset, proactively mitigate and addresses conflict, and communicate results to clients. In addition, this position contributes to teammate development and training. The Data Developer- Senior develops and administers data management processes; query and reporting tools, and analytical applications. Analyzes data systems and decision support business requirements and associated source systems to design and develop the application architecture and data management architecture. Handles the detailed design specification development, coding, testing, debugging, and documentation associated with implementing and maintaining the analytical applications and data management processes.
Essential Functions
With limited direction, customizes data solutions and offers insights for medical research projects, translating research requirements into an actionable data management plan and successfully executing that plan, including all relevant documentation; collaborates with customers to formulate scope/objectives. Identifies unique solutions to problems within the project's scope; demonstrates exceptional customer service/communications skills.
Demonstrates subject matter expertise related to two or more internal data sources, external data sources, data analysis tools and/or the healthcare industry. Is regularly sought out for their expertise by teammates.
Leads process improvement initiatives to successful completion, collaborating with immediate teammates. (e.g. improve data accuracy/availability, streamline workflow, institute best practices, etc.).
Contributes to ongoing projects and team initiatives, supporting teammates through mentoring, trouble shooting, and sharing knowledge of healthcare, standards, and regulations. Provides input for performance evaluation on teammates.
Develops work plans, estimated tasks, and provides budget input for items related to the development of data management processes and analytical applications. Provides input into determining project scope and is able to communicate and manage project scope.
Identifies, defines, documents, analyzes, and obtains customer sign-off on data systems/decision support business requirements. Analyzes these requirements to establish the application architecture.
Establishes the data management architecture through an analysis of business requirements source system data, and source system application architectures. Confers with peers to evaluate and approve technical architecture and to define the optimal location and configuration of extract, load, and transformation, routines.
Coordinates development of extract routines with source system team leads. Identifies and coordinates timeframes for test file and production file delivery.
Plans and manages system tests, customer acceptance tests, and performance tests efforts. Establishes testing procedures. Coordinates the execution of such tests and the resolution of any issues, escalating to management when appropriate.
Physical Requirements
Works under normal office conditions; may include sitting for long periods of time, standing, walking, climbing stairs, using repetitive wrists/arms motions of lifting articles up to 35 lbs. Must travel in personal care to other facilities occasionally. This position requires overtime occasionally.
Education, Experience And Certifications
High School Diploma or GED required; Bachelor's Degree preferred. 4 years in healthcare planning, operations, or managed care is strongly preferred. 4 years of experience designing, implementing, and supporting data systems/decision support systems strongly preferred. High proficiency using computer software tools and working with databases required. Expertise in Epic is strongly preferred. Strong skills in design and development of SQL Server, SSIS, SSAS, relational database design, star schema design/dimensional modeling, Business Objects, and MS Windows. Familiarity with data visualization design principles and with database performance tuning concepts preferred. Strong business perspective and facilitation skills. Thorough knowledge or ability to rapidly acquire such knowledge of the operations, functions, and procedures. Strong communication skills necessary to be effective communicating at all levels of the organization.
Show more
Show less","SQL, SSIS, SSAS, Relational database design, Star schema design, Dimensional modeling, Business Objects, MS Windows, Data visualization, Healthcare, Data analysis, Epic, SQL Server","sql, ssis, ssas, relational database design, star schema design, dimensional modeling, business objects, ms windows, data visualization, healthcare, data analysis, epic, sql server","business objects, dataanalytics, dimensional modeling, epic, healthcare, ms windows, relational database design, sql, sql server, ssas, ssis, star schema design, visualization"
Senior Data Engineer (Python / AWS),Jobot,"Charlotte, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-python-aws-at-jobot-3785312564,2023-12-17,Denver,United States,Mid senior,Onsite,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!
Job details
Fast growing AI powered Fintech startup / Python + AWS & Building Data Pipelines!!
This Jobot Job is hosted by Craig Rosecrans
Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.
Salary $120,000 - $180,000 per year
A Bit About Us
We are on the hunt for a highly skilled and passionate Senior Data Engineer to join our dynamic technology team. This is a permanent, full-time position where you will have the opportunity to shape the future of Fintech by developing, maintaining and testing our cutting-edge data systems. You will be working with a team of talented engineers and data scientists to design scalable and efficient data pipelines, create data architectures, and ensure all system designs and implementations adhere to the latest data management principles and security standards.
Why join us?
Competitive Base Salary
Equity in high-growth start-up (not in lieu of a salary)
Flexible Hours
Very generous PTO
Dental and Vision, FSA, HSA
Small team, autonomy
Many more great perks!
Job Details
Responsibilities
Design, construct, install, test and maintain highly scalable data management systems.
Collaborate with data scientists and architects on several projects.
Develop and maintain scripts and queries to import, clean, transform, and augment data.
Develop data set processes for data modeling, mining and production.
Employ an array of technological languages and tools to connect systems together.
Recommend ways to improve data reliability, efficiency, and quality.
Collaborate with data architects to visualize data in a manner that is accessible to data scientists and analysts.
Use AWS cloud services to build services and data stores.
Ensure all solutions are aligned with the cloud design principles and strategy.
Work with the team to evaluate new and emerging technologies and developmental methodologies to improve performance and feasibility of future projects.
Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical discipline.
Minimum of 5+ years of experience in a Data Engineer role, preferably in a Fintech or startup environment.
Proven experience with the following or equivalent skillset Python, AWS, AWS Aurora, Dynamo DB, Athena.
Solid experience in data architecture, data modeling, master data management, data staging, ETL processes, and business intelligence.
Proficient understanding of distributed computing principles.
Experience with building stream-processing systems and big data technologies.
Strong problem-solving skills with an emphasis on product development.
Excellent written and verbal communication skills.
Ability to work in a fast-paced environment and manage multiple tasks simultaneously.
Strong team player with the ability to work independently when required.
Join us and be a part of a team that values innovation, growth, and collaboration. If you are passionate about data and eager to make a significant impact in the Fintech industry, we would love to hear from you.
Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.
Want to learn more about this role and Jobot?
Click our Jobot logo and follow our LinkedIn page!
Show more
Show less","Python, AWS, AWS Aurora, Dynamo DB, Athena, Data architecture, Data modeling, Master data management, Data staging, ETL processes, Business intelligence, Distributed computing, Streamprocessing systems, Big data technologies, Problemsolving, Product development, Communication skills, Fastpaced environment, Multiple tasks, Team player, Independent work","python, aws, aws aurora, dynamo db, athena, data architecture, data modeling, master data management, data staging, etl processes, business intelligence, distributed computing, streamprocessing systems, big data technologies, problemsolving, product development, communication skills, fastpaced environment, multiple tasks, team player, independent work","athena, aws, aws aurora, big data technologies, business intelligence, communication skills, data architecture, data staging, datamodeling, distributed computing, dynamo db, etl, fastpaced environment, independent work, master data management, multiple tasks, problemsolving, product development, python, streamprocessing systems, team player"
Senior Big Data Engineer,Agile Tech Labs,"Charlotte, NC",https://www.linkedin.com/jobs/view/senior-big-data-engineer-at-agile-tech-labs-3587208204,2023-12-17,Denver,United States,Mid senior,Onsite,"Job Title: Senior Big Data Engineer
Job Location: Charlotte, NC (HYBRID)
Job Type: Full Time
Job Description/ Requirement
5+ years of Experience utilizing SQL
5+ years of Experience in UNIX & Shell Scripting
5+ years of Experience using Hadoop Technologies
5+ years of Experience in design, development, testing and implementation of UI and Data based applications
5+ years of Experience utilizing Agile Development Methodology
Ability to think out of box and provide innovative solutions
Strong analytical and problem-solving skills
Good verbal and written communication skills
Bachelor's Degree in Computer Science or job related field or equivalent experience
Interested candidates can send their updated resumes at anupam.pal@agile-techlabs.com
Show more
Show less","SQL, Unix, Shell scripting, Hadoop, UI development, Databased applications, Agile development methodology, Innovative solutions, Analytical skills, Problemsolving skills, Communication skills, Computer science","sql, unix, shell scripting, hadoop, ui development, databased applications, agile development methodology, innovative solutions, analytical skills, problemsolving skills, communication skills, computer science","agile development methodology, analytical skills, communication skills, computer science, databased applications, hadoop, innovative solutions, problemsolving skills, shell scripting, sql, ui development, unix"
Lead Data Engineer,Vanguard,"Charlotte, NC",https://www.linkedin.com/jobs/view/lead-data-engineer-at-vanguard-3783802764,2023-12-17,Denver,United States,Mid senior,Onsite,"Provides expert level data solutions by using software to process, store, and serve data to others. Tests data quality and optimizes data availability. Ensures that data pipelines are scalable, repeatable, and secure. Utilizes the deepest dive analytical skillset on a variety of internal and external data. Leads, instructs, and mentors newer Data Engineering crew.
The VISTA Modernization Program is in need of a Data Expert to select a cloud based database solution to replace DB2 and VSAM. After selecting the technology, this expert will provide guidance to delivery teams as we modernize IIG's Recordkeeping System to a modern solution in AWS.
Core Responsibilities
Writes ETL (Extract / Transform / Load) processes, designs database systems and, develops tools for real-time and offline analytic processing.
Troubleshoots software and processes for data consistency and integrity. Integrates complex and large scale data from a variety of sources for business partners to generate insight and make decisions.
Translates business specifications into design specifications and code. Responsible for writing complex programs, ad hoc queries, and reports. Ensures that all code is well structured, includes sufficient documentation, and is easy to maintain and reuse.
Partners with internal clients to gain an expert understanding of business functions and informational needs. Works closely with other technical and data analytics experts across the business to implement data solutions.
Leads all phases of solution development. Explains technical considerations at related meetings, including those with internal clients and less experienced team members.
Assesses data quality and tests code thoroughly for accuracy of intended purpose. Provides data analysis guidance and serves as a technical consultant for the client.
Educates and develops junior data engineers on the team while applying quality control to their work. Develops data engineering standards and contributes expertise to other data expert teams across Vanguard.
Tests and implements new software releases through regression testing. Identifies issues and engages with vendors to resolve and elevate software into production.
Participates in special projects and performs other duties as assigned.
Qualifications
Minimum of eight years data analytics, programming, database administration, or data management experience.
Undergraduate degree or equivalent combination of training and experience. Graduate degree preferred.
Special Factors
Sponsorship
Vanguard is not offering visa sponsorship for this position.
About Vanguard
We are Vanguard. Together, we’re changing the way the world invests.
For us, investing doesn’t just end in value. It starts with values. Because when you invest with courage, when you invest with clarity, and when you invest with care, you can get so much more in return. We invest with purpose – and that’s how we’ve become a global market leader. Here, we grow by doing the right thing for the people we serve. And so can you.
We want to make success accessible to everyone. This is our opportunity. Let’s make it count.
Inclusion Statement
Vanguard’s continued commitment to diversity and inclusion is firmly rooted in our culture. Every decision we make to best serve our clients, crew (internally employees are referred to as crew), and communities is guided by one simple statement: “Do the right thing.”
We believe that a critical aspect of doing the right thing requires building diverse, inclusive, and highly effective teams of individuals who are as unique as the clients they serve. We empower our crew to contribute their distinct strengths to achieving Vanguard’s core purpose through our values.
When all crew members feel valued and included, our ability to collaborate and innovate is amplified, and we are united in delivering on Vanguard's core purpose.
Our core purpose: To take a stand for all investors, to treat them fairly, and to give them the best chance for investment success.
How We Work
Vanguard has implemented a hybrid working model for the majority of our crew members, designed to capture the benefits of enhanced flexibility while enabling in-person learning, collaboration, and connection. We believe our mission-driven and highly collaborative culture is a critical enabler to support long-term client outcomes and enrich the employee experience.
Show more
Show less","Data solutions, Data processing, Data storage, Data serving, Data quality testing, Data availability optimization, Data pipeline scalability, Data pipeline repeatability, Data pipeline security, Analytical skillset, Data engineering, ETL processes, Database systems design, Realtime analytic processing, Offline analytic processing, Software troubleshooting, Data consistency, Data integrity, Data integration, Data analysis, Business specifications, Design specifications, Code writing, Structured code, Code documentation, Code maintainability, Code reusability, Business functions, Informational needs, Solution development, Technical considerations, Data quality assessment, Code testing, Data analysis guidance, Technical consulting, Junior data engineer education, Data engineering standards, Data expert teams, Software regression testing, Software release testing, Software issue identification, Software vendor engagement, Special projects, Data analytics, Programming, Database administration, Data management, Undergraduate degree, Graduate degree","data solutions, data processing, data storage, data serving, data quality testing, data availability optimization, data pipeline scalability, data pipeline repeatability, data pipeline security, analytical skillset, data engineering, etl processes, database systems design, realtime analytic processing, offline analytic processing, software troubleshooting, data consistency, data integrity, data integration, data analysis, business specifications, design specifications, code writing, structured code, code documentation, code maintainability, code reusability, business functions, informational needs, solution development, technical considerations, data quality assessment, code testing, data analysis guidance, technical consulting, junior data engineer education, data engineering standards, data expert teams, software regression testing, software release testing, software issue identification, software vendor engagement, special projects, data analytics, programming, database administration, data management, undergraduate degree, graduate degree","analytical skillset, business functions, business specifications, code documentation, code maintainability, code reusability, code testing, code writing, data analysis guidance, data availability optimization, data consistency, data engineering, data engineering standards, data expert teams, data integration, data integrity, data management, data pipeline repeatability, data pipeline scalability, data pipeline security, data processing, data quality assessment, data quality testing, data serving, data solutions, data storage, dataanalytics, database administration, database systems design, design specifications, etl, graduate degree, informational needs, junior data engineer education, offline analytic processing, programming, realtime analytic processing, software issue identification, software regression testing, software release testing, software troubleshooting, software vendor engagement, solution development, special projects, structured code, technical considerations, technical consulting, undergraduate degree"
Data Analyst,Lorven Technologies Inc.,"Charlotte, NC",https://www.linkedin.com/jobs/view/data-analyst-at-lorven-technologies-inc-3688711034,2023-12-17,Denver,United States,Mid senior,Onsite,"ROLE: Data Analyst Python - Fraud & Claims
LOCATION: Charlotte
5+ Years using scripting language such as python, javascript, or shell scripting to process, clean, and prepare data
3+ years with ingesting and extracting data from large data store
2+ years with SQL
4+ years of Data Engineering experience
Desired Qualifications
1+ Year working with a graph database such as Teradata, neo4J, ArangoDB, Tigergraph
Experience working with Apache Spark
Experience with Elasticsearch""
Show more
Show less","Python, Javascript, Shell scripting, Data ingestion, Data extraction, SQL, Data engineering, Graph database, Teradata, Neo4J, ArangoDB, Tigergraph, Apache Spark, Elasticsearch","python, javascript, shell scripting, data ingestion, data extraction, sql, data engineering, graph database, teradata, neo4j, arangodb, tigergraph, apache spark, elasticsearch","apache spark, arangodb, data engineering, data extraction, data ingestion, elasticsearch, graph database, javascript, neo4j, python, shell scripting, sql, teradata, tigergraph"
Sr. Data Engineer- Customer Analytics (Hybrid),Selective Insurance,"Charlotte, NC",https://www.linkedin.com/jobs/view/sr-data-engineer-customer-analytics-hybrid-at-selective-insurance-3747073700,2023-12-17,Denver,United States,Mid senior,Onsite,"At Selective, we don't just insure uniquely, we employ uniqueness.
Our Business
Selective is a midsized U.S. domestic property and casualty insurance company with a history of strong, consistent financial performance for nearly 100 years. Selective's unique position as both a leading insurance group and an employer of choice is recognized in a wide variety of awards and honors, including listing in Forbes Best Midsize Employers in 2023 and certification as a Great Place to Work® in 2023.
Working at Selective
At Selective, we don't just insure uniquely – we employ uniqueness. Employees are empowered and encouraged to
Be Uniquely You
by being their true, unique selves and contributing their diverse talents, experiences, and perspectives to our shared success. Together, we are a high-performing team working to serve our customers responsibly by helping to mitigate loss, keep them safe, and restore their lives and businesses after an insured loss occurs. Employees receive comprehensive total rewards packages - including competitive compensation and performance awards, health benefits, and retirement savings - and professional development opportunities and flexible schedules to support their health, wealth, and well-being. Join our team and help make a difference.
Overview
Selective Insurance is seeking an energetic and collaborative Sr. Data Engineer to work on data and analytics projects supporting the Customer Analytics team within the Information Management group. This group is responsible for technology support of all Data Engineering, Analytics and Reporting for the Marketing, Customer Experience and Contact Center business areas. This includes Data Engineering services, Enterprise reporting support and ML Ops Engineering operations for these groups. The candidate must be hands on with very good technical skills and a proven track record of project delivery.
Responsibilities
Hands on development and support of new or existing data applications.
Work closely with business and analysts to understand data and business process and make recommendations to clients as requested on best practices or long-term solutions to resolve current issues and also for future system design
Works closely with Application and Enterprise Architects to create/review low level implementation designs, understand high level data flow designs developed by data architects.
Provide technical guidance to the team for implementing complex data solutions.
Provide support in the design, development, code reviews, test deploy and documentation of data engineering and data integration Applications.
Maintain detailed documentation to support downstream integrations
Provide support for production issues
Performs activities of a scrum master
Identify technology trends and explore opportunities for use within the organization
Qualifications
Five to seven years of experience in Data Warehousing, Data integration or Data Engineering projects
Ability to effectively work well with people in other departments and/or outside of the enterprise.
Proficient in SQL.
Experience working within Azure ecosystem
Experience in Informatica Powercenter, IICS, Cognos, Netezza Performance servers
Experienced in any of these analytical platforms - PowerBI, AzureML, Databricks or Synapse
Experience using Python or Scala.
Experience in Azure DevOps and Github is preferred
P&C Insurance experience is preferred
Possesses excellent communication skills.
Bachelor’s degree in computer science or related engineering field preferred.
Salary range: $90,600 - $149,600. The actual base salary is based on geographic location, and the range is representative of salaries for this role throughout Selective's footprint. Additional considerations include the candidate's qualifications and experience.
Selective is an Equal Employment Opportunity employer. That means we respect and value every individual’s unique opinions, beliefs, abilities, and perspectives. We are committed to promoting a welcoming culture that celebrates diverse talent, individual identity, different points of view and experiences – and empowers employees to contribute new ideas that support our continued and growing success. Building a highly engaged team is one of our core strategic imperatives, which we believe is enhanced by diversity, equity, and inclusion. We expect and encourage all employees and all of our business partners to embrace, practice, and monitor the attitudes, values, and goals of acceptance; address biases; and foster diversity of viewpoints and opinions.
Selective maintains a drug-free workplace
.
Show more
Show less","Azure ML, Power BI, Azure DevOps, GitHub, SQL, Informatica Powercenter, IICS, Cognos, Netezza Performance servers, Databricks, Synapse, Python, Scala, Data Warehousing, Data integration, Data Engineering, Scrum master","azure ml, power bi, azure devops, github, sql, informatica powercenter, iics, cognos, netezza performance servers, databricks, synapse, python, scala, data warehousing, data integration, data engineering, scrum master","azure devops, azure ml, cognos, data engineering, data integration, databricks, datawarehouse, github, iics, informatica powercenter, netezza performance servers, powerbi, python, scala, scrum master, sql, synapse"
Lead ETL Data Analyst,"Anveta, Inc","Charlotte, NC",https://www.linkedin.com/jobs/view/lead-etl-data-analyst-at-anveta-inc-3681062764,2023-12-17,Denver,United States,Mid senior,Onsite,"Please read the below mentioned detail carefully before applying for this opportunity
:
W2 job opportunity
Please note
:
Resources local to the below mentioned locations are only requested to apply
.
Position: Lead ETL Data Analyst
Experience
: 9+ Years
Location: Charlotte, NC / MN / AZ (3 days Hybrid & Wednesday in office is mandatory)
Duration: 12 months on W2 basis
Must Have
9+ years of exp. This role is technical and requires high level of expertise with ETL Data Analysis.
Prior ETL development exp is highly preferred.
Advance SQL skills will be required.
Informatica
Data warehousing experience a MUST. Exp with Data integration
Source to Target Mapping
Creating Data Models ( Strong exp needed )
Adv Excel
Preferable but not Mandatory
Exp with BI tools
Prior HR exp
If you or anyone in your network are interested, please send the relevant resume to
:
abhishek@anveta.com
OR
asad@anveta.com
.
Please include few times when you would be able to speak via Video Call!
Thanks!
Show more
Show less","ETL, Data Analysis, SQL, Informatica, Data Warehousing, Data Integration, Data Modeling, Excel, BI Tools","etl, data analysis, sql, informatica, data warehousing, data integration, data modeling, excel, bi tools","bi tools, data integration, dataanalytics, datamodeling, datawarehouse, etl, excel, informatica, sql"
Lead Data Engineer,TechFetch.com - On Demand Tech Workforce hiring platform,"Charlotte, NC",https://www.linkedin.com/jobs/view/lead-data-engineer-at-techfetch-com-on-demand-tech-workforce-hiring-platform-3785973015,2023-12-17,Denver,United States,Mid senior,Onsite,"""ALL our jobs are US based and candidates must be in the US with valid US Work Authorization. Please apply on our website directly."" Job Title: Lead Data Engineer)
Location: CHARLOTTE, NC
Employment Type: (Full-time or contract)
Duration: 12 + Months
About VLink: Started in 2006 and headquartered in Connecticut, VLink is one of the fastest growing digital technology services and consulting companies. Since its inception, our innovative team members have been solving the most complex business, and IT challenges of our global clients.
Job Description
""Job Description (Lead Data Engineer) ? Data Masking project ? You have 6-8 years of relevant software development experience ? You have hands-on experience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical ? Highly analytical and data oriented ? Experience in SQL, NoSql Database ? Data masking of on prem PII data. ? Develop API calls with using secure data transfer ? Take standard output data to lower environments for pre prod testing ? Enable secured channels for data models and data science activities ? Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15 mins ? You have experience with development tools and agile methodologies Skill set: Java/Scala/Python, Spark, S3, Glue, Redshift Location - Scottsdale ""
Employment Practices: EEO, ADA, FMLA Compliant VLink is an equal opportunity employer. At VLink, we are committed to embracing diversity, multiculturalism, and inclusion. VLink does not discriminate on the basis of race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. All aspects of employment including the decision to hire, promote, or discharge, will be decided on the basis of qualifications, merit, performance, and business needs.
Show more
Show less","Java, Scala, Python, Spark, S3, Glue, Redshift, SQL, NoSql Database, Data Masking, API Calls, Secure Data Transfer, Agile Methodologies, Development Tools","java, scala, python, spark, s3, glue, redshift, sql, nosql database, data masking, api calls, secure data transfer, agile methodologies, development tools","agile methodologies, api calls, data masking, development tools, glue, java, nosql database, python, redshift, s3, scala, secure data transfer, spark, sql"
Sr Logistics Data Engineer,Trane Technologies,"Davidson, NC",https://www.linkedin.com/jobs/view/sr-logistics-data-engineer-at-trane-technologies-3774889169,2023-12-17,Denver,United States,Mid senior,Onsite,"At Trane TechnologiesTM and through our businesses including Trane® and Thermo King®, we create innovative climate solutions for buildings, homes, and transportation that challenge what’s possible for a sustainable world. We're a team that dares to look at the world's challenges and see impactful possibilities. We believe in a better future when we uplift others and enable our people to thrive at work and at home. We boldly go.
Job Summary
The Data Engineer will be responsible for developing a data ecosystem to incorporate supply chain related data into a unified and integrated data model. This model will serve as a foundation to all analytical efforts within the company’s Supply Chain function, enabling them to make higher quality decisions faster and with a higher degree of confidence. This position will be seen as vital to the execution of our vision and design of the data model from processes that ingest and process incoming data, enhance data products with analytical insights, and enforce standards and quality measurements to ensure that everything is working properly.
Work Arrangement: Hybrid
work schedule (3 or more days onsite a week) and will be based out of our Davidson, NC location.
Responsibilities
Developing large-scale data pipelines to ingest, move, transform, and integrate data to help generate insights and meet reporting needs
Design and build the end-to-end solution to move data from various sources into a unified cloud-based data hub
Liberate inaccessible data by partnering with process owners to capture and integrate information currently stored in closed-loop systems
Document requirements and translate into system design specifications
Educate functional analysts on proper methods of extracting and using data
Execute and coordinate requirements management and change management processes
Documentation of each of the created data products
Qualifications
Bachelors degree
3-5 years of experience
Understanding of database design principles
Experience writing SQL is required
Hands-on experience using Google BigQuery and Data Fusion a plus
Highly self‐motivated, self‐directed, and attentive to detail
Ability to effectively prioritize and execute tasks.
Strong project management and communication skills
Prior experience with ERPs or TMS software is preferred
Supply chain knowledge preferred
Optimize database run times
Support existing data pipelines
Monitor and troubleshoot data pipeline issues
Key Competencies and Success Factors
Project Management: Partners and collaborates with cross-functional leaders to align objectives, set project KPIs, identify gaps and solutions and oversee overall project implementation.
Problem Solving: Identifies/anticipates potential problems, performs root cause analysis, evaluates robustness of corrective action plan, and oversees plan implementation
Communication: Must be able to communicate in a clear, concise, and persuasive manner
Attention to Detail: Follows all standard work thoroughly and ensures that all standards are met
Dealing with Ambiguity: Able to fill in gaps in understanding and requirements and work with customers to improve clarity
Innate Curiosity – constant drive to learn more, ask questions, seek better outcomes
What’s In It For You
Benefits kick in day one!
6% 401K match, plus an additional 2% of eligible pay in core contributions
3 weeks of vacation per calendar year, plus paid holidays
Benefits*: https://www.tranetechnologies.com/en/index/careers/benefits.html
Base Pay Range: $100,000 - $135,000 / year and will include a bonus plan
Disclaimer: This base pay range is based on US national averages. Actual base pay could be a result of seniority, merit, geographic location where the work is performed.
We offer competitive compensation and comprehensive benefits and programs. We are an equal opportunity employer; all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, pregnancy, age, marital status, disability, status as a protected veteran, or any legally protected status.
2309131
Show more
Show less","Data Engineering, Data Pipelines, Data Integration, Data Modeling, Cloud Computing, Google BigQuery, Google Data Fusion, SQL, ERPs, TMS, Supply Chain Management, Project Management, Problem Solving, Communication, Attention to Detail, Curiosity, Database Optimization, Data Troubleshooting","data engineering, data pipelines, data integration, data modeling, cloud computing, google bigquery, google data fusion, sql, erps, tms, supply chain management, project management, problem solving, communication, attention to detail, curiosity, database optimization, data troubleshooting","attention to detail, cloud computing, communication, curiosity, data engineering, data integration, data troubleshooting, database optimization, datamodeling, datapipeline, erps, google bigquery, google data fusion, problem solving, project management, sql, supply chain management, tms"
Senior Data Analyst,Union (Joins Valtech),"Charlotte, NC",https://www.linkedin.com/jobs/view/senior-data-analyst-at-union-joins-valtech-3646120637,2023-12-17,Denver,United States,Mid senior,Onsite,"We’ve worked hard to establish an open, collaborative and creative agency environment over the past decade, and we are continuing to grow. If you think you’re a great candidate, let us know.
Over the past decade at Union, we’ve worked hard to establish an open, collaborative, creative agency environment. We continue to grow and change with the needs of our clients to include our most recent evolution, joining Valtech, a global business transformation company. Our creative & performance marketing expertise, combined with Valtech’s market reach & resources, will accelerate Valtech’s expansion of marketing services in North America. This new partnership will take our existing boutique shop to a global scale. Together, we will transform how our clients build relationships with their customers by providing end-to-end technology and marketing services that support the entire customer experience journey. We’re thrilled for this next chapter under a shared vision with our new Valtech colleagues & clients around the world.
We're looking for a Senior Data Analyst with proven experience in tracking implementation, A/B testing, audience targeting, performance marketing measurement, reporting and data analysis implementing tracking, and finding opportunities for improvement in marketing initiatives.
If you’re looking to create best in class, integrated campaigns at a boutique creative agency backed by the power of a global network, drop us a line.
Requirements
Marketing Analytics Skills
Strong communicator with excellent verbal, written and presentation skills for presenting data in a digestible way
Experience working with digital paid & organic marketing data (Paid Display, Google Ads, Programmatic, Paid Social, Facebook/Instagram, Twitter, LinkedIn etc.)
Understanding of conversion tracking and Attribution Models
Ability to digest data and provide actionable insights for key client stakeholders
Analytics & Implementation Skills
Expert-level experience using Google Tag Manager
Working experience using Google Analytics 4
Experience using and manipulating the dataLayer on sites for advanced tracking/tagging for Google Analytics 4
Nice to Haves
Experience tagging with Floodlight and u-variables
Experience using visualization & BI platforms (Tableau, Google Data Studio, PowerBI, Datorama, etc)
Experience with alternative Analytics platforms to GA (i.e Piwik Pro, Adobe Analytics, Amplitude, etc)
A/B/Multivariate testing (Google Optimize, Optimizely, VWO, or similar)
Experience with App Analytics (Firebase or similar)
If you do not meet all of the requirements we will still love to review your application and talk to you about this opportunity.
Qualifications:
Minimum of 5 years’ data analysis experience within a digital or data agency
Proficient in Google Tag Manager & Google Analytics 4
Experience developing clear, actionable insights from digital, paid and organic marketing data
What we’re looking for:
Over the years, Union has built an open and fun environment around our six core values; Creativity, Innovation, Collaboration, Kindness, Passion, and Hard Work. Therefore, regardless of the position, we are looking for people to join the team that have:
Ability to think creatively
Ability to bring innovative solutions to the table
Ability & desire to work collaboratively within a team environment
Genuinely nice and kind personality
Passion for not just fulfilling an assignment, but in adding value, pushing the boundaries and creating amazing work
Hard-working & self-motivated mindset
Ability to juggle multiple projects
Ability to quickly resolve issues as they arise
Up-to-date knowledge of current trends
Meticulous attention to details and overall product quality
Benefits
Benefits
Paid Time Off
Summer Fridays
Health Insurance
Profit Sharing Program
Latest MacBook Pro
Matching 401K
Professional Development
Show more
Show less","Marketing Analytics, Tableau, Google Data Studio, PowerBI, Datorama, Piwik Pro, Adobe Analytics, Amplitude, Floodlight, UVariables, Firebase, Google Tag Manager, Google Analytics, Google Ads, Social Media Analytics, Performance Marketing Measurement, Reporting, Data Analysis, Conversion Tracking, Attribution Modeling, Visualization, Data Manipulation, DataLayer, Campaign Tracking, A/B Testing, Google Optimize, Optimizely, VWO, Multivariate Testing","marketing analytics, tableau, google data studio, powerbi, datorama, piwik pro, adobe analytics, amplitude, floodlight, uvariables, firebase, google tag manager, google analytics, google ads, social media analytics, performance marketing measurement, reporting, data analysis, conversion tracking, attribution modeling, visualization, data manipulation, datalayer, campaign tracking, ab testing, google optimize, optimizely, vwo, multivariate testing","ab testing, adobe analytics, amplitude, attribution modeling, campaign tracking, conversion tracking, data manipulation, dataanalytics, datalayer, datorama, firebase, floodlight, google ads, google analytics, google data studio, google optimize, google tag manager, marketing analytics, multivariate testing, optimizely, performance marketing measurement, piwik pro, powerbi, reporting, social media analytics, tableau, uvariables, visualization, vwo"
"Lead Data Engineer (AWS, Azure, GCP)",CapTech,"Charlotte, NC",https://www.linkedin.com/jobs/view/lead-data-engineer-aws-azure-gcp-at-captech-3751644355,2023-12-17,Denver,United States,Mid senior,Remote,"Company Description
CapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.
Job Description
CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.
The Value You Deliver (or What You’ll Do)
Be trusted advisor to customers with best practices, methodologies, and technologies to implement data engineering solutions.
Design, implement, and maintain modern data pipelines to deliver optimal solutions utilizing appropriate cloud technologies.
Partner with product owners and business SMEs to analyze customer requirements and provide a supportable and sustainable engineered solution.
Provide technical leadership and collaborate within and across teams to ensure that the overall technical solution is aligned with the customer needs.
Stay current with the latest cloud technologies, patterns, and methodologies; share knowledge by clearly articulating results and ideas to key stakeholders.
Qualifications
Experience building/operating highly available distributed systems of data extraction, ingestion, and processing large data sets
5+ years of experience delivering data engineering solutions on cloud platform
5+ years of experience implementing modern designs using at least one cloud-based solution/platform (AWS, Azure, GCP)
Advanced level proficiency with at least one ETL / Data Orchestration technology (Azure Data Factory, SSIS, Informatica, Alteryx, Ab Initio, Pentaho, Talend, Matillion)
Experience cloud-based data warehousing and data lake solutions like Snowflake, Redshift, Databricks
5+ years of experience with SQL or NoSQL database (PostgreSQL, MySQL, SQL server, Oracle, Aurora, Presto, BigQuery)
Expertise with SQL, database design/structure and data structure (star, snowflake schemas, de/normalized designs)
5+ years of experience with at least one programming language (Python, Java, R, C / C# / C++, Shell)
Familiarity with one or more DevOps tools (git, Jenkins, CI/CD, Jira)
Fundamental understanding of big data, open source, and data streaming concepts
Ability to think strategically and provide recommendations utilizing traditional and modern architectural components based on business needs
Experience providing technical leadership and mentoring other engineers in data engineering space
Cloud certification on any platform a plus
Additional Information
We want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs.
CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clients
Learning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths
Modern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs
Carrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care
Fringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and more
Employee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations
Philanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities.
401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.
At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
Show more
Show less","Data Engineering, Cloud Computing, AWS, Azure, GCP, ETL / Data Orchestration, Data Warehousing, Data Lake Solutions, Snowflake, Redshift, Databricks, SQL, NoSQL, Database Design, Data Structure, Programming Languages, Python, Java, R, C / C# / C++, Shell, DevOps Tools, Git, Jenkins, CI/CD, Jira, Big Data, Open Source, Data Streaming, Traditional Architecture, Modern Architecture, Technical Leadership, Mentoring, Cloud Certification","data engineering, cloud computing, aws, azure, gcp, etl data orchestration, data warehousing, data lake solutions, snowflake, redshift, databricks, sql, nosql, database design, data structure, programming languages, python, java, r, c c c, shell, devops tools, git, jenkins, cicd, jira, big data, open source, data streaming, traditional architecture, modern architecture, technical leadership, mentoring, cloud certification","aws, azure, big data, c c c, cicd, cloud certification, cloud computing, data engineering, data lake solutions, data streaming, data structure, database design, databricks, datawarehouse, devops tools, etl data orchestration, gcp, git, java, jenkins, jira, mentoring, modern architecture, nosql, open source, programming languages, python, r, redshift, shell, snowflake, sql, technical leadership, traditional architecture"
Senior Data Engineer,BambooHR,"Charlotte, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-bamboohr-3760296665,2023-12-17,Denver,United States,Mid senior,Remote,"About Us
Our mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.
What You'll Do
As a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.
Your initial areas of focus will include:
Collaborate with stakeholders to make effective use of core data assets
With Spark and Pyspark libraries, load both streaming and batched data
Engineer lakehouse models to support defined data patterns and use cases
Leverage a combination of tools, engines, libraries, and code to build scalable data pipelines
Work within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environments
Documentation of data pipelines, cloud infrastructure, and standard operating procedures
Express data platform cloud infrastructure, services, and configuration as code
Automate load, scaling, and performance testing of data platform pipelines and infrastructure
Monitor, operate, and optimize data pipelines and distributed applications
Help ensure appropriate data privacy and security
Automate continuous upgrades and testing of data platform infrastructure and services
Build data pipeline unit, integration, quality, and performance tests
Participate in peer code reviews, code approvals, and pull requests
Identify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!)
Experience developing, scaling, and tuning data pipelines in Spark with PySpark
Understanding of data lake, lakehouse, and data warehouse systems, and related technologies
Knowledge and understanding of data formats, data patterns, models, and methodologies
Experience storing data objects in hadoop or hadoop like environments such as S3
Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark
Experience working with streaming technologies such as Kafka and Kinesis
Experience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or Greenplum
Ability to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets manager
Understanding of security around cloud infrastructure and data systems
Git-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!)
Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and Delta
Experience with Flink, Presto, Dremio, Databricks, or Kubernetes
Experience with expressing infrastructure as code leveraging tools like Terraform
Experience and understanding of a zero trust security framework
Experience developing CI/CD pipelines for automated testing and code deployment
Experience with QA and test automation
Exposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are:
Clear communicators with team members and stakeholders
Analytical and perceptive of patterns
Creative in coding
Detail-oriented and persistent
Productive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.
An Equal Opportunity Employer--M/F/D/V
Because our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.
For information on our Privacy Policy, click here.
Show more
Show less","Spark, Pyspark, Data lake, Lakehouse, AWS, VPC, Hadoop, Kafka, Kinesis, Snowflake, Databricks, Redshift, Vertica, Greenplum, Git, EMR, S3, RDS, IAM, Security groups, AMIs, Cloudwatch, Cloudtrail, Secrets manager, Hudi, Iceberg, Delta, Flink, Presto, Dremio, Kubernetes, Terraform, CI/CD, Tableau","spark, pyspark, data lake, lakehouse, aws, vpc, hadoop, kafka, kinesis, snowflake, databricks, redshift, vertica, greenplum, git, emr, s3, rds, iam, security groups, amis, cloudwatch, cloudtrail, secrets manager, hudi, iceberg, delta, flink, presto, dremio, kubernetes, terraform, cicd, tableau","amis, aws, cicd, cloudtrail, cloudwatch, data lake, databricks, delta, dremio, emr, flink, git, greenplum, hadoop, hudi, iam, iceberg, kafka, kinesis, kubernetes, lakehouse, presto, rds, redshift, s3, secrets manager, security groups, snowflake, spark, tableau, terraform, vertica, vpc"
Sr. Data Analyst,Varo Bank,"Charlotte, NC",https://www.linkedin.com/jobs/view/sr-data-analyst-at-varo-bank-3748836435,2023-12-17,Denver,United States,Mid senior,Remote,"Varo is an entirely new kind of bank. All digital, mission-driven, FDIC insured and designed for the way our customers live their lives. A bank for all of us.
The Analytics team is lean, high functioning and impact-focused. If you can see different angles for business/product opportunities, synthesize large amounts of data, connect the dots amidst ambiguity, and want to deliver clear and measurable impact from day one, Varo welcomes you!
What You'll Be Doing
Develop a deep understanding of the existing business and share insights around the key metrics that drive the company
Conduct data analysis, using different analytical and statistical approaches, to make business recommendations
Analysis areas might include (but not limited to): commercial analytics, product feature analysis, portfolio credit exposure, delinquency propensity models, forecasting)
Create & automate reports, iteratively build & prototype dashboards to provide insights at scale, solving for business priorities
Work with Product, Engineering and cross functional teams to provide analytical support for ongoing needs and large initiatives
Deliver effective presentations of findings and recommendations to multiple levels of stakeholders, creating visual displays of quantitative information.
You Bring The Following Required Skills And Experiences
4+ years of experience in an analytical role providing business analysis, developing dashboards, and presenting results to leadership
4+ years of experience in SQL queries, Tableau (and other BI tools), and day-to-day use of R/Python is a plus
Experience articulating business questions, scripting with SQL/Python to pull data from datasets, and adapting quantitative techniques to solve complex problems
Experience with experimentation, risk assessment, measuring ROI of initiatives, and automating dashboards and reports to track business performance
Track record of successfully communicating data-driven insights
Solid understanding of statistical analysis
Distinctive problem-solving skills and impeccable business judgment
We recognize not everyone will have all of these requirements. If you meet most of the criteria above and you’re excited about the opportunity and willing to learn, we’d love to hear from you!
About Varo
Varo launched in 2017 with the vision to bring the best of fintech into the regulated banking system. We’re a new kind of bank – all-digital, mission-driven, FDIC-insured, and designed around the modern American consumer.
As the first consumer fintech to be granted a national bank charter in 2020, we make financial inclusion and opportunity for all a reality by empowering everyone with the products, insights, and support they need to get ahead. Through our core product offerings and suite of customer-first features, we aim to address a broad range of consumer needs while profitably serving underserved communities that have been historically excluded from the traditional financial system.
We are growing quickly in our hub locations of San Francisco, Salt Lake City, and Charlotte along with colleagues located across the country. We have been recognized among Fast Company’s Most Innovative Companies, Forbes’ Fintech 50, and earned the No. 7 spot on Inc. 5000’s list of fastest-growing companies across the country.
Varo. A bank for all of us.
Our Core Values
Customers First
Take Ownership
Respect
Stay Curious
Make it Better
Learn More About Varo By Following Us
Facebook - https://www.facebook.com/varomoney
Instagram - www.instagram.com/varobank
LinkedIn - https://www.linkedin.com/company/varobank
Twitter - https://twitter.com/varobank
Engineering Blog - https://medium.com/engineering-varo
SoundCloud - https://soundcloud.com/varobank
Varo is an equal opportunity employer. Varo embraces diversity and we are committed to building teams that represent a variety of backgrounds, perspectives, and skills. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.
Beware of fraudulent job postings!
Varo will never ask for payment to process documents, refer you to a third party to process applications or visas, or ask you to pay costs. Never send money to anyone suggesting they can provide work with Varo. If you suspect you have received a phony offer, please e-mail careers@varomoney.com with the pertinent information and contact information.
CCPA Notice At Collection For California Employees And Applicants
https://varomoney.box.com/s/q7eockvma9nd2b0utwryruh4ze6gf8eg
Show more
Show less","SQL, Python, R, Tableau, Business intelligence tools, Data analysis, Statistical analysis, Experimentation, Risk assessment, Dashboarding, Reporting, Business communication, Problemsolving, Business judgment","sql, python, r, tableau, business intelligence tools, data analysis, statistical analysis, experimentation, risk assessment, dashboarding, reporting, business communication, problemsolving, business judgment","business communication, business intelligence tools, business judgment, dashboard, dataanalytics, experimentation, problemsolving, python, r, reporting, risk assessment, sql, statistical analysis, tableau"
"Experienced Software Engineer, Enterprise Data and Applications",Principal Financial Group,"Charlotte, NC",https://www.linkedin.com/jobs/view/experienced-software-engineer-enterprise-data-and-applications-at-principal-financial-group-3783940718,2023-12-17,Denver,United States,Mid senior,Remote,"What You'll Do
We're looking for a Software Engineer to join our Enterprise Data and Analytics (EDA) team. In this role, you’ll join a group of engineers in our data space focusing on our Master Data Management platform. Here in EDA, we are at the intersection of many strategic and digital initiatives that impact the enterprise! This provides a unique opportunity to provide highly valuable and critically important solutions to Principal as an enterprise while gaining exposure to cross functional engineers and teams.
If you like working with data and understanding how data flows through our various systems, then this is just the team for you. If you don't know the specific tools that we use, that's fine as we are happy to mentor and train someone who knows a thing or two about object-oriented development and has an innate curiosity and eagerness to learn and grow. As a software engineer in our data space, you'll be performing many functions, but not limited to eliciting requirements, developing software and integrating data for new vendor based and in-house systems, and providing operational support.
You'll have the opportunity to:
Embrace a Product Mentality by focusing on outcomes over outputs, pursue fast feedback loops, and deliver solutions iteratively with low risk and low cost
Grow our DevOps approach and culture by continuously maturing and optimizing our SDLC through automation and safe/frequent delivery
Engage in all facets of software engineering from understanding the problem, evaluating designs, creating the solution, validating the outcome, etc.
Understand and continue driving our journey to a cloud-first technology community
Appreciate and promote Cloud Engineering – AWS PaaS, cloud integration patterns, cloud security, cloud operations, etc.
Network and communicate with cross-functional teams and collaborate with both IT and non-IT partners
Learn new technology and continuously grow through creative solutioning
Operating at the intersection of financial services and technology, Principal builds financial tools that help our customers live better lives. We take pride in being a purpose-led firm, motivated by our mission to make financial security accessible to all. Our mission, integrity, and customer focus have made us a trusted leader for more than 140 years.
As Principal continues to modernize its systems, this role will offer you an exciting opportunity to build solutions that will directly impact our long-term strategy and tech stack, all while ensuring that our products are robust, scalable, and secure!
Who You Are
Associate's or bachelor's degree (preference in a computer science, technology, engineering or math-related field) or equivalent work experience
3+ years of engineering experience in modern object oriented technologies
Experience/exposure to data and/or data technologies (more specifically, how that data moves from system to system)
Strong motivation for continuous learning, mentoring, problem solving, analytical thinking, and helping others grow along with you
Passion for working in a highly collaborative environment to solve problems and deliver customer value
Rotational on-call support is required
Skills That Will Help You Stand Out
Cloud technologies (AWS)
Enterprise level environment experience
Experience with the following technologies and tools
Programming (preference Java)
Master Data Management (Informatica)
ETL (PowerCenter)
Data engineering (data management, data transformation, data modeling, SQL, data lake, data warehousing)
DevOps practices (TDD, CI/CD, etc.)
Salary Range Information
Salary ranges below reflect targeted base salaries. Non-sales positions have the opportunity to participate in a bonus program. Sales positions are eligible for sales incentives, and in some instances a bonus plan, whereby total compensation may far exceed base salary depending on individual performance. Actual compensation for all roles will be based upon geographic location, work experience, education, licensure requirements and/or skill level and will be finalized at the time of offer.
Salary Range (Non-Exempt expressed as hourly; Exempt expressed as yearly)
$77350 - $182400 / year
Additional Information
Our Engineering Culture
Through our product-driven Agile/Lean DevOps environment, we’ve fostered a culture of innovation and experimentation across our development teams. As a customer-focused organization, we work closely with our end users and product owners to understand and rapidly respond to emerging business needs.
Collaboration is embedded into everything we do – from the products we develop to the quality service we provide. We’re driven by the belief that diversity of thought, background, and perspective is critical to creating the best products and experiences for our customers.
Job level
We’ll consider talent at the next levels with the right experiences and skills.
Hours
This team has Tuesday-Thursday meetings starting at 7:30am CST.
Work Environments
This role offers in-office, hybrid (blending at least three office days in a typical workweek), and remote work arrangements (only if residing more than 30 miles from Des Moines, IA, or Charlotte, NC). You’ll work with your leader to figure out which option may align best based on several factors.
Work Authorization/Sponsorship
At this time, we're not considering applicants that need any type of immigration sponsorship (additional work authorization or permanent work authorization) now or in the future to work in the United States. This includes, but IS NOT LIMITED TO: F1-OPT, F1-CPT, H-1B, TN, L-1, J-1, etc. For additional information around work authorization needs please use the following links.
Nonimmigrant Workers and Green Card for Employment-Based Immigrants
Investment Code of Ethics
For Principal Asset Management positions, you’ll need to follow an Investment Code of Ethics related to personal and business conduct as well as personal trading activities for you and members of your household. These same requirements may also apply to other positions across the organization.
Experience Principal
At Principal, we value connecting on both a personal and professional level. Together, we’re imagining a more purpose-led future for financial services – and that starts with you. Our success depends on the unique experiences, backgrounds, and talents of our employees. And we support our employees the same way we support our customers: with comprehensive, competitive benefit offerings crafted to protect their physical, financial, and social well-being. Check out our careers site to learn more about our purpose, values and benefits.
Principal is an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
Posting Window
We will be accepting applications for at least 3 days from when the job was originally posted, after which we may keep open or remove the posting based upon applications we receive. Please submit applications in a timely manner as there is no guarantee the posting will be available beyond 3 days of the original posting date.
Original Posting Date
11/20/2023
LinkedIn Remote Hashtag
LinkedIn Hashtag
Show more
Show less","AWS, Java, Hadoop, Informatica, PowerCenter, TDD, CI/CD, ETL, SQL, DevOps, Cloud Engineering, Cloud Computing, Data Engineering, Data Management, Data Transformation, Data Modeling, Data Lake, Data Warehousing, Cloud Technologies, Enterprise Level Environment, Agile, Lean, Object Oriented Development","aws, java, hadoop, informatica, powercenter, tdd, cicd, etl, sql, devops, cloud engineering, cloud computing, data engineering, data management, data transformation, data modeling, data lake, data warehousing, cloud technologies, enterprise level environment, agile, lean, object oriented development","agile, aws, cicd, cloud computing, cloud engineering, cloud technologies, data engineering, data lake, data management, data transformation, datamodeling, datawarehouse, devops, enterprise level environment, etl, hadoop, informatica, java, lean, object oriented development, powercenter, sql, tdd"
"Cloud Analytics Engineer (AWS, Data Analytics, Python)",Optomi,Charlotte Metro,https://www.linkedin.com/jobs/view/cloud-analytics-engineer-aws-data-analytics-python-at-optomi-3782239980,2023-12-17,Denver,United States,Mid senior,Hybrid,"Cloud Analytics Engineer (AWS, Data Analytics, Python
*6-month Contract to Hire*
Hybrid: 3x a week onsite in Charlotte, NC
Optomi, in partnership with our premier client in the information technology and services industry, is seeking an experienced Cloud Engineer to join their team for a hybrid role in Charlotte, NC! The Cloud Analytics Engineer provides expertise in the development and enablement of enterprise analytics capabilities that provide clarity to our client's key success criteria. They will designs and implement scalable solutions from available cloud services and analytics tooling as part of an enterprise platform to achieve specific business objectives. This candidate is a technical expert helping to lead a team of talented analytics engineers in the consolidation and refinement of our client's data architecture and business intelligence dashboards. Partners with cloud engineers, information security, core infrastructure teams and application developers to enable agility through best-in-class automation and infrastructure as code.
The candidate should have deep hands-on experience with technical implementations for cloud and solutions as well as the capability to deliver those solutions.
Job Responsibilities:
Lead by example in a team of highly skilled, talented Cloud Analytics Engineers
Collaborate and partner with counterparts from Security, Enterprise Architecture, and CIO application teams
Design and implement solutions that measure and demonstrate the value of our client's service offering.
Always maintain an audit ready posture
Implement automation that validates controls for security, compliance, and audit
Stay up with technology trends. The right candidate has a passion for technology and will push for constant improvement, even if we try and fail and tweak the solution and try again.
Lead initiatives that improve Cloud Engineering, Cloud Enablement and even our client's Infrastructure organization.
Stay focused on business objectives and use measurements to track success of our client's architecture and implementation in achieving those objectives
Champion a collaborative development approach in tune with our client's Agile methodology that encourages and accepts contributions and emphasizes transparency.
Assist with the architecture and implementation of infrastructure-as-code, and policy-as-code objectives.
Show more
Show less","AWS, Data Analytics, Python, Cloud Services, Analytics Tooling, Enterprise Platform, Data Architecture, Business Intelligence Dashboards, Automation, Infrastructure as Code, Security, Compliance, Audit, Technology Trends, Agile Methodology, InfrastructureasCode, PolicyasCode","aws, data analytics, python, cloud services, analytics tooling, enterprise platform, data architecture, business intelligence dashboards, automation, infrastructure as code, security, compliance, audit, technology trends, agile methodology, infrastructureascode, policyascode","agile methodology, analytics tooling, audit, automation, aws, business intelligence dashboards, cloud services, compliance, data architecture, dataanalytics, enterprise platform, infrastructure as code, infrastructureascode, policyascode, python, security, technology trends"
"Data Engineer :: Contract on W2 :: Hybrid for Charlotte, NC","Anveta, Inc","Charlotte, NC",https://www.linkedin.com/jobs/view/data-engineer-contract-on-w2-hybrid-for-charlotte-nc-at-anveta-inc-3675752807,2023-12-17,Denver,United States,Mid senior,Hybrid,"My name is Prabhat Kumar and I am a Staffing Specialist at Anveta. I am reaching out to you on an exciting job opportunity with one of our clients.
Position: Data Engineer
Location: Hybrid Charlotte, NC
Must Haves
Minimum of 5+ years of experience working as a hands on ETL Developer
Databased used is SQL
(manager is seeking a senior resource so the more years of experience, the better!)
Strong experience with Pyspark and Python
Apache Spark
Spark SQL
Nice To Have
Financial Background, highly preferred
Springboot/java
S3 compliant Object store
Prabhat Kumar Jha
Technical Recruiter
US Direct: 469-914-9242
Email: prabhat@anveta.com | URL: http://www.anveta.com
Show more
Show less","Data Engineering, ETL Development, SQL, Python, Pyspark, Apache Spark, Spark SQL, Financial Background, Springboot, Java, S3","data engineering, etl development, sql, python, pyspark, apache spark, spark sql, financial background, springboot, java, s3","apache spark, data engineering, etl development, financial background, java, python, s3, spark, spark sql, springboot, sql"
Lead Data Engineer,VLink Inc,"Charlotte, NC",https://www.linkedin.com/jobs/view/lead-data-engineer-at-vlink-inc-3782832474,2023-12-17,Denver,United States,Mid senior,Hybrid,"Hi
This is Mukul from V link . We have an urgent
Requirement of
Lead Data Engineer in CHARLOTTE, NC
Job Title: Lead Data Engineer
Location: CHARLOTTE, NC
Note : Need only candidates who have prior experience with financial industry or banking domain
About VLink:
Started in 2006 and headquartered in Connecticut, VLink is one of the fastest growing digital technology services and consulting companies. Since its inception, our innovative team members have been solving the most complex business, and IT challenges of our global clients.
Job Description:
We are looking for Databricks Architect great sense of urgency, senior technical person with Databricks exp.
What You'll Do
Must have hands-on experience in Java/Scala/Python, Spark, S3, Glue, Redshift.
Highly analytical and data oriented.
Experience in SQL, NoSql Database
Data masking of on prem PII data.
Develop API calls with using secure data transfer.
Take standard output data to lower environments for pre prod testing!
Enable secured channels for data models and data science activities.
Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15 mins
You have experience with development tools and agile methodologies
Expertise You'll Bring
You have hands-on experience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical
Highly analytical and data oriented.
Experience in SQL, NoSql Database
Data masking of on prem PII data.
Develop API calls with using secure data transfer.
Take standard output data to lower environments for pre prod testing!
Enable secured channels for data models and data science activities.
Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15 mins
You have experience with development tools and agile methodologies
Show more
Show less","Data Engineering, Spark, Java, Scala, Python, AWS S3, AWS Glue, AWS Redshift, SQL, NoSQL, Data Masking, API Development, Agile Development, Development Tools","data engineering, spark, java, scala, python, aws s3, aws glue, aws redshift, sql, nosql, data masking, api development, agile development, development tools","agile development, api development, aws glue, aws redshift, aws s3, data engineering, data masking, development tools, java, nosql, python, scala, spark, sql"
Sr. Data Engineer ETL,Stellent IT,"Charlotte, NC",https://www.linkedin.com/jobs/view/sr-data-engineer-etl-at-stellent-it-3690508388,2023-12-17,Denver,United States,Mid senior,Hybrid,"Job Title - Data Engineer
Location - Charlotte, North Carolina
Phone + Skype
Job Description
Overview
: Maintain and migrate ETL Talend jobs into Spark, Maintain Hadoop Jobs, and Redesign the data streaming
from legacy using Java.
Primary Skills
Hadoop, Hive, Spark with basic Talend ETL knowledge.
Secondary Skills
Java
Show more
Show less","Data Engineering, ETL, Java, Apache Spark, Hive, Hadoop","data engineering, etl, java, apache spark, hive, hadoop","apache spark, data engineering, etl, hadoop, hive, java"
Associate Data Engineer (Data Integration) - CLT,Brooksource,"Charlotte, NC",https://www.linkedin.com/jobs/view/associate-data-engineer-data-integration-clt-at-brooksource-3641286455,2023-12-17,Denver,United States,Mid senior,Hybrid,"<< Return to Search Results
ETL Developer – Data Integration
Charlotte, NC
12 month ongoing contract (1/4/21 – 12/31/21)
Brooksource is looking for an Data Integration ETL Developer for our Fortune 100 Telecommunications client in the Charlotte, NC area. The Data Integration ETL Developer will be an integral member of the Data Operations Architecture team. You will join the advertising/media organization and contribute to data solutions that power some of the largest and most successful marketing/advertising campaigns.
The primary responsibility of this role is to design and develop reusable components, code, and document custom automation frameworks, maintain scripts, and update these items as needed to build continuous delivery pipelines, ensuring rapid availability of the product.
This position requires strong collaboration with you Scrum Team (composed of Scrum Master, Product Owner, Lead Architect, Developers and QA) to understand business requirements and provide appropriate data integration solutions in alignment with solution implementation architecture.
Minimum Requirements
5+ years’ experience with the following:
Experienced in building Data Integration and Workflow Solutions and Extract, Transform, and Load (ETL) solutions for data warehousing
Strong data architecture knowledge around enterprise data warehousing concepts, SQL development and optimization, and data integration
Advanced SQL scripting and querying experience - Expert in creating SQL objects like Tables, Complex Stored Procedures, Triggers, Views, Indexes, and User Defined Functions to facilitate efficient data manipulation and consistent data storage.
Strong understanding of relation database management systems with experience in SQL Server, Oracle, or similar systems.
Writing Python scripts with knowledge of relevant frameworks and libraries, such as workflow management utilities and libraries for accessing and extracting data.
Creating, scheduling and monitoring workflows using Airflow
Experience working with AWS infrastructure
Experience with Docker
Hands-on experience with GitHub for version control
Experience in ETL performance testing, including data validation
Familiarity with Agile development methodologies such as Scrum and SAFe
Good team player, extremely adaptable and fast learner.
Nice To Have
Experience working with Media/Advertising MSO data and applications
Experience working with Snowflake and SnowSQL
Strong desire to mentor others in the data integration space, providing technical and business guidance.
Responsibilities
Design and develop reusable components, code, and document custom automation frameworks, maintain scripts, and update these items as needed to build continuous delivery pipelines, ensuring rapid availability of the product
Assist in requirements gathering and documents processes with internal stakeholders and collaborate with Product Owners, Scrum Master and other team members to determine data extraction and transformation requirements
Develop data mapping documentation to establish relationships between source and target tables including transformation processes.
Write and analyze complex SQL for the purpose of data extraction and processing - Design, develop, and deploy data movement using SQL server Integrations Services (SSIS), TSQL and stored procedures
Extract and transform data from multiple sources and load data into one or more destinations, and monitor integration and replication performance to ensure quality and stability.
Create, schedule and maintain data pipelines/workflows using Apache Airflow and Python
Design and develop ETL to load the data warehouse and data marts and test ETL data pipelines to maintain data infrastructures
Support testing efforts as a part of Agile/Scrum teams
Validate data quality and perform all aspects of verification, including functional, structural, regression and system testing
Work closely with Operations Production Support team in resolving escalated high priority incidents and the development coding issues.
Environment
Company size: Fortune 100
Hours: Standard 40-hour weeks
Dress code: Business casual
COVID Schedule: Rotational, 2-week onsite/remote schedule
Benefits Of Working With Brooksource
Direct communication with the hiring manager, which allows us to have a clear understanding of the timeline and move candidates through the interview process faster.
Dedication to keep an open line of communication and provide full transparency.
Associate Data Engineer (Data Integration) - CLT Charlotte, North Carolina
Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.
JO-2011-106016
Show more
Show less","Data Integration, Workflow Solutions, Extract Transform Load (ETL), Data Warehousing, SQL, SQL Server, Oracle, Python, Airflow, AWS, Docker, GitHub, ETL performance testing, Data validation, Agile, Scrum, SAFe, Snowflake, SnowSQL, SSIS, TSQL, Apache Airflow","data integration, workflow solutions, extract transform load etl, data warehousing, sql, sql server, oracle, python, airflow, aws, docker, github, etl performance testing, data validation, agile, scrum, safe, snowflake, snowsql, ssis, tsql, apache airflow","agile, airflow, apache airflow, aws, data integration, data validation, datawarehouse, docker, etl performance testing, extract transform load etl, github, oracle, python, safe, scrum, snowflake, snowsql, sql, sql server, ssis, tsql, workflow solutions"
"Senior Data Engineer, Talent Analytics",RVO Health,"Charlotte, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-talent-analytics-at-rvo-health-3766409455,2023-12-17,Denver,United States,Mid senior,Hybrid,"AT A GLANCE
RVO Health is looking to grow our Talent Analytics team by adding a Senior Data Engineer. In this role, you’ll be challenged to help shape our Talent Analytics strategy while working on high-priority data efforts – all in line with our broader mission of attracting diverse talent and giving them an experience that will bring out their very best. You will be responsible for scoping, executing, and delivering technical projects to stakeholders across the Human Capital organization, and producing data engineering & analytical solutions that connect them to the data they need.
Where You'll Be
To prioritize togetherness, culture, and accountability, RVO Health operates on a hybrid in-office work schedule. We expect employees to work from our South Charlotte office Tuesday, Wednesday and Thursday each week. You are welcome to work remotely Mondays and Fridays if you wish.
1101 Red Ventures Dr Fort Mill, SC 29707
What You’ll Do
Develop/maintain data pipelines from various data sources (ADP WFN, Greenhouse Recruiting/Onboarding, CultureAmp, Docebo, etc) to a target data warehouse using batch data load strategies utilizing cutting edge cloud technologies.
Conduct hands-on, advanced data engineering & analytics using multiple data sources originating from different applications and systems.
Collaborate with the data science team to identify new opportunities for deep analytics within the Human Capital organization.
Provide input into strategies as they drive the team forward with delivery of business value and technical acumen.
Execute on proof of concepts, where appropriate, to help improve our technical processes.
Documenting database designs that include data models, metadata, ETL specifications and process flows for business data project integrations.
What We’re Looking For
5+ years of Data Engineering experience
3+ years of writing SQL experience against complex databases for data extraction using AWS Athena (Presto), Databricks Delta Lake along with Data Modeling & Data warehousing experience.
3+ years of experience working on Spark (RDDs / Data Frames / Dataset API) using Scala/Python to build and maintain complex ETL pipelines and experience data processing using Parquet and Avro
3+ years of Python coding experience, familiar with utilizing packages such as pandas, boto3, requests, json, csv, os
3+ years of experience working on AWS services including Glue, Athena, Lambda, S3, SNS, SQS, Cloud formation, Step Functions, Serverless architecture.
Experience with GitHub, Code check-in, versioning, Git commands
Introduce and drive adoption of CI/CD framework within the team and build/deploy CI/CD Pipelines using Terraform or AWS Cloud Formation
Experience with visualization tools such as Tableau, Looker or PowerBI to build dynamic/scalable dashboards and reports.
Strong analytical and interpersonal skills
Knowledge or experience within Talent/People analytics is a plus
Enthusiastic, highly motivated and ability to learn quickly.
Able to work through ambiguity in a fast-paced, dynamically changing business environment.
Ability to manage multiple tasks at the same time with minimal supervision.
Pursuant to various state Fair Pay Acts, below is a summary of compensation elements for this role at the company. The following benefits are provided by RVO Health, subject to eligibility requirements.
Starting Salary: $100,000 - $170,000
Note actual salary is based on geographic location, qualifications and experience
Access to a Free Udemy for Business subscription—thousands of hours of learning content on hundreds of different subjects at your fingertips
Health Insurance Coverage (medical, dental, and vision)
Life Insurance
Short and Long-Term Disability Insurance
Flexible Spending Accounts
Paid Time Off
Holiday Pay
401(k) with match
Employee Assistance Program
Paid Parental Bonding Benefit Program
This position may occasionally require travel for training and other work-related duties.
Who We Are
Founded in 2022, RVO Health is a new healthcare platform of digital media brands, services and technologies focused on building relationships with people throughout their health & wellness journey. We meet people where they are in their personal health journeys and connect them with both the information and the care they need. RVO Health was created by joining teams from both Red Ventures and UnitedHealth Group’s Optum Health. Together we’re focused on delivering on our vision of a stronger and healthier world.
RVO Health is comprised of Healthline Media (Healthline, Medical News Today, Psych Central, Greatist and Bezzy), Healthgrades, FindCare and PlateJoy; Optum Perks, Optum Store and the virtual coaching platforms Real Appeal, Wellness Coaching, and QuitForLife.
We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program.
RVO Health is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at RVO Health is based solely on a person's merit and qualifications.
We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodations@rvohealth.com.
RVO Health Privacy Policy: https://rvohealth.com/legal/privacy
Show more
Show less","Data Engineering, ETL, SQL, Scala, Python, AWS, Spark, Parquet, Avro, GitHub, Code checkin, Versioning, Git, CI/CD, Terraform, AWS Cloud Formation, Tableau, Looker, PowerBI, Data Visualization","data engineering, etl, sql, scala, python, aws, spark, parquet, avro, github, code checkin, versioning, git, cicd, terraform, aws cloud formation, tableau, looker, powerbi, data visualization","avro, aws, aws cloud formation, cicd, code checkin, data engineering, etl, git, github, looker, parquet, powerbi, python, scala, spark, sql, tableau, terraform, versioning, visualization"
Staff Data Engineer,RVO Health,"Charlotte, NC",https://www.linkedin.com/jobs/view/staff-data-engineer-at-rvo-health-3787308113,2023-12-17,Denver,United States,Mid senior,Hybrid,"AT A GLANCE
RVO Health is looking for a talented Staff Data Engineer to join our team! As a Staff Data Engineer at RVO Health, you will have the chance to build technology that drives real improvements to consumer health outcomes and has the potential to have widespread impact across the healthcare industry. You will design, develop, test, and maintain big data pipelines for ingestion, segmentation, and reporting to drive our vision!
What You'll Do
Provide technology ownership for data solutions for projects that the team has been tasked with.
Work with a cross functional team of business analysts, architects, engineers, data analysts and data scientists to formulate both business and technical requirements.
Design and build data pipelines from various data sources to a target data warehouse using batch data load strategies utilizing cutting edge cloud technologies.
Conceptualizing and generating infrastructure that allows data to be accessed and analyzed effectively.
Documenting database designs that include data models, metadata, ETL specifications and process flows for business data project integrations.
Perform periodic code reviews and test plans to ensure data quality and integrity.
Provide input into strategies as they drive the team forward with delivery of business value and technical acumen.
Execute on proof of concepts, where appropriate, to help improve our technical processes.
Conduct Technical assessments and mentor junior team members
What We're Looking For
7-10 years of Data Engineering experience
4+ years of experience working on Spark (RDDs / Data Frames / Dataset API) using Scala/Python to build and maintain complex ETL pipelines.
4+ years of experience in the big data space
Experience in translating business requirements into technical data solutions on a large scale.
2+ years of experience working on AWS (Kinesis / Kafka / S3 / RedShift) or Azure.
Able to research and troubleshoot potential issues presented by stakeholders within the data ecosystem.
Experience with GitHub and CI/CD processes
Experience with Compute technologies like EMR and Databricks
Experience working job orchestration (eg., Airflow / AWS Step Function)
Experience with Data Modeling, Data warehousing
Working with Kubernetes is a plus
Strong analytical and interpersonal skills.
Enthusiastic, highly motivated and ability to learn quickly.
Able to work through ambiguity in a fast-paced, dynamically changing business environment.
Ability to manage multiple tasks at the same time with minimal supervision.
Pursuant to various state Fair Pay Acts, below is a summary of compensation elements for this role at the company. The following benefits are provided by RVO Health, subject to eligibility requirements.
Starting Salary: $150,000 - $200,000
Note actual salary is based on geographic location, qualifications and experience
Access to a Free Udemy for Business subscription—thousands of hours of learning content on hundreds of different subjects at your fingertips
Health Insurance Coverage (medical, dental, and vision)
Life Insurance
Short and Long-Term Disability Insurance
Flexible Spending Accounts
Paid Time Off
Holiday Pay
401(k) with match
Employee Assistance Program
Paid Parental Bonding Benefit Program
Who We Are
Founded in 2022, RVO Health is a new healthcare platform of digital media brands, services and technologies focused on building relationships with people throughout their health & wellness journey. We meet people where they are in their personal health journeys and connect them with both the information and the care they need. RVO Health was created by joining teams from both Red Ventures and UnitedHealth Group’s Optum Health. Together we’re focused on delivering on our vision of a stronger and healthier world.
RVO Health is comprised of Healthline Media (Healthline, Medical News Today, Psych Central, Greatist and Bezzy), Healthgrades, FindCare and PlateJoy; Optum Perks, Optum Store and the virtual coaching platforms Real Appeal, Wellness Coaching, and QuitForLife.
We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program.
RVO Health is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at RVO Health is based solely on a person's merit and qualifications.
We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodations@rvohealth.com.
RVO Health Privacy Policy: https://rvohealth.com/legal/privacy
Show more
Show less","Data Engineering, Spark, Scala, Python, ETL, Big Data, AWS, Kinesis, Kafka, S3, RedShift, Azure, GitHub, CI/CD, EMR, Databricks, Airflow, AWS Step Function, Data Modeling, Data Warehousing, Kubernetes, Udemy for Business","data engineering, spark, scala, python, etl, big data, aws, kinesis, kafka, s3, redshift, azure, github, cicd, emr, databricks, airflow, aws step function, data modeling, data warehousing, kubernetes, udemy for business","airflow, aws, aws step function, azure, big data, cicd, data engineering, databricks, datamodeling, datawarehouse, emr, etl, github, kafka, kinesis, kubernetes, python, redshift, s3, scala, spark, udemy for business"
Data Analyst,TekIntegral,"Charlotte, NC",https://www.linkedin.com/jobs/view/data-analyst-at-tekintegral-3683243340,2023-12-17,Denver,United States,Mid senior,Hybrid,"POSITION
Data Analyst
LOCATION
Onsite Charlotte, NC
DURATION
3+ months CTH
PAY RATE
$50/hr C2C
INTERVIEW TYPE
Video
Required Skills
6-10 years of experience
SQL
Data Analysis
GIS experience
Show more
Show less","SQL, Data Analysis, GIS","sql, data analysis, gis","dataanalytics, gis, sql"
Senior Data Engineer,TTX Company,"Charlotte, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-ttx-company-3749779191,2023-12-17,Denver,United States,Mid senior,Hybrid,"Job Description
JOB SUMMARY
The Senior Data Engineer is an interdisciplinary individual on the Data Analytics team who collaborates closely with multiple stakeholders, across the enterprise and Information Technology (IT), to ensure the most important data is accessible and well-understood.
A Senior Data Engineer designs, develops, implements, and supports new and existing highly efficient ELT/ETL processes and data sets. Other responsibilities include working closely with data consumers, solution architects, security and governance teams to implement solutions to answer complex questions and drive business decisions. Apply your proven communication skills, problem-solving skills, and knowledge of best practices in designing, developing, and deploying data and analytic solutions.
Senior Data Engineers need to be adept in several technical and business skills. These include working with diverse datasets, parsing and understanding data, working with domain experts, data scientists and analysts in framing the business problems and provisioning integrated data quickly across multiple environments. Senior Data Engineers need to be inquisitive and motivated to learn modern technologies and capabilities which could benefit the organization and lead the effort in evaluating the technology for acceptance at TTX. Senior Data Engineers also assist the business data science efforts with source data, building data sets, helping evaluate models and integrating analytics and data science model outputs into business processes.
Responsibilities
Hybrid cloud environment: the Senior Data Engineer works in a hybrid cloud ecosystem, composed of Azure, Oracle and on-premises technologies, building and supporting data and analytics solutions. The Senior Data Engineer will need to learn the data, tools and capabilities resident in this hybrid ecosystem, such as Synapse, Data Lakes, Dedicated Pool, Azure ML, SQL Server, SSIS and SSAS.
Build data pipelines: Managed data pipelines consist of a series of stages through which data flows. Designing, building and maintaining data pipelines, in Azure and the on-premises ecosystems, will be the primary responsibility of the data engineer.
Drive data centric decision making. Assists with enhancing the data and metadata management infrastructure to ensure data quality, accessibility and security.
Collaborate across departments: Collaborates with business data consumers, of various skill levels, in refining their requirements for various data and analytics initiatives. This collaboration can lead to building enterprise data products, enabling data-driven decision making.
Lead, educate and train: Be curious and knowledgeable about innovative technologies and data initiatives. Research and propose data ingestion, preparation, integration and operationalization tools or techniques to aid these initiatives. Train team members, data consumers, data scientists and data analysts in these technologies and preparation techniques.
Participate in ensuring compliance and governance during data use: Data engineers work with data governance teams (and information stewards within these teams) in building, vetting and promoting content, which adheres to data governance and compliance initiatives.
Become a data and analytics evangelist: TTX considers the Senior Data Engineer a blend of data and analytics “evangelist,” “guru” and “fixer.” This role promotes the available data and analytics capabilities and expertise to business unit leaders educating them in leveraging these capabilities in achieving their business goals.
Qualifications
Education and Training
A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is required.
An advanced degree in computer science (MS), statistics, information science (MIS), data management, information systems, information science (post-graduation diploma or related) or a related quantitative field [or equivalent work experience] is preferred.
The ideal candidate will have a combination of IT, data governance, analytics, and communication skills.
Previous Experience
At least 5 years or more of work experience in data management disciplines including data integration, modeling, optimization, data quality and/or other areas directly relevant to data engineering responsibilities and tasks.
At least 5 years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative.
Technical [and Business] Knowledge/Skills
Strong experience with various languages and advanced analytics tools such as SQL, Python, PowerBI, Microsoft SSIS/SSAS, Azure Synapse and others.
Strong ability to design, build and manage data structures and pipelines for encompassing data transformation, data models, schemas, metadata and workload management. Work with both IT and business in integrating analytics and data science output into business processes and workflows.
Strong experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using data integration technologies, including ETL/ELT, data replication/CDC, message-oriented data movement, API design and development.
Physical Job Requirements
The physical demands and work environment characteristics described here are representative of those needed to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform the essential functions.
General office environment with limited physical demands and travel. While performing the duties of this job, the employee is regularly required to communicate using hands and fingers, handle, feel, and reach. The employee frequently is required to remain stationary within work area and occasionally move about. The employee must occasionally transport up to 10 pounds. Correctable vision and hearing required.
The above job description is intended to describe the general content of and requirements for the performance of this job. It is not to be construed as an exhaustive statement of duties, responsibilities or requirements.
About Us
TTX Company is a leading provider of railcars and related freight car management services to the North American rail industry. TTX’s pool of railcars is ideal for supporting shippers in the intermodal, automotive, paper & forest, metals, machinery, wind energy and other markets where flatcars, boxcars and gondolas are required.
TTX’s generous Total Rewards package includes Paid Time Off, Health, Dental and Vision benefits, and 401(k) with company match. This position participates in Railroad Retirement.
TTX Company is an Equal Employment Opportunity Employer.
Show more
Show less","Data Engineering, ELT, ETL, Data Structures, Data Pipelines, SQL, Python, PowerBI, Microsoft SSIS/SSAS, Azure Synapse, Data Transformation, Data Models, Schemas, Metadata, Workload Management, Data Integration, ETL/ELT, Data Replication/CDC, MessageOriented Data Movement, API Design, API Development, Azure, Oracle, Synapse, Data Lakes, Dedicated Pool, Azure ML, SQL Server","data engineering, elt, etl, data structures, data pipelines, sql, python, powerbi, microsoft ssisssas, azure synapse, data transformation, data models, schemas, metadata, workload management, data integration, etlelt, data replicationcdc, messageoriented data movement, api design, api development, azure, oracle, synapse, data lakes, dedicated pool, azure ml, sql server","api design, api development, azure, azure ml, azure synapse, data engineering, data integration, data lakes, data models, data replicationcdc, data structures, data transformation, datapipeline, dedicated pool, elt, etl, etlelt, messageoriented data movement, metadata, microsoft ssisssas, oracle, powerbi, python, schemas, sql, sql server, synapse, workload management"
Lead Data Visualization Engineer - Cybersecurity,Optomi,Charlotte Metro,https://www.linkedin.com/jobs/view/lead-data-visualization-engineer-cybersecurity-at-optomi-3782267537,2023-12-17,Denver,United States,Mid senior,Hybrid,"Lead Data Visualization Engineer - Cybersecurity
Hybrid: Must be located in Charlotte, Denver, or St. Louis metro areas
*Unable to offer sponsorship or work C2C*
Optomi, in partnership with an industry leader in Telecom, is seeking an experienced Data and Tableau specialist to serve as a SME / Lead Visualization Engineer on a Cybersecurity team. This role will work closely with other Automation Engineers, Data Visualization Engineers, and Security Assurance specialists.
What you will do:
Be subject mater expert with Tableau dashboards, reports, charts, and visuals. You are a champion of all internal dashboards.
Source data, clean data, filter data, automate data, massage data, and be comfortable working in a variety of data sources.
Help establish and execute Data Governance principals for data quality and integrity for the department.
Mentor younger associate developers in BI and Analytic standards and best practices.
Create visuals, gather requirements, work with clients and stakeholders, present to technical and non technical groups, help impact the business as a valued partner.
Understand and appreciate the importance of Cybersecurity in an enterprise organization while supporting all Cyber goals and initiatives.
Qualifications:
BS/MS in Computer Science or equivalent
7+ years of professional experience in data and visualization engineering
5+ years of experience in Tableau
4+ years working with Python, SQL, Alteryx, Powershell, NoSQL databases, API's, and similar tools
Experience working with or on Cybersecurity teams
Strong collaboration and communication skills
Additional Qualifications:
Ability to be self-motivated, independent, and detail-oriented
Desire to wear multiple hats daily and support the broader teams success
Show more
Show less","Tableau, Data Visualization, Python, SQL, Alteryx, Powershell, NoSQL Databases, APIs, Cybersecurity, Data Governance, Data Quality, Data Integrity, BI and Analytics Standards, Best Practices, Presentation Skills, Communication Skills, Collaboration Skills","tableau, data visualization, python, sql, alteryx, powershell, nosql databases, apis, cybersecurity, data governance, data quality, data integrity, bi and analytics standards, best practices, presentation skills, communication skills, collaboration skills","alteryx, apis, best practices, bi and analytics standards, collaboration skills, communication skills, cybersecurity, data governance, data integrity, data quality, nosql databases, powershell, presentation skills, python, sql, tableau, visualization"
Lead Cybersecurity Data Visualization Engineer,Optomi,"Charlotte, NC",https://www.linkedin.com/jobs/view/lead-cybersecurity-data-visualization-engineer-at-optomi-3779388679,2023-12-17,Denver,United States,Mid senior,Hybrid,"Lead Cybersecurity Data Visualization Engineer | Hybrid
Optomi, in partnership with a global leader in the technology industry, is seeking a Lead Cybersecurity Data Visualization Engineer to join their team! This candidate will be responsible for combining data analysis and visualization techniques to transform raw cybersecurity data into meaningful insights that can be easily interpreted by security professionals, executives, and other stakeholders.
What the right candidate will enjoy!
Working for a company who serves over 80% of Fortune 500 Companies!
Working for a Fortune 100 Company!
Working for a company who won the ‘Competitive Strategy Leadership Award in 2022’ from Frost & Sullivan!
Working for a company who affordable medical, dental, and vision benefits as well as a 401k plan!
What the right candidate will have:
5+ years of BI experience and/or scripting
Experience querying data using SQL
Experience working extensively with Tableau to create complex dashboards from scratch
Experience using Alteryx and Python to pull in data
Experience building out dashboards for cybersecurity teams across the organization
Responsibilities of the right candidate:
Analyzing large volumes of cybersecurity data, including logs, network traffic, and system events, to identify patterns, anomalies, and potential security incidents.
Designing and developing effective data visualizations that present cybersecurity information in a clear, concise, and actionable manner.
Tailoring visualizations to meet the specific needs of different users, teams, or organizations, ensuring that the information presented is relevant and actionable.
Ensuring that data visualization practices comply with security and privacy regulations, and implementing measures to protect sensitive information.
Providing training and documentation to end-users on how to interpret and use cybersecurity visualizations effectively.
Collaborating with cybersecurity analysts, engineers, and other stakeholders to understand their requirements and incorporate feedback into the design and development of visualizations.
Show more
Show less","Data Visualization, Cybersecurity, Data Analysis, BI, SQL, Tableau, Alteryx, Python, Log Analysis, Network Traffic Analysis, System Events Analysis, Anomaly Detection, Security Incident Detection, Data Presentation, Dashboard Creation, User Interface Design, Security Regulations Compliance, Data Privacy Regulations Compliance, Training, Documentation, Collaboration","data visualization, cybersecurity, data analysis, bi, sql, tableau, alteryx, python, log analysis, network traffic analysis, system events analysis, anomaly detection, security incident detection, data presentation, dashboard creation, user interface design, security regulations compliance, data privacy regulations compliance, training, documentation, collaboration","alteryx, anomaly detection, bi, collaboration, cybersecurity, dashboard creation, data presentation, data privacy regulations compliance, dataanalytics, documentation, log analysis, network traffic analysis, python, security incident detection, security regulations compliance, sql, system events analysis, tableau, training, user interface design, visualization"
Senior Cloud Data Engineer,BDO USA,"Charlotte, NC",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765471242,2023-12-17,Denver,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, SQL, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, Computer Vision","data analytics, business intelligence, artificial intelligence, application development, sql, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, computer vision","ai algorithms, application development, artificial intelligence, automation tools, azure analysis services, batch data ingestion, business intelligence, c, cloud data analytics, computer vision, data lake medallion architecture, dataanalytics, datamodeling, datawarehouse, devops, git, java, linux, machine learning, microsoft fabric, powerbi, python, scala, semantic model definition, sql, star schema construction, streaming data ingestion, tabular modeling"
Sr. Data Engineer ( Data Integration) - STL,Brooksource,"Charlotte, NC",https://www.linkedin.com/jobs/view/sr-data-engineer-data-integration-stl-at-brooksource-3679137041,2023-12-17,Denver,United States,Mid senior,Hybrid,"<< Return to Search Results
ETL Developer – Data Integration
Charlotte, NC
12 month ongoing contract (1/4/21 – 12/31/21)
Brooksource is looking for an Data Integration ETL Developer for our Fortune 100 Telecommunications client in the Charlotte, NC area. The Data Integration ETL Developer will be an integral member of the Data Operations Architecture team. You will join the advertising/media organization and contribute to data solutions that power some of the largest and most successful marketing/advertising campaigns.
The primary responsibility of this role is to design and develop reusable components, code, and document custom automation frameworks, maintain scripts, and update these items as needed to build continuous delivery pipelines, ensuring rapid availability of the product.
This position requires strong collaboration with you Scrum Team (composed of Scrum Master, Product Owner, Lead Architect, Developers and QA) to understand business requirements and provide appropriate data integration solutions in alignment with solution implementation architecture.
Minimum Requirements
5+ years’ experience with the following:
Experienced in building Data Integration and Workflow Solutions and Extract, Transform, and Load (ETL) solutions for data warehousing
Strong data architecture knowledge around enterprise data warehousing concepts, SQL development and optimization, and data integration
Advanced SQL scripting and querying experience - Expert in creating SQL objects like Tables, Complex Stored Procedures, Triggers, Views, Indexes, and User Defined Functions to facilitate efficient data manipulation and consistent data storage.
Writing Python scripts with knowledge of relevant frameworks and libraries, such as workflow management utilities and libraries for accessing and extracting data.
Creating, scheduling and monitoring workflows - Airflow preferred
Hands-on experience with GitHub for version control
Experience in ETL performance testing, including data validation
Familiarity with Agile development methodologies such as Scrum and SAFe
Good team player, extremely adaptable and fast learner.
Experience with atleast one MPP database (Teradata, Netezza, Snowflake, MongoDB, etc.)
Experience with cloud infrastructure - AWS preferred
Nice To Have
Experience working with Media/Advertising MSO data and applications
Experience working with Snowflake and SnowSQL
Strong desire to mentor others in the data integration space, providing technical and business guidance.
Experience working with S3 buckets
Experience with Docker
Responsibilities
Design and develop reusable components, code, and document custom automation frameworks, maintain scripts, and update these items as needed to build continuous delivery pipelines, ensuring rapid availability of the product
Assist in requirements gathering and documents processes with internal stakeholders and collaborate with Product Owners, Scrum Master and other team members to determine data extraction and transformation requirements
Develop data mapping documentation to establish relationships between source and target tables including transformation processes.
Write and analyze complex SQL for the purpose of data extraction and processing - Design, develop, and deploy data movement using SQL server Integrations Services (SSIS), TSQL and stored procedures
Extract and transform data from multiple sources and load data into one or more destinations, and monitor integration and replication performance to ensure quality and stability.
Create, schedule and maintain data pipelines/workflows using Apache Airflow and Python
Design and develop ETL to load the data warehouse and data marts and test ETL data pipelines to maintain data infrastructures
Support testing efforts as a part of SaFE Agile team
Validate data quality and perform all aspects of verification, including functional, structural, regression and system testing
Work closely with Operations Production Support team in resolving escalated high priority incidents and the development coding issues.
Environment
Company size: Fortune 100
Hours: Standard 40-hour weeks
Dress code: Business casual
COVID Schedule: Rotational, 2-week onsite/remote schedule
Benefits Of Working With Brooksource
Direct communication with the hiring manager, which allows us to have a clear understanding of the timeline and move candidates through the interview process faster.
Dedication to keep an open line of communication and provide full transparency.
Sr. Data Engineer ( Data Integration) - STL Charlotte, North Carolina
Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.
JO-2010-105718
Show more
Show less","Data Warehousing, SQL Development, SQL Optimization, Data Integration, Python Scripting, Workflow Management, Data Extraction, Data Loading, Apache Airflow, GitHub, ETL Performance Testing, Data Validation, Agile Development, Scrum, SAFe, MPP Database, AWS, Snowflake, SnowSQL, S3 Buckets, Docker, SSIS, TSQL, Stored Procedures, Data Quality Validation, Functional Testing, Structural Testing, Regression Testing, System Testing","data warehousing, sql development, sql optimization, data integration, python scripting, workflow management, data extraction, data loading, apache airflow, github, etl performance testing, data validation, agile development, scrum, safe, mpp database, aws, snowflake, snowsql, s3 buckets, docker, ssis, tsql, stored procedures, data quality validation, functional testing, structural testing, regression testing, system testing","agile development, apache airflow, aws, data extraction, data integration, data loading, data quality validation, data validation, datawarehouse, docker, etl performance testing, functional testing, github, mpp database, python scripting, regression testing, s3 buckets, safe, scrum, snowflake, snowsql, sql development, sql optimization, ssis, stored procedures, structural testing, system testing, tsql, workflow management"
Stratascale Sr. Data Engineer,Stratascale – An SHI Company,"Charlotte, NC",https://www.linkedin.com/jobs/view/stratascale-sr-data-engineer-at-stratascale-%E2%80%93-an-shi-company-3775627737,2023-12-17,Denver,United States,Mid senior,Hybrid,"Job Summary
If you are interested in working in a startup like environment where you can directly influence the future of cloud-based solutions and services, then Stratascale is the place for you.
The Stratascale engineering team is at the forefront of cybersecurity innovation, providing cutting-edge solutions to protect global enterprises against evolving digital threats. Our commitment to excellence and innovation is matched by our dedication to creating an inclusive and collaborative work environment where each contribution is valued and every individual can thrive.
We are seeking a highly skilled and motivated Senior Data Engineer to join our software engineering organization. The ideal candidate will play a pivotal role in building data intelligence capabilities on top of our rich cybersecurity product data. Your work will directly contribute to enhancing our product's value by deriving actionable insights and enabling data-driven decision-making across the company.
This position will report to Charlotte, NC on a hybrid work schedule as determined by Stratascale management.
About Us
Stratascale, an SHI company, brings together the benefits of 31 years' experience delivering the very best technologies with a fresh consultative approach to designing, delivering and supporting the technology our customers need to transform their business. We call it Digital Agility.
To learn more about Stratascale visit our website: https://stratascale.com/
Responsibilities
Include but not limited to:
Design, build, and maintain scalable and robust data pipelines to support analytics and intelligence gathering from various cybersecurity product data sources.
Collaborate with cross-functional teams to understand data needs and implement systems for collecting, storing, processing, and analyzing large datasets.
Ensure data quality and integrity throughout all designed systems and processes.
Develop and optimize data models to improve storage efficiency and data retrieval performance.
Implement secure data handling practices to maintain the confidentiality and integrity of sensitive information in compliance with industry standards.
Stay abreast of emerging technologies and industry trends in data engineering and cybersecurity to drive continuous innovation.
Craft and maintain documentation regarding data engineering processes, systems, and data dictionaries to ensure transparency and knowledge sharing within the team.
Provide mentorship and guidance to junior data engineering staff.
Be on-call for services that the team owns.
Qualifications
Bachelor’s degree in Computer Science, Engineering, Data Science, or related field.
5+ years of experience in a data engineering role, with a proven track record of building high-volume data pipelines.
Required Skills
Experience with machine learning algorithms and data science techniques.
Strong programming skills in languages such as Python, Go, or Rust.
Excellent communication and teamwork abilities to effectively collaborate with both technical and non-technical teams.
Experience with Compute, Database, Storage, Networking, and Security services in at least one of the major cloud providers; AWS, Azure, or GCP.
Excellent written and verbal communication skills.
Excellent organizational and time management skills.
Ability to collaborative with other internal departments.
Ability to work independently as well as in a team environment.
Ability to adapt to changing technology environment.
Expertise in SQL and NoSQL databases, data warehousing solutions, and ETL tools.
Experience with big data technologies such as Hadoop, Spark, Kafka, etc.
Knowledge of cloud services (AWS, Azure, GCP) and their data-related offerings.
Familiarity with cybersecurity principles and experience working with cybersecurity data is a plus.
Strong analytical and problem-solving skills with an attention to detail.
Preferred
A master's degree or higher in a relevant field.
Certifications in cloud technologies or data engineering.
Experience supporting mission critical, 24x7 systems.
Experience with automation tools like CircleCI, GitLab, GitHub, or Jenkins.
Experience working within Agile/Scrum frameworks.
Certifications Required
Preferred: Experience with multiple data intelligence integrated platforms, AWS, Azure, Google data and analytics certifications
Additional Information
The estimated annual pay range for this position is $175,000 - $218,750 which includes a base salary and bonus. The compensation for this position is dependent on job-related knowledge, skills, experience, and market location and, therefore, will vary from individual to individual. Benefits may include, but are not limited to, medical, vision, dental, 401K, and flexible spending.
Equal Employment Opportunity – M/F/Disability/Protected Veteran Status
Compensation Structure
Base Plus Bonus
Approved Min (Total Target Comp)
USD $175,000.00/Yr.
Approved Max (Total Target Comp)
USD $218,750.00/Yr.
Job Wrapping 1
Show more
Show less","Data Engineering, Machine Learning, Python, Go, Rust, SQL, NoSQL, Hadoop, Spark, Kafka, AWS, Azure, GCP, Cybersecurity, Agile, Scrum, CircleCI, GitLab, GitHub, Jenkins","data engineering, machine learning, python, go, rust, sql, nosql, hadoop, spark, kafka, aws, azure, gcp, cybersecurity, agile, scrum, circleci, gitlab, github, jenkins","agile, aws, azure, circleci, cybersecurity, data engineering, gcp, github, gitlab, go, hadoop, jenkins, kafka, machine learning, nosql, python, rust, scrum, spark, sql"
HR Data Analyst,Obsidian HR,"Denver, NC",https://www.linkedin.com/jobs/view/hr-data-analyst-at-obsidian-hr-3769889496,2023-12-17,Denver,United States,Mid senior,Hybrid,"About Obsidian HR
We're a quickly-growing company on a mission to empower employers in Colorado to succeed by lifting the HR burdens from their shoulders! We partner with our clients to provide world-class caring service for them and their teams, along with the HR platform that they need. We partner with our customers to simplify their employee administration tasks and amplify their human resources function, enabling them to focus on the most important things: their work and their people.
What You'll Love About Obsidian HR
At Obsidian HR, we believe in working hard but doing so in a fun and stimulating environment. Some of the highlights include:
Our team. Our employees and customers consistently express the best thing about Obsidian HR is our team. We appreciate and have empathy for each other. And we stay customer-obsessed. When our customers win, we win. We are the F.A.C.E.S. (Fun. Appreciation. Customer-Obsessed. Empathy. Self-Awareness.) of our culture.
A commitment to holistic well being. Generous employer contributions toward medical benefits, paid holidays and PTO, paid parental leave, student loan repayment assistance and 401k match are just a few of the ways we demonstrate our commitment to our team's physical, mental and financial well being.
Professional Growth & Development. We offer opportunities for continuing education and make new opportunities available internally first.
At Obsidian HR, we learn and we play for keeps. We hold ourselves to the highest standard and strive towards excellence. However, we know (from experience!) that the road to excellence is paved with lessons learned and we give our team the space to take on new challenges, to make mistakes, and to learn from them!
What You'll Do
As a data analyst, your responsibility will be to collect, analyze, and interpret data for both our clients and internal stakeholders. This includes building custom reporting, data export/import templates, and constructing data visualizations. You will work mainly with data from our HRIS system, which consists of payroll and HR data. On a daily basis, you may be asked to construct simple to complex reports in a short period of time, jump into projects that require data analysis or manipulation, and work with client stakeholders to fulfill data requests.
Responsibility
Report Creation. Excellence with understanding data tables and connecting data points. Ability to create complex reports using various data systems and languages (i.e. SQL, Javascript).
Data Manipulation. Ability to summarize large amounts of data into easy to understand charts and graphs. Interpret data values and translate them into different data values for necessary data imports.
Product owner. Looks to own all aspects of report creation and data analysis.
Training & presenting. Learns and applies expert knowledge of our HRIS technology and data points to effectively train internal stakeholders and clients in a non-technical form factor.
Competencies
Be an owner with your responsibilities. We're depending on you!
Organized, with ability to prioritize. You'll face many competing demands on your time and you'll need to be good at juggling them and not dropping any balls. Priority and demands will vary based on the project.
Resourceful problem-solver. A can-do attitude with an ability to get your hands dirty while problem solving will make you an invaluable member of our team.
Flexible/Adaptable. You will be faced with many different data elements that present challenges. You will need to use various resources to tackle these elements.
Team player with strong communication. Making sure you keep your team and clients informed is key to making sure we all work together smoothly.
Experience
4+ years of prior experience data analytics or data management.
Associate's or Bachelors' Degree in Information Systems, Data management, Computer Science or related field..
Knowledge and/or prior experience working with HR technology is a plus but not required.
Knowledge and/or prior experience with CRM systems is a plus but not required.
Knowledge in SQL, Javascript, Excel, R, Python
Experience using Business Intelligence Tools and data frameworks with large data sets and relational databases
What Your Compensation Looks Like
Salary
: $65,000-75,000
Benefits And Perks
We invest in our people in many ways including training, career development, and competitive compensation. To name just a few of our benefits...
Company contribution towards individual healthcare plans (medical, drug, vision, and dental)
401k Company Match
Paid Parental Leave
Student Loan Repayment Assistance
Generous paid time off: including 7 observed holidays, floating holidays, and paid volunteer time
Flexible Spending, Life, and Accident insurance programs
Professional Development and Certification assistance
Quarterly company-wide events
Complimentary coffee, drinks, and snacks in the break room
Obsidian HR is an equal opportunity employer. All aspects of employment including the decision to hire, promote, discipline, or discharge, will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, and local law.
Show more
Show less","Data Analytics, Data Management, Data Manipulation, Business Intelligence, Report Creation, Data Import/Export, Data Visualizations, Data Interpretation, Data Presentation, SQL, JavaScript, Excel, R, Python, CRM, HRIS Technology","data analytics, data management, data manipulation, business intelligence, report creation, data importexport, data visualizations, data interpretation, data presentation, sql, javascript, excel, r, python, crm, hris technology","business intelligence, crm, data importexport, data interpretation, data management, data manipulation, data presentation, data visualizations, dataanalytics, excel, hris technology, javascript, python, r, report creation, sql"
Associate Principal - Data Engineering,GuruSchools LLC,"Charlotte, NC",https://www.linkedin.com/jobs/view/associate-principal-data-engineering-at-guruschools-llc-3774936595,2023-12-17,Denver,United States,Mid senior,Hybrid,"Job Title: Senior Data Engineer
Work Location
Charlotte , NC OR Phoenix, AZ
Job Description:
Data Engineer
Primary skills:
Azure Cloud experience – Data Engineer experience with using Azure Data Factory (ADF)
Proficient in Python
Compliance Technology experience (Actimize, ETL and experience with AML would be added advantage)
Experience with relational SQL and NoSQL databases
Strong analytical and communication skills: verbal and written.
Nice to have: Experience with Cosmos DB
Key Responsibilities:
Build frameworks for data ingestion pipeline for a variety of data sources: batch and real-time.
Participate in technical decisions.
Design, develop, test, and maintain data processing pipelines.
Design and build scalable, reliable data infrastructure with paramount focus on data quality, security, and privacy techniques.
Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (""LTIM""):
Benefits and Perks:
Comprehensive Medical Plan Covering Medical, Dental, Vision
Short Term and Long-Term Disability Coverage
401(k) Plan with Company match
Life Insurance
Vacation Time, Sick Leave, Paid Holidays
Paid Paternity and Maternity Leave
The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.
Disclaimer:
The compensation and benefits information provided herein is accurate as of the date of this posting.
LTIMindtree
is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
Safe return to office
:
In order to comply with LTIMindtree' s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree's COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree's applicable processes.
Show more
Show less","Azure Cloud, Azure Data Factory (ADF), Python, Compliance Technology, Actimize, ETL, AML, SQL, NoSQL, Cosmos DB, Data Ingestion, Data Processing, Data Quality, Data Security, Data Privacy","azure cloud, azure data factory adf, python, compliance technology, actimize, etl, aml, sql, nosql, cosmos db, data ingestion, data processing, data quality, data security, data privacy","actimize, aml, azure cloud, azure data factory adf, compliance technology, cosmos db, data ingestion, data privacy, data processing, data quality, data security, etl, nosql, python, sql"
Database Automation Engineer,Motion Recruitment,"Charlotte, NC",https://www.linkedin.com/jobs/view/database-automation-engineer-at-motion-recruitment-3730274533,2023-12-17,Denver,United States,Mid senior,Hybrid,"Great contract opportunity! A well-known Financial Services Company is looking for a Lead Technology Business Services Consultant in Charlotte, NC (Hybrid).
Work with the brightest minds at one of the largest financial institutions in the world. This is long-term contract opportunity that includes a competitive benefit package! Our client has been around for over 150 years and is continuously innovating in today's digital age. If you want to work for a company that is not only a household name, but also truly cares about satisfying customers' financial needs and helping people succeed financially, apply today.
Contract Duration: 6 Months
Required Skills & Experience
5-7 years of advanced database testing skills.
3+ years of Awetest Automation Framework development experience using Ruby & Cucumber.
Experience with Awetest setup, administration – including Awetest-Jira integration, Jenkins and Autosys integration with Awetest, Invoking Awetest jobs via API.
Should have ability to perform ETL testing.
Experience in testing web services and web service testing using Awetest Services Module.
Ability to write complex SQL queries and test batch SQL jobs.
Automation /coding skills.
5+ years of the end-to-end software development life cycle (SDLC) with emphasis on the test cycle including effort sizing, test approach design, performing complex data analysis, debugging SQL script, results reporting and documentation.
Extensive experience in doing integration testing across large applications to ensure data integrity across multiple systems.
Excellent written and oral communication skills with ability to clearly communicate ideas and results to diverse business and technical audiences.
Desired Skills & Experience
Acquired and prior working knowledge on the IHub application with a deep understanding of automated conformance testing using Awetest Data and Services framework.
Awetest deployment including Awetest-Shamisen setup and configuration is a huge plus.
Prior knowledge of Awetest Web module and framework.
Experience performing moderately complex to complex DB testing, test data conditioning, regression testing and testing validation.
5+ years working on multiple projects simultaneously. Analytical skills, ability to 'think outside the box' and move concepts to implementation.
Proven experience with automation, regression, and performance testing.
Strong analytical skills with high attention to detail and accuracy.
Advanced MS Office experience.
Experience in testing web services and web service testing tools like Postman, SoapUI.
5+ years of Information Technology in support of Business Services experience, or equivalent demonstrated through one or a combination of the following work experience, training, military experience, or education.
What You Will Be Doing
Build, maintain and execute a Data Automation Framework using Ruby, Cucumber and Awetest Framework.
Performs functional, Database (ETL), Web service, Regression testing during SDLC project lifecycle.
Strong analytical skills with high attention to detail and accuracy.
Create, parametrize & integrate complex SQL statements into the existing framework.
Logs, tracks, and verifies resolution of software and specification defects.
Creates, prepares, and implements systems quality assurance reviews for numerous applications using data scenarios/tests to validate.
Responsible for primary development of system testing strategies leveraging data.
Lead or participate in managing and supporting the delivery of technology products and services of high-profile business unit clients and contribute to large scale solutions planning related to Technology Business Services.
Ensure service quality and cost effectiveness of solutions.
Review customer information with the technology team and implement technical initiatives for highly complex projects impacting multiple lines of business or across the enterprise.
Understand and ensure compliance and risk management requirements for supported area and work with other stakeholders to implement key risk initiatives.
Make decisions in complex and multifaceted situations requiring solid understanding of managing multiple medium technology development efforts that influence and lead broader work team and drive new initiatives.
Collaborate and consult with the technology team and more experienced management within Technology Business Services functional area to resolve Information Technology issues and provide customers with best possible solutions.
Provide guidance on leveraging new technology.
Posted By:
Rene Sadlier
Show more
Show less","Data Automation Framework, ETL testing, Cucumber, Ruby, SQL, Jenkins, Software development life cycle (SDLC), Data integrity, Integration testing, IHub application, Awetest Data and Services framework, AwetestShamisen setup, Awetest Web module and framework, Regression testing, Postman, SoapUI, MS Office, Functional testing, Database testing, Web service testing, Quality assurance, Risk management, Solution planning, Collaboration","data automation framework, etl testing, cucumber, ruby, sql, jenkins, software development life cycle sdlc, data integrity, integration testing, ihub application, awetest data and services framework, awetestshamisen setup, awetest web module and framework, regression testing, postman, soapui, ms office, functional testing, database testing, web service testing, quality assurance, risk management, solution planning, collaboration","awetest data and services framework, awetest web module and framework, awetestshamisen setup, collaboration, cucumber, data automation framework, data integrity, database testing, etl testing, functional testing, ihub application, integration testing, jenkins, ms office, postman, quality assurance, regression testing, risk management, ruby, soapui, software development life cycle sdlc, solution planning, sql, web service testing"
Associate Principal - Data Engineering,LTIMindtree,"Charlotte, NC",https://www.linkedin.com/jobs/view/associate-principal-data-engineering-at-ltimindtree-3772900336,2023-12-17,Denver,United States,Mid senior,Hybrid,"About Us:
LTIMindtree
is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com.
Job Title: Senior Data Engineer
Work Location
Charlotte , NC OR Phoenix, AZ
Job Description:
Data Engineer
Primary skills:
Azure Cloud experience – Data Engineer experience with using Azure Data Factory (ADF)
Proficient in Python
Compliance Technology experience (Actimize, ETL and experience with AML would be added advantage)
Experience with relational SQL and NoSQL databases
Strong analytical and communication skills: verbal and written.
Nice to have: Experience with Cosmos DB
Key Responsibilities:
Build frameworks for data ingestion pipeline for a variety of data sources: batch and real-time.
Participate in technical decisions.
Design, develop, test, and maintain data processing pipelines.
Design and build scalable, reliable data infrastructure with paramount focus on data quality, security, and privacy techniques.
Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”):
Benefits and Perks:
Comprehensive Medical Plan Covering Medical, Dental, Vision
Short Term and Long-Term Disability Coverage
401(k) Plan with Company match
Life Insurance
Vacation Time, Sick Leave, Paid Holidays
Paid Paternity and Maternity Leave
The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.
Disclaimer:
The compensation and benefits information provided herein is accurate as of the date of this posting.
LTIMindtree
is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
Safe return to office
:
In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.
Show more
Show less","Azure Cloud, Data Engineer, Azure Data Factory (ADF), Python, Apache Spark, Compliance Technology, Actimize, ETL, AML, SQL, NoSQL, Cosmos DB, Data ingestion, Data processing, Data infrastructure, Data quality, Data security, Data privacy","azure cloud, data engineer, azure data factory adf, python, apache spark, compliance technology, actimize, etl, aml, sql, nosql, cosmos db, data ingestion, data processing, data infrastructure, data quality, data security, data privacy","actimize, aml, apache spark, azure cloud, azure data factory adf, compliance technology, cosmos db, data infrastructure, data ingestion, data privacy, data processing, data quality, data security, dataengineering, etl, nosql, python, sql"
Data Scientist,Net2Source Inc.,"West Sacramento, CA",https://www.linkedin.com/jobs/view/data-scientist-at-net2source-inc-3787116809,2023-12-17,Sacramento,United States,Associate,Onsite,"Please find below mentioned job details:
Job Title- Data Scientist
Job Location- West Sacramento CA 95691
Job Duration- 09 months Contract
Payrate :$40- $60/hr. on w2 without benefits
Description:
The Data Scientist will be responsible for data analytics processes and results for the Platform R&D team. This position will be a contributor on the team responsible for:
• Identifying customer populations and key data elements in support of data enrichment and analysis input.
• Define and build data repositories to support long-term analytics needs and centralize commonly referenced data.
• Identifying data sources and data gaps.
• Perform analytics required to support various projects or information requests.
• Identify requirement for and work with teams to build reports to support ongoing information needs relative to processes and controls.
• Defining and executing queries or logic to retrieve data from various sources within and possibly outside
• Analyzing data and creating summaries in MS Word or PowerPoint for engineering lead review and communication.
Required Qualifications
• Bachelors degree in a quantitative field such as applied math, statistics, engineering, physics, accounting, finance, economics, econometrics, computer sciences, or business AND 2+ years of experience in one or a combination of the following: reporting, analytics, or modeling.
Desired Qualifications
• SQL and R programming experience
• Statistical analysis experience (regression, correlation, variability analysis)
• Ability to manage multiple and competing priorities
• Ability to work independently
• Excellent verbal, written, and interpersonal communication skills
• Extensive knowledge and understanding of research and analysis
• Strong analytical skills with high attention to detail and accuracy
Other Desired Qualifications
• 1+ years' experience in performing in a data science role within analytics teams
• Experience implementing and defining process to support data analytics and result communication.
• Experience building and retaining documentation to track analysis results and logic
• Experience working with multiple database solutions and retrieving and consolidating data from disparate information sources
• Experience as data scientist in the medical laboratory instrumentation line of business.
Show more
Show less","Data Analytics, Data Enrichment, Data Repositories, Database Solutions, Data Science, Data Scientist, Engineering, MS Word, PowerPoint, R Programming, Regression, Reporting, SQL, Statistical Analysis, Statistics","data analytics, data enrichment, data repositories, database solutions, data science, data scientist, engineering, ms word, powerpoint, r programming, regression, reporting, sql, statistical analysis, statistics","data enrichment, data repositories, data science, data scientist, dataanalytics, database solutions, engineering, ms word, powerpoint, r programming, regression, reporting, sql, statistical analysis, statistics"
Data Scientist (Medical Device),Net2Source Inc.,"West Sacramento, CA",https://www.linkedin.com/jobs/view/data-scientist-medical-device-at-net2source-inc-3787105245,2023-12-17,Sacramento,United States,Associate,Onsite,"Title: Data Scientist
Duration: 9 Months Contract
Location: West Sacramento, CA 95691
Pay Rate: $60/hr on W2 (without benefits)
Description/Comment:
The Data Scientist will be responsible for data analytics processes and results for the Platform R&D team. This position will be a contributor on the team responsible for:
• Identifying customer populations and key data elements in support of data enrichment and analysis input
• Define and build data repositories to support long-term analytics needs and centralize commonly referenced data
• Identifying data sources and data gaps
• Perform analytics required to support various projects or information requests
• Identify requirement for and work with teams to build reports to support ongoing information needs relative to Beckman Coulter processes and controls
• Defining and executing queries or logic to retrieve data from various sources within and possibly outside of Beckman Coulter
• Analyzing data and creating summaries in MS Word or PowerPoint for engineering lead review and communication
Required Qualifications:
• Bachelor's degree in a quantitative field such as applied math, statistics, engineering, physics, accounting, finance, economics, econometrics, computer sciences, or business AND 2+ years of experience in one or a combination of the following: reporting, analytics, or modeling.
Desired Qualifications:
• SQL and R programming experience
• Statistical analysis experience (regression, correlation, variability analysis)
• Ability to manage multiple and competing priorities
• Ability to work independently
• Excellent verbal, written, and interpersonal communication skills
• Extensive knowledge and understanding of research and analysis
• Strong analytical skills with high attention to detail and accuracy
Other Desired Qualifications:
• 1+ years’ experience in performing in a data science role within analytics teams
• Experience implementing and defining processes to support data analytics and result communication
• Experience building and retaining documentation to track analysis results and logic
• Experience working with multiple database solutions and retrieving and consolidating data from disparate information sources
• Experience as a data scientist in the medical laboratory instrumentation line of business.
Show more
Show less","Data Analytics, Data Enrichment, Data Analysis, Data Reporting, Data Warehousing, Data Mining, Statistical Analysis, Regression Analysis, Correlation Analysis, Variability Analysis, SQL, R Programming, Data Science, Research, MS Word, PowerPoint","data analytics, data enrichment, data analysis, data reporting, data warehousing, data mining, statistical analysis, regression analysis, correlation analysis, variability analysis, sql, r programming, data science, research, ms word, powerpoint","correlation analysis, data enrichment, data mining, data reporting, data science, dataanalytics, datawarehouse, ms word, powerpoint, r programming, regression analysis, research, sql, statistical analysis, variability analysis"
Data Analyst,Zealthy,"Miami, FL",https://www.linkedin.com/jobs/view/data-analyst-at-zealthy-3763373151,2023-12-17,Florida,United States,Associate,Onsite,"Zealthy is a telemedicine company on a mission to provide critical healthcare services to all who need them. We firmly believe that one’s location or financial means should not limit their ability to access the highest quality of care. At Zealthy, the culture is collaborative, fun, and open, with a small office located in Miami Beach, Florida. While the Founder previously built a unicorn healthcare startup, it is a lean, flat culture where everyone’s perspective is valued. As such, you will need to be prepared to move quickly, iterate often, and be open to new ideas and experimentation. We work hard at Zealthy to ensure that patients can access high-quality healthcare from anywhere. And as the organization scales, you will have the opportunity to develop alongside it into a leadership position if that is an ambition of yours. Because we believe that this culture is integral to our success, we require in-person work. As this is a start-up, you will need to be comfortable working hard Monday-Friday.
As the first data analytics hire, you will pave the way for data analysis at the company. We are currently seeking a highly skilled and innovative individual to pioneer the development of data analysis capabilities within our organization. This role presents a unique chance to shape and lead data-driven initiatives, laying the foundation for a robust data analytics function within the company.
Responsibilities:
Set up initial data stack for team
Collect, clean, and analyze large datasets to extract meaningful insights. Utilize statistical methods to interpret data trends and patterns.
Create clear and concise reports and dashboards to communicate data-driven insights to various stakeholders.
Ensure data accuracy and reliability by developing and implementing data quality checks. Identify and address data discrepancies or anomalies.
Collaborate with cross-functional teams to understand business requirements and provide analytical support for decision-making. Identify opportunities for improvements and optimizations.
Develop and implement predictive models to forecast trends and support future business planning. Continuously refine and enhance models based on feedback and evolving business needs.
Discover hidden patterns and trends within large datasets to inform strategic business decisions. Conduct exploratory data analysis to uncover insights and opportunities.
Qualifications:
Must have experience with SQL.
Bachelor's degree in a related field such as Statistics, Mathematics, Computer Science, or a related quantitative discipline.
At least of 1 year of experience in management consulting, investment banking, data, or at startups generally
Strong analytical and problem-solving skills, with attention to detail.
Excellent communication skills with the ability to convey complex findings in a clear and understandable manner.
Familiarity with database management and data warehousing concepts.
Knowledge of statistical methods and predictive modeling techniques.
Show more
Show less","SQL, Data Analysis, Data Analytics, Data Warehousing, Statistical Analysis, Data Quality Management, Data Visualization, DataDriven Decision Making, Exploratory Data Analysis, Predictive Modeling, Machine Learning, Business Intelligence, Statistical Methods, Communication Skills","sql, data analysis, data analytics, data warehousing, statistical analysis, data quality management, data visualization, datadriven decision making, exploratory data analysis, predictive modeling, machine learning, business intelligence, statistical methods, communication skills","business intelligence, communication skills, data quality management, dataanalytics, datadriven decision making, datawarehouse, exploratory data analysis, machine learning, predictive modeling, sql, statistical analysis, statistical methods, visualization"
Data Center Engineer,Verizon,"Pompano Beach, FL",https://www.linkedin.com/jobs/view/data-center-engineer-at-verizon-3776444872,2023-12-17,Florida,United States,Associate,Onsite,"When you join Verizon
Verizon is one of the world's leading providers of technology and communications services, transforming the way we connect around the world. We're a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together-lifting up our communities and striving to make an impact to move the world forward. If you're fueled by purpose, and powered by persistence, explore a career with us. Here, you'll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.
What you'll be doing...
Continuous improvement efforts drive our business and you will work with customer technical teams to provide solutions for customers' internetworking communications requirements. You will be performing network design activity for our large enterprise customer data networks. As the main interface to customers in support of their WAN/LAN enterprise projects, you will make us better by building great enterprise customer data networks.
Investigating, crafting, planning, and implementing communications networks.
Partnering with customer teams to develop solutions that support their requirements.
Preparing rack elevation schematics to define and illustrate network design solutions.
Crafting hardware and software configuration upgrades.
Preparing detailed work requests for technicians.
Installing hardware and cabling into datacenters in coordiation with other engineering teams.
Perform smarthands activities on location for remote engineers.
Manage work orders that impact the datacenter as the on-site engineering lead.
Remotely assist other engineers with scheduled maintenance and other tasks
What we're looking for...
Your technical skills are top notch, and you are known for digging into the details to find the best solutions. You know how to get to the bottom of your customers' needs, and how to manage your partners' expectations. Talking to people comes very naturally to you and you have an ability to communicate complex information in a way that people understand - whether verbal or written.
You'll need to have:
Bachelor's degree or four or more years of work experience.
Four or more years of relevant work experience.
Prior experience operating datacenters and overseeing implementation in support of customer buildouts.
Must be able to pass an extensive background investigation as a condition of employment.
Even better if you have one or more of the following:
A degree.
Cisco Certified Network Administrator (CCNA) certification.
Project management experience.
Experience with contemporary technologies including MPLS, TCP/IP, IPSec, BGP, QoS, IP Telephony, Multicast, and/or other similar tools.
Designed and implemented large scale datacenter operations.
Why Verizon?
Verizon is committed to maintaining a Total Rewards package which is competitive, valued by our employees, and differentiates us as an Employer of Choice.
We are a 'pay for performance' company and your contribution is rewarded through competitive salaries, performance-based incentives and an employee Stock Program. We create an opportunity for us all to share in the success of Verizon and the value we help to create through this broad-based discretionary equity award program.
Your benefits are market competitive and delivered by some of the best providers.
You are provided with a full spectrum of health and wellbeing resources, including a first in-class Employee Assistance Program, to empower you to make positive health decisions.
We offer generous paid time off benefits.
Verizon provides training and development for all levels, to help you enhance your skills and develop your career, from funding towards education assistance, award-winning training, online development tools and access to industry research.
You will be able to take part in volunteering opportunities as part of our environmental, community and sustainability commitment.
If Verizon and this role sound like a fit for you, we encourage you to apply even if you don't meet every ""even better"" qualification listed above.
#STSERP22
Where you'll be working
In this worksite-based role, you'll work onsite at a defined location(s).
Scheduled Weekly Hours
40
Equal Employment Opportunity
We're proud to be an equal opportunity employer - and celebrate our employees' differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.
Show more
Show less","Networking, Data Center Operations, Project Management, CCNA, MPLS, TCP/IP, IPSec, BGP, QoS, IP Telephony, Multicast, Rack Elevation Schematics, Hardware Configuration, Software Configuration, Work Requests, Smarthands Activities, Work Orders, Background Investigation, Employee Assistance Program, Education Assistance, Online Development Tools, Industry Research, Volunteering Opportunities","networking, data center operations, project management, ccna, mpls, tcpip, ipsec, bgp, qos, ip telephony, multicast, rack elevation schematics, hardware configuration, software configuration, work requests, smarthands activities, work orders, background investigation, employee assistance program, education assistance, online development tools, industry research, volunteering opportunities","background investigation, bgp, ccna, data center operations, education assistance, employee assistance program, hardware configuration, industry research, ip telephony, ipsec, mpls, multicast, networking, online development tools, project management, qos, rack elevation schematics, smarthands activities, software configuration, tcpip, volunteering opportunities, work orders, work requests"
Business Data Analyst,"Pinnacle Group, Inc.","Tampa, FL",https://www.linkedin.com/jobs/view/business-data-analyst-at-pinnacle-group-inc-3787306721,2023-12-17,Florida,United States,Associate,Onsite,"Title:
Business Data Analyst
Location:
Tampa, FL
Job Description:
The IT Business Senior Analyst is an intermediate-level position responsible for liaising between business users and technologists to exchange information in a concise, logical and understandable way in coordination with the Technology team. The overall objective of this role is to contribute to continuous iterative exploration and investigation of business performance and other measures to gain insight and drive business planning.
Responsibilities:
• Support system change processes from requirements through implementation and provide input based on analysis of information
• Consult with business clients to determine system functional specifications and provides user and operational support
• Work with business clients providing demos and manage User Acceptance Testing (UAT) cycles
• Create user guides, release notes and other supporting documentation
• Act as SME to stakeholders and /or other team members.
• Support day today business queries as a SME of the application
• Be the advisor or coach to new or lower level analysts and work as a team to achieve business objectives, performing other duties and functions as assigned
• Has the ability to operate with a limited level of direct supervision.
• Can exercise independence of judgement and autonomy.
• Appropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding client, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.
Qualifications:
• 4-6 years of relevant experience
• Previous experience working as a Business Analyst in
Agile methodology and/or SDLC is a must
• Beginner to intermediate knowledge of SQL
• Ability to perform in
client facing situations
• Experience in data analysis with intermediate/advanced Microsoft Office Suite skills
• Proven interpersonal, data analysis, diplomatic, management and prioritization skills
• Consistently demonstrate clear and concise written and verbal communication
• Proven ability to manage multiple activities and build/develop working relationships
• Proven self-motivation to take initiative and master new tasks quickly
• Demonstrated ability to work under pressure to meet tight deadlines and approach work methodically with attention to detail
Show more
Show less","Business Analysis, Agile, SDLC, SQL, Microsoft Office Suite, Data Analysis, Communication, Prioritization, Time Management, Teamwork, Initiative, Detailoriented","business analysis, agile, sdlc, sql, microsoft office suite, data analysis, communication, prioritization, time management, teamwork, initiative, detailoriented","agile, business analysis, communication, dataanalytics, detailoriented, initiative, microsoft office suite, prioritization, sdlc, sql, teamwork, time management"
Distributed Systems Engineer - Analytical Database Platform,Cloudflare,"Miami, FL",https://www.linkedin.com/jobs/view/distributed-systems-engineer-analytical-database-platform-at-cloudflare-3776254882,2023-12-17,Florida,United States,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
About Role
We are looking for an experienced and highly motivated engineer to join our team and contribute to our analytical database platform. The platform is a critical component of Cloudflare Analytics which provides real-time visibility into the health and performance of Cloudflare customers' online properties.
The team builds and maintains a high-performance, scalable database platform powered by ClickHouse, optimized for analytical workloads. We help our customers, both internal and external, to gain a deeper understanding of their online properties, identify trends and patterns, and make informed decisions about how to optimize their web performance, security, and other key metrics. Our mission is to empower customers to leverage their data to drive better outcomes for their business.
As a Distributed systems engineer - Analytical Database Platform, you will:
Develop and implement new platform components for the Cloudflare Analytical Database Platform to improve functionality and performance.
Add more database clusters to accommodate the growing volume of data generated by Cloudflare products and services.
Monitor and maintain the performance and reliability of existing database platform clusters, and identify and troubleshoot any issues that may arise.
Work to identify and remove bottlenecks within the analytics database platform, including optimizing query performance and streamlining data ingestion processes.
Collaborate with the ClickHouse open-source community to add new features and functionality to the database, as well as contribute to the development of the upstream codebase.
Collaborate with other teams across Cloudflare to understand their data needs and build solutions that empower them to make data-driven decisions.
Participate in the development of the next generation of the database platform engine, including researching and evaluating new technologies and approaches that can improve the database's performance and scalability.
Key qualifications:
3+ years of experience working in software development covering distributed systems, and databases.
Strong programming skills (C++ is preferable), as well as a deep understanding of software development best practices and principles.
Strong knowledge of SQL and database internals, including experience with database design, optimization, and performance tuning.
A solid foundation in computer science, including algorithms, data structures, distributed systems, and concurrency.
Ability to work collaboratively in a team environment, as well as communicate effectively with other teams across Cloudflare.
Strong analytical and problem-solving skills, as well as the ability to work independently and proactively identify and solve issues.
Experience with ClickHouse is a plus.
Experience with SALT or Terraform is a plus.
Experience with Linux container technologies, such as Docker and Kubernetes, is a plus.
If you're passionate about building scalable and performant databases using cutting-edge technologies, and want to work with a world-class team of engineers, then we want to hear from you! Join us in our mission to help build a better internet for everyone!
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Linux, SQL, Kubernetes, Docker, C++, SALT, Terraform, ClickHouse, Distributed systems, Databases, Software development, Data structures, Algorithms, Concurrency, Computer science, Data science","linux, sql, kubernetes, docker, c, salt, terraform, clickhouse, distributed systems, databases, software development, data structures, algorithms, concurrency, computer science, data science","algorithms, c, clickhouse, computer science, concurrency, data science, data structures, databases, distributed systems, docker, kubernetes, linux, salt, software development, sql, terraform"
"Data Analyst, Asset Management",Royal Caribbean Group,"Miami, FL",https://www.linkedin.com/jobs/view/data-analyst-asset-management-at-royal-caribbean-group-3752358501,2023-12-17,Florida,United States,Associate,Onsite,"Journey with us!
Combine your career goals and sense of adventure by joining our exciting team of employees. Royal Caribbean Group is pleased to offer a competitive compensation & benefits package, and excellent career development opportunities, each offering unique ways to explore the world.
Position Summary: ***This is NOT a remote position. Selected candidate will work on site at office in Miami, Florida***
Data Analyst, Asset Management is responsible for providing detailed analytical high-level status reports to support all Royal Caribbean Group brand vessels with Asset Management initiatives, contribute, and support the Asset Management Digitalization strategy and lead cross functional collaboration across the organization (IT, Newbuild, Supply Chain, Ship/Shore Operations and Finance) to deliver high-quality and successful projects and strive for continuous improvement.
Main responsibilities of the position include but not limited to, contribute, and maintain data governance strategy that delivers trusted data at the speed of the business, grow, and manage the Asset Management data platform and spearhead the effort to automate and digitize the reporting catalog with tools such as Azure/Power BI/Tableau.
Position will support tracking and establishing standardized operational metrics and data driven performance monitoring by using predictive and descriptive analytics for Inventory/Maintenance optimization, data governance and enterprise applications. This will include consistent collaboration with the vessel operations (shipboard visits) and internal/external stakeholders.
Essential Duties And Responsibilities
Contribute and guide Development for Asset Data Platform, AMOS, APIs, Data Governance, Inventory/Maintenance Optimization, AMOS Registrations and other AM applications.
Data/Technical role for projects within the Asset Management department
Strive for continuous improvement of data quality and standardization across the fleet.
Support Asset Management projects ensuring adherence to PMO and Global Project Management Standards
Participate and support Asset Management Digitalization Strategy in coordination with PMO to expand business analysis capabilities of the Marine Operation and drive data driven decision making.
Works closely with key internal/external stakeholders to understand how to adapt and evolve the strategy for delivering high-quality and successful projects.
Facilitates communication between internal and external stakeholders.
Continuous research on improved efficiencies of current products and its content including new innovations.
Remains current with industry trends, especially as related to contemporary management techniques.
Performs other duties as required. This job description in no way states or implies that these are the only duties to be performed by the employee occupying this position. Employees will be required to perform any other job-related duties assigned by their supervisor or management.
Qualifications
Bachelor’s degree in Business/Technology/Marine or related field preferred
1-2 years of experience with ERP systems, AMOS experience preferred but not mandatory
1-2 years of experience developing reports and dashboards with PowerBI or Tableau.
Expert in Data Management/Optimization (Technical, Business, Communications, Digitalization)
In depth understanding of project management processes. Professional accreditation preferred.
Expert knowledge of database architecture, content, and development
Experience with APIs, Oracle, SQLSVR, Azure and Python preferred.
Sound knowledge of Microsoft Products such as Excel, Visio, Word, Azure, Power BI, SharePoint, and Project are preferred.
Experience in the travel/cruise industry preferred
Fluent in English
Knowledge And Skills
Candidate must possess a combination of strategy, execution, organizational and communication skills.
Ability to analyze data and research results to make recommendations.
Ability to apply strong analytical, financial, conceptual, and strategic thinking.
Able to manage and prioritize multiple projects simultaneously.
Experience in managing internal and/or cross functional teams.
Ability to assemble, lead and influence cross functional teams to develop strategies that include input and buy-in from all stakeholders.
Proven ability to collaborate with others.
We know there is a lot to consider.
As you go through the application process, our recruiters will be glad to provide guidance, and more relevant details to answer any additional questions. Thank you again for your interest in Royal Caribbean Group. We will hope to see you onboard soon!
It is the policy of the Company to ensure equal employment and promotion opportunity to qualified candidates without discrimination or harassment on the basis of race, color, religion, sex, age, national origin, disability, sexual orientation, sexuality, gender identity or expression, marital status, or any other characteristic protected by law. Royal Caribbean Group and each of its subsidiaries prohibit and will not tolerate discrimination or harassment.
Show more
Show less","Data Analysis, Business Intelligence, Azure, Power BI, Tableau, Oracle, SQL Server, APIs, Python, Microsoft Products, Excel, Visio, Word, SharePoint, Project, ERP Systems, AMOS, Data Governance, Project Management, Database Architecture, Strategy, Execution, Organizational Skills, Communication Skills, Analytical Thinking, Strategic Thinking, Project Management, CrossFunctional Teams","data analysis, business intelligence, azure, power bi, tableau, oracle, sql server, apis, python, microsoft products, excel, visio, word, sharepoint, project, erp systems, amos, data governance, project management, database architecture, strategy, execution, organizational skills, communication skills, analytical thinking, strategic thinking, project management, crossfunctional teams","amos, analytical thinking, apis, azure, business intelligence, communication skills, crossfunctional teams, data governance, dataanalytics, database architecture, erp systems, excel, execution, microsoft products, oracle, organizational skills, powerbi, project, project management, python, sharepoint, sql server, strategic thinking, strategy, tableau, visio, word"
Data Programmer Analyst II,Kforce Inc,"Davie, FL",https://www.linkedin.com/jobs/view/data-programmer-analyst-ii-at-kforce-inc-3782228575,2023-12-17,Florida,United States,Associate,Onsite,"Responsibilities
Kforce has a client that is seeking a Data Programmer Analyst II in Davie, FL. Responsibilities:
In this role, the Data Programmer Analyst II will write complex queries to perform data analysis to ensure the accuracy and integrity of data
Identify problematic areas and conduct research to determine the best course of action to solve the problems
Troubleshoot and research data anomalies and recommend corrective actions
Monitor for timely and accurate completion of data processes and scheduled jobs and troubleshoot as necessary
Identify, analyze, and interpret trends or patterns in complex data sets
In collaboration with business owners, the Data Programmer Analyst II will interpret data and develop recommendations based on findings
Develop dashboard, graphs, reports, and presentations of project results
Collaborate with the database administration and infrastructure teams
Perform basic statistical analyses for projects and reports
Provides timely and accurate progress information to project status reports
Provides technical assistance to other developers and business analysts when needed
Works on multiple high priority projects concurrently
Employs productivity aids in all aspects of assignments
Maintains and enhances technical skills through formal and on-the-job training
Responds to and assists with other problem tickets as required
Performs other duties as assigned
Requirements
Bachelor's or Associate degree in Information Technology, Computer Science, Mathematics, Statistics, or related fields preferred or equivalent experience
3-5 years of related experience as BI Developer or similar position
Experience with the T-SQL scripting/programming language, including the ability to develop, test, and debug ad-hoc queries, stored procedures, and data migration scripts
Experience with Microsoft Reporting and Tools (SSIS, SSRS, Data Integration Platform, etc.)
Experience with BI Development Tools (Tableau, MicroStrategy, Power BI, etc.)
Experience with Data modeling and management on Cloud platform
Strong analysis, coding, testing, documenting, and implementation experience in both a development and maintenance environment
Knowledge of standard data modeling patterns and practices, e.g., Relational and Dimensional models
Basic knowledge of statistics
Solid understanding of ETL strategies and design, (staging environments, data transformation, change data capture, slowly changing dimensions, etc.), with an eye towards delivering functional and useful solutions in a timely manner
Excellent troubleshooting and problem-solving skills
Exceptional interpersonal and communication skills with the ability to deal with a diverse range of people, which includes the upper levels of corporate management
Self-sufficient, requiring limited supervision over job knowledge, expectations, and successful project completion
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $60 - $65 per hour
Show more
Show less","Data Analysis, Data Quality Management, Data Integration, Business Intelligence, TSQL scripting/programming, Microsoft Reporting and Tools (SSIS SSRS Data Integration Platform), BI Development Tools (Tableau MicroStrategy Power BI), Data Modeling, Data Management, Cloud Platform, ETL, Statistics, Troubleshooting, ProblemSolving, Interpersonal Skills, Communication Skills","data analysis, data quality management, data integration, business intelligence, tsql scriptingprogramming, microsoft reporting and tools ssis ssrs data integration platform, bi development tools tableau microstrategy power bi, data modeling, data management, cloud platform, etl, statistics, troubleshooting, problemsolving, interpersonal skills, communication skills","bi development tools tableau microstrategy power bi, business intelligence, cloud platform, communication skills, data integration, data management, data quality management, dataanalytics, datamodeling, etl, interpersonal skills, microsoft reporting and tools ssis ssrs data integration platform, problemsolving, statistics, troubleshooting, tsql scriptingprogramming"
Data Warehouse Engineer,How To MANAGE a Small Law Firm,"Miami, FL",https://www.linkedin.com/jobs/view/data-warehouse-engineer-at-how-to-manage-a-small-law-firm-3769535249,2023-12-17,Florida,United States,Associate,Remote,"Warning:
This isn’t a normal, typical, run-of-the-mill, job posting for a Data Warehouse Engineer.
Please read the questions below. If your answer to each question is “Yes” then keep reading. If you run across a “No” please stop reading and move on to the next job post. (It will save us both valuable time). Fair enough?
Are you someone who has built data warehouses? Do the words Extract, Transform, and Load mean anything to you?
Do you have strong communication and teamwork skills? Can you handle working with multiple personalities with the same goal in mind?
Not only do you know what we mean we say structured vs unstructured data, but you aren’t afraid of unstructured data?
Have you not only heard of but also have experience with Azure, AWS, PowerBI, and/or Neo4J?
Would you say that you have the ability to “thrive” in a fast-paced environment where priorities shift and requirements change on the regular? In other words, do you have a realistic plan for dealing with change requests that keeps everyone happy, yet still meets deadlines?
While gaining your experience did you make mistakes and were the mistakes your fault? Did you own them? Learn from them? Put things in place to prevent them in the future?
Do you learn quickly? And if so, when you learn something you believe to be true today, and then that truth changes tomorrow, are you the kind of person who can accept that change, and roll with it?
If you are still reading this, then you just answered
“Yes”
to all the above, and that is fairly impressive!
Keep reading:
At How to Manage a Small Law Firm, we believe in the power of data to drive informed decisions and foster growth. As a Data Warehouse Engineer, you will play a crucial role in the development, construction, maintenance, and support of our data warehouse. This dynamic position requires a keen understanding of data extraction, transformation, and loading (ETL) processes, data modeling, and architecture. You will collaborate closely with data analysts, data scientists, and other stakeholders to provide data solutions that align with our organizational objectives.
Now the real question- do you have what it takes to join the team? We probably want to talk to you if you have the following:
• STRONG experience in prior data warehouse design, ETL, and data modeling roles (think multiple years of experience!)
• You know all about ER diagramming tools, data graphing, and SQL/ NoSQL databases.
• You know how to adapt to changes quickly and can pivot from project to project without getting dizzy
• You’ve not just managed databases but can show you were SUCCESSFUL at the maintenance, tuning, and upkeep of those databases.
If the above has you nodding your head affirming that “yeah, that sounds like me”, then we’d love to hear about your experiences.
IMPORTANT:
please complete the full application via the external link provided. Applications through LinkedIn will NOT be accepted. Start this process off right by showing us how well you can follow these instructions!
Now More About Us:
How to MANAGE a Small Law Firm is a fast-growing and highly entrepreneurial business management, personal development and coaching company, specializing in solo and small law firms. Inc. Magazine has named How to MANAGE a Small Law Firm to its list of the 5,000 fastest growing privately held companies in the United States every year since 2015. We've also been named by Inc. as one of the ""Best Places To Work"". We are very much a “start-up” with a fast-paced growth focused environment. Working with us can prove to be the best job you ever had or the most frustrating depending on your flexibility, team-spirit, commitment to our clients and your realistic expectations about what it’s like to work in a fast-paced growing business.
Our culture....
We are a rapidly growing company, and we are seeking an individual who can keep up and can maintain a comfortable presence in our ever-changing environment. If you're intellectually curious, ready to learn, and a team player - this will be the perfect role for you! If you're not a self-starter, need a lot of supervision, or are looking for a 9 - 5 that won't challenge you, we're not it.
HTM provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Show more
Show less","Data Warehousing, ETL Processes, Data Modeling, Data Architecture, Data Analytics, Data Science, ER Diagramming, SQL, NoSQL, Agile Development, Database Tuning, Database Upkeep, Data Extraction, Data Transformation, Data Loading, Communication, Teamwork, Adaptability, Problem Solving, Critical Thinking, Attention to Detail, Time Management, Flexibility, SelfMotivation, Initiative, Learning Agility, Commitment to Excellence","data warehousing, etl processes, data modeling, data architecture, data analytics, data science, er diagramming, sql, nosql, agile development, database tuning, database upkeep, data extraction, data transformation, data loading, communication, teamwork, adaptability, problem solving, critical thinking, attention to detail, time management, flexibility, selfmotivation, initiative, learning agility, commitment to excellence","adaptability, agile development, attention to detail, commitment to excellence, communication, critical thinking, data architecture, data extraction, data loading, data science, data transformation, dataanalytics, database tuning, database upkeep, datamodeling, datawarehouse, er diagramming, etl, flexibility, initiative, learning agility, nosql, problem solving, selfmotivation, sql, teamwork, time management"
Data Visualization Engineer,How To MANAGE a Small Law Firm,"Miami, FL",https://www.linkedin.com/jobs/view/data-visualization-engineer-at-how-to-manage-a-small-law-firm-3769547793,2023-12-17,Florida,United States,Associate,Remote,"Do your strengths include forward thinking and translating data into something visually stunning? Do you thrive in fast-paced ever-changing environments?
If so- then it’s time to join the HTM team as a
Data Visualization Engineer
! Our Data Visualization Engineer will work to craft visualizations that not only tell a story, but also inspire smarter decision-making.
Intrigued? Here are some of the tasks you will be working on:
• Partnering with various departments to understand data needs, objectives, and user wishes
• Crafting captivating interactive data visualizations, dashboards, and reports
• Transform raw data into meaningful moments using a variety of techniques (charts, graphs, heatmaps, scatter plots, etc.)
• Staying ahead of the curve, keeping tabs on the latest data visualization trends and technologies- we want to hear fresh ideas from you!
• Test, gather feedback, and fine-tune our visualizations to ensure they are not only beautiful but also effective.
• Communicate with internal team to ensure a smooth flow of information
• Maintaining a positive, empathetic, and professional attitude
Here is how YOU can tell if you may be the right person for the job:
• YOU know how to use data visualization tools like Neo4J, Tableau, Power BI
• YOU are a strong communicator who understands the importance in working with people
• YOU have programming skills in Python, JavaScript, or R.
• YOU have some patience, humor, and tolerance for what other people may view as “silly questions”
• YOU love customer service and helping others
Still excited about this role? Here are the ways your resume can show us how great you are:
• Bachelor’s degree in Computer Science, Data Science, Information Design, or related field. (Bonus points for a Master’s degree!)
• Solid experience (like over 3 years) in data manipulation, ETL processes, and data cleansing techniques.
• Familiarity with UX/UI principles
• Knowledge of database systems and SQL for data retrieval
**This position may require travel to quarterly conferences and other trainings**
Now here’s more about us:
How to MANAGE a Small Law Firm is a fast-growing and highly-entrepreneurial business management, personal development and coaching company, specializing in solo and small law firms. Inc. Magazine has named How to MANAGE a Small Law Firm to its list of the 5,000 fastest growing privately held companies in the United States every year since 2015. We've also been named by Inc. as one of the 'Best Places To Work'. We are very much a “start-up” with a fast-paced growth focused environment. Working with us can prove to be the best job you ever had or the most frustrating depending on your flexibility, team-spirit, commitment to our clients and your realistic expectations about what it’s like to work in a fast-paced growing business.
Our culture....
We are a rapidly growing company and we are seeking an individual who can keep up and can maintain a comfortable presence in our ever-changing environment. If you're intellectually curious, ready to learn, and a team player - this will be the perfect role for you! If you're not a self-starter, need a lot of supervision, or are looking for a 9 - 5 that won't challenge you, we're not it.
HTM provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.
IMPORTANT:
please complete the full application via the external link provided. Applications through LinkedIn will NOT be accepted. Start this process off right by showing us how well you can follow these instructions!
Show more
Show less","Data Visualization, Neo4j, Tableau, PowerBI, Python, JavaScript, R, SQL, ETL, Data Manipulation, UX/UI, Database Systems","data visualization, neo4j, tableau, powerbi, python, javascript, r, sql, etl, data manipulation, uxui, database systems","data manipulation, database systems, etl, javascript, neo4j, powerbi, python, r, sql, tableau, uxui, visualization"
"Senior Software Engineer, Orders Data Platform",Square,"Miami, FL",https://www.linkedin.com/jobs/view/senior-software-engineer-orders-data-platform-at-square-3784936680,2023-12-17,Florida,United States,Associate,Remote,"Company Description
Since we opened our doors in 2009, the world of commerce has evolved immensely, and so has Square. After enabling anyone to take payments and never miss a sale, we saw sellers stymied by disparate, outmoded products and tools that wouldn’t work together.
So we expanded into software and started building integrated, omnichannel solutions – to help sellers sell online, manage inventory, offer buy now, pay later functionality through Afterpay, book appointments, engage loyal buyers, and hire and pay staff. Across it all, we’ve embedded financial services tools at the point of sale, so merchants can access a business loan and manage their cash flow in one place. Afterpay furthers our goal to provide omnichannel tools that unlock meaningful value and growth, enabling sellers to capture the next generation shopper, increase order sizes, and compete at a larger scale.
Today, we are a partner to sellers of all sizes – large, enterprise-scale businesses with complex operations, sellers just starting, as well as merchants who began selling with Square and have grown larger over time. As our sellers grow, so do our solutions. There is a massive opportunity in front of us. We’re building a significant, meaningful, and lasting business, and we are helping sellers worldwide do the same.
Job Description
We are the Orders Data Platform team, a team whose mission is to help merchants of all sizes gain insights from their sales data through a variety of interfaces. We sit at the center of critical domains and data flows, and we’re building a multi-layered platform to achieve our goals with Square-wide impact. We’re looking for a senior engineer who can help us build new platform features to dramatically improve user-facing search and reporting experiences at Square.
We provide a unified view of Orders/Orders-adjacent data at Square, and power a variety of interfaces for our teams and third-party developers to access that data. You’ll work with teams to understand requirements, and ensure the platform we’re building works across several distinct use cases.
To power that view of Orders, we’re also building a general-purpose Elasticsearch- and GraphQL-based search and reporting platform that can operate at Square scale. This general-purpose platform already powers search and reporting for a variety of teams at Square in production, and we’re continuing to evolve it for our collective use cases. We build in the open, and look forward to open-sourcing this platform in 2024 for even wider impact.
You will:
Work with Product and partners across Square to identify platform requirements, and work within the engineering team to develop the corresponding features
Provide high-quality hands-on contributions across multiple code bases
Identify technical and architectural end states for the project, and influence/evolve the code case in those directions
Play a key role in choosing technical investments for the team
Qualifications
You Have:
5+ years of software development experience
Familiarity with architecting/implementing Java-/Kotlin-based backend services
Strong product intuition and interest, with platform-building experience
Strategic leadership experience on medium/large-scale software projects
Interest and experience in mentoring other engineers
Even better:
Experience with real-time data streaming platforms such as Kafka and Kinesis
Experience and familiarity with Kotlin, GraphQL, Elasticsearch, and AWS technologies
Proficiency in large-scale Ruby projects, or a history and interest in quickly learning new technologies and stacks
History of contributions to open-source projects
Familiarity with the payments-processing domain
Technologies we use within Orders Data Platform:
Java, Kotlin, Ruby
GraphQL
Elasticsearch, DynamoDb
Terraform, AWS Lambda, SQS, Cloudwatch
Additional Information
Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.
Zone A: USD $167,300 - USD $204,500
Zone B: USD $158,900 - USD $194,300
Zone C: USD $150,600 - USD $184,000
Zone D: USD $142,200 - USD $173,800
To find a location’s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information.
Full-time employee benefits include the following:
Healthcare coverage (Medical, Vision and Dental insurance)
Health Savings Account and Flexible Spending Account
Retirement Plans including company match
Employee Stock Purchase Program
Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance
Paid parental and caregiving leave
Paid time off (including 12 paid holidays)
Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees)
Learning and Development resources
Paid Life insurance, AD&D, and disability benefits
These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.
US and Canada EEOC Statement
We’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.
We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.
Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our
I+D page
.
Additionally, we consider qualified applicants with criminal histories for employment on our team, and always assess candidates on an individualized basis.
We’ve noticed a rise in recruiting impersonations across the industry, where individuals are sending fake job offer emails. Contact from any of our recruiters or employees will always come from an email address ending with @block.xyz, @squareup.com, @tidal.com, or @afterpay.com, @clearpay.co.uk.
Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.
Show more
Show less","Java, Kotlin, Ruby, GraphQL, Elasticsearch, DynamoDb, Terraform, AWS Lambda, SQS, Cloudwatch, Kafka, Kinesis, AWS, Realtime data streaming platforms, Paymentsprocessing domain","java, kotlin, ruby, graphql, elasticsearch, dynamodb, terraform, aws lambda, sqs, cloudwatch, kafka, kinesis, aws, realtime data streaming platforms, paymentsprocessing domain","aws, aws lambda, cloudwatch, dynamodb, elasticsearch, graphql, java, kafka, kinesis, kotlin, paymentsprocessing domain, realtime data streaming platforms, ruby, sqs, terraform"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"St Petersburg, FL",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783186484,2023-12-17,Florida,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Tampa/St-DataResearchAn.001
Show more
Show less","Python, JavaScript, JSON, R, OOP, Generative AI, Data Analytics, Machine Learning, Natural Language Processing, Agile, SCRUM, SQL, NoSQL, Big Data, Cloud Computing, AWS, Azure, GCP, Docker, Kubernetes, Hadoop, Spark, Flink, Kafka, Git, GitHub, Slack, Jira, Confluence, Trello","python, javascript, json, r, oop, generative ai, data analytics, machine learning, natural language processing, agile, scrum, sql, nosql, big data, cloud computing, aws, azure, gcp, docker, kubernetes, hadoop, spark, flink, kafka, git, github, slack, jira, confluence, trello","agile, aws, azure, big data, cloud computing, confluence, dataanalytics, docker, flink, gcp, generative ai, git, github, hadoop, javascript, jira, json, kafka, kubernetes, machine learning, natural language processing, nosql, oop, python, r, scrum, slack, spark, sql, trello"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Orlando, FL",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783187485,2023-12-17,Florida,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Orlando-DataResearchAn.039
Show more
Show less","Python, JavaScript, JSON, Natural language processing, Generative AI, Machine learning, Data analysis, Data science, Research, Product development, Education technology, EdTech, Artificial intelligence, OOP","python, javascript, json, natural language processing, generative ai, machine learning, data analysis, data science, research, product development, education technology, edtech, artificial intelligence, oop","artificial intelligence, data science, dataanalytics, edtech, education technology, generative ai, javascript, json, machine learning, natural language processing, oop, product development, python, research"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Cape Coral, FL",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783183929,2023-12-17,Florida,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-CapeCora-DataResearchAn.010
Show more
Show less","Generative AI, Machine Learning, Python, JavaScript, JSON, R, ObjectOriented Programming, Data Science, Research, Product Development, Data Analytics, Coaching, Communication, Teamwork, Project Management, Remote Work, Independent Contractor","generative ai, machine learning, python, javascript, json, r, objectoriented programming, data science, research, product development, data analytics, coaching, communication, teamwork, project management, remote work, independent contractor","coaching, communication, data science, dataanalytics, generative ai, independent contractor, javascript, json, machine learning, objectoriented programming, product development, project management, python, r, remote work, research, teamwork"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Gainesville, FL",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783184596,2023-12-17,Florida,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Gainesvi-DataScientist.002
Show more
Show less","Python, JavaScript, JSON, Generative AI, Educational tools, Product engineering, Technical functionalities, Education, English communication, R, OOP language, Proficient in technology, Proficient in science, Proficient in data science, Proficient in financial services, Proficient in programming, Monday through Friday, CST, Communication, Stakeholder management, EdTech, Technology, AI, Subject matter experts, Learning science, Data analytics, Highperformance coaching","python, javascript, json, generative ai, educational tools, product engineering, technical functionalities, education, english communication, r, oop language, proficient in technology, proficient in science, proficient in data science, proficient in financial services, proficient in programming, monday through friday, cst, communication, stakeholder management, edtech, technology, ai, subject matter experts, learning science, data analytics, highperformance coaching","ai, communication, cst, dataanalytics, edtech, education, educational tools, english communication, generative ai, highperformance coaching, javascript, json, learning science, monday through friday, oop language, product engineering, proficient in data science, proficient in financial services, proficient in programming, proficient in science, proficient in technology, python, r, stakeholder management, subject matter experts, technical functionalities, technology"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Hialeah, FL",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783184794,2023-12-17,Florida,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Hialeah-DataScientist.008
Show more
Show less","Python, JavaScript, JSON, Generative AI, AI prompts, R, OOP, Data science, Machine learning, Research, Product engineering, Product development, Communication, Multitasking, Time management, Project management, Collaboration, EdTech, Data analytics, Coaching","python, javascript, json, generative ai, ai prompts, r, oop, data science, machine learning, research, product engineering, product development, communication, multitasking, time management, project management, collaboration, edtech, data analytics, coaching","ai prompts, coaching, collaboration, communication, data science, dataanalytics, edtech, generative ai, javascript, json, machine learning, multitasking, oop, product development, product engineering, project management, python, r, research, time management"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Pompano Beach, FL",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783189131,2023-12-17,Florida,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-PompanoB-DataScientist.007
Show more
Show less","* Python, * JavaScript, * JSON, * OOP language, * Data science, * Research, * Product development, * Data analytics, * Generative AI, * Algorithms, * EdTech, * AI training models, * Technical functionalities, * Written communication, * Verbal communication, * Proactive communication, * Stakeholder management, * Coaching, * Learning science","python, javascript, json, oop language, data science, research, product development, data analytics, generative ai, algorithms, edtech, ai training models, technical functionalities, written communication, verbal communication, proactive communication, stakeholder management, coaching, learning science","ai training models, algorithms, coaching, data science, dataanalytics, edtech, generative ai, javascript, json, learning science, oop language, proactive communication, product development, python, research, stakeholder management, technical functionalities, verbal communication, written communication"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Pensacola, FL",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783184638,2023-12-17,Florida,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Pensacol-DataScientist.011
Show more
Show less","Python, JavaScript, JSON, R, OOP, Data Science, Generative AI, Natural Language Processing, Machine Learning, Algorithms, Product Development, Software Development, SQL, Big Data, Data Analytics, Data Visualization, Project Management, Stakeholder Management, Communication, Teamwork","python, javascript, json, r, oop, data science, generative ai, natural language processing, machine learning, algorithms, product development, software development, sql, big data, data analytics, data visualization, project management, stakeholder management, communication, teamwork","algorithms, big data, communication, data science, dataanalytics, generative ai, javascript, json, machine learning, natural language processing, oop, product development, project management, python, r, software development, sql, stakeholder management, teamwork, visualization"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Coral Springs, FL",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783190322,2023-12-17,Florida,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-CoralSpr-DataScientist.013
Show more
Show less","Generative AI, Python, JavaScript, JSON, OOP, Data Analytics, AI Training, R, English Communication, Project Management, Coaching, EdTech, Stakeholder Management","generative ai, python, javascript, json, oop, data analytics, ai training, r, english communication, project management, coaching, edtech, stakeholder management","ai training, coaching, dataanalytics, edtech, english communication, generative ai, javascript, json, oop, project management, python, r, stakeholder management"
Business Data Analyst,DISYS,"Tampa, FL",https://www.linkedin.com/jobs/view/business-data-analyst-at-disys-3781752080,2023-12-17,Florida,United States,Associate,Hybrid,"Role : Business Data Analyst
Location : Tampa, FL
Duration : 12+ months
• 4-6 years of relevant experience
• Previous experience working as a Business Analyst in Agile methodology and/or SDLC is a must
• Beginner to intermediate knowledge of SQL
• Ability to perform in client facing situations
• Experience in data analysis with intermediate/advanced Microsoft Office Suite skills
• Proven interpersonal, data analysis, diplomatic, management and prioritization skills
• Consistently demonstrate clear and concise written and verbal communication
• Proven ability to manage multiple activities and build/develop working relationships
• Proven self-motivation to take initiative and master new tasks quickly
• Demonstrated ability to work under pressure to meet tight deadlines and approach work methodically with attention to detail
Lakshay Taneja
Technical Recruiter , Banking and Financial Services
Digital Intelligence Systems (DISYS)
For any feedback à Have your say @ BusinessExcellence@Disys.com
Search & Apply for more jobs @ careers.disys.com
Dexian is a leading provider of staffing, IT, and workforce solutions with over 12,000 employees and 70 locations worldwide. As one of the largest IT staffing companies and the 2nd largest minority-owned staffing company in the U.S., Dexian was formed in 2023 through the merger of DISYS and Signature Consultants. Combining the best elements of its core companies, Dexian's platform connects talent, technology, and organizations to produce game-changing results that help everyone achieve their ambitions and goals.
Dexian's brands include Dexian DISYS, Dexian Signature Consultants, Dexian Government Solutions, Dexian Talent Development and Dexian IT Solutions. Visit https://dexian.com/ to learn more.
Dexian is an Equal Opportunity Employer that recruits and hires qualified candidates without regard to race, religion, sex, sexual orientation, gender identity, age, national origin, ancestry, citizenship, disability, or veteran status.
Show more
Show less","Business Analysis, Agile, SQL, Microsoft Office Suite, Communication, Management, Prioritization, Data Analysis, SDLC","business analysis, agile, sql, microsoft office suite, communication, management, prioritization, data analysis, sdlc","agile, business analysis, communication, dataanalytics, management, microsoft office suite, prioritization, sdlc, sql"
EIM Data Analyst,Virtusa,"Tampa, FL",https://www.linkedin.com/jobs/view/eim-data-analyst-at-virtusa-3765397364,2023-12-17,Florida,United States,Associate,Hybrid,"Responsibilities
Job Description:
8+ years of IT experience as technical data Analyst with hands-on expertise in data analysis and Oracle SQL knowledge. Experience in Financial sector is advantage Person should have very good understanding on SQL queries and should be able to write SQL queries to perform data analysis. Resource should be able to work with multiple stakeholders/teams Should be able to understand the application data model and able to explain the functional requirements to stakeholders Need to take the ownership for the tasks assigned and deliver them successfully Quick understanding the new concepts and application details Strong communication and presentation skills
Required Skills: Data Analysis, SQL, PLSQL
Nice to have skills: Excellent communication skills
Show more
Show less","Data Analysis, SQL, Oracle SQL, PLSQL","data analysis, sql, oracle sql, plsql","dataanalytics, oracle sql, plsql, sql"
Data Analysis & Integration (SAP S/4HANA) Supervisor,Pinellas County Government,"Clearwater, FL",https://www.linkedin.com/jobs/view/data-analysis-integration-sap-s-4hana-supervisor-at-pinellas-county-government-3780159121,2023-12-17,Florida,United States,Associate,Hybrid,"Work Location: 14 S. Ft Harrison Ave, Clearwater, FL 33756
Schedule: Monday - Friday; 8:00 AM - 5:00 PM. Hours may differ at times during system upgrades/implementations. Some remote work.
If you enjoy working with Data Life Cycle Management: Data Sources, Data Analysis, Data Integration, Data Collection, Data Extraction, Data Cleansing, Data Transformation and Data Consolidation, Pinellas County has the ideal opportunity!!!
Please note: This recruitment will not write system coding or programming.
Performs technical work and plays a key role in advancing technology, implementing, upgrading, and testing software throughout the Business & Customer Services Division; coordinates technical aspects and component portions of major projects that relates to our customer information system, SAP S/4HANA, Cityworks, Sharepoint, geographic information systems (GIS), utilities payment system, technical research, training, processes, and quality assurance or operations within the Utilities Department. Performs as a project administrator, leader, or may supervise projects and teams; ensures that work meets technical and other agency requirements; works closely with various officials and subject matter experts to ensure successful integration and completion of projects and operations with other project managers, County staff and departments, consultants, various municipalities, public agencies, and private contractors; assists in developing technical program guidelines and technical procedures while implementing and maintaining programs. The position also supports the annual budget process tracking software needs/upgrades; responsible for the development coordination, and implementation of work standard, methods, process improvements, and overall assessment, testing and evaluating procedures and practices and serves as the department expert in the assigned area. Supervises and mentors a team of direct reports in technical aspects, data analytics, and reporting.
Our benefits rank among the top in the area!
Comprehensive medical coverage at a low cost:
2024 Medical Plan Premiums
Choose from two medical plans which offer the same low premiums and include medical care, prescription coverage, behavioral/mental health and vision care.
Coverage
Biweekly Cost
Employee Only
$13.09
Employee and Spouse or Domestic Partner
$151.16
Employee and Child(ren)
$120.60
Family
$247.67
Benefits package includes health, vision, prescription, dental, life insurance, disability, and Flexible Spending Account (FSA) options
Florida Retirement System (FRS)
:
Choose the Investment Plan or Pension Plan
Generous leave time including 9 to 11 paid holidays, 2 floating holidays, 2 personal days (pro-rated), and 15 days of annual leave the first year
Tuition reimbursement up to $2,800 per year
Wellness Program including a Wellness Center, gym discounts, and classes.
Deferred compensation plans from four providers
To learn more, see What We Offer
Essential Job Functions
Initiates, plans, coordinates, and oversees project assignments requiring advanced technical expertise.
Establishes written specifications and sources for services, products, contractors for automated data processing, construction, information technology, operations, systems maintenance, and highly technical operations.
Assesses, plans, implements, and evaluates highly technical specification requirements and advises management on the most efficient and effective means to achieve goals for projects and operations.
Researches and gathers required technical data to evaluate plans and activities.
Manages technical assignments and projects including the planning and operations phases that may include phases in design, development, testing, implementation, and production.
Oversees projects and troubleshoots technical problems interacting with other subject matter experts, officials, and managers, including monitoring acquisitions, performance, materials, and orders product testing, as needed.
Ensures projects progress is schedule and minimizes any delays by taking action to avoid technical problems.
Evaluates and inspects progress on assigned subject matter areas of responsibility and reports to management.
Coordinates important utilities activities, water management operations, information technology systems, and other programs or operations that may involve major construction and support services acquisitions.
Participates in activities required for systems, operations, and production testing to establish technical procedures.
Serves as principal agency liaison for contractors, consultants, and key personnel on projects and processes.
Analyzes results, monitors progress, evaluates changes, and negotiates change management implementation.
Develops and establishes appropriate technical guidance, training, customer communications, and manage a proactive approach to inform, educate, and train stakeholders.
Examines and evaluates best practices of other departments, governments, agencies, or private sector organizations to acquire cutting edge technologies.
Leads or participates as a technical expert on cross functional project teams.
Manages assignments and reports to senior managers on progress on technical assignments, special projects, services, operations, and construction.
Prepares annual budget and manages contracts.
Performs other related job duties as assigned.
Position Specific Requirements
Experience: Technical and professional level experience in system, software integrations, utilities related industry, including 1 year of lead worker or supervisory training.
Degree: Engineering, environmental science, information technology, economics, health science, math, chemistry, physics, or subject directly related to the specialized assignments.
8 years of experience as described above.
Associate’s degree as described above, and 6 years of experience as described above.
Bachelor’s degree as described above, and 4 years of experience as described above.
Master’s degree as described above, and 2 years of experience as described above.
An equivalent combination of education, training, and/or experience.
Possession and maintenance of a valid State of Florida Driver’s License upon hire with eligibility based upon evaluation of a Motor Vehicle Record (MVR) driving report.
Assignment to work various work schedules, including critical work periods, emergencies, and disaster situations.
Highly Desirable
Strong interpersonal skills and leadership abilities to support and achieve organizational goals.
Knowledge and experience working with SAP Hana, Cityworks, Microsoft Excel, GIS systems/softwares
Highly analytical techniques and queries to create datasets and reporting to improve processes.
Knowledge, Skills, and Abilities
Knowledge and technical expertise in the assigned subject matter areas of responsibility.
Knowledge of automated project management documentation, tracking, and control processes.
Knowledge of building design and construction procedures.
Knowledge of general management practices and principles.
Knowledge of principles and procedures of public administration and project management.
Skill in identifying, analyzing, and isolating problems, and problem resolution.
Skill in interpersonal skills, communications, team building, facilitation, networking, and negotiations.
Ability to coordinate, lead, and supervise work completed or performed by others.
Ability to apply computer applications and software.
Ability to coordinate and resolve complex and technical issues arising during the course of project design and implementation, as well as delays in business, operations, or construction activities.
Ability to facilitate management level teams and bring the teams to consensus.
Ability to manage and organize major projects and programs.
Ability to operate a personal computer and other automated systems to enter and retrieve information, monitor work performed, and to communicate information in reports, etc.
Physical/Mental Demands
The work is heavy work which requires exerting up to 100 pounds of force occasionally, and/or up to 50 pounds of force frequently, and/or up to 20 pounds of force constantly to move objects. Additionally, the following physical abilities are required:
Fingering: Picking, pinching, typing, or otherwise working, primarily with fingers rather than with the whole hand as in handling.
Grasping: Applying pressure to an object with the fingers and palm.
Handling: Picking, holding, or otherwise working, primarily with the whole hand.
Lifting: Raising objects from a lower to a higher position or moving objects horizontally from position-to-position. Occurs to a considerable degree and requires substantial use of upper extremities and back muscles.
Pulling: Using upper extremities to exert force in order to draw, haul or tug objects in a sustained motion.
Pushing: Using upper extremities to press against something with steady force in order to thrust forward, downward or outward.
Reaching: Extending hand(s) and arm(s) in any direction.
Visual ability: Sufficient to effectively operate office equipment including copier, computer, etc.; and to read and write reports, correspondence, instructions, etc.
Hearing ability: Sufficient to hold a conversation with other individuals both in person and over a telephone; and to hear recording on transcription device.
Speaking ability: Sufficient to communicate effectively with other individuals in person and over a telephone.
Mental acuity: Ability to make rational decisions through sound logic and deductive processes.
Talking: Expressing or exchanging ideas by means of the spoken word including those activities in which they must convey detailed or important spoken instructions to other workers accurately, loudly, or quickly.
Show more
Show less","SAP S/4HANA, Cityworks, Sharepoint, GIS, Utilities payment system, Technical research, Project management, Data Life Cycle Management, Data Sources, Data Analysis, Data Integration, Data Collection, Data Extraction, Data Cleansing, Data Transformation, Data Consolidation, Microsoft Excel, SAP Hana, Technical writing, Troubleshooting, Communication, Leadership, Teamwork, Problemsolving, Critical thinking, Analytical skills, Research skills, Project management skills","sap s4hana, cityworks, sharepoint, gis, utilities payment system, technical research, project management, data life cycle management, data sources, data analysis, data integration, data collection, data extraction, data cleansing, data transformation, data consolidation, microsoft excel, sap hana, technical writing, troubleshooting, communication, leadership, teamwork, problemsolving, critical thinking, analytical skills, research skills, project management skills","analytical skills, cityworks, communication, critical thinking, data collection, data consolidation, data extraction, data integration, data life cycle management, data sources, data transformation, dataanalytics, datacleaning, gis, leadership, microsoft excel, problemsolving, project management, project management skills, research skills, sap hana, sap s4hana, sharepoint, teamwork, technical research, technical writing, troubleshooting, utilities payment system"
Database Engineer,Pierce,"Melbourne, FL",https://www.linkedin.com/jobs/view/database-engineer-at-pierce-3640165260,2023-12-17,Florida,United States,Mid senior,Onsite,"Use Azure DevOps for application planning, development delivery, and operations
Design highly available and scalable database solutions that meet defined technical requirements and are optimized for performance, security, and manageability
Develop large scale applications designed for scalability, performance, and reliability
Maintain databases and troubleshoot performance issues
Create well designed, efficient SQL queries using best practices
Gather and refine specifications and requirements based on technical needs
Create data for dev and test environments
Strong T-SQL development skills
Stay in touch with emerging technologies / industry trends and apply them as necessary
Requirements
SQL Server 2008 R2 – 2019
Full Stack application development exposure
Object-oriented programming experience preferred
Experience with developing and managing ETL packages
Experience with Azure data concepts, methods, and operations
10+ years’ experience developing SQL server databases, SQL server integration services, SQL server analysis services
10+ years’ experience developing reports using SQL server reporting services
10+ years’ experience performing data warehouse architecture development and management
10+ years programming in Visual Basic, C#, and T-SQL
Experience developing code, quality assurance testing, and administering RDBMS
Experience with mainstream database monitoring tools such as Foglight
Proficiency in dimensional modeling techniques and their applications
Ability to work independently and flexibly in a rapidly changing environment
Collaborate and work effectively on cross-functional development initiatives
Ability to prioritize work, multi-task, and exercise time management
Bachelor of Science degree in a related discipline preferred
Strong analytical, conceptual, and problem-solving abilities and critical thinking
Show more
Show less","Azure DevOps, SQL Server (2008 R22019), ETL packages, SQL Server Reporting Services, Data Warehouse Architecture, Objectoriented programming, TSQL, Visual Basic, C#, Foglight, Dimensional modeling, RDBMS, SQL Server Integration Services, SQL Server Analysis Services","azure devops, sql server 2008 r22019, etl packages, sql server reporting services, data warehouse architecture, objectoriented programming, tsql, visual basic, c, foglight, dimensional modeling, rdbms, sql server integration services, sql server analysis services","azure devops, c, data warehouse architecture, dimensional modeling, etl packages, foglight, objectoriented programming, rdbms, sql server 2008 r22019, sql server analysis services, sql server integration services, sql server reporting services, tsql, visual basic"
Senior Data Engineer (On-Site),PrismHR,"Sarasota, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-on-site-at-prismhr-3768117514,2023-12-17,Florida,United States,Mid senior,Onsite,"Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data.
We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:
Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds
Defining streaming event data feeds required for real-time analytics and reporting
Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance
As a Senior Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product.
Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!
Responsibilities
Build our next generation data warehouse
Build our event stream platform
Translate user requirements for reporting and analysis into actionable deliverables
Enhance automation, operation, and expansion of real-time and batch data environment
Manage numerous projects in an ever-changing work environment
Extract, transform, and load complex data into the data warehouse using cutting-edge technologies
Build processes for topnotch security, performance, reliability, and accuracy
Provide mentorship and collaborate with fellow team members
Requirements
Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required
5+ years of experience building data pipelines
5+ years of experience building data frameworks for unit testing, data lineage tracking, and automation
Fluency in Scala is required
Working knowledge of Apache Spark
Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)
Nice To Have
Experience with Machine Learning
Familiarity with Looker a plus
Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)
PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company’s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR’s Marketplace Partners.
Diversity, Equity And Inclusion Program/Affirmative Action Plan
We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.
Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren't just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about our roles but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.
As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual’s qualifications as they relate to the job under consideration. The Company’s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.
The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers.
Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy.
PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response.
Show more
Show less","Data Architectures, ETL, Data Engineering, Data Foundations, Platform Leveling, Data Streaming, Data Warehouse, Cloud, Realtime Analytics, Reporting, Automation, Test Coverage, Observability, Alerting, Performance, Scalability, Customer Base, Crossfunctional Teams, Best Practices, Technology Architecture, Process, Collaboration, Mentorship, Career Growth, Data Pipeline Building, Data Frameworks, Unit Testing, Data Lineage Tracking, Scala, Apache Spark, Streaming Technologies, Kafka, Kinesis, Flink, Machine Learning, Looker, Serverside Programming Languages, Golang, C#, Ruby, SaaS, Cloudbased Payroll Process, Professional Services, System Implementation Consulting, Custom Configurations, Training, Marketplace Platform, PrismHR, Diversity, Equity, Inclusion, Affirmative Action, Equal Opportunity, Privacy Policy, Reasonable Accommodation","data architectures, etl, data engineering, data foundations, platform leveling, data streaming, data warehouse, cloud, realtime analytics, reporting, automation, test coverage, observability, alerting, performance, scalability, customer base, crossfunctional teams, best practices, technology architecture, process, collaboration, mentorship, career growth, data pipeline building, data frameworks, unit testing, data lineage tracking, scala, apache spark, streaming technologies, kafka, kinesis, flink, machine learning, looker, serverside programming languages, golang, c, ruby, saas, cloudbased payroll process, professional services, system implementation consulting, custom configurations, training, marketplace platform, prismhr, diversity, equity, inclusion, affirmative action, equal opportunity, privacy policy, reasonable accommodation","affirmative action, alerting, apache spark, automation, best practices, c, career growth, cloud, cloudbased payroll process, collaboration, crossfunctional teams, custom configurations, customer base, data architectures, data engineering, data foundations, data frameworks, data lineage tracking, data pipeline building, data streaming, datawarehouse, diversity, equal opportunity, equity, etl, flink, golang, inclusion, kafka, kinesis, looker, machine learning, marketplace platform, mentorship, observability, performance, platform leveling, prismhr, privacy policy, process, professional services, realtime analytics, reasonable accommodation, reporting, ruby, saas, scala, scalability, serverside programming languages, streaming technologies, system implementation consulting, technology architecture, test coverage, training, unit testing"
Senior Data Engineer (Remote),MMS,"Tampa, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-mms-3782260698,2023-12-17,Florida,United States,Mid senior,Onsite,"MMS is an innovative, data focused CRO that supports the pharmaceutical, biotech, and medical device industries with a proven, scientific approach to complex trial data and regulatory submission challenges. Strong industry experience, technology-enabled services, and a data-driven approach to drug development make MMS a valuable CRO partner, creating compelling submissions that meet rigorous regulatory standards. With a global footprint across four continents, MMS maintains a 97 percent customer satisfaction rating, and the company has been recognized as a leading CRO inGlobal Health & Pharma’sinternational awards programs for the last three consecutive years. For more information, visit www.mmsholdings.com or follow MMS on LinkedIn.
Job Description:
Maintains a strong understanding of regulations and guidance as they pertain to data curation deliverables.
Strong understanding of new methods, tools and solutions to meet the data engineering needs of internal and external stakeholders and teams.
Mentors others and advises on MMS, industry trends and technologies to give the technical and non-technical stakeholders a better understanding of data science methodologies and results.
Maintains a strong understanding of Data Science department methodologies and standard practices.
Proficient in conducting peer reviews for others and validation of project deliverables within the team.
Proficient in developing and delivering training for internal and external stakeholders regarding data engineering processes and deliverables.
Strong understanding of CROs and/or Health Systems and the drug development process.
Proficient in developing requirements and specifications from analysis of business needs.
Proficient in preparing, correcting, modifying and analyzing data sets using complex analytic techniques.
Create reusable, highly parameterized pipelines using Microsoft Azure, driven by project-based configuration files to orchestrate landingdatain thedatalake as well as staging to SQLdatabases for analysis.
Applydatamodeling and architecture best practices to stage and transformdatato a commondatamodel. Incorporatedatawarehouse concepts to support dashboard reporting via star schemas and support auditing viadatalineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer. Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Requirements:
College graduate in data engineering discipline or related field, or related experience.
Minimum of 7 years’ experience in data engineering or similar field required or an equivalent combination of education and experience.
Create reusable, highly parameterized Azure data factory pipelines, driven by project-based configuration files to orchestrate landing data in the data lake as well as staging to Microsoft SQL databases for analysis.
Apply data modeling and architecture best practices to stage and transform data to a common data model.Incorporate data warehouse concepts to support dashboard reporting via star schemas and support auditing via data lineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer.Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Expert knowledge of data engineering concepts.
Reputation as emerging leader in field with sustained performance and accomplishment.
Hands-on experience with clinical trial and pharmaceutical development preferred.
Good communication skills and willingness to work with others to clearly understand needs and solve problems.
Excellent problem-solving skills.
Good organizational and communication skills.
Familiarity with data privacy and anonymization regulations preferred.
Familiarity with current ISO 9001 and ISO 27001 standards preferred.
Familiarity with 21 CFR Part 11, FDA, and GCP requirements.
Familiarity with industry standard data models (CDISC, FHIR, OMOP) preferred.
Basic understanding of CROs and scientific & clinical data/terminology, & the drug development process
Proficiency with MS Office applications.
Show more
Show less","Data Engineering, Data Science, TSQL, SQL, Azure Data Factory, Microsoft Azure, Data Modeling, Data Architecture, Common Data Model, Data Warehouse, Dashboard Reporting, Star Schema, Data Lineage, Clinical Trial, Pharmaceutical Development, CDISC, FHIR, OMOP, ISO 9001, ISO 27001, 21 CFR Part 11, FDA, GCP, MS Office","data engineering, data science, tsql, sql, azure data factory, microsoft azure, data modeling, data architecture, common data model, data warehouse, dashboard reporting, star schema, data lineage, clinical trial, pharmaceutical development, cdisc, fhir, omop, iso 9001, iso 27001, 21 cfr part 11, fda, gcp, ms office","21 cfr part 11, azure data factory, cdisc, clinical trial, common data model, dashboard reporting, data architecture, data engineering, data lineage, data science, datamodeling, datawarehouse, fda, fhir, gcp, iso 27001, iso 9001, microsoft azure, ms office, omop, pharmaceutical development, sql, star schema, tsql"
Data Engineer (USSOCOM),SimIS Inc.,"Tampa, FL",https://www.linkedin.com/jobs/view/data-engineer-ussocom-at-simis-inc-3787743097,2023-12-17,Florida,United States,Mid senior,Onsite,"Data Engineer for USSOCOM IDST
Position Description
. Support the USSOCOM J2 Intelligence Data Science Team (IDST)’s ongoing data analytic programs. Build and maintain data systems and construct datasets that are easy to analyze and support customer requirements. Implement methods to improve data reliability and quality. Combine raw information from different sources to create consistent and machine-readable formats. Develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling. Develop and deploy Application Programming Interfaces (API) to expose IDST maintained data to the enterprise. Collaborate with the Digital and Artificial Intelligence personnel, Knowledge Management teams, and other activities supporting data analysis for the intelligence community to include planning events, customer requirement discussions, and client intelligence capability development.
Knowledge.
Expert knowledge on data integration processes and supporting documentation requirements.
Expert knowledge using Python, SQL, noSQL, Cypher, POSTGRES and AGILE software development methodology.
Preferred: Working knowledge of
SOF community intelligence process and analysis tradecraft to compile, collate, analyze, produce, and evaluate all-source intelligence and support the Ops-Intel fusion process.
Various tools that support Special Operations Force (SOF) intelligence analysis, data sources relevant to the needs of the analyst, and the tradecraft associated with SOF intelligence analysis.
Skills.
Building, testing, and maintaining data infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using modern data technologies
Developing analytics tools and transformative algorithms for data to provide actionable insights into customer processes, operational efficiency and other key business performance metrics
Creating new data validation methods for analytics and data scientist team members that assist them in building and optimizing products to fulfill customer objectives
Working with IDST stakeholders including USSOCOM leadership, customers, and design teams to assist with data-related technical issues and support their data infrastructure needs
Analyzing procedures for USSOCOM data separation, access, and security across users and the enterprise data architecture
Working with IDST data and analytics experts to strive for greater functionality in our data systems and capability integration
Creating and maintaining optimal data pipeline architecture and support associated process improvements for automating manual processes, data delivery, and infrastructure re-design for scalability
Preferred - using Gitlab, and JIRA.
Providing written and verbal products and supporting business process documentation.
Working independently and as part of many teams.
Applying excellent time management and work prioritization to complex and integrated program requirements.
Executing complex projects within government defined timelines across a geographically dispersed workforce and user base.
Excellent communication, organizational, and problem-solving skills.
Experience.
Required:
Eight years' experience as a data engineer providing services similar in required tasks, scope, and complexity.
Documented experience employing data models, data mining, and segmentation techniques.
Documented experience developing, deploying and/or maintaining enterprise level data solutions.
Documented experience using SQL database design.
Preferred:
Using SOFNET-U, SOFNET-S, JIANT, SOIS, SOCRATES, NSANet and Commercial internet
Having Data Engineering certification
USSOCOM military or civil service experience.
Currently residing in the Tamps, FL area.
Education.
Bachelor’s degree in a computer science discipline, IT or similar field.
Certifications.
Preferred: Current certification for completion of the USSOCOM training and deployment requirements at the CONUS Replacement Center (CRC) as mandated by Principal Assistant for contracting (PARC) Policy Alert 12-01 dated 24 October 2011, SUBJECT: Contractor Deployment and Redeployment Requirements in Support of the U.S. Central Command Area of Responsibility.
Security.
Required:
Current DoD Top Secret clearance and eligible for SCI access and ACCM read-on and associated duty activities for access to SCI, FGI, and NATO material.
Travel.
Some travel (estimate less than 10%) both within and outside the Continental United States. Current active US passport required.
Place of Performance.
On-site at USSOCOM Headquarters, McDIll AFB, Florida; office space with utilities and Government-provided office equipment.
Full-Time employment
. 11 Federal Holidays; core work hours are 0900-1500 hrs with military team is Monday-Friday; contractors can report as early as 0630 hrs and latest to depart time is 1730 hrs and adjusted as necessary to meet installation network support requirements.
Period of Performance.
1 year Base period (starting Aug 2023); option periods under consideration.
SimIS Offers:
Flexible Spending Account (FSA)
Medical, Dental, and Vision
Short Term Disability (SimIS provides Short-Term Disability benefits at no cost to you)
LTD
Life Insurance
401(k) Savings Plan
Tuition Assistance Program
Paid Time Off (PTO)
10 Holidays each year
SimIS, Inc. is an AA / EOE / M / F / Disability / Vet / V3 certified / Drug Free Employer
Powered by JazzHR
6qopbWtjAo
Show more
Show less","Data engineering, Data integration, Data mining, Data science, Data visualization, Machine learning, Python, SQL, NoSQL, Cypher, PostgreSQL, AGILE, GitLab, JIRA, SOFNETU, SOFNETS, JIANT, SOIS, SOCRATES, NSANet, Commercial internet, Data Engineering certification, USSOCOM military or civil service experience, DoD Top Secret clearance, SCI access, ACCM readon, US passport","data engineering, data integration, data mining, data science, data visualization, machine learning, python, sql, nosql, cypher, postgresql, agile, gitlab, jira, sofnetu, sofnets, jiant, sois, socrates, nsanet, commercial internet, data engineering certification, ussocom military or civil service experience, dod top secret clearance, sci access, accm readon, us passport","accm readon, agile, commercial internet, cypher, data engineering, data engineering certification, data integration, data mining, data science, dod top secret clearance, gitlab, jiant, jira, machine learning, nosql, nsanet, postgresql, python, sci access, socrates, sofnets, sofnetu, sois, sql, us passport, ussocom military or civil service experience, visualization"
Senior Data Engineer (Remote),MMS,"Miami, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-mms-3782279390,2023-12-17,Florida,United States,Mid senior,Onsite,"MMS is an innovative, data focused CRO that supports the pharmaceutical, biotech, and medical device industries with a proven, scientific approach to complex trial data and regulatory submission challenges. Strong industry experience, technology-enabled services, and a data-driven approach to drug development make MMS a valuable CRO partner, creating compelling submissions that meet rigorous regulatory standards. With a global footprint across four continents, MMS maintains a 97 percent customer satisfaction rating, and the company has been recognized as a leading CRO inGlobal Health & Pharma’sinternational awards programs for the last three consecutive years. For more information, visit www.mmsholdings.com or follow MMS on LinkedIn.
Job Description:
Maintains a strong understanding of regulations and guidance as they pertain to data curation deliverables.
Strong understanding of new methods, tools and solutions to meet the data engineering needs of internal and external stakeholders and teams.
Mentors others and advises on MMS, industry trends and technologies to give the technical and non-technical stakeholders a better understanding of data science methodologies and results.
Maintains a strong understanding of Data Science department methodologies and standard practices.
Proficient in conducting peer reviews for others and validation of project deliverables within the team.
Proficient in developing and delivering training for internal and external stakeholders regarding data engineering processes and deliverables.
Strong understanding of CROs and/or Health Systems and the drug development process.
Proficient in developing requirements and specifications from analysis of business needs.
Proficient in preparing, correcting, modifying and analyzing data sets using complex analytic techniques.
Create reusable, highly parameterized pipelines using Microsoft Azure, driven by project-based configuration files to orchestrate landingdatain thedatalake as well as staging to SQLdatabases for analysis.
Applydatamodeling and architecture best practices to stage and transformdatato a commondatamodel. Incorporatedatawarehouse concepts to support dashboard reporting via star schemas and support auditing viadatalineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer. Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Requirements:
College graduate in data engineering discipline or related field, or related experience.
Minimum of 7 years’ experience in data engineering or similar field required or an equivalent combination of education and experience.
Create reusable, highly parameterized Azure data factory pipelines, driven by project-based configuration files to orchestrate landing data in the data lake as well as staging to Microsoft SQL databases for analysis.
Apply data modeling and architecture best practices to stage and transform data to a common data model.Incorporate data warehouse concepts to support dashboard reporting via star schemas and support auditing via data lineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer.Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Expert knowledge of data engineering concepts.
Reputation as emerging leader in field with sustained performance and accomplishment.
Hands-on experience with clinical trial and pharmaceutical development preferred.
Good communication skills and willingness to work with others to clearly understand needs and solve problems.
Excellent problem-solving skills.
Good organizational and communication skills.
Familiarity with data privacy and anonymization regulations preferred.
Familiarity with current ISO 9001 and ISO 27001 standards preferred.
Familiarity with 21 CFR Part 11, FDA, and GCP requirements.
Familiarity with industry standard data models (CDISC, FHIR, OMOP) preferred.
Basic understanding of CROs and scientific & clinical data/terminology, & the drug development process
Proficiency with MS Office applications.
Show more
Show less","Data Engineering, Data Science, Data Curation, Azure Data Factory, SQL, TSQL, Data Modeling, Data Architecture, Common Data Model, Data Warehouse, Star Schema, Data Lineage, Stored Procedures, Window Functions, Common Table Expression, Derived Tables, Dynamic TSQL, Query Optimization, Code Refactoring, Design Patterns, Abstraction","data engineering, data science, data curation, azure data factory, sql, tsql, data modeling, data architecture, common data model, data warehouse, star schema, data lineage, stored procedures, window functions, common table expression, derived tables, dynamic tsql, query optimization, code refactoring, design patterns, abstraction","abstraction, azure data factory, code refactoring, common data model, common table expression, data architecture, data curation, data engineering, data lineage, data science, datamodeling, datawarehouse, derived tables, design patterns, dynamic tsql, query optimization, sql, star schema, stored procedures, tsql, window functions"
Senior Data Engineer,Brown & Brown Insurance,"Daytona Beach, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-brown-brown-insurance-3781527117,2023-12-17,Florida,United States,Mid senior,Onsite,"Brown & Brown Inc. is searching for
Senior Data Engineer
to join our growing team. The selected candidate will engage in complex business analysis, data extraction, data cleansing and data manipulation. They will participate in the architecture, design, coding, testing, debugging, and deployment of new and existing applications. They will solve customer automation challenges and respond to suggestions for improvements and enhancements. Candidate should be able to work well with co-located and distributed team members, including vendor resources.
We are building numerous leading-edge AI & ML solutions for all our business segments. A high number of these solutions are for deployment in our Healthcare, Risk Management, Loss Reporting & Financial teams. Their data is continuing to get more complex, and our design goal is to automate as much as possible of the business process.
Essential Duties And Responsibilities
Demonstrate a high degree of creativity and problem-solving skill
Must be adept at clustering & classification, data analysis & visualization to increase business efficiency
Needs to have a comprehensive knowledge and experience in Data Normalization, Data Extraction, Data Cleansing, and Data Manipulation
Proven experience in building successful algorithms & predictive models for different datasets and stakeholders
Design, develop, test, debug, and deploy software applications using various data engineering tools and techniques, covering some of the following: Anaconda navigator, Visual studio, R Studio, MySQL, Numpy, Pandas, Matplotlib, Seaborn, Scikit-learn, NLTK, Keras. Also having experience of the following will be an advantage - Azure DevOps, SQL Server, Azure Cloud, etc.
Work with product owners, business analysts and application users to design robust user-friendly solutions, that have AI & ML at the heart of the solution
Work as part of an Agile development team; participating in daily standups, code reviews, backlog refinement and other Scrum activities
Follow secure coding guidelines
Skills And Competencies
Excellent verbal and written communication skills
Awareness and some hands-on experience applying design and architectural patterns
Comfortable accepting criticism in the context of a Pull Request. Enjoy discussing code quality. Not afraid to offer critiques of other team members’ code.
Enjoy learning new technologies and enhancing existing skills
Experience integrating with SOAP and/or Restful Web Services (WCF, Web API)
Hands-on experience using Git to perform basic source code control operations: pull/push, staging, commits, pull requests, cherry-picking, rebasing, etc.
Required Competencies
Bachelor’s degree in Computer Science or, in-lieu of degree, equivalent education, training and work-related experience
5+ years of experience in all phases of software development including design, coding, testing, debugging, and source code management
Preferred development languages are Python, R & C, with the highest emphasis on Python
Must be adept at clustering & classification, data analysis & visualization to increase business efficiency
Needs to have a comprehensive knowledge and experience in Data Normalization, Data Extraction, Data Cleansing, and Data Manipulation
Building model, testing the performance of the model using unknown data, Hyper Parameter tuning
Be experienced in building algorithms & predictive models for different vertical solutions and data types
Preferred Competencies
Experience in the insurance industry and/ or basic knowledge of insurance or financial services.
Experience with Azure DevOps: building and maintaining CI/CD Pipelines, blue/green deployments, creating NuGet packages, etc.
Understanding of advanced data modeling concepts
Experience with Azure SQL, App Services, API Management, PowerBI, Data Factory, Container technologies (Docker) or any other Microsoft Azure resources
Employment Type: Full Time
Bonus/Commission: No
Show more
Show less","Data engineering, Data analysis, Data visualization, Data normalization, Data extraction, Data cleansing, Data manipulation, Software development, Coding, Testing, Debugging, Deployment, Algorithm building, Predictive modeling, Python, R, C, Anaconda Navigator, Visual Studio, R Studio, MySQL, NumPy, Pandas, Matplotlib, Seaborn, Scikitlearn, NLTK, Keras, Azure DevOps, SQL Server, Azure Cloud, SOAP, RESTful Web Services, Git, Design patterns, Architectural patterns, Agile development, Unit testing, Integration testing, Performance testing, Continuous integration, Continuous delivery, DevOps, Microservices, Containerization, Cloud computing, Machine learning, Artificial intelligence","data engineering, data analysis, data visualization, data normalization, data extraction, data cleansing, data manipulation, software development, coding, testing, debugging, deployment, algorithm building, predictive modeling, python, r, c, anaconda navigator, visual studio, r studio, mysql, numpy, pandas, matplotlib, seaborn, scikitlearn, nltk, keras, azure devops, sql server, azure cloud, soap, restful web services, git, design patterns, architectural patterns, agile development, unit testing, integration testing, performance testing, continuous integration, continuous delivery, devops, microservices, containerization, cloud computing, machine learning, artificial intelligence","agile development, algorithm building, anaconda navigator, architectural patterns, artificial intelligence, azure cloud, azure devops, c, cloud computing, coding, containerization, continuous delivery, continuous integration, data engineering, data extraction, data manipulation, data normalization, dataanalytics, datacleaning, debugging, deployment, design patterns, devops, git, integration testing, keras, machine learning, matplotlib, microservices, mysql, nltk, numpy, pandas, performance testing, predictive modeling, python, r, r studio, restful web services, scikitlearn, seaborn, soap, software development, sql server, testing, unit testing, visual studio, visualization"
Data Engineer,Octagon Talent Solutions,"Fort Lauderdale, FL",https://www.linkedin.com/jobs/view/data-engineer-at-octagon-talent-solutions-3771573698,2023-12-17,Florida,United States,Mid senior,Onsite,"Octagon Talent Solutions seeks an experienced and enthusiastic Data Engineer to join our client company team! Our ideal candidate will have a strong background in working with Azure Data Factory and experience in database design and development. Applicants should be able to demonstrate their knowledge of ETL (extract, transform, load) processes and data engineering concepts such as database architecture, coding using SQL, stored procedures, and developing schemas. An understanding of best practices related to data governance is also expected. This is an excellent opportunity for a self-motivated person who is excited about technology! The successful candidate must be a creative problem solver with excellent analytical skills. They must have the ability to stay organized while managing multiple tasks simultaneously. The person selected should also demonstrate exceptional communication skills, both written and verbal. If you have what it takes, we want you on our team!
RESPONSIBILITIES:
Design and develop databases, stored procedures, triggers, and functions following best practices.
Create ETL pipelines efficiently using Azure Data Factory to support data storage and processing.
Develop automated processes for loading, transforming, and integrating data from multiple sources.
Monitor the health of databases & ETL pipelines, troubleshoot issues, and recommend optimizations for performance.
Collaborate with stakeholders to ensure best practices are followed regarding data governance.
Execute data quality checks to identify problems and resolve accuracy issues.
Build dashboards, trackers, and reports as required by business needs.
REQUIREMENTS:
Proven experience working with Azure Data Factory.
Knowledge of database architecture, design, and development.
Proficient in Python, SQL coding, and developing stored procedures.
Experience with ETL processes, including data extraction, transformation, and loading.
Strong problem-solving skills combined with excellent analytical skills.
Exceptional communication skills, both written and verbal.
Highly organized with the ability to manage multiple tasks concurrently.
Show more
Show less","Azure Data Factory, Database architecture, Database design, Database development, ETL, Data engineering, SQL, Python, Stored procedures, Data governance, Data quality checks, Data visualization, Data integration, Problemsolving, Analytical skills, Communication skills, Multitasking","azure data factory, database architecture, database design, database development, etl, data engineering, sql, python, stored procedures, data governance, data quality checks, data visualization, data integration, problemsolving, analytical skills, communication skills, multitasking","analytical skills, azure data factory, communication skills, data engineering, data governance, data integration, data quality checks, database architecture, database design, database development, etl, multitasking, problemsolving, python, sql, stored procedures, visualization"
Lead Data Engineer,Vinsys Information Technology Inc,"Tampa, FL",https://www.linkedin.com/jobs/view/lead-data-engineer-at-vinsys-information-technology-inc-3728254369,2023-12-17,Florida,United States,Mid senior,Onsite,"Hope you're doing well. We have an open position for a
Lead Data Engineer at Tampa, FL (Hybrid Onsite).
Pl. see the details below and let me know your interest. If interested, pl. share a copy of your resume along with your salary / rate expectations and the best time to reach you.
Location
: Tampa, FL (Hybrid Onsite)
Technical Skills
4-8 years of experience is preferred. Any visa type and work authorization is ok but should have at least one years of validity
Designing and implementing, highly performant data ingestion pipelines from multiple sources.
Experience with data integration and migration projects; ETL/data integration tools.
Experience with Scala/Python, Hadoop, Kafka, Spark/Pyspark
Experience on workflow orchestration using Google Cloud Data Engineering components like Pub/Sub, DataFlow, Cloud Functions, Cloud Composer, Big Query, etc.
Working with batch and event based / streaming technologies to ingest and process data.
Exposure to containerization of applications and Kubernetes.
Show more
Show less","Data Engineering, Data Ingestion, ETL, Scala, Python, Hadoop, Kafka, Spark, Pyspark, Google Cloud Data Engineering, Pub/Sub, DataFlow, Cloud Functions, Cloud Composer, Big Query, Batch Processing, Streaming Technologies, Containerization, Kubernetes","data engineering, data ingestion, etl, scala, python, hadoop, kafka, spark, pyspark, google cloud data engineering, pubsub, dataflow, cloud functions, cloud composer, big query, batch processing, streaming technologies, containerization, kubernetes","batch processing, big query, cloud composer, cloud functions, containerization, data engineering, data ingestion, dataflow, etl, google cloud data engineering, hadoop, kafka, kubernetes, pubsub, python, scala, spark, streaming technologies"
Big Data Developer- L3 Support,LTI - Larsen & Toubro Infotech,"Tampa, FL",https://www.linkedin.com/jobs/view/big-data-developer-l3-support-at-lti-larsen-toubro-infotech-3716396671,2023-12-17,Florida,United States,Mid senior,Onsite,"A little about us... LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 750 clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. • We provide our employees with a learning environment that promotes growth and creativity. • To learn more plase visit us at https://www.ltimindtree.com/Twitter @ https://twitter.com/ltimindtree.
Big data Hadoop-SparkSQL-Python / Scala / Java- L3 support
Duration: Full time
Location: Tampa, FL
Mandatory Certificate: Databricks Certified Developer: Apache Spark 3.0
Skills: Databricks
Skills: PySpark, Spark, Spark SQL, ETL, Hadoop, Databricks certification
Responsibilities:
• Ensure effective Design, Development, Validation and Support activities in line with client needs and architectural requirements.,
• Ensure continual knowledge management.
• Adherence to the organizational guidelines and processes.
• As part of the delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain.
• You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements.
• You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers
• You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey.
Requirements:
• A good professional with at least 9-12+ yrs. of experience in Bigdata, Pyspark, HIVE, Oracle, PL/SQL.
• Prior experience in L3 Support is a Must.
• Candidate with prior experience working on technologies on Fund transfer. AML knowledge will be an added advantage.
• Excellent written and oral communications kills.
• Self-starter with quick learning abilities.
• Multi-task and should be able to work under stringent deadlines.
• Ability to understand and work on various internal systems.
• Ability to work with multiple stakeholders.
Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”):
Benefits and Perks:
Comprehensive Medical Plan Covering Medical, Dental, Vision
Short Term and Long-Term Disability Coverage
401(k) Plan with Company match
Life Insurance
Vacation Time, Sick Leave, Paid Holidays
Paid Paternity and Maternity Leave
The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.
Disclaimer:
The compensation and benefits information provided herein is accurate as of the date of this posting.
LTIMindtree
is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
Safe return to office
:
In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.
Show more
Show less","Hadoop, Spark, Spark SQL, Python, Scala, Java, Databricks, PySpark, ETL, HIVE, Oracle, PL/SQL, AML","hadoop, spark, spark sql, python, scala, java, databricks, pyspark, etl, hive, oracle, plsql, aml","aml, databricks, etl, hadoop, hive, java, oracle, plsql, python, scala, spark, spark sql"
Senior Data Engineer,BuzzDoc LLC dba Performance Management Simulation and Analysis Group (PMSA Group),"Tampa, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-buzzdoc-llc-dba-performance-management-simulation-and-analysis-group-pmsa-group-3782574719,2023-12-17,Florida,United States,Mid senior,Onsite,"PMSA Group is looking for an expereinced Senior Data Engineer to support analysts, data scientists and senior decion commanders at United States Central Command (USCENTCOM) in Tampa FL. Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there’s more structured and unstructured data available today than ever before. As a Data Engineer, you know that organizing big data can yield pivotal insights when it’s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions—from fraud detection to cancer research to national intelligence.
Responsibilities
As a Data Engineer, you’ll implement data engineering activities on some of the most mission-driven projects in the industry. You’ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you’ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You’ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients.
Qualifications
Must Have:
10 years of experience supporting DoD at CCMD or similar level
5 years of experience in application development
5 years of experience designing, developing, operationalizing, and maintaining complex data applications at enterprise scale
3 years of experience creating software for retrieving, parsing, and processing structured and unstructured data
3 years of experience building scalable ETL/ELT workflows for reporting and analytics
Experience with Python, SQL, Scala, or Java
Experience creating solutions within a collaborative, cross-functional team environment
Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms
TS/SCI clearance
Bachelor’s degree
Nice to have:
Experience with UNIX/Linux, including basic commands and Shell scripting
Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud
Experience with distributed data and computing tools including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka
Experience working on real-time data and streaming applications
Experience with NoSQL implementation, including MongoDB or Cassandra
Experience with data warehousing using AWS Redshift, MySQL, or Snowflake
Experience with Agile engineering practices
TS/SCI clearance with a polygraph
Show more
Show less","Data Engineering, Machine Learning, Artificial Intelligence, Big Data, Data Analytics, Data Examination, Data Warehousing, Agile Engineering, Python, SQL, Scala, Java, UNIX/Linux, Shell Scripting, AWS, Microsoft Azure, Google Cloud, Spark, Databricks, Hadoop, Hive, AWS EMR, Kafka, NoSQL, MongoDB, Cassandra, AWS Redshift, MySQL, Snowflake","data engineering, machine learning, artificial intelligence, big data, data analytics, data examination, data warehousing, agile engineering, python, sql, scala, java, unixlinux, shell scripting, aws, microsoft azure, google cloud, spark, databricks, hadoop, hive, aws emr, kafka, nosql, mongodb, cassandra, aws redshift, mysql, snowflake","agile engineering, artificial intelligence, aws, aws emr, aws redshift, big data, cassandra, data engineering, data examination, dataanalytics, databricks, datawarehouse, google cloud, hadoop, hive, java, kafka, machine learning, microsoft azure, mongodb, mysql, nosql, python, scala, shell scripting, snowflake, spark, sql, unixlinux"
Senior Data Analyst,LTI - Larsen & Toubro Infotech,"Tampa, FL",https://www.linkedin.com/jobs/view/senior-data-analyst-at-lti-larsen-toubro-infotech-3762495801,2023-12-17,Florida,United States,Mid senior,Onsite,"About Us:
LTIMindtree
is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com.
Job Title:
Data Analyst
Work Location -
Tampa FL
Job Description:
Collecting, processing, and cleaning data from various sources
Creating visualizations and reports to present findings to stakeholders
Identifying patterns and trends in data to inform business decisions
Collaborating with other teams to develop and implement data-driven solutions
Continuously monitoring and evaluating data quality to ensure accuracy and completeness.
Qualifications:
Proficiency in advanced SQL (querying data ) in Big Data ,Oracle , Unix, shell scripts,
Knowledge on data analysis tools such as Excel, Tableau, or PowerBI
Strong analytical and problem-solving skills
Ability to communicate complex data findings in a clear and concise manner
Attention to detail and a commitment to data accuracy
Familiarity with data modeling , data warehousing, ETL concepts is a plus
Familiarity with Finance is a plus
Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”):
Benefits and Perks:
Comprehensive Medical Plan Covering Medical, Dental, Vision
Short Term and Long-Term Disability Coverage
401(k) Plan with Company match
Life Insurance
Vacation Time, Sick Leave, Paid Holidays
Paid Paternity and Maternity Leave
The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.
Disclaimer:
The compensation and benefits information provided herein is accurate as of the date of this posting.
LTIMindtree
is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
Safe return to office
:
In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.
Show more
Show less","SQL, Big Data, Oracle, Unix, Shell Scripts, Excel, Tableau, PowerBI, Data Analysis, Data Visualization, Data Modeling, Data Warehousing, ETL, Finance","sql, big data, oracle, unix, shell scripts, excel, tableau, powerbi, data analysis, data visualization, data modeling, data warehousing, etl, finance","big data, dataanalytics, datamodeling, datawarehouse, etl, excel, finance, oracle, powerbi, shell scripts, sql, tableau, unix, visualization"
Business Intelligence Data Engineer 2,Tampa General Hospital,"Tampa, FL",https://www.linkedin.com/jobs/view/business-intelligence-data-engineer-2-at-tampa-general-hospital-3781248487,2023-12-17,Florida,United States,Mid senior,Onsite,"Job Summary
Under general direction of Business Intelligence (BI) leadership, A BI Data Engineer III works as a functional team lead of the enterprise BI team. The Data Engineer has primary responsibility of building Enterprise Data Integration solutions by working on enterprise class data integration initiatives. The Data Engineer will be responsible for building solutions which are flexible, performant and scalable. Demonstrates subject matter expertise and can integrate domain knowledge with an understanding of overall strategy and impact. They provide data that is accurate, congruent, reliable and is easily accessible. As a BI professional you will also educate and train customers to use the data as an analytical tool, displaying the information in new form and content for analysis and exploring options. The BI Engineer is responsible for the full life cycle development, implementation, production support, and performance tuning of the Enterprise Data Warehouse, Data Mart, Business Intelligence Reporting environments, and support the integration of those systems with other applications. Performs routine analysis to include evaluation of hardware and software based on end user criteria and workflow analysis, design, development, testing, training, implementation, management and support of assigned application systems. The BI Data Engineer develops and maintains system specification and technical documentation. Acts as liaison between Information Technology vendors and Tampa General Hospital. Demonstrates proficiency with computer-based analytical and reporting tools such as Qlik Sense and Microsoft tools. Ensures appropriate documentation of standard operating procedures and user checklists. Responsible for performing job duties in accordance with mission, vision and values of Tampa General Hospital and the BI Team. A BI Data Engineer should be able to explore newer technology options, if need be, and must have a high sense of ownership over every deliverable. Members of this role understand how data is turned into information and knowledge and how the knowledge supports and enables key business processes. They must have an in-depth understanding of the business environment, and strong analytical and communication skills. Individuals must work well within a team environment.
Requires a minimum of a Bachelor's degree in Computer Science, Information Systems, Business Management or specialized training/certification.
Experience beyond the work experience requirement may be substituted for the degree on a 2 years of experience for 1 year of education basis.
Typically requires 5+ years of related technical experience.
Knowledge of business intelligence tools and systems required.
Advanced level experience with ETL Tools like SSIS, ADF, QDI. Deep experience in SQL tuning, tuning ETL solutions, physical optimization of databases.
Advanced Level experience with Microsoft as a Database Platform.
Advanced skills in data analysis and data mining.
Certification in at least one Epic module within the first year.
Experience mentoring colleagues, Project lead with minimal supervision.
Preferred Qualifications
Master’s degree Experience or understanding of Big Data Platforms and Azure Cloud Services.
Health Care Industry knowledge is a plus.
7+ years of experience in enterprise databases and analytic reporting 5+ year of experience using Qlik Sense/QlikView/Tableau/PowerBI.
1+ of experience or understanding of programming languages like Python, Java, R.
Certified in at least one Data Platform and Analytics Tools
Primary Location
Tampa
Work Locations
TGH Corporate Center
Eligible for Remote Work
Hybrid Remote
Job
Information Technology
Organization
TGH - Hospital
Schedule
Full-time
Scheduled Days
Monday, Tuesday, Wednesday, Thursday, Friday
Shift
Day Job
Minimum Salary
100,068.80
Job Posting
Dec 6, 2023, 7:02:14 PM
Show more
Show less","Data Integration, Enterprise Data Integration, Data Warehouse, Data Mart, Business Intelligence Reporting, Qlik Sense, Microsoft Tools, ETL Tools, SSIS, ADF, QDI, SQL, Microsoft, Data Analysis, Data Mining, Epic, Big Data Platforms, Azure Cloud Services, Health Care Industry, Python, Java, R","data integration, enterprise data integration, data warehouse, data mart, business intelligence reporting, qlik sense, microsoft tools, etl tools, ssis, adf, qdi, sql, microsoft, data analysis, data mining, epic, big data platforms, azure cloud services, health care industry, python, java, r","adf, azure cloud services, big data platforms, business intelligence reporting, data integration, data mart, data mining, dataanalytics, datawarehouse, enterprise data integration, epic, etl tools, health care industry, java, microsoft, microsoft tools, python, qdi, qlik sense, r, sql, ssis"
Senior BI Data Engineer,GenesisCare,"Fort Myers, FL",https://www.linkedin.com/jobs/view/senior-bi-data-engineer-at-genesiscare-3781931891,2023-12-17,Florida,United States,Mid senior,Onsite,"At GenesisCare we want to hear from people who are as passionate as we are about innovation and working together to drive better life outcomes for patients around the world.
GenesisCare USA Services, LLC, Senior BI Data Engineer, Fort Myers FL
Work closely with stakeholders and IT development teams to build modern and highly scalable cloud data platform that enables data ingestion, storage, transformations, and preparation of massive datasets for data analytics and machine learning models
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Design and develop optimal data pipeline architecture and infrastructure for data movement and data orchestration aligned with trending data pipeline design patterns in the industry and best practices
Assemble large, complex data sets that meet functional/non-functional business requirements
Perform proof of concepts for innovation and to continuously improve and enhance the capabilities of the business intelligence platform in cloud and scale them out for production use
Contribute ongoing monitoring including cloud resource capacity management, performance monitoring, troubleshooting, and resolving technical issues
Collaborate with the business and technical teams to ensure active support and resolution of risk and incidents.
Job Requirements
Must have a Bachelor's degree or foreign equivalent in Computer Science, Computer Engineering, Information Technology, or a related field, and 5 years of post-bachelor’s, progressive related work experience;
OR a Master's degree or foreign equivalent in Computer Science, Computer Engineering, Information Technology, or a related field, and 3 years of related work experience.
Of the required experience, must have 3 years of experience with the following: Utilizing SQL Azure platform and Microsoft business intelligence suite; Designing and developing data movement and orchestration pipelines using Azure Data Factory, Azure Data Lake Storage, Azure Blob Storage, and Azure SQL; Managing CI/CD build, release, deploy process with Git and Docker containers; Automating Azure resources using Azure CLI, ARM templates and SQL Server PowerShell; and Writing scripts in Python.
Work Schedule: 40 hours per week, M - F (9:00am - 5:00pm)
Telecommuting permitted 5 days a week
Employer will accept any suitable combination of education, training, or experience.
Qualified Applicants: Apply online by clicking the ‘apply for this job’ button at the top of the page
GenesisCare is an Equal Opportunity Employer.
Show more
Show less","Data Engineering, Cloud Computing, Data Analytics, Machine Learning, Data Pipelines, Data Orchestration, SQL, Azure, Microsoft Business Intelligence Suite, Azure Data Factory, Azure Data Lake Storage, Azure Blob Storage, Azure SQL, CI/CD, Git, Docker, Python, Azure CLI, ARM Templates, SQL Server PowerShell","data engineering, cloud computing, data analytics, machine learning, data pipelines, data orchestration, sql, azure, microsoft business intelligence suite, azure data factory, azure data lake storage, azure blob storage, azure sql, cicd, git, docker, python, azure cli, arm templates, sql server powershell","arm templates, azure, azure blob storage, azure cli, azure data factory, azure data lake storage, azure sql, cicd, cloud computing, data engineering, data orchestration, dataanalytics, datapipeline, docker, git, machine learning, microsoft business intelligence suite, python, sql, sql server powershell"
Sr Data Engineer,Lennar,"Miami, FL",https://www.linkedin.com/jobs/view/sr-data-engineer-at-lennar-3749326481,2023-12-17,Florida,United States,Mid senior,Onsite,"Overview
The Senior Data Engineer plays a critical role in designing, developing, and maintaining data solutions that ensure the efficient and effective management of our organization's data assets. This position is responsible for the design, development and optimization of data pipelines, data integration, data observability, quality, and compliance solutions to support the organization's data-driven decision-making and analytics needs. The Senior Data Engineer works with the Enterprise Data Management team and is part of the Enterprise Data and Analytics Organization, which aims to drive improved business outcomes using insights gleaned from data and analytics, infusing them into Lennar’s corporate fabric and is a key role in operationalizing Lennar’s enterprise data fabric.
Responsibilities
Principal Duties and Responsibilities:
Design, build, and operationalize data engineering solutions for Lennar’s Enterprise Data Management platforms and products.
Develop self-service data and analytics capabilities to empower business users to navigate our data ecosystem.
Instrument platforms with robust metrics and monitoring.
Work closely with Platform and Infrastructure teams to integrate data from diverse sources, ensuring seamless data flow and accessibility throughout the organization.
Collaborate closely with technical product owners and data engineers to ensure that the software solutions align with business goals and customer needs.
Create and maintain data models that support the structured storage of data, optimizing data retrieval and analytical processes. Collaborate with Business Intelligence teams to understand their reporting needs and tailor data models accordingly.
Implement and enforce data security measures to protect sensitive information, ensuring compliance with data privacy regulation.
Build integrations into the current technology portfolio to streamline and enhance workflows, specifically for the enterprise data management domain.
Perform code reviews and approve Git pull requests.
Responsible for new development and providing oversight of support for existing developed functionalities by support engineers.
Ensure all solutions meet the business requirements on time and within budget.
Collaborate with cross-functional teams to gather requirements and deliver solutions that align with business objectives.
Qualifications
Education and Experience Requirements:
Bachelor’s or master’s degree in computer science, Information Technology, Engineering, or related technical discipline.
10+ years as a data engineer
5-7 years of experience with Python, YAML, SQL and building CICD pipelines.
5+ years of experience in cloud computing, with a focus on Microsoft Azure.
5+ years in the ‘modern data stack’ – Snowflake, dbt, Fivetran, Prefect, and similar tools.
Demonstrated experience delivering data ingestion, modeling, and consumption capabilities in an enterprise environment.
Demonstrated experience in all aspects of development including, but not limited to, gathering requirements, development of technical components related to process scope and supporting testing and post implementation support.
Expertise in SQL and RDBMS; Experience working with NoSQL database systems.
Experience with database replication tools.
Familiarity with software development tools for task management (Atlassian JIRA).
Proficiency in data wrangling and integration.
Strong understanding of Enterprise ELT/ETL tools, Data Engineering Technology stacks and solutions.
Strong adherence to core software engineering principles (code modularization, versioning, git, testing, Agile etc.)
Additional Requirements:
Ability and willingness to learn about the business, its strategy, objectives, and core business processes.
Ability and willingness to quickly learn new technologies.
Physical Requirements:
This is primarily a sedentary office position which requires the Senior Data Engineer to have the ability to operate computer equipment, speak, hear, bend, stoop, reach, lift, and move and carry up to 25 lbs. Finger dexterity is necessary.
This description outlines the basic responsibilities and requirements for the position noted. This is not a comprehensive listing of all job duties of the Associates. Duties, responsibilities and activities may change at any time with or without notice.
Lennar is an equal opportunity employer and complies with all applicable federal, state, and local fair employment practices laws.
Type
Regular Full-Time
Show more
Show less","Data Engineering, Data Pipelines, Data Integration, Data Observability, Data Quality, Data Compliance, DataDriven DecisionMaking, Data Analytics, Data Fabric, Data Management Platforms, SelfService Data, Analytics, Metrics, Monitoring, Data Security, Data Privacy Regulation, Git, Agile, Cloud Computing, Python, YAML, SQL, CICD Pipelines, Snowflake, dbt, Fivetran, Prefect, SQL, RDBMS, NoSQL, Atlassian JIRA, Data Wrangling, Data Integration, ELT/ETL Tools, Data Engineering Technology Stacks, Core Software Engineering Principles, Ability to Learn, Physical Requirements","data engineering, data pipelines, data integration, data observability, data quality, data compliance, datadriven decisionmaking, data analytics, data fabric, data management platforms, selfservice data, analytics, metrics, monitoring, data security, data privacy regulation, git, agile, cloud computing, python, yaml, sql, cicd pipelines, snowflake, dbt, fivetran, prefect, sql, rdbms, nosql, atlassian jira, data wrangling, data integration, eltetl tools, data engineering technology stacks, core software engineering principles, ability to learn, physical requirements","ability to learn, agile, analytics, atlassian jira, cicd pipelines, cloud computing, core software engineering principles, data compliance, data engineering, data engineering technology stacks, data fabric, data integration, data management platforms, data observability, data privacy regulation, data quality, data security, data wrangling, dataanalytics, datadriven decisionmaking, datapipeline, dbt, eltetl tools, fivetran, git, metrics, monitoring, nosql, physical requirements, prefect, python, rdbms, selfservice data, snowflake, sql, yaml"
Senior Data Engineer,Kobie Marketing,"St Petersburg, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kobie-marketing-3765717278,2023-12-17,Florida,United States,Mid senior,Remote,"Why you will love working for a National Top Workplace
We are a global leader in loyalty marketing.
We work with some of the most well-known brands in the world to deliver market-leading, end-to-end loyalty solutions to enable customer experiences. With a strategy-led, technology-enabled approach, we are consistently named an industry leader by Forrester. The programs we deliver reach more than 330M consumers through loyalty. The impact of these loyalty programs affords us deep brand partnerships, owning a niche in the loyalty space where outcomes matter most.
We Are a Mission And Values Driven Company.
Our mission is to grow enterprise value through loyalty for our clients. Every role within Kobie has a purpose and directly contributes to us achieving this mission.
We are values driven at every point. Over our 30+ year journey, we've created a fun, high-trust, transparent workplace. We believe in leadership and ownership. Our hybrid work environment, personal holidays, casual dress code and focus on diversity and inclusion add to a culture that makes our teammates proud. That pride shines through in the work we do for our clients.
About The Team And What We'll Build Together
We find actionable insights in our clients' loyalty data that helps drive enterprise value. We create real time dashboards to inform internal and external stakeholders.
As a Senior Data Engineer at Kobie, you will play a vital role within our data engineering team, under the direct supervision of the Manager of Data Engineering. Your expertise will be instrumental in implementing ETL/ELT processes and data integrations, populating Kimball style star schemas from a diverse range of data sources across multiple data warehouse implementations in support of our product. Your involvement will extend from requirements gathering to designing business processes and dimensional models. Your profound comprehension of OLTP, Data Vault, and star schemas will be crucial as you delve into source data analysis to assess its potential in addressing business needs. Your objective will be to create scalable, efficient, auditable, and as much as possible, reusable processes.
How You Will Make An Impact
Ensure seamless production support of daily running ETL/ELT processes.
Develop, design, optimize, and maintain ETL/ELT processes.
Conduct data profiling, and source to target mappings, capturing ETL and business metadata for populating Kimball style dimensional models.
Design automated tests and audit logging processes for every stage of our data pipelines.
Design, manage and build event-driven architectures to support real-time data flows and event processing.
Document ETL processes comprehensively including process flow diagrams.
Conduct functional and performance testing to identify bottlenecks and data quality issues.
Implement slowly changing dimensions as well as transaction, accumulating snapshot, and periodic snapshot fact tables.
What You Need To Be Successful
At least 6 years of Data Engineering experience, with a minimum of 2 operating in Snowflake.
Deep understanding of Snowflake Data Platform (data sharing, data clean rooms, marketplace).
Understanding of Database Replication and how data flows from OLTP database systems, ELT architecture and design.
The ability to work independently across multiple projects, communicating effectively to internal data stakeholders across the organization.
The ability to integrate with a wide range of data sources, including APIs messaging systems to capture and normalize streaming data.
Possess the ability to design data pipelines from end to end and train other team members in best practices and processes.
Have a good grasp of the Software Development Life Cycle (SDLC) and Agile Development processes.
Deep understanding of Event Driven Architectures, how they impact Data Pipeline designs and integrations.
Cloud Experience with Azure or OCI, preferred.
Proficient in scripting languages such as Python, and JavaScript.
Experience with orchestration tools like Apache Airflow, Matillion, Mage.ai is preferred.
Data Replication tools experience like Kafka, Goldengate, HVR, Qlik Replicate, preferred.
Our teammates are at the heart of everything we do
Healthy people are happy people, which makes mental and physical health a top priority at Kobie. From robust health insurance and benefits options to free fitness programs like FitOn, to generous vacation time for yourself, we support your health needs fully. In today's job market, we know that employees are choosing only what works best for their life. For those that want career growth, Kobie is the perfect place. We have developed a comprehensive people strategy that helps every teammate know how to advance and progress on their career journey. Beyond title progression, Kobie's competitive pay, 401k matching, annual profit sharing and bonuses all make Kobie a perfect place to build your career.
Kobie a place for all
We don't just accept differences – we embrace, share, and celebrate them!
Employment at Kobie is based solely on a person's merit and qualifications, directly related to professional competence. We do not discriminate against any teammate or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or related condition (including breastfeeding), or any other basis under the law.
We are fiercely committed to fostering a workplace where teammates draw upon their own diverse backgrounds, experiences, and perspectives so that they feel welcomed to bring their authentic self to work every day. While our leadership team fully and completely supports our policy of nondiscrimination and equal opportunity, all teammates share the responsibility to ensure we incorporate the principles of equity, diversity, and inclusion throughout Kobie.
Show more
Show less","Python, JavaScript, Kafka, Goldengate, HVR, Qlik Replicate, Apache Airflow, Matillion, Mage.ai, Snowflake, Dimensional Modeling Techniques, Data Profiling, Data Integration, Data Normalization, Data Warehousing, Data Quality, ETL/ELT Processes, Kimball Style Star Schemas, OLTP, Event Driven Architectures, Agile Development, Software Development Life Cycle, Data Pipeline Design","python, javascript, kafka, goldengate, hvr, qlik replicate, apache airflow, matillion, mageai, snowflake, dimensional modeling techniques, data profiling, data integration, data normalization, data warehousing, data quality, etlelt processes, kimball style star schemas, oltp, event driven architectures, agile development, software development life cycle, data pipeline design","agile development, apache airflow, data integration, data normalization, data pipeline design, data profiling, data quality, datawarehouse, dimensional modeling techniques, etlelt processes, event driven architectures, goldengate, hvr, javascript, kafka, kimball style star schemas, mageai, matillion, oltp, python, qlik replicate, snowflake, software development life cycle"
Senior/Staff Data Engineer,EvenUp,"Tampa, FL",https://www.linkedin.com/jobs/view/senior-staff-data-engineer-at-evenup-3728157945,2023-12-17,Florida,United States,Mid senior,Remote,"EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year.
Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds
. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.
Why we are hiring a Senior/Staff Data Engineer now?
We have experienced unprecedented growth and need to scale out our data warehousing, data tooling and internal analytics. We need to architect the future of our data infrastructure at EvenUp and we’re seeking engineering leaders to help drive that vision.
We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
We need to design & build data warehousing that democratizes data for our entire organization. We will invest in identifying and integrating tools and services that empower our teams to build on top of our data and analytics.
What you’ll do:
Democratize data at EvenUp. Ensure our organization can scale with consistent, standardized access to our data stores and accelerate our ability to build and experiment with data products
Architect and build out the future of data warehousing at EvenUp
Enable and empower our Data Science team to rapidly iterate on model experimentation
Design, organize and refine data storage strategies that reduce development friction for our tech organization
Collaborate with cross functional teams to solve critical data problems
Help grow our nascent Data Insights team and define a “data first” mentality across our organization
What we are seeking:
8+ years of data engineering experience
Previous experience building out data warehousing, data pipelines, and internal analytics
Strong understanding and practical experience with data tooling, BI tools, and systems such as DBT, BigQuery, Elasticsearch
The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions
Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure
You enjoy owning a project from start to finish and love to drive a project across the finish line
Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves:
Have previously built out a Data Insights team at a data-oriented startup
Have previously planned and architected data migrations at scale
Have stood up analytics tooling to enable cross-functional teams
Domain expertise in legal technology, medical records, and working with unstructured data
A successful first year may look like:
75% doing system design and contributing code, starting with shipping code within 2 weeks!
25% collaborating with stakeholders and mentoring, lunch and learns, and more
Leverage a self-starter mindset by taking a product concept and building the feature end to end (whether it’s a component of the system or a significant piece of functionality).
Collaborate with the team to scale the tech stack based on our rapidly growing user base!
Benefits & Perks:
We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:
Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!)
Flexible working hours to match your style
Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months!
Choice of great medical, dental, and vision insurance plan options
Flexible paid time off
A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Data engineering, Data warehousing, Data pipelines, Internal analytics, DBT, BigQuery, Elasticsearch, BI tools, Legal technology, Medical records, Unstructured data","data engineering, data warehousing, data pipelines, internal analytics, dbt, bigquery, elasticsearch, bi tools, legal technology, medical records, unstructured data","bi tools, bigquery, data engineering, datapipeline, datawarehouse, dbt, elasticsearch, internal analytics, legal technology, medical records, unstructured data"
"SR. Scala Engineer, Database Engineering",Experfy,"Hollywood, FL",https://www.linkedin.com/jobs/view/sr-scala-engineer-database-engineering-at-experfy-3676188814,2023-12-17,Florida,United States,Mid senior,Remote,"As a Sr. Software Engineer for our Data Platform Engineering team you will join skilled Scala engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL
processing frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership.
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, and others)
3+ years experience with Scala and Apache Spark
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Apache Spark, Apache Arrow, Scala, SQL, Data Warehousing, Distributed Systems, Query Optimization, Database Optimization, Cluster Management, Performance Tuning, Software Engineering, Web3, Blockchain, Data Acquisition, Data Processing, Data Engineering, Data Management, HTAP Database","apache spark, apache arrow, scala, sql, data warehousing, distributed systems, query optimization, database optimization, cluster management, performance tuning, software engineering, web3, blockchain, data acquisition, data processing, data engineering, data management, htap database","apache arrow, apache spark, blockchain, cluster management, data acquisition, data engineering, data management, data processing, database optimization, datawarehouse, distributed systems, htap database, performance tuning, query optimization, scala, software engineering, sql, web3"
Senior Data Engineer,Raft,"Tampa, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-raft-3702860380,2023-12-17,Florida,United States,Mid senior,Remote,"This is a U.S. based position. All of the programs we support require
U.S. citizenship to be eligible for employment. All work must be conducted within the continental U.S.
Who we are:
Raft (https://goraft.tech) is a customer-obsessed non-traditional small business with purposeful focus on Distributed Data Systems, Platforms at Scale, and Complex Application Development, with headquarters in Reston, VA. Our range of clients include innovative federal and public agencies leveraging design thinking, cutting edge tech stack, and cloud native ecosystem. We build digital solutions that impact the lives of millions of Americans.
Our team is rapidly growing and looking for an experienced
Senior Data Engineer
to support our customer and join our passionate team of high-impact problem solvers. We enjoy the challenges of human-centered design, security, and scale to create better outcomes for our federal agency partners. We are a remote-first and work completely in the
open source
.
About the role:
Senior Data Engineers
on our
Distributed Systems
team are focused on building data platforms that make it easy for different types of user personas to access data from a central control plane. This includes building backend services, connecting OSS projects in a repeatable and performant way, and extending feature sets.
Required Qualifications:
Experience building data infrastructure and platforms using streaming frameworks
Experience/Interest in OSS projects like: Apache Flink, Apache Pulsar, Apache Kafka, Apache Beam, Apache Storm, Apache Airflow
Hands-on experience with Golang
Hands-on experience with Spark
Ability to build cloud-native, scalable services
Higher education degree or drop-out in Mathematics, CS, Statistics
Obtain Security+ within the first 90 days of employment with Raft
Highly preferred:
Work with the Platform team to run distributed OSS systems at scale on Kubernetes
Think of ways to implement data security at the row and column levels
Contribute features back to the OSS projects in dedicated time
Build prototypes, gather/implement feedback, delight users.
Design and develop data best practices using Kafka, ElasticSearch, Presto/Trinio
Experience with - Gremlin / GraphQL
Experience with Nebula / Neo4j / ArcadeDB
Clearance Requirements:
Active Top Secret Security clearance
Work Type:
Remote - local in Tampa, FL
May require up to 25% travel
What we will offer you:
Highly competitive salary
Fully covered healthcare, dental, and vision coverage
401(k) and company match
Unlimited PTO + 11 paid holidays
Education & training benefits
Annual budget for your tech/gadgets needs
Monthly box of yummy snacks to eat while doing meaningful work
Remote, hybrid, and flexible work options
Team off-site in fun places!
Generous Referral Bonuses
And More!
Our Vision Statement:
We bridge the gap between humans and data through radical transparency and our obsession with the mission.
Our Customer Obsession:
We will approach every deliverable like it's a product. We will adopt a customer-obsessed mentality. As we grow, and our footprint becomes larger, teams and employees will treat each other not only as teammates but customers. We must live the customer-obsessed mindset, always. This will help us scale and it will translate to the interactions that our Rafters have with their clients and other product teams that they integrate with. Our culture will enable our success and set us apart from other companies.
How do we get there?
Public-sector modernization is critical for us to live in a better world. We, at Raft, want to innovate and solve complex problems. And, if we are successful, our generation and the ones that follow us will live in a delightful, efficient, and accessible world where out-of-box thinking, and collaboration is a norm.
Raft’s core philosophy is
Ubuntu: I Am, Because We are
. We support our “nadi” by elevating the other Rafters. We work as a hyper collaborative team where each team member brings a unique perspective, adding value that did not exist before. People make Raft special. We celebrate each other and our cognitive and cultural diversity. We are devoted to our practice of innovation and collaboration.
We’re an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.
Show more
Show less","Data Engineering, Distributed Systems, Streaming Frameworks, Apache Flink, Apache Pulsar, Apache Kafka, Apache Beam, Apache Storm, Apache Airflow, Golang, Spark, Cloudnative, Kubernetes, Data Security, Gremlin, GraphQL, Nebula, Neo4j, ArcadeDB","data engineering, distributed systems, streaming frameworks, apache flink, apache pulsar, apache kafka, apache beam, apache storm, apache airflow, golang, spark, cloudnative, kubernetes, data security, gremlin, graphql, nebula, neo4j, arcadedb","apache airflow, apache beam, apache flink, apache kafka, apache pulsar, apache storm, arcadedb, cloudnative, data engineering, data security, distributed systems, golang, graphql, gremlin, kubernetes, nebula, neo4j, spark, streaming frameworks"
Senior Data Engineer,Insight Global,"Tampa, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-insight-global-3779394476,2023-12-17,Florida,United States,Mid senior,Remote,"***W2 ONLY - CANNOT SPONSOR***
Must Haves
-7+ years of data engineering experience
-7+ years of SQL (TSQL) programming experience
-4+ years of experience with full Azure Stack (Blob Storage, Data Factory, Data Lake, DevOps)
-Strong experience with ETL (dbt, SSIS)
-Experience building Data Pipelines in Azure
-Proven experience take on a project independently or leading a project
-Excellent communication skills
Plusses
-Snowflake
-Data Visualization Tools (Power BI, Tableau)
-Microsoft Azure Data Fundamentals Certified
Day-to-Day
Insight Global is seeking a senior data engineer to join the Data & analytics team. They will lead the technical design and development of the enterprise data ecosystem. Some responsibilities include: Review requirements and design data architectures based on the standards set by the manager which drive the client's data strategy implementation and transform business strategies into information architectures. They will be responsible for scoping new architecture, developing and implementing scalable enterprise data solutions to support the client's current & future initiatives. They will also be responsible for the designing and implementation of solutions that proactively manage and monitor the data ecosystem of analytics, data lakes, warehouses, MDM platforms, and other tools.
Show more
Show less","Data Engineering, SQL (TSQL), Azure Stack (Blob Storage Data Factory Data Lake DevOps), ETL (dbt SSIS), Data Pipelines, Snowflake, Data Visualization Tools (Power BI Tableau), Microsoft Azure Data Fundamentals, Data Architectures, Scalable Enterprise Data Solutions, Data Ecosystem Management, Data Lakes, Warehouses, MDM Platforms","data engineering, sql tsql, azure stack blob storage data factory data lake devops, etl dbt ssis, data pipelines, snowflake, data visualization tools power bi tableau, microsoft azure data fundamentals, data architectures, scalable enterprise data solutions, data ecosystem management, data lakes, warehouses, mdm platforms","azure stack blob storage data factory data lake devops, data architectures, data ecosystem management, data engineering, data lakes, data visualization tools power bi tableau, datapipeline, etl dbt ssis, mdm platforms, microsoft azure data fundamentals, scalable enterprise data solutions, snowflake, sql tsql, warehouses"
Senior Data Engineer,Vista,"Fort Lauderdale, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-vista-3784291953,2023-12-17,Florida,United States,Mid senior,Remote,"Job Profile
About Team
The Data Foundation Team is highly critical for the organization to provide timely, accurate and most up to date data so that the business can take decisions accordingly. The team works with several application teams, data analytics, data science team etc. We are looking for a highly skilled and experienced
Senior Data Engineer
to design, implement, and maintain robust and scalable data pipelines.
About Company
Vista Tech plays a vital role in the Vista group operations by delivering and accelerating comprehensive technology solutions across all brands. Vista’s end-to-end and click-to-flight solutions offer the industry's only comprehensive flight booking platform, seamlessly integrating global operations, and leveraging AI and machine learning to optimize pricing and fleet movement. Comprised of the Product Management, Engineering, and IT teams, Vista Tech’s mission is to enhance transparency and accessibility in private aviation through the development of the world's largest digital private aviation marketplace. In achieving this, Vista Tech always ensures the utmost safety and efficiency for FLIGHT CREW, EMPLOYEES and Members, while fostering a culture of innovation and excellence.
You will report to Engineering Manager and play a crucial role in driving the technical direction of our projects and guiding the team in adopting best practices and cutting-edge technologies. This position is a 100% remote role with regular shif timings (9 AM to 6 PM EST). You will collaborate with cross-functional teams, provide technical leadership, and contribute to the entire software development lifecycle.
Your Responsibilities
Scalable Data Infrastructure: Lead the development and maintenance of highly scalable data pipelines, playing a crucial role in fortifying our data foundation.
Technical Excellence: Demonstrate hands-on technical expertise in designing, building, and documenting complex data pipelines while adhering to data engineering best practices.
Cross-Functional Collaboration: Collaborate closely with data engineering, analytics, and data science leadership to continuously enhance the functionality and capabilities of our data systems.
Process Optimization: Identify opportunities for internal process improvements, spearheading automation of manual tasks, optimizing data delivery mechanisms, and redesigning infrastructure to ensure greater scalability and efficiency.
Data Integration Mastery: Define and construct the infrastructure necessary to facilitate efficient extraction, transformation, and loading (ETL) of data from a diverse array of sources.
Required Skills, Qualifications, And Experience
Strong Analytical Foundation: A robust background in mathematics, statistics, computer science, data science, or a related discipline, showcasing your analytical prowess.
Programming Proficiency: Advanced expertise in programming languages, particularly Python and SQL, to tackle complex data challenges.
Production Experience: Proven experience in the production environment with a range of essential tools and platforms, including Snowflake, DBT, Airflow, Amazon Web Services (AWS), Docker/Kubernetes, and PostgreSQL.
Database Mastery: Proficiency in database technologies, including Snowflake, PostgreSQL, Redshift, and others, enabling efficient data management.
Exceptional Organizational Skills: Strong organizational capabilities, allowing you to manage multiple projects and priorities concurrently while consistently meeting deadlines.
Additional Assets: Familiarity and experience with additional tools and technologies, such as AWS certification, Kafka Streaming/Kafka Connect, MongoDB, and CI/CD tools like GitLab, Jira, and Confluence, are highly advantageous.
Show more
Show less","Data Pipelines, Data Engineering, Machine Learning, Artificial Intelligence, Cloud Computing, Software Development Lifecycle, ETL, Python, SQL, Snowflake, DBT, Airflow, AWS, Docker, Kubernetes, PostgreSQL, Redshift, MongoDB, Kafka Streaming, Kafka Connect, GitLab, Jira, Confluence, CI/CD","data pipelines, data engineering, machine learning, artificial intelligence, cloud computing, software development lifecycle, etl, python, sql, snowflake, dbt, airflow, aws, docker, kubernetes, postgresql, redshift, mongodb, kafka streaming, kafka connect, gitlab, jira, confluence, cicd","airflow, artificial intelligence, aws, cicd, cloud computing, confluence, data engineering, datapipeline, dbt, docker, etl, gitlab, jira, kafka connect, kafka streaming, kubernetes, machine learning, mongodb, postgresql, python, redshift, snowflake, software development lifecycle, sql"
Senior Data Engineer,GOBEL,"St Petersburg, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-gobel-3768099912,2023-12-17,Florida,United States,Mid senior,Hybrid,"GOBEL is seeking a highly experienced and detail-oriented professional to join our team as a Senior Data Engineer. In this role, you will lead the effort to manage and optimize data infrastructure, ensuring the collection, organization, analysis, and maintenance of data in our data warehouses. As a Senior Data Engineer, you must have a deep understanding of data modeling, analytics, and technology in the context of fundraising operations, as well as a commitment to providing excellent support and services to our clients.
The ideal candidate for this role is a proactive problem solver, team-oriented, and thrives in a collaborative environment. You will be responsible for building and maintaining scalable data pipelines, implementing data privacy principles, and supporting machine learning models. Additionally, you will collaborate with cross-functional teams to understand reporting requirements, drive data management projects, and innovate solutions to enhance our products and services.
Responsibilities
Cross-Functional Team Collaboration
Meet with internal teams to clarify and document reporting requirements.
Collaborate to understand existing issues and new requirements.
Data Integration and Transformation:
Assemble large, complex datasets that meet business requirements.
Identify, design, and implement process improvements for scalability.
Build infrastructure for data extraction, transformation, and loading using AWS and SQL technologies.
Analytical Insights:
Develop analytical tools to provide insights into key performance metrics.
Work closely with data scientists to fulfill data-related requests.
Monitor daily screenings and product offerings to clients.
Data Cleaning and Optimization:
Access and query various data sources, including relational databases, SQL databases, and APIs.
Clean, deduplicate, and optimize data at scale for ingestion and consumption.
Proactively address data management issues to improve data quality.
Technical Leadership:
Implement automated workflows and production deployment frameworks.
Drive the design, code, and maintenance of data engineering standards.
Troubleshoot complex data issues and perform root cause analysis.
Interact with cross-functional teams to understand their needs and resolve issues.
Perform quality checks and user acceptance testing.
Innovation and Documentation:
Seek innovative ways to enhance products and services.
Develop and maintain documentation and materials.
Qualifications
Bachelor's degree in Computer Science, Information Systems, Business, or a related field.
Minimum of six (6) years of experience in developing scalable Big Data applications on distributed computing platforms.
Proficiency in SQL, Python, APIs, data warehouses, and managing large datasets.
Strong problem-solving skills, ability to collaborate on complex issues, and innovate solutions.
Deep understanding of data warehousing, data modeling, governance, and architecture.
Excellent relationship-building skills across cross-functional teams.
Experience with cloud platforms, including Azure.
Familiarity with reporting and analytical tools like Power BI, Tableau, etc.
Comfort working in a fast-paced, results-oriented environment.
Effective verbal and written communication skills.
Quick adaptability to new programming languages, technologies, and frameworks.
Exceptional attention to detail, responsiveness, and project management abilities.
Willingness to work outside of traditional business hours and interact with clients virtually.
Company Values
Our People:
We are hard workers, united in a common purpose to affect positive change. We have fun and celebrate our successes.
Mission Oriented:
We seek to provide the best ideas and smartest technologies to help healthcare organizations generate more philanthropic revenue to build healthier communities.
Client Oriented:
We value every client relationship and are committed to delivering incredible experiences and outcomes.
Integrity:
We act with honesty and transparency, and we are committed to the success of each other and our clients.
Innovation:
We won't settle for incremental change. We want to transform healthcare philanthropy.
Accountability:
We don't make excuses; we find ways to succeed. We take full responsibility for delivering on our expectations.
GOBEL offers compensation in the range of $100,000-$150,000 based on experience, with the opportunity to earn an annual bonus. In addition, GOBEL offers excellent healthcare, including company-paid employee coverage for medical, dental, vision, prescription coverage, disability, and life insurance, and retirement savings plan (401k) with a company match. We also provide twenty (20) days of annual paid time off, and twelve (12) paid holidays.
If interested, please forward your resume to Human Resources at careers@gobelgroup.com. Work samples and manager references will be requested as part of the interview process.
Show more
Show less","Data Engineering, Big Data, Data Infrastructure, Data Collection, Data Organization, Data Analysis, Data Maintenance, Data Warehousing, Data Modeling, Analytics, Machine Learning, CrossFunctional Collaboration, Data Integration, Data Transformation, Data Extraction, Data Loading, AWS, SQL, Analytical Tools, Key Performance Metrics, Data Cleaning, Data Optimization, Data Quality, Automated Workflows, Production Deployment Frameworks, Data Engineering Standards, Data Warehousing, Data Governance, Data Architecture, Cloud Platforms, Azure, Reporting Tools, Tableau, Power BI, FastPaced Environment, Verbal Communication, Written Communication, Programming Languages, Technologies, Frameworks, Project Management, Attention to Detail, Responsiveness","data engineering, big data, data infrastructure, data collection, data organization, data analysis, data maintenance, data warehousing, data modeling, analytics, machine learning, crossfunctional collaboration, data integration, data transformation, data extraction, data loading, aws, sql, analytical tools, key performance metrics, data cleaning, data optimization, data quality, automated workflows, production deployment frameworks, data engineering standards, data warehousing, data governance, data architecture, cloud platforms, azure, reporting tools, tableau, power bi, fastpaced environment, verbal communication, written communication, programming languages, technologies, frameworks, project management, attention to detail, responsiveness","analytical tools, analytics, attention to detail, automated workflows, aws, azure, big data, cloud platforms, crossfunctional collaboration, data architecture, data cleaning, data collection, data engineering, data engineering standards, data extraction, data governance, data infrastructure, data integration, data loading, data maintenance, data optimization, data organization, data quality, data transformation, dataanalytics, datamodeling, datawarehouse, fastpaced environment, frameworks, key performance metrics, machine learning, powerbi, production deployment frameworks, programming languages, project management, reporting tools, responsiveness, sql, tableau, technologies, verbal communication, written communication"
Data Engineer,Axiom Global Technologies,"Tampa, FL",https://www.linkedin.com/jobs/view/data-engineer-at-axiom-global-technologies-3746776753,2023-12-17,Florida,United States,Mid senior,Hybrid,"Want to join a global leader in consulting, technology services and digital transformation? Our client offers great career opportunities, competitive compensation, and wonderful training programs
Minimum Qualifications:
Possess a minimum of 3 years of practical experience in Data Engineering, demonstrating proficiency in crafting intricate SQL queries using Oracle PL/SQL.
Have a minimum of 3 years of hands-on experience in performance optimization and tuning methodologies, particularly in SQL tuning.
Bring to the table at least 3 years of experience working with extensive datasets.
Showcase a minimum of 3 years of expertise in ETL strategies, encompassing design and development.
Demonstrate a minimum of 3 years of experience incorporating resiliency and stability design principles, including Site Reliability Engineering (SRE).
Show more
Show less","Oracle PL/SQL, SQL, Data Engineering, ETL, Performance optimization, Tuning, Site Reliability Engineering (SRE), Resiliency, Stability, Extensive datasets","oracle plsql, sql, data engineering, etl, performance optimization, tuning, site reliability engineering sre, resiliency, stability, extensive datasets","data engineering, etl, extensive datasets, oracle plsql, performance optimization, resiliency, site reliability engineering sre, sql, stability, tuning"
Senior Data Engineer,Gravity IT Resources,Miami-Fort Lauderdale Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-gravity-it-resources-3776618980,2023-12-17,Florida,United States,Mid senior,Hybrid,"Job Title: Data Engineer
Location:
Dania Beach, FL (Hybrid onsite)
Job-Type:
Contract to Hire
Referral Fee:
+/- 2,600
Employment Eligibility:
Gravity cannot transfer nor sponsor a work visa for this position. Applicants must be eligible to work in the U.S. for any employer directly (we are not open to contract or “corp to corp” agreements).
Position Overview:
Gravity is looking for talented Data Engineer to be responsible for developing and supporting the data management and analytics platforms including data infrastructure in order to support the evolution of travel products. The Data Engineer is focused on the design and development of data ingestion, processing and storage pipelines as well as the transformation of raw data into business insights that enable our client to become a digital disruptor.
Our client is major travel company that is building out their technology team and eCommerce application here in Broward County. Engineering reigns supreme with this client, making it a great place to work for individuals that have solid tech skills and want to work with most innovative technologies. Our client’s corporate culture is focused on safety, caring, integrity, passion and fun. If you are looking to be a part of a start-up environment with the support and resources of a large enterprise, this could be the company for you!
Need resources that work with data at large scale/ proven experience working with Terabytes of data.
Experience working with real time data
Strong/expert level Python skills
Dataflow
Understand Data structure
Understand dataflow
First in first out
NoSQL
T-SQL (Microsoft SQL and Oracle are not helpful)
Redshift
PostgreSQL
This resource will be architecting the design and perform maintenance/support later
Big Database exp: Apache Beam
ML experience is a plus
Strong experience with Google Big Query
Show more
Show less","Data Engineer, Data Management, Data Analytics, Data Infrastructure, Data Ingestion, Data Processing, Data Storage, Data Transformation, Business Insights, Digital Disruption, eCommerce, Python, Dataflow, Data Structure, First in First Out, NoSQL, TSQL, Redshift, PostgreSQL, Apache Beam, Machine Learning, Google Big Query","data engineer, data management, data analytics, data infrastructure, data ingestion, data processing, data storage, data transformation, business insights, digital disruption, ecommerce, python, dataflow, data structure, first in first out, nosql, tsql, redshift, postgresql, apache beam, machine learning, google big query","apache beam, business insights, data infrastructure, data ingestion, data management, data processing, data storage, data structure, data transformation, dataanalytics, dataengineering, dataflow, digital disruption, ecommerce, first in first out, google big query, machine learning, nosql, postgresql, python, redshift, tsql"
Data Engineer,Trademark Recruiting/Consulting,"Tampa, FL",https://www.linkedin.com/jobs/view/data-engineer-at-trademark-recruiting-consulting-3654471496,2023-12-17,Florida,United States,Mid senior,Hybrid,"Our client is looking for an energetic high-performing Data/BI Engineer who will be responsible for both Data development on BI & Analytics as well as software application design, development, testing, and problem resolution of software and web applications. The scope of work includes small system enhancements to major system projects. Act as a key contributor in a complex team environment with multiple departments, vendors and functions. May lead teams or projects while sharing your expertise
Team of 6 Data Engineers reporting to the Manager of Data and Development
Hybrid position so the person must be located in the Tampa Bay area or planning to relocate to the Tampa Bay area
Responsibilities:
Produce intelligence reports (obtain information from internal and external sources, assess the value and provide actionable insight).
Work on ad-hoc requests and produce the required deliverables as mandated by the requests.
Responsible for data quality both for the raw data as well as the integrity of the calculations used for Measures
Analyze and troubleshoot complex data cleansing, data integration, and data warehouse system issues, identify the reasons for problems, failures and malfunctions and develop optimal solutions.
Monitor data integration and reporting anomalies and make adjustments as required.
Work on Data conversion projects – most applications are on-prem
Will be responsible for a set of Compliance related reporting from Enterprise Data Warehouse
Function as an active member of an agile team through consistent development practices (tools, common components, and documentation)
Conduct integration tests as defined in the test specifications, including event logging and reporting of results
Perform unit and assembly testing of data software components including the design, implementation, evaluation, and execution of unit and assembly test scripts
Conduct code reviews and tests of automated build scripts
Debug data components, identifies, fixes and verifies remediation of code defects
Qualifications:
Bachelor’s degree required in Computer Science, Information Technology or a related field. Valid experience will be considered along with a Bachelor’s degree in another discipline.
3+ years of database (SQL Server) development experience with excellent query and stored procedure skills (T-SQL). Senior Engineers will have 5+ years of experience
Experience with ETL using SSIS and other ETL tools. Should have excellent knowledge of building and using T-SQL Stored Procedures.
Experience with standard frameworks for reconciliation and traceability of data.
Strong and effective inter-personal and communication skills and the ability to work well with teams internal to the department as well as other departments
Experience with Power BI is desirable, but not required. Will get the opportunity to learn Power BI
Our client cannot sponsor any Visas at this time (US Citizen or Green card holder only)
Show more
Show less","Data Engineering, BI, Analytics, Software Development, Testing, Problem Resolution, SQL Server, TSQL, SSIS, ETL, Data Reconciliation, Data Traceability, Power BI","data engineering, bi, analytics, software development, testing, problem resolution, sql server, tsql, ssis, etl, data reconciliation, data traceability, power bi","analytics, bi, data engineering, data reconciliation, data traceability, etl, powerbi, problem resolution, software development, sql server, ssis, testing, tsql"
Data Engineer II,Shift4,"Tampa, FL",https://www.linkedin.com/jobs/view/data-engineer-ii-at-shift4-3777168604,2023-12-17,Florida,United States,Mid senior,Hybrid,"Shift4 (NYSE: FOUR)
is boldly redefining commerce by simplifying complex payments ecosystems across the world. As the leader in commerce-enabling technology, Shift4 powers billions of transactions annually for hundreds of thousands of businesses in virtually every industry. For more information, visit www.shift4.com.
Shift4 is expanding globally and we are looking for a Data Engineer to join our team of world-class in-house engineers! As a Data Engineer II, you will be responsible for designing, developing, and maintaining robust data infrastructure solutions to support the organization's data-driven initiatives. We are looking for self-driven and motivated individuals who take ownership of their projects. You will work closely with cross-functional teams, including project managers, business analysts, and software engineers, to ensure the availability, reliability, and scalability of our systems. This role requires expertise in data engineering and ETL processes.
This role is hybrid / onsite and can be based at any of the following Shift4 locations (relocation assistance may be available): Center Valley, PA (headquarters) / Silver Spring, MD / Tampa, FL / Las Vegas, NV
Responsibilities
Develop and maintain automated data pipelines and ETL processes to ingest, transform, and load large volumes of data from diverse sources into relational databases utilizing SQL, stored procedures, functions and other database technologies to move data.
Ensure the quality, integrity, and security of data by implementing data validation, data cleansing, and data governance processes.
Work on necessary new system developments, enhancements and bugs utilizing an agile software development lifecycle (SDLC).
Optimize and fine-tune data processes and systems for performance, scalability, and reliability.
Monitor and troubleshoot data pipelines, data processing jobs, and database performance issues including off hours support when necessary.
Aid in the design and implementation of efficient data models and schemas to support data analysis, reporting, and visualization needs.
Stay up-to-date with emerging data engineering technologies, tools, and best practices, and recommend improvements to existing data infrastructure.
Collaborate with software engineers to integrate data engineering solutions into applications and software products.
Document data engineering processes, data models, and system configurations for knowledge sharing and future reference.
Qualifications
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Minimum of 3 years of experience in data engineering or a similar role.
Advanced experience with relational databases like MSSQL, PostgreSQL, or AWS Redshift.
Advanced experience with SQL.
Experience with MSSQL SSIS or other ETL Tools.
Experience with reading data models.
Experience working in an agile software development lifecycle (SDLC).
Familiarity with data warehousing concepts and technologies.
Understanding of data governance, data security, and data privacy principles.
Strong problem-solving and analytical skills, with a keen attention to detail.
Ability to prioritize multiple tasks and easily adjust to changing priorities.
Excellent interpersonal, verbal and written skills including documentation of complex technical solutions.
Ability to identify problems, initiate solutions, and effectively collaborate and communicate with team members.
Relevant certifications in data engineering.
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.
Show more
Show less","Data Engineering, ETL processes, SQL, MSSQL, PostgreSQL, AWS Redshift, MSSQL SSIS, Data modeling, Data warehousing, Agile development, Data governance, Data security, Data privacy, Problemsolving, Analytical skills, Attention to detail, Prioritization, Adaptability, Interpersonal skills, Verbal communication, Written communication, Documentation, Collaboration, Team work, Data engineering certification","data engineering, etl processes, sql, mssql, postgresql, aws redshift, mssql ssis, data modeling, data warehousing, agile development, data governance, data security, data privacy, problemsolving, analytical skills, attention to detail, prioritization, adaptability, interpersonal skills, verbal communication, written communication, documentation, collaboration, team work, data engineering certification","adaptability, agile development, analytical skills, attention to detail, aws redshift, collaboration, data engineering, data engineering certification, data governance, data privacy, data security, datamodeling, datawarehouse, documentation, etl, interpersonal skills, mssql, mssql ssis, postgresql, prioritization, problemsolving, sql, team work, verbal communication, written communication"
Senior Data Engineer - GCP Google Cloud Platform,Dun & Bradstreet,"Jacksonville, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-gcp-google-cloud-platform-at-dun-bradstreet-3778710420,2023-12-17,Florida,United States,Mid senior,Hybrid,"Why We Work at Dun & Bradstreet
Dun & Bradstreet unlocks the power of data through analytics, creating a better tomorrow. Each day, we are finding new ways to strengthen our award-winning culture and accelerate creativity, innovation and growth. Our 6,000+ global team members are passionate about what we do. We are dedicated to helping clients turn uncertainty into confidence, risk into opportunity and potential into prosperity. Bold and diverse thinkers are always welcome. Come join us!
This position is a hybrid role and will need to be in Austin office 2 days a week - NO REMOTE
The Senior Data Operations Engineer will develop, maintain, and analyze datasets from diverse sources, including mobile and web, government agencies, web crawls, social media, and proprietary datasets, to create insights for our clients, power our platform, and create an innovative market understanding.
The Senior Data Operations Engineer will create designs and share ideas for creating and improving data pipelines and tools.
Key Requirements:
Collaborate with the data, platform, QA, and DevOps teams to design and construct advanced systems for processing, analyzing, searching, and visualizing vast datasets.
Architect resilient systems and write highly fault-tolerant software to consistently deliver high-quality results.
Take initiative to become familiar with existing application code and achieve a complete understanding of how the applications function.
Pioneering novel methods for extracting intelligence from a wide array of unique data sources.
Generate fresh insights for our clients, provide novel perspectives on their markets.
Co-create and document data processing systems that are easy to maintain, fostering collaborative and supportive team environment.
Help maintain existing systems, including troubleshooting and resolving alerts.
Be a good collaborator with your peers. Be easy to get ahold of and attend all required meetings.
Share ideas across teams to spread awareness and use of frameworks and tooling.
Share a friendly, supportive, and reliable attitude with a great team that hold each other accountable.
This position is a hybrid role and will need to be in Austin office 2 days a week - NO REMOTE
Key Requirements:
Extensive experience working with GCP services, including Big Query, Dataflow, Pub/Sub, Cloud Storage, Cloud Run, Cloud Functions and related technologies is required.
Extensive experience with SQL and relational databases, including optimization and design.
Expertise in containerized infrastructure and CI/CD systems, including CloudBuild, Docker, Kubernetes, and GitHub Actions.
Testable and efficient Python coding for data processing and analysis.
Experience with Amazon Web Services (EC2, RDS, S3, Redshift, EMR, and more).
Experience with OS level scripting (bash, sed, awk, grep, etc.).
Experience in AdTech, web cookies, and online advertising technologies.
Familiarity with parallelization of applications on a single machine and across a network of machines.
Experience with version control (GIT/Github/BitBucket) and Agile Project Management tools (Clickup/Jira/Confluence).
Experience with object-oriented programming, functional programming a plus
Analytic tools and ETL/ELT/data pipeline frameworks a plus.
This position is a hybrid role and will need to be in Austin office 2 days a week - NO REMOTE
All Dun & Bradstreet job postings can be found at https://www.dnb.com/about-us/careers-and-people/joblistings.html . Official communication from Dun & Bradstreet will come from an email address ending in @dnb.com.
Notice to Applicants: Please be advised that this job posting page is hosted and powered by Lever. Your use of this page is subject to Lever's Privacy Notice and Cookie Policy , which governs the processing of visitor data on this platform.
Show more
Show less","Data engineering, Data analytics, Data visualization, Cloud Computing, GCP, BigQuery, Dataflow, Pub/Sub, Cloud Storage, Cloud Run, Cloud Functions, SQL, Relational databases, Containerization, Docker, Kubernetes, GitHub Actions, Python, Agile Project Management, GIT, Clickup, Jira, Confluence, ETL/ELT, Data pipeline frameworks, Objectoriented programming, Functional programming, Unix scripting, AdTech, Web cookies, Online advertising technologies","data engineering, data analytics, data visualization, cloud computing, gcp, bigquery, dataflow, pubsub, cloud storage, cloud run, cloud functions, sql, relational databases, containerization, docker, kubernetes, github actions, python, agile project management, git, clickup, jira, confluence, etlelt, data pipeline frameworks, objectoriented programming, functional programming, unix scripting, adtech, web cookies, online advertising technologies","adtech, agile project management, bigquery, clickup, cloud computing, cloud functions, cloud run, cloud storage, confluence, containerization, data engineering, data pipeline frameworks, dataanalytics, dataflow, docker, etlelt, functional programming, gcp, git, github actions, jira, kubernetes, objectoriented programming, online advertising technologies, pubsub, python, relational databases, sql, unix scripting, visualization, web cookies"
Google Databrick Data Engineer - 10+ years,TekIntegral,"Tampa, FL",https://www.linkedin.com/jobs/view/google-databrick-data-engineer-10%2B-years-at-tekintegral-3687051162,2023-12-17,Florida,United States,Mid senior,Hybrid,"Title: Google Databrick Data Engineer
Location: Candidates will work Hybrid from their location (Texas, Florida, Georgia, Chicago, Pennsylvania, etc. etc.)
Please don't submit candidates from NY or CA
Duration: Long term contract
Work Auth: USC/GC
LinkedIn: Yes
Must have both Google (recently) and Databricks(extensive)
They'll need to have a Github demonstrating their history of work
Required Skills (Very Hands-on)
Google Cloud (Big Query, Data Proc, etc.)
Google Cloud Functions(Java, Python, Go, Node. js, and Rust)
Databricks (Python, PySpark, Scala, Airflow)
Show more
Show less","Google Cloud, Big Query, Data Proc, DataBricks, PySpark, Scala, Airflow, Python, Java, Node.js, Rust, Go","google cloud, big query, data proc, databricks, pyspark, scala, airflow, python, java, nodejs, rust, go","airflow, big query, data proc, databricks, go, google cloud, java, nodejs, python, rust, scala, spark"
Data Engineer,Moran Towing Corporation,"Jacksonville, FL",https://www.linkedin.com/jobs/view/data-engineer-at-moran-towing-corporation-3774905036,2023-12-17,Florida,United States,Mid senior,Hybrid,"Brief Description
Join a leading company in the Maritime Industry in a critical role on our IT team!
Moran Towing Corporation (www.morantug.com) has an exciting opportunity available as a Data Engineer in our Jacksonville, FL office. This full-time, Exempt position reports to the Director, Application Development and Support.
Moran Towing Corporation is one of the oldest companies in America and has prospered by continuously reinventing itself for over 160 years. Starting in 1860 as a tugboat company, we have grown from those roots to become a dominant provider of commercial ship docking services, marine transportation, LNG support operations, environmental recovery, and commercial diving services. While economies, cultures, markets, and technology will always continue to evolve, our steadfast commitment to partnering with our employees and customers to solve our customers’ marine requirements has enabled us to stay the course year after year.
Position Summary
The Data Engineer is skilled in Azure and has a crucial role in gathering, analyzing and transforming data to provide valuable insights for Moran’s business. The primary focus of the position will be to leverage Microsoft Azure services, including Azure Data Lake and Synapse Analytics, to optimize data storage, retrieval, and reporting processes. The ideal candidate will have a strong foundation in data analytics, excellent SQL skills, and expertise in building and optimizing data solutions on the Azure platform. The Data Engineer will also apply data visualization skills to create reports and dashboards.
Specific Responsibilities include, but are not limited, to:
Primary Responsibilities
Utilizes Azure Data Lake Storage Gen2, Azure Data Factory, SQL Server Integration Services, and Power Query to transform and move data across various formats and sources.
Creates and manages computing and data resources in Moran’s on-premises servers and in the Cloud environment (Azure).
Leads the design and development of robust and scalable data warehouse solutions, ensuring efficient storage, retrieval, and analysis of large datasets.
Utilizes Apache Spark for distributed data processing, implementing efficient ETL (Extract, Transform, Load) processes to handle diverse data sources and formats.
Optimizes Spark jobs for performance, scalability, and reliability, leveraging Spark's core features to meet data processing and analytics needs.
Designs, implements, and maintains data pipelines on Azure Synapse Analytics, ensuring seamless integration with other Azure services and tools.
Works with Azure Synapse SQL Pools to optimize data storage and retrieval; Implements best practices for data security and compliance.
Programs and scripts using C#, Python, R, PowerShell, JSON, and XML.
Develops queries in multiple languages such as SQL, DAX, and Python.
Programs databases using stored procedures and user-defined functions.
Works with Moran's data warehouse and contributes to strategies and solutions for data migration, database optimization, and data architecture for analytics projects.
Transforms and moves data in multiple formats and sources using Azure Data Factory, SQL Server Integration Services, and Power Query.
Creates strategies and designs solutions for a wide variety of use cases such as data migration (end to end ETL/ELT process), database optimization, and data architectural solutions for analytics data projects.
Collaborates with stakeholders on requirements for reports and analytics.
Secondary Responsibilities
Designs and develops reports, data sets, data flows, and data marts in Power BI and SQL Server Reporting Services.
Supports the various SQL Server services.
Shares responsibility for performing and maintaining backups, restorations, disaster recovery, and security.
Required Qualifications
Minimum of a Bachelor’s Degree in a related field.
At least 4 years’ experience working in the field of Data Analytics or Data Warehousing.
Experience with Microsoft Azure Data Lake, Azure Synapse Analytics, and SQL Data Warehouse.
Strong experience with data visualization best practices.
Experience with programming languages such as C#, Python or R.
Deep understanding of data warehousing concepts.
Demonstrated ability to be a strong individual contributor and team player, including the ability to constructively interact with all departments and employee levels.
Excellent customer service skills.
Excellent written and verbal communication skills.
Proven ability to effectively prioritize and work on multiple projects.
Preferred Qualifications
Experience with Business Intelligence.
A fundamental understanding of object-oriented programming.
Knowledge of the maritime industry.
Physical demands of this position include being able to:
Bend, kneel, reach overhead, and lift up to 55 pounds.
Ability and willingness to travel if required (less than 10%).
Working Conditions
Ability and willingness to work in the Jacksonville, FL office a minimum of 3 days per week and work remotely the remainder of the week.
IN RETURN, WE OFFER:
Competitive compensation
Career development opportunities
Team-oriented work environment
Medical, Dental, and Vision Insurance
Health Care Flexible Spending Account (FSA)
Company paid Life, AD&D, and Long-Term Disability Insurance Plans
Employee Assistance, Health Advocate, and Wellness Programs
Generous 401(k) Plan with employer non-elective, matching, and discretionary contributions
Company paid Financial Advice Program
Tuition Reimbursement Program
Paid Vacation based on years of service
Moran Towing Corporation is an Equal Opportunity Employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, color, national origin, religion, gender, age, disability, veteran status, genetic data, or other legally protected status.
Show more
Show less","Azure, Data Lake, Synapse Analytics, Azure Data Lake Storage Gen2, Azure Data Factory, SQL Server Integration Services, Power Query, Apache Spark, ETL, Azure Synapse SQL Pools, C#, Python, R, PowerShell, JSON, XML, SQL, DAX, Stored procedures, Userdefined functions, Power BI, SQL Server Reporting Services, Business Intelligence, Objectoriented programming, Maritime industry","azure, data lake, synapse analytics, azure data lake storage gen2, azure data factory, sql server integration services, power query, apache spark, etl, azure synapse sql pools, c, python, r, powershell, json, xml, sql, dax, stored procedures, userdefined functions, power bi, sql server reporting services, business intelligence, objectoriented programming, maritime industry","apache spark, azure, azure data factory, azure data lake storage gen2, azure synapse sql pools, business intelligence, c, data lake, dax, etl, json, maritime industry, objectoriented programming, power query, powerbi, powershell, python, r, sql, sql server integration services, sql server reporting services, stored procedures, synapse analytics, userdefined functions, xml"
Data Mining and Analytics Engineer (Junior),ICF,"Pensacola, FL",https://www.linkedin.com/jobs/view/data-mining-and-analytics-engineer-junior-at-icf-3726716396,2023-12-17,Florida,United States,Mid senior,Hybrid,"ICF International seeks a Junior Data Mining and Analytics Engineer to support the research and development of new cyber analytic capabilities that will help the US protect and defend its networks and critical information systems. The successful cleared candidate will act as a Data Mining and Analytics Engineer to support a large federal cyber security analytic program. Your work will contribute to the knowledge of how cyber-attacks work, how vulnerabilities are exploited, and the way hostile cyber actors operate. Utilize your skills to help experiment and prototype future cyber capabilities for implementation at large-scale.
As the Junior Data Mining and Analytics Engineer, your skillset will create useful and actionable insight for the customer through the development of analytic solutions (hardware, analytics, tools, techniques, practices, deployment, standards, performance specifications, etc.) for analytic use cases developed during the performance of this project. You will work closely with the Analytics Research team to identify platform enhancements that support the forward-looking analytics under consideration.
The ideal candidate has extensive knowledge of a wide variety of systems and networks to include high-volume/high-availability systems. You are focused on results, a self-starter, and have demonstrated success for using analytics to drive the understanding, growth, and success of the analysis. This is an opportunity to contribute to an important project from its beginning, work with the latest and emerging technologies, and all while building a great career at ICF!
This role is primarily telework-based with occasional meetings at client locations (Arlington, VA or Pensacola, FL) or ICF facilities within the National Capital Region.
What You Will Be Doing
Perform knowledge elicitation from customer subject matter experts and convert that to build analytic solutions
Design, engineer, and optimize sustainment of large-scale distributed computation platforms and supporting environment (ecosystems) for various stakeholders, business owners, and industry partners
Oversee the transition of services from third-party vendors to the analytic environment and be responsible for ad hoc and formal end-user training
Identify applicable data to perform analytics and create solutions to acquire, transform, and load or correlate data components to and from the analytic environment
Develop custom data modeling procedures to assist with data mining, modeling, and production
Assess the effectiveness and accuracy of new data sources and data gathering techniques
Develop processes and tools to monitor and analyze model performance and data accuracy
Interpret and communicate results to non-technical customers
What You Must Have
Active high-level security clearance required as part of client contract requirements
Bachelor’s degree in Computer Science, Mathematics, Engineering, or related field
US Citizenship required as part of client contract requirements
Practical working experience and advanced knowledge of cyber threats, tools, techniques, and processes.
Experience in data modeling and working with datasets of all sizes using a variety of data mining and data analysis methods/tools
Preferred Skills/Experience
Master’s degree in Computer Science, Mathematics, Engineering, or related field
Interpersonal skills and the ability to communicate effectively with various clients in order to explain and elaborate on technical details
Experience in developing analytic tools, processes, and governance for storing, modeling, capturing, and delivering data to the client’s enterprise
Experience with computational notebook software such as Zeppelin or Jupyter
Experience with the application of visual analytics to computational analytic results
Fluency in one or more programming languages (e.g., Python, JavaScript, R, etc.)
Experience with database querying like SQL
Readiness to collaborate with engineering teams, product teams, and customers to develop prototypes and software products
Scaled Agile Framework (SAFe) experience
Amazon Web Services (AWS) Certified Cloud Practitioner or higher desired
CompTIA Security+ or higher cybersecurity certification preferred
#cybsr1
Working at ICF
ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our EEO & AA policy.
Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: Know Your Rights and Pay Transparency Statement.
Pay Range
- There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
$64,372.00 - $109,432.00
Arlington, VA (VA31)
Show more
Show less","Data Mining, Analytics Engineering, Machine Learning, Cyber Security, HighVolume Systems, HighAvailability Systems, Data Modeling, Data Analysis, Programming Languages (Python JavaScript R), SQL, Scaled Agile Framework (SAFe), Amazon Web Services (AWS), Jupyter Notebook, Zeppelin, Visual Analytics, CompTIA Security+, Cybersecurity","data mining, analytics engineering, machine learning, cyber security, highvolume systems, highavailability systems, data modeling, data analysis, programming languages python javascript r, sql, scaled agile framework safe, amazon web services aws, jupyter notebook, zeppelin, visual analytics, comptia security, cybersecurity","amazon web services aws, analytics engineering, comptia security, cyber security, cybersecurity, data mining, dataanalytics, datamodeling, highavailability systems, highvolume systems, jupyter notebook, machine learning, programming languages python javascript r, scaled agile framework safe, sql, visual analytics, zeppelin"
Business Data Analyst,Indotronix International Corporation,"St Petersburg, FL",https://www.linkedin.com/jobs/view/business-data-analyst-at-indotronix-international-corporation-3763364430,2023-12-17,Florida,United States,Mid senior,Hybrid,"Title: Technical Business Data Analyst
Duration: 12+ Months Contract with possibility of Converting it to Fulltime
Client Location: St Petersburg, FL(Hybrid)
Job Description:
Project Details: looking for a quite technical role to support the team.
It’s a BA with a strong technical profile to support complex financial systems.
Job Summary:
• Database experience
• Report development experience
• Agile work / Team work
• Familiar with agile work tools
• Process / Data modelling
• Scripting experience
• Experience working with large scale financial / accounting system (low / no code) [most important]
Show more
Show less","Database, Report development, Agile, Agile work tools, Process modelling, Data modelling, Scripting, Financial systems, Accounting systems","database, report development, agile, agile work tools, process modelling, data modelling, scripting, financial systems, accounting systems","accounting systems, agile, agile work tools, data modelling, database, financial systems, process modelling, report development, scripting"
Data Engineer IV,Navy Federal Credit Union,"Pensacola, FL",https://www.linkedin.com/jobs/view/data-engineer-iv-at-navy-federal-credit-union-3768692870,2023-12-17,Florida,United States,Mid senior,Hybrid,"Overview
Develop strategies for data acquisition, archive recovery, and database implementation. Responsible for designing, building, integrating data from various resources, and managing big data. Develop data consumption patterns to share data with internal/external channels, while ensuring they are easily accessible, work smoothly, with the goal of optimizing the performance of Navy Federal’s big data ecosystem. Recognized as an expert with a specialized depth and/or breadth of expertise in discipline. Solves highly complex problems; takes a broad perspective to identify solutions. Leads functional teams or projects. Works independently.
Responsibilities
Define and build data integration processes to be used across the organization
Build channel contracts and data consumption patterns for customer facing (On-line/Mobile) channels
Analyze and validate data sharing requirements within and outside data partners
Recognize potential issues and risks during the project implementation and suggest mitigation strategies
Communicate and own the processes related to contracts and data consumption patterns
Expert and key point of contact between the operational data hubs and the channel contracts for On-line/Mobile
Apply engineering principles into the design and enhancement of new and existing data management systems
Coach and mentor project team members in carrying out project implementation activities
Work directly with business leadership to understand data requirements; propose and develop solutions that enable effective decision-making and drives business objectives
Prepare advanced project implementation plans which highlight major milestones and deliverables, leveraging standard methods and work planning tools
Lead the preparation of high-quality project deliverables that are valued by the business and present them in such a manner that they are easily understood by project stakeholders
Ensure the security and integrity of system and product solutions including compliance with Navy Federal, industry engineering and Information Security principles and practices
Present clear, organized and concise information to all audiences through a variety of media to enable effective business decisions
Perform other duties as assigned
Qualifications
Master’s degree in Information Systems, Computer Science, Engineering, or related field, or the equivalent combination of education, training and experience
Advanced skills in systems and application integration in a large, distributed architecture environments
Proficient skill level in .Net/C#, Python, Agile Frameworks (SAFE), Microsoft Databricks, Azure Data Factory
Proficient skills in developing and operationalizing various data distribution patterns like, APIs, event based, pub/sub models
Proficient in Data Architecture, Web Services, REST APIs, Event and Pub/Sub messaging architecture
Proficient in Mobile and Web application technologies
Ability to understand the business problem and determine what aspects of it require optimization; articulate those aspects in a clear and concise manner
Proficient skills in understanding SQL and NoSQL and JSON structure
Ability to understand other projects or functional areas to consolidate analytical and operational needs and processes
Demonstrates change management and/or excellent communication skills
Working knowledge of various data structures and the ability to extract data from various data sources
Understands the concepts and application of data mapping and building requirements
Understands data models, large datasets, business/technical requirements
Skilled in managing the process between updating and maintaining data source systems and implementing data related requirements
Desired Qualifications
Knowledge of Navy Federal Credit Union instructions, standards, and procedures
Hours:
Monday - Friday, 8:00AM - 4:30PM
Location:
820 Follin Lane, Vienna, VA 22180 | 5550 Heritage Oaks Dr. Pensacola, FL 32526 | 141 Security Dr. Winchester, VA 22602
About Us
You have goals, dreams, hobbies, and things you're passionate about—what's important to you is important to us. We're looking for people who not only want to do meaningful, challenging work, keep their skills sharp and move ahead, but who also take time for the things that matter to them—friends, family, and passions. And we're looking for team members who are passionate about our mission—making a difference in military members' and their families' lives. Together, we can make it happen. Don't take our word for it:
Military Times 2022 Best for Vets Employers
WayUp Top 100 Internship Programs
Forbes® 2022 The Best Employers for New Grads
Fortune Best Workplaces for Women
Fortune 100 Best Companies to Work For®
Computerworld® Best Places to Work in IT
Ripplematch Campus Forward Award - Excellence in Early Career Hiring
Fortune Best Place to Work for Financial and Insurance Services
Equal Employment Opportunity: Navy Federal values, celebrates, and enacts diversity in the workplace. Navy Federal takes affirmative action to employ and advance in employment qualified individuals with disabilities, disabled veterans, Armed Forces service medal veterans, recently separated veterans, and other protected veterans. EOE/AA/M/F/Veteran/Disability EOE/AA/M/F/Veteran/Disability
Disclaimers: Navy Federal reserves the right to fill this role at a higher/lower grade level based on business need. An assessment may be required to compete for this position. Job postings are subject to close early or extend out longer than the anticipated closing date at the hiring team’s discretion based on qualified applicant volume. Navy Federal Credit Union assesses market data to establish salary ranges that enable us to remain competitive. You are paid within the salary range, based on your experience, location and market position
Bank Secrecy Act: Remains cognizant of and adheres to Navy Federal policies and procedures, and regulations pertaining to the Bank Secrecy Act.
Show more
Show less","Data acquisition, Data recovery, Database implementation, Big data management, Data consumption patterns, Data distribution patterns, APIs, Pub/sub models, Data architecture, Web services, REST APIs, Event and Pub/Sub messaging architecture, Mobile and Web application technologies, SQL, NoSQL, JSON structure, Data mapping, Data models, Data source systems, Data requirements, Data integration, Agile Frameworks (SAFE), Microsoft Databricks, Azure Data Factory, .Net/C#, Python, Data governance, Data security, Business intelligence, Data analytics, Machine learning, Artificial intelligence, Cloud computing","data acquisition, data recovery, database implementation, big data management, data consumption patterns, data distribution patterns, apis, pubsub models, data architecture, web services, rest apis, event and pubsub messaging architecture, mobile and web application technologies, sql, nosql, json structure, data mapping, data models, data source systems, data requirements, data integration, agile frameworks safe, microsoft databricks, azure data factory, netc, python, data governance, data security, business intelligence, data analytics, machine learning, artificial intelligence, cloud computing","agile frameworks safe, apis, artificial intelligence, azure data factory, big data management, business intelligence, cloud computing, data acquisition, data architecture, data consumption patterns, data distribution patterns, data governance, data integration, data mapping, data models, data recovery, data requirements, data security, data source systems, dataanalytics, database implementation, event and pubsub messaging architecture, json structure, machine learning, microsoft databricks, mobile and web application technologies, netc, nosql, pubsub models, python, rest apis, sql, web services"
Junior Data Analyst,ITA International,"Newport News, VA",https://www.linkedin.com/jobs/view/junior-data-analyst-at-ita-international-3771285197,2023-12-17,Portsmouth,United States,Mid senior,Onsite,"At ITA International, we’re a tech-enabled professional services company. Headquartered in Newport News, Virginia, we leverage subject matter expertise, data analytics and technology to challenge boundaries and transform possibilities.
With a global presence and a passionate team of over 300 ITAers, we’re driven by mission success for our customers, “In The Arena.” Our expertise spans Operations, Training, Engineering, Nanotechnology, Statistics, Machine Learning and Software Engineering – enabling data and tech-enabled solutions that deliver real value.
Join our impactful journey at ITA International. As Theodore Roosevelt said, “The credit belongs to the man who is actually in the arena.” We’re here, standing beside our customers, ready to serve and succeed.
ITA is seeking a Junior Data Analyst to join the team at our HQ office in Newport News, VA.
The Junior Data Analyst is a junior level position who will be a part of the ITA Data Solutions team. This team is responsible for building and maintaining dashboards in PowerBI, automating Microsoft tasks, optimizing SharePoint, and administering Microsoft Teams. The candidate will also work closely with the team in designing technical solutions for HQ business operations needs and ensuring that data and reporting needs are met.
Duties include but are not limited to
Gather and analyze data for reporting and analytics solutions
Build and maintain dashboards in PowerBI or other data visualization tools to support decision-making
Automate data entry, workflows, and reports using scripting and programming languages, Microsoft Power Automate, and Microsoft Power Apps
Administer Microsoft Teams by creating and managing channels and related groups
Work closely with stakeholders to understand their data and reporting needs
Develop process guides or presentations in support of requirements
Troubleshoot data-related issues and provide technical support
Bachelor's degree with coursework in analytics or data science is preferred
Verifiable proficiency in building and maintaining BI dashboards (e.g., PowerBI, Tableau, or Business Objects)
1-2 years of experience with Microsoft Power Platform development to include Microsoft Power Automate and Microsoft Power Apps
Knowledge and/or experience of SharePoint development and Microsoft Teams administration
Knowledge and/or experience with data science software
Excellent oral and written communication skills
Ability to work independently and collaborate in a team environment
Ability to organize multiple work assignments and establish priorities
Ability to be flexible and adapt quickly to changing requirements
U.S. Citizenship
ITA International proudly complies with all federal and state benefit and pay transparency laws. Employees of ITA can expect a robust benefit package, including
Medical, dental and vision plans
Life Insurance
Short Term Disability insurance (where applicable)
Voluntary ancillary benefit options
401k retirement benefits with employer matching contributions
ITA International is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.
In compliance with the ADA Amendments Act (ADAAA), ITA International would like to ensure that your application process goes as smoothly as possible. If you would like to preview the physical requirements for this position, or if you have a disability and would like to request an accommodation in order to apply for a currently open position with ITA, please contact us by phone at 757-246-6781 or email us at HR@ita-intl.com.
Show more
Show less","Data Analytics, Data Science, Business Intelligence (BI), Microsoft PowerBI, Microsoft Power Automate, Microsoft Power Apps, SharePoint, Microsoft Teams, Data Visualization, Tableau, Business Objects, Microsoft SQL Server, Python, R, SAS, Statistics, Machine Learning","data analytics, data science, business intelligence bi, microsoft powerbi, microsoft power automate, microsoft power apps, sharepoint, microsoft teams, data visualization, tableau, business objects, microsoft sql server, python, r, sas, statistics, machine learning","business intelligence bi, business objects, data science, dataanalytics, machine learning, microsoft power apps, microsoft power automate, microsoft powerbi, microsoft sql server, microsoft teams, python, r, sas, sharepoint, statistics, tableau, visualization"
Senior Data Analyst,Amtex Systems Inc.,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-analyst-at-amtex-systems-inc-3716007236,2023-12-17,Greenwich,United States,Associate,Onsite,"Amtex Systems Inc is an information technology and talent solutions company offering talent and BI consulting to the companies in US for over 20 years.
Our solutions are designed to fill resource gaps, by providing the right candidates who deliver value to the organization. Our propensity to nurture and build strong relationships with our clients helps us better understand their business demands and gives us the ability to provide services that are on time and rise above the rest.
Specific Skills Required For Position
Data analysis; familiar with MS Office, especially Excel, Access and PowerPoint.
Ability to write scripts to update files, run reports and simplify manual work.
Bachelor's Degree from an accredited college or university with at least five years of full-time experience in project management, construction management, project design management, business administration or public administration.
Explain the function(s) to be performed by the requested staff member:
Compile data from various resources and perform data analysis;
Run ad hoc reports at request;
Produce and update pre-formatted report;
Ability to write scripts in Excel or Access for reports, data cleanup and advanced query
Build Access relational database and set up queries and reports
Understand the business context of report and be able to prepare presentations for internal and external audience;
Support team with other assignments.
Show more
Show less","Data analysis, Microsoft Office, Excel, Access, Powerpoint, Scripting, Project management, Construction management, Project design management, Business administration, Public administration, Data compilation, Report generation, Presentation preparation, Relational database creation, Query setup, Business context interpretation","data analysis, microsoft office, excel, access, powerpoint, scripting, project management, construction management, project design management, business administration, public administration, data compilation, report generation, presentation preparation, relational database creation, query setup, business context interpretation","access, business administration, business context interpretation, construction management, data compilation, dataanalytics, excel, microsoft office, powerpoint, presentation preparation, project design management, project management, public administration, query setup, relational database creation, report generation, scripting"
Senior Data Analyst,Amtex Systems Inc.,"Queens, NY",https://www.linkedin.com/jobs/view/senior-data-analyst-at-amtex-systems-inc-3783113035,2023-12-17,Greenwich,United States,Associate,Onsite,"Amtex Systems Inc is an information technology and talent solutions company offering talent and BI consulting to the companies in US for over 20 years.
Our solutions are designed to fill resource gaps, by providing the right candidates who deliver value to the organization. Our propensity to nurture and build strong relationships with our clients helps us better understand their business demands and gives us the ability to provide services that are on time and rise above the rest.
Position Overview:
Develop analysis and reporting.
Responsibilities
Compile data from various resources and perform data analysis;
Run ad hoc reports at request;
Produce and update pre-formatted report;
Ability to write scripts in Excel or Access for reports, data cleanup and advanced query
Build Access relational database and set up queries and reports
Understand the business context of report and be able to prepare presentations for internal and external audience;
Support team with other assignments.
Qualifications And Skills (Required)
Data analysis; familiar with MS Office, esp. Excel, Access and PowerPoint; ability to write scripts to update files, run reports and simplify manual work.
Bachelor's Degree.
Show more
Show less","Data analysis, MS Office, Excel, Access, PowerPoint, Relational database, Scripting, Report writing, Presentation skills, Communication skills, Problem solving, Team work","data analysis, ms office, excel, access, powerpoint, relational database, scripting, report writing, presentation skills, communication skills, problem solving, team work","access, communication skills, dataanalytics, excel, ms office, powerpoint, presentation skills, problem solving, relational database, report writing, scripting, team work"
Junior Data Engineer II,"Kiss Products, Inc.","Port Washington, NY",https://www.linkedin.com/jobs/view/junior-data-engineer-ii-at-kiss-products-inc-3636799852,2023-12-17,Greenwich,United States,Mid senior,Onsite,"Job Description Summary
Responsible for the design, development, and maintenance of ETL processes that extract data from SAP systems and load it into BW and other data warehouses. The ideal candidate will have experience with in SAP ERP and BW systems, as well as SAP ABAP and Python programming languages. The ideal candidate will be responsible for developing and maintaining BW system development, designing and developing ETL processes from SAP to the SQL data warehouse, and working closely with cross-functional teams to ensure data accuracy and consistency.
Job Description
knowledge/experience, skills, ability & attitude Required:
Required Skills
Bachelor's or Master's degree in Computer Science, Information Systems, or related field.
3+ years of experience in SAP ERP and BW systems.
Strong experience in SAP ABAP and Python programming languages.
Experience with BW system development, including data models, data sources, and data transformations.
Experience with ETL processes and data warehousing, specifically extracting data from SAP and loading into SQL data warehouse.
Experience in one of the machine learning platform; GCP, AWS, Azure, Databricks and Snowflake.
Strong analytical skills and ability to work with complex data sets.
Excellent communication skills and ability to work collaboratively with cross-functional teams.
Ability to work in a fast-paced environment and manage multiple projects simultaneously. Analyze problems and solve the issue
Able to juggle multiple tasks simultaneously when required
Willing and able to learn new concepts, processes with a positive attitude
We offer a competitive benefits package!
(*Eligibility may vary.)
401(k) Savings Plan
Premium Medical Insurance Coverage
Year-end Bonus Plan
Paid Time Off (PTO) based on seniority
Paid Holidays
Onsite Employee Fitness Center with Indoor Racquetball Court and Yoga Room
Summer Friday
Complimentary Gourmet Breakfast, Lunch, and Dinner
Relocation Support for New Hires*
Work Anniversary Recognitions
Congratulatory & Condolence Gifts
Employee Referral Bonus Program
License/Certification Reimbursements*
Corporate Employee Discounts
Visa Sponsorships (100% paid by the company) i.e., New H-1B, H-1B Transfer, O-1, and Green Card
Commuter Support (Shuttle Bus Program)*
Vehicle Perks*
The anticipated compensation range is
22.75 - 44.50 USD Hourly
Actual compensation will be determined based on various factors including qualifications, education, experience, and location. The pay range is subject to change at any time dependent on a variety of internal and external factors.
Kiss Nail Products, Inc., or Kiss Distribution Corporation or Ivy Enterprises, Inc., or AST Systems, LLC, or Red Beauty, Inc., Dae Do, Inc. (collectively, the “Company”) is an equal opportunity employer and is committed to a diverse and inclusive workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, or any other characteristic protected by law.
Show more
Show less","SAP ERP, SAP BW, ABAP, Python, ETL, Data warehousing, Machine learning, GCP, AWS, Azure, Databricks, Snowflake, Data models, Data sources, Data transformations, SQL","sap erp, sap bw, abap, python, etl, data warehousing, machine learning, gcp, aws, azure, databricks, snowflake, data models, data sources, data transformations, sql","abap, aws, azure, data models, data sources, data transformations, databricks, datawarehouse, etl, gcp, machine learning, python, sap bw, sap erp, snowflake, sql"
Big Data Developer,ASK Consulting,"Ridgefield Park, NJ",https://www.linkedin.com/jobs/view/big-data-developer-at-ask-consulting-3776138343,2023-12-17,Greenwich,United States,Mid senior,Onsite,"""All candidates must be directly contracted by ASK Consulting on their payroll and cannot be subcontracted. We are unable to provide sponsorship at this moment"".
Job Title: Big Data Developer
Location: Ridgefield Park, NJ
Duration: Contract Position
Pay Range : 65-71 per hour
Job Description:
Job Opportunity: Big Data Engineer (Contract)
Are you ready to take on a pivotal role in shaping the data landscape of a cutting-edge analytics organization? We are seeking a skilled and passionate Big Data Engineer to join our dynamic team. As a Big Data Engineer, you will play a crucial role in ensuring the reliability and applicability of our data products across our organization. This is a contract position where you will have the opportunity to showcase your expertise in SQL (Hive), Python, and Spark.
Responsibilities:
Translate intricate functional and technical requirements into comprehensive designs.
Engineer highly scalable end-to-end pipelines using open source tools and innovative techniques.
Design, develop, and implement Hadoop solutions, leveraging the power of Kafka for data loading from diverse sources.
Utilize Hive, Impala, Spark, and Pig to preprocess data efficiently.
Create and execute effective data modeling strategies to drive actionable insights.
Maintain top-notch security and data privacy standards in a secured environment.
Harness in-memory technologies like Spark for high-speed querying.
Contribute to best practices in source control, release management, and deployment.
Provide production support, monitor job scheduling, and ensure ETL data quality and freshness reporting.
Qualifications:
5-8 years of hands-on experience crafting complex SQL queries.
4+ years of Python development expertise, demonstrating your programming prowess.
5+ years of proven technical proficiency in Hadoop and big data projects.
3+ years of success in data modeling, transforming concepts into structured solutions.
Ability to streamline processes through automated ETL implementations.
Proficiency in writing shell scripts (bash, korn) for efficient workflows.
Knowledge and aptitude in implementing Oozie workflows and schedulers.
Familiarity with AWS components to harness cloud capabilities.
Strong analytical and problem-solving skills, tailored for the Big Data domain.
Proven understanding and hands-on experience with Hadoop, Hive, Presto, and Spark.
Adept at multi-threading and concurrency concepts.
B.S. or M.S. in Computer Science or Engineering.
Why Join Us:
This is your chance to be at the forefront of innovation, shaping the data-driven future of our organization. As a contracted Big Data Engineer, you will have the unique opportunity to work closely with cutting-edge technologies and contribute to impactful projects from day one.
About ASK:
ASK Consulting is an award-winning technology and professional services recruiting firm servicing Fortune 500 organizations nationally. With 5 nationwide offices, two global delivery centers, and employees in 42 states-ASK Consulting connects people with amazing opportunities
ASK Consulting is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all associates.
Show more
Show less","SQL, Python, Spark, Hadoop, Kafka, Hive, Impala, Pig, Shell scripting, Oozie, AWS, ETL, Data modeling, Multithreading, Concurrency, Presto","sql, python, spark, hadoop, kafka, hive, impala, pig, shell scripting, oozie, aws, etl, data modeling, multithreading, concurrency, presto","aws, concurrency, datamodeling, etl, hadoop, hive, impala, kafka, multithreading, oozie, pig, presto, python, shell scripting, spark, sql"
Senior Data Engineer,What If Media Group,"Fort Lee, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-what-if-media-group-3787096012,2023-12-17,Greenwich,United States,Mid senior,Onsite,"Job Description
Senior Data Engineer
The Company:
Founded in 2012, What If Media Group is an award-winning performance marketing company that enables the world’s leading brands to acquire valuable new customers at scale. By leveraging data-driven engagement and re-engagement strategies across multiple marketing channels and utilizing insights based on billions of consumer ad interactions daily, WIMG delivers cost-effective and profitable performance marketing solutions that extend lifetime value and maximize ROI. Headquartered in Fort Lee, New Jersey, WIMG is a recipient of Crain’s Fast 50 and is a member of the Inc. 5000 list of the fastest-growing private companies in America.
The Role:
We are looking for a Senior Data Engineer to join What If Media Group’s Data Engineering team. Our Data Engineering team is responsible for building and managing data pipelines that power our analytics and machine learning across What If Media Group, including Engineering, finance, growth, product analysis, Ops, sales, and marketing. We maintain What If Media Group’s Enterprise data warehouses and data lake, build creative, reliable, and scalable data solutions and models to provide unified insights and enable cross-channel decision-making models and drive growth and innovations. With opportunities to work with technologies used by our various Engineering and Data Science teams. This position can be remote or on-site at our Fort Lee headquarters. You'll be joining a team that is very smart and very direct. We ask hard questions and challenge each other to improve our work continually. We are self-driven yet collaborative. We're all about enabling growth by delivering the right data and insights in the right way to teams across the organization.
Your Responsibilities:
Work with our Data Science teams, Product Engineering, Messaging Engineering, Delivery Engineering, Voice Engineering, and DevOps teams to create accurate and efficient data pipelines between the key components of our infrastructure
Design and implement processes that combine data from all of our marketing channels that enable cross-channel decision-making engines and models
Work with stakeholders, including the Executive, Product, Engineering, and Data Science, Operation teams to assist with data-related technical issues and support their data needs
Implement or contribute towards data/process optimization as we focus on providing reliable and timely data with optimal performance and cost
Ensure our data processes adhere to security and compliance protocols
Propose new additions to our Data Engineering and Data Science technology stacks
Identify, design, and implement internal process improvements, including automating manual processes and optimizing data delivery
You Today:Education & Experience
Bachelor’s degree or higher, preferably in Computer Science or a related STEM field discipline. Relevant practical experience will supersede a specific academic qualification.
Broad experience designing, programming, and implementing large, data-intensive, systems
7+ years of prior professional experience
SkillsRequired
Extensive knowledge of data warehousing technologies such as Snowflake, Databricks and its APIs
Proficient in programming languages such as Python/Scala and solid SQL are required
Expertise with tools and data integration technologies, with a focus on open-source solutions such as Apache Airflow, Apache NiFi, or Talend
Experience building scalable data pipelines in Spark using Airflow scheduler/executor framework or similar scheduling tools
Working experience designing and implementing of efficient data systems and architecture, with a focus on scalability, performance, and cost
Strong understanding of data modeling and schema design, including dimensional modeling techniques, implementation, and maintenance of data models that can support the organization's data storage and analysis needs
Experience with DevOps practices and tools such as Docker and Kubernetes
Must have extensive knowledge of cloud computing platforms such as AWS (preferred), Azure, or GCP
Solid experience and understanding of database management systems (DBMS) such as MySQL, Postgresql, or MongoDB
Familiar with big data technologies like Hadoop, Spark, and Kafka
Experience with Agile methodologies and project management tools such as Jira
Strong problem-solving and analytical skills, with the ability to troubleshoot complex data issues
Excellent communication skills with ability to collaboratively with cross-functional teams, communicate effectively with technical and non-technical stakeholders, and document their work
Ability to diagram processes and connected systems and present ideas to the teams and or stakeholders
Experience with streaming technologies like Kafka and/or Kinesis
Strong skill in leading the design and development of more complex feature enhancements involving yourself and one or more additional engineers
Excellent Problem-Solving and Analytical Skills with a track record of the ability to identify problems, troubleshoot issues, and analyze data to derive insights and make informed decisions
Tools We Use
Languages and frameworks: Python, Node.js, Java, and Go
AWS services: EC2, ECS, EKS, RDS, S3, DMS, Lambda, Athena, and MSK
Databases: PostgreSQL, MariaDB, DynamoDB, MongoDB, Snowflake, Databricks, Neo4j
Business intelligence tools: Looker
Other tools: Airflow, Kafka, DMS
What If Media Group is an Equal Opportunity Employer. We provide employees with a competitive salary, bonus plan, and generous medical, dental, vision, & 401k plans. We look forward to hearing from you!
Show more
Show less","Python, Scala, SQL, Apache Airflow, Apache NiFi, Talend, Spark, Docker, Kubernetes, AWS, Azure, GCP, MySQL, Postgresql, MongoDB, Hadoop, Kafka, Jira, Agile methodologies","python, scala, sql, apache airflow, apache nifi, talend, spark, docker, kubernetes, aws, azure, gcp, mysql, postgresql, mongodb, hadoop, kafka, jira, agile methodologies","agile methodologies, apache airflow, apache nifi, aws, azure, docker, gcp, hadoop, jira, kafka, kubernetes, mongodb, mysql, postgresql, python, scala, spark, sql, talend"
Healthcare Data Analyst,Essen Health Care,"Bronx, NY",https://www.linkedin.com/jobs/view/healthcare-data-analyst-at-essen-health-care-3788729523,2023-12-17,Greenwich,United States,Mid senior,Onsite,"At Essen Health Care, we care for that!
As the largest privately held multispecialty medical group in the Bronx, we provide high-quality, compassionate, and accessible medical care to some of the most vulnerable and under-served residents of New York State. Guided by a Population Health model of care, Essen has five integrated clinical divisions offering urgent care, primary care, and specialty services, as well as nursing home staffing and care management. Founded in 1999, our over 20-year commitment has fueled an unwavering dedication toward innovating a better healthcare delivery system. Essen has expanded from a single primary care office to an umbrella organization offering specialties from women’s health to endocrinology, from psychiatry to a vast array of other specialties. All clinical services are offered via telehealth or in-person at over 35 medical offices and at home through the Essen House Calls program.
Essen Health Care is the place Where Care Comes Together! We are looking for the most talented and effective individuals to join our rapidly growing company. With over 1,100 employees and 400+ Practitioners, we care for over 250,000 patients annually in New York City and beyond. From medical providers to administration & operational staff, there is a career here for you. Join our team today!
The Data Analyst will be responsible for gathering and analyzing healthcare data from multiple sources (e.g. insurance processing, clinical operations, and patient behavior) to extract trends and business insights. Then evaluates encounter data and identifies errors created by operational processes, claims and eligibility processes. Must research the analysis and categorize the encounter. The primary responsibility for this position is to fully support the ongoing healthcare business intelligence and data analytics objectives. To succeed in this role, you should be analytical, resourceful and works under moderate supervision.
Provides consulting and analytic services to leadership, operations and finance.
Monitors encounter data from various databases and creates monthly, quarterly and ad-hoc reports
Deliver reports in pre-defined formats based on submitted and agreed upon requirements
Analyze, present and explain information in accessible and actionable way (i.e. budgeting reports, cost and claims or clinical data)
Summarizes large volumes of encounter data in analytical reports that include tables and graphs for management review and use
Reviews and ensures accuracy of all ad-hoc and automated reports and corrects discrepancies
Programs using Excel, Power BI, Python and/or SQL or other appropriate software to generate reports based on input/direction from management and other report users
Collaborates with other departments and vendors to identify missing data, errors or other anomalies; works with them to correct these encounter data errors, gather necessary information and provide guidance on regulatory reporting needs
Building models and analyzing data to discover discrepancies and patterns
Pull and integrate data from disparate sources (i.e. budgeting reports, cost and claims or clinical data)
Collaborate with management and internal teams to implement and evaluate improvements
Expert knowledge of MS Excel
Strong knowledge of SQL – Formulate analytic questions as database queries
Strong hands-on familiarity with eClinical Works (ECW)
Technical Skills Requirements
3+ Year Hands on experience in Data Warehousing skills
SQL/noSQL/Postgres, Amazon Redshift, Hadoop/Spark, Azure, Data Pipeline and ETL Development (Python, Scala, AWS Glue, Data Factory)
API development or process real-time data thru API access
3+ Year Hands on experience in Data Science Skills
Python, Analytical SQL Functions, R Language Data Visualization Skills
Microsoft Power BI, Tableau, Excel Microsoft Development experience
Power APPS, SharePoint Lists, .Net/Web Development
Additional skills Desired
Hive, Apache Kafka, Streams,
General Requirements
Bachelor's degree in Computer Science/engineering, health services research, actuarial science, statistics, or economics
Experience performing data analysis from ECW, Claims, Rosters, etc. in a health care or health plan organization, required.
Formal informatics training a plus.
Strong analytic and reporting skills required.
Effective oral, written and interpersonal communication skills required.
Essen Health care is proud to be an equal opportunity employer, and we seek candidates who desire to work in and serve an ethnically diverse population.
Show more
Show less","Healthcare Data Analytics, Business Intelligence, Data Warehousing, SQL, noSQL, Postgres, Amazon Redshift, Hadoop/Spark, Azure, Data Pipeline, ETL Development, Python, Scala, AWS Glue, Data Factory, API Development, Data Science, Analytical SQL Functions, R Language, Data Visualization, Microsoft Power BI, Tableau, Excel, Microsoft Development, Power APPS, SharePoint Lists, .Net/Web Development, Hive, Apache Kafka, Streams, ECW, Claims, Rosters","healthcare data analytics, business intelligence, data warehousing, sql, nosql, postgres, amazon redshift, hadoopspark, azure, data pipeline, etl development, python, scala, aws glue, data factory, api development, data science, analytical sql functions, r language, data visualization, microsoft power bi, tableau, excel, microsoft development, power apps, sharepoint lists, netweb development, hive, apache kafka, streams, ecw, claims, rosters","amazon redshift, analytical sql functions, apache kafka, api development, aws glue, azure, business intelligence, claims, data factory, data pipeline, data science, datawarehouse, ecw, etl development, excel, hadoopspark, healthcare data analytics, hive, microsoft development, microsoft power bi, netweb development, nosql, postgres, power apps, python, r language, rosters, scala, sharepoint lists, sql, streams, tableau, visualization"
CLINICAL DATA ANALYST,Montefiore Health System,"Bronx, NY",https://www.linkedin.com/jobs/view/clinical-data-analyst-at-montefiore-health-system-3750690804,2023-12-17,Greenwich,United States,Mid senior,Onsite,"The Clinical Data Analyst will assist Bronx Health Collective with performing analytical and clinical report builds. The analyst will report to the Program Supervisor in the Research Department. The Clinical Data Analyst is responsible for helping to produce and present standardized reports or analyses and developing data frameworks for monitoring and evaluating operational effectiveness/efficiency, supporting decision-making and strategic planning, as well as quality improvement efforts. This position will work closely with physicians and administrative leaders to understand requirements and be responsible for delivering accurate data products in a timely manner.
Creates clinical and project-related reports including analysis, design, documentation, configuration, testing, implementation, and ongoing support for EPIC Clarity reports. Uses tools such as Reporting Workbench, SQL, Crystal Reports, and Business Objects Enterprise.
Ability to translate user requirements into functional & design specifications, communicate with report consumers to gather specifications, review designs, present drafts, and validate results.
Education / Experience
Bachelor’s Degree in Informatics, Statistics, Computer Science, Public Health or related required
Master’s Degree preferred.
3 years’ work experience
Skills Required
Experience in Epic, data reporting, data mining, and/or analytics in health care or related field
Experience developing intuitive data visualizations and facilitating visual data analysis.
Experience with, at least, one data visualization software tool (e.g., Power BI, Web Intelligence, or Tableau)
Strong troubleshooting skills: logical and creative problem-solving skills.
Excellent interpersonal, writing, presentation, and time management.
Department:
Pediatrics
Bargaining Unit:
Non Union
Campus:
MOSES
Employment Status:
Regular Full-Time
Address:
853 Longwood Avenue, Bronx
Shift:
Day
Scheduled Hours:
9 AM-5:30 PM
Req ID:
216190
Salary Range/Pay Rate:
$67,500.00 - $90,000.00
For positions that have only a rate listed, the displayed rate is the hiring rate but could be subject to change based on shift differential, experience, education or other relevant factors.
To learn more about the “Montefiore Difference” – who we are at Montefiore and all that we have to offer our associates, please click
here .
Diversity, equity and inclusion are core values of Montefiore. We are committed to recruiting and creating an environment in which associates feel empowered to thrive and be their authentic selves through our inclusive culture. We welcome your interest and invite you to join us.
Montefiore is an equal employment opportunity employer. Montefiore will recruit, hire, train, transfer, promote, layoff and discharge associates in all job classifications without regard to their race, color, religion, creed, national origin, alienage or citizenship status, age, gender, actual or presumed disability, history of disability, sexual orientation, gender identity, gender expression, genetic predisposition or carrier status, pregnancy, military status, marital status, or partnership status, or any other characteristic protected by law.
N/A
Show more
Show less","Data Analytics, Data Reporting, Data Mining, Clinical Data, Data Visualization, Tableau, Power BI, Web Intelligence, SQL, Crystal Reports, Business Objects Enterprise, Data Frameworks, Epic, Reporting Workbench","data analytics, data reporting, data mining, clinical data, data visualization, tableau, power bi, web intelligence, sql, crystal reports, business objects enterprise, data frameworks, epic, reporting workbench","business objects enterprise, clinical data, crystal reports, data frameworks, data mining, data reporting, dataanalytics, epic, powerbi, reporting workbench, sql, tableau, visualization, web intelligence"
Senior Data Visualization Developer,Altice USA,"Bethpage, NY",https://www.linkedin.com/jobs/view/senior-data-visualization-developer-at-altice-usa-3779117659,2023-12-17,Greenwich,United States,Mid senior,Onsite,"Altice USA is a cutting-edge communications, media, and tech company. We connect people to what matters most to them; texting with friends, advertising that resonates, or binge watching their favorite show. Our differentiated approach centers around technologies that push the envelope and deliver the ultimate customer experience. We’re building a workforce that attracts and retains the best talent, not only to meet the needs of our customers, but that also reflects the diverse communities we serve.
Job Summary
As a Data Visualization developer at Altice USA, you will be responsible for the expansion of rich interactive graphics, data visualizations and charting, designing, developing and supporting interactive data visualizations used across the enterprise. This position emphasizes a need for an artistic mind to conceptualize, design, and develop reusable graphic/data visualizations as well as very strong technical knowledge for implementing these visualizations using the very latest technologies such as Tableau and Looker.
Responsibilities
Design and develop data visualizations, dashboards, and reports using Tableau Desktop to provide insights into business performance and trends.
Create compelling data narratives by combining visualizations and insights to tell a story that aids in decision-making.
Collaborate with cross-functional teams, including data engineers, business analysts, and stakeholders, to gather requirements and understand data needs.
Enforce data governance standards and best practices within Tableau, including data naming conventions, calculated fields, and metadata management.
Identify and resolve performance bottlenecks in Tableau Server, such as slow queries and dashboard load times, to ensure optimal performance.
Optimize Tableau workbooks and dashboards for efficient data loading and rendering, ensuring a smooth user experience, especially for large datasets.
Qualifications
3+ years of relevant employment experience
Strong knowledge of SQL, Google Big Query and Tableau Desktop and Tableau Server.
Experience working in cross-functional teams and driving projects to successful completion.
Hands-on approach to learning the Altice USA business to quickly and thoughtfully augment the strategic value of a unified analytics function.
Excellent communication and interpersonal skills to collaborate effectively with stakeholders at all levels.
Proficiency in creating calculated fields, custom formulas, and scripting within Tableau.
Experienced at optimizing Tableau workbooks and dashboards for enhanced performance, especially for handling large datasets.
Hands on knowledge of creating and scheduling automated data refreshes and report distribution to minimize manual intervention and enhance efficiency.
Altice USA is an Equal Opportunity Employer committed to recruiting, hiring and promoting qualified people of all backgrounds regardless of gender, race, color, creed, national origin, religion, age, marital status, pregnancy, physical or mental disability, sexual orientation, gender identity, military or veteran status, or any other basis protected by federal, state, or local law.
Altice USA, Inc. collects personal information about its applicants for employment that may include personal identifiers, professional or employment related information, photos, education information and/or protected classifications under federal and state law. This information is collected for employment purposes, including identification, work authorization, FCRA-compliant background screening, human resource administration and compliance with federal, state and local law.
This position is identified as being performed in/or reporting to company operations in New York State. Salary ranges are supplied in compliance with New York State law. Pay is competitive and based on a number of job-related factors, including skills and experience. The starting pay rate/range at time of hire for this position in the posted location is $90,899.00 - $145,439.00 / year. The rate/Range provided herein is the anticipated pay at the time of hire, and does not reflect future job opportunity.
Applicants for employment with Altice will never be asked to provide money (even if reimbursable) as part of the job application or hiring process. Please review our Fraud FAQ for further details.
Show more
Show less","SQL, Google Big Query, Tableau Desktop, Tableau Server, Tableau, Looker, Javascript, CSS, HTML","sql, google big query, tableau desktop, tableau server, tableau, looker, javascript, css, html","css, google big query, html, javascript, looker, sql, tableau, tableau desktop, tableau server"
Data Warehouse Analyst II,Hyatt Hotels Corporation,"Jamaica, NY",https://www.linkedin.com/jobs/view/data-warehouse-analyst-ii-at-hyatt-hotels-corporation-3719152351,2023-12-17,Greenwich,United States,Mid senior,Onsite,"Description
Job Description
The Data Warehouse Analyst II (“DW Analyst”) plays a critical role in developing and maintaining a data warehouse for Genting Americas to deliver strategic business decisions across all levels of the organization.
DW Analyst will report to Director Planning and Analysis, and oversee the evolution of the current data warehouse, which is a comprehensive data repository for all relevant KPIs to include financial and statistical data. The individual must be proficient in database engineering and have expertise in
interacting with a data warehouse using SQL language. DW Analyst will bring the necessary skills to the team with respect to structuring the warehouse using best data science practices that result in an efficient and scalable structure. In working with an application specialist, he/she will be responsible for the design and validation of all data warehouse feeds governed by automated or manual procedures. The ideal
candidate will create a work environment that supports our core values, and promotes integrity, teamwork, performance, recognition, mutual respect, and employee satisfaction.
Essential Duties
Develops processes for the extraction, transformation, and loading of data from source systems to the data warehouse.
Ensures that the data warehouse properly supports the functionality of Anaplan - our BI
Builds, manages and maintains Power-Automate
Carries sufficient knowledge of the Anaplan import process and can evolve the data ingestion with the evolution of Anaplan.
Ensures a high level of accuracy by validating the newly ingested data in
Takes the lead on data management, data quality, and
Is responsible for data mapping and data
Assist P&A in setting up departmental and property reports, analyses, and
Provide technical input when necessary in the implementation of new BI software to ensure maximum compatibility of any new software with the existing apparatus
Conduct audits and follow-up on completed projects to verify sustained impact
Adapt quickly and effectively to procedural changes and assist with other special projects
Perform other duties as Job Requirements
Physical and Mental
Ability to sit for extended periods of time as necessary to complete job assignments and job duties.
Ability to work on a PC. The individual may occasionally be asked to work a flexible schedule to include nights and weekends. Ability to lift 10 lbs. Required to understand and communicate and follow directions both verbally and in written English.
Work/Educational Experience
Must be at least 18 years old, possess a high school or equivalent diploma and have the ability to obtain the appropriate license pursuant to the New York State Lottery regulations
Bachelor’s Degree in Information Systems (IS), or related field
Master of Science or equivalent degree preferred with financial and quantitative orientation preferred
Two (2+) years’ work experience with large scale relational databases, Data Warehouses or similar
Certified Model Builder Level 1 in Anaplan highly preferred
Intermediate to advanced knowledge in setting up a SQL based relational databases that reflect best practices with respect to scalability, compatibility, and comprehensiveness
Experience with gaming and hospitality industry systems (IGT/Konami, Agilysis, OnQ/ Opera, RedRock/Stratton Warren, SAP, Ultimate/Kronos etc.)
Working knowledge with Microsoft SQL Server Suite (SSMS/SSIS/SSAS/SSRS) and business intelligence reporting software (Tableau/Power BI/IBM Cognos Analytics etc.)
BPI Certification (Lean and/or Six Sigma), or experience with Kaizen
Previous experience with project management and continuous improvement
Intermediate to advanced knowledge of Power Automate or similar automation platforms
Working knowledge of Microsoft Office
Ability to work varied shifts, including nights, weekends and holidays
Ability to effectively communicate in English
Polished appearance and demeanor
Excellent customer service skills
Working knowledge of gaming regulations
Previous experience working in a large, luxury resort setting preferred
Salary: $84,500 - $85,000
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)
This position is at a location where Hyatt is not the employer. The employer of individuals working at this hotel may be a third-party management company that is responsible for all employment benefits and obligations at this location.
Show more
Show less","Data Warehouse, SQL, Data Science, Data Mapping, Data Quality, ETL, Power Automate, Anaplan, Tableau, Power BI, IBM Cognos Analytics, Microsoft SQL Server Suite (SSMS/SSIS/SSAS/SSRS), Business Intelligence, Kaizen, Lean, Six Sigma, Microsoft Office, Gaming Regulations","data warehouse, sql, data science, data mapping, data quality, etl, power automate, anaplan, tableau, power bi, ibm cognos analytics, microsoft sql server suite ssmsssisssasssrs, business intelligence, kaizen, lean, six sigma, microsoft office, gaming regulations","anaplan, business intelligence, data mapping, data quality, data science, datawarehouse, etl, gaming regulations, ibm cognos analytics, kaizen, lean, microsoft office, microsoft sql server suite ssmsssisssasssrs, power automate, powerbi, six sigma, sql, tableau"
Data Warehouse Analyst II,Hyatt Hotels Corporation,"Jamaica, NY",https://www.linkedin.com/jobs/view/data-warehouse-analyst-ii-at-hyatt-hotels-corporation-3719149851,2023-12-17,Greenwich,United States,Mid senior,Onsite,"Description
Job Description
The Data Warehouse Analyst II (“DW Analyst”) plays a critical role in developing and maintaining a data warehouse for Genting Americas to deliver strategic business decisions across all levels of the organization.
DW Analyst will report to Director Planning and Analysis, and oversee the evolution of the current data warehouse, which is a comprehensive data repository for all relevant KPIs to include financial and statistical data. The individual must be proficient in database engineering and have expertise in
interacting with a data warehouse using SQL language. DW Analyst will bring the necessary skills to the team with respect to structuring the warehouse using best data science practices that result in an efficient and scalable structure. In working with an application specialist, he/she will be responsible for the design and validation of all data warehouse feeds governed by automated or manual procedures. The ideal
candidate will create a work environment that supports our core values, and promotes integrity, teamwork, performance, recognition, mutual respect, and employee satisfaction.
Essential Duties
Develops processes for the extraction, transformation, and loading of data from source systems to the data warehouse.
Ensures that the data warehouse properly supports the functionality of Anaplan - our BI
Builds, manages and maintains Power-Automate
Carries sufficient knowledge of the Anaplan import process and can evolve the data ingestion with the evolution of Anaplan.
Ensures a high level of accuracy by validating the newly ingested data in
Takes the lead on data management, data quality, and
Is responsible for data mapping and data
Assist P&A in setting up departmental and property reports, analyses, and
Provide technical input when necessary in the implementation of new BI software to ensure maximum compatibility of any new software with the existing apparatus
Conduct audits and follow-up on completed projects to verify sustained impact
Adapt quickly and effectively to procedural changes and assist with other special projects
Perform other duties as Job Requirements
Physical and Mental
Ability to sit for extended periods of time as necessary to complete job assignments and job duties.
Ability to work on a PC. The individual may occasionally be asked to work a flexible schedule to include nights and weekends. Ability to lift 10 lbs. Required to understand and communicate and follow directions both verbally and in written English.
Work/Educational Experience
Must be at least 18 years old, possess a high school or equivalent diploma and have the ability to obtain the appropriate license pursuant to the New York State Lottery regulations
Bachelor’s Degree in Information Systems (IS), or related field
Master of Science or equivalent degree preferred with financial and quantitative orientation preferred
Two (2+) years’ work experience with large scale relational databases, Data Warehouses or similar
Certified Model Builder Level 1 in Anaplan highly preferred
Intermediate to advanced knowledge in setting up a SQL based relational databases that reflect best practices with respect to scalability, compatibility, and comprehensiveness
Experience with gaming and hospitality industry systems (IGT/Konami, Agilysis, OnQ/ Opera, RedRock/Stratton Warren, SAP, Ultimate/Kronos etc.)
Working knowledge with Microsoft SQL Server Suite (SSMS/SSIS/SSAS/SSRS) and business intelligence reporting software (Tableau/Power BI/IBM Cognos Analytics etc.)
BPI Certification (Lean and/or Six Sigma), or experience with Kaizen
Previous experience with project management and continuous improvement
Intermediate to advanced knowledge of Power Automate or similar automation platforms
Working knowledge of Microsoft Office
Ability to work varied shifts, including nights, weekends and holidays
Ability to effectively communicate in English
Polished appearance and demeanor
Excellent customer service skills
Working knowledge of gaming regulations
Previous experience working in a large, luxury resort setting preferred
Salary: $84,500 - $85,000
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)
This position is at a location where Hyatt is not the employer. The employer of individuals working at this hotel may be a third-party management company that is responsible for all employment benefits and obligations at this location.
Show more
Show less","SQL, PowerAutomate, Anaplan, SSMS/SSIS/SSAS/SSRS, Tableau/Power BI, IBM Cognos Analytics, Lean, Six Sigma, Kaizen, Power Automate, Microsoft Office, Data Warehouse, Data Science, Data Mapping, Data Management, Data Quality","sql, powerautomate, anaplan, ssmsssisssasssrs, tableaupower bi, ibm cognos analytics, lean, six sigma, kaizen, power automate, microsoft office, data warehouse, data science, data mapping, data management, data quality","anaplan, data management, data mapping, data quality, data science, datawarehouse, ibm cognos analytics, kaizen, lean, microsoft office, power automate, powerautomate, six sigma, sql, ssmsssisssasssrs, tableaupower bi"
Senior Data Systems Engineer - BI,VISTRADA,"Queens County, NY",https://www.linkedin.com/jobs/view/senior-data-systems-engineer-bi-at-vistrada-3787721641,2023-12-17,Greenwich,United States,Mid senior,Remote,"Vistrada is seeking highly motivated candidates to serve as senior data systems engineers to work as part of our Business Intelligence team supporting multiple clients. Vistrada’s clients look to us to provide technical leadership and assist them in solving many of today’s most challenging digital transformation issues and complex data needs. In this role, you will help address operational challenges associated with data modeling, system architectures, and reporting requirements by applying advanced capabilities, modern technology, and best practices to real-world scenarios.
Key Responsibilities:
Work closely with clients to develop technology innovation plans and enhancements on a broad range of areas, including:
Model-based system engineering
Relational data architectures
Data modeling
Data integration processes and ETL development
Report / Dashboard creation using leading visualization technology platforms (i.e., Tableau; Qlik; Power BI)
Data analytics platform design, development, and testing
Machine Learning and/or Statistical Analysis.
Required Qualifications:
Bachelor’s Degree and 8+ years of related experience
5+ years of experience with SQL databases, such as MSSQL, Oracle, MySQL, and/or PostgreSQL
5+ years of experience with leading visualization technology platforms (i.e., Tableau; Qlik; Power BI)
Ability to work independently and eager to learn new technologies, techniques, processes, software languages, platforms, and systems
Expertise to provide unbiased advice, formulate courses of action, analyze programs, and make recommendations across a wide spectrum of issues
Passionate, goal driven, team-oriented, and outgoing
Flexible, self-starter, and demonstrated ability to operate effectively with ambiguous and evolving objectives in a client-facing environment
Effective communication skills.
Preferred Qualifications
2+ years of experience with cloud architecture systems, such as Azure, Google Cloud, or AWS
Competency with source code management systems
Competency with Office365
Strong written communication skills.
Powered by JazzHR
jp7lMuTS1t
Show more
Show less","Modelbased system engineering, Relational data architectures, Data modeling, Data integration processes, ETL development, Report / Dashboard creation, Tableau, Qlik, Power BI, Data analytics platform design, Data analytics platform development, Data analytics platform testing, Machine Learning, Statistical Analysis, SQL databases, MSSQL, Oracle, MySQL, PostgreSQL, Cloud architecture systems, Azure, Google Cloud, AWS, Source code management systems, Office365","modelbased system engineering, relational data architectures, data modeling, data integration processes, etl development, report dashboard creation, tableau, qlik, power bi, data analytics platform design, data analytics platform development, data analytics platform testing, machine learning, statistical analysis, sql databases, mssql, oracle, mysql, postgresql, cloud architecture systems, azure, google cloud, aws, source code management systems, office365","aws, azure, cloud architecture systems, data analytics platform design, data analytics platform development, data analytics platform testing, data integration processes, datamodeling, etl development, google cloud, machine learning, modelbased system engineering, mssql, mysql, office365, oracle, postgresql, powerbi, qlik, relational data architectures, report dashboard creation, source code management systems, sql databases, statistical analysis, tableau"
Lead Data Statistician,Jackson Lewis P.C.,"Melville, NY",https://www.linkedin.com/jobs/view/lead-data-statistician-at-jackson-lewis-p-c-3755008542,2023-12-17,Greenwich,United States,Mid senior,Remote,"Focused on labor and employment law since 1958, Jackson Lewis P.C.’s 950+ attorneys located in major cities nationwide consistently identify and respond to new ways workplace law intersects business. We help employers develop proactive strategies, strong policies and business-oriented solutions to cultivate high-functioning workforces that are engaged, stable and diverse, and share our clients' goals to emphasize inclusivity and respect for the contribution of every employee.
The Firm is ranked in the First Tier nationally in the category of Labor and Employment Litigation, as well as in both Employment Law and Labor Law on behalf of Management, in the U.S. News - Best Lawyers® “Best Law Firms”.
Job Summary
The Lead Data Statistician role in the Wage & Hour Data Analytics group provides support on various projects, including but not limited to, single plaintiff, class action, collective action, and DOL audit client exposure analyses, largely based on client timekeeping and payroll data. This individual will work autonomously to handle complex attorney/client issues and assist in overseeing the work of analysts.
Essential Functions
Regularly work as primary contact with attorneys, clients, and team manager to determine strategies on projects involving but not limited to class action data analytics.
Provide support on Practice Group initiatives, marketing efforts and pitches
Assess client exposure under various litigation scenarios – class action, collective action, etc.
Provide clear and concise communications regarding the analysis processes and results – both written and oral – to attorneys and client contacts, with the ability to explain complex topics in an easy-to-understand manner.
Develop methods and strategies to reach the end goal of a project based on various types/formats of raw client data.
Work with attorneys and analysts for analysis revisions, often on-the-fly during mediation proceedings.
Develop strong and effective relationships with clients
Execute assigned tasks in a self-directed, proactive manner, while actively managing own time and quality of work
Appropriately identify areas of discrete code and/or analytical methods that should be utilized for project analysis.
Expand scope of data tools employed by analysis team, including R, in conjunction with data and process integration, which is ongoing.
Ability to lead a project, as needed, from data intake and methodology development to analysis completion.
Qualifications/Skills Required
Minimum three years of experience directly working with Wage & Hour topics and data (timekeeping and payroll data especially) preferred, ideally within a legal or HR context.
Direct experience with California Wage & Hour laws a large plus.
Expert working knowledge with Excel, especially with basic and intermediate Excel formulas (vlookups, sumifs, etc.). VBA experience a large plus.
Ability to use Excel formulas and mathematics to manipulate raw data into dollar-figure exposure estimates.
Critical thinking and problem-solving skills with attention to detail. Experience in R, SQL, Python, other statistical programs a plus.
Ability to write programs/macros to automate repetitive statistical and data tasks.
Strong organizational and communication skills.
Ability to determine client and project needs and independently design and prepare data and analyses
Ability to multi-task and manage multiple projects, from start to finish, with shifting priority levels.
Willingness to conform to standardized group practices.
Educational Requirements
Master’s degree in applied statistics, mathematics, econometrics or a similar field or commensurate experience or equivalent level of training.
For Dallas, TX, the expected hourly wage range for this position is between $38.00/hour – $51.00/hour. The actual compensation will be determined based on experience and other factors permitted by law.
In accordance with the Colorado Equal Pay for Equal Work Act, the expected salary range for this Colorado position is between $38.00/hour – $51.00/hour. The actual compensation will be determined based on experience and other factors permitted by law.
Jackson Lewis offers a competitive benefits package that includes:
medical, dental, vision, life and disability insurance
401(k) Retirement Plan
Flexible Spending & Health Savings Account
firm-paid holidays, vacation, and sick time
For California and NY State, the expected salary range for this position is between $38/hour and $51/hour. The actual compensation will be determined based on experience and other factors permitted by law.
Jackson Lewis understands that embracing our differences makes us a stronger, better firm. We appreciate the importance of having a workforce that reflects the various communities in which we work, and we strive to create an inclusive environment where diverse employees want to work and where they can flourish professionally. In furtherance of our culture, all qualified applicants will receive consideration for employment without regard to race, national origin, gender, age, religion, disability, sexual orientation, veteran status, marital status or any other characteristics protected by law.
Show more
Show less","Wage & Hour law, Labor law, Employment law, Class action data analytics, Excel, VBA, SQL, R, Python, Data manipulation, Statistical programming, Data analysis, Project management, Time management, Coding, Macros, Communication, Critical thinking, Problemsolving, Attention to detail","wage hour law, labor law, employment law, class action data analytics, excel, vba, sql, r, python, data manipulation, statistical programming, data analysis, project management, time management, coding, macros, communication, critical thinking, problemsolving, attention to detail","attention to detail, class action data analytics, coding, communication, critical thinking, data manipulation, dataanalytics, employment law, excel, labor law, macros, problemsolving, project management, python, r, sql, statistical programming, time management, vba, wage hour law"
Senior Software Engineer - Data Strategy (NYC-Hybrid),Rad Hires,"Roslyn, NY",https://www.linkedin.com/jobs/view/senior-software-engineer-data-strategy-nyc-hybrid-at-rad-hires-3747284602,2023-12-17,Greenwich,United States,Mid senior,Hybrid,"Role
The Senior Software Engineer position in the Data Strategy team presents a chance to create and execute data products for the world's biggest and most reputable reinsurance brokerage. Data Strategy has a “start-up style” mandate (within a $2 billion company) to enhance the acquisition, storage, analysis, fidelity, and monetization of client, internal, and third-party data across the organization. This innovation spans our petabyte-scale insured assets, including property, business, marine, and aviation entities, and their associated risks, such as hurricanes, wildfires, cyber-attacks, and wars, in a financial and economic context.
As a member of the Data Strategy group, the Senior Software Engineer will work with fellow data and web engineers, data scientists, product managers, business analysts, and stakeholders from other internal groups to design and improve data-centric projects with the dual mandate of (1) increasing the efficiency of the data collection and analysis process across the company and (2) driving the monetization of data via newly designed and existing products for the company’s reinsurance clients. The Senior Software Engineer will be the head facilitator on multiple innovative initiatives and will have ownership over the design, development, and delivery of projects requiring direct reporting to senior-level management in both business and technical groups.
Leadership Responsibilities
Work with a product manager as technical lead of a team of ~5 engineers, data scientists, and analysts to design, scope, and oversee work in an Agile environment.
Manage junior data and web engineers, focusing on productivity, quality, and professional development.
Partner with the head of Data Strategy and other senior engineers to create and evangelize best-in-class engineering competency and tooling within the organization.
Enforce strong development standards across the team through code reviews, automated testing, and monitoring.
Establish strong relationships with internal clients as an engineering representative for data strategy.
Contribute to the overall Data Strategy vision and execution via quarterly planning and executive committee reporting.
Partner regularly improving engineering recruiting process for the required skillsets and resourcing demands.
Learn the complex business of reinsurance to coach data technologists and execute the team's initiatives more effectively.
Software Engineer Responsibilities
Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting, and new data products.
Innovate new ways to leverage large and small datasets to drive revenue via the development of new products with the Data Strategy team, as well as the enhancement of existing products.
Architect engineering solutions using the latest cloud technologies in a process that spans hypothesis-validating prototypes to large-scale production data products, ensuring internal security and regulatory compliance.
Design solutions that account for unstructured data and document management system(s), including ingesting, tracking, parsing, analyzing, and summarizing documents at scale.
Perform exploratory and goal-oriented data analyses to understand and validate the requirements of data products and help create product roadmaps.
Develop, implement, and deploy front-ends and APIs, which may involve business intelligence dashboards, data pipelines, machine learning algorithms, and file ingestion mechanisms.
Work closely with data scientists, data engineers, web engineers, PMs, and other stakeholders to design & develop products.
Keep current on the latest trends and innovations in data technology and how these trends apply to the company's business and data strategy.
Required Qualifications
5-8+ years of relevant experience in data-focused software engineering
Master’s Degree or Ph.D. in data science, computer science, or related quantitative field such as applied mathematics, statistics, engineering or operations research, or equivalent experience
Experience in Python and familiarity with OOP and functional programming principles
Strong knowledge of SQL and familiarity with the high-level properties of modern data stores.
Strong understanding of the contemporary SDLC, including dev/QC/prod environments, unit/integration/UA testing, CI/CD, etc.
Experience building and maintaining CI/CD pipelines with tools such as Azure DevOps, GitLab, Travis, Jenkins, etc.
At least two and ideally all of the following sets of experience:
Data Engineering
2+ years’ experience with data engineering
Extensive experience with (py)Spark, Python, JSON, and SQL
Experience integrating data from semi-structured and unstructured sources
Knowledge of various industry-leading SQL and NoSQL database systems
Backend Web
2+ years of backend/full-stack web engineering
Experience working with Python-based server-side web frameworks like FastAPI or Django
Experience with complex backends involving multiple data stores, asynchronous worker queues, pub-sub messaging, and the like
Knowledge of cloud-based web deployments (AWS/Azure/GCP, Kubernetes, auto-scaling, etc.)
Experience with one or more major frontend frameworks (React strongly preferred)
Data Science/Analytics
2+ years of data analysis, AI, or data science work
Experience with data cleaning, enrichment, and reporting to business users
Experience selecting, training, validating, and deploying machine-learning models
Experience with or strong interest in learning about LLMs in a productized context
Experience working in an Agile environment to facilitate the quick and effective fulfillment of group goals
Good interpersonal and communication skills for establishing and maintaining sound internal relationships, working well as part of a team, and for presentations and discussions
Strong analytical skills and intellectual curiosity (interest in the meaning and usefulness of the data), as demonstrated through academic experience or work assignments
Excellent English verbal and writing skills for complex communications with company colleagues in all departments and levels of the organization, including communicating technical concepts to a non-technical audience
Good ability to prioritize workload according to volume, urgency, etc., and to deliver on required projects in a timely fashion
Preferred Qualifications
Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks
Experience with web scraping and crowdsourcing technologies
Experience with Databricks and optimizing Spark clusters
Experience architecting web ecosystems from the ground up, including monolith vs. microservice decisions, caching technologies, security integrations, etc.
Experience working with data visualization dashboarding tools (PowerBI, Tableau)
Insurance domain knowledge or strong interest in developing it
Experience with the MS Azure cloud environment
Show more
Show less","Python, OOP, Functional programming, SQL, Data stores, SDLC, CI/CD, DevOps, GitLab, Travis, Jenkins, Data engineering, Spark, JSON, NoSQL, Backend web, FastAPI, Django, Cloudbased web deployments, Kubernetes, React, Data science, AI, Machine learning, Agile","python, oop, functional programming, sql, data stores, sdlc, cicd, devops, gitlab, travis, jenkins, data engineering, spark, json, nosql, backend web, fastapi, django, cloudbased web deployments, kubernetes, react, data science, ai, machine learning, agile","agile, ai, backend web, cicd, cloudbased web deployments, data engineering, data science, data stores, devops, django, fastapi, functional programming, gitlab, jenkins, json, kubernetes, machine learning, nosql, oop, python, react, sdlc, spark, sql, travis"
Senior Software Engineer - Data Strategy (NYC-Hybrid),Rad Hires,"Bronx, NY",https://www.linkedin.com/jobs/view/senior-software-engineer-data-strategy-nyc-hybrid-at-rad-hires-3747287237,2023-12-17,Greenwich,United States,Mid senior,Hybrid,"Role
The Senior Software Engineer position in the Data Strategy team presents a chance to create and execute data products for the world's biggest and most reputable reinsurance brokerage. Data Strategy has a “start-up style” mandate (within a $2 billion company) to enhance the acquisition, storage, analysis, fidelity, and monetization of client, internal, and third-party data across the organization. This innovation spans our petabyte-scale insured assets, including property, business, marine, and aviation entities, and their associated risks, such as hurricanes, wildfires, cyber-attacks, and wars, in a financial and economic context.
As a member of the Data Strategy group, the Senior Software Engineer will work with fellow data and web engineers, data scientists, product managers, business analysts, and stakeholders from other internal groups to design and improve data-centric projects with the dual mandate of (1) increasing the efficiency of the data collection and analysis process across the company and (2) driving the monetization of data via newly designed and existing products for the company’s reinsurance clients. The Senior Software Engineer will be the head facilitator on multiple innovative initiatives and will have ownership over the design, development, and delivery of projects requiring direct reporting to senior-level management in both business and technical groups.
Leadership Responsibilities
Work with a product manager as technical lead of a team of ~5 engineers, data scientists, and analysts to design, scope, and oversee work in an Agile environment.
Manage junior data and web engineers, focusing on productivity, quality, and professional development.
Partner with the head of Data Strategy and other senior engineers to create and evangelize best-in-class engineering competency and tooling within the organization.
Enforce strong development standards across the team through code reviews, automated testing, and monitoring.
Establish strong relationships with internal clients as an engineering representative for data strategy.
Contribute to the overall Data Strategy vision and execution via quarterly planning and executive committee reporting.
Partner regularly improving engineering recruiting process for the required skillsets and resourcing demands.
Learn the complex business of reinsurance to coach data technologists and execute the team's initiatives more effectively.
Software Engineer Responsibilities
Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting, and new data products.
Innovate new ways to leverage large and small datasets to drive revenue via the development of new products with the Data Strategy team, as well as the enhancement of existing products.
Architect engineering solutions using the latest cloud technologies in a process that spans hypothesis-validating prototypes to large-scale production data products, ensuring internal security and regulatory compliance.
Design solutions that account for unstructured data and document management system(s), including ingesting, tracking, parsing, analyzing, and summarizing documents at scale.
Perform exploratory and goal-oriented data analyses to understand and validate the requirements of data products and help create product roadmaps.
Develop, implement, and deploy front-ends and APIs, which may involve business intelligence dashboards, data pipelines, machine learning algorithms, and file ingestion mechanisms.
Work closely with data scientists, data engineers, web engineers, PMs, and other stakeholders to design & develop products.
Keep current on the latest trends and innovations in data technology and how these trends apply to the company's business and data strategy.
Required Qualifications
5-8+ years of relevant experience in data-focused software engineering
Master’s Degree or Ph.D. in data science, computer science, or related quantitative field such as applied mathematics, statistics, engineering or operations research, or equivalent experience
Experience in Python and familiarity with OOP and functional programming principles
Strong knowledge of SQL and familiarity with the high-level properties of modern data stores.
Strong understanding of the contemporary SDLC, including dev/QC/prod environments, unit/integration/UA testing, CI/CD, etc.
Experience building and maintaining CI/CD pipelines with tools such as Azure DevOps, GitLab, Travis, Jenkins, etc.
At least two and ideally all of the following sets of experience:
Data Engineering
2+ years’ experience with data engineering
Extensive experience with (py)Spark, Python, JSON, and SQL
Experience integrating data from semi-structured and unstructured sources
Knowledge of various industry-leading SQL and NoSQL database systems
Backend Web
2+ years of backend/full-stack web engineering
Experience working with Python-based server-side web frameworks like FastAPI or Django
Experience with complex backends involving multiple data stores, asynchronous worker queues, pub-sub messaging, and the like
Knowledge of cloud-based web deployments (AWS/Azure/GCP, Kubernetes, auto-scaling, etc.)
Experience with one or more major frontend frameworks (React strongly preferred)
Data Science/Analytics
2+ years of data analysis, AI, or data science work
Experience with data cleaning, enrichment, and reporting to business users
Experience selecting, training, validating, and deploying machine-learning models
Experience with or strong interest in learning about LLMs in a productized context
Experience working in an Agile environment to facilitate the quick and effective fulfillment of group goals
Good interpersonal and communication skills for establishing and maintaining sound internal relationships, working well as part of a team, and for presentations and discussions
Strong analytical skills and intellectual curiosity (interest in the meaning and usefulness of the data), as demonstrated through academic experience or work assignments
Excellent English verbal and writing skills for complex communications with company colleagues in all departments and levels of the organization, including communicating technical concepts to a non-technical audience
Good ability to prioritize workload according to volume, urgency, etc., and to deliver on required projects in a timely fashion
Preferred Qualifications
Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks
Experience with web scraping and crowdsourcing technologies
Experience with Databricks and optimizing Spark clusters
Experience architecting web ecosystems from the ground up, including monolith vs. microservice decisions, caching technologies, security integrations, etc.
Experience working with data visualization dashboarding tools (PowerBI, Tableau)
Insurance domain knowledge or strong interest in developing it
Experience with the MS Azure cloud environment
Show more
Show less","Data Strategy, Machine Learning, SQL, Azure DevOps, Python, Spark, OOP, Agile, NoSQL, CI/CD, Cloud Computing, AWS, GCP, Kubernetes, React, Data Science, GitLab, Travis, Jenkins, FastAPI, Django, PowerBI, Tableau","data strategy, machine learning, sql, azure devops, python, spark, oop, agile, nosql, cicd, cloud computing, aws, gcp, kubernetes, react, data science, gitlab, travis, jenkins, fastapi, django, powerbi, tableau","agile, aws, azure devops, cicd, cloud computing, data science, data strategy, django, fastapi, gcp, gitlab, jenkins, kubernetes, machine learning, nosql, oop, powerbi, python, react, spark, sql, tableau, travis"
Senior Data Platform Engineer,EverBright,"Palm Beach Gardens, FL",https://www.linkedin.com/jobs/view/senior-data-platform-engineer-at-everbright-3776608721,2023-12-17,West Palm Beach,United States,Mid senior,Remote,"EverBright is a clean energy company on a mission to accelerate the decarbonization of residences and businesses across the United States. We are a leading provider of financing for residential solar, storage, and energy efficiency products, fueling our rapid growth through innovative software solutions. We understand that the energy systems of tomorrow will be clean, distributed, and powered by advanced technology.
As a Senior Data Platform Engineer, you will play a critical role in designing, building, and maintaining the data infrastructure for our organization, employing a strong combination of data engineering and software engineering skills as we build out our new data platform. You will be responsible for developing and maintaining scalable and reliable data pipelines that enable data science, analytics, software, and business teams to build products and make critical data-driven strategic decisions.
Your role will involve collaborating with cross-functional teams to identify business requirements and translate them into technical solutions. You will also be responsible for designing and implementing data models and warehouses that can efficiently handle large-scale data.
In addition to technical expertise, you should have strong leadership and communication skills to effectively lead and mentor junior data engineers on the team. You should be comfortable working in an agile development environment and have experience with agile methodologies.
What You'll Do
Design, build, and maintain scalable and reliable data pipelines, infrastructure and services
Develop and maintain data models and data warehouses
Lead and mentor junior data engineers on the team
Continuously evaluate and improve data infrastructure to ensure scalability and reliability
Preferred Qualifications
Bachelor's degree in Computer Science or other related Engineering field
4+ years of experience in data engineering
7+ years experience software engineering or related field
Strong proficiency in SQL and programming languages such as Python, Java or Scala
Hands-on experience with data warehousing and data modeling, with a strong understanding of relational database schema design
Experience building systems with cloud providers such as AWS or Azure
Good knowledge of the current data engineering landscape + available tools
Strong leadership and communication skills
Required Qualifications
Bachelor’s Degree or equivalent work experience
Experience: 7+ years
Show more
Show less","Data Engineering, Software Engineering, Data Science, Analytics, Python, Java, Scala, SQL, Agile Methodologies, Agile Development, Cloud Providers (AWS Azure), Data Warehousing, Data Modeling, Leadership Skills, Communication Skills","data engineering, software engineering, data science, analytics, python, java, scala, sql, agile methodologies, agile development, cloud providers aws azure, data warehousing, data modeling, leadership skills, communication skills","agile development, agile methodologies, analytics, cloud providers aws azure, communication skills, data engineering, data science, datamodeling, datawarehouse, java, leadership skills, python, scala, software engineering, sql"
Sr Data Engineer with Security Clearance,ClearanceJobs,"Agency, IA",https://www.linkedin.com/jobs/view/sr-data-engineer-with-security-clearance-at-clearancejobs-3753459897,2023-12-17,Ottumwa,United States,Mid senior,Onsite,"Desired Skills
Talint's client, a management consulting firm provides technology services and delivers deep-tech/innovation for a variety of mission-critical fields to support U.S. National Security, is a fast growing, employee focused company. They are actively hiring talented data and engineering experts looking for a challenging yet rewarding career path in the Intelligence Community (IC). Our client has an immediate need for a Sr Data Engineer. This person will provide technical support in data science, data engineering, and systems engineering and reviewing and providing technical assessments on a new Five-year contract. The right candidate will have at least three years of experience in data analysis, using data programming tools (Python and R), JavaScript, REACT, and AngularJS. Additionally, the right person will have a Top Secret Clearance with a Full-Scope Polygraph. This role is onsite in McLean, VA. Desired Skills and Experience:
Top Secret Clearance with Full-Scope Polygraph;
5 years of experience in business intelligence reporting tools and data visualization software including Tableau;
Knowledgeable with JavaScript, REACT, and AngularJS;
Experience with data cleaning and transformation efforts in delivery of CDRLs;
Expert with extracting and aggregating structured and unstructured data;
Knowledgeable in data programming languages such as Python and R;
Familiar with SQL or similar database language;
Experience with designing and implementing data models to enable, sustain, and enhance the value of information they contain. Why our client?
Competitive total compensation and benefits package that includes health and paid time off;
Our client fosters an open-door policy between employees and leadership to ensure direct lines of communication;
Incredible opportunities for advancement as they continue to grow;
Training and development is a key tenet of our client. Responsibilities:
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers;
Make sure pedigree and provenance of the data is maintained such that the access to data is protected;
Clean and preprocess data to enable analytic access;
Collaborate with the engineering team, data stewards, and mission partners to aid in getting actionable value out of the data holdings architects complex, repeatable ETL processes;
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic, and others;
Develop API connectors to enable ingest of new data catalog entries from databases and files.
Show more
Show less","JavaScript, Python, R, React, AngularJS, Tableau, SQL, Hadoop, Spark, Hudi, EMR, Kubernetes, Oracle, MySQL, MariaDB, MongoDB, Elastic, ETL","javascript, python, r, react, angularjs, tableau, sql, hadoop, spark, hudi, emr, kubernetes, oracle, mysql, mariadb, mongodb, elastic, etl","angularjs, elastic, emr, etl, hadoop, hudi, javascript, kubernetes, mariadb, mongodb, mysql, oracle, python, r, react, spark, sql, tableau"
Senior Data Scientist for AI-Driven Productivity Platform,SSA Group,"Chiasson Office, New Brunswick, Canada",https://ca.linkedin.com/jobs/view/senior-data-scientist-for-ai-driven-productivity-platform-at-ssa-group-3780290682,2023-12-17,New Brunswick, Canada,Mid senior,Onsite,"Lead BDM and Head of the Data & Desktop Department tells about team structure, technology stack, candidate requirements and other project details
Join SSA Group team working on an AI-powered productivity platform that enhances work efficiency and empowers employees, teams, and executives with real-time productivity awareness and actionable recommendations to unleash the potential for optimization and growth.
Project
The platform’s core is a cutting-edge AI that provides the world’s first standardized productivity score, determines optimal work patterns, identifies which actions most positively impact team productivity growth, and makes personalized, prioritized AI-driven recommendations.
The project caught the attention of Forbes and has been recognized as a Sample Vendor in the Gartner Report.
Technology Stack
Programming language: Python
Data manipulation libraries: Pandas, NumPy, Dask
ML: TensorFlow, PyTorch, Scikit-learn
AWS: S3, Data Pipelines, Athena
Data visualization: Apache SuperSet
Team
CTO, Fullstack Engineer, AI team of 2 Data Scientists
Candidate Requirements
5+ years of experience in data science or data analysis
Good skills in prototyping using Jupyter notebook
Practical experience with data manipulation in Python using Pandas, NumPy
Practical experience with data analysis and machine learning related Python libraries: TensorFlow, PyTorch, Scikit-learn
Knowledge of data quality tools such as Great Expectations
Bachelor’s or higher degree in a relevant field
Upper intermediate or higher English level
Will Be a Plus
Experience with AWS: Data Pipelines, S3, Athena
Experience with Dask
Team Leading experience
Responsibilities
Perform data manipulation: ingestion, normalization, structuring, cleansing, validation, storage
Perform deep data analysis and feature engineering
Develop platform core features as well as customer specific implementations
Communicate with CTO, development and AI teams
Show more
Show less","Python, Pandas, NumPy, Dask, TensorFlow, PyTorch, Scikitlearn, AWS, Apache Superset, Jupyter Notebook, Data manipulation, Data analysis, Machine learning, Data quality tools, Data ingestion, Data normalization, Data structuring, Data cleansing, Data validation, Data storage, Deep data analysis, Feature engineering, Platform development, Customer specific implementations","python, pandas, numpy, dask, tensorflow, pytorch, scikitlearn, aws, apache superset, jupyter notebook, data manipulation, data analysis, machine learning, data quality tools, data ingestion, data normalization, data structuring, data cleansing, data validation, data storage, deep data analysis, feature engineering, platform development, customer specific implementations","apache superset, aws, customer specific implementations, dask, data ingestion, data manipulation, data normalization, data quality tools, data storage, data structuring, data validation, dataanalytics, datacleaning, deep data analysis, feature engineering, jupyter notebook, machine learning, numpy, pandas, platform development, python, pytorch, scikitlearn, tensorflow"
Customer Service Representative/Data Analyst/Data Entry Clerk/Urgent,Rodtookjing,"Saint John, New Brunswick, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-urgent-at-rodtookjing-3741434821,2023-12-17,New Brunswick, Canada,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk/Urgent will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Analysis, SQL, R, Python, Data Visualization, Tableau, Power BI, Data Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL, Data Quality Control, Data Cleansing, Data Manipulation","data analysis, statistical analysis, sql, r, python, data visualization, tableau, power bi, data modeling, hypothesis testing, ab testing, data management, etl, data quality control, data cleansing, data manipulation","ab testing, data management, data manipulation, data quality control, dataanalytics, datacleaning, datamodeling, etl, hypothesis testing, powerbi, python, r, sql, statistical analysis, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk,Toyandsons,"Fredericton, New Brunswick, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-toyandsons-3756471506,2023-12-17,New Brunswick, Canada,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Data Interpretation, Data Visualization, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL (Extract Transform Load), Data integrity, Data accuracy, Data completeness","data analysis, data interpretation, data visualization, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl extract transform load, data integrity, data accuracy, data completeness","ab testing, data accuracy, data completeness, data integrity, data interpretation, data management, dataanalytics, etl extract transform load, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Ropesgray,"Campbellton, New Brunswick, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-ropesgray-3752011687,2023-12-17,New Brunswick, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analytics, Data Interpretation, Datadriven Decisionmaking, Statistical Techniques, Performance Metrics, Data Quality, Data Manipulation, Data Visualization, Tableau, Power BI, SQL, R, Python, A/B Testing, Statistical Modeling, ETL","data analytics, data interpretation, datadriven decisionmaking, statistical techniques, performance metrics, data quality, data manipulation, data visualization, tableau, power bi, sql, r, python, ab testing, statistical modeling, etl","ab testing, data interpretation, data manipulation, data quality, dataanalytics, datadriven decisionmaking, etl, performance metrics, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Dukeduchessinternational,"Tracadie-Sheila, New Brunswick, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-dukeduchessinternational-3756823017,2023-12-17,New Brunswick, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical techniques, R, Python, SQL, Tableau, Power BI, Data visualization, Statistical modeling, Hypothesis testing, A/B testing, Data management, ETL processes, Data quality, Data integrity, Data completeness, Data collection, Data cleansing, Data manipulation","data analysis, statistical techniques, r, python, sql, tableau, power bi, data visualization, statistical modeling, hypothesis testing, ab testing, data management, etl processes, data quality, data integrity, data completeness, data collection, data cleansing, data manipulation","ab testing, data collection, data completeness, data integrity, data management, data manipulation, data quality, dataanalytics, datacleaning, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Villarestaurantgroup,"Miramichi, New Brunswick, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-villarestaurantgroup-3756474070,2023-12-17,New Brunswick, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Python, R, SQL, Tableau, Power BI, A/B testing, Hypothesis testing, Statistical modeling, Data visualization, Data analysis, Data management, Data manipulation, Data quality management, ETL processes, Data integrity, Communication, Collaboration, Problemsolving, Analytical thinking, Critical thinking","python, r, sql, tableau, power bi, ab testing, hypothesis testing, statistical modeling, data visualization, data analysis, data management, data manipulation, data quality management, etl processes, data integrity, communication, collaboration, problemsolving, analytical thinking, critical thinking","ab testing, analytical thinking, collaboration, communication, critical thinking, data integrity, data management, data manipulation, data quality management, dataanalytics, etl, hypothesis testing, powerbi, problemsolving, python, r, sql, statistical modeling, tableau, visualization"
Data Engineer,Hedge Fund,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-3784599285,2023-12-17,Oxnard,United States,Associate,Onsite,"Position Overview:   Reporting to the Head of Data Analytics & Strategy, the Data Engineer will design, implement, and maintain complex data storage, pipeline, and analytics systems that support and further develop the data infrastructure of the organization’s Quantitative and Fundamental investment groups.  Responsibilities include building large data pipelines and developing APIs to retrieve data, all to innovate the next level of analytics systems that effectively integrate with and enhance current research applications
Responsibilities:
•           Work with researchers and portfolio managers to analyze and improve the architecture of or proprietary and derived data sets
•           Solve interesting data engineering problems in support of quantitative researches leveraging time-series data
•           Implement cloud data platforms capable of handling the computational complexities of machine learning workflows on large data sets
•           Write code that provides reliable automated data pipelines, included automated quality checks
•           Take ownership of the data engineering code base, focusing on high-quality delivery of both code and data
•           Be a technical leader by building pipelines to explore and leverage alternative non-traditional data sources
Qualifications:
•           1-8 years’ relevant experience and a degree in Computer Science (or related field) with a strong GPA
•           Strong foundational skills in SQL, query performance tuning, and bash/shell scripting
•           Expert-level programming skills in Python and proficient with one of the following: Java/Scala, C/C++
•           Expertise in data modeling and choosing an appropriate model given the situation
•           Capital Markets and Asset Management experience and related datasets/vendors is a plus
•           Ability to provide thought leadership and contribute to the firm's overall technology strategy
Show more
Show less","Data Engineering, Data Storage, Data Pipeline, Analytics Systems, Quantitative Research, Fundamental Investment, Machine Learning, SQL, Bash/Shell Scripting, Python, Java, Scala, C, C++, Data Modeling, Capital Markets, Asset Management","data engineering, data storage, data pipeline, analytics systems, quantitative research, fundamental investment, machine learning, sql, bashshell scripting, python, java, scala, c, c, data modeling, capital markets, asset management","analytics systems, asset management, bashshell scripting, c, capital markets, data engineering, data pipeline, data storage, datamodeling, fundamental investment, java, machine learning, python, quantitative research, scala, sql"
Data Center Engineer,Cloudflare,"Austin, TX",https://www.linkedin.com/jobs/view/data-center-engineer-at-cloudflare-3732383582,2023-12-17,Oxnard,United States,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
Data Center Operations Engineer
About the department
In this role, you will be focused on maintaining the Clou dflare global network. You 'll work closely with Cloudflare’s SRE (Site Reliability Engineering) team, Network Engineering team, Network Deployment Engineering team and with various vendors and partners (including hardware vendors, datacenter and network providers, and ISPs) to maintain and improve our global infrastructure. You will further be responsible for the development and implementation of consistent processes and visibility measurements for consistent and effective management of our infrastructure. This is a highly visible position that requires deep technical understanding of datacenter infrastructure, networking (physical), and basic experience with data analysis and project management.
To be successful in this position, you should have excellent technical skills, communication skills, and be able to navigate a range of challenges and constraints (e.g. schedule adherence, time zones, and cultures). You will have the opportunity to (literally) build a faster, safer Internet for our millions of users and the billions of web surfers that visit their sites each month.
Who You Are
You will thrive in a hypergrowth engineering environment and be self driven with a keen attention to detail. You will come with a deep technical understanding of Data Center colocation environments, network architecture and server technologies. You will be used to working through partners to support infrastructure delivery to a number of remote locations. You will have had experience managing operational environments, and used to developing new approaches to improve delivery efficiency or operational stability.
What You'll Do
Collaborating with internal teams (Infrastructure, Network Engineering and SRE). Create documentation and manage remote contractors to complete datacenter tasks, working with hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 300+ datacenter locations
Maintain Data Center environment operational availability
Creating and maintaining documentation, plans, SOP’s, MOP’s etc.
Support and configure network infrastructure where required
Providing feedback to internal teams to support internal tools and external vendor partnerships
Required Experience
Minimum of 5 yrs of Linux systems administration
Experience with Juniper, Cisco and DWDM network equipment
Experience managing and instructing remote contractors
Familiarity with work required to stand up infrastructure in remote colocation facilities
Experience running and improving operational processes, including automation tooling, in a rapidly changing environment
Familiarity with day-to-day tasks and projects common to Data Center Operations (deployment, migration, decommissioning etc.)
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Incident management
Other Responsibilities May Include
Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure
Assist with the definition, documentation and implementation of consistent processes across all region
Limited travel
Examples Of Desirable Skills, Knowledge And Experience
Bachelor’s degree; technical background in engineering, computer science, or MIS
Direct experience executing on complex data center/infrastructure projects
Previous experience installing / maintaining data center (and other IT) infrastructure and DCIM tools
Experience running and improving operational processes in a rapidly changing environment
Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills
Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously
Ability to manage MS excel and Google spreadsheets
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Must be a team player
Bonus Points
Multi-lingual; experience working with infrastructure in multiple countries
Comfortable with remote “lights-out” and out-of-band access to data center resources
Linux certifications (RHCSA etc.)
Network certifications (CCNA, JNCIA or higher)
Compensation
Compensation may be adjusted depending on work location.
For Colorado-based hires: Estimated annual salary of $ 111,000 - $ 135,000 .
For New York City, Washington, and California (excluding Bay Area) based hires: Estimated annual salary of $ 135,000 - $ 165,000
For Bay Area-based hires: Estimated annual salary of $ 142,000 - $ 174,000 .
Equity
This role is eligible to participate in Cloudflare’s equity plan.
Benefits
Cloudflare offers a complete package of benefits and programs to support you and your family. Our benefits programs can help you pay health care expenses, support caregiving, build capital for the future and make life a little easier and fun! The below is a description of our benefits for employees in the United States, and benefits may vary for employees based outside the U.S.
Health & Welfare Benefits
Medical/Rx Insurance
Dental Insurance
Vision Insurance
Flexible Spending Accounts
Commuter Spending Accounts
Fertility & Family Forming Benefits
On-demand mental health support and Employee Assistance Program
Global Travel Medical Insurance
Financial Benefits
Short and Long Term Disability Insurance
Life & Accident Insurance
401(k) Retirement Savings Plan
Employee Stock Participation Plan
Time Off
Flexible paid time off covering vacation and sick leave
Leave programs, including parental, pregnancy health, medical, and bereavement leave
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Linux, Juniper, Cisco, DWDM, Network equipment, Data Center Operations, Deployment, Migration, Decommissioning, JIRA, Program management, MS Excel, Google Spreadsheets, RHCSA, CCNA, JNCIA","linux, juniper, cisco, dwdm, network equipment, data center operations, deployment, migration, decommissioning, jira, program management, ms excel, google spreadsheets, rhcsa, ccna, jncia","ccna, cisco, data center operations, decommissioning, deployment, dwdm, google spreadsheets, jira, jncia, juniper, linux, migration, ms excel, network equipment, program management, rhcsa"
Data Center Engineer,Cloudflare,"Boston, MA",https://www.linkedin.com/jobs/view/data-center-engineer-at-cloudflare-3732385209,2023-12-17,Oxnard,United States,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
Data Center Operations Engineer
About the department
In this role, you will be focused on maintaining the Clou dflare global network. You 'll work closely with Cloudflare’s SRE (Site Reliability Engineering) team, Network Engineering team, Network Deployment Engineering team and with various vendors and partners (including hardware vendors, datacenter and network providers, and ISPs) to maintain and improve our global infrastructure. You will further be responsible for the development and implementation of consistent processes and visibility measurements for consistent and effective management of our infrastructure. This is a highly visible position that requires deep technical understanding of datacenter infrastructure, networking (physical), and basic experience with data analysis and project management.
To be successful in this position, you should have excellent technical skills, communication skills, and be able to navigate a range of challenges and constraints (e.g. schedule adherence, time zones, and cultures). You will have the opportunity to (literally) build a faster, safer Internet for our millions of users and the billions of web surfers that visit their sites each month.
Who You Are
You will thrive in a hypergrowth engineering environment and be self driven with a keen attention to detail. You will come with a deep technical understanding of Data Center colocation environments, network architecture and server technologies. You will be used to working through partners to support infrastructure delivery to a number of remote locations. You will have had experience managing operational environments, and used to developing new approaches to improve delivery efficiency or operational stability.
What You'll Do
Collaborating with internal teams (Infrastructure, Network Engineering and SRE). Create documentation and manage remote contractors to complete datacenter tasks, working with hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 300+ datacenter locations
Maintain Data Center environment operational availability
Creating and maintaining documentation, plans, SOP’s, MOP’s etc.
Support and configure network infrastructure where required
Providing feedback to internal teams to support internal tools and external vendor partnerships
Required Experience
Minimum of 5 yrs of Linux systems administration
Experience with Juniper, Cisco and DWDM network equipment
Experience managing and instructing remote contractors
Familiarity with work required to stand up infrastructure in remote colocation facilities
Experience running and improving operational processes, including automation tooling, in a rapidly changing environment
Familiarity with day-to-day tasks and projects common to Data Center Operations (deployment, migration, decommissioning etc.)
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Incident management
Other Responsibilities May Include
Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure
Assist with the definition, documentation and implementation of consistent processes across all region
Limited travel
Examples Of Desirable Skills, Knowledge And Experience
Bachelor’s degree; technical background in engineering, computer science, or MIS
Direct experience executing on complex data center/infrastructure projects
Previous experience installing / maintaining data center (and other IT) infrastructure and DCIM tools
Experience running and improving operational processes in a rapidly changing environment
Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills
Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously
Ability to manage MS excel and Google spreadsheets
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Must be a team player
Bonus Points
Multi-lingual; experience working with infrastructure in multiple countries
Comfortable with remote “lights-out” and out-of-band access to data center resources
Linux certifications (RHCSA etc.)
Network certifications (CCNA, JNCIA or higher)
Compensation
Compensation may be adjusted depending on work location.
For Colorado-based hires: Estimated annual salary of $ 111,000 - $ 135,000 .
For New York City, Washington, and California (excluding Bay Area) based hires: Estimated annual salary of $ 135,000 - $ 165,000
For Bay Area-based hires: Estimated annual salary of $ 142,000 - $ 174,000 .
Equity
This role is eligible to participate in Cloudflare’s equity plan.
Benefits
Cloudflare offers a complete package of benefits and programs to support you and your family. Our benefits programs can help you pay health care expenses, support caregiving, build capital for the future and make life a little easier and fun! The below is a description of our benefits for employees in the United States, and benefits may vary for employees based outside the U.S.
Health & Welfare Benefits
Medical/Rx Insurance
Dental Insurance
Vision Insurance
Flexible Spending Accounts
Commuter Spending Accounts
Fertility & Family Forming Benefits
On-demand mental health support and Employee Assistance Program
Global Travel Medical Insurance
Financial Benefits
Short and Long Term Disability Insurance
Life & Accident Insurance
401(k) Retirement Savings Plan
Employee Stock Participation Plan
Time Off
Flexible paid time off covering vacation and sick leave
Leave programs, including parental, pregnancy health, medical, and bereavement leave
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Linux systems administration, Juniper Cisco DWDM network equipment, Remote contractors, Data Center Operations, Automation tooling, MS Excel Google spreadsheets, JIRA, Incident management, Engineering computer science MIS, Data center/infrastructure projects, DCIM tools, Project management, Verbal and written communication, Problemsolving, Attention to detail, Interpersonal skills, Multilingual, Lightsout access, RHCSA certification, CCNA JNCIA certification","linux systems administration, juniper cisco dwdm network equipment, remote contractors, data center operations, automation tooling, ms excel google spreadsheets, jira, incident management, engineering computer science mis, data centerinfrastructure projects, dcim tools, project management, verbal and written communication, problemsolving, attention to detail, interpersonal skills, multilingual, lightsout access, rhcsa certification, ccna jncia certification","attention to detail, automation tooling, ccna jncia certification, data center operations, data centerinfrastructure projects, dcim tools, engineering computer science mis, incident management, interpersonal skills, jira, juniper cisco dwdm network equipment, lightsout access, linux systems administration, ms excel google spreadsheets, multilingual, problemsolving, project management, remote contractors, rhcsa certification, verbal and written communication"
Associate Data Engineer,Orbital Kitchens,"New York, NY",https://www.linkedin.com/jobs/view/associate-data-engineer-at-orbital-kitchens-3785599101,2023-12-17,Oxnard,United States,Associate,Onsite,"About the Role
Orbital Kitchens seeks a highly motivated and detail-oriented Associate Data Engineer to join our dynamic team. This role is ideal for someone with a strong background in data pipeline development and experience in utilizing Snowflake as a Data Warehouse and Tableau as a Business Intelligence (BI) tool. As an Associate Data Engineer at Orbital Kitchens, you will be crucial in managing and optimizing our data infrastructure and supporting business intelligence and analytics initiatives.
Key Responsibilities
Data Pipeline Development: Design, develop, and maintain robust and scalable data pipelines to ensure efficient data flow across different systems and platforms. Collaborate with cross-functional teams to meet business data requirements.
Snowflake Data Warehouse Management: Utilize Snowflake expertise to design, implement, and manage data warehouse architecture, optimizing configurations for performance and scalability.
Tableau BI Integration: Seamlessly integrate Tableau into our data ecosystem for effective data visualization. Work alongside business analysts to develop interactive, insightful dashboards and reports.
Data Modeling and Optimization: Engage in data modeling to support operational and analytical requirements. Continuously enhance data structures and queries for improved performance.
Quality Assurance and Documentation: Implement best data quality and validation practices and document engineering processes, configurations, and troubleshooting guides.
Collaboration and Communication: Effectively collaborate with cross-functional teams and communicate technical findings to non-technical stakeholders.
Technology Trends: Stay updated with industry trends in data engineering and analytics and recommend improvements for our data infrastructure
Qualifications
Bachelor’s degree in Computer Science, IT, or related field.
Demonstrated experience in data pipeline setup and data infrastructure management.
Proficiency in Snowflake and Tableau.
Strong SQL skills and understanding of database management systems.
Familiarity with cloud platforms (AWS, Azure, or Google Cloud) is a plus.
Excellent problem-solving, analytical, communication, and collaboration skills.
2-4 years, preferably in a startup environment
Show more
Show less","Data Pipeline Development, Snowflake Data Warehouse, Tableau BI Integration, Data Modeling and Optimization, SQL, Database Management Systems, AWS, Azure, Google Cloud","data pipeline development, snowflake data warehouse, tableau bi integration, data modeling and optimization, sql, database management systems, aws, azure, google cloud","aws, azure, data modeling and optimization, data pipeline development, database management systems, google cloud, snowflake data warehouse, sql, tableau bi integration"
Data Center Engineer,Cloudflare,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-center-engineer-at-cloudflare-3732380840,2023-12-17,Oxnard,United States,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
Data Center Operations Engineer
About the department
In this role, you will be focused on maintaining the Clou dflare global network. You 'll work closely with Cloudflare’s SRE (Site Reliability Engineering) team, Network Engineering team, Network Deployment Engineering team and with various vendors and partners (including hardware vendors, datacenter and network providers, and ISPs) to maintain and improve our global infrastructure. You will further be responsible for the development and implementation of consistent processes and visibility measurements for consistent and effective management of our infrastructure. This is a highly visible position that requires deep technical understanding of datacenter infrastructure, networking (physical), and basic experience with data analysis and project management.
To be successful in this position, you should have excellent technical skills, communication skills, and be able to navigate a range of challenges and constraints (e.g. schedule adherence, time zones, and cultures). You will have the opportunity to (literally) build a faster, safer Internet for our millions of users and the billions of web surfers that visit their sites each month.
Who You Are
You will thrive in a hypergrowth engineering environment and be self driven with a keen attention to detail. You will come with a deep technical understanding of Data Center colocation environments, network architecture and server technologies. You will be used to working through partners to support infrastructure delivery to a number of remote locations. You will have had experience managing operational environments, and used to developing new approaches to improve delivery efficiency or operational stability.
What You'll Do
Collaborating with internal teams (Infrastructure, Network Engineering and SRE). Create documentation and manage remote contractors to complete datacenter tasks, working with hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 300+ datacenter locations
Maintain Data Center environment operational availability
Creating and maintaining documentation, plans, SOP’s, MOP’s etc.
Support and configure network infrastructure where required
Providing feedback to internal teams to support internal tools and external vendor partnerships
Required Experience
Minimum of 5 yrs of Linux systems administration
Experience with Juniper, Cisco and DWDM network equipment
Experience managing and instructing remote contractors
Familiarity with work required to stand up infrastructure in remote colocation facilities
Experience running and improving operational processes, including automation tooling, in a rapidly changing environment
Familiarity with day-to-day tasks and projects common to Data Center Operations (deployment, migration, decommissioning etc.)
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Incident management
Other Responsibilities May Include
Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure
Assist with the definition, documentation and implementation of consistent processes across all region
Limited travel
Examples Of Desirable Skills, Knowledge And Experience
Bachelor’s degree; technical background in engineering, computer science, or MIS
Direct experience executing on complex data center/infrastructure projects
Previous experience installing / maintaining data center (and other IT) infrastructure and DCIM tools
Experience running and improving operational processes in a rapidly changing environment
Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills
Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously
Ability to manage MS excel and Google spreadsheets
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Must be a team player
Bonus Points
Multi-lingual; experience working with infrastructure in multiple countries
Comfortable with remote “lights-out” and out-of-band access to data center resources
Linux certifications (RHCSA etc.)
Network certifications (CCNA, JNCIA or higher)
Compensation
Compensation may be adjusted depending on work location.
For Colorado-based hires: Estimated annual salary of $ 111,000 - $ 135,000 .
For New York City, Washington, and California (excluding Bay Area) based hires: Estimated annual salary of $ 135,000 - $ 165,000
For Bay Area-based hires: Estimated annual salary of $ 142,000 - $ 174,000 .
Equity
This role is eligible to participate in Cloudflare’s equity plan.
Benefits
Cloudflare offers a complete package of benefits and programs to support you and your family. Our benefits programs can help you pay health care expenses, support caregiving, build capital for the future and make life a little easier and fun! The below is a description of our benefits for employees in the United States, and benefits may vary for employees based outside the U.S.
Health & Welfare Benefits
Medical/Rx Insurance
Dental Insurance
Vision Insurance
Flexible Spending Accounts
Commuter Spending Accounts
Fertility & Family Forming Benefits
On-demand mental health support and Employee Assistance Program
Global Travel Medical Insurance
Financial Benefits
Short and Long Term Disability Insurance
Life & Accident Insurance
401(k) Retirement Savings Plan
Employee Stock Participation Plan
Time Off
Flexible paid time off covering vacation and sick leave
Leave programs, including parental, pregnancy health, medical, and bereavement leave
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Linux, Juniper, Cisco, DWDM, JIRA, MS Excel, Google Spreadsheets, RHCSA, CCNA, JNCIA","linux, juniper, cisco, dwdm, jira, ms excel, google spreadsheets, rhcsa, ccna, jncia","ccna, cisco, dwdm, google spreadsheets, jira, jncia, juniper, linux, ms excel, rhcsa"
Statistician/Data Analyst - (JP9785),"3 Key Consulting, Inc.","Thousand Oaks, CA",https://www.linkedin.com/jobs/view/statistician-data-analyst-jp9785-at-3-key-consulting-inc-3619870531,2023-12-17,Oxnard,United States,Mid senior,Onsite,"Job Title:
Statistician/Data Analyst - (JP9785)
Location:
Thousand Oaks, CA.
Employment Type:
Contract
Business Unit:
CIDA Design and Innovation
Duration:
1+ years (with likely extensions)
Notes
Posting Date:
02/23/2022
3 Key Consulting is hiring a
Statistician/Data Analyst
for a consulting engagement with our direct client, a leading global biopharmaceutical company.
Job Description
The ideal candidate will build machine learning models and other statistical models, as well as support their implementation and deployment through a good understanding of the Software Development Life Cycle (SDLC). Develop business requirements thru collaborations with cross-functional stakeholders and the use of excellent communication skills. Experience with clinical trials and its data collection process, storage, quality, and use would be looked for. The latter will be particularly critical in a regulated environment. Functional requirements, prototype code, good documentation, testing, and relevant visualizations will be expected as deliverables when producing new algorithms as required by the stakeholders.
The ideal candidate must enjoy data analysis challenges and possess a wide variety of tools under their belt to tackle these challenges while exhibiting creativity and innovation in so doing. Excellent organization skills will be needed. These will be important when managing information, data sources, and/or user documentation. This person will be able to adapt and abide by controlled processes (SOPs), as they will be working under regulatory requirements from official sources. The candidate must be self-motivated and demonstrate good time-management skills.
Responsibilities Will Include Among Others
Ability to perform data analysis in a wide range of data and analytics solutions, from descriptive to prescriptive, using advanced statistical and machine learning models.
Leverage large data sets to conduct end-to end analytics that will include data gathering, requirements specifications, processing, analytics, ongoing deliverables, and presentations.
Interact cross-functionally with a wide variety of people and teams. Work closely with subject matter experts to deliver value by developing novel, practical, scientific data-driven solutions to meet business needs.
Why is the Position Open?
Supplement additional workload on team.
Top Must-Have Skill Sets
Ability to perform data analysis including using advanced statistical and machine learning models.
Excellent communication skills.
Good understanding of software development life cycle (SDLC).
Day To Day Responsibilities
Perform data analysis in a wide range of data and analytics solutions, from descriptive to prescriptive, using advanced statistical and machine learning models.
Leverage large data sets to conduct end-to end analytics that will include data gathering, requirements specifications, processing, analytics, ongoing deliverables, and presentations.
Interact cross-functionally with a wide variety of people and teams. Work closely with subject matter experts to deliver value by developing novel, practical, scientific data-driven solutions to meet business needs.
Basic Qualifications
PhD in Statistics or in a related subject with substantial statistical knowledge OR
Master's degree in Statistics or in a related subject & 2+ years of directly related experience OR
Bachelor's degree in Statistics or in a related subject & 4+ years of directly related experience
Preferred Qualifications
Experience working with large data sets, data mining, and machine learning tools.
A good understanding of clinical data.
Experience working in a software development environment.
Experience constructing data visualizations of clinical data.
Experience with SAS programming and/or R.
Understanding of Clinical Trials.
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Independent, self-motivated, organized, able to multi-task in time-sensitive environments, and skilled in communication, facilitation, and collaboration.
Familiarity with DevOps and software best practices (i.e., version control, continuous integration, test driven development).
Excellent communication skills (written and verbal).
Red Flags
No proven ability to work independently
Poor communication and organization skills
Lack of programming skills (SAS and/or R)
(Clinical trials experience is desired but not essential)
Interview Process
Phone screening followed by in-person interview.
We invite qualified candidates to send your resume to resumes@3keyconsulting.com. If you decide that you’re not interested in pursuing this particular position, please feel free to take a look at the other positions on our website www.3keyconsulting.com/careers. You are also welcome to share this opportunity with anyone you think might be interested in applying for this role.
Regards,
3KC Talent Acquisition Team
Show more
Show less","Machine Learning, Statistical Models, Software Development Life Cycle, Clinical Trials, Data Analysis, Communication Skills, SAS Programming, R Programming, Clinical Data, Data Visualizations, DevOps, Version Control, Continuous Integration, Test Driven Development","machine learning, statistical models, software development life cycle, clinical trials, data analysis, communication skills, sas programming, r programming, clinical data, data visualizations, devops, version control, continuous integration, test driven development","clinical data, clinical trials, communication skills, continuous integration, data visualizations, dataanalytics, devops, machine learning, r programming, sas programming, software development life cycle, statistical models, test driven development, version control"
Senior Data Analyst - 15+ Years,Dice,"Ventura, CA",https://www.linkedin.com/jobs/view/senior-data-analyst-15%2B-years-at-dice-3787365195,2023-12-17,Oxnard,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, OSO Ventures Inc., is seeking the following. Apply via Dice today!
Job Title: Data Analyst (Need only 15+ Years Experience profile)
Location: Ventura, CA (93003) 3 Days Onsite and 2 days Remote
Duration/Type: Long Term Contract
Description:
Note: Excellent hands on exposure on SQL.
Integrated Justice System currently shares data across agencies using the following approaches :
Data replication from one agency's database to another agencies database writing into another agencies database allowing another agency user access to the host agency and hence enter data into the database of the host agency.
There is a high-level effort done by each agency in identifying the current set of data exchanges, but it does not identify the processes, events and the exact data elements that are being transferred.
More importantly the exact set of data elements that needs to be exchanged between agencies for each event that needs to initiate this exchange.
Responsibilities:
The goal of this analyst is to identify all such events, agency processes (in BPM swim lane models, combined with the detailed identification of the associated objects on the PowerBuilder forms) and data element(s) involved in the exchange.
Involved in Data Mapping, Data Migration, Data Masking
The other objective of this analyst is to document all the reports specifications that are needed by each agency, the workflow / process that supplies / consumes the reported data along with all the data source elements of this report.
We want individuals with Law Safety Justice background preferably, some familiarity with SQL so that they can do some backend analysis.
Individual with computer information systems degree and 5 years of experience in large applications.
Senior Data Analyst - 15+ Years
Show more
Show less","SQL, Data Mapping, Data Migration, Data Masking, BPM, PowerBuilder, Law Safety Justice, Computer Information Systems","sql, data mapping, data migration, data masking, bpm, powerbuilder, law safety justice, computer information systems","bpm, computer information systems, data mapping, data masking, data migration, law safety justice, powerbuilder, sql"
Staff Data Engineer,Recruiting from Scratch,"Santa Monica, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744389900,2023-12-17,Thousand Oaks,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, Pyspark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Kafka, Storm, Spark streaming, Dimensional data modeling, Schema design, Data warehouses, ETL, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, spark streaming, dimensional data modeling, schema design, data warehouses, etl, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data retention, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, spark streaming, sql, storm, tdd"
Lead Data Engineer / Gaming space / onsite,Motion Recruitment Partners LLC,"Santa Monica, CA",https://www.linkedin.com/jobs/view/lead-data-engineer-gaming-space-onsite-at-motion-recruitment-partners-llc-3777090219,2023-12-17,Thousand Oaks,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Motion Recruitment Partners, LLC, is seeking the following. Apply via Dice today!
Our client is in the gaming space looking for a Senior Data Engineer with 6-10 years of experience to lead a team across the Data landscape. This person ideally has experience in the gaming industry and has experience across Data Engineering, Analytics, and machine learning. Located near Santa Monica this role will be mostly onsite.
Basic Qualifications (Required Skills & Experience)
6-10 years of experience
Data Engineering experience
Data Analytics experience
Machine Learning experience
Python, R, SQL, AWS, Tableau, PowerBI
Other Qualifications & Desired Competencies
Onsite in Santa Monica / El segundo
Bonus on top of base salary
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
401(k) matching
Lead Data Engineer / Gaming space / onsite
Show more
Show less","Data Engineering, Data Analytics, Machine Learning, Python, R, SQL, AWS, Tableau, PowerBI","data engineering, data analytics, machine learning, python, r, sql, aws, tableau, powerbi","aws, data engineering, dataanalytics, machine learning, powerbi, python, r, sql, tableau"
Data Scientist,Aditi Consulting,"Thousand Oaks, CA",https://www.linkedin.com/jobs/view/data-scientist-at-aditi-consulting-3780803837,2023-12-17,Thousand Oaks,United States,Mid senior,Hybrid,"Job Title:
Data Scientist
Location: Thousand Oaks, CA (Hybrid)
Duration: 18 months Contract with potential extension
Description:
Length - 18 months, with option to extend
3 days a week onsite (required), and 2 days remote
Shift - 8am - 5pm
Must local
Position Overview:
We are seeking a Data Scientist with a proven track record in data analytics, data processing, and data-driven decision-making to join the Combination Product Operations Digital and Data Strategy team. The ideal candidate will have a strong foundation in data querying, data visualization creation, and data analytics, and a passion for turning data into actionable insights. This role is tailored for individuals who have a solid foundation in data science and possess the expertise to lead and drive data initiatives within our organization. In this position, the candidate will need experience working with customers, understanding their challenges, and providing them with solutions that meet their requirements.
Key Responsibilities:
Data Strategy and Execution:
• Retrieve and preprocess data from various sources, such as databases, APIs, and flat files.
• Develop and implement data strategies, including data acquisition, storage, and processing.
• Initiating and supporting the digitalization of experimental workflows.
• Establish and manage data pipelines to ensure data integrity and accessibility.
• Lead the execution of complex data projects, ensuring high data quality and efficiency.
Advanced Data Analysis:
• Conduct advanced exploratory data analysis to identify patterns, anomalies, and trends in the data to derive actionable insights from data.
Data Visualization and Storytelling:
• Create insightful and visually appealing data visualizations using tools like Tableau and Spotfire.
• Communicate findings to technical and non-technical stakeholders through charts, graphs, and dashboards.
Team Leadership and Collaboration:
• Collaborate with cross-functional teams and customers to understand their data requirements and provide data-driven solutions.
• Generate regular reports and presentations to communicate insights and recommendations.
• Lead cross-functional teams and mentor junior data scientists.
• Communicate to management through live dashboards and tools for data-driven insights and improvements.
Top 3 Must Have Skill Sets:
• Proficiency in advanced data querying languages, such as SQL, and with data manipulation.
• Proficiency in programming languages, such as Python or R.
• Expertise in creating dashboards and using data visualization tools, such as Tableau and Spotfire.
*Must have experience with engineering (educational background - degree)
Show more
Show less","Data Analytics, Data Processing, DataDriven DecisionMaking, Data Querying, Data Visualization, Data Analytics, Tableau, Spotfire, Data Science, Data Initiatives, Data Preprocessing, Data Pipelines, Data Integrity, Data Accessibility, Data Projects, Data Quality, Data Analysis, Exploratory Data Analysis, Pattern Identification, Anomaly Detection, Trend Analysis, Actionable Insights, Data Visualization, Data Storytelling, Charts, Graphs, Dashboards, CrossFunctional Collaboration, Data Requirements, DataDriven Solutions, Communication, Presentations, Insights, Recommendations, DataDriven Insights, SQL, Python, R, Dashboards, Data Visualization Tools, Tableau, Spotfire, Engineering Degree","data analytics, data processing, datadriven decisionmaking, data querying, data visualization, data analytics, tableau, spotfire, data science, data initiatives, data preprocessing, data pipelines, data integrity, data accessibility, data projects, data quality, data analysis, exploratory data analysis, pattern identification, anomaly detection, trend analysis, actionable insights, data visualization, data storytelling, charts, graphs, dashboards, crossfunctional collaboration, data requirements, datadriven solutions, communication, presentations, insights, recommendations, datadriven insights, sql, python, r, dashboards, data visualization tools, tableau, spotfire, engineering degree","actionable insights, anomaly detection, charts, communication, crossfunctional collaboration, dashboard, data accessibility, data initiatives, data integrity, data preprocessing, data processing, data projects, data quality, data querying, data requirements, data science, data storytelling, data visualization tools, dataanalytics, datadriven decisionmaking, datadriven insights, datadriven solutions, datapipeline, engineering degree, exploratory data analysis, graphs, insights, pattern identification, presentations, python, r, recommendations, spotfire, sql, tableau, trend analysis, visualization"
Data Scientist- Computational Mechanics on W2,Aditi Consulting,"Thousand Oaks, CA",https://www.linkedin.com/jobs/view/data-scientist-computational-mechanics-on-w2-at-aditi-consulting-3780772082,2023-12-17,Thousand Oaks,United States,Mid senior,Hybrid,"Job Title: Data Scientist- Computational Mechanics on W2
Location: Thousand Oaks, CA (Hybrid)
Duration: 18+ months Contract with potential extension
Description:
The Digital Data Scientist will support the Combination Product Operations organization by improving the way manages and utilizes data to enhance data analysis and decision making within the organization. We are seeking a highly motivated individual who will be primarily responsible for development and lifecycle management of digital modeling assets and analyzing scientific and combination product performance data. This individual will leverage in-silico and data-driven modeling to evaluate potential opportunities that enable changes in business and operation performance.
The ideal candidate enjoys tackling challenges and excels at enabling insights for decision making using data-driven and physics-based modeling.
This may include, but is not limited to, the following:
• Applying engineering principles to develop in-silico models for combination products
• Developing, enhancing, automating, and managing analytics and data-driven models
• Performing ad-hoc analysis and supporting special projects; Providing input to management for trend and failure investigation process improvements
• Demonstrating modeling and visualization approaches as part of proof-of-concept projects
• Transforming ambiguous business and technical questions into measurable and impactful projects
• Demonstrating critical and analytical thinking skills to explore new opportunities in in-silico and data-driven models for combination products.
Skills:
Experience with programming in Python, MATLAB, JMP, and/or Minitab for engineering purposes
Experience with model simulation and analysis (MS&A) techniques for structural, fluidic, and heat transfer problems using commercial software such as ANSYS, LS-Dyna, ABAQUS, COMSOL
Experience with mathematical/first principles modeling, numerical techniques, and uncertainty quantification such as Monte Carlo simulations
Familiar with utilizing GitLab for version control, code collaboration, and project management
Data analysis expertise and statistical or mechanistic modeling experience
Experience in deriving technical recommendations and specifications from the analysis of measured data
Strong communication, presentation, and technical documentation skills are a plus, as is knowledge of process controls
Understanding business needs and developing novel yet practical solutions to meet those needs
Experience with combination products and device regulatory requirements and medical device development and engineering
Preferred Traits:
• Passion for proactively identifying opportunities through creative modeling and data analysis
• Transform ambiguous business and technical questions into measurable and impactful projects
• Partner with multi-discipline digital teams (data analysts, data engineers, data scientists, and business product owners) to advance data analytics tools/features (such as predictive/ prescriptive algorithms and machine learning)
• Ability to deliver work and provide positive leadership in a fast-paced, multi-project team-oriented environment
• Intellectual curiosity with ability to learn new concepts/frameworks, algorithms and technology rapidly as needs arise
• Ability to manage multiple competing priorities simultaneously
• Ability to work in highly collaborative, cross-functional environments
Basic Qualifications:
Bachelor's degree in Engineering plus 5 years of simulation, modeling, and data analysis experience
Or
Master's degree in Science or Engineering plus 2 years of simulation, modeling, and data analysis experience
Or
Ph.D. in Science or Engineering (simulation, modeling, and data analysis)
Rupalim Dutta
Team Lead
Direct:
650-389-9548
www.aditiconsulting.com
LinkedInI
Glassdoor
One of WA state’s Best Companies to Work For
Rupalim Dutta
Team Lead
Direct: 650-389-9548
www.aditiconsulting.com
Show more
Show less","Python, MATLAB, JMP, Minitab, ANSYS, LSDyna, ABAQUS, COMSOL, GitLab, Data analysis, Statistical modeling, Mechanistic modeling, Monte Carlo simulations, Documentation, Machine learning, Predictive algorithms, Prescriptive algorithms","python, matlab, jmp, minitab, ansys, lsdyna, abaqus, comsol, gitlab, data analysis, statistical modeling, mechanistic modeling, monte carlo simulations, documentation, machine learning, predictive algorithms, prescriptive algorithms","abaqus, ansys, comsol, dataanalytics, documentation, gitlab, jmp, lsdyna, machine learning, matlab, mechanistic modeling, minitab, monte carlo simulations, predictive algorithms, prescriptive algorithms, python, statistical modeling"
Data Engineer,Whitbread,"Dunstable, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-whitbread-3769099738,2023-12-17,Milton Keynes, United Kingdom,Mid senior,Onsite,"Data Engineer, Full-Time, Permanent, Dunstable, Bedfordshire
As the new Data Engineer you'll be working as part of our rapidly growing data engineering team, who build products and services for Premier Inn.
You will support the creation and production of data products in conjunction with decision science team which includes creation and maintenance of data and CI/CD deployment pipelines, MLOps, setup of Azure data products as required for different data science prototyping scenarios, deployments to Pre-Prod and Prod and eventually accountability for supporting productionised data products.
Role: Data Engineer
Salary: £54,000pa - £60,000pa
Contract Type: Full-Time and Permanent
Location: Dunstable, Bedfordshire (LU5 5XE) and a hybrid way of working
Why You’ll Love It Here
Healthcare: Family BUPA healthcare
Discounts: Up to 60% discount on Premier Inn stays and 25% discount on our Restaurant brand
Check out all our benefits here: https://www.whitbreadcareers.com/about-us/benefits/
What You’ll Be Doing
Deliver across the entire software delivery life cycle in an agile fashion – concept, design, build, deploy, test, release through to post implementation support for new data pipelines and data science products.
Profile data, build data pipelines and orchestrate data flows.
Design, creation and testing of CI/CD pipelines for each product to be productionised.
Ensure all deliverables are well documented, re-usable, tested and conform to agreed architecture design patterns and coding standards.
Undertake analytical activities required to support the development and maintenance of systems.
Provide support on key products should there be an incident / problem related to a productionised product that requires a development fix.
Support the project/backlog delivery events; planning / user story estimation, daily stand-ups, sprint reviews/ demos & retrospectives. Support / collaborate with the data science and platform teams by taking ownership of driving forward relevant stories (updating the ticket on the Kanban board & Jira).
Passionate about building, maintaining and optimising scalable ML/data science-based data products.
What You’ll Need
2-4 years + experience in a data engineering role with responsibility for data and CI/CD pipelines.
Strong hands-on experience with of Python and/or Java to build data pipelines and orchestrate data flows.
Proven experience using R, Python or SQL.
Azure Data Services (Data Factory, Databricks, Synapse Analytics)
Good understanding of data profiling and data types such as JSON and XML
Hands-on experience with data storage systems such as Azure Blob Storage and Azure Data Lake.
Good knowledge of version control system such as Git for code check in and PR’s
Solid experience of T-SQL (stored procedures, functions)
Integration and ETL experience
Experience deploying code as part of CI/CD pipelines in Azure DevOps stack
Experience of provisioning the Azure PAAS/IAAS environments through CI/CD pipelines
Understanding of general infrastructure, networking & security best practices
Experience of using AzureML and Databricks.
Be part of our IT Team at Whitbread
Keeping up with new platforms and real-world technologies allows us to get closer to our audience than ever before. Our dynamic Data Platform Team are subject experts in  raising the bar in the profession and at the forefront of innovation. They dive headfirst into the world of billions of data points, shaping them into insights that drive our organisation forward.
We encourage your application. Apply today or contact our recruitment team for more information.
We believe that everyone is unique and there should be no barriers to entry and no limits to ambition. We are committed to being an inclusive organisation that values diversity and welcomes your application whatever your background or situation.
Under-represented groups such as women, ethnic minorities, people with disabilities & members of the LGBTQIA+ community (those who identify as lesbian, gay, bi, trans and non-binary or those who use a different LGBTQIA+ term), are strongly encouraged to build a career with us. Speak to us about workplace adjustments, part-time and flexible working. Where possible we will support this.
Job ref:
824104-1629
Advertised:
29 Nov 2023
Show more
Show less","Data Engineering, Data Pipelines, CI/CD, MLOps, Azure Data Services, Data Profiling, Data Storage, Version Control, TSQL, ETL, Azure DevOps, Azure PAAS/IAAS, AzureML, Databricks, Python, Java, R, SQL, JSON, XML, Azure Blob Storage, Azure Data Lake, Git","data engineering, data pipelines, cicd, mlops, azure data services, data profiling, data storage, version control, tsql, etl, azure devops, azure paasiaas, azureml, databricks, python, java, r, sql, json, xml, azure blob storage, azure data lake, git","azure blob storage, azure data lake, azure data services, azure devops, azure paasiaas, azureml, cicd, data engineering, data profiling, data storage, databricks, datapipeline, etl, git, java, json, mlops, python, r, sql, tsql, version control, xml"
Data Analyst,Searchability,"Bicester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-searchability-3785255614,2023-12-17,Milton Keynes, United Kingdom,Mid senior,Onsite,"Job Title: Data Analyst
Company Industry: Utilities
Location: Bicester, Oxfordshire
Salary: £30,000 - £35,000 DOE
Are you a motivated and analytically minded individual seeking a challenging role in business intelligence? Join our dynamic team at a rapidly growing energy supplier, where innovation, dedication, and excellence are at the core of what we do.
About Us
We are a fast-growing energy supplier, leveraging leading-edge technology and a customer-focused approach to redefine industry standards. Our commitment to exceptional service extends beyond our innovative solutions – it's ingrained in our culture. At our company, you'll experience a dynamic work environment that encourages growth, offers competitive perks, and fosters a friendly team spirit.
Our Commitment to You
A dynamic and evolving culture
Regularly updated benefits, structured training opportunities, and engaging social events
Support to help you be your best within our friendly team
The Role
We are looking for a Data Analyst to join our Business Intelligence Team in the Commercial function. In this role, you will collaborate with senior team members to develop key reports, offer valuable insights, and support overall commercial performance. Utilizing tools such as MySQL, Power BI, and Excel, you will engage in both regular tasks and ad-hoc analysis, gaining exposure to various operational, pricing, and reporting functions.
Your Responsibilities
Develop, enhance, and create reports using MySQL, Power BI, Excel, and R as needed
Take ownership of running regular reports, ensuring accuracy and meeting external deadlines
Conduct insightful ad-hoc analysis using Excel, MySQL, Power BI, and other tools
Foster effective communication and collaboration across different business functions
Challenge existing practices to ensure accuracy and efficiency in data-gathering and reporting
About You
We seek a driven and detail-oriented individual with at least two years of analytical experience. While experience in the energy industry is beneficial, it is not mandatory, as on-the-job training will be provided.
Requirements
Experience with Excel and at least one programming language such as SQL or R
At least two years’ experience in an analytical role
Excellent attention to detail
Naturally inquisitive mindset
Ability to interrogate data for insights
Your Benefits
Performance-based company bonus scheme
Generous annual leave and bank holidays
Monday to Friday working hours for a healthy work-life balance
Private Medical Insurance with Vitality Health, featuring exciting perks and discounts
Life Insurance coverage at four times your salary
Recharge kitchen with complimentary snacks, drinks, and lunch options
Salary sacrifice pension scheme matching contributions up to 4%
In-house learning and development team dedicated to nurturing your talent
Strong communication skills, both written and verbal
Collaborative team player with a desire to share knowledge
Right to work in the UK
What We Offer
10% annual performance-related bonus
Full training provided
Monthly team-building events and activities
TO BE CONSIDERED….
Please either apply by clicking online or emailing me directly to chelsea.hackett@searchability.com.
For further information please call me on 07719051923. By applying for this role, you give express consent for us to process & submit (subject to required skills) your application to our client in conjunction with this vacancy only.
Contact: Chelsea.hackett@searchability.com
Show more
Show less","Data Analysis, Business Intelligence, SQL, R, Python, Power BI, Excel, MySQL","data analysis, business intelligence, sql, r, python, power bi, excel, mysql","business intelligence, dataanalytics, excel, mysql, powerbi, python, r, sql"
Data Engineering Consultant,Nigel Frank International,"Northampton, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineering-consultant-at-nigel-frank-international-3739798982,2023-12-17,Milton Keynes, United Kingdom,Mid senior,Onsite,"I am working with a Microsoft Partnered consultancy who are looking for a Data Engineering Consultant to join their growing team. You will have the opportunity to work on a variety of client projects across a number of different sectors such as retail, finance and the public sector.
In this role you will utilise your experience with the latest Azure technologies to provide guidance to clients. As a hands-on consultant you use your experience as a data engineer to ensure the timely delivery on clients data driven projects.
You will be joining a people centred business, who invest heavily in its team. You will be given the opportunity to explore other areas of technologies that may be of interest to you, including data science, machine learning and AI. You will have time designated purely to training and development, with internal development schemes as well as funded Microsoft learning, training courses and certifications.
This is an exciting time to join a hugely successful and growing business who have a number of exciting roadmap of projects planned all the way into 2024!
As part of this role, you will be responsible for some of the following areas.
Offer guidance to clients on data driven projects
Take the lead on data engineering projects focused around the Microsoft and Azure tech stack
Deliver on all aspects of the project including requirements gathering, testing, implementation and maintenance
This is a salaried role paying up to £55,000 per annum depending on experience and a company benefits package. This is a home based role with occasional, fully expensed visits to client sites as and when required. You will also need to commute to company retreats once per quarter to meet other members of the team.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Coding experience with languages such as SQL and Python.
Strong knowledge of Databricks for data ingestion and transformation would be beneficial
Excellent communication skills
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure, Synapse, Data Factory, Azure Data Lake, ETL, SQL, Python, Databricks, Data Engineering","azure, synapse, data factory, azure data lake, etl, sql, python, databricks, data engineering","azure, azure data lake, data engineering, data factory, databricks, etl, python, sql, synapse"
Business Data Analyst,RTA Engineering Services,"Northamptonshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/business-data-analyst-at-rta-engineering-services-3773387677,2023-12-17,Milton Keynes, United Kingdom,Mid senior,Onsite,"Company description:Do you want to help shape the future of the city? We connect the dots of a new mobility revolution that will transform cities all over the world.We do this by empowering mobility, bringing it to the next level with intelligent road traffic technology.Job description:Complex problems, focused solutions, and direct impact on the satisfaction of millions of motorists, pedestrians, and cyclists all over the world. We are the magic behind the scenes, the team responsible for installing and maintaining traffic controllers, signals, and other traffic products. Our technology keeps our roads moving safely. We are the UK's #1 Intelligent Traffic Solutions (ITS) provider and maintain over 40% of the UK's ITS assets. The Traffic Enforcement market is booming! We're at the forefront of developing smart answers to these challenges to actively shape the mobility world for the next generation.Do you want to join a business that puts its people at the forefront of all we do, an agile, innovative business that is leading the way in traffic solutions? Come and join us at Yunex Traffic.We are seeking an experienced and results-oriented Business Analyst, based out of our office in Leicester, with a strong focus on driving business improvements within our service organisation. You will be responsible for analysing and optimizing existing processes, identifying areas of inefficiency, and recommending solutions to enhance operational service performance and increase profitability. You will collaborate closely with cross-functional teams, stakeholders, and senior management to implement strategic changes and achieve measurable business outcomes.What will you do?
Process Optimisation: Conduct thorough analysis of current business processes and workflows to identify bottlenecks, inefficiencies, and areas for improvement across the full spectrum of our support and maintenance contracts.
Continuous Improvement Initiatives: Proactively lead and participate in continuous improvement initiatives across the Enforcement organization, leveraging Lean, Six Sigma, or other relevant methodologies.
Cost Reduction Strategies: Identify cost-saving opportunities and recommend strategies to optimize resource allocation, reduce operational expenses, and increase overall profitability.
Data-Driven Insights: Utilize data analytics and modelling techniques to extract insights, identify trends, and support evidence-based decision-making for business improvement initiatives.
Performance Metrics: Utilise and present contractual and internal key performance indicators to measure the success and impact of business improvement projects and contract performance.
Innovation Initiatives: Identify opportunities for innovation and propose technology or digital solutions that streamline operations and enhance customer experiences.Who are you?
An operational or commercial background coupled with a formal business qualification.
You have excellent communication skills, with customers, external partners, and cross functionally.
Experience in driving positive change and enhancing business performance.
Confident to initiate, negotiate, and conclude difficult contractual discussions where necessary to ensure that change is managed and communicated effectively.
The ability to lead change through small projects, control tasks, resource, and cost to ensure and measure the success of sponsored initiatives.
Good knowledge of SQL and Microsoft Office tools; particularly Excel, PowerPoint, Word, Microsoft Project, Power Automate and other relevant modelling and Management Information Products to analyse and present large data sets.What do we offer?
Base salary and annual bonus
26 days holiday, increasing up to 29 days with length of service
Excellent pension, matching contributions up to 10% of pensionable salary
Flexible benefits to suit your personal needs (plenty of choice)
Investment in personal development and support to membership of professional institutionsAbout us We're Yunex Traffic. We have offices across the UK and worldwide, full of hardworking individual's helping us to challenge the today and work towards a brighter tomorrow. We're excited to hear that you would like to join us here at Yunex Traffic. Our people love it here and we want you to be a part of helping us make real, what matters. Our Commitment: Yunex Traffic is committed to promoting equality, diversity, and inclusivity. We recognise that building a diverse workforce is essential to the success of our business. We strongly encourage applications from a diverse talent pool and welcome the opportunity to discuss any requirements you may have, including workplace adjustments. How do I apply? We are looking forward to receiving your online application. Please ensure you complete all areas, of the application form, to the best of your ability as we will use this data to review your suitability for the role.
Show more
Show less","Process Optimisation, Continuous Improvement, Cost Reduction, Data Analytics, Performance Metrics, Innovation, SQL, Microsoft Office, Excel, PowerPoint, Word, Microsoft Project, Power Automate, Lean, Six Sigma","process optimisation, continuous improvement, cost reduction, data analytics, performance metrics, innovation, sql, microsoft office, excel, powerpoint, word, microsoft project, power automate, lean, six sigma","continuous improvement, cost reduction, dataanalytics, excel, innovation, lean, microsoft office, microsoft project, performance metrics, power automate, powerpoint, process optimisation, six sigma, sql, word"
Installation Supervisor (Data Cables / International Travel),Ernest Gordon Recruitment,"Northampton, England, United Kingdom",https://uk.linkedin.com/jobs/view/installation-supervisor-data-cables-international-travel-at-ernest-gordon-recruitment-3781130526,2023-12-17,Milton Keynes, United Kingdom,Mid senior,Onsite,"40,000 - 45,000 + Company Van + (47k - 52k) OTE + Autonomy
Northampton, Northamptonshire
Do you have hands on experience installing / removing Data Cables and are looking for a days based, Monday to Friday role covering the globe boasting a no two days are the same, field based role where you will have the autonomy to manage a team of installation engineers and projects?
On offer is a days based role where you will be visiting sites nationally and internationally supervising installation projects and engineers within a variety of blue chip telecommunication clients. There is plenty of optional overtime available to boost your earnings and you will be required to stay away occasionally during international install projects.
This company are renowned within the telecommunications industry for their professional approach and strong customer service skills. Since their start in 2008 this company has grown exponentially and have gained a sterling reputation for their staff retention and professionality.
This role would suit an Installation Engineer or similar with supervisory experience looking for a highly autonomous, days based role travelling the globe with plenty of overtime to boost your earnings and the opportunity to further progress into senior managerial positions.
The Role
Supervising installation projects and engineers globally
Travelling across the world with occasional staying away required
Optional overtime available paid at 1.33 on Saturday and 1.5 on Sunday
Monday to Friday, 8am - 5pm core hours, 40 hours a week
The Person
Installation Supervisor / Engineer or similar
Experience with data cables or similar
Full right to work in the UK
Reference Number: BBBH11294
Service, Engineer, Installation, Data, Cables, Mechanical, Leicester, Birmingham, Northampton, Nottingham, Supervisor, Field, Junior, Cambridge, Peterborough, Milton Keynes
If you're interested in this role, click 'apply now' to forward an up-to-date copy of your CV.
We are an equal opportunities employer and welcome applications from all suitable candidates. The salary advertised is a guideline for this position. The offered remuneration will be dependent on the extent of your experience, qualifications, and skill set.
Ernest Gordon Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workers. By applying for this job, you accept the T&C's, Privacy Policy and Disclaimers which can be found at our website.
Show more
Show less","Installation, Data Cables, Telecommunications, Supervision, Project Management, Field Service, Travel","installation, data cables, telecommunications, supervision, project management, field service, travel","data cables, field service, installation, project management, supervision, telecommunications, travel"
Rewards Data / HR Analyst,Latcom Ltd,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/rewards-data-hr-analyst-at-latcom-ltd-3786321869,2023-12-17,Milton Keynes, United Kingdom,Mid senior,Onsite,"Rewards Data / HR Analyst, required to work in Milton Keynes, this will be a Hybrid position so working 2/3 days in the office and the remainder working from home. Please note: you will be required to work in the office for the first 3/4 weeks, during training.
Team: The Reward Team are responsible for the delivery of all compensation & benefit initiatives. This includes both the cyclical business as usual reward activity such as pay review, bonus & benefits windows and also ad hoc projects.
Job Purpose
To support the delivery of the Reward strategy - Including Compensation, Benefits, HRMI & Recognition.
To support the day-to-day reward processes, such as job evaluations, queries and data.
To be a technical mind within the HR Team, offering support where appropriate.
Other Responsibilities
Build pay and bonus scenario models - Using technical knowledge to deliver Reward Business Partner & Head of Reward requirements.
Delivery of operational reward outputs, such as bonus letters, total reward statements and benefits communications.
Assist HRMI Analyst to deliver Gender & Ethnicity pay gap reporting.
Provide guidance to the wider HR team on pay and benefit related topics.
Document repeatable processes - building a knowledge bank for the reward team.
Skills Required
Previous HR or Data working environment highly desirable.
Advanced Microsoft Excel & Intermediate Word and PowerPoint
Ability to communicate complex information, adapting style to suit audience
Strong organisational skills with the ability to prioritise effectively. Self-motivated and driven, highly numerate with attention to detail.
Must have ability to work under pressure and adjust.
For the successful candidate my client offers good benefits; 27 days holiday, 2 days voluntary, Health Cover, good pension, 6% bonus and other benefits too.
Please send in your CV if you have the above skills.
Show more
Show less","Microsoft Excel, Microsoft Word, Microsoft PowerPoint, Data Analysis, HR Data, Data Management, Compensation and Benefits, HRIS (HR Management Information Systems), Gender and Ethnicity Pay Gap Reporting, Pay and Bonus Scenario Modeling, Knowledge Management","microsoft excel, microsoft word, microsoft powerpoint, data analysis, hr data, data management, compensation and benefits, hris hr management information systems, gender and ethnicity pay gap reporting, pay and bonus scenario modeling, knowledge management","compensation and benefits, data management, dataanalytics, gender and ethnicity pay gap reporting, hr data, hris hr management information systems, knowledge management, microsoft excel, microsoft powerpoint, microsoft word, pay and bonus scenario modeling"
Installation Supervisor Data Cables / International Travel,Ernest Gordon Recruitment,"Northampton, England, United Kingdom",https://uk.linkedin.com/jobs/view/installation-supervisor-data-cables-international-travel-at-ernest-gordon-recruitment-3784828122,2023-12-17,Milton Keynes, United Kingdom,Mid senior,Onsite,"Installation Supervisor (Data Cables / International Travel)
£40,000 - £45,000 + Company Van + (£47k - £52k) OTE + Autonomy
Northampton, Northamptonshire
Do you have hands on experience installing / removing Data Cables and are looking for a days based, Monday to Friday role covering the globe boasting a no two days are the same, field based role where you will have the autonomy to manage a team of installation engineers and projects?On offer is a days based role where you will be visiting sites nationally and internationally supervising installation projects and engineers within a variety of blue chip telecommunication clients. There is plenty of optional overtime available to boost your earnings and you will be required to stay away occasionally during international install projects. This company are renowned within the telecommunications industry for their professional approach and strong customer service skills. Since their start in 2008 this company has grown exponentially and have gained a sterling reputation for their staff retention and professionality.
Role
This role would suit an Installation Engineer or similar with supervisory experience looking for a highly autonomous, days based role travelling the globe with plenty of overtime to boost your earnings and the opportunity to further progress into senior managerial positions.
The Role:
Supervising installation projects and engineers globally
Travelling across the world with occasional staying away required
Optional overtime available paid at 1.33 on Saturday and 1.5 on Sunday
Monday to Friday, 8am - 5pm core hours, 40 hours a week
The Person
Installation Supervisor / Engineer or similar
Experience with data cables or similar
Full right to work in the UK
Reference Number: BBBH11294Service, Engineer, Installation, Data, Cables, Mechanical, Leicester, Birmingham, Northampton, Nottingham, Supervisor, Field, Junior, Cambridge, Peterborough, Milton KeynesIf you're interested in this role, click 'apply now' to forward an up-to-date copy of your CV.We are an equal opportunities employer and welcome applications from all suitable candidates. The salary advertised is a guideline for this position. The offered remuneration will be dependent on the extent of your experience, qualifications, and skill set.
Ernest Gordon Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workers. By applying for this job, you accept the T&C's, Privacy Policy and Disclaimers which can be found at our website.
Show more
Show less","Installation Supervisor, Data Cables, Telecommunication, Project Supervision, Team Management, Project Management, Customer Service, ProblemSolving, Time Management, International Travel, Quality Assurance, Safety Protocols, Industry Standards, Communication Skills","installation supervisor, data cables, telecommunication, project supervision, team management, project management, customer service, problemsolving, time management, international travel, quality assurance, safety protocols, industry standards, communication skills","communication skills, customer service, data cables, industry standards, installation supervisor, international travel, problemsolving, project management, project supervision, quality assurance, safety protocols, team management, telecommunication, time management"
Customer Service Representative/Data Analyst/Data Entry Clerk/Full Time,Townepaucekltd,"Luton, England, United Kingdom",https://uk.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-townepaucekltd-3742326142,2023-12-17,Milton Keynes, United Kingdom,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk/Full Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : application@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data Analysis, Data Interpretation, Data Visualization, Statistical Techniques, Data Modeling, Algorithms, A/B Testing, Data Quality, Data Collection, Data Cleansing, Data Manipulation, Reporting, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL","data analysis, data interpretation, data visualization, statistical techniques, data modeling, algorithms, ab testing, data quality, data collection, data cleansing, data manipulation, reporting, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, etl","ab testing, algorithms, data collection, data interpretation, data manipulation, data quality, dataanalytics, datacleaning, datamodeling, etl, hypothesis testing, powerbi, python, r, reporting, sql, statistical modeling, statistical techniques, tableau, visualization"
HR Systems and Data Analyst,Wickes,"Northamptonshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/hr-systems-and-data-analyst-at-wickes-3782768024,2023-12-17,Milton Keynes, United Kingdom,Mid senior,Onsite,"We are recruiting a HR Systems and Data Analyst to join our wider HR team in Northampton on a permanent basis.
This role will involve administering and configuring our different HR Systems (e.g. user set up, structure changes), act as a HR systems specialist stakeholder when dealing with escalated support tickets, or managing regular auditing processes and high volume data entry.
Key Responsibilities:
Administer & maintain governance for HR systems e.g. user set up, hierarchy changes, process user requests in line with HR governance, Supporting with data extraction and distribution.
Act as an escalation point to problem solve user issues as part of wider HR Shared Services team
Provide HR data and systems specialist knowledge to the wider business, Being solely responsible for audits across our HR Systems and maintaining data integrity.
Provide support with processes that impact the full employee life-cycle, such as new joiners.
Proactively recognise trends in support tickets and escalating system issues immediately to the HR Systems Senior Analyst. Identify system improvements to increase efficiencies.
Complete accurate Data Entry in good time, spot inconsistencies in audits and correcting/managing those as soon as they arise.
Follow the correct process for accessing and distributing HR data, liaising with the wider team on their reporting requirements.
Improve the Line Manager’s experience by identifying training needs, and maintaining/creating user guides/supporting documentation.
What are we looking for:
An understanding of the HR employee life-cycle.
Experience with Data Entry/Customer Service
Experience of working with different HR Systems (HRe ideal but not essential)
Experience of working with multiple data sources
Awareness of HR processes and practices.
Analytical and problem-solving skills
High attention to detail
Strong Organisation Skills
Experience with Pivot Tables/basic formula within Excel/Google Sheets
Excellent communication skills to effectively present and explain complex systems processes in a clear and simple way
Ability to maintain confidentiality and exercise discretion
Committed to delivering good customer service
Willing to adapt to change
Thrives in a busy environment
Maintains high quality of detail during repetitive tasks.
What can we offer you?
You’ll be supported with fantastic learning and development and have the opportunity to grow and develop your career with us
We’ll also equip you with a benefits package that includes
Annual bonus
Save-as-you-earn scheme
Contributory pension scheme
Colleague discount
Discount platform including savings and cash back at numerous retailers, savings on gym membership, cycle to work scheme
Our widely recognised wellbeing strategy is something we’re proud of at Wickes. As part of this, we offer a range of health and wellbeing benefits and support, including an Employee Assistance Programme, financial education & loans, and access to parental, menopause and fertility support.
We recognise the value of bringing our teams together to collaborate, support each other and build on our amazing culture. We are also encouraging our teams to work flexibly, with a blend of remote / office working.
About Us:
Wickes is a multi-channel retailer operating in the home improvement market. With 40 years in industry, Wickes now generates revenue in excess of £1.6Bn across 230 stores delivered by 8,000+ colleagues.
But it is the Wickes’ culture that is considered its best kept secret; it’s a collaborative, down to earth, fun and inclusive environment where people feel part of a winning team. All our colleagues come from different backgrounds, but what we all have in common is a determination to succeed and a passion for being the best we can be. If that sounds like you, we’ll make you feel right at home.
Please note: All offers of employment are subject to DBS / background checks
Vacancy Reference #
Please contact us here if you require any adjustments within the application process. If you require any reasonable adjustments at the interview stage you will have an opportunity to inform us when we invite you to interview. Please note, this link is only for reasonable adjustments required - general enquiries, or direct CV applications cannot be accepted via this form.
Show more
Show less","HR Systems, Data Entry, Data Extraction, Data Analysis, Pivot Tables, Formulas, Excel, Google Sheets, Communication, Confidentiality, Discretion, Customer Service, Adaptability, Analytical Skills, ProblemSolving Skills, Attention to Detail, Organization Skills","hr systems, data entry, data extraction, data analysis, pivot tables, formulas, excel, google sheets, communication, confidentiality, discretion, customer service, adaptability, analytical skills, problemsolving skills, attention to detail, organization skills","adaptability, analytical skills, attention to detail, communication, confidentiality, customer service, data entry, data extraction, dataanalytics, discretion, excel, formulas, google sheets, hr systems, organization skills, pivot tables, problemsolving skills"
Senior Oracle Data Analyst,Talenterprize,"Brackley, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-oracle-data-analyst-at-talenterprize-3776265328,2023-12-17,Milton Keynes, United Kingdom,Mid senior,Hybrid,"We are looking for a highly competent and motivated Data Analyst to join the Procurement Team. This role is responsible for the deep analysis of data, identifying, gathering, analysing data and provide insights to Procurement and wider business.
Key Responsibilities
Understand and interpret data from Oracle and other systems - spend, inflation, wastage, inventory, supplier information etc.
Analyse the results and identify areas to increase efficiency, identify trends and spend patterns.
Provide insight through graphs, charts, tables, reports and dashboards.
Set up and maintain automated data processes.
Build and run reports to support Procurement.
Build and maintain a dashboard for Procurement KPI’s to report to the wider business.
What We Are Looking For
Good knowledge of Oracle Supply chain, Inventory and Procurement.
Excellent hands-on experience of Excel and Power BI, with experience in Reporting.
Excellent analytical skills, strong mathematical skills and excellent problem-solving skills.
Highly organised with Great attention to detail.
Ability to interface with cross functional teams and varying levels in the business.
Excellent communication skills with confidence and good presentation skills.
Proactive in building working relationships both internally and externally
Show more
Show less","Data Analysis, Data Mining, Data Visualization, Oracle Supply Chain Management, Oracle Inventory Management, Oracle Procurement, Excel, Power BI, Reporting, Analytical Skills, ProblemSolving Skills, Mathematical Skills, Attention to Detail, Communication Skills, Presentation Skills, Relationship Building, CrossFunctional Collaboration","data analysis, data mining, data visualization, oracle supply chain management, oracle inventory management, oracle procurement, excel, power bi, reporting, analytical skills, problemsolving skills, mathematical skills, attention to detail, communication skills, presentation skills, relationship building, crossfunctional collaboration","analytical skills, attention to detail, communication skills, crossfunctional collaboration, data mining, dataanalytics, excel, mathematical skills, oracle inventory management, oracle procurement, oracle supply chain management, powerbi, presentation skills, problemsolving skills, relationship building, reporting, visualization"
Data Governance Analyst,Barclays,"Northampton, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-governance-analyst-at-barclays-3783914992,2023-12-17,Milton Keynes, United Kingdom,Mid senior,Hybrid,"Northampton
As a Barclays Data Governance Analyst, you will get an exciting opportunity to work within our Chief Data Office. In your role, you will be responsible for ensuring that group data management principles are managed and governed across the UK business. You will also work alongside various functions and different aspects of the business including operations, technology, architects, and business SME’s.
Barclays is one of the world's largest and most respected financial institutions, established in 1690, with a legacy of success, quality, and innovation. We offer careers that provide endless opportunity – helping millions of individuals and businesses thrive and creating financial and digital solutions that the world now takes for granted.
At Barclays, we offer a hybrid working experience that blends the positives of working alongside colleagues at our onsite locations, together with working from home. We have a structured approach where colleagues work at an onsite location on fixed, ‘anchor’, days of the week, for a minimum of two days a week or more, as set by the business area (or nearest equivalent if working part-time hours). Please discuss the working pattern requirements for the role you are applying for with the hiring manager. Please note that as we continue to embed our hybrid working environment, we remain in a test and learn phase, which means that working arrangements may be subject to change on reasonable notice to ensure we meet the needs of our business.
What will you be doing?
Outlining initiatives to identify data capture points across customer journeys and business processes to both understand and interpret the opportunities to improve our data
Interpreting key business data sets and assessing the criticality and quality of this data
Working with data and associated business processes to elicit data related risks, issues, pain points and ‘fit for purpose’ data quality and control requirements
Working with the Data Governance Control Office and Change Management teams to identify control weaknesses and establish a feedback loop
Creating the delivery approach and plan for executing data controls requirements and risk mitigations to improve key customer journeys and improve the control framework
Providing relevant artefacts for the Data Governing councils, working groups and other related forums
What We’re Looking For
Working and practical understanding of data analysis concepts and experience in data management capabilities including Data Lineage and Data Definitions
Understanding of Retail, Cards, or Customer domain knowledge within Financial Services or a combination of these
Working knowledge in Change Management, Process Excellence, Project Management, and functional experience in Data Analysis
Experience in shaping and driving scoping and prioritisation discussions
Skills That Will Help You In The Role
Good understanding of and or experience in other data related capabilities including Data Architecture
Working knowledge of SQL queries and SQL driven data analysis
Industry experience of working in a CDO function on Data Quality and Issues Management and Remediation
Reasonable understanding of tools and assets used within the industry to manage data quality and measures
Where will you be working?
Northampton is a key strategic hub, and home to a community of over 3,000 talented people.
You’ll find our brilliant Barclays minds in our lakeside office, built in 1996. It’s not only environmentally friendly, but also people-friendly too. Here, we encourage wellbeing – from mental health to healthy living to health awareness.
Show more
Show less","Data Governance, Data Analysis, Data Lineage, Data Definitions, Retail Domain Knowledge, Cards Domain Knowledge, Customer Domain Knowledge, Change Management, Process Excellence, Project Management, SQL, Data Architecture, Data Quality Management, Data Remediation","data governance, data analysis, data lineage, data definitions, retail domain knowledge, cards domain knowledge, customer domain knowledge, change management, process excellence, project management, sql, data architecture, data quality management, data remediation","cards domain knowledge, change management, customer domain knowledge, data architecture, data definitions, data governance, data lineage, data quality management, data remediation, dataanalytics, process excellence, project management, retail domain knowledge, sql"
Energy Data and Forecast Analyst,Energy Jobline,"Milton Keynes, England, United Kingdom",https://uk.linkedin.com/jobs/view/energy-data-and-forecast-analyst-at-energy-jobline-3772979498,2023-12-17,Milton Keynes, United Kingdom,Mid senior,Hybrid,"Trilogic Recruitment is pleased to present an exciting opportunity to join a dynamic and environmentally-conscious business. Our client is the only employee-owned net zero and carbon consultancy in the UK, dedicated to assisting organizations in achieving their net zero objectives and making a positive impact on the planet.
Whats in it for you?
This role comes with a range of benefits, including flexible working options, wellbeing perks, and an annual profit share bonus. In addition, youll have access to excellent training opportunities to advance your career. Our client places a strong emphasis on valuing employee input and encourages participation through their Employee Voice Framework, ensuring everyone has a voice in shaping the business.
Having achieved carbon neutrality in 2023, our client is committed to becoming net zero by 2030, setting an example for others in the industry. To further this mission, they are seeking individuals who share their passion for the planet to join their team.
Snapshot Of Your Role
As an Energy Industry Analyst, you will provide support to customers by conducting Cost, Revenue, and Servicing contract analysis, offering insights into their energy consumption, trends, and potential savings. You will manage their energy portfolio, ensuring it is appropriately configured and maintained.
You Will
Calculate annual budgets and in-term forecasts
Utilize forecast costs for all channels (Direct Suppliers, Landlords, and Supplies Not Billed) to produce PPU Rates for England, Scotland, Wales (ESW), and Northern Ireland (NI) annually.
Manage site changes related to tenants throughout the year
Identify and propose resolutions
Receive and validate proposals for landlord charging changes
Implement resolutions when applicable (e.g., issuing tenant billing ad hoc invoices, incorporating into future estimate and reconciliation invoices)
Validate landlord invoices and received data
Calculate customer invoices to tenants
Issue annual cost and consumption forecasts as needed
Requirements
Ideally, you are educated to Degree level in a technical, energy, or engineering discipline.
Possess knowledge of the Electricity Industry, particularly in Billing (Understanding elements affecting charges and their variation, e.g., read estimation, credit and rebilling, pass through tariff charges)
Proficient in Data Analysis (Advanced Excel and SQL for sourcing of data)
Skilled in Business Analysis (Requirement elicitation, process analysis, documentation, and implementation)
Experience with a Business Intelligence system (Power BI, Qlik Sense, Vertica)
Benefits
£40,000 – £55,000, dependent on experience, plus bonus
Employee Ownership Profit Share bonus scheme (annual)
Sales Opportunity identification incentive scheme
22 days holidays rising to 25 days plus bank holidays
Health cash plan including family cover
Employee Wellbeing Programme
Life Assurance
Stakeholder Pension
Perks at Work Shopping Discounts
Home office IT equipment provided
Hybrid Working – home and office-based role
Flexible holidays
Cycle Salary Sacrifice Scheme
Electric Vehicle Salary Sacrifice Scheme
Company-funded social events and team building
Free on-site parking
Relaxed office dress code
Show more
Show less","Energy Industry Analysis, Cost Analysis, Revenue Analysis, Servicing Contract Analysis, Energy Consumption, Energy Trends, Energy Portfolio Management, Budgeting, Forecasting, Electricity Industry, Billing, Data Analysis, Excel, SQL, Business Analysis, Requirement Elicitation, Process Analysis, Documentation, Implementation, Business Intelligence System, Power BI, Qlik Sense, Vertica","energy industry analysis, cost analysis, revenue analysis, servicing contract analysis, energy consumption, energy trends, energy portfolio management, budgeting, forecasting, electricity industry, billing, data analysis, excel, sql, business analysis, requirement elicitation, process analysis, documentation, implementation, business intelligence system, power bi, qlik sense, vertica","billing, budgeting, business analysis, business intelligence system, cost analysis, dataanalytics, documentation, electricity industry, energy consumption, energy industry analysis, energy portfolio management, energy trends, excel, forecasting, implementation, powerbi, process analysis, qlik sense, requirement elicitation, revenue analysis, servicing contract analysis, sql, vertica"
Junior Data Scientist,Fairmont Recruitment,"Manchester Area, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-scientist-at-fairmont-recruitment-3767396517,2023-12-17,Wigan, United Kingdom,Associate,Onsite,"Join the Data Revolution! Become a Junior Data Scientist!
Are you ready to transform security solutions using advanced biometrics and AI? Unlike traditional methods such as FaceID and fingerprints, we focus on individual habits, ensuring a highly secure and hack-resistant environment.
Why our client?
They are pioneers, working on a ground-breaking global project.
What we Need:
• Good Python development skills.
• Experience with or excitement to learn AI and machine learning.
• Familiarity with tools like Scikit, PyTorch, or TensorFlow.
• Strong problem-solving skills and teamwork.
• Ability to communicate your work effectively to everyone.
Why Join?
• Be part of shaping the future with a revolutionary project.
• Thrive in a fun and friendly start-up, always learning.
• Play a key role in driving the company's growth with your skills.
Ready to make a difference? Apply now and be a vital part of their dynamic team!
British Citizen or Settled Status applicants only – No Sponsorship offered.
Show more
Show less","Python, AI, Machine learning, Scikit, PyTorch, TensorFlow, Problemsolving, Teamwork, Communication","python, ai, machine learning, scikit, pytorch, tensorflow, problemsolving, teamwork, communication","ai, communication, machine learning, problemsolving, python, pytorch, scikit, teamwork, tensorflow"
