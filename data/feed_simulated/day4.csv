job_title,company,job_location,job_link,first_seen,search_city,search_country,job level,job_type,job_summary,job_skills,job_skills_raw,job_skills_cleaned
Data Engineer - Scala(U.S. remote),Railroad19,"Hialeah, FL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3783320898,2023-12-17,Highland Park,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Data engineering, Scala, Spark, AWS, EMR, S3, Relational databases, Nonrelational databases, RESTful APIs, Software architecture, Testing, Troubleshooting, Problem solving, Communication, Selfdirection, AWS","data engineering, scala, spark, aws, emr, s3, relational databases, nonrelational databases, restful apis, software architecture, testing, troubleshooting, problem solving, communication, selfdirection, aws","aws, communication, data engineering, emr, nonrelational databases, problem solving, relational databases, restful apis, s3, scala, selfdirection, software architecture, spark, testing, troubleshooting"
Data Engineer - Scala(U.S. remote),Railroad19,"Miami, FL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3783321671,2023-12-17,Highland Park,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, AWS, S3, EMR, SQL, NoSQL, Restful APIs, Version Control","scala 212, spark 24, aws, s3, emr, sql, nosql, restful apis, version control","aws, emr, nosql, restful apis, s3, scala 212, spark 24, sql, version control"
Data Engineer - Scala(U.S. remote),Railroad19,"Miramar, FL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3783322686,2023-12-17,Highland Park,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, AWS, EMR, S3, Relational databases, Nonrelational databases, Restful APIs, Apache Spark","scala 212, spark 24, aws, emr, s3, relational databases, nonrelational databases, restful apis, apache spark","apache spark, aws, emr, nonrelational databases, relational databases, restful apis, s3, scala 212, spark 24"
Data Engineer - Scala(U.S. remote),Railroad19,"Pembroke Pines, FL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3783322684,2023-12-17,Highland Park,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Apache Spark 2.4, AWS, S3, Relational and nonrelational databases, RESTful APIs, EMR clusters","scala 212, apache spark 24, aws, s3, relational and nonrelational databases, restful apis, emr clusters","apache spark 24, aws, emr clusters, relational and nonrelational databases, restful apis, s3, scala 212"
Data Engineer - Scala(U.S. remote),Railroad19,"Hollywood, FL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3783327032,2023-12-17,Highland Park,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, Restful APIs, AWS, EMR clusters, S3, Relational databases, Nonrelational databases, Apache Spark","scala 212, spark 24, restful apis, aws, emr clusters, s3, relational databases, nonrelational databases, apache spark","apache spark, aws, emr clusters, nonrelational databases, relational databases, restful apis, s3, scala 212, spark 24"
Data Engineer - Scala(U.S. remote),Railroad19,"Davie, FL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3783320918,2023-12-17,Highland Park,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark 2.4, AWS, EMR, S3, Relational databases, Nonrelational databases","scala, spark 24, aws, emr, s3, relational databases, nonrelational databases","aws, emr, nonrelational databases, relational databases, s3, scala, spark 24"
Lead Data Scientist,Alldus,"Tennessee, United States",https://www.linkedin.com/jobs/view/lead-data-scientist-at-alldus-3732426381,2023-12-17,Watertown,United States,Mid senior,Onsite,"Company Overview:
Join a dynamic and innovative healthcare-focused organization at the forefront of leveraging advanced technologies in data science. As a Senior Data Scientist specializing in Natural Language Processing (NLP) and Large Language Models (LLMs), you will play a pivotal role in building end-to-end data science projects that transform healthcare data into actionable insights, ultimately improving patient outcomes and healthcare delivery.
Key Responsibilities:
NLP and LLM Model Development:
Develop state-of-the-art NLP and LLM models to extract valuable information and patterns from electronic health records, medical records, and medical claims data.
Optimize and fine-tune models for accuracy, efficiency, and scalability, ensuring they can handle high volumes of healthcare data in production environments.
End-to-End Data Science Projects:
Lead the design, development, and deployment of end-to-end data science projects from data collection and preprocessing to model development, evaluation, and production deployment.
Collaborate with cross-functional teams to ensure seamless integration of NLP and LLM models into healthcare systems and processes.
Data Processing and Integration:
Engineer and preprocess healthcare data to ensure it is suitable for modeling, addressing challenges such as data cleaning, data augmentation, and feature engineering.
Integrate various data sources and types, including electronic health records, medical records, and medical claims data, to derive comprehensive insights.
Performance Monitoring and Optimization:
Establish monitoring systems to track model performance, detect anomalies, and measure the impact of models on healthcare outcomes.
Continuously optimize models and algorithms to improve performance, scalability, and efficiency in real-world production settings.
Collaboration and Knowledge Sharing:
Collaborate with multidisciplinary teams, including clinicians, data engineers, and product managers, to ensure the alignment of data science projects with organizational goals and objectives.
Share knowledge and insights with the team, keeping them informed about advancements in NLP, LLMs, and healthcare data analytics.
Qualifications
:
Education: Master’s or Ph.D. in a relevant field such as Data Science, Computer Science, Artificial Intelligence, or Healthcare Informatics.
Experience:
Minimum of 3 years of hands-on experience in data science, with a focus on NLP and LLMs in healthcare.
Proven track record of successfully building and deploying data science projects in healthcare settings, particularly using electronic health records and medical claims data.
Technical Skills:
Proficiency in Python and relevant data science libraries (e.g., NLTK, spaCy, transformers).
Strong understanding of NLP techniques, LLMs (e.g., BERT, GPT), and deep learning architectures.
Experience working with healthcare data, including electronic health records and medical claims data.
Healthcare Domain Knowledge:
Understanding of healthcare terminologies, healthcare systems, and healthcare data privacy and compliance standards (e.g., HIPAA).
Communication and Collaboration:
Excellent communication skills, with the ability to effectively convey complex ideas and findings to both technical and non-technical stakeholders.
Proven ability to collaborate in a team-oriented environment and work effectively across multiple departments.
Join us in revolutionizing healthcare data science with NLP and LLMs, and make a significant impact on patient care and outcomes!
Apply directly or send your resume to angelo@alldus.com!
38204
Show more
Show less","Natural Language Processing (NLP), Large Language Models (LLMs), Data Science, Machine Learning, Deep Learning, Python, NLTK, spaCy, transformers, Electronic Health Records (EHR), Medical Records, Medical Claims Data, Data Cleaning, Data Augmentation, Feature Engineering, Model Performance Monitoring, Model Optimization, Scalability, Efficiency, Collaboration, Communication, Healthcare Informatics, BERT, GPT","natural language processing nlp, large language models llms, data science, machine learning, deep learning, python, nltk, spacy, transformers, electronic health records ehr, medical records, medical claims data, data cleaning, data augmentation, feature engineering, model performance monitoring, model optimization, scalability, efficiency, collaboration, communication, healthcare informatics, bert, gpt","bert, collaboration, communication, data augmentation, data cleaning, data science, deep learning, efficiency, electronic health records ehr, feature engineering, gpt, healthcare informatics, large language models llms, machine learning, medical claims data, medical records, model optimization, model performance monitoring, natural language processing nlp, nltk, python, scalability, spacy, transformers"
Data Analyst,Viant Technology,"Irvine, CA",https://www.linkedin.com/jobs/view/data-analyst-at-viant-technology-3788856068,2023-12-17,Pendleton,United States,Associate,Hybrid,"What You’ll Do
Come help us build Viant’s industry leading advanced measurement capabilities. Help marketers understand the impact of their digital media spend through comprehensive, yet simple to read, dashboards. Viant’s Advanced Reporting suite of reports is all about bridging the gap between back-end technical data know-how & business application. Be a part of the process end-to-end; from concept, to scoping, developing, and release of next generation closed-loop measurement offerings.
THE DAY-TO-DAY
Work in synergy with the Product Management team to create automated & ad-hoc reporting to support pre, mid and post-campaign deliverables
Write complex SQL queries to build procedures, functions, and workflows to support application development, reporting and data extraction processes
Keen attention to detail with ability to identify, explain, and troubleshoot data discrepancies
Design, develop, test, debug, and deploy applications and reports using various technologies
GREAT TO HAVE
Professional SQL/GBQ experience with ability to create and optimize queries
Digital advertising, programmatic, or analytics experience
Strong analytical and logical thinking skills with ability to identify trends and draw conclusions based on data
Who We Are
Viant ® (NASDAQ: DSP) is a leading advertising software company that enables marketers to plan, execute and measure omnichannel ad campaigns through a cloud-based platform. Viant’s self-service Demand Side Platform, Adelphic®, powers programmatic advertising across Connected TV, Linear TV, mobile, desktop, audio, gaming and digital out-of-home channels. In 2022, Viant was recognized as a Leader in the DSP category , earned Great Place to Work® certification and Co-Founders Tim and Chris Vanderhook were named EY Entrepreneurs of the Year. To learn more, please visit viantinc.com .
LIFE AT VIANT
Investing in our employee’s professional growth is important to us, but so is investing in their well-being. That’s why Viant was voted one of the best places to work and some of our favorite employee benefits include fully paid health insurance, paid parental leave and unlimited PTO and more.
$57,000 - $70,000 a year
In accordance with California law, the range provided is Viant’s reasonable estimate of the compensation for this role. Final title and compensation for the position will be based on several factors including work experience and education.
Not the right position for you? Check out our other opportunities!
Viant Careers
About Viant
Viant® (NASDAQ: DSP) is a leading people-based advertising technology company that enables marketers to plan, execute and measure omnichannel ad campaigns through a cloud-based platform. Viant’s self-service demand side platform (“DSP”) powers programmatic advertising across Connected TV, Linear TV, mobile, desktop, audio, gaming and digital out-of-home channels. As an organization committed to sustainability, Viant’s Adricity® carbon reduction program helps clients achieve their sustainability goals. In 2023, Viant was recognized by G2 as a Leader in the DSP category and as the Best Software in Marketing & Advertising, earned Great Place to Work® certification, and became a founding member of Ad Net Zero. Viant’s Co-Founders Tim and Chris Vanderhook were also recently named EY Entrepreneurs of the Year.
Based in Irvine, CA, Viant has more than 334 employees in 10 offices around the U.S. To learn more, please visit viantinc.com .
Viant is an equal opportunity employer and makes employment decisions on the basis of merit. Viant prohibits unlawful discrimination against employees or applicants based on race (including traits historically associated with race, such as hair texture and protective hairstyles), religion, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, reproductive health decision making, gender, gender identity, gender expression, age, military status, veteran status, uniformed service member status, sexual orientation, transgender identity, citizenship status, pregnancy, or any other consideration made unlawful by federal, state, or local laws. Viant also prohibits unlawful discrimination based on the perception that anyone has any of those characteristics, or is associated with a person who has or is perceived as having any of those characteristics.
By clicking “Apply for this Job” and providing any information, I accept the Viant California Personnel Privacy Notice.
Show more
Show less","SQL, GBQ, Analytical thinking, Data visualization, Dashboard creation, Programmatic advertising, Digital advertising, Advertising measurement, Data analysis, SQL queries, Report design, Report development, Report deployment, Data troubleshooting, Data validation, Closedloop measurement, Data extraction, Data processing, Report automation, Adhoc reporting, Application development, Data integration, Data modeling, Data mining, Data presentation, Data quality assurance","sql, gbq, analytical thinking, data visualization, dashboard creation, programmatic advertising, digital advertising, advertising measurement, data analysis, sql queries, report design, report development, report deployment, data troubleshooting, data validation, closedloop measurement, data extraction, data processing, report automation, adhoc reporting, application development, data integration, data modeling, data mining, data presentation, data quality assurance","adhoc reporting, advertising measurement, analytical thinking, application development, closedloop measurement, dashboard creation, data extraction, data integration, data mining, data presentation, data processing, data quality assurance, data troubleshooting, data validation, dataanalytics, datamodeling, digital advertising, gbq, programmatic advertising, report automation, report deployment, report design, report development, sql, sql queries, visualization"
Sr Data Analyst,Cushman & Wakefield,"Tampa, FL",https://www.linkedin.com/jobs/view/sr-data-analyst-at-cushman-wakefield-3779634249,2023-12-17,Largo,United States,Mid senior,Onsite,"Job Title
Sr Data Analyst
Job Description Summary
The role is for a Senior Data Analyst as part of a team collecting and analyzing data supporting ad hoc and strategic client projects primarily involving architectural building design.
The candidate will provide advanced expertise in data analysis, collaborate with key client partners, support the data team with identifying project requirements, refining project work, and providing recommendations on optimizing data management and workflows.
Job Description
Core Responsibilities
Collect, clean, study, transform, load, and visualize data for ad hoc and strategic projects
Identify trends and provide insights from data that contribute to solving business problems
Code programs, as needed, to help capture and organize relevant data
Collaborate directly with internal and external partners to satisfy project needs
Clearly communicate useful information to business partners derived from data analysis
Assist in managing completion of team data analysis tasks
Lead problem-solving and refinement of project activities and tasks
Lead project identifying requirements from analysis of current state versus desired future state
Identify opportunities for workflow optimization
Qualifications
Three or more years of experience in data analytics, data management, or related roles
Advanced knowledge of data analytics, cleaning, preparation, and visualization techniques
Advanced experience with data analytics tools and programs (Microsoft Excel, Microsoft Power BI, Python, SQL, Tableau)
Advanced understanding of best practices in data management and visualization
Strong critical thinking and problem solving skills
Strong focus on solutions serving client/end user
Ability to write and speak clearly to both technical and non-technical audiences
Keen attention to both technical detail and quality of work acceptable to client/end user
Demonstrated ability to collaborate effectively with partners across multiple teams
Strong ability to prioritize work tasks in alignment with changing project and team needs
Preferred candidate will have experience managing/analyzing architectural design data
Cushman & Wakefield provides equal employment opportunity. Discrimination of any type will not be tolerated. Cushman & Wakefield is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veteran status or any other characteristic protected by state, federal, or local law.
In compliance with the Americans with Disabilities Act Amendments Act (ADAAA), if you have a disability and would like to request an accommodation in order to apply for a position at Cushman & Wakefield, please call the ADA line at
1-888-365-5406
or email
HRServices@cushwake.com
. Please refer to the job title and job location when you contact us.
Show more
Show less","Data Analytics, Data Management, Data Visualization, Microsoft Excel, Microsoft Power BI, Python, SQL, Tableau, Data Cleaning, Data Preparation, Database Management, Critical Thinking, Problem Solving, Communication, Teamwork, Project Management, Architectural Design","data analytics, data management, data visualization, microsoft excel, microsoft power bi, python, sql, tableau, data cleaning, data preparation, database management, critical thinking, problem solving, communication, teamwork, project management, architectural design","architectural design, communication, critical thinking, data cleaning, data management, data preparation, dataanalytics, database management, microsoft excel, microsoft power bi, problem solving, project management, python, sql, tableau, teamwork, visualization"
Data Scientist,BST Global,"Tampa, FL",https://www.linkedin.com/jobs/view/data-scientist-at-bst-global-3770676645,2023-12-17,Largo,United States,Mid senior,Hybrid,"Candidates must be authorized to work in the US without sponsorship.
Summary of Duties & Responsibilities
Data Scientist is an exceptional critical thinker that is adept at solving complex problems and creating elegant business solutions. You will be involved in the design and development efforts for our big data solutions including data lake, Business Intelligence Solutions, Machine Learning, Data Pipeline, and cloud-based data warehouse products.
You must have direct experience in the design and implementation in areas such as machine learning, artificial intelligence, operational research, or statistical methods.
Essential Functions
Demonstrated experience in machine learning techniques, probabilistic reasoning, data science, and/or optimization
Proven ability in creating explainable models and implementing advanced algorithms into production
Experience in implementing supervised and unsupervised machine learning techniques, analysis of variance (ANOVA) and statistical significance test
Demonstrated programming experience in Python, R, Keras, TensorFlow with .NET integration
Acts as a key contributor to all phases of the design and development lifecycle of analytic applications utilizing various technology platforms
Experience developing/implementing analytic solutions in the Amazon or Azure cloud that leverage relational, in-memory, NoSQL, document and/or graph databases
Strong analytical skills, able to translate complex business requirements into sound architectural solutions
Experience designing and implementing data pipeline for analytical model consumption of structured, semi-structured, and/or unstructured data in batch and real-time environments
Experience with APIs, Web Services
Good understanding of dimensional data modeling, data transformation & designing analytical data structures
Good SQL skills, broad exposure to all language constructs
Ability to work in a fast-paced, collaborative team environment
Data Integration / ETL / ELT tools
Excellent written and verbal communication skills and ability to express ideas clearly and concisely
Performs other related duties as directed
Skills & Competencies
5+ years of hand on experience working as a Data Scientist
Power BI with ML integration
Working in Azure environments including Azure ML
Familiarity with DevOps and CI/CD as well as Agile tools and processes including Git, and Azure DevOps
Education or Prior Work Experience
Minimum of a bachelor's degree in computer science, Applied Mathematics or related technical field
Reports to
Director, Software Engineering
Number Supervised
0
Travel
Up to 0%
Classification
Exempt
Work Environment
This job operates in a professional office environment. This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets and fax machines.
Physical Demands
While performing the duties of this job, the employee is regularly required to talk or hear. The employee frequently is required to stand; walk; use hands to finger, handle or feel; and reach with hands and arms.
Show more
Show less","Data Science, Machine Learning, Artificial Intelligence, Optimization, Explainable Models, Advanced Algorithms, Supervised Learning, Unsupervised Learning, Analysis of Variance (ANOVA), Statistical Significance Testing, Python, R, Keras, TensorFlow, .NET Integration, Relational Databases, Inmemory Databases, NoSQL Databases, Document Databases, Graph Databases, Dimensional Data Modeling, Data Transformation, Analytical Data Structures, SQL, APIs, Web Services, Data Integration, ETL, ELT Tools, Power BI, Azure ML, DevOps, CI/CD, Git, Azure DevOps, Agile, Computer Science, Applied Mathematics","data science, machine learning, artificial intelligence, optimization, explainable models, advanced algorithms, supervised learning, unsupervised learning, analysis of variance anova, statistical significance testing, python, r, keras, tensorflow, net integration, relational databases, inmemory databases, nosql databases, document databases, graph databases, dimensional data modeling, data transformation, analytical data structures, sql, apis, web services, data integration, etl, elt tools, power bi, azure ml, devops, cicd, git, azure devops, agile, computer science, applied mathematics","advanced algorithms, agile, analysis of variance anova, analytical data structures, apis, applied mathematics, artificial intelligence, azure devops, azure ml, cicd, computer science, data integration, data science, data transformation, devops, dimensional data modeling, document databases, elt tools, etl, explainable models, git, graph databases, inmemory databases, keras, machine learning, net integration, nosql databases, optimization, powerbi, python, r, relational databases, sql, statistical significance testing, supervised learning, tensorflow, unsupervised learning, web services"
Data Scientist (Active TS Clearance),Motion Recruitment,"Tampa, FL",https://www.linkedin.com/jobs/view/data-scientist-active-ts-clearance-at-motion-recruitment-3737592391,2023-12-17,Largo,United States,Mid senior,Hybrid,"We are working with a Federal consulting company that is transforming the nations defense and national security with their AI platforms. This company works with various Federal agencies and will need this person to work on-site 2 to 3 days a week.
Candidates should hold at least an active Top Secret clearance.
Requirements
Over 3 years of experience in data science
Expertise in Python
Experience with MySQL and Oracle
Experience with machine learning
Bonus to have PhD in Sciences, Mathematics, or Engineering
Offer
Competitive salary
Annual Bonus
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
Paid Time Off (PTO)
401(k)
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Sean Thompson
Show more
Show less","Data science, Python, MySQL, Oracle, Machine learning, Data science","data science, python, mysql, oracle, machine learning, data science","data science, machine learning, mysql, oracle, python"
Sr. Data Engineer,Mondelēz International,"East Hanover, NJ",https://www.linkedin.com/jobs/view/sr-data-engineer-at-mondel%C4%93z-international-3755542556,2023-12-17,Little Falls,United States,Mid senior,Onsite,"Job Description
Are You Ready to Make It Happen at Mondelēz International?
Join our Mission to Lead the Future of Snacking. Make It With Pride.
Together with analytics team leaders you will support our business with excellent data models to uncover trends that can drive long-term business results.
How You Will Contribute
Looking for a savvy Sr Data Engineer to join team of Modeling / Architect experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.
The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
The Data Engineer will support architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.
They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
Experience
8+ years of overall industry experience and minimum of 6-7 years of experience in building and deploying large scale data processing pipelines in a production environment using GCP.
Qualification
Science or Engineering Degree with master’s in computer science (desirable)
A master’s in computer science, Computer, Electrical Engineering, or a related field.
Self-reflective, has a hunger to improve, has a keen interest to drive their own learning. Applies theoretical knowledge to practice
Ability to work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues
Ability to think creatively, plan and prioritize work in a fast-paced environment.
Experience working with Google Cloud Platform (Big Query, GCS, Cloud Function, Composer etc.)
GCP Professional Data Engineer certification (desirable)
Having Experience of business domains like FMCG/ CPG /Retail /Supply Chain will be added advantage.
What extra ingredients you will bring:
Data engineering Concepts: Experience in working with data lake, data warehouse, data mart and Implemented ETL/ELT and SCD concepts.
ETL or Data integration tool: Experience in Talend is highly desirable.
Analytics: Fluent with SQL, PL/SQL and have used analytics tools like Big Query for data analytics
Cloud experience: Experienced in GCP services like cloud function, cloud run, data flow, data proc and big query.
Data sources: Experience of working with structure data sources like SAP, BW, Flat Files, RDBMS etc. and semi structured data sources like PDF, JSON, XML etc.
Programming: Understanding of OOPs concepts and hands-on experience with Python/Java for programming and scripting.
Data Processing: Experience in working with any of the Data Processing Platforms like Dataflow, Databricks.
Orchestration: Experience in orchestrating/scheduling data pipelines using any of the tools like Airflow and Alteryx
Nice to have
Visualization: Exposure of reporting tools like Tableau, Looker and PowerBI.
API: Experience in implementing solutions involving REST APIs, Graph QL etc.
DevOps: Worked with framework involving CI-CD pipelines, like Jenkins and Git setup. Should have understanding about Identity and Access Management (IAM) from cloud standpoint.
Data Quality: Exposure of all the phases of testing, data validation and data reconciliation.
Have Built analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.
No Relocation support available
Business Unit Summary
The United States is the largest market in the Mondelēz International family with a significant employee and manufacturing footprint. Here, we produce our well-loved household favorites to provide our consumers with the right snack, at the right moment, made the right way. We have corporate offices, sales, manufacturing and distribution locations throughout the U.S. to ensure our iconic brands—including
Oreo
and
Chips Ahoy!
cookies,
Ritz
,
Wheat Thins
and
Triscuit
crackers, and
Swedish Fish
and
Sour Patch
Kids
confectionery products —are close at hand for our consumers across the country.
Mondelēz Global LLC is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected Veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Applicants who require accommodation to participate in the job application process may contact 847-943-5460 for assistance.
Job Type
Regular
Analytics & Modelling
Analytics & Data Science
Show more
Show less","Data Engineering, Data Pipeline Architecture, Data Science, Data Analytics, Machine Learning, Data Modeling, Data Warehousing, Data Integration, Data Lake, Data Mart, ETL, ELT, SCD, Talend, SQL, PL/SQL, Big Query, GCP, Cloud Function, Cloud Run, Data Flow, Data Proc, SAP, BW, Flat Files, RDBMS, PDF, JSON, XML, Python, Java, OOPs, Dataflow, Databricks, Airflow, Alteryx, Tableau, Looker, PowerBI, REST APIs, Graph QL, Jenkins, Git, Identity and Access Management (IAM), Data Quality, DevOps","data engineering, data pipeline architecture, data science, data analytics, machine learning, data modeling, data warehousing, data integration, data lake, data mart, etl, elt, scd, talend, sql, plsql, big query, gcp, cloud function, cloud run, data flow, data proc, sap, bw, flat files, rdbms, pdf, json, xml, python, java, oops, dataflow, databricks, airflow, alteryx, tableau, looker, powerbi, rest apis, graph ql, jenkins, git, identity and access management iam, data quality, devops","airflow, alteryx, big query, bw, cloud function, cloud run, data engineering, data flow, data integration, data lake, data mart, data pipeline architecture, data proc, data quality, data science, dataanalytics, databricks, dataflow, datamodeling, datawarehouse, devops, elt, etl, flat files, gcp, git, graph ql, identity and access management iam, java, jenkins, json, looker, machine learning, oops, pdf, plsql, powerbi, python, rdbms, rest apis, sap, scd, sql, tableau, talend, xml"
Business Intelligence 340B - Data Analyst II,MUSC College of Health Professions,"Charleston, SC",https://www.linkedin.com/jobs/view/business-intelligence-340b-data-analyst-ii-at-musc-college-of-health-professions-3744184418,2023-12-17,Charleston,United States,Mid senior,Hybrid,"Job Description Summary
The Business Intelligence 340B Data Analyst conducts complex analyses and reporting solutions to support the 340B Program decision making. Leveraging data from different platforms, the reporting analyst is responsible for normalizing, aggregating, and reporting pharmacy, financial, and prescribing data to support the 340B team and other stakeholders throughout the organization.
Entity
Medical University Hospital Authority (MUHA)
Worker Type
Employee
Worker Sub-Type
Regular
Cost Center
CC005026 CHS - 340B Oversight
Pay Rate Type
Salary
Pay Grade
Health-27
Scheduled Weekly Hours
40
Work Shift
Job Description
Compile, standardize, aggregate, and analyze data to develop strategies for process enhancements and program modifications.
Develop and build predictive models and databases as needed for program maintenance and analysis.
Process, clean and verify the integrity of the data used for analysis.
Identify and analyze risk as it pertains to program compliance and financial trends.
Analyze monthly, quarterly and annual trends to identify and investigate irregularities or variances in prescribing, compliance, financial and other identified metrics related to the 340B Program.
Validate 340B prices for related programs and internal processes.
Improve and automate current manual processes to increase accuracy, productivity and reduce errors for 340B related processes when needed.
Develop and maintain technical processes for management and optimization of the 340B program.
Performs detailed project gap analyses between current and future state of 340B pharmacy systems and provides input to shape project plans that meet future state goals.
Create and distribute “scorecards” in key areas to member hospitals and system leadership, focusing on key areas of compliance and financial performance.
Assist and assess data submission request for 340B related audits.
Create and maintain related standard operating procedures.
Apply analytical processes to evaluate large 340B data sets to provide actionable insights and drive decision making for senior executives.
Maintain and report monthly program dashboards for key metrics such as 340B prescription capture, financial savings, reconciliation, and contract pharmacy utilization.
Convert data analysis results into clear and compelling narratives to inform and educate other departments including finance.
Assist with quarterly 340B compliance risk assessments for executive management and board committees to identify areas of potential exposure.
Keep abreast of 340B landscape to better contextualize 340B data and understand organization implications.
Represent the hospital at internal and external professional meetings
Respond to departmental analytical requests as it relates to pharmacy data and reporting solutions.
Collaborate with other departments to support cross-organizational projects and assist with quality improvement.
Additional Job Description
Epic Certification in Cogito, Clarity, and Caboodle Required
Bachelor’s degree in related field and / or high competency level and significant experience with pharmacy information systems or 340B compliance / operations work
Minimum of 3+ years applicable work experience with managing large data sets
Advanced knowledge and use of Excel
Advanced knowledge and demonstrated skill in building, maintaining, and querying large databases in SQL from disparate data sources.
Advanced skills in data aggregation, cleaning, and analysis. Demonstrated ability to create, maintain, and query large data sets to create concise executive management reports and guidance.
Excellent organizational skills and demonstrated ability to work independently.
Must possess strong communication and written skills.
Ability to create and maintain relationships with multiple business partners and stakeholders.
If you like working with energetic enthusiastic individuals, you will enjoy your career with us!
The Medical University of South Carolina is an Equal Opportunity Employer. MUSC does not discriminate on the basis of race, color, religion or belief, age, sex, national origin, gender identity, sexual orientation, disability, protected veteran status, family or parental status, or any other status protected by state laws and/or federal regulations. All qualified applicants are encouraged to apply and will receive consideration for employment based upon applicable qualifications, merit and business need.
Medical University of South Carolina participates in the federal E-Verify program to confirm the identity and employment authorization of all newly hired employees. For further information about the E-Verify program, please click here: http://www.uscis.gov/e-verify/employees
Show more
Show less","Data Analysis, Predictive Modeling, Data Cleaning, Risk Analysis, Trend Analysis, SQL, Excel, Data Aggregation, Database Management, Data Visualization, Reporting, Epic Cogito, Epic Clarity, Epic Caboodle","data analysis, predictive modeling, data cleaning, risk analysis, trend analysis, sql, excel, data aggregation, database management, data visualization, reporting, epic cogito, epic clarity, epic caboodle","data aggregation, data cleaning, dataanalytics, database management, epic caboodle, epic clarity, epic cogito, excel, predictive modeling, reporting, risk analysis, sql, trend analysis, visualization"
Business Intelligence 340B - Data Analyst II,MUSC Health,"Charleston, South Carolina Metropolitan Area",https://www.linkedin.com/jobs/view/business-intelligence-340b-data-analyst-ii-at-musc-health-3676601336,2023-12-17,Charleston,United States,Mid senior,Hybrid,"Job Description Summary
The Business Intelligence 340B Data Analyst conducts complex analyses and reporting solutions to support the 340B Program decision making. Leveraging data from different platforms, the reporting analyst is responsible for normalizing, aggregating, and reporting pharmacy, financial, and prescribing data to support the 340B team and other stakeholders throughout the organization.
Entity
Medical University Hospital Authority (MUHA)
Worker Type
Employee
Worker Sub-Type
Regular
Cost Center
CC005026 CHS - 340B Oversight
Pay Rate Type
Salary
Pay Grade
Health-27
Scheduled Weekly Hours
40
Work Shift
Job Description
Compile, standardize, aggregate, and analyze data to develop strategies for process enhancements and program modifications.
Develop and build predictive models and databases as needed for program maintenance and analysis.
Process, clean and verify the integrity of the data used for analysis.
Identify and analyze risk as it pertains to program compliance and financial trends.
Analyze monthly, quarterly and annual trends to identify and investigate irregularities or variances in prescribing, compliance, financial and other identified metrics related to the 340B Program.
Validate 340B prices for related programs and internal processes.
Improve and automate current manual processes to increase accuracy, productivity and reduce errors for 340B related processes when needed.
Develop and maintain technical processes for management and optimization of the 340B program.
Performs detailed project gap analyses between current and future state of 340B pharmacy systems and provides input to shape project plans that meet future state goals.
Create and distribute “scorecards” in key areas to member hospitals and system leadership, focusing on key areas of compliance and financial performance.
Assist and assess data submission request for 340B related audits.
Create and maintain related standard operating procedures.
Apply analytical processes to evaluate large 340B data sets to provide actionable insights and drive decision making for senior executives.
Maintain and report monthly program dashboards for key metrics such as 340B prescription capture, financial savings, reconciliation, and contract pharmacy utilization.
Convert data analysis results into clear and compelling narratives to inform and educate other departments including finance.
Assist with quarterly 340B compliance risk assessments for executive management and board committees to identify areas of potential exposure.
Keep abreast of 340B landscape to better contextualize 340B data and understand organization implications.
Represent the hospital at internal and external professional meetings
Respond to departmental analytical requests as it relates to pharmacy data and reporting solutions.
Collaborate with other departments to support cross-organizational projects and assist with quality improvement.
Additional Job Description
Epic Certification in Cogito, Clarity, and Caboodle Required
Bachelor’s degree in related field and / or high competency level and significant experience with pharmacy information systems or 340B compliance / operations work
Minimum of 3+ years applicable work experience with managing large data sets
Advanced knowledge and use of Excel
Advanced knowledge and demonstrated skill in building, maintaining, and querying large databases in SQL from disparate data sources.
Advanced skills in data aggregation, cleaning, and analysis. Demonstrated ability to create, maintain, and query large data sets to create concise executive management reports and guidance.
Excellent organizational skills and demonstrated ability to work independently.
Must possess strong communication and written skills.
Ability to create and maintain relationships with multiple business partners and stakeholders.
If you like working with energetic enthusiastic individuals, you will enjoy your career with us!
The Medical University of South Carolina is an Equal Opportunity Employer. MUSC does not discriminate on the basis of race, color, religion or belief, age, sex, national origin, gender identity, sexual orientation, disability, protected veteran status, family or parental status, or any other status protected by state laws and/or federal regulations. All qualified applicants are encouraged to apply and will receive consideration for employment based upon applicable qualifications, merit and business need.
Medical University of South Carolina participates in the federal E-Verify program to confirm the identity and employment authorization of all newly hired employees. For further information about the E-Verify program, please click here: http://www.uscis.gov/e-verify/employees
Show more
Show less","Data Analysis, Reporting, Data Warehousing, Data Cleaning, Data Aggregation, Risk Analysis, Data Visualization, SQL, Data Warehousing, Data Mining, Data Exploration, Python, Predictive Modeling, Advanced Excel, Data Integration, Data Standardization, Healthcare, Business Intelligence, Cogito, Clarity, Caboodle, 340B Program, HIPAA, FDA","data analysis, reporting, data warehousing, data cleaning, data aggregation, risk analysis, data visualization, sql, data warehousing, data mining, data exploration, python, predictive modeling, advanced excel, data integration, data standardization, healthcare, business intelligence, cogito, clarity, caboodle, 340b program, hipaa, fda","340b program, advanced excel, business intelligence, caboodle, clarity, cogito, data aggregation, data cleaning, data exploration, data integration, data mining, data standardization, dataanalytics, datawarehouse, fda, healthcare, hipaa, predictive modeling, python, reporting, risk analysis, sql, visualization"
Senior Data Engineer,Adecco,"Lincoln, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-adecco-3767378265,2023-12-17,Lincoln, United Kingdom,Mid senior,Onsite,"We are currently seeking to recruit for the role of Senior Data Engineer to work for Lincolnshire Police at their headquarters in Nettleham.
The role will be hybrid working.
PLEASE NOTE DUE TO POLICE VETTING CRITERIA YOU MUST HAVE RESIDED WITHIN THE UK CONTINUOUSLY FOR AT LEAST 5 YEARS AT THE TIME OF APPLICATION. UNFORTUNATELY ANYTHING LESS THAN THIS WILL NOT BE CONSIDERED.
This is a new and exciting role, pivotal to shaping our future ambitions and delivering our priorities. Whereby the successful candidate will support the Data Architect in the design and implementation of a new data platform and data flows to connect operational systems and data for analytics to deliver business intelligence and insight.
Reporting to the Data Architect, this is a fantastic opportunity for a highly motivated and talented individual to co-ordinate with teams across the Force setting best practice and standards, identifying opportunities to reuse existing data flows, optimise the code to ensure processes perform optimally, be responsible for the build of data-streaming systems and champion data engineering across the Force to ensure that the Data platform delivers on the Forces strategic and operational aims and objectives.
As a trusted, values-led member of the team you will constantly seek to engage with people to cultivate strong and collaborative working relationships with colleagues, including external partners. You will be a passionate advocate for the user experience and will work collaboratively with our digital technology users in the co-design and development of our services to ensure their experience is always positive.
Because we work in a climate of constant change and uncertainty, we are looking for an experienced, independently minded, and creative individual with a 'can do' style, whose focus is innovation and adaptability.
You can be assured that this role will prove to be highly rewarding and give the opportunity to demonstrate the force values at all times. All staff involved in carrying out functions in this role will do so in accordance with the principles of the Code of Ethics. The aim of the Code of Ethics is to support each member of the policing profession to deliver the highest professional standards in their service to the public.
Shift pattern
The standard shift pattern for the role is shown, however flexible working patterns will be considered on a full and part-time basis. Applications will be considered on an individual basis and all requests must meet the organisational need as well as individual need, which can be determined with the successful applicant.
Monday
Tuesday
Wednesday
Thursday
Friday
Saturday
Sunday
Week 1
0830-1630
0830-1630
0830-1630
0830-1630
0830-1600
RD
Adecco acts as an employment agency for permanent recruitment and an employment business for the supply of temporary workers. The Adecco Group UK & Ireland is an Equal Opportunities Employer.
By applying for this role your details will be submitted to Adecco. Our Candidate Privacy Information Statement explains how we will use your information - please copy and paste the following link in to your browser https://www.adecco.co.uk/candidate-privacy
Show more
Show less","Data Engineering, Data Platform, Data Architecture, Data Streaming, Business Intelligence, Code Optimization, Data Flows, Data Analytics, Data Warehousing, Cloud Computing, Big Data, Agile, Python, SQL, Linux","data engineering, data platform, data architecture, data streaming, business intelligence, code optimization, data flows, data analytics, data warehousing, cloud computing, big data, agile, python, sql, linux","agile, big data, business intelligence, cloud computing, code optimization, data architecture, data engineering, data flows, data platform, data streaming, dataanalytics, datawarehouse, linux, python, sql"
Senior Data Engineer,Adecco,"Lincoln, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-adecco-3770053396,2023-12-17,Lincoln, United Kingdom,Mid senior,Onsite,"Are you an experienced Data Engineer with a passion for transforming data into valuable insights? We are currently seeking a skilled Senior Data Engineer to join our client's team on a temporary basis. This is a fantastic opportunity to showcase your expertise in managing and optimising large-scale data sets to drive business decisions.
Contract Type: 6 months initially
Pay Rate: Up to £36.28 per hour
Hours of Availability: Full-time
Key Responsibilities
Design, build, and maintain large-scale data processing systems, including data lakes and data warehouses.
Collaborate with cross-functional teams to define and implement data models and data pipelines.
Develop and optimise SQL queries and ETL processes to retrieve and transform data.
Identify and resolve data quality and performance issues.
Implement data governance and security measures to safeguard sensitive information.
Stay up-to-date with industry trends and emerging technologies to drive continuous improvement.
Required Skills And Qualifications
Bachelor's degree in Computer Science, Data Science, or a related field.
Minimum of 5 years of proven experience as a Data Engineer or similar role.
Strong proficiency in SQL and relational database systems (e.g., MySQL, PostgreSQL).
Expertise in building ETL pipelines using tools like Apache Spark, Apache Beam, or similar frameworks.
Solid understanding of data warehousing and data modelling concepts.
Proficient in programming languages such as Python, Scala, or Java.
Familiarity with Cloud platforms (e.g., AWS, GCP, Azure) and related big data technologies (e.g., Redshift, BigQuery).
Experience with data visualisation tools, such as Tableau or Power BI, is a plus.
Excellent problem-solving and analytical skills.
Ability to work independently and collaboratively in a fast-paced environment.
Strong communication and interpersonal skills to effectively collaborate with stakeholders.
Location: Lincolnshire
Nearest Cities (approximate Distance)
Mansfield: 30 miles
Doncaster: 32 miles
Kingston upon Hull: 34 miles
If you are a driven and skilled Data Engineer looking for a temporary opportunity to make a significant impact, then we want to hear from you. Take the next step in your career by applying today!
To apply for this position, please send your updated CV.
Please note that only successful applicants will be contacted for an interview.
Show more
Show less","Data Engineering, Data Warehousing, Data Modelling, SQL, NoSQL, Apache Spark, Apache Beam, Python, Scala, Java, AWS, GCP, Azure, Redshift, BigQuery, Tableau, Power BI, Data Visualization, Problem Solving, Analytical Skills, Cloud Platforms, ETL Pipelines","data engineering, data warehousing, data modelling, sql, nosql, apache spark, apache beam, python, scala, java, aws, gcp, azure, redshift, bigquery, tableau, power bi, data visualization, problem solving, analytical skills, cloud platforms, etl pipelines","analytical skills, apache beam, apache spark, aws, azure, bigquery, cloud platforms, data engineering, data modelling, datawarehouse, etl pipelines, gcp, java, nosql, powerbi, problem solving, python, redshift, scala, sql, tableau, visualization"
"Cloud Data Architect, Azure, Charity, Lincoln, Hybrid, COR5399",Beautyk Creative,"Lincoln, England, United Kingdom",https://uk.linkedin.com/jobs/view/cloud-data-architect-azure-charity-lincoln-hybrid-cor5399-at-beautyk-creative-3775792754,2023-12-17,Lincoln, United Kingdom,Mid senior,Onsite,"The Role
This is an excellent Cloud Data Architect role, working for a company with a growing reputation in their field. The Cloud Data Architect will have expert experience of Azure cloud platforms and be an experienced, driven and enthusiastic individual. The Cloud Data Architect will report into the Principle Architect and will be required to act as the SME on the businesses data model, supporting in the design, development and implementation of the data strategy, data processes, platforms & repositories.
The Company
The Cloud Data Architect will join a thriving and well-established not-for-profit charity organisation within the care sector. The Cloud Data Architect will contribute to a supportive team environment, which rewards hard work and innovative thinking. The current work pattern in place is hybrid working, going on-site 2 days per week, although some occasional travel to other sites as well as overnight stay may be required - Therefore, you will need a full UK driving licence.
Benefits
Some of the benefits on offer for the Cloud Data Architect, include:
Up to 28 days' holiday
6% pension contribution
Life assurance
What's Required?
The Ideal candidate for the role will have some of the following:
Prior experience within a similar Data Architecture role - Helping to develop data strategy and acting as the in-house SME on all things data!
Solid commercial experience of using an Azure cloud platform and with a Microsoft Azure Data Engineer certification
Any relevant experience working in an organisation with multiple sites, as well as experience managing enterprise-wide systems would be advantageous
The list above is important, but not as important as hiring the right person! So if you don't meet all of the criteria above, but feel the role is of interest, please apply or get in touch today to discuss further.
So What's Next?
If you are a Cloud Data Architect currently, or looking to harness your career in this direction, and would like to know more about this excellent opportunity; apply now for immediate consideration!
Cloud Data Architect, Azure, Charity, Lincoln, Hybrid, COR5399
Corriculo Ltd acts as an employment agency and an employment business. #INDITO
Show more
Show less","Azure, Data Architecture, Data Strategy, Data Model, Data Processes, Data Platforms, Data Repositories, Microsoft Azure Data Engineer, Enterprisewide Systems, Hybrid Working","azure, data architecture, data strategy, data model, data processes, data platforms, data repositories, microsoft azure data engineer, enterprisewide systems, hybrid working","azure, data architecture, data model, data platforms, data processes, data repositories, data strategy, enterprisewide systems, hybrid working, microsoft azure data engineer"
Data Architect,Adecco,"Lincoln, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-architect-at-adecco-3767376645,2023-12-17,Lincoln, United Kingdom,Mid senior,Onsite,"We are currently recruiting for an experienced Data Architect to work for Lincolnshire Police at their headquarters in Nettleham.
This role will be hybrid working Monday to Friday 37 hours a week.
PLEASE NOTE DUE TO POLICE VETTING CRITERIA YOU MUST HAVE RESIDED WITHIN THE UK CONTINUOUSLY FOR AT LEAST 5 YEARS AT THE TIME OF APPLICATION. UNFORTUNATELY ANYTHING LESS THAN THIS WILL NOT BE CONSIDERED.
JOB PURPOSE AND SCOPE
The Force's Data Architect is accountable for the design and build of the Force's overall data architecture, taking full responsibility for its implementation and its ability to serve the data required to produce information, intelligence and insights products for the Force and its partners.
CORE WORK AREAS
Leadership
Subject matter expert on data architecture for the Force and, as required provides expertise to partner organisations, and representing the Force's interest in regard to data.
Accountable, on behalf of the Chief Digital Information Officer (CDIO) for the design and build of the Force's overall data architecture, taking full responsibility for its implementation
Responsible for the manage, storage and provision of the data required to produce information, intelligence and insights products for the Force and its partners.
Accountable for the selection and adoption of appropriate data tools, technologies, and frameworks to optimise data management and enhance data analytics capabilities across the Force.
Provides technical leadership and mentorship to the data engineering, data analysis and BI development team, guiding them in best practices for these disciplines.
Works in partnership with the Solutions Manager and their Principal Developer and Principal Database Administrator to continually develop the services and technical skills of the teams that make up the digital and data solutions department.
Leads projects and programme workstreams related to building or changing the Force's data architecture, including the defining of the technical strategy, developing the business case, supporting the Enterprise Architect in planning and securing funding, and leading the implementation and transition into BAU.
Is part of the out of hours on-call rota ensuring any critical IT incident is responded to immediately, and that officers and staff are requiring critical service and are being supported.
Data Architecture Design
Engages in national and regional data strategy.
Develops, implements, and maintains a comprehensive and robust data architecture strategy, aligned to the strategic aims and the data-drive initiatives of the Force.
Designs data models, data flows and integration, data storage, and working with the Principal DBA, defines the database schemas that enable efficient and accurate data collection, integration, and analysis.
Designs and builds the target data architecture (or series of architectures) for the organisation
Data Solutions
Partners with and guides chief officers and heads of department across the Force to help them understand what is possible with data, turning their visions into tangible data solutions working together to build any related business case for investment and lead on its implementation.
Leads the design and build scalable and efficient data pipelines and platforms (be these fabric or mesh, whichever is strategically appropriate) that enables seamless data flow and more effective combination, analysis and sharing of multiple data sources, across the Force and between the Force and partner organisations.
Makes major contributions to the planning and implementation of the Force's Cloud Strategy.
Works closely with performance, intelligence and data analysts, engaging with operational subject matter experts to understand and define the analytical requirements of the Force. Translates these requirements into effective data models, marts and structures that will support the production of the Forces' analytics, predictive modelling and reporting capabilities.
Provides expert input into the adoption and application of appropriate analytical techniques to create information which supports Force in its business decision-making and its crime and justice investigations and research.
Leads the team in building strategic, tactical and operational business intelligence products for the Force, and supports frontline specialist crime and performance analysts in developing their own intelligence products.
Essential Criteria
Degree in Computer Science, Information Systems, or a related field. or a combination of education and relevant experience
Query languages e.g. SQL, Java, Hive, R
Data Management technologies e.g. ETL tools, data integration platforms
Proven experience as a Data Architect, Data Engineer (or a related role, with a strong emphasis on data architecture design and implementation) employing modern data engineering tools and technologies
Hands-on experience with cloud-based data platforms and services (e.g., Azure, AWS) including familiarity with cloud-based data storage, and big data technologies
Establishing data classification systems and standards
Excellent analytical and problem-solving skills, and the ability to translate complex requirements into practical, scalable data architecture solutions
Able to architect and deploy data cloud solutions based on service requirements ensuring effectiveness, price efficiency and security.
Planning, coordinating and prioritising workload for self and team
Adecco acts as an employment agency for permanent recruitment and an employment business for the supply of temporary workers. The Adecco Group UK & Ireland is an Equal Opportunities Employer.
By applying for this role your details will be submitted to Adecco. Our Candidate Privacy Information Statement explains how we will use your information - please copy and paste the following link in to your browser https://www.adecco.co.uk/candidate-privacy
Show more
Show less","Data Architecture, Data Engineering, Cloud Computing, Big Data, Data Pipelines, Data Platforms, Data Analysis, Machine Learning, Business Intelligence, Data Visualization, Data Governance, Data Security, SQL, Hive, R, Java, ETL Tools, Azure, AWS, Oracle, Tableau, Power BI","data architecture, data engineering, cloud computing, big data, data pipelines, data platforms, data analysis, machine learning, business intelligence, data visualization, data governance, data security, sql, hive, r, java, etl tools, azure, aws, oracle, tableau, power bi","aws, azure, big data, business intelligence, cloud computing, data architecture, data engineering, data governance, data platforms, data security, dataanalytics, datapipeline, etl tools, hive, java, machine learning, oracle, powerbi, r, sql, tableau, visualization"
Lead Data Analyst,Harnham,"Nottinghamshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-analyst-at-harnham-3781945106,2023-12-17,Lincoln, United Kingdom,Mid senior,Onsite,"To Apply for this Job Click Here
LEAD DATA ANALYST
UP TO £60,000
NOTTINGHAM
This is an exciting new opportunity where you can expect a great career trajectory within the company. You will be part of an experienced team supporting the company to improve customer journeys and drive better outcomes.
The Company
This opportunity is for a collections, recovery and debt consultancy working across multiple different sectors. The company is young and planning constant growth through using analytics to optimise performance.
THE ROLE
You can expect to carry out the following tasks on a day-to-day basis:
Conducting portfolio analysis to provide insights in various formats for pre-sales and ongoing client management
Delivering presentations to key stakeholders and clients
Taking responsibility for meeting client analytical requirements
Influencing key business areas based on analytical findings
Utilising SQL for data creation, extraction, and manipulation
Creating and updating customer profiles for key business decisions
Enhancing customer profiles by incorporating external data sources and influencing their selection
Assuming a key contact role within the Analytics team
Mentoring junior members of the team
Your Skills And Experience
Possesses an analytical and logical mindset with the capability to apply problem-solving skills to real-world challenges
Holds 2-5 years of experience in an analytical environment
Preferably have experience in collections/recoveries
Desirable knowledge of Litigation and Enforcement
Demonstrates innovative and creative thinking, introducing new ideas and challenging existing ways of working
Displays ambition and proactivity with a keen attention to detail
Effectively communicate with both junior and senior team members
Exhibit excellent written and verbal communication skills, fostering a willingness to build personal relationships
Adapts quickly to tight deadlines, flexing workload to support both long-term and short-term projects
A strong numerate degree
The Benefits
A salary of up to £60,000
Predominately remote work with occasional travel to Nottingham
Additional skills training
Company pension
Private health care
How To Apply
Please register your interest by sending your CV to Gaby Adamis via the Apply link on this page.
To Apply for this Job Click Here
Show more
Show less","Data Analysis, SQL, ProblemSolving, Data Manipulation, Analytical Thinking, Creative Thinking, Communication, Teamwork, Adaptability, Numerate Degree, Collections, Recoveries, Litigation, Enforcement","data analysis, sql, problemsolving, data manipulation, analytical thinking, creative thinking, communication, teamwork, adaptability, numerate degree, collections, recoveries, litigation, enforcement","adaptability, analytical thinking, collections, communication, creative thinking, data manipulation, dataanalytics, enforcement, litigation, numerate degree, problemsolving, recoveries, sql, teamwork"
Data Engineer,BJSS,"Lincoln, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-3629571129,2023-12-17,Lincoln, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
About the Role
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","DataOps, Software engineering, Data engineering, Python, Cloud data services, CI/CD tooling, Coding best practices, Design patterns, Objectoriented programming, Parallel computing, Workflow programming, Data storage, Data processing, Relational data stores, Nonrelational data stores, AWS, Azure, GCP, Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion","dataops, software engineering, data engineering, python, cloud data services, cicd tooling, coding best practices, design patterns, objectoriented programming, parallel computing, workflow programming, data storage, data processing, relational data stores, nonrelational data stores, aws, azure, gcp, databricks, data factory, synapse, kafka, redshift, glue, athena, bigquery, s3, cloud data fusion","athena, aws, azure, bigquery, cicd tooling, cloud data fusion, cloud data services, coding best practices, data engineering, data factory, data processing, data storage, databricks, dataops, design patterns, gcp, glue, kafka, nonrelational data stores, objectoriented programming, parallel computing, python, redshift, relational data stores, s3, software engineering, synapse, workflow programming"
Data Scientist,Downer Professional Services,"Canberra, Australian Capital Territory, Australia",https://au.linkedin.com/jobs/view/data-scientist-at-downer-professional-services-3785870438,2023-12-17,Canberra, Australia,Mid senior,Onsite,"Work on leading edge projects for Defence engagements
Work with purpose delivering critical Defence projects and programs for Australia
Investment in you – L&D, experienced mentors, and accessible leadership
At Downer Professional Services, you’ll find purpose, impact and huge opportunity. Join us and help make it happen.
A real endeavour,
achieved with you
As Australia’s multi-disciplinary advisory and business partner of choice for clients in Defence, Government and infrastructure, we deliver complex things that matter. It's not always an easy task, but anything worthwhile seldom is. Great things are rarely accomplished alone. We’re building something remarkable. Now we just need you.
A real endeavour,
shaped by you
As a
Data Scientist,
you will play a pivotal role in developing advanced analytics, tools, and methodologies to optimise the utilisation of data assets, meeting the specific requirements of the business. Your responsibilities will include planning, organising, creating, and integrating advanced analytic solutions.
To succeed in this endeavour, you will:
Assess the necessity for analytics, appraise the issues to be addressed, and identify internal or external data sources required.
Utilise appropriate mathematical, statistical, predictive modelling, or machine-learning techniques to scrutinise data, extract insights, contribute value, and facilitate decision-making.
Assume technical responsibility throughout all phases of software/analytic development. Plan and oversee software/analytic construction activities, employing suitable development methods, tools, and techniques.
Contribute to and support leadership initiatives aimed at enhancing data science processes and techniques.
Contribute to the formulation of solution architectures in data analysis and exploitation.
Identify system interdependencies and collaborate with other system stakeholders to resolve conflicts and coordinate integrated solutions.
A real endeavour,
driven by you
Our work is highly complex and confidential, so there are some things you must have to fulfil this role.
We’re looking for:
Previous experience in a similar role, preferably within a professional services firm or the Defence industry.
Proficiency in architecting and developing software/analytic solutions.
Strong problem-solving skills, enabling you to develop effective solutions to complex data-related challenges.
Proven expertise in the application and integration of machine learning approaches is highly desirable for this role.
Strong communication skills, both in written and verbal form.
You must hold a current NV1 security clearance or higher.
More than anything, you’ll bring a strong work ethic, a positive attitude and a drive to manage critical projects and programs that build sovereign capability and deliver complex things that matter.
Why Downer Professional Services?
At Downer Professional Services, you’ll find the opportunity to build something remarkable. Here are some of the reasons why our people love working here:
Work with purpose.
You’ll deliver or enable critical Defence projects and programs in Australia, for Australia. Your expertise will help build sovereign capability.
Great projects, influence and impact.
You might join us for the projects, but we think you’ll stay for the impact you can have. With high project ownership, agile problem solving and accessible leaders, you can go further at Downer Professional Services.
Your opportunity to build something remarkable.
We’re proud of all we’ve achieved. But there’s much more to do. Together, we’re building a better, stronger, more agile Downer Professional Services. From process and systems to culture and communications, we're working hard to enable our teams to do their best work, whoever and wherever they are.
We encourage you to join us on our greatest endeavour. We’ll support and recognise your contribution, and you’ll also enjoy a range of benefits including:
Veteran support initiatives and policies, including up to 20 days of Reservist Leave for veterans to remain connected as they transition out of active service
An Employee Assistance Program providing access to professionals to support you (and your family) to achieve your personal or professional goals through wellbeing coaching, counselling, financial advice, legal advice and more.
Affordable healthcare covers, discounted car purchase and rental (with selected brands), and other retail discounts.
To learn more about why our people choose Downer Professional Services, read some of the stories of incredible people on our website.
Ready to join us on our biggest endeavour?
Take your career to the next level.
Apply
for this position.
Downer Professional Services is an equal opportunity employer committed to fostering a diverse and inclusive culture. We do not discriminate on the basis of gender, race, religion, colour, national origin, sexual orientation, age, marital status, veteran status, or disability. Accordingly, we encourage applications from people of diverse backgrounds, including First Nations People and those from culturally and linguistically diverse backgrounds.
Show more
Show less","Data Science, Machine Learning, Predictive Modeling, Statistical Analysis, Software Development, Analytics, Data Exploitation, Problem Solving, Communication, NV1 Security Clearance","data science, machine learning, predictive modeling, statistical analysis, software development, analytics, data exploitation, problem solving, communication, nv1 security clearance","analytics, communication, data exploitation, data science, machine learning, nv1 security clearance, predictive modeling, problem solving, software development, statistical analysis"
Senior Database Administrator / Developer,"ENSCO, Inc.","Charlottesville, VA",https://www.linkedin.com/jobs/view/senior-database-administrator-developer-at-ensco-inc-3766036365,2023-12-17,Waynesboro,United States,Mid senior,Onsite,"City
Charlottesville
State
Virginia
Country
United States
Job Description
ENSCO’s Applied Technology and Engineering (ATE) Division designs, develops, and manufactures precision, high-speed, real-time track inspection systems and associated.
decision-making software packages for the national and international railway market. We are looking for a Senior Database Developer to build, optimize, and maintain conceptual.
and logical database models.
Responsibilities
For this role,
You should know how to analyze system requirements
Design database solutions to manage large volumes of data effectively and securely
Develop data pipelines for predictive analysis and data visualization using relevant tools
As needed implement migration methods for existing data
Ideally, you are excited by the opportunity to work with Big data
Contract position with 6-month duration
Qualifications Required (Skills)
BS in Computer Science or relevant field or equivalent and 5 Years relevant experience including 3 years of proven work experience in Database Architecture, Database Development, or similar skills.
In-depth understanding of database structure principles
Experience gathering and analyzing system requirements
In-depth understanding of data management (e.g. permissions, recovery, security, migration, scaling, performance tuning, optimization, and monitoring)
Hands on experience writing SQL, T-SQL, PL/SQL, Stored Procedures, Views and Triggers
Work experience with Database Technologies like MySQL, MS SQL, Oracle, NoSQL and Hadoop
Experience with AWS database services like AWS Aurora, DynamoDB, DMS, RedShift
Experience developing and implementing database backup, archival procedures and disaster recovery
Experience with ETL and related tools (Talend, Informatica, Snowflake)
Knowledge of software development lifecycle for web and desktop applications
Proven analytical skills and documentation
Coupled with the technical skills the successful candidate will have strong verbal and written communication skills and problem-solving ability.
Qualifications Desired
Master's in computer science or relevant field
Familiarity with data visualization tools (e.g. Tableau, Power BI, D3.js, and R)
Previous exposure to Geographic Information Systems
Hands-on experience with Java or C#/.net
Req ID
3522BR
Internal Position Title
Senior Software Engineer
Employment Status
Temporary
U.S. Citizenship
No
Background Check Type
7 year Pre-employment
Drug Screen Type
None
Export Control and Licensing
This position may involve access to technology or technical data that is controlled under U.S. export control laws and regulations and the release of which to a foreign national may require an export license from the U.S. Government.
Contract Award Contingency
No
Recruiter
Judi VanDerWesthuizen
Benefit Highlights
401(k) Plan with up to 6% dollar-for-dollar company match
Holidays Off
Recognition and reward programs
Patent awards
Technical paper awards for presentations and publications
Recruitment awards
Length of service awards
Community service recognition
Division Description
At ENSCO, we've been at the forefront of engineering, science, and advanced technology solutions for over five decades, continually pushing the boundaries of innovation. We're not just a company; we're a dynamic force shaping the future of governments and private industries worldwide.
ENSCO's Rail Division stands out on the global stage as a leader in railway infrastructure inspection technology. Our products and services have reached a dozen countries across five continents, playing a pivotal role in preventing train derailments; safeguarding human lives and the environment; and supporting the sustainment of economies.
What sets us apart is our commitment to pioneering technology across various exciting domains:
Sensor Systems Integration: We utilize a diverse range of sensors: from inertial and positioning (GPS/RFID) to laser scanners, imaging (line scan, area scan, thermal), ultrasonic, ground-penetrating radar, and LIDAR. We’re masters at sensor integration.
Autonomous Sensor Deployment: Imagine sensors, positioning systems, artificial intelligence capabilities and wireless communications on active rail cars, all autonomously inspecting railways. It's not science fiction; it's what we do.
Cutting-Edge Machine Vision: We're in the business of AI and data science. Developing machine vision algorithms to detect conditions that were traditionally inspected by humans. We're making rail inspections smarter and more efficient.
Enterprise Data Management and Software as a Service: Our data management systems are not just sophisticated; they're, enabling us to display, report, and conduct big data analytics with ease. We automate and support data-driven decision making.
Join our team and be part of a stable and growing company that's making a tangible impact on some of the world's most challenging transportation problems. At ENSCO, we're not just building the future; we're defining it. Explore more about us at ENSCO's website (www.ensco.com) and be part of our exciting journey.
Strength in Diversity
ENSCO, Inc. and its wholly owned U.S. subsidiaries are equal opportunity/affirmative action employers, committed to diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender identity, sexual orientation, national origin, disability, or protected veteran status, or any other protected characteristic under state or local law.
Show more
Show less","SQL, TSQL, PL/SQL, Stored Procedures, Views, Triggers, MySQL, MS SQL, Oracle, NoSQL, Hadoop, AWS Aurora, DynamoDB, DMS, RedShift, Talend, Informatica, Snowflake, Tableau, Power BI, D3.js, R, Java, C#/.net","sql, tsql, plsql, stored procedures, views, triggers, mysql, ms sql, oracle, nosql, hadoop, aws aurora, dynamodb, dms, redshift, talend, informatica, snowflake, tableau, power bi, d3js, r, java, cnet","aws aurora, cnet, d3js, dms, dynamodb, hadoop, informatica, java, ms sql, mysql, nosql, oracle, plsql, powerbi, r, redshift, snowflake, sql, stored procedures, tableau, talend, triggers, tsql, views"
"Lead Software Engineer, Data Engineering",Jobs via eFinancialCareers,"Charlottesville, VA",https://www.linkedin.com/jobs/view/lead-software-engineer-data-engineering-at-jobs-via-efinancialcareers-3725934093,2023-12-17,Waynesboro,United States,Mid senior,Onsite,"The Role: Data Engineer
Location: Team is in Boston, but is available for remote or on-site throughout CST and EST Time zones
GL (for internal use only): 11
Panjiva is a data-driven technology company that uses machine learning to provide powerful search, analysis, and visualization of billions of shipping records from nearly every country in the world. More than 3,000 customers in over 100 countries, ranging from Fortune 500 companies and startups to government agencies and hedge funds, rely on our platform for supply chain intelligence. In global trade, better insight means better decision making and stronger connections between companies and governments across the globe.
Recognizing Panjiva's cutting-edge technology, S&P Global acquired Panjiva in 2018. This acquisition has grown our resources, dramatically expanded our access to data, and accelerated our growth plans.
People are Panjiva's greatest strength - join our engineering team as we map out a key part of the world economy!
Job Description
As a data engineer on our team, you will play a key role in developing our next-generation data science infrastructure and underlying core technologies. You will work with Panjiva's world-class data scientists, analysts, and engineers to create products that solve important real-world business problems in a collaborative, fast-paced, and fun environment.
You'll work closely with our data science team to develop new platforms, infrastructure, and tools that will allow for machine learning applications at production scale over ever-growing datasets.
You'll design and leverage distributed computing technologies, data schemas, and APIs to construct data science pipelines. In addition, you'll be expected to participate in augmenting our infrastructure to seamlessly integrate new data sets through constant R&D of the technologies and systems we use.
Join us in building the next generation of products as we continue to deliver valuable and actionable insights to decision-makers in the $15 trillion global trade industry.
Responsibilities
Architect and implement distributed systems that perform complex transformations, processing, and analysis over very large scale datasets
Develop processes to monitor and automate detection of quality regressions in raw data or in the output of Panjiva's machine learning models
Working with our data scientists to turn large-scale messy, diverse, and often unstructured data into a source of meaningful insights for our customers
Optimizing slow-running database queries and data pipelines
Helping enhance our search engine, capable of running sophisticated user queries quickly and efficiently
Building internal tools and backend services to enable our data scientists and product engineers to improve efficiency
Qualifications
B.S., M.S., or Ph.D. in Computer Science (or a related field) or equivalent work experience
7+ years of experience working with data-at-scale in a production environment
Experience designing and implementing large-scale, distributed systems
Experience in multi-threaded software development (or some form of parallelism)
Significant performance engineering experience (e.g., profiling slow code, understanding complicated query plans, etc.)
Solid understanding of core algorithms and data structures, including the ability to select (and apply) the optimal ones to computationally expensive operations over data-at-scale
Strong understanding of relational databases and proficiency with SQL
Deep knowledge of at least one scripting language (e.g., Python, Ruby, JavaScript)
Deep knowledge of at least one compiled language (e.g., Scala, C++, Java, Go)
Experience developing software on Linux-based operating systems
Experience with distributed version control systems
Nice-to-Haves
Familiarity with relational database internals (especially PostgreSQL)
Proficiency with cloud computing platforms, specifically AWS
Working knowledge of probability & statistics
Contributions to open-source software
Experience building customer-centric products
Compensation/Benefits Information (US Applicants Only):
S&P Global states that the anticipated base salary range for this position is $85,300 - $170,000 . Base salary ranges may vary by geographic location.
In addition to base compensation, this role is eligible for an annual incentive bonus plan.
This role is eligible to receive additional S&P Global benefits. For more information on the benefits we provide to our employees, visit https://www.spgbenefitessentials.com/newhires .
About S&P Global
S&P Global delivers essential intelligence that powers decision making. We provide the world's leading organizations with the right data, connected technologies and expertise they need to move ahead. As part of our team, you'll help solve complex challenges that equip businesses, governments and individuals with the knowledge to adapt to a changing economic landscape.
S&P Global Market Intelligence partners with customers to broaden their perspective and operate with confidence by bringing them leading data sources and technologies that embed insight in their daily work.
We pride ourselves on our agility and diversity, and we welcome requests to work flexibly. For most roles, flexible hours and/or an element of remote working are usually possible. Please talk to us at interview about the type of arrangement that is best for you. We will always try to be adaptable wherever we can.
Equal Opportunity Employer
S&P Global is an equal opportunity employer and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law. Only electronic job submissions will be considered for employment.
If you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.
US Candidates Only:
The EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf describes discrimination protections under federal law.
20 - Professional (EEO-2 Job Categories-United States of America), IFTECH202.2 - Middle Professional Tier II (EEO Job Group), SWP Priority - Ratings - (Strategic Workforce Planning)
Job ID:
277527
Posted On:
2023-09-21
Location:
Cambridge, Massachusetts, United States
Show more
Show less","Machine learning, Data science, Data engineering, Distributed computing, Data schemas, APIs, Data pipelines, SQL, Python, Ruby, JavaScript, Scala, C++, Java, Go, Linux, Version control systems, PostgreSQL, AWS, Probability, Statistics","machine learning, data science, data engineering, distributed computing, data schemas, apis, data pipelines, sql, python, ruby, javascript, scala, c, java, go, linux, version control systems, postgresql, aws, probability, statistics","apis, aws, c, data engineering, data schemas, data science, datapipeline, distributed computing, go, java, javascript, linux, machine learning, postgresql, probability, python, ruby, scala, sql, statistics, version control systems"
Data Engineer,GA-CCRi,"Charlottesville, VA",https://www.linkedin.com/jobs/view/data-engineer-at-ga-ccri-3713947851,2023-12-17,Waynesboro,United States,Mid senior,Onsite,"GA-CCRi maintains and deploys production systems for users across the Intelligence Community, Department of Defense, and commercial industry. We build and develop best-in-class all domain and globally focused situational awareness capabilities, including THRESHER and DRAGONSPELL, that process petabytes of data from numerous streaming data sources in near real time. Our systems apply state-of-the-art algorithms and machine learning techniques to extract features and fuse data from multiple phenomenologies to form a rich live view of objects in the sky, on the sea, and on the ground. These analytics are designed to determine not just where something is, but what it is, where it's been and what it's doing. All of this ""data to knowledge"" is made available to end users in our own browser-based application for visualization, analysis, and understanding. We always want to do more, and that's where you come in!
CCRi is looking for a Data Engineer with a mix of DevOps, Software, and Systems Engineering experience who is comfortable working in a secure, cloud-based environment.
Positions available in: Charlottesville, VA
Duties And Responsibilities
Build and maintain data pipelines
You’ll be responsible for bringing in new data sources as well as maintaining existing data sources. “Data to Knowledge” is our mantra. Ensuring that data sources are ready and available for use in our products is priority for both our customers and users.
Analyze and organize data
We bring in numerous data feeds and additional feeds are always in the works. You’ll be responsible for understanding what’s in the data, how should we bring it in, and how best to maintain it going forward.
Data ingestion and quality checks
Bringing in streaming data sources can sometimes be a tricky process. You’ll be responsible for reaching out to data providers, utilize Apache NiFi and other tooling for wiring data flows, and then bringing that data into our system.
Metrics and monitoring
You can’t manage what you can’t measure. We utilize data visualization and alerting tools to help monitor feeds and help ensure availability. As a data engineer you will help scope the appropriate alerting thresholds through evaluation of the feed volume and flow rates.
Work alongside project management
You'll work closely with the Data Project Manager in preparation for meetings with our customers, such as the monthly Data Council.
Essential Functions
Under general direction, this position is responsible for the design and implementation of secure automation solutions for development, testing, and production environments
Builds and deploys automation, monitoring, and analysis solutions
Manages continuous integration and delivery pipeline to maximize efficiency
Implements industry best practices for system hardening and configuration management
Develops and maintains solutions for operational administration, system/data backup, disaster recovery, and security/performance monitoring
Continuously evaluates existing systems with industry standards and make recommendations for improvement
Builds and maintains tools, solutions and microservices associated with deployment and our operations platform, ensuring that all meet our customer service standards and reduce errors
Tests system integrity, implemented designs, application developments and other processes related to infrastructure, making improvements as needed
Manages code deployments, fixes, updates and related processes
Works with open-source technologies as needed
Works with CI and CD tools, and source control such as GIT and SVN
Offers technical support where needed, developing software for our back-end systems
Stays current with industry trends and source new ways for our business to improve
Work with the Program Manager, Project Technical Lead, and the Customer to create technical plans for new deployments, including requirements, implementation details (e.g. sizing of the resources), verification, and validation
Ongoing verification, validation, and documentation of deployments
Plan and execute maintenance and upgrade tasks
Support the deployments including monitoring and providing answers to customer inquiries
Participate in an on-call rotation for supporting the systems during work and weekend hours
Work 100% on-site at our Charlottesville facilities
Requirements
Typically requires a Bachelor's or Master's degree in Computer Science, Engineering, or related field and five years of related experience
Understanding of system administration in Linux
Strong knowledge of configuration management tools
Scripting/coding with Python, Java/Scala, Bash· Strong communication and documentation skills
An ability to drive to goals and milestones while valuing and maintaining a strong attention to detail
Excellent judgment, analytical thinking, and problem-solving skills
Self-motivated individual that possesses excellent time management and organizational skills
Strong Customer Advocacy skills / Passion for the Customer
Strong understanding and experience with at least one cloud environment such as Amazon Web Services (Preferred), Google Cloud and/or Microsoft Azure
Experience with software development life-cycles and best practices
Strong troubleshooting skills, and capable to solve problems logically
Collaborative and willing to work with remote team members
Desired Skills And Qualifications
An ideal candidate will have additional specific experience with many of the following:
Apache NiFi and Apache Kafka, Secure File Transfer Protocol, Web scrapers
Experience in systems engineering or architecture (balancing hardware and networking capacities with data transactions)
Containers and container orchestration (docker, kubernetes, helm, istio, rancher)
CI/CD, GitOps
Provisioning (CloudFormation, Terraform) and configuration management (Puppet) tools
Security & networking - cloud security, IAM, ABAC, SSO, Okta, Keycloak, A&A
Databases (Postgres, MySQL, and Accumulo)
Support or operations engineering roles
Preference will be given to candidates with an active TS clearance
Travel Percentage Required
0-10%
Relocation Assistance Provided
Yes
US Citizenship Required?
Yes
Clearance Required?
Ability to obtain and maintain security clearance required.
Clearance Level
TS with SCI eligibility
Pay Range
$110,000-$150,000
Benefits
Casual Work Environment
Intellectually Challenging Work
Health Insurance including FSA, HSA and Tricare Supplement options
Short/Long Term Disability Insurance
Generous Defined Retirement Benefit, including both a 401K match and pension plan
Very Flexible Vacation Policy
The job description above is not intended to be comprehensive list. Responsibilities, activities, duties, and/or tasks may change or be assigned at any time.
CCRi is committed to a diverse and inclusive workforce because we know that our differences benefit our employees, our customers, and our community. We are proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, color, religion, age, sex, sexual orientation, gender identity, national origin, status as a an individual with a disability, status as a protected veteran, or any other applicable legally protected characteristics.
Show more
Show less","Apache NiFi, Apache Kafka, Secure File Transfer Protocol, Linux, Python, Bash, Java, Scala, GitOps, CloudFormation, Terraform, Puppet, Postgres, MySQL, Accumulo, Docker, Kubernetes, Helm, Istio, Rancher, CI/CD, Git, SVN, Amazon Web Services, Google Cloud, Microsoft Azure","apache nifi, apache kafka, secure file transfer protocol, linux, python, bash, java, scala, gitops, cloudformation, terraform, puppet, postgres, mysql, accumulo, docker, kubernetes, helm, istio, rancher, cicd, git, svn, amazon web services, google cloud, microsoft azure","accumulo, amazon web services, apache kafka, apache nifi, bash, cicd, cloudformation, docker, git, gitops, google cloud, helm, istio, java, kubernetes, linux, microsoft azure, mysql, postgres, puppet, python, rancher, scala, secure file transfer protocol, svn, terraform"
"Data Scientist, Department of Anesthesiology",University of Virginia,"Charlottesville, VA",https://www.linkedin.com/jobs/view/data-scientist-department-of-anesthesiology-at-university-of-virginia-3766084973,2023-12-17,Waynesboro,United States,Mid senior,Onsite,"The Department of Anesthesiology at the University of Virginia (UVA) School of Medicine seeks applications for a Data Scientist. The Data Scientist will report to the Director of Administration.
UVA Anesthesiology provides an environment where expertise and passions thrive, utilizing our extraordinary team of anesthesia physicians, trainees, nurses and staff to advance patient care within UVA Health. With excellence in physician leadership, research, education and innovations, we strive to improve perioperative outcomes, establish unparalleled practices in patient care, and advance the field of anesthesiology globally.
The successful candidate will be expected to provide sophisticated data management and analysis to support projects or programs. The Data Scientist will focus primarily on high-level data projections and statistical analysis. To include managing the designing and programming of all data entry forms and the training and supervision of project research coders, student workers, and volunteers. They will oversee regular assessments of reliability, submit data on a monthly basis, and assist with literature searches pertinent to various research project topics.
Understand all phases of the analytic process including data collection, preparation, modeling, evaluation, and deployment.
Explore and examine data from multiple disparate sources in order to identify, provide reporting on, and analyze trends in the data as well as regular reporting for assigned departments.
Establish links across existing data sources and find new, interesting data correlations.
Develop statistical, mathematical and predictive models using strategic business understanding to build the algorithms necessary to ask the right questions and find the right answers.
Stay appraised of all assigned initiatives, providing reporting and analytics that will allow the group to make informed, data-driven decisions to meet business objectives.
Visualize and report data findings creatively in a variety of visual formats that appropriately provides insights to the stakeholders.
Understands all phases of the analytic process including data collection, preparation, modeling, evaluation, and deployment.
Under general supervision, formulates and defines analytic scope and objectives through research and fact-finding.
Competent to work on most phases of data analysis and associated programming activities.
Analyze and interpret the results of experiments to create cost effective and process-efficient alternatives, enhancements, or modification.
In addition to the above job responsibilities, other duties may be assigned.
Qualified candidates must have a master’s degree with three years of relevant experience.
This position is located in Charlottesville, VA.
The anticipated start date is December 2023. This position is a restricted position and is dependent upon project need, availability of funding and performance. This is an Exempt level, benefited position. For more information on the benefits at UVA, visit hr.virginia.edu/benefits.
This position will remain open until filled. This position will not sponsor applicants for work visas. The University will perform background checks on all new hires prior to employment. A completed pre-employment health screen is required for this position prior to employment.
To Apply
Please apply through Workday, and search for R0054303. Internal applicants must apply through their UVA Workday profile by searching 'Find Jobs'. Complete an application online with the following documents:
CV
Cover letter
Upload all materials into the resume submission field, multiple documents can be submitted into this one field. Alternatively, merge all documents into one PDF for submission. Applications that do not contain all the required documents will not receive full consideration.
References will be completed via UVA’s standardized process Skill Survey. A total of five references will be requested via SkillSurvey during the final phase of the interview process.
For questions about the application process, please contact Ashley Cochran, Senior Recruiter at alc6dk@virginia.edu.
For more information about UVA and the Charlottesville community please see http://www.virginia.edu/life/charlottesville and https://embarkcva.com/.
The University of Virginia, i
ncluding the UVA Health System which represents the UVA Medical Center, Schools of Medicine and Nursing, UVA Physician’s Group and the Claude Moore Health Sciences Library,
are fundamentally committed to the diversity of our faculty and staff. We believe diversity is excellence expressing itself through every person's perspectives and lived experiences. We are equal opportunity and affirmative action employers. All qualified applicants will receive consideration for employment without regard to age, color, disability, gender identity or expression, marital status, national or ethnic origin, political affiliation, race, religion, sex (including pregnancy), sexual orientation, veteran status, and family medical or genetic information.
MINIMUM REQUIREMENTS Education: Master's Degree Experience: Three years of relevant experience. Licensure: None PHYSICAL DEMANDS This is primarily a sedentary job involving extensive use of desktop computers. The job does occasionally require traveling some distance to attend meetings, and programs.
Show more
Show less","Data Management, Data Analysis, Statistical Analysis, Data Projections, Data Entry, Data Cleaning, Data Integration, Data Visualization, Data Mining, Predictive Modeling, Machine Learning, Business Intelligence, SQL, Python, R, SAS, Tableau, Power BI","data management, data analysis, statistical analysis, data projections, data entry, data cleaning, data integration, data visualization, data mining, predictive modeling, machine learning, business intelligence, sql, python, r, sas, tableau, power bi","business intelligence, data cleaning, data entry, data integration, data management, data mining, data projections, dataanalytics, machine learning, powerbi, predictive modeling, python, r, sas, sql, statistical analysis, tableau, visualization"
Data Scientist,Altamira Technologies Corporation,"Dayton, OH",https://www.linkedin.com/jobs/view/data-scientist-at-altamira-technologies-corporation-3787769717,2023-12-17,Vandalia,United States,Mid senior,Onsite,"Altamira delivers a variety of analytic and engineering capabilities to the US National Security community, but the tech culture and the caliber of the individuals that bring these capabilities to fruition are what really set us apart.
We’re a curious, responsive, dedicated bunch spread across many corporate cultures. Dayton, OH is highly focused on the Space-based mission set with a heavy emphasis on sensor exploitation and analysis; Tampa, FL focuses on ‘art-of-the-possible’ analytics of all kinds with an emphasis on graph technologies, NLP, and wrangling complex data sets; and all forces converge at our headquarters in the Northern Virginia/Washington DC area where we host our tech events and support engineering and analytic missions across several IC and DOD agencies.
While our work occurs in different states and different mission domains, we’ve got analytics at the heart of every operation and genuine curiosity for new methods, techniques, and solutions.
Our specialties are data science and analytics, data engineering, software engineering, and end-to-end analytic solutions architecture.
We’ve also got some awesome benefits like the Altamira Healthy Living program, with ongoing competitions and a flexible spending stipend for health and wellness-related items.
Location
: WPAFB, Dayton, Ohio
The Role
: An Altamira Data Scientist/Analyst is an integral member of an analytic and engineering team. The data scientist is expected to participate in the design, development, and implementation of novel analytics to address a variety of customer problem sets. We are highly interested in individuals who can craft narratives around their analytics, producing highly visual representations of both the output and the innerworkings of analytic methods. The Data Scientist should be versed in statistics, predictive modeling, machine learning, computational simulation, geospatial modeling, network science, or other analytic techniques. This individual will also have a firm grasp of the environments in which the analytics must be deployed, whether stand-alone, within a broader software architecture, in the cloud, etc. While a heavy programming background is not required, sufficient technical knowledge to implement the analytics is highly desired.
Your skills
:
Demonstrable expertise in an area of data science or analytics (e.g. machine learning, deep learning, NLP, computer vision, predictive modeling and forecasting, statistics)
Develop statistical algorithms and models, automate data conditioning and data analysis, support machine learning, and generate visualizations for use in predictive and prescriptive modeling and proof of concept initiatives.
Apply and utilize new or established qualitative and quantitative methods to analyze and resolve analytical issues.
Working knowledge of 1 or more programming languages
Experience working with machine learning frameworks (e.g. TensorFlow, PyTorch, scikit-learn)
Design and develop new, scalable tools, dashboards, and visualizations to enable users to consume and understand data more effectively and efficiently.
Ability to visually represent and communicate complex analytics is strongly desired
Your quals
:
TS/SCI clearance
Significant coursework or professional experience in machine learning, deep learning, NLP, computer vision, predictive modeling and forecasting, statistics, or similar analytic domain
Experience delivering software or technical solutions to DOD or IC customers, USAF, JAIC, NRO, NGA, or other intelligence organizations strongly desired
Familiarity with data visualization software, tools, and techniques is desired
Bachelor's degree from an accredited institution in a quantitative or technical field of study, such as: Statistics, Mathematics, Computer Science, Physical Science or Geographic Information Systems (GIS).
Three (3) years of practical work experience integrating data mining, conditioning, and analysis that includes the use of current data analysis software and methods including two or more (or similar/equivalent) of the following software:
ESRI ArcGIS
Hive/Hadoop
MATLAB
SAS
SPSS Statistics
Three (3) years of practical experience in computer programming using one or more (or similar/equivalent) of the following programming languages:
C++
Interactive Data Language (IDL)
JavaScript
Python
R
Two (2) years of experience writing technical analysis reports.
One (1) year practical work experience with geospatial data and/or imagery data
One (1) year practical work experience with database management systems, database structures and query tools using one or more (or similar/equivalent) of the following:
GeoDB
Microsoft Access
Microsoft Excel
SQL
Demonstrated problem solving skills
Demonstrated presentation skills, including but limited to presenting at professional or academic conferences.
Altamira is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex,
national origin, disability, or protected veteran status.
Powered by JazzHR
CuLYbjP5GS
Show more
Show less","Data science, Machine learning, Statistics, Predictive modeling, Data engineering, Python, R, C++, JavaScript, Geospatial modeling, TensorFlow, PyTorch, scikitlearn, ESRI ArcGIS, Hive / Hadoop, SAS, SPSS Statistics, SQL, JavaScript","data science, machine learning, statistics, predictive modeling, data engineering, python, r, c, javascript, geospatial modeling, tensorflow, pytorch, scikitlearn, esri arcgis, hive hadoop, sas, spss statistics, sql, javascript","c, data engineering, data science, esri arcgis, geospatial modeling, hive hadoop, javascript, machine learning, predictive modeling, python, pytorch, r, sas, scikitlearn, spss statistics, sql, statistics, tensorflow"
Senior Database Developer,Winsupply,"Dayton, OH",https://www.linkedin.com/jobs/view/senior-database-developer-at-winsupply-3728065163,2023-12-17,Vandalia,United States,Mid senior,Hybrid,"Position Summary
The Database Engineer (DBE) needs a thorough understanding of relational database theory and practice, along with in-depth knowledge of data systems and database methodology, design and modeling. This individual must be analytical and adept at problem solving, can strategically design and implement production databases, and provide support for specific applications. The DBE focuses on the logical and physical aspects of the database and application data contained within. They are responsible for developing, implementing, and overseeing database policies and procedures to ensure the integrity of data. The Database Engineer will also monitor and ensure system performance issues do not arise due to database related issues. The database engineer is a person who can bridge the gaps and interact with the other portions of IT. A DBE is someone who can communicate on both a technical level and a business level. They must be able to partner with the infrastructure team, the application development team and the information management team.
Accountabilities/Responsibilities
Database Architecture, Design and Implementation
The DBE is responsible for the architecture, design and implementation of database systems and components in support of information management goals. This includes the database and data life cycle. It includes understanding and applying the science and art of data modeling practiced via modern methods, tools and interfaces. The DBE is involved in gathering business requirements, logical modeling, physical modeling, implementing the model and maintaining the model.
Data Availability, Control, Governance and Integrity
The DBE, in partnership with the data owner, is responsible for the accessibility, availability and integrity of the database and data. The DBE is responsible for defining and implementing the appropriate strategy and methods to maintain control and governance of the database and the data, as defined and mandated by the owner. This includes understanding and accounting for all the potential data access interfaces and methods, and ensuring proper security and auditing measures are in place. A critical success factor is identifying, understanding and reconciling the associated business and technical requirements for data and information management.
Data Centric Application Design and Programming
The DBE is responsible for ensuring proper and adequate data centric techniques are used in the design, development and implementation of applications. This includes taking advantage of built in database management system features and functions such as constraints, functions and triggers. The DBE is responsible for understanding and applying the science and art of coding and implementing SQL (DDL, DML and PSM) via modern methods, tools and interfaces. Leading, guiding and reviewing data centric application development is a fundamental task. As appropriate, the DBE is also involved in all aspects of maintaining, modernizing and enhancing existing database applications that are based on DDS, high level language record level access and other non-SQL interfaces.
Database Performance and Scalability
The DBE is responsible for identifying, understanding, reconciling and meeting the data serving performance and scalability requirements. This includes applying the best practices of indexing, work management and set-at-a-time SQL requests. The DBE is responsible for understanding and applying the science and art of monitoring, analyzing and tuning data access and data processing via modern methods, tools and interfaces. The DBE is involved in planning, sizing and configuring database systems to meet business and technical requirements. This should include the set up and use of prototypes, proofs of concept, proofs of technology and realistic benchmarks.
Competencies for Success
Analytical and Critical Thinking
Complex Problem Solving
Collaboration and Communication
Planning and Organizing
Presentation Skills
Proficient with SQL
Knows the Data
Qualifications/Experience
Minimum Qualifications
Greater than 8 years related professional experience
Associate’s degree in Computer Science or related field
Similar work experience, technical skills or relevant certification may be considered in place of bachelor's degree; must maintain awareness of changing technology
At least 5 years’ experience with MariaDB
Desired Qualifications
Bachelor’s degree in Computer Science or related field
Certification directly related to computer programming
Physical Demands
The physical demands here are representative of those that must be met to successfully perform the essential job functions with or without reasonable accommodations:
Sitting for extended periods of time
Dexterity of hands and fingers to operate a computer keyboard, mouse, power tools, and other computer components
Winsupply is an equal opportunity employer, so it encourages all qualified individuals to apply including minorities, veterans, women, and those with disabilities.
Show more
Show less","SQL, Data Modeling, Data Governance, Data Integrity, Database Design, Database Implementation, Database Performance Tuning, Database Scalability, Data Centric Application Design, Data Centric Application Programming, MariaDB, Relational Database Theory, Analytical Thinking, Critical Thinking, Complex Problem Solving, Collaboration, Communication, Planning, Organizing, Presentation Skills","sql, data modeling, data governance, data integrity, database design, database implementation, database performance tuning, database scalability, data centric application design, data centric application programming, mariadb, relational database theory, analytical thinking, critical thinking, complex problem solving, collaboration, communication, planning, organizing, presentation skills","analytical thinking, collaboration, communication, complex problem solving, critical thinking, data centric application design, data centric application programming, data governance, data integrity, database design, database implementation, database performance tuning, database scalability, datamodeling, mariadb, organizing, planning, presentation skills, relational database theory, sql"
Data Scientist/ Sr. Data Scientist - (CPG),Tiger Analytics,"Nashville, TN",https://www.linkedin.com/jobs/view/data-scientist-sr-data-scientist-cpg-at-tiger-analytics-3735445137,2023-12-17,Nashville,United States,Associate,Onsite,"Tiger Analytics is looking for experienced Data Scientists to join our fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world.
As a Data Scientist, you will apply strong expertise in AI through the use of machine learning, data mining, and information retrieval to design, prototype, and build next-generation advanced analytics engines and services. You will collaborate with cross-functional teams and business partners to define the technical problem statement and hypotheses to test. You will develop efficient and accurate analytical models which mimic business decisions and incorporate those models into analytical data products and tools. You will have the opportunity to drive the current and future strategy by leveraging your analytical skills as you ensure business value and communicate the results.
Key Responsibilities
Collaborate with business partners to develop innovative solutions to meet objectives utilizing cutting-edge techniques and tools
Effectively communicate the analytics approach and how it will meet and address objectives to business partners.
Advocate and educate on the value of data-driven decision-making; focus on the “how and why” of solutions
Lead analytic approaches; integrate solutions collaboratively into applications and tools with data engineers, business leads, analysts, and developers
Create repeatable, interpretable, dynamic, and scalable models that are seamlessly incorporated into analytic data products
Engineer features by using your business acumen to find new ways to combine disparate internal and external data sources
Share your passion for Data Science with the broader enterprise community; identify and develop long-term processes, frameworks, tools, methods and standards
Collaborate, coach, and learn with a growing team of experienced Data Scientists.
Stay connected with external sources of ideas through conferences and community engagements
Requirements
Bachelor's Degree in Data Science, Computer Science, or a related field
5-7 years of Data Science and Machine Learning experience required
Proficiency with Data Science concepts and modeling techniques to solve problems such as clustering, classification, regression, anomaly detection, simulation, and optimization problems on large-scale data sets
At least 2-3 years of experience in CPG and Retail space
Comfortable working in a hybrid setup in Nashville
Ability to apply various analytical models to business use cases (NLP, Supervised, Unsupervised, Neural Nets, etc.)
Exceptional communication and collaboration skills to understand business partner needs and deliver solutions
Bias for action, with the ability to deliver outstanding results through task prioritization and time management
Experience with data visualization tools — Tableau, R Shiny, etc. preferred
Benefits
This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.
Show more
Show less","Data Science, Machine Learning, AI, Clustering, Classification, Regression, Anomaly Detection, Simulation, Optimization, CPG, Retail, NLP, Supervised Learning, Unsupervised Learning, Neural Nets, Tableau, R Shiny, Data Visualization","data science, machine learning, ai, clustering, classification, regression, anomaly detection, simulation, optimization, cpg, retail, nlp, supervised learning, unsupervised learning, neural nets, tableau, r shiny, data visualization","ai, anomaly detection, classification, clustering, cpg, data science, machine learning, neural nets, nlp, optimization, r shiny, regression, retail, simulation, supervised learning, tableau, unsupervised learning, visualization"
Sr. Data Engineer (Python/Google Cloud Platform),BGSF,"Nashville, TN",https://www.linkedin.com/jobs/view/sr-data-engineer-python-google-cloud-platform-at-bgsf-3782248443,2023-12-17,Nashville,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, BGSF Inc., is seeking the following. Apply via Dice today!
Zycron has joined with a large healthcare client in search of a Data Engineer.
No subcontractors or agencies please
Hybrid position- Must be in Nashville, Tennesse area or willing be willing to relocate to Nashville, TN area.
If not, local travel will be required.
The Data Engineer will be responsible for designing and implementing data pipelines, data transformation processes, and data integration solutions. This role plays a crucial part in ensuring the availability and accessibility of high-quality data for analytical and business intelligence purposes.
Essential Duties And Responsibilities
Data Pipeline Development: Design, build, and maintain scalable data pipelines to collect, process, and transform data from various sources.
Data Integration: Develop and maintain data integration solutions that facilitate the flow of data between different systems and platforms.
Data Transformation: Implement ETL (Extract, Transform, Load) processes to ensure data accuracy and consistency.
Data Quality: Monitor and improve data quality by implementing data validation and cleansing processes.
Database Management: Assist in managing and optimizing database systems for performance and scalability.
Collaboration: Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.
Documentation: Maintain documentation for data pipelines, processes, and best practices.
Minimum Job Requirements
Bachelor's degree in Computer Science, Information Technology, or a related field.
Proven experience as a Data Engineer or similar role.
Proficiency in programming languages such as Python, Java, or Scala.
Strong SQL skills.
Experience with data integration tools and technologies.
Knowledge of Big Data frameworks (e.g., Hadoop, Spark) is a plus.
Familiarity with cloud-based data solutions (Google Cloud Platform).
Problem-solving and analytical skills.
Job ID Number: 821929
(Please reference in call or email)
Only candidates with backgrounds who match our client's requested experience will be contacted. Do not take this as a poor reflection on your experience, just a decision for the specific needs of our client's project/job. We look forward to working with you.
Sr. Data Engineer (Python/Google Cloud Platform)
Show more
Show less","Python, Java, Scala, SQL, Data Integration, ETL, Data Pipelines, Data Transformation, Data Quality, Database Management, Hadoop, Spark, Google Cloud Platform","python, java, scala, sql, data integration, etl, data pipelines, data transformation, data quality, database management, hadoop, spark, google cloud platform","data integration, data quality, data transformation, database management, datapipeline, etl, google cloud platform, hadoop, java, python, scala, spark, sql"
Data Engineer/Admin,Keylent Inc,"Franklin, TN",https://www.linkedin.com/jobs/view/data-engineer-admin-at-keylent-inc-3768066187,2023-12-17,Nashville,United States,Mid senior,Onsite,"Role: Data Engineer/Admin
Location: Franklin, TN, 37068
Duration:12 Months
Skills
AWS EMR
Job Description
Hands on experience on Hadoop /Big Data related technology experience in Storage, Querying, Processing, and analysis of data.
Strong knowledge on Pyspark programming and spark-based applications to load streaming data with minimal latency.
Hands on Experience with AWS cloud data warehouse and AWS S3 bucket for integrating data from multiple source system.
Work experience in Scrum / Agile framework and Waterfall project execution methodologies.
Expert in writing complex SQL Queries to check the integrity of data to perform database testing.
Ability to Communicate with clients and gather requirements to convey the same to Off-shore resources.
Experience in developing Hive Queries and Hive query optimization.
Experience in Importing and exporting data from different databases like MySQL, RDBMS into HDFS using Sqoop.
Experience in deploying and managing the multi-node development and production Cluster.
Ability to Monitor and schedule the workflows using Oozie.
Show more
Show less","AWS EMR, Hadoop, Big Data, SQL, Pyspark, Spark, AWS cloud data warehouse, AWS S3 bucket, Scrum, Agile, Waterfall, Hive, Oozie, Sqoop, HDFS, MySQL, RDBMS","aws emr, hadoop, big data, sql, pyspark, spark, aws cloud data warehouse, aws s3 bucket, scrum, agile, waterfall, hive, oozie, sqoop, hdfs, mysql, rdbms","agile, aws cloud data warehouse, aws emr, aws s3 bucket, big data, hadoop, hdfs, hive, mysql, oozie, rdbms, scrum, spark, sql, sqoop, waterfall"
Sr SQL Data Engineer,NR Consulting,"Nashville, TN",https://www.linkedin.com/jobs/view/sr-sql-data-engineer-at-nr-consulting-3768014995,2023-12-17,Nashville,United States,Mid senior,Onsite,"Position Sr SQL Data Engineer
Location Nashville, TN
Note - Need the candidate to be in Nashville at the Client location on Tuesdays and Wednesdays.
Sr SQL Data Engineer
"" Work within a collaborative Agile environment
"" Analyze requirements and create performant, scalable database structures, objects, and stored procedures.
"" Analyze, create, and tune complex T-SQL code to ensure optimal performance and data integrity across very large data sets.
"" Create, modify, and deploy SQL Server Integration Services packages to extract, transform and load large sets of data.
"" Troubleshoot database performance issues in an MSSQL environment.
"" Review and tune code to ensure best practices, quality standards and data integrity.
"" Configure database parameters and define data repository requirements, data dictionaries, and warehousing.
"" Design and implement approaches to improve database performance, capacity, and scalability.
"" Confer with appropriate managers and peer team members regarding problems with and capabilities of databases.
"" Experience with Oracle database, Elasticsearch, GitLab, MySQL, PostgreSQL, Kafka a plus
Job Description
Essential Job Duties:
"" Ensure SQL Server services and databases are operational and available to end users and processes.
"" Perform backup and restore operations, including cloning production databases to development, quality control, and customer test environments.
"" Review, install, test, schedule, and move to production all security patches for all environments, including database engine, SSIS, SSAS, SSRS
"" Add, delete, and modify logins and permissions in all environments as needed.
"" Monitor SQL Server and Windows Server performance, adjusting when necessary following approved technical and Change Control procedures.
"" Create, manage, and troubleshoot SQL Server Agent jobs and SSIS packages.
"" Design, document, and implement database schema and ETLs.
"" Write, document, implement, review, and tune stored procedures, functions, and ad hoc T-SQL queries.
"" Analyze and optimize database table indexes and statistics.
"" Manage database space usage and participate in storage capacity planning.
"" Work all SQL Server incidents and requests to completion, including running scripts, reviewing logs and/or trace files, troubleshooting performance issues, and assisting developers as necessary
"" Assumes other duties as assigned.
Requirements
Education & Experience:
"" BA/BS degree or equivalent experience working in a SQL Server database administrator role
Knowledge/Skills/Abilities
"" Must have the ability to communicate highly technical information to both technical and non-technical personnel.
"" SQL Server 2000, 2005, 2008, 2008 R2, 2012, 2014+
"" Windows Server 2003, 2008, 2008 R2, 2012+
"" SSIS, SSAS, SSRS as well as the SQL Server Database Engine and SQL Server Agent
"" Microsoft Office (Word, Excel, Visio)
Essential Physical Demands/Work Environment
"" This position requires individuals to have the ability to sit and concentrate for long periods of time.
"" Must be attentive to detail and have the ability to work with frequent interruptions and in stressful situations.
Preferred Qualifications
"" Experience with PostgreSQL and Elasticsearch is a plus.
Show more
Show less","SQL, TSQL, SQL Server Integration Services, MSSQL, Oracle, Elasticsearch, GitLab, MySQL, PostgreSQL, Kafka, ETL, Windows Server, SSIS, SSAS, SSRS, Microsoft Office","sql, tsql, sql server integration services, mssql, oracle, elasticsearch, gitlab, mysql, postgresql, kafka, etl, windows server, ssis, ssas, ssrs, microsoft office","elasticsearch, etl, gitlab, kafka, microsoft office, mssql, mysql, oracle, postgresql, sql, sql server integration services, ssas, ssis, ssrs, tsql, windows server"
Senior Data Engineer,Ingram Content Group,"La Vergne, TN",https://www.linkedin.com/jobs/view/senior-data-engineer-at-ingram-content-group-3729295016,2023-12-17,Nashville,United States,Mid senior,Onsite,"Job Description
Ingram Content Group (ICG)
is currently recruiting for a
Senior Data Engineer
join our team in
LaVergne, TN
(Greater Nashville area). This individual will work to deliver enterprise grade software solutions with high customer impact. They’ll lead architecture and development activities with a specialization in at least one major enterprise IT application, one major database platform (e.g, MySQL, Oracle, SQL Server), and one major operating system (e.g. Linux). The Senior Data Engineer also performs all aspects of the development life cycle. They will act as the senior technical programmer for the assigned enterprise system and/or application of responsibility. Finally, they will deliver results through independent contributions and through mentoring of junior engineers.
Want to help explore and build new ways to deliver content to the world?
At Ingram, our Technology team is blazing a trail by providing content distribution services to thousands of publishers with key initiatives around business intelligence, machine learning, continuous integration and omnichannel.  We support diverse people and technology that highlights innovation through SaaS platforms, metadata, cloud, and containerization.   Our teams are agile, and emphasize authenticity, creativity, and transparency upon a fact-based foundation.
The world is reading, and it is our goal to connect as many people as possible to the content they want in the simplest ways.  If you are an IT professional who strives to deliver results through collaborative partnerships, understanding what drives business, and enjoys working in a connected culture, we can’t wait to meet you!
The ideal candidate will have the following minimum qualifications:
Bachelor’s Degree in Computer Science or related field or directly related year for year experience
6 years’ experience in designing, developing, implementing, and supporting enterprise level IT solutions
We have a preference for:
6+ years of experience with writing and optimizing existing complex SQL queries
6+ years of database application development experience
Advanced knowledge of SQL relational databases, query authoring (SQL)
Experience with Vertica (projections, segmentation, columnar data)
Experience with columnar databases or non-relational databases
Knowledge of common tools for CentoOS Linux (logs, piping, redirections, grep, sed, yum)
Knowledge of Linux scripting (Python, Perl, shell scripts) and/or advanced stored procedures
Experience with architecting data modeling and meeting requirements for data visualization or reporting tools
Experience with collaborating in a cross-functional capacity across teams, building consensus and executing the necessary vision for application and other analytical needs.
Knowledge of developing in Visual Studio, SSMS and DB Visualizer
Knowledge of JIRA and Confluence
Ability to take business requirements and transpose them into technical details
The Sr Data Developer key responsibilities are:
Serves as Designer/Architect/Engineer for a major enterprise IT application.
Creates, develops, modifies, and maintains data models for internal and external facing application as part of an Agile/SCRUM engineering team
Assembles large and complex data sets that meet business requirements.
Coordinates and communicates with users, developers, and product owners to gather and understanding requirements.
Develops new design patterns, standards, documentation, etc. and works with other developers for implementation.
This list is not exhaustive
Additional Information
Perks/Benefits:
A highly competitive compensation package with generous benefits beginning first day of employment for Medical/Prescription Drug plans, HSA, Vision, Dental and Health Care FSA.
15 vacation days & 12 sick days accrued annually and 3 personal days
401K match, Life and AD&D, Employee Assistant programs, Group Legal, & more
Wellness program with access to onsite gym and basketball court for associates
Avid reader? Numerous opportunities to engage with books and authors
Free card registration at the Nashville Public Library
Discounted offers to self-publish with IngramSpark®!
Encouraged continued education with our tuition reimbursement program
Financial and in-kind opportunities to engage with non-profits in your community
Company match program for United Way donations
Volunteer opportunities and in-kind drives for non-profits throughout the year
Take breaks or brainstorm in our game room with ping pong & foosball
Casual Dress Code & Flexible Schedules (per team)
The world is reading, and Ingram Content Group (“Ingram”) connects people with content in all forms. Providing comprehensive services for publishers, retailers, libraries and educators, Ingram makes these services seamless and accessible through technology, innovation and creativity. With an expansive global network of offices and facilities, Ingram’s services include digital and physical book distribution, print-on-demand, and digital learning. Ingram Content Group is a part of Ingram Industries Inc. and includes Ingram Book Group LLC, Ingram Publisher Services LLC, Lightning Source LLC, Ingram Library Services LLC, Tennessee Book Company LLC, Ingram Content Group UK Ltd. and Ingram Content Group Australia Pty Ltd.
Ingram Content Group LLC is an affirmative action/equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, work related mental or physical disability, veteran status, sexual orientation, gender identity, or genetic information.
EEO/AA Employer/Vet/Disabled
We participate in EVerify.
EEO Poster in English
EEO Poster in Spanish
Show more
Show less","Linux, Python, Perl, Shell scripts, SQL, Vertica, Columnar databases, Nonrelational databases, Data modeling, Data visualization, Reporting tools, Agile, Scrum, Visual Studio, SSMS, DB Visualizer, JIRA, Confluence, Data warehousing, Business intelligence, Machine learning, Continuous integration, Omnichannel, SaaS platforms, Metadata, Cloud, Containerization","linux, python, perl, shell scripts, sql, vertica, columnar databases, nonrelational databases, data modeling, data visualization, reporting tools, agile, scrum, visual studio, ssms, db visualizer, jira, confluence, data warehousing, business intelligence, machine learning, continuous integration, omnichannel, saas platforms, metadata, cloud, containerization","agile, business intelligence, cloud, columnar databases, confluence, containerization, continuous integration, datamodeling, datawarehouse, db visualizer, jira, linux, machine learning, metadata, nonrelational databases, omnichannel, perl, python, reporting tools, saas platforms, scrum, shell scripts, sql, ssms, vertica, visual studio, visualization"
Senior Data Engineer  - 3352,"National Aerospace Solutions, LLC","Nashville, TN",https://www.linkedin.com/jobs/view/senior-data-engineer-3352-at-national-aerospace-solutions-llc-3776994667,2023-12-17,Nashville,United States,Mid senior,Onsite,"Title: Senior Data Engineer- 3352
Function: Business Operations
Position Type: Regular, Full-Time
Pay Type: Exempt
Grade: 36-38
US Citizenship Required
This opportunity is being offered by National Aerospace Solutions, the team selected by the U.S. Air Force to conduct Test and Operations Sustainment at the Arnold Engineering Development Complex (AEDC), Arnold Air Force Base, Tennessee. National Aerospace Solutions is a limited liability company comprised of Bechtel National Inc. (as lead company) and Sierra Lobo Inc. with teaming Subcontractors nLogic Inc. and Chugach Federal Solutions Inc. AEDC operates more than 55 aerodynamic and propulsion wind tunnels, rocket and turbine engine test cells, space environmental chambers, arc heaters, ballistic ranges and other specialized units located in six states. Many of the complex's test units have capabilities unmatched elsewhere in the United States; some are unique in the world. AEDC is one of three installations which are part of the Air Force Test Center (AFTC), one of six subordinate commands of the Air Force Materiel Command organization and an important national resource.
AEDC’s mission is to conduct developmental test and evaluation for the Nation through modeling, simulation, ground, and flight test. Execution of the mission requires a large team of highly qualified and dedicated professionals from multiple technical disciplines to effectively accomplish the objectives of our test customers.
Job Summary
This job is to develop and apply statistical and modern analytical methods to improve operations and maintenance of the Arnold Engineering Development Complex (AEDC) air and space ground test equipment and facilities. The successful candidate will work as a member of the Digital Enterprise group (DE) in the Mission Support Branch to lead implementation of data-centered projects to improve the AEDC ground test data infrastructure, facility operations, and business systems. The person selected for this role will work closely with multidisciplinary work teams throughout the NAS organization to identify opportunities for leveraging data to drive decisions and will help design and implement a data architecture working with the Digital Enterprise team.
Job Duties
Development and implement data-engineering strategies and programs.
Utilize test data sources to optimize data analytics at AEDC and suggest ways which insights obtained might be used to inform testing sustainment and operational strategies.
Automate manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
Utilize machine learning tools to select features, create, and optimize classifiers.
Define & develop approaches and demonstrate abilities to mine and analyze data from databases to drive optimization and improvement of data collected at AEDC.
Identify and assess the effectiveness of data engineering, data sources and data gathering techniques.
Utilize applicable experience in “big data,” analytics, algorithmic, custom data models, algorithms and/or machine learning approaches to help extract data that will help drive engineering decisions.
Coordinate with different multidisciplinary teams to implement models and monitor outcomes.
Define & develop processes and tools to monitor and analyze model performance and data accuracy.
Define & develop the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.
It is a condition of employment to wear company issued PPE (Personal Protective Equipment) in accordance with supervisory direction and company policy.
Identify and complete projects associated to data-engineering.
Performs other related duties as required.
Basic Qualifications:
B.S. in Computer Science, Statistics, Mathematics, Engineering, or another relevant engineering field from an accredited university program plus a minimum 7 years of progressive and relevant experience.
Current U.S. Citizenship is required.
Preferred Qualifications
Master’s or PhD degree in Computer Science, Statistics, Mathematics, Physics, Engineering, or another relevant engineering field from an accredited university program.
Experience with development lifecycle methodologies such as Agile DevSecOps.
Experience with data scripting language software like Python, Java, C++, or Ruby and SQL.
Experience with data extraction tools and processes, data ingestion, ETL, data mining, API’s, and data warehousing.
Demonstrated experience in a data engineering role.
Experience working with and creating data architectures.
Knowledge of a variety of machine learning techniques and their real-world advantages/drawbacks.
Coding knowledge and experience with multiple languages
Due to Air Force Security requirements, U.S. Citizenship is required for employment at AEDC.
NAS is an Equal Opportunity Employer of Minority/Women/Veterans/Disabled (AA/EOE). All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, sexual orientation, gender identity and expression, pregnancy, physical or mental disability, citizenship, genetic information, protected veteran status or any other characteristic protected by federal, state or local law. Applicants with a physical or mental disability, who require a reasonable accommodation for any part of the application or hiring process, may e-mail their request to
careers@nas-llc.us
.
PLEASE DO NOT SUBMIT RESUMES
to this address as they will not be considered for employment opportunities.
Show more
Show less","Data Engineering, Test Data, Data Analytics, Machine Learning, SQL, Python, Java, C++, Ruby, ETL, Data Mining, Data Warehousing, Data Architectures, Agile DevSecOps","data engineering, test data, data analytics, machine learning, sql, python, java, c, ruby, etl, data mining, data warehousing, data architectures, agile devsecops","agile devsecops, c, data architectures, data engineering, data mining, dataanalytics, datawarehouse, etl, java, machine learning, python, ruby, sql, test data"
Data Security Engineer,Insight Global,"Nashville, TN",https://www.linkedin.com/jobs/view/data-security-engineer-at-insight-global-3770175899,2023-12-17,Nashville,United States,Mid senior,Onsite,"Title: Data Security Engineer
Hybrid Schedule (1-2 days on site in Brentwood)
6-month contract with extensions
Must Haves:
Experience managing, full to back, data loss prevention tools.
Experience with Netskope or Proofpoint
Experience with data encryption and monitoring tools
Strong understanding of Incident remediation and data at rest process development
At least one security certification
Looking for an Engineer to protect information assets. They will be configuring and improving DLP policies. This will also include maintaining these tools to monitor, detect, and prevent leakage outside of the corporation.
Show more
Show less","Data Loss Prevention, Netskope, Proofpoint, Data Encryption, Monitoring Tools, Incident Remediation, Data at Rest Process Development, Security Certification, DLP Policies, Leakage Prevention","data loss prevention, netskope, proofpoint, data encryption, monitoring tools, incident remediation, data at rest process development, security certification, dlp policies, leakage prevention","data at rest process development, data encryption, data loss prevention, dlp policies, incident remediation, leakage prevention, monitoring tools, netskope, proofpoint, security certification"
Senior Data Engineer,AllianceBernstein,"Nashville, TN",https://www.linkedin.com/jobs/view/senior-data-engineer-at-alliancebernstein-3702528894,2023-12-17,Nashville,United States,Mid senior,Onsite,"Who You'll Work With
Fixed Income Investment Technology (“FIIT”) group builds software that the Fixed Income business of AB uses in performing functions such as Fundamental Research, Quantitative research, Portfolio Management, Order Generation, Trading and Middle office and BackOffice operations. It partners with business to understand their challenges and help them by providing innovative technology solutions. We re-engineer the process where applicable in collaboration with business to help scale their business and be efficient in this dynamic market conditions. Our Data Warehouse ingests and egresses critical reference and transaction data from up-stream and down-stream systems.
We are seeking a Nashville based senior professional to join our Fixed Income Investment Technology team. With complex architecture which is core to how we service FI clients (and beyond), this is a critical role where someone with industry experience will be a good fit allowing us to scale our processes. Data is a constantly growing entity and we need to have a clean architecture to align our data with supporting investment team needs. The current data team has a lot of junior – mid level staff and needs someone with strong technical skills and leadership skills to design scalable infrastructure. The ideal candidate would have experience with designing, implementing, maintaining, monitoring and performance tuning ETL and ELT pipelines using technologies like Azure, Airflow, Python, Oracle and SQL server. They would be proficient in various data patterns like Gamma, Kappa, Data mesh / fabric, AKS, Data Lakes and Delta Lake
What You'll Do
The primary project that the candidate will contribute to all modules within Fixed Income technology. These modules support Senior Portfolio Managers, Portfolio Managers, Assistant Portfolio Managers, Fundamental and Quantitative research analysts, Traders, Middle-office, and Operations functions.
Data Management Tools and Technologies:
Proficiency in data management tools such as SQL-based databases (e.g., SQL Server, MySQL, PostgreSQL, Oracle) and NoSQL databases (e.g., MongoDB, Cassandra).
Experience with data integration and ETL tools.
Knowledge of data warehousing concepts and technologies, including star schemas, data marts, and data lakes.
Familiarity with cloud-based data platforms (e.g., AWS Redshift, Google BigQuery, Azure SQL Data Warehouse) and related services.
Understanding of big data technologies such as Hadoop, Spark, and related ecosystem components (e.g., HDFS, Hive, Pig).
Data Modeling and Design:
Proficient with designing an optimal Data warehouse and Operational data store using modern cloud patterns.
Proficient in data modeling techniques and tools, such as ER modeling and UML diagrams.
Experience in designing and implementing data schemas, including conceptual, logical, and physical models.
Knowledge of dimensional modeling and familiarity with industry-standard frameworks such as Kimball or Data Vault.
Ability to optimize database performance through indexing, partitioning, and query tuning.
Data Governance and Compliance:
Understanding of data governance frameworks, data stewardship, and data ownership concepts.
Knowledge of data privacy regulations (e.g., GDPR, CCPA) and experience implementing data protection measures.
Familiarity with data security best practices, encryption methods, access controls, and user authentication mechanisms.
Data Quality Assurance:
Experience in data profiling techniques and tools to assess data quality and identify data anomalies.
Proficiency in data cleansing methods, data validation techniques, and data transformation rules.
Knowledge of data quality frameworks, metrics, and best practices for measuring and monitoring data quality.
Programming and Scripting:
Proficiency in one or more programming languages such as Python, Java, or Scala for data manipulation and transformation tasks.
Experience in writing complex SQL queries and stored procedures for data retrieval and manipulation.
Familiarity with scripting languages such as Bash or PowerShell for automation and process improvement.
Data Visualization and Reporting:
Knowledge of data visualization tools such as Tableau, Power BI, or QlikView for creating interactive and meaningful visualizations.
Ability to design and develop reports, dashboards, and data-driven presentations to communicate insights to stakeholders.
Cloud and Distributed Computing:
Understanding of cloud computing concepts and experience working with cloud platforms (e.g., AWS, Azure, or GCP).
Knowledge of distributed computing frameworks such as Apache Spark for large-scale data processing and analytics.
Software Development and Agile Practices:
Familiarity with software development methodologies, particularly Agile/Scrum, and experience working in Agile development teams.
Understanding of software development lifecycle (SDLC) processes and experience collaborating with development teams.
What We're Looking For
Educational Background: A bachelor's or master's degree in Computer Science, Software Engineering, Data Science, or a related field is often required. Advanced degrees can be advantageous.
Programming Skills: Proficiency in programming languages commonly used in data engineering, such as Python, Java, Scala, or SQL.
Big Data Technologies: Strong knowledge of big data technologies and frameworks, such as Apache Hadoop, Apache Spark, Apache Kafka, Apache Hive, or Apache Flink.
Database Management: Experience with various database systems, both traditional relational databases (e.g., MySQL, PostgreSQL) and NoSQL databases (e.g., MongoDB, Cassandra).
Data Modeling: Expertise in designing and implementing data models to facilitate efficient data storage, retrieval, and analysis.
Data Warehousing: Familiarity with data warehousing concepts and technologies, like Azure Synapse, Amazon Redshift, Google BigQuery, or Snowflake.
ETL/ ELT: Experience in designing and implementing ETL / ELT processes to extract, transform, and load data from various sources into the data warehouse.
Data Pipeline Management: Ability to create and manage data pipelines for data ingestion, integration, and transformation.
Experience with scheduling tools like Airflow, Control-M or Auto-sys
Cloud Services: Experience with cloud computing platforms like AWS, Azure, or Google Cloud, and familiarity with their data services.
Data Governance and Security: Knowledge of data governance principles and best practices, including data security and privacy measures.
Data Quality and Testing: Understanding of data quality assessment methods and proficiency in data testing techniques.
Performance Optimization: Skills to optimize data pipelines and queries for better performance and scalability.
Problem-Solving and Troubleshooting: Strong problem-solving abilities and the capability to identify and resolve complex data engineering issues.
Team Leadership: Leadership and mentoring skills, as senior data engineers often lead and guide other members of the data engineering team.
Communication: Effective communication skills to collaborate with cross-functional teams, communicate technical concepts to non-technical stakeholders, and document data engineering processes.
Who We Are
We are a leading global investment management firm offering high-quality research and diversified investment services to institutional clients, retail investors, and private-wealth clients in major markets around the globe. With over 4,000 employees across 57 locations operating in 26 countries and jurisdictions, our ambition is simple: to be the most trusted investment firm in the world. We realize that it's our people who give us a competitive advantage and drive success in the market, and our goal is to create an inclusive culture that rewards hard work.
Our culture of intellectual curiosity and collaboration creates an environment where you can thrive and do your best work. Whether you're producing thought-provoking research, identifying compelling investment opportunities, infusing new technologies into our business or providing thoughtful advice to our clients, we are fully invested in you. If you're ready to challenge your limits and empower your career, join us!
AB does not discriminate against any employee or applicant for employment on the basis of race, color, religion, creed, ancestry, national origin, sex, age, disability, marital status, citizenship status, sexual orientation, gender identity, military or veteran status or any other basis that is prohibited by applicable law. AB’s policies, as well as practices, seek to ensure that employment opportunities are available to all employees and applicants, based solely on job-related criteria.
In accordance with applicable law, the minimum and maximum base annual salary for this role is as follows:
Base Salary Range: $190,000 to $200,000
Actual base salaries may vary based on factors including but not limited to education, training, experience, past performance, and other job-related factors. Base salary is just one component of total compensation at AB, which may include, depending on eligibility, commissions, year-end incentive compensation, short- and long-term incentives and Department-specific awards. In addition AB provides a variety of benefits to eligible employees, including health insurance coverage, an employee wellness program, life and disability insurance, a retirement savings plan, paid holidays, sick and vacation time off
Nashville, Tennessee
Show more
Show less","SQL, NoSQL, Python, Java, Scala, Airflow, Oracle, Azure, AWS, Hadoop, Spark, BigQuery, Redshift, SQL Data Warehouse, Delta Lake, Data Vault, Data Lake, Data Mesh, AWS Redshift, Kimball, Tableau, Power BI, QlikView, Apache Kafka, Apache Hive, Apache Flink, MySQL, PostgreSQL, MongoDB, Cassandra, Azure Synapse, Snowflake, ControlM, Autosys, ETL, ELT, Data Engineering, Cloud Computing, Databricks","sql, nosql, python, java, scala, airflow, oracle, azure, aws, hadoop, spark, bigquery, redshift, sql data warehouse, delta lake, data vault, data lake, data mesh, aws redshift, kimball, tableau, power bi, qlikview, apache kafka, apache hive, apache flink, mysql, postgresql, mongodb, cassandra, azure synapse, snowflake, controlm, autosys, etl, elt, data engineering, cloud computing, databricks","airflow, apache flink, apache hive, apache kafka, autosys, aws, aws redshift, azure, azure synapse, bigquery, cassandra, cloud computing, controlm, data engineering, data lake, data mesh, data vault, databricks, delta lake, elt, etl, hadoop, java, kimball, mongodb, mysql, nosql, oracle, postgresql, powerbi, python, qlikview, redshift, scala, snowflake, spark, sql, sql data warehouse, tableau"
Data Analyst,Premier Franchise Management,"Franklin, TN",https://www.linkedin.com/jobs/view/data-analyst-at-premier-franchise-management-3787914422,2023-12-17,Nashville,United States,Mid senior,Onsite,"Job Summary:
We are actively seeking a Data Analyst to work full-time in our Franklin, Tennessee location in support of our ever-growing national franchise platform. The ideal candidate will have at least two years of experience in an environment where aggregating data from disparate data sources, building/maintaining operations dashboard, using Power BI to build data sets for operations usage, and assisting with the month-end close and annual audit were part of their daily responsibilities. The essential functions of this role will be finding insights in large datasets, synthesizing, and communicating results to drive business results and growth. This person will work cross-functionally with the Finance, Accounting, Sales, and Operations Departments. This position requires a successful candidate to be a self-starter, managing and providing consistent and reliable reporting and analysis, in addition to utilizing primary and secondary research methods.
Job Responsibilities include but are not limited to:
Develop and implement databases, data collection systems, data analytics, and other strategies that optimize statistical efficiency and quality.
Acquire data from internal and external sources to construct varying standardized reporting and dashboarding for utilization by the Executive Leadership Team and other stakeholders.
Filter and “clean” data by developing and reviewing variance reports, period-over-period analyses, and performance indicators.
Identify, analyze, and interpret trends and patterns in complex data sets.
Provide analytical reporting to identify key market trends and insights.
Work with the management team to prioritize business data and information needs.
Monitor performance and quality control plans to identify improvements.
Ideal candidate will possess:
Coachability, friendly demeanor, and willingness to demonstrate ownership of job and department functions within our company while also being an amazing team member to the rest of our PFM Family!
Strong time-management skills and an ability to organize and coordinate multiple concurrent projects.
Proficiency in Microsoft Excel, PowerBI, Tableau, and an aptitude for learning new software and systems.
Technical expertise regarding data models, database design development, and segmentation techniques.
Strong knowledge of financial reporting packages, databases, understanding of programming language(s), with the ability to run queries, write reports, and present findings with strong data and graphs.
Ability to maintain the confidentiality of information related to the company and its employees.
Excellent verbal and written communication skills.
Highest attention to detail and accuracy with creative critical thinking and problem-solving skills.
Position Details:
This position is based in the Finance/Accounting department. The role is full-time, exempt salaried at our Franklin, Tennessee location. The schedule is day shift Monday – Friday onsite, with additional flexibility needed around important deadlines or cutoffs. We offer a full comprehensive benefits package to include Medical, Dental, Vision, and Life insurance; Paid Time Off; Sick Time; and 401(k) retirement plan.
About Premier Franchise Management:
For over 30 years, the Premier brand has been making swimming pool dreams become reality. We Build and Service residential pools and spas nationwide with over 160 locations across the United States. Premier Franchise Management is the Franchisor for all our locations and provides support to help grow each individual business in hopes to grow the brand internationally into the future!
Company Culture
– every individual that works for the Premier brand is an extension of our family. Our exciting culture and family values are the utmost important mission. From the CEO to our franchise pool builders, everyone is treated with the highest level of respect and held to the greatest level of standards in their work and how they carry themselves both personally and professionally.
Core Values
– from a humble background, our CEO has built this organization off strict core values to carry the continuance of our company culture into every home we enter to build a pool. These values are as follows:
Integrity. The quality of being honest and having strong moral principles – 100% honest, 100% of the time!
Work Ethic. The principle that hard work is intrinsically virtuous or worthy of reward – Give your customers, your company, and your co-workers maximum effort…everyday!
Team Player. The ability to put aside your personal goals and work well with your co-workers towards a common goal – Support and work together with everyone within the organization!
Accountability. The willingness to accept responsibility for one’s actions – Acknowledge your mistakes, learn from them and commit to ensuring the same mistakes do not reoccur!
Quality of Work. All work shall be performed in a professional manner in accordance with the predetermined specifications, and expectations – Ensure that your work is completed promptly and void of error!
Learn more about our company, read our success story, and learn about our future growth and opportunity for individuals to write their own story by visiting our website at .
Powered by JazzHR
H6fgjsWshU
Show more
Show less","Data Analysis, Data Aggregation, Data Visualization, Power BI, Financial Reporting, Data Modeling, Database Design, SQL, Tableau, Microsoft Excel, Business Intelligence, Data Interpretation, Trend Analysis, Data Warehousing, Data Mining, Data Cleaning, Statistics, Programming Languages, Report Writing, Presentation Skills, Communication Skills, Problem Solving, Critical Thinking, Time Management, Project Management, Teamwork, Attention to Detail, Accuracy, Confidentiality","data analysis, data aggregation, data visualization, power bi, financial reporting, data modeling, database design, sql, tableau, microsoft excel, business intelligence, data interpretation, trend analysis, data warehousing, data mining, data cleaning, statistics, programming languages, report writing, presentation skills, communication skills, problem solving, critical thinking, time management, project management, teamwork, attention to detail, accuracy, confidentiality","accuracy, attention to detail, business intelligence, communication skills, confidentiality, critical thinking, data aggregation, data cleaning, data interpretation, data mining, dataanalytics, database design, datamodeling, datawarehouse, financial reporting, microsoft excel, powerbi, presentation skills, problem solving, programming languages, project management, report writing, sql, statistics, tableau, teamwork, time management, trend analysis, visualization"
Consulting Data Engineer,HCA Healthcare,"Nashville, TN",https://www.linkedin.com/jobs/view/consulting-data-engineer-at-hca-healthcare-3688797961,2023-12-17,Nashville,United States,Mid senior,Onsite,"Description
Introduction
Do you want to join an organization that invests in you as a Consulting Data Engineer? At HCA Healthcare, you come first. HCA Healthcare has committed up to $300 million in programs to support our incredible team members over the course of three years.
Benefits
HCA Healthcare offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn More About Employee Benefits
Note: Eligibility for benefits may vary by location.
You contribute to our success. Every role has an impact on our patients’ lives and you have the opportunity to make a difference. We are looking for a dedicated Consulting Data Engineer like you to be a part of our team.
Job Summary And Qualifications
The Consulting Data Engineer is part of the CSG Data & Analytics organization of HCA. This team, both technically-focused and customer-facing, is responsible for the design, development, and support of HCA’s enterprise clinical reporting. Serving as a subject matter expert in architecture and data transformation, the Consulting Data Engineer will be responsible for optimizing the data processes needed to power a broad range of large-scale BI products. This position requires working with cross-functional teams to translate business and clinical needs into reporting structures that are easily consumed by BI, and a willingness to learn the subject-matter necessary to develop and implement clinical data requirements.
The Consulting Data Engineer is a seasoned professional in data structures, warehousing, and BI who thrives on a team dedicated to developing and supporting products that promote clinical care excellence at HCA Facilities.
General Responsibilities
Architect and optimize data solutions to power large-scale BI applications
Design ETL processes that integrate disparate data into a common data model for BI delivery (gather requirements, design, develop, test, and debug)
Understand and translate strategic business needs into a flexible and scalable data architecture
Act as a mentor and provide direction to colleagues with less experience
Serve as the first-line responder for issues related to the data, architecture, and ETL powering the BI product portfolio
Support data-related questions and requests received from Corporate, Facility, and Division stakeholders (customer-focused attitude and willingness to troubleshoot)
Facilitate effective communication among all team members and leadership to ensure comprehensive understanding of needs, expectations, and requirements
Maximize productivity through good time management, coordination, and communication
Perform other related duties as assigned to support the CSG Analytics Team
Experience
10+ years of relevant work experience
EDUCATION
Bachelor’s Degree Required
Qualifications
Seasoned data professional and subject matter expert with a full understanding of data tools and concepts; ability to resolve a wide range of complex issues in creative ways
Advanced knowledge and application of SQL required, with preferred experience in Microsoft SQL and Teradata
Extensive expertise/experience in the areas of data structures and data warehousing
Advanced knowledge of ETL processes and capabilities (e.g. SSIS)
Extensive expertise/experience in data analysis, modeling and visualization
Experience with large-scale cloud implementation of data solutions – Google Cloud Platform (GCP) preferred
Experience using a distributed version control system (e.g., GitHub)
Comfortable working in a high profile and fast-paced environment
Ability to troubleshoot and solve complex problems
Results-oriented with a demonstrated commitment and willingness to do what it takes to get the job done
Proven ability to manage and prioritize multiple concurrent projects within deadlines
Proven ability to work in a team environment or independently as needed
Bachelor’s degree in Computer Science, Engineering, or related field
Healthcare experience preferred
HCA Healthcare (Corporate)
, based in Nashville, Tennessee, supports a variety of corporate roles from business operations to administrative positions. Like our colleagues in any HCA Healthcare hospital, our corporate campus employees enjoy unparalleled
resources and opportunities
to reach their potential as healthcare leaders and innovators. From market rate compensation to continuing education and
career advancement opportunities
, every person has a solid foundation for success. Nashville is also home to our
Executive Development Program
, where exceptional employees are groomed to take on CNO- and COO-level roles in our hospitals. This selective program focuses on ethics, leadership and the financial and clinical knowledge required of professionals at this level of the industry.
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""Good people beget good people.""- Dr. Thomas Frist, Sr.
HCA Healthcare Co-Founder
We are a family 270,000 dedicated professionals! Our Talent Acquisition team is reviewing applications for our Consulting Data Analyst opening. Qualified candidates will be contacted for interviews.
Submit your resume today to join our community of caring!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","SQL, Microsoft SQL, Teradata, Data warehousing, ETL, SSIS, Data analysis, Data modeling, Data visualization, Cloud implementation, Google Cloud Platform (GCP), GitHub, Distributed version control system, Computer Science, Engineering","sql, microsoft sql, teradata, data warehousing, etl, ssis, data analysis, data modeling, data visualization, cloud implementation, google cloud platform gcp, github, distributed version control system, computer science, engineering","cloud implementation, computer science, dataanalytics, datamodeling, datawarehouse, distributed version control system, engineering, etl, github, google cloud platform gcp, microsoft sql, sql, ssis, teradata, visualization"
Senior Data Engineer,TailorCare,"Nashville, TN",https://www.linkedin.com/jobs/view/senior-data-engineer-at-tailorcare-3785764815,2023-12-17,Nashville,United States,Mid senior,Remote,"About TailorCare
TailorCare is transforming the experience of specialty care. Our comprehensive care program takes a deeply personal, evidence-based approach to improving patient outcomes for joint, back, and muscle conditions. By combining a careful assessment of patients' symptoms, health histories, preferences, and goals with predictive data and latest evidence-based guidelines, we help patients choose—and navigate—the most effective treatment pathway for them, every step of the way.
TailorCare values the experiences and perspectives of individuals from all backgrounds. We are a highly collaborative, curious, and determined team passionate about scaling a high-growth start-up to improve the lives of those in pain. TailorCare is a remote-first company with hybrid offices in New York City and Nashville.
We are seeking a stellar senior data engineer to join our Data team. You will play an integral role in the creation, maintenance and ongoing enhancement of our data platform and infrastructure. You will own the development of our cloud infrastructure and data pipelines to enable our stakeholders including analytics and reporting, data science, clinical operations, and product and engineering. You will mentor others on the team and partner with the Senior Director of Data to create and execute on the data vision and roadmap. We generate and capture our own data as well as integrate with outside data providers. This is a high impact role at TailorCare with ample opportunity for leadership and growth.
Primary Responsibilities:
Build and enhance data pipelines to bring together information from different source systems
Optimize and automate data infrastructure to better support analytics and reporting
Practice agile principles while building a culture of DevOps
Write traditional code and server-less functions using the language best suited for the task, typically Scala, Python, and SQL
Build APIs and data microservices to share our data across and outside of the organization, and write interfaces to public data sets to enrich our analytics data stores
Collaborate across functions and departments to deliver to meet business objectives
Qualifications:
Bachelor's degree in Computer Science, Computer Engineering, Information Systems, or equivalent experience
7+ years' experience building scalable, real-time, and high-performance cloud data lake solutions
In depth healthcare data standardization and automation experience
Skills:
Strong experience with relational SQL and programming languages such as Python, Scala, or Java
Experience with source control tools such as GitHub and CI/CD processes
Experience with distributed computing technologies such as Hadoop/Spark
Built pipelines using Databricks and well versed with their APIs
Experience working with Big Data streaming services such as Kinesis, Kafka, etc.
Familiar with AWS cloud infrastructure and security
Experience with IaaC tools like Terraform
Experience working in a fast-paced environment while staying organized and focused
Strong communication skills and sound decision-making skills
Experience with healthcare coding standards, including CPT, HCPCS, ICD10 Procedure, ICD10 Diagnostic, DRG, MS-DRG, APR-DRG, etc.
Propensity for building infrastructure and designing processes rather than relying on those already in place
What's In It For You
Meaningful work each day, we care deeply about our mission, our patients, and each-other.
Work from anywhere in the US that best fits your lifestyle, or, for those that enjoy an in-person environment, join teammates in our hybrid hubs in NYC or Nashville.
Rich PTO and holiday plans to ensure you have time away to rest and recharge
We offer paid parental leave, support a healthy work-life integration, and offer work flexibility – we love to talk about our pets and families.
Medical, dental, vision, life, disability, wellness resources, and an employer HSA contribution all from Day 1.
We are committed to fair and equitable pay for all employees, and we help you achieve your future goals with an employer match 401k.
An inclusive workplace where you can lean on your teammates, offer candid feedback, and bring your true self to work each day.
TailorCare seeks to recruit and retain staff from diverse backgrounds and encourages qualified candidates to apply. TailorCare is an equal opportunity employer and does not discriminate on the basis of age, sex, gender identity/expression, sexual orientation, color, race, creed, national origin, ancestry, religion, marital status, political belief, physical or mental disability, pregnancy, military, or veteran status.
Show more
Show less","Data Engineering, Cloud Infrastructure, Data Pipelines, DevOps, SQL, Python, Scala, GitHub, CI/CD, Hadoop, Spark, Databricks, Kinesis, Kafka, AWS, Terraform, Relational SQL, Healthcare Coding Standards, CPT, HCPCS, ICD10 Procedure, ICD10 Diagnostic, DRG, MSDRG, APRDRG","data engineering, cloud infrastructure, data pipelines, devops, sql, python, scala, github, cicd, hadoop, spark, databricks, kinesis, kafka, aws, terraform, relational sql, healthcare coding standards, cpt, hcpcs, icd10 procedure, icd10 diagnostic, drg, msdrg, aprdrg","aprdrg, aws, cicd, cloud infrastructure, cpt, data engineering, databricks, datapipeline, devops, drg, github, hadoop, hcpcs, healthcare coding standards, icd10 diagnostic, icd10 procedure, kafka, kinesis, msdrg, python, relational sql, scala, spark, sql, terraform"
Senior Data Engineer (Remote Available),Vanderbilt University Medical Center,Nashville Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-remote-available-at-vanderbilt-university-medical-center-3713091902,2023-12-17,Nashville,United States,Mid senior,Remote,"Discover Vanderbilt University Medical Center:
Located in Nashville, Tennessee, and operating at a global crossroads of teaching, discovery, and patient care, VUMC is a community of diverse individuals who come to work each day with the simple aim of changing the world. It is a place where your expertise will be valued, your knowledge expanded, and your abilities challenged. Vanderbilt Health recognizes that diversity is essential for excellence and innovation. We are committed to an inclusive environment where everyone has the chance to thrive and where your diversity of culture, thinking, learning, and leading is sought and celebrated. It is a place where employees know they are part of something that is bigger than themselves, take exceptional pride in their work and never settle for what was good enough yesterday. Vanderbilt’s mission is to advance health and wellness through preeminent programs in patient care, education, and research.
Organization:
HealthIT Data Platform Svcs
Job Summary:
Job Summary
The Data Platform Services team is seeking an experienced Data Engineer with an inquisitive and analytical mindset to join our team. In your role, you will be responsible for developing, maintaining, and optimizing our cloud-based data infrastructure, as well as designing and implementing efficient data extraction and ingestion processes. You will also be responsible for working with stakeholders to identify data needs, investigating opportunities to improve platform scalability, building generic solutions for patterned problems, and ensuring data accuracy and integrity.
Key Responsibilities
Independently and full proficient to:
Design and develop performant data pipelines that support a variety of source system types (flat files, APIs, databases, etc.)
Troubleshoot complex issues with production pipelines and work with internal stakeholders to deploy fixes
Develop tools and services used by other teams to create, test, and deliver data-related assets
Design and develop cloud infrastructure-as-code to support the data platform
Participate in code review sessions for merge requests and assist in mentoring developers in best development practices
May assist with onboarding and training new employees as needed
Technical Capabilities
Minimum Qualifications
Bachelor's Degree
4+ years C# and/or Python in an OOP paradigm experience
2+ years with SQL experience
2+ years implementing cloud solutions with Azure, AWS, or GCP
2+ years with containerization with Docker or Podman
Preferred Qualifications
2+ years with CI/CD automation in GitLab, GitHub, or Azure DevOps
Experience with REST data services, APIs, and microservices
Experience with Infrastructure as Code solutions using Terraform, Bicep, or ARM
Experience with Git
Bonus Qualifications
Experience with Databricks or Apache Spark
Experience with PowerShell and/or Bash
GitHub profile link to some personal code examples
Our professional administrative functions include critical supporting roles in information technology and informatics, finance, administration, legal and community affairs, human resources, communications and marketing, development, facilities, and many more.
At our growing health system, we support each other and encourage excellence among all who are part of our workforce. High-achieving employees stay at Vanderbilt Health for professional growth, appreciation of benefits, and a sense of community and purpose.
Core Accountabilities:
Organizational Impact: Independently delivers on objectives with understanding of how they impact the results of own area/team and other related teams. Problem Solving/ Complexity of work: Utilizes multiple sources of data to analyze and resolve complex problems; may take a new perspective on existing solution. Breadth of Knowledge: Has advanced knowledge within a professional area and basic knowledge across related areas. Team Interaction: Acts as a ""go-to"" resource for colleagues with less experience; may lead small project teams.
Core Capabilities :
Supporting Colleagues: - Develops Self and Others: Invests time, energy, and enthusiasm in developing self/others to help improve performance e and gain knowledge in new areas. - Builds and Maintains Relationships: Maintains regular contact with key colleagues and stakeholders using formal and informal opportunities to expand and strengthen relationships. - Communicates Effectively: Recognizes group interactions and modifies one's own communication style to suit different situations and audiences. Delivering Excellent Services: - Serves Others with Compassion: Seeks to understand current and future needs of relevant stakeholders and customizes services to better address them. - Solves Complex Problems: Approaches problems from different angles; Identifies new possibilities to interpret opportunities and develop concrete solutions. - Offers Meaningful Advice and Support: Provides ongoing support and coaching in a constructive manner to increase employees' effectiveness. Ensuring High Quality: - Performs Excellent Work: Engages regularly in formal and informal dialogue about quality; directly addresses quality issues promptly. - Ensures Continuous Improvement: Applies various learning experiences by looking beyond symptoms to uncover underlying causes of problems and identifies ways to resolve them. - Fulfills Safety and Regulatory Requirements: Understands all aspects of providing a safe environment and performs routine safety checks to prevent safety hazards from occurring. Managing Resources Effectively: - Demonstrates Accountability: Demonstrates a sense of ownership, focusing on and driving critical issues to closure. - Stewards Organizational Resources: Applies understanding of the departmental work to effectively manage resources for a department/area. - Makes Data Driven Decisions: Demonstrates strong understanding of the information or data to identify and elevate opportunities. Fostering Innovation: - Generates New Ideas: Proactively identifies new ideas/opportunities from multiple sources or methods to improve processes beyond conventional approaches. - Applies Technology: Demonstrates an enthusiasm for learning new technologies, tools, and procedures to address short-term challenges. - Adapts to Change: Views difficult situations and/or problems as opportunities for improvement; actively embraces change instead of emphasizing negative elements.
Position Qualifications:
Responsibilities:
Certifications:
Work Experience:
Relevant Work Experience
Experience Level:
5 years
Education:
Bachelor's
Vanderbilt Health recognizes that diversity is essential for excellence and innovation. We are committed to an inclusive environment where everyone has the chance to thrive and to the principles of equal opportunity and affirmative action. EOE/AA/Women/Minority/Vets/Disabled
Show more
Show less","Data Engineer, C#, Python, OOP, SQL, Azure, AWS, GCP, Docker, Podman, GitLab, GitHub, Azure DevOps, REST data services, APIs, Microservices, Terraform, Bicep, ARM, Databricks, Apache Spark, PowerShell, Bash","data engineer, c, python, oop, sql, azure, aws, gcp, docker, podman, gitlab, github, azure devops, rest data services, apis, microservices, terraform, bicep, arm, databricks, apache spark, powershell, bash","apache spark, apis, arm, aws, azure, azure devops, bash, bicep, c, databricks, dataengineering, docker, gcp, github, gitlab, microservices, oop, podman, powershell, python, rest data services, sql, terraform"
Data Analyst III,Modea,"Nashville, TN",https://www.linkedin.com/jobs/view/data-analyst-iii-at-modea-3787769841,2023-12-17,Nashville,United States,Mid senior,Remote,"Introduction to Position:
A Data Analyst III at Modea usually works with individual clients, primarily large healthcare organizations, to manage, improve and report on their digital properties (apps and websites).
A Data Analyst III is an advanced role, filled by someone with previous experience as an analyst who has proven their ability to: successfully work autonomously, execute at a high level of competence, effectively manage up and consult clients, and proactively identify areas for additional value-add.
Essential Duties and Responsibilities:
Comfortable working in a cross-functional team environment with minimal supervision and support.
Work both collaboratively and independently.
Effectively Communicate with both internal teams and external clients.
Build and maintain strong relationships with other teams at Modea and to client teams and stakeholders.
Maintain accountability to Modea’s best practices - log time accurately and manage work and time effectively.
Feels comfortable presenting findings, analysis and recommendations to clients with minimal to moderate support.
Can mentor other team members in a formal direct report or informal mentoring setting.
Able to offer meaningful feedback and insights across teams in formal and informal settings (comfortable offering perspectives when necessary)
Advanced to expert in the primary toolkit
Fluency in popular web analytics tools; Google Analytics, Adobe Analytics, Piwik Pro, Matomo and/or Snowplow analytics preferred.
Deep knowledge of the digital analytics landscape – what tactics can be used and thorough understanding of strategies for measurement.
Able to troubleshoot most issues with implementation and configuration of web analytics tools without needing support.
Experienced with configuring, using and maintaining popular cloud data storage tools (Google Coud Platform, AWS, Microsoft Azure).
Able to conduct advanced UX or clickstream analysis to answer almost all client questions.
Able to explain detailed metrics and tradeoffs in tracking philosophy - the ‘why’ of Modea’s tracking choices. Analysts should be able to ask the right questions and provide support to other areas where appropriate.
Able to run basic SEO Audits and communicate not only their findings but also basic recommendations for improvements.
Comfortable implementing and managing secure and HIPAA-compliant data collection and data storage.
Competent Coder
Able to conduct analysis in R and/or Python, using pre-configured and common API methods to query and pull down data from established sources (GA as a common example, using the GoogleAnalyticsR Library in R or the equivalent in Python) and able to pull data from unknown sources that aren’t pre-configured. Working knowledge of API structures.
Able to read and understand code in both R and Python and have a solid knowledge of Git and how to push code to a shared repository, as well as best practices of version control and collaborative coding.
Able to stretch with help
Analysts should be able to tackle more advanced analysis and more complex problems with support and supervision from a more experienced team member.
A successful Analyst III will demonstrate the following qualities:
Curiosity: Our clients often have unpredictable questions, and analysts should be both self-directed in generating their own queries and areas for investigation as well as interested in digging into client questions for ad-hoc reporting and great service.
Adaptability: Our toolkit is changing regularly, and our clients often use a wide range of tools across a range of engagements. An ability to work with varied tools, concepts and datasets is key.
Excellent communication skills: We ask analysts to present their findings and results to clients and to internal teams frequently and through a range of methods (oral, written, visual) Communicating the work is a huge part of the consultant’s job!
Minimum Requirements:
3 years of experience with digital analytics or related analytical field
Experience implementing, maintaining and reporting from popular Digital Analytics platforms - GA, Adobe, Piwik Pro, Snowplow, etc.
Bachelor's Degree or equivalent work experience
Certification or demonstrated experience equivalency with cloud data management systems - Google Cloud Platform, Amazon AWS, Microsoft Azure, etc.
Experienced and knowledgeable about existing tools and software in the digital analytics space - a working knowledge of common options and their drawbacks and advantages.
Must be an experienced and polished oral and written communicator
Work environment:
Modeans are able to work at any location in Virginia, Tennessee, or North Carolina, unless as otherwise required by job responsibilities. We offer modern offices located in Nashville, TN and Blacksburg, VA. Our office provides an indoor space with moderate temperatures and noise levels. This allows employees to choose a fully-office, fully-remote, or hybrid work experience. Employees may occasionally need to travel to our offices or other locations for items such as training or client visits. We will also provide employees with tools to do their job at home as well as in the office.
Who is Modea?
We are a privately owned, technology consultancy with a mission to infuse greater control, transparency, and choice into the consumer healthcare experience. To do this, we work with large healthcare providers to help them build out a digital roadmap, then design and build mobile, web, and other digital experiences. We fundamentally believe that this work helps consumers have better information and access to care, and our clients to build stronger customer relationships.
Here are a few ways to get to know our work in healthcare:
Learn about our award-winning EHR-integrated mobile app development for Children’s Wisconsin. This app delivers a fully integrated MyChart to allow parents to manage their children’s health in one place.
Discover Luminis Health’s website development that was built with a firm accessibility standard and patient-centric solutions.
Check out the web analytics and consumer research we completed to help Lurie Children’s drive the design decision-making process and ultimately create a more user-friendly site.
Review our design research and Drupal work used to create a Healthcare Social Network experience for the American Medical Association.
If you’re interested in learning more, we encourage you to dive into some of our other work.
Here are a few other reasons why you should apply:
First, you’ll love the work we do and the people you’ll get to do it with. We’ve assembled an exceptionally smart and hardworking team and you’ll enjoy the work associations and general camaraderie with other “Modeans.” We also work with great clients who value our contributions and work side-by-side with us.
Second, Modea offers a great benefits package that provides both a fun and enjoyable workplace and great perks that will improve your quality of life:
Generous paid time off
Generous health, dental, and vision insurance
401k with fully vested company matching
Long term & short term disability
Some of these benefits, like parties and snacks, have had to adapt to the remote work environment we are currently in, but our operations team has found innovative ways to support our employees. We take care of our team.
Physical Demands:
Must be able to remain in a stationary position during the duration of the workday. Individual must have the ability to constantly operate a computer. Must have the ability to converse with and exchange information to clients and coworkers both in person and through a computer. Capable of transporting items of 15lbs or less to and from work.
You must be legally authorized to work in the United States for this position.
Modea participates in the E-Verify program.
Modea is committed to a diverse and inclusive workplace. Modea is an equal opportunity employer and does not discriminate on the basis of rage, color, religion, national origin, gender, gender identity, gender expression, sexual orientation, veteran status, disability, age, or other legally protected status.
We aspire to be an highly inclusive employer – providing fair and appropriate opportunities to as many people from different backgrounds as possible, being welcoming and meeting their needs as best we can. We are committed to keeping great people representative of a variety of backgrounds, perspectives, and skills, not just because it's the right thing to do, but because we believe it makes Modea stronger.
You must be legally authorized to work in the United States for this position.
Modea participates in the E-Verify program.
Modea is committed to a diverse and inclusive workplace. Modea is an equal opportunity employer and does not discriminate on the basis of race, color, religion, national origin, gender, gender identity, gender expression, sexual orientation, veteran status, disability, age, or other legally protected status.
Powered by JazzHR
tIXVR6ww9d
Show more
Show less","Data Analytics, Digital Analytics, Google Analytics, Adobe Analytics, Piwik Pro, Matomo, Snowplow Analytics, Google Cloud Platform, AWS, Microsoft Azure, R, Python, Git, SEO, HIPAA, SQL, REST API, SOAP API, JSON, XML, CSV, Unix, Linux, Windows, Presentation Skills, Communication Skills, Problem Solving Skills, Analytical Skills, Attention to Detail, Ability to Work Independently, Ability to Work in a Team, Ability to Meet Deadlines, Ability to Manage Multiple Projects, Ability to Prioritize Tasks, Ability to Stay Organized, Ability to Handle Stress, Ability to Learn New Things","data analytics, digital analytics, google analytics, adobe analytics, piwik pro, matomo, snowplow analytics, google cloud platform, aws, microsoft azure, r, python, git, seo, hipaa, sql, rest api, soap api, json, xml, csv, unix, linux, windows, presentation skills, communication skills, problem solving skills, analytical skills, attention to detail, ability to work independently, ability to work in a team, ability to meet deadlines, ability to manage multiple projects, ability to prioritize tasks, ability to stay organized, ability to handle stress, ability to learn new things","ability to handle stress, ability to learn new things, ability to manage multiple projects, ability to meet deadlines, ability to prioritize tasks, ability to stay organized, ability to work in a team, ability to work independently, adobe analytics, analytical skills, attention to detail, aws, communication skills, csv, dataanalytics, digital analytics, git, google analytics, google cloud platform, hipaa, json, linux, matomo, microsoft azure, piwik pro, presentation skills, problem solving skills, python, r, rest api, seo, snowplow analytics, soap api, sql, unix, windows, xml"
Data Scientist,Thyme Care,"Nashville, TN",https://www.linkedin.com/jobs/view/data-scientist-at-thyme-care-3716786130,2023-12-17,Nashville,United States,Mid senior,Remote,"Thyme Care is a value-based oncology management platform that provides personalized, clinically coordinated care to individuals with cancer. Thyme Care pairs human guidance with software and analytics to engage members with a cancer diagnosis, quickly connects them to the right care and provides ongoing support through targeted, evidence-based interventions. The company's unique approach establishes deep provider relationships and integrates with a health plan's existing infrastructure, coordinating value-driven care that leads to better outcomes, lower costs and an improved member experience. Backed with venture funding from Andreessen Horowitz (a16z), AlleyCorp, Frist Cressey Ventures, Casdin Capital and Bessemer, Thyme Care partners with health insurance plans and providers to extend the reach of high-quality cancer care through flexible value-based payment arrangements, including risk-based programs.
Your Role:
As a Data Scientist, you will join our Data team to help us build and maintain strong analytic capabilities that will support Thyme Care's mission. You will model data and create visualizations to report to stakeholders and deliver insights that drive action. To excel in this role, you should have experience working with healthcare data such as claims, prior auth, and electronic health records.
In this position, you will collaborate with our Product, Engineering, Clinical, and Operations teams to identify opportunities to further improve outcomes and track the efficacy of our interventions as we serve our members and clients. You will help us establish deep provider relationships by building data products to aid in practice transformation efforts.
After three months you will:
Gained a deep understanding of our data platform and contributed to improving our data models and pipelines.
Built relationships with oncology practices and providers and demonstrated Thyme Care's expertise in building data & analytics products.
Collaborated with stakeholders to understand their business requirements and translate them into technical plans. This involves developing data models, pipelines, and analytics dashboards with Looker or RStudio. For example, you may use dbt to build models that analyze medical claims data to identify utilization of value care across oncology practices or analyze dispersion of medical care among our members and its impact on out-of-pocket costs.
After 6 months you will:
Take ownership and led the development of data models to measure the effectiveness of our interventions with oncology practices and report outcomes internally and externally.
Play a key role in building our data infrastructure and creating a roadmap for scalable and modular data architecture to support our team's growth.
Lead the development of utilization and quality metrics and become the go-to person for resolving stakeholder inquiries. Additionally, you will leverage our data assets for business opportunities and strategic initiatives at Thyme Care.
What leads to success:
Act with our members in mind. Thyme Care's mission, and in particular our member experience, matters deeply to you.
Move with purpose. You're biased to action. You know how to identify and prioritize your initiative's needs, and do what it takes to ensure that urgent and important needs are acted on immediately.
Seek diverse perspectives. You are humble and actively seek feedback from others, eager to learn and share knowledge.
Experience. You have worked with large healthcare datasets, ideally in a health plan or healthcare-focused technology startup with mature data structures and pipelines as an Analytics Engineer or Data Engineer. Experience with medical claims, pharmacy claims, eligibility files, and other relevant healthcare data is essential for building data marts for reporting and analysis.
Technical ability. You are proficient in analytical, data modeling, and data transformation is crucial. Familiarity with DBT is preferred, but we are open to candidates willing to learn it quickly. A working knowledge of Python or R and experience with Looker or similar BI tools for data analysis and visualization is a plus.
Clear communication. You can effectively convey your thoughts and ideas to both technical and non-technical colleagues and stakeholders.
Comfort with ambiguity. You have a successful track record working at scaling organizations, in fast-paced environments, and at ambitious startups. You navigate through challenges and find solutions in uncertain situations.
OUR VALUES
At Thyme Care, our core values—Act with our members in mind, Move with purpose, and Seek diverse perspectives—guide us in everything we do. They anchor our business decisions, including how we grow, the products we make, and the paths we choose—or don't choose.
We are committed to promoting the health and well-being of all individuals. As a provider of cancer care navigation, we recognize that those with cancer constitute a vulnerable population at risk of contracting COVID-19. As such, Thyme Care employees are expected to be fully vaccinated against COVID-19 as defined by the
Centers for Disease Control and Prevention
, subject to conflicting laws and exemptions based on medical or religious objections.
Our salary ranges are based on paying competitively for our size and industry, and are one part of the total compensation package that also includes equity, benefits, and other opportunities at Thyme Care. Individual pay decisions are based on a number of factors, including qualifications for the role, experience level, skillset, geography, and balancing internal equity relative to other Thyme Care employees. In accordance with New York City law, the base salary for this role if filled within New York City is $130,000-$160,000
. The salary range could be lower or higher than this if the role is hired in another location. We also believe that your personal needs and preferences should be taken into consideration so we allow some choice between equity and cash.
We recognize a history of inequality in healthcare. We're here to challenge the status quo and create a culture of inclusion through the care we give and the company we build. We embrace and celebrate a diversity of perspectives in reflection of our members and the members we serve. We are an equal opportunity employer.
Show more
Show less","Data Science, Machine Learning, Healthcare Analytics, Data Modeling, Data Visualization, Data Transformation, Claims Data Analysis, Medical Records Analysis, DBT, Python, R, Looker, RStudio","data science, machine learning, healthcare analytics, data modeling, data visualization, data transformation, claims data analysis, medical records analysis, dbt, python, r, looker, rstudio","claims data analysis, data science, data transformation, datamodeling, dbt, healthcare analytics, looker, machine learning, medical records analysis, python, r, rstudio, visualization"
Data Architect,Arcadis,"Nashville, TN",https://www.linkedin.com/jobs/view/data-architect-at-arcadis-3749501151,2023-12-17,Nashville,United States,Mid senior,Remote,"Arcadis is the world's leading company delivering sustainable design, engineering, and consultancy solutions for natural and built assets.
We are more than 36,000 people, in over 70 countries, dedicated to improving quality of life. Everyone has an important role to play. With the power of many curious minds, together we can solve the world’s most complex challenges and deliver more impact together.
Overview:
The Information Systems Data Architect will be part of part of a design and development that includes database developers, data architects, data analysts and process automation team. The mission will be to develop and maintain application integration, Data Warehouse by using on-premises MS SQL Server, Azure SQL Managed instance, Azure SQL Database, SSIS, Azure data factory, Azure storage account, Azure App Service, and other data management tools.
Responsibilities:
Lead Data Warehouse design, ETL architecture design, security profile management, data share design and other data architecture application for internal users and corporate transactional applications. Also act as other billable sector data architect consultant.
Act as application solutions architect, lead and design corporation internal application by using Microsoft C#, Azure Web App Service, Web jobs, Azure function App Service.
Admin and monitor corporate service Azure Cloud resources. Based on architecture design, create resources for corporate service application, monitoring resources status and billing status, manage resources access privilege, Suggest new functions in resources for development team.
Act as scrum master in corporate service development teams. Manage sprint by hosting daily standup meeting and bi-weekly sprint meeting, check development activities fit to agile methodology and continuously helping improve teams’ productivity.
Manage a high dynamic development team, and prioritize multiple tasks by working with various stakeholders, maximize team resources utilization to meet each project deadline within limited resources.
Admin Azure DevOps, manage users, user access level, user privileges, processes and its rules, repos for DW, Applications, ETL, etc. Sprints planning, executing, and reviewing.
Act as company Power BI Admin, manage all kinds of Power BI license (such as pro license, premium capacity, embedded capacity) companywide, research and learn Power BI admin parameter in Power BI portal and implement proper setup. Manage embedded Power BI solutions by providing training and guide to development team. Manage Power BI gateways which are used by Power BI, Power Automate.
Qualifications:
Please ensure to provide a description of the projects you have worked on within your resume.
A University bachelor’s degree in computer science with programming experience, or a degree in a related discipline.
Minimum 3 to 5 years of related Data Warehouse, ETL, application integration design and development experience with Microsoft Azure SQL database, data factory, storage, app service, synapse analytics, data lake, network, Azure resource management.
Possess excellent communication, presentation, and leadership skills and be able to work in a dynamic environment with rapidly changing environment. Demonstrated ability to achieve goals by leading team in an innovative and fast paced environment.
Strong experience with SSIS package and pipeline design, development, deployment, monitoring and scheduling.
Strong experience with MS SQL Server design, development, deployment, performance tuning, monitoring.
Strong experience with Azure DevOps in user, project, repo, wiki management. Can observe and improve current process.
Strong experience with Power BI license and parameter admin, can provide user proper Power BI license and proper guide to start with, can research and setup proper parameters under Power BI admin portal, can manage Power BI gateway by applying patch, observing performance, resolving bottle neck.
Strong experience with application integration between corporate service applications such as HRIS, CRM, ERP, CPM by using C# and Azure App Service.
MS Certified: Azure Solution Architect Expert and/or MS Certified: Azure Data Engineer Associate will be an asset.
Previous working experience with extracting data from an ERP application, HRIS application, CRM application is highly preferred.
Why Arcadis?
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills-based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You’ll do meaningful work, and no matter what role, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark, on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Join Arcadis. Create a Legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We are an equal opportunity and affirmative action employer. Women, minorities, people with disabilities and veterans are strongly encouraged to apply. We are dedicated to a policy of non-discrimination in employment on any basis including race, creed, color, religion, national origin, sex, age, disability, marital status, sexual orientation, gender identity, citizenship status, disability, veteran status, or any other basis prohibited by law.
Arcadis offers benefits for full time and part time positions. These benefits include medical, dental, and vision, EAP, 401K, STD, LTD, AD&D, life insurance, paid parental leave, reward & recognition program and optional benefits including wellbeing benefits, adoption assistance and tuition reimbursement. We offer seven paid holidays and potentially up to two floating holidays per calendar year depending on start date, and 15 days PTO that accrue per year. The salary range for this position is $95000 - $110000.
Show more
Show less","SSIS, SQL Server, Azure SQL Managed instance, Azure SQL Database, Azure data factory, Azure storage account, Azure App Service, Azure DevOps, Power BI, Azure Web App Service, Web jobs, Azure function App Service, Microsoft C#, ETL, Data Warehouse, Data architecture, Application integration, Data management, Azure Cloud, Azure resource management, Azure Solution Architect Expert, Azure Data Engineer Associate, HRIS, CRM, ERP, CPM","ssis, sql server, azure sql managed instance, azure sql database, azure data factory, azure storage account, azure app service, azure devops, power bi, azure web app service, web jobs, azure function app service, microsoft c, etl, data warehouse, data architecture, application integration, data management, azure cloud, azure resource management, azure solution architect expert, azure data engineer associate, hris, crm, erp, cpm","application integration, azure app service, azure cloud, azure data engineer associate, azure data factory, azure devops, azure function app service, azure resource management, azure solution architect expert, azure sql database, azure sql managed instance, azure storage account, azure web app service, cpm, crm, data architecture, data management, datawarehouse, erp, etl, hris, microsoft c, powerbi, sql server, ssis, web jobs"
Data Analyst,"IDR, Inc.","Nashville, TN",https://www.linkedin.com/jobs/view/data-analyst-at-idr-inc-3781944162,2023-12-17,Nashville,United States,Mid senior,Hybrid,"IDR is seeking an
onsite, Data Analyst
to join one of our top clients in
Nashville, TN
. If you are looking for an opportunity to join a large organization and work within an ever-growing team-oriented culture, please apply today!
NO C2C**
Position Overview For The Data Analyst
Collaborates across the organization’s divisions to understand their data analysis needs, priorities, and expectations.
Coordinates and oversees data collection activities in partnership with internal and external stakeholders
Supports managers in tasks related to data analysis and visualization.
Implements analytical solutions that bring valuable insights
Engages with emerging technologies and datasets.
Identifies and recommends software and hardware systems for data analysis
Develops technical reports, presentations, training materials, and other data and visualization-based productions
Required Skills For The Data Analyst
3+ years experience in Data Analysis
Background in data science, mathematics, information technologies, engineering or other relevant technical field of study
Experience coordinating data collections
Ability to work effectively and communicate with stakeholders
What’s in it for you?
Competitive compensation package
Full Benefits; Medical, Vision, Dental, and more!
Opportunity to get in with an industry leading organization
Close-knit and team-oriented culture
Why IDR?
20+ Years of Proven Industry Experience in 4 major markets
Employee Stock Ownership Program
Dedicated Engagement Manager who is committed to you and your success
Medical, Dental, Vision, and Life Insurance
ClearlyRated’s Best of Staffing® Client and Talent Award winner 8 years in a row
Show more
Show less","Data Analysis, Data Science, Mathematics, Information Technology, Engineering, Data Collection, Stakeholder Communication, Data Visualization, Data Reporting, Software Recommendation, Hardware Recommendation, Training Material Development","data analysis, data science, mathematics, information technology, engineering, data collection, stakeholder communication, data visualization, data reporting, software recommendation, hardware recommendation, training material development","data collection, data reporting, data science, dataanalytics, engineering, hardware recommendation, information technology, mathematics, software recommendation, stakeholder communication, training material development, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Nashville, TN",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712025,2023-12-17,Nashville,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, NoSQL, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Classification, Retention, 401K, Equity Programs, GenderAffirming Offerings, HRT, Flexible Vacation Policy, Cell Phone Stipend, Internet Stipend, Wellness Stipend, Food Stipend, HomeOffice Setup Stipend","data engineering, machine learning, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, retention, 401k, equity programs, genderaffirming offerings, hrt, flexible vacation policy, cell phone stipend, internet stipend, wellness stipend, food stipend, homeoffice setup stipend","401k, airflow, applied machine learning, aws, azure, bash, cell phone stipend, data classification, data engineering, docker, equity programs, etl, flexible vacation policy, food stipend, gcp, genderaffirming offerings, git, helm, homeoffice setup stipend, hrt, internet stipend, java, kafka, kubernetes, machine learning, nosql, python, retention, snowflake, spark, sparkstreaming, sql, storm, wellness stipend"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Nashville, TN",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773088680,2023-12-17,Nashville,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Git, Pandas, R, Airflow, KubeFlow, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, Kafka, Storm, SparkStreaming, Machine Learning, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Data Pipelines, Data Infrastructure, Data Governance, Risk Compliance, Data PreProcessing, Data PostProcessing, Databases, NoSQL, ETL, Data Engineering, NLP, LLM, AWS, GCP, Azure, Data Visualization","python, java, bash, sql, git, pandas, r, airflow, kubeflow, snowflake, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, machine learning, data mining, data cleaning, data normalization, data modeling, data pipelines, data infrastructure, data governance, risk compliance, data preprocessing, data postprocessing, databases, nosql, etl, data engineering, nlp, llm, aws, gcp, azure, data visualization","airflow, aws, azure, bash, data cleaning, data engineering, data governance, data infrastructure, data mining, data normalization, data postprocessing, data preprocessing, databases, datamodeling, datapipeline, docker, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, llm, machine learning, nlp, nosql, pandas, python, r, risk compliance, snowflake, spark, sparkstreaming, sql, storm, visualization"
GCP Data Architect,3i Infotech Ltd.,"Nashville, TN",https://www.linkedin.com/jobs/view/gcp-data-architect-at-3i-infotech-ltd-3780708013,2023-12-17,Nashville,United States,Mid senior,Hybrid,"Position: GCP Data Architect
Location: Nashville, TN (onsite 3 days a week)
Duration: Perm
Description:
This person needs to communicate across all levels of the organization and will engage with C-Level execs often. They need to be a thought leaders and servant leader to the team. Preference is for a Data Architect. However, if we have a strong Sr. Data Engineer with some design experience, they are open to that as well. They have a solutions consulting team handling the heavy lifting in GCP at the moment.
Project Details – Size, Scale, Scope?
How does candidate fit into this?
Azure Cloud journey started 10 years ago and switched to GCP. Now 3 years into Google journey. Identifying applications that need to move into GCP. Attempted data strategy several times over past 10 years and as a whole, it’s always evolving. The Enterprise Data Architect will be working across several levels and need to be open to collaboration with difference opinions and perspectives coming from other individuals.
Day to Day Responsibilities/typical day look like:
Supporting the Design of a GCP clinical data store for machine learning. Won’t need to start from scratch. Moving into lake house architecture. Implementing data strategy from enterprise high level projects. Uplifting 190 hospitals to new electronic health record system. Will work closely with the new Principal Architect for high availability, business continuity, etc.
Maturing and evolving lake house implementation to production in Q1. Started in September. Just finishing proof of technology.
Responsible AI gen models
Building out a sandbox lake house that will mirror Databricks and Synapse.
New data store ingestion
Don’t need to be GCP expert (they do have a GCP partnership), supporting configurations and best practices, when services is enabled, etc.
8000 applications, 100 thousand of servers, 1 million user identities, several source systems, every EHR on the market
Need them to learn for 1st year and lead work streams
Selling Points for Candidates:
Perks? What is exciting about
position
?
Company
?
What are the top 3 MUST HAVE technologies or Required experience for the position?
7 -10 years of general architecture exp. Approximately 5 years of GCP design experience (open to seeing strong engineers here)
Ability to manage up (receive direction from Sr. Leaders)
People focused (interacting with as many as 100 individuals daily); Servant leader and have strategic leadership skills
NICE TO HAVES (OR WHAT GETS THE WIN)?
Healthcare exp.
Heavy GCP Architecture
Show more
Show less","GCP, Azure Cloud, Data Architect, Senior Data Engineer, Data Strategy, Machine Learning, Lake House Architecture, Electronic Health Records, High Availability, Business Continuity, Responsible AI, Databricks, Apache Synapse, Data Store Ingestion, Big Data, Healthcare","gcp, azure cloud, data architect, senior data engineer, data strategy, machine learning, lake house architecture, electronic health records, high availability, business continuity, responsible ai, databricks, apache synapse, data store ingestion, big data, healthcare","apache synapse, azure cloud, big data, business continuity, data architect, data store ingestion, data strategy, databricks, electronic health records, gcp, healthcare, high availability, lake house architecture, machine learning, responsible ai, senior data engineer"
"Looking for Data Governance Epidemiologist - Nashville, TN",Software Technology Inc.,"Nashville, TN",https://www.linkedin.com/jobs/view/looking-for-data-governance-epidemiologist-nashville-tn-at-software-technology-inc-3750612376,2023-12-17,Nashville,United States,Mid senior,Hybrid,"Job Title: Data Governance Epidemiologist
Location: Nashville, TN
Duration: 6+ Months
Work hours: 8am to 4.30pm Monday through Friday
Job Overview
The Tennessee Department of Health (TDH) collects a variety of public health data and information. These data are collected and used by TDH programs and teams for public health activities, such as surveillance, response, prevention, treatment, research, and reporting. These data are also used by people and organizations outside of TDH to support a wide array of public health activities.
The Data Governance Program within TDH manages processes and tools that facilitate the sharing of data across the Tennessee Department of Health and our stakeholders/partners while providing for the security, privacy, and confidentiality of citizens.
As a member of the Data Governance team, this individual will work with administrators and fellow epidemiologists to evaluate and process incoming data requests and outgoing data sets. This individual will work with a team and will have the opportunity to work on a wide range of projects including improving the existing data requests system, reporting on number and type of data requests, and supporting the roll-out of a set of updated Aggregate Date Release guidelines.
Responsibilities
Assist the Data Manager in the day-to-day maintenance of the Data Request (DR) System in terms of updates to the DR Database in MS ACCESS, Excel worksheets and workbooks, analysis and generation of qualitative and quantitative graphs, charts, tables regarding the incoming DRs and outgoing DR fulfillment outputs.
Assist with the gathering of requirements to improve the TDH data request system and methods for storing fulfilled data requests.
Implement improvements to current Data Request system(s).
Work with Core Informatics to create/update the TDH Data Inventory and the TDH Data Catalog.
Apply basic statistical knowledge to data analyses.
Conduct disclosure risk assessments for data requests, data presentations, summaries, reports for various offices and divisions.
Provide consultations on risk assessment for aggregate data release.
Develop and run processes to ensure weekly back-ups are taken of all DR related files, SW, outputs.
Minimum Qualifications
Excellent written and oral communication skills.
Strong knowledge of Microsoft Office Suite.
Able to work independently or as part of a team.
Working understanding of IT systems.
Understanding of data visualization techniques and statistical software tools (SAS, SPSS, R).
Experience in adopting and using software API’s as needed to connect to data sources (both internal and external).
Intermediate to expert level knowledge of visual data analysis tools such as Excel, Tableau, Power BI, database tools such as ACCESS, SEQUEL, ORACLE, and database protocols such as ODBC/JDBC.
Show more
Show less","Data Governance, Data Sharing, Data Security, Data Privacy, Data Confidentiality, Data Requests, Data Inventory, Data Catalog, Statistical Analysis, Disclosure Risk Assessment, Data Visualization, Microsoft Office Suite, SAS, SPSS, R, API, Excel, Tableau, Power BI, ACCESS, SEQUEL, ORACLE, ODBC, JDBC","data governance, data sharing, data security, data privacy, data confidentiality, data requests, data inventory, data catalog, statistical analysis, disclosure risk assessment, data visualization, microsoft office suite, sas, spss, r, api, excel, tableau, power bi, access, sequel, oracle, odbc, jdbc","access, api, data catalog, data confidentiality, data governance, data inventory, data privacy, data requests, data security, data sharing, disclosure risk assessment, excel, jdbc, microsoft office suite, odbc, oracle, powerbi, r, sas, sequel, spss, statistical analysis, tableau, visualization"
Data Scientist,"IDR, Inc.","Franklin, TN",https://www.linkedin.com/jobs/view/data-scientist-at-idr-inc-3774672790,2023-12-17,Nashville,United States,Mid senior,Hybrid,"IDR is looking for a Data Scientist to drive innovation and deliver organizational value for a consulting client of ours in Franklin, TN. If you are looking for a small/collaborative team where your ideas can be heard, please apply today! *Direct Hire*
Position Overview For The Data Scientist
Find/analyze/import and curate data from free and paid third party sources
Collaborate with stakeholders to understand business objectives
Clean and organize raw data to ensure usability and quality
Select appropriate algorithms and building predictive models
Improve performance of machine learning models
Required Experience for the Data Scientist:
5-7 years of experience in Data Science
Professional working experience with pricing and forecasting
Strong experience with Microsoft stack (Azure & Data Bricks)
Experience in the consumer products industry or ecommerce space is preferred.
What's in it for you?
Tight, close-knit team
Bonus opportunity
Direct Hire
Latest and greatest technologies!
Much, much more!
Why IDR?
15+ Years of Proven Industry Experience in 4 major markets
Medical, Dental, Vision, and Life Insurance
Inavero's Best of Staffing® Client Award Winner for the Past 10 Years
Show more
Show less","Data Science, Data Analytics, Data Mining, Machine Learning, Predictive Modeling, Data Visualization, Data Wrangling, Data Cleaning, Algorithm Selection, Microsoft Azure, Apache Spark, Python, R, SQL, NoSQL, Cloud Computing, Big Data, Statistics, Business Intelligence, Pricing, Forecasting, Consumer Products, Ecommerce","data science, data analytics, data mining, machine learning, predictive modeling, data visualization, data wrangling, data cleaning, algorithm selection, microsoft azure, apache spark, python, r, sql, nosql, cloud computing, big data, statistics, business intelligence, pricing, forecasting, consumer products, ecommerce","algorithm selection, apache spark, big data, business intelligence, cloud computing, consumer products, data cleaning, data mining, data science, data wrangling, dataanalytics, ecommerce, forecasting, machine learning, microsoft azure, nosql, predictive modeling, pricing, python, r, sql, statistics, visualization"
Data Center Engineer,Steneral Consulting,"Naperville, IL",https://www.linkedin.com/jobs/view/data-center-engineer-at-steneral-consulting-3732889418,2023-12-17,Homer,United States,Mid senior,Onsite,"Must be local
Location: Onsite in Rochelle or Naperville IL
Interview Mode: In person final interview
Possible CTH role
W2 candidates only
Need valid linkedin
Notes
More of an add / move / change work, providing assistance for task oriented work. Running projects.
Experience in a larger data center. Sizing up and Cooling experience. Power background.
Someone familiar with change management is nice to have
Somewhat physical but more project planning, etc.
Ability to commute to two data centers, one in Rochelle and Naperville, IL.
Schedule will be fluid based on which data center requires more work at the a particular cycle.
Flex schedule Tuesday-Saturday for 3-4 weeks/month, Mon-Friday 1-2 weeks a month depending on blackout windows.
Weekdays will be heavily 8am - 4:30pm schedule, weekends will be based on change schedules for specific activities (i.e. Friday night or Saturday morning/afternoon)
Be part of on-call rotation and be able to come to either site for high priority incidents as needed
All work to be conducted on-site, no work from home
Need to be “on call” when needed.
This is more of a lower to mid-level position
Fiber and copper technologies is important
Working with interview platform towers to devise solutions any decommissions, adds, moves, etc.
Add move change work, providing assistance for task oriented work.
Project Overview
To provide staffing for NT’s Data Center Management Team. Tasks will include day to day data center operations including technical planning, incident response/troubleshooting, on-site coverage, execution of data center changes.
Contractor’s Role
Data Center Engineer that will plan and execute deployments with a focus around connectivity, power, and cooling capacities. Will see projects from start to execution which include new installs, decommissioning, physical migrations, and day to day data center operations like inventory, shipping/receiving, etc. Contractor will work within ITSM guidelines for all work within the data centers. Physical on-site coverage in Naperville and Rochelle, IL with a flex schedule (either Tuesday-Saturday or Monday-Friday). Will be part of on-call rotation for off hour incident response.
Qualifications (must Haves)
3+ years as a data center engineer
Expert knowledge in cabling and structured cabling infrastructure
Experience working within ITSM and change management standards
Knowledge of data center power and cooling
Must have great communication skills and provide a professional level of customer service to our clients
Nice To Have
Experience with DCIM tools (particularly Device42) and ServiceNow
Be able to utilize advanced test equipment for layer 1 troubleshooting (i.e. Fluke Versiv, Viavi, OTDR)
Live within one hour of each data center location
RCDD
Top 3-5 IT Skillsets/Experience
Experience planning deployments and physical cabling migrations within a larger/corporate data center
Low voltage cabling standards and troubleshooting
Basic understanding of core data center network technologies and topologies
Operating within change management standards (particularly Service Requests, Incidents, and Changes)
Project planning with minimal guidance, operates mostly independently
Tasks & Responsibilities
Troubleshooting of cabling, power, and cooling of devices within an enterprise data center
Create and follow documentation and processes that are used in preparation, installation and cabling of systems
Validate installations which lead to successful outcomes
Interface with vendors and contractors directly and via ticketing systems
Pack/unpack servers, hardware, and other related equipment
Create innovative solutions to help streamline current processes
Work with relevant groups to ensure all equipment is received and documented in a timely manner
Manage and execute on projects independently; ownership of project work for internal applications and infrastructure towers
Be part of an on-call schedule, be able to drive to site if need arises to assist in repair/troubleshooting within the SLA
Oversee weekend work regularly
Ability to be flexible with schedule for unplanned off-hour incidents and emergency changes
Build bill of materials, work with vendors and procurement to purchase hardware
Design and execute solutions for data center hardware and systems
Coordinate with facility management on maintenance of critical systems (UPS, generator, cooling, etc.)
Conduct and attend engineering meetings
Show more
Show less","Data Center Engineer, Cabling, Structured Cabling Infrastructure, ITSM, Change Management, Data Center Power, Cooling, Communication Skills, Customer Service, DCIM Tools, ServiceNow, Advanced Test Equipment, Layer 1 Troubleshooting, RCDD, Deployment Planning, Physical Cabling Migrations, Low Voltage Cabling, Network Technologies, Topologies, Project Planning, Troubleshooting, Documentation, Installation, Cabling, Validation, Vendor Management, Ticketing Systems, Bill of Materials, Procurement, Hardware, Facility Management, Maintenance, UPS, Generator, Engineering Meetings","data center engineer, cabling, structured cabling infrastructure, itsm, change management, data center power, cooling, communication skills, customer service, dcim tools, servicenow, advanced test equipment, layer 1 troubleshooting, rcdd, deployment planning, physical cabling migrations, low voltage cabling, network technologies, topologies, project planning, troubleshooting, documentation, installation, cabling, validation, vendor management, ticketing systems, bill of materials, procurement, hardware, facility management, maintenance, ups, generator, engineering meetings","advanced test equipment, bill of materials, cabling, change management, communication skills, cooling, customer service, data center engineer, data center power, dcim tools, deployment planning, documentation, engineering meetings, facility management, generator, hardware, installation, itsm, layer 1 troubleshooting, low voltage cabling, maintenance, network technologies, physical cabling migrations, procurement, project planning, rcdd, servicenow, structured cabling infrastructure, ticketing systems, topologies, troubleshooting, ups, validation, vendor management"
Senior Data Engineer,Calamos Investments,"Naperville, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-calamos-investments-3769529692,2023-12-17,Homer,United States,Mid senior,Onsite,"Calamos is seeking a Senior Data Engineer with a passion for both data and the financial services industry. This person will be part of the Data Engineering Team which is responsible for the management, curation, and growth of the firm’s mission-critical data. This individual should be interested in turning our data and data platform into a competitive advantage.
Primary Responsibilities:
Responsible for all aspects of software development following the Agile methodology using the Scrum framework.
Design and build Modern Data Pipelines
Design and create ETL processes supplying Data Warehouse
Deliver software projects that adhere to both team and firm software standards.
Participate in the team on-call rotation to
Ensure all critical overnight data delivery SLA(s) are met.
Perform daily review and actions for any non-critical overnight failures, data exceptions, or other business user inquiries.
Active participation in team meetings and discussions.
Collaborate with architects and engineers from other teams.
Preferred Qualifications:
Bachelor’s degree in computer science or related field.
At least 8 years of experience developing in a data management environment. This includes database design, data warehousing concepts, and ETL.
1- 2 years of experience in software development projects using Python and Databricks
Familiarity with Azure cloud infrastructure
At least 2 years of experience working in the financial services industry.
Usage of modern CI/CD tools
Must have excellent technical and analytical skills.
Must have strong communication skills.
Must have a passion for new technologies and frameworks including Cloud and Agile Scrum.
Show more
Show less","Python, Databricks, Azure cloud infrastructure, Software development, Data management, Database design, Data warehousing concepts, ETL processes, Agile Scrum framework, Team meetings and discussions, SQL, Big Data","python, databricks, azure cloud infrastructure, software development, data management, database design, data warehousing concepts, etl processes, agile scrum framework, team meetings and discussions, sql, big data","agile scrum framework, azure cloud infrastructure, big data, data management, data warehousing concepts, database design, databricks, etl, python, software development, sql, team meetings and discussions"
Senior Data Engineer,KeHE Distributors,"Naperville, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kehe-distributors-3752893383,2023-12-17,Homer,United States,Mid senior,Onsite,"Good people, working with good people, for our common good.
Sound good?
KeHE-a natural, organic, specialty and fresh food distributor-is all about ""good"" and is growing, so there's never been a more exciting time to join our team. If you're enthusiastic about working in an environment with a people-first culture and an organization committed to good living, good food and good service, we'd love to talk to you!
KeHE is looking for experienced engineers that are passionate about backend API's and cloud technologies. Ideal candidates have a desire to automate development process, scale and optimize efficiently, a strong technical competency, opinions on future technologies, and most importantly, thrive in a team environment.
To be successful in this role, you must have the following technical skills:
.NET C# or Node.js (including experience identifying and applying patterns)
Entity Framework (Fluent API)
SQL Database (MSSQL or Postgres)
Microsoft WebAPI
AWS experience specifically using API Gateway, SQS, SNS, RDS, S3
Experience using Git in a collaborative setting (feature branches and PR review)
Experience writing unit tests
It's also great, but not mandatory if you have any/all of the following skills:
Additional AWS experience in Lambda, Dynamo DB, RDS, Elasticache, Elasticsearch
.NET Core 2.0+
Experience with micro-services
Jenkins for CI/CD
Developed in an Agile or Kanban environment
Optimization (caching, database query optimization, memory
Show more
Show less",".NET C#, Node.js, Entity Framework (Fluent API), SQL Database (MSSQL or Postgres), Microsoft WebAPI, AWS (API Gateway SQS SNS RDS S3), Git (feature branches and PR review), Unit testing, AWS (Lambda Dynamo DB RDS Elasticache Elasticsearch), .NET Core 2.0+, Microservices, Jenkins for CI/CD, Agile or Kanban environment, Optimization (caching database query optimization memory)","net c, nodejs, entity framework fluent api, sql database mssql or postgres, microsoft webapi, aws api gateway sqs sns rds s3, git feature branches and pr review, unit testing, aws lambda dynamo db rds elasticache elasticsearch, net core 20, microservices, jenkins for cicd, agile or kanban environment, optimization caching database query optimization memory","agile or kanban environment, aws api gateway sqs sns rds s3, aws lambda dynamo db rds elasticache elasticsearch, entity framework fluent api, git feature branches and pr review, jenkins for cicd, microservices, microsoft webapi, net c, net core 20, nodejs, optimization caching database query optimization memory, sql database mssql or postgres, unit testing"
"Sr Engineer, Data Engineering & Analytics",KeHE Distributors,"Naperville, IL",https://www.linkedin.com/jobs/view/sr-engineer-data-engineering-analytics-at-kehe-distributors-3765376332,2023-12-17,Homer,United States,Mid senior,Onsite,"Why Work for KeHE?
Full-time
Pay Range: $95,000.00/Yr. - $145,000.00/Yr.
Shift Days: , Shift Time:
Benefits on Day 1
Health/Rx
Dental
Vision
Flexible and health spending accounts (FSA/HSA)
Supplemental life insurance
401(k)
Paid time off
Paid sick time
Short term & long term disability coverage (STD/LTD)
Employee stock ownership (ESOP)
Holiday pay for company designated holidays
Overview
Good people, working with good people, for our common good.
Sound good?
KeHE-a natural, organic, specialty and fresh food distributor – is all about “good” and is growing, so there’s never been a more exciting time to join our team. If you’re enthusiastic about working in an environment with a people-first culture and an organization committed to good living, good food and good service, we’d love to talk to you!
Primary Responsibilities
As the Senior Data Engineering and Analytics Engineer, you will have the opportunity to make a significant impact on the success and growth of KeHE by engaging with internal business stakeholders of all levels. You will become an enabler by providing support & guidance in defining and delivering their data engineering and analytics needs by turning data into actionable information leveraging on-prem and cloud technologies.
Essential Functions
Demonstrated ability to partner with internal product stakeholders to help define requirements and outcomes for data-focused initiatives
Ability to decompose large problems and execute smaller, manageable bodies of work to demonstrate continuous architecture delivery
Architecting and managing scalable data pipelines across on-premise and cloud data repositories
Create data flow diagrams, and document source to target mapping
Utilizing their business acumen, analytical mindset, and strong communication/people skills to extract business requirements and translating them into actionable tasks
Maintain the data warehouse performance by optimizing batch processing through parallelization, performance tuning, aggregations etc.
Create user-facing dashboards that provide key insights at a glance for its specific audience
Recognize and adopt best practices and cost-effective solutions in developing analytical insights on prem and in the cloud
Provide guidance and mentor junior team members
Keep current with Business Intelligence data trends and technological innovations
Execute on POC’s with new technologies, drive innovation, and new ideas
Minimum Requirements, Qualifications, Additional Skills, Aptitude
4 Year College Degree from an accredited university in one of the following: Finance, Computer Science Information Systems, Operations Research, Mathematics, Statistics, or related technical field
7+ years leveraging Microsoft and its associated array of offerings (SQL, PL/SQL, SSIS, SSRS, SSAS, PowerBI, etc.)
5+ years leveraging AWS and its associated array of offerings (Glue, Redshift, Athena, S3, Spectrum, PySpark etc.)
You have deep expertise in Python and C#
You have collaborated with a development team with established source control (git).
Strong understanding data modeling (i.e. conceptual, logical and physical model design, experience with Operation Data Stores, Enterprise Data warehouses and Data Marts).
Strong experience with business intelligence and data warehousing design principles and industry best practices including multi-dimensional modelling (star schemas, snowflakes, de-normalized models, handling “slow-changing” dimensions)
Ability to work with stakeholders and changing priorities
Ability to learn and apply new technologies to business problems
You would be a great fit if:
You love a challenge and problem solving
Enjoy learning and working with new technologies
You are extremely articulate and concise
You are proactive and resourceful with a drive to match
You have an intrinsic curiosity for tinkering with data products
You recognize the importance of data storytelling
You have excellent written and verbal skills
You are hardworking and have a strong work ethic
You are a quick learner and able to apply new technologies to business problems
Requisition ID
2023-21574
Equal Employer Opportunity Statement
KeHE Distributors provides equal employment opportunities to all employees and applicants for employment and prohibits all forms of discrimination and harassment on the basis of race, color, religion or faith, sex, gender, age, ancestry, national origin, mental or physical disability or medical condition, sexual orientation, gender identity or expression, marital status, military or veteran status, genetic information, or any other category protected under federal, state, or local law. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training as well as the administration of all Human Resources and Talent Acquisition processes.
Show more
Show less","SQL, PL/SQL, SSIS, SSRS, SSAS, PowerBI, AWS, Glue, Redshift, Athena, S3, Spectrum, PySpark, Python, C#, Git, Data modeling, Business intelligence, Data warehousing, Multidimensional modeling, Star schemas, Snowflakes, Denormalized models, Data storytelling","sql, plsql, ssis, ssrs, ssas, powerbi, aws, glue, redshift, athena, s3, spectrum, pyspark, python, c, git, data modeling, business intelligence, data warehousing, multidimensional modeling, star schemas, snowflakes, denormalized models, data storytelling","athena, aws, business intelligence, c, data storytelling, datamodeling, datawarehouse, denormalized models, git, glue, multidimensional modeling, plsql, powerbi, python, redshift, s3, snowflakes, spark, spectrum, sql, ssas, ssis, ssrs, star schemas"
Staff Data Engineer,Recruiting from Scratch,"Cicero, IL",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744399109,2023-12-17,Homer,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, ETL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, Data Management Tools, Data Classification, Data Retention","python, etl, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, data management tools, data classification, data retention","airflow, automated testing, continuous integration, data classification, data management tools, data retention, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Orland Hills, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742673963,2023-12-17,Homer,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, MLOps, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Apache Beam, Kafka, ETL","python, mlops, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, apache beam, kafka, etl","apache beam, aws, azure, cicd, devops, etl, gcp, iac, kafka, mlops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Palos Park, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742676613,2023-12-17,Homer,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Machine Learning, ETL, ELT, Data Pipelines, AWS, Azure, GCP, DevOps, CI/CD, IaC, Kafka, Spark, Scala, PySpark, Apache Beam, Communication skills, Collaboration skills","python, machine learning, etl, elt, data pipelines, aws, azure, gcp, devops, cicd, iac, kafka, spark, scala, pyspark, apache beam, communication skills, collaboration skills","apache beam, aws, azure, cicd, collaboration skills, communication skills, datapipeline, devops, elt, etl, gcp, iac, kafka, machine learning, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Oak Forest, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742679364,2023-12-17,Homer,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, MLOps, ETL, ELT, Apache Beam, Kafka, Spark, PySpark, AWS, Azure, GCP, CI/CD, IaC, DevOps, SQL, Data Analysis, Machine Learning, AI/ML, VR, NFT, Blockchain","python, mlops, etl, elt, apache beam, kafka, spark, pyspark, aws, azure, gcp, cicd, iac, devops, sql, data analysis, machine learning, aiml, vr, nft, blockchain","aiml, apache beam, aws, azure, blockchain, cicd, dataanalytics, devops, elt, etl, gcp, iac, kafka, machine learning, mlops, nft, python, spark, sql, vr"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Worth, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742675590,2023-12-17,Homer,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","ETL, ML Ops, Python, Apache Beam, Kafka, Spark, PySpark, AWS, Azure, GCP, CI/CD, IaC, Scala","etl, ml ops, python, apache beam, kafka, spark, pyspark, aws, azure, gcp, cicd, iac, scala","apache beam, aws, azure, cicd, etl, gcp, iac, kafka, ml ops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Willow Springs, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742677456,2023-12-17,Homer,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, ETL, ELT, Apache Beam, Spark, Scala, PySpark, Kafka, CI/CD, IaC, DevOps, MLOps, ML Ops, CI/CD, AWS, Azure, GCP, Machine learning, AI/ML, SQL, NoSQL, Data analysis, Data visualization, Data modeling, Data mining, Linux, Unix, Windows, Git, Jira, Confluence, Slack, Zoom, Google Meet, Microsoft Teams","python, etl, elt, apache beam, spark, scala, pyspark, kafka, cicd, iac, devops, mlops, ml ops, cicd, aws, azure, gcp, machine learning, aiml, sql, nosql, data analysis, data visualization, data modeling, data mining, linux, unix, windows, git, jira, confluence, slack, zoom, google meet, microsoft teams","aiml, apache beam, aws, azure, cicd, confluence, data mining, dataanalytics, datamodeling, devops, elt, etl, gcp, git, google meet, iac, jira, kafka, linux, machine learning, microsoft teams, ml ops, mlops, nosql, python, scala, slack, spark, sql, unix, visualization, windows, zoom"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Chicago Ridge, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742675589,2023-12-17,Homer,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, MLOps, Spark, Scala, PySpark, AWS, Azure, GCP, CI/CD, IaC (Infrastructure as Code), Apache Beam, Kafka, ETL, ELT, Data pipelines, Data quality, Performance issues, Machine Learning models, CI/CD pipelines, Model versioning, Testing, Rollout, Software development, DevOps, Project management, Full stack, QA, NFT marketplaces, VR imaging, AI/ML, Case study, Portfolio, Community collaboration, Local community events, Online hackathons, Competitions","python, mlops, spark, scala, pyspark, aws, azure, gcp, cicd, iac infrastructure as code, apache beam, kafka, etl, elt, data pipelines, data quality, performance issues, machine learning models, cicd pipelines, model versioning, testing, rollout, software development, devops, project management, full stack, qa, nft marketplaces, vr imaging, aiml, case study, portfolio, community collaboration, local community events, online hackathons, competitions","aiml, apache beam, aws, azure, case study, cicd, cicd pipelines, community collaboration, competitions, data quality, datapipeline, devops, elt, etl, full stack, gcp, iac infrastructure as code, kafka, local community events, machine learning models, mlops, model versioning, nft marketplaces, online hackathons, performance issues, portfolio, project management, python, qa, rollout, scala, software development, spark, testing, vr imaging"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Justice, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742676557,2023-12-17,Homer,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Spark, Scala, PySpark, AWS, Azure, GCP, Apache Beam, Kafka, ETL, CI/CD, IaC, DevOps, MLOps, Machine Learning, NLP, AI","python, spark, scala, pyspark, aws, azure, gcp, apache beam, kafka, etl, cicd, iac, devops, mlops, machine learning, nlp, ai","ai, apache beam, aws, azure, cicd, devops, etl, gcp, iac, kafka, machine learning, mlops, nlp, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Crestwood, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742675592,2023-12-17,Homer,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, ETL, ML Ops, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Kafka, Apache Beam, NFT, VR, AI/ML, Machine Learning, Software Development, Data Engineering","python, etl, ml ops, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, kafka, apache beam, nft, vr, aiml, machine learning, software development, data engineering","aiml, apache beam, aws, azure, cicd, data engineering, devops, etl, gcp, iac, kafka, machine learning, ml ops, nft, python, scala, software development, spark, vr"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Midlothian, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742678429,2023-12-17,Homer,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, MLOps, ETL, ELT, Apache Beam, Kafka, Spark, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Machine Learning, AI/ML, VR, NFT, QA, Blockchain","python, mlops, etl, elt, apache beam, kafka, spark, pyspark, aws, azure, gcp, devops, cicd, iac, machine learning, aiml, vr, nft, qa, blockchain","aiml, apache beam, aws, azure, blockchain, cicd, devops, elt, etl, gcp, iac, kafka, machine learning, mlops, nft, python, qa, spark, vr"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Alsip, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742674925,2023-12-17,Homer,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Machine Learning, ETL, ELT, Spark, Scala, PySpark, AWS, Azure, GCP, CI/CD, IaC, Kafka, DevOps","python, machine learning, etl, elt, spark, scala, pyspark, aws, azure, gcp, cicd, iac, kafka, devops","aws, azure, cicd, devops, elt, etl, gcp, iac, kafka, machine learning, python, scala, spark"
Sr. Data Engineer (AWS / SSIS) - PERM - 100% REMOTE,"Resource 1, Inc.","Naperville, IL",https://www.linkedin.com/jobs/view/sr-data-engineer-aws-ssis-perm-100%25-remote-at-resource-1-inc-3762921663,2023-12-17,Homer,United States,Mid senior,Remote,"Primary Responsibilities:
Design, build and manage scalable
data pipelines across (AWS) cloud data repositories
Create data flow diagrams, and document source to target mapping
Extract business requirements and translate them into actionable tasks
Maintain the
data warehouse performance
by optimizing batch processing through
parallelization, performance tuning, aggregations
etc.
Create user-facing
dashboards
that provide key insights at a glance for its specific audience
Recognize and adopt best practices and cost effective solutions in developing analytical insights on-premise and in the cloud
Provide guidance and mentor junior team members
Keep current with Business Intelligence data trends and technological innovations
Execute on POC’s with new technologies, drive innovation, and new ideas
Job Requirements:
5+ years leveraging
AWS
and its associated array of offerings (
Glue, Redshift, Athena, S3, Spectrum, PySpark)
Expertise in
Python and C#
2+ years leveraging Microsoft and its associated array of offerings (SQL, T-SQL, SSIS, SSRS, SSAS, Power BI)
Collaboration with a development team with established source control (GIT).
Data modeling (conceptual, logical and physical model design, Operation Data Stores, Enterprise Data Warehouses and Data Marts).
Business Intelligence and data warehousing design principles
and industry best practices including multi-dimensional modelling (star schemas, snowflakes, de-normalized models, handling “slow-changing” dimensions)
Show more
Show less","AWS, Glue, Redshift, Athena, S3, Spectrum, PySpark, Python, C#, SQL, TSQL, SSIS, SSRS, SSAS, Power BI, GIT, Data modeling, Business Intelligence, Data warehousing, Multidimensional modelling, Star schemas, Snowflakes, Denormalized models, Handling ""slowchanging"" dimensions","aws, glue, redshift, athena, s3, spectrum, pyspark, python, c, sql, tsql, ssis, ssrs, ssas, power bi, git, data modeling, business intelligence, data warehousing, multidimensional modelling, star schemas, snowflakes, denormalized models, handling slowchanging dimensions","athena, aws, business intelligence, c, datamodeling, datawarehouse, denormalized models, git, glue, handling slowchanging dimensions, multidimensional modelling, powerbi, python, redshift, s3, snowflakes, spark, spectrum, sql, ssas, ssis, ssrs, star schemas, tsql"
Business Data Analyst,School Specialty,"Lombard, IL",https://www.linkedin.com/jobs/view/business-data-analyst-at-school-specialty-3766325747,2023-12-17,Homer,United States,Mid senior,Remote,"Description
People Passion Purpose
Everything School Specialty offers is designed for one purpose – to help students succeed. We believe every student can flourish in an environment where they feel safe and inspired to explore and grow.
We’re Determined To Positively Impact The Future, One Child At a Time. We Need To Talk If You Share Our Passion
Transforming more than classrooms.®
Benefits
School Specialty offers Medical, Dental, & Vision plans (Effective Day 1), Wellness programs, Health Savings Accounts, Flexible Spending Accounts, 401 (k), Unlimited PTO for Salaried Exempt employees, which can also be used for dedicated
volunteer
hours, Education Reimbursement, Paid Holidays, Fall & Winter Flexible Hours, Employee Discounts and much more!
Business Data Analyst
The Business Data Analyst is a critical member of School Specialty’s enterprise Analytics team. Business Data Analysts work closely with the other members of the Analytics team, and work cross-functionally with database and infrastructure professionals, program managers, data analysts, and, especially, end business users to research, develop, test, and deploy BI and Analytic solutions. The ideal candidate will be eager to learn, hungry for professional development, and feel most at home working in a less structured environment that demands creativity, adaptability, and self-motivation. To succeed in this role, excellent communication, with technical and non-technical colleagues, is a must.
The base salary range for this role is
$65K-85K Annually
Summary Of Primary Responsibilities
Assist in designing, writing, and testing analyses and reports in an enterprise data warehouse (EDW) environment using SQL
Perform detailed requirements analysis and translate business needs into Analytics / technical requirements
Design, write, and test analyses, reports, and dashboards in an enterprise data warehouse (EDW) environment using SQL, Snowflake, and Tableau
Assist in building extract, transform, and load (ETL) processes
Develop business, statistical, and learning models to create predictive and prescriptive data elements within the enterprise data flow
Build end-user reporting and dashboards using visualization tools such as Tableau
Gather critical information from meetings with various stakeholders and produce useful reports
Prioritize initiatives based on business needs and requirements
Contribute every stage of analytic projects (following industry standard methods, such as the Cross-Industry Standard Process for Data Mining (CRISP-DM) model)
Evaluate business processes, anticipate requirements, uncover areas for improvement, and develop and implement solutions
Lead ongoing reviews of business processes and develop optimization strategies
Minimum Experience Requirements
Bachelor’s degree in business intelligence, data analytics, statistics, informatics, or similar field, with relevant experience in business intelligence and analytics preferred, or
Associate degree in related field, with a minimum of three years of relevant, progressive experience, or
Five years of relevant, progressive experience
Expert in SQL, reporting, and data visualization- specifically Tableau
Experience interacting with all levels of management, including senior leadership
Exceptional analytical and critical reasoning thinking skills
Ability to influence stakeholders and work closely with them to determine acceptable solutions
Excellent documentation skills
Preferred Knowledge and Skills
Basic familiarity with the principles of ETL processes, relational databases, data warehousing, and dimensional modeling
Experience with, or education in, Python, R, SAS, or other language (or application) typically used for statistical learning, modeling, and/or data munging
Basic familiarity with current-generation data visualization tools and information design principles
Experience with Oracle EDW and Snowflake, or equivalent, environments
Experience or familiarity with semi-structured data (beneficial)
Ability to collaborate, high emotional intelligence, and a passion for serving our internal customers
Familiarity with, and understanding of, business principles and functions
Disclaimers
The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. All personnel may be required to perform duties outside of their normal responsibilities from time to time, as needed.
School Specialty, LLC. is a Drug Free Workplace. All applicants are subject to a drug screen and background check as a condition of employment.
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity and/or expression, status as a veteran, and basis of disability or any other federal, state or local protected class.
If you need a reasonable accommodation for any part of the employment process, please contact us by email at Opportunities@SchoolSpecialty.com and let us know the nature of your request and your contact information.
Job Grade: 11
Show more
Show less","SQL, Tableau, Snowflake, ETL, Data warehousing, Data visualization, Statistical learning, Modeling, Data munging, Oracle EDW, R, SAS, Python, Dimensional modeling","sql, tableau, snowflake, etl, data warehousing, data visualization, statistical learning, modeling, data munging, oracle edw, r, sas, python, dimensional modeling","data munging, datawarehouse, dimensional modeling, etl, modeling, oracle edw, python, r, sas, snowflake, sql, statistical learning, tableau, visualization"
"AWS Database Engineer || Elmhurst, IL (Hybrid Role: 3 days Onsite and 2 days Remote)",Steneral Consulting,"Elmhurst, IL",https://www.linkedin.com/jobs/view/aws-database-engineer-elmhurst-il-hybrid-role-3-days-onsite-and-2-days-remote-at-steneral-consulting-3762487204,2023-12-17,Homer,United States,Mid senior,Hybrid,"Share only 2 profiles
Hybrid in Elmhurst, IL
local candidates only
Duration: 6+ Months C2H
Our client is a global organization, named one of the largest in the Automobile industry! They are rapidly growing and going through a robust digital transformation. The business and senior leadership have put a huge emphasis on technology initiatives and are looking for passionate team members to contribute to the growth and development of the technical environment.
Interested in being apart of a dynamic data engineering and analytics team? This role offers a unique opportunity to engage with cutting-edge technologies in the realms of data engineering and cloud computing. You will have the chance to craft high-performance, scalable data pipelines while upholding industry-leading standards in data management, data quality, and automation. You will join at a pivotal time for the organization as they are in the midst of a global AWS migration. You will collaborate with the entire data organization, providing support to legacy systems while also adapting to the new cloud platform. If you are passionate about data and eager to leverage it for meaningful insights that drive business value, then we encourage you to join this team!
Required Experience;
6+ years experience working on a enterprise data engineering team (experience building/enhancing data lakes and big data technologies).
4+ years of database experience (relational and non-relational/NoSQL databases).
Strong SQL experience (Must write complex queries from scratch).
Strong scripting experience with ability to build complex data pipelines in python.
Experience with SQL Server (SSIS, SSRS, TSQL, C#).
At least 3 years AWS experience, Lambda required (AWS IAM, S3, API Gateway, Glue, Lake Formation, Redshift, and RDS).
Must have experience with Apache Airflow.
Foundational expertise in IaC (terraform and CloudFormation).
A strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling.
Knowledge of ETL tools (Talend Open Studio, Informatica, Pentaho).
If you are familiar with any of the technologies below you are a star!
Kubernetes
DevOps (Gitlab, Bitbucket)
Spark
NoSQL database
DynamoDB
MongoDB
EMR
Presto
Show more
Show less","Data engineering, Cloud computing, AWS, Lambda, IAM, S3, API Gateway, Glue, Lake Formation, Redshift, RDS, Apache Airflow, IaC, Terraform, CloudFormation, Data warehousing, Business intelligence (BI), Data security, Data quality, Data profiling, ETL tools, Kubernetes, DevOps, Spark, NoSQL database, DynamoDB, MongoDB, EMR, Presto","data engineering, cloud computing, aws, lambda, iam, s3, api gateway, glue, lake formation, redshift, rds, apache airflow, iac, terraform, cloudformation, data warehousing, business intelligence bi, data security, data quality, data profiling, etl tools, kubernetes, devops, spark, nosql database, dynamodb, mongodb, emr, presto","apache airflow, api gateway, aws, business intelligence bi, cloud computing, cloudformation, data engineering, data profiling, data quality, data security, datawarehouse, devops, dynamodb, emr, etl tools, glue, iac, iam, kubernetes, lake formation, lambda, mongodb, nosql database, presto, rds, redshift, s3, spark, terraform"
Data Analyst,Darwill,"Hillside, IL",https://www.linkedin.com/jobs/view/data-analyst-at-darwill-3766973254,2023-12-17,Homer,United States,Mid senior,Hybrid,"We are seeking a skilled and analytical Data Analyst/Business Intelligence Analyst to join our Data Science team. The ideal candidate will have a strong understanding of data analysis principles, proficiency in SQL and Python, and the ability to extract meaningful insights from consumer data. The selected candidate will play a crucial role in developing prospect targeting lists and driving data-driven marketing initiatives.
Responsibilities:
Working within the Databricks Data Intelligence Platform:
Utilize SQL, Python and PySpark to extract, transform, and analyze consumer data from various sources
Clean and prepare data for analysis, ensuring data integrity and consistency
Perform data analysis to identify trends, patterns, and insights from consumer behavior
Develop prospect targeting lists based on data-driven segmentation and profiling
Collaborate with account managers and account executives to integrate data-driven insights into marketing strategies for clients
Collaborate with data engineers to identify available source data and ELT process updates necessary to address new client requests
Prepare and present data-driven reports to stakeholders, effectively communicating findings and recommendations
Stay up-to-date with the latest data analysis techniques and tools
Qualifications:
Databricks Data Analyst Associate certification
Bachelor's degree in Data Science, Business Intelligence, or a related field
3+ years of experience and high proficiency in data analysis or business intelligence using SQL and Python for data manipulation and analysis
Experience developing dashboards with data visualization tools such as Tableau or Power BI
Excellent analytical and problem-solving skills
Strong written communication and presentation skills
Ability to work independently and as part of a team
Highly desirable, but not required to apply:
Experience with machine learning and data mining techniques
Experience with CRM systems and marketing automation platforms
Experience in developing predictive models and forecasting
Experience with market segmentation techniques
Work Environment/Physical Demands:
May be required to work extended/evening hours as needed
This job offers hybrid work, with approximately 2 days per week in-office at our Oakbrook Terrace headquarters location and the remainder remote from the Chicago area
Show more
Show less","SQL, Python, PySpark, Data Analysis, Business Intelligence, Data Science, Databricks Data Intelligence Platform, Prospect Targeting, DataDriven Marketing, Data Integrity, Tableau, Power BI, SQL Data Analyst Associate certification, Data Visualization, Analytical Skills, ProblemSolving Skills, Communication Skills, Data Mining, Machine Learning, CRM Systems, Marketing Automation, Predictive Modeling, Forecasting, Market Segmentation","sql, python, pyspark, data analysis, business intelligence, data science, databricks data intelligence platform, prospect targeting, datadriven marketing, data integrity, tableau, power bi, sql data analyst associate certification, data visualization, analytical skills, problemsolving skills, communication skills, data mining, machine learning, crm systems, marketing automation, predictive modeling, forecasting, market segmentation","analytical skills, business intelligence, communication skills, crm systems, data integrity, data mining, data science, dataanalytics, databricks data intelligence platform, datadriven marketing, forecasting, machine learning, market segmentation, marketing automation, powerbi, predictive modeling, problemsolving skills, prospect targeting, python, spark, sql, sql data analyst associate certification, tableau, visualization"
Sr. Data Engineer (Remote),Chamberlain Group,"Oak Brook, IL",https://www.linkedin.com/jobs/view/sr-data-engineer-remote-at-chamberlain-group-3699221246,2023-12-17,Homer,United States,Mid senior,Hybrid,"If you are a current Chamberlain Group employee, please click here to apply through your Workday account.
Chamberlain Group is a global leader in access solutions with top brands, such as LiftMaster and Chamberlain, found in millions of homes, businesses, and communities worldwide.
As a leader in the Smart Home industry, we boast one of the largest IoT install bases, with innovative products consisting of cameras, locks, card readers, garage door openers, gates and more, all powered by our myQ digital ecosystem.
This role is responsible for providing technical expertise and leadership to design and deliver end-to-end data engineering solutions to support advanced analytics capabilities and drive innovation and decision-making
across Chamberlain.
Essential Duties And Responsibilities
Build and maintain real-time and batch data pipelines across the advanced analytics platform.
Design, develop and orchestrate highly robust and scalable ETL pipelines.
Design and implement Dimensional and NoSQL data modelling as per the business requirements.
Develop highly optimal codebase and perform Spark optimizations for Big Data use cases.
Design, develop and deploy optimal monitoring and testing strategy for the data products.
Collaborate with stakeholders and advanced analytics business partners to understand business needs and translate requirements into scalable data engineering solutions.
Collaborate with data scientists to prepare data for model development and production.
Collaborate with data visualization and reporting application developers to ensure the sustainability of production applications and reports.
Collaborate with data architects on the enhancement of Chamberlain’s enterprise data architecture and platforms.
Provide leadership to third-party contractors.
Comply with health and safety guidelines and rules.
Protect CGI’s reputation by keeping information confidential.
Maintain professional and technical knowledge by attending educational workshops, professional publications, establishing personal networks, and participating in professional societies.
Minimum Qualifications
Education/Certifications:
Bachelor’s degree in computer science or related quantitative field of study
Experience:
4+ years of professional experience
Knowledge, Skills, and Abilities:
Natural sense of urgency, teamwork, and collaboration reflected in daily work ethic.
Proficient in Spark or Databricks, Cloud Data Engineering Services preferably Azure, Streaming frameworks like Event Hubs or Kafka.
Proficient in Microsoft Office.
Familiarity with modern Machine Learning Operationalization techniques.
Agile methodologies.
Familiarity with Data visualization tools, such as Qlik or Power BI.
Preferred Qualifications
Education/Certifications:
Master’s degree in computer science or related quantitative field of study
Experience:
4+ years of professional experience
2+ years of professional experience delivering engineering for advanced analytics or data science solutions
Knowledge, Skills, and Abilities:
Agile methodologies
Experience with IoT Data Architecture.
Machine Learning Operationalization (MLOps) proficiency.
REST API design and development.
Proficiency with streaming design patterns.
Chamberlain Group wants all of its employees to succeed and encourages people of all backgrounds to apply. We’re proud to be an Equal Opportunity Employer, and you’ll be considered for this role regardless of race, color, religion, sex, national origin, age, sexual orientation, ancestry; marital, disabled or veteran status. We’re committed to fostering an environment where people of all lived experiences feel welcome.
Persons with disabilities who anticipate needing accommodations for any part of the application process may contact, in confidence Recruiting@Chamberlain.com.
NOTE: Staffing agencies, headhunters, recruiters, and/or placement agencies, please do not contact our hiring managers directly.
Show more
Show less","Data Engineering, Big Data, ETL, Spark, Databricks, Cloud Data Engineering Services, Azure, Streaming frameworks, Event Hubs, Kafka, Microsoft Office, Machine Learning Operationalization, Agile methodologies, Data visualization tools, Qlik, Power BI, REST API, IoT Data Architecture, Machine Learning Operationalization (MLOps)","data engineering, big data, etl, spark, databricks, cloud data engineering services, azure, streaming frameworks, event hubs, kafka, microsoft office, machine learning operationalization, agile methodologies, data visualization tools, qlik, power bi, rest api, iot data architecture, machine learning operationalization mlops","agile methodologies, azure, big data, cloud data engineering services, data engineering, data visualization tools, databricks, etl, event hubs, iot data architecture, kafka, machine learning operationalization, machine learning operationalization mlops, microsoft office, powerbi, qlik, rest api, spark, streaming frameworks"
Senior Cloud Data Engineer,BDO USA,"Oak Brook, IL",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765467844,2023-12-17,Homer,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Cloud Computing, Data Warehousing, Data Modeling, Data Ingestion, Data Visualization, Automation, Machine Learning, AI Algorithms, Data Lake, DevOps, Python, Scala, SQL, C#, Java, Linux, Git","data analytics, business intelligence, cloud computing, data warehousing, data modeling, data ingestion, data visualization, automation, machine learning, ai algorithms, data lake, devops, python, scala, sql, c, java, linux, git","ai algorithms, automation, business intelligence, c, cloud computing, data ingestion, data lake, dataanalytics, datamodeling, datawarehouse, devops, git, java, linux, machine learning, python, scala, sql, visualization"
Lead Data Engineer (Hybrid),Chamberlain Group,"Oak Brook, IL",https://www.linkedin.com/jobs/view/lead-data-engineer-hybrid-at-chamberlain-group-3699216993,2023-12-17,Homer,United States,Mid senior,Hybrid,"If you are a current Chamberlain Group employee, please click here to apply through your Workday account.
Chamberlain Group is a global leader in access solutions with top brands, such as LiftMaster and Chamberlain, found in millions of homes, businesses, and communities worldwide.
As a leader in the Smart Home industry, we boast one of the largest IoT install bases, with innovative products consisting of cameras, locks, card readers, garage door openers, gates and more, all powered by our myQ digital ecosystem.
This role is responsible for providing technical leadership and execution expertise to design and deliver end-to-end data engineering solutions to support new product development and advanced analytics capabilities and drive innovation and decision-making across Chamberlain.
Summary
This role is responsible for providing technical leadership and execution expertise to design and deliver end-to-end data engineering solutions to support new product development and advanced analytics capabilities.
Collaborate with stakeholders, including product owners and developers, as well as advanced analytics colleagues, including data scientists and data visualization developers, to understand product and business requirements and translate into scalable data engineering solutions
Build and maintain real-time and batch data pipelines across the advanced analytics platform, including design of data flows, procedures, and schedules for internally and externally sourced data
Collaborate with data scientists to prepare data for model development and production
Collaborate with data visualization and reporting application developers to ensure the sustainability of production applications and reports
Partner with the Lead Architect in data architecture and solution design
Perform design, code, and test plan reviews in support of maintaining data engineering standards
Comply with health and safety guidelines and rules
Protect CG's reputation by keeping information confidential
Maintain professional and technical knowledge by attending educational workshops, professional publications, establishing personal networks, and participating in professional societies
Contribute to the team effort by accomplishing related results and participating on projects as needed
Minimum Qualifications:
Education/Certifications:
Bachelor’s degree in computer science or related quantitative field of study
Experience:
7+ years of hands-on professional data engineering experience
2+ years of professional experience delivering engineering for advanced analytics or data science solutions, preferably related to transportation, logistics, or network optimization
Demonstrated experience with technologies such as Databricks, Azure Data Factory, Event Hubs and Azure Devops
Knowledge, Skills, and Abilities:
Natural sense of urgency, teamwork, and collaboration reflected in daily work ethic
Expert proficiency in programming and querying languages including Python and SQL
Proficiency in big data processing, including Apache Spark or Databricks
Proficiency in building and optimizing big data and real time pipelines, datasets, and architectures in cloud environments, preferably Azure using services such as Event Hubs or Kafka
Strong knowledge of modern machine learning techniques, preferred
Agile methodologies
Data visualization tools, such as Power BI, preferred
Chamberlain Group wants all of its employees to succeed and encourages people of all backgrounds to apply. We’re proud to be an Equal Opportunity Employer, and you’ll be considered for this role regardless of race, color, religion, sex, national origin, age, sexual orientation, ancestry; marital, disabled or veteran status. We’re committed to fostering an environment where people of all lived experiences feel welcome.
Persons with disabilities who anticipate needing accommodations for any part of the application process may contact, in confidence Recruiting@Chamberlain.com.
NOTE: Staffing agencies, headhunters, recruiters, and/or placement agencies, please do not contact our hiring managers directly.
Show more
Show less","Python, SQL, Apache Spark, Databricks, Azure Data Factory, Event Hubs, Azure DevOps, Machine Learning, Agile, Power BI","python, sql, apache spark, databricks, azure data factory, event hubs, azure devops, machine learning, agile, power bi","agile, apache spark, azure data factory, azure devops, databricks, event hubs, machine learning, powerbi, python, sql"
Developer - Database III_US #: 23-06806,HireTalent - Diversity Staffing & Recruiting Firm,"Na-Au-Say, IL",https://www.linkedin.com/jobs/view/developer-database-iii-us-%23-23-06806-at-hiretalent-diversity-staffing-recruiting-firm-3768841303,2023-12-17,Homer,United States,Mid senior,Hybrid,"The Role: Quantitative Data Engineer, Research Signals
The Team: The Research Signals group provides independent research and investment consulting services to the institutional asset management community. The group researches and develops individual factors and multi-factor models across a range of asset classes by applying a systematic evaluation process to a variety of fundamental, technical, and industry-specific data sources.
The Impact
The Quantitative Data Engineer develops and supports new and existing quantitative products and datasets. The Research Signals team’s research and datasets are widely used across the investment management industry. This position will partner with stakeholders in technology, research and operations and the broader product management team to build a data platform, deliver robust production systems and ensure high data integrity of our quantitative products which include alpha factors, risk models and quantitative investing software solutions.
What’s In It For You
Build technology workflows to deliver new data products and enhance the Research Signals suite of quantitative factors and models
Work with our specialized quantitative research team to deliver a variety of proprietary and unique datasets
Exposure to quantitative research, strategy development, and other innovation initiatives across *** Market Intelligence
Responsibilities
Develop both on-premises and cloud-based data ingestion processes using SQL,
Python
Engineer data models and infrastructure for a wide variety of market and alternative dataset
Author tests to validate data quality and measure the stability of the data acquisition processes
Work directly with Analysts, Product Specialists, and the technology team to understand requirements and provide end-to-end data solutions
Communicate with data providers to onboard new datasets and troubleshoot technical issues
Investigate and defuse time-sensitive data incidents
What We’re Looking For
Bachelor/Master’s degree in Computer Science, Information Systems, or related field
Strong analytical, data and programming skills (Python/SQL/NoSQL/JavaScript)
3&plus;years of experience with large data sets ETL and techniques to architect them for performance, experience using alternative unstructured data is a plus
1&plus; year of experience with cloud computing services, AWS preferred
Aptitude for designing infrastructure, and data products for Quant/Data Scientists is a plus
Ability to work effectively in an agile environment with numerous stakeholders on complex research and new development projects
A genuine interest in investment strategies, equities, and fixed income. Asset management industry experience is a plus.
Strong verbal and written communication skill, must be a team player
Show more
Show less","Python, SQL, NoSQL, JavaScript, AWS, Data modeling, Data engineering, ETL, Unstructured data, Agile methodology, Investment strategies, Equities, Fixed income, Cloud computing","python, sql, nosql, javascript, aws, data modeling, data engineering, etl, unstructured data, agile methodology, investment strategies, equities, fixed income, cloud computing","agile methodology, aws, cloud computing, data engineering, datamodeling, equities, etl, fixed income, investment strategies, javascript, nosql, python, sql, unstructured data"
Data Analyst / Business Analyst,Sterling Engineering,"Elmhurst, IL",https://www.linkedin.com/jobs/view/data-analyst-business-analyst-at-sterling-engineering-3709863802,2023-12-17,Homer,United States,Mid senior,Hybrid,"Position:
Business Analyst / Project Manager
Location:
Itasca, IL 60143 - this is onsite daily.
I am looking for a Business Analyst with project management skills that has ERP System experience, dashboard reporting and report development experience. Come join a collaborative organization that values hard work, recognition, work life balance, competitive pay, and benefits!
In this role you will:
Administer, support, monitor, maintain and upgrade the ERP environment.
Provide internal support and training on ERP operation.
Support the different phases of the ERP development lifecycle, adhering to defined standards and methodologies.
Create or revise reports from ERP system, using Crystal Reports and/or MS Excel
Answer routine, day-to-day ERP questions.
Assist with resolving ERP-related issues.
Create queries for pervasive SQL database.
Build interactive reports in Microsoft Excel (VBA experience desired).
Collaborate with end users to design and optimize reports.
Qualifications
B.S. in Computer Science or related coursework.
Experience in ERP implementation and system integration (Preferred).
SQL querying ability
MS Excel - Reports development
Dashboard creation from ERP
Overview:
Sterling Engineering / Staffing has a rich history of delivering top talent to our clients. We are a nationwide Staffing Firm that has been in business for over 54 years. With over 200 currently active clients, Sterling works within the Automation, Energy, Facilities, Information Technology, Food, Logistics/Supply Chain, Manufacturing, Packaging, Life Sciences, Pharmaceuticals, Engineering and R&D industries.
Qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or genetic information.
#POST
Show more
Show less","Business Analyst, Project Management, ERP Systems, Dashboard Reporting, Report Development, Crystal Reports, MS Excel, SQL, VBA, Computer Science","business analyst, project management, erp systems, dashboard reporting, report development, crystal reports, ms excel, sql, vba, computer science","business analyst, computer science, crystal reports, dashboard reporting, erp systems, ms excel, project management, report development, sql, vba"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Cicero, IL",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712035,2023-12-17,Homer,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning, Data mining, Data cleaning, Data normalization, Data modeling, Data pipelines, Data platforms, Data processing, Data governance, Data compliance, Data visualization, Pandas, R, Statistical analysis, NoSQL, Conversational AI, Recommender systems, Microservices, Docker images, Streamprocessing systems","python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data mining, data cleaning, data normalization, data modeling, data pipelines, data platforms, data processing, data governance, data compliance, data visualization, pandas, r, statistical analysis, nosql, conversational ai, recommender systems, microservices, docker images, streamprocessing systems","airflow, aws, azure, bash, conversational ai, data cleaning, data compliance, data governance, data mining, data normalization, data platforms, data processing, datamodeling, datapipeline, docker, docker images, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, microservices, nosql, pandas, python, r, recommender systems, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, streamprocessing systems, visualization"
Data Scientist,Koch Industries,"Lisle, IL",https://www.linkedin.com/jobs/view/data-scientist-at-koch-industries-3756940542,2023-12-17,Homer,United States,Mid senior,Hybrid,"Your Job
We are seeking a motivated and innovative Data Scientist to help advance our HR Analytics journey in support of our vision. A successful candidate will bring advanced knowledge of best-in-class data science methodologies, deep understanding of AI/ML space, proven track record of solving business problems using data science and ability to work with global teams to deliver and execute complex analytics problems. You must be enthusiastically collaborative, value seeking, open to challenge and be challenged with new ideas and established approaches with an appetite for learning and innovation.
Our Team
This role is part of the Global KGS HR Technology organization requiring excellent communication and collaboration skills with other technical groups as well as business leaders. You will work with Business HR partners and leaders to develop and implement advanced analytics solutions to capture unmet opportunities and create value for our customers.
What You Will Do
Exhibits a can-do, growth mindset, consistently pursuing skill enhancement and enthusiastically tackling new challenges to drive advancement of business and data science capabilities
Communicate with clients to understand the challenges they face and explore data driven solutions
Exploring and extracting data from multiple sources and interrogate it to discover trends and patterns, creating meaningful insights
Assess the effectiveness and accuracy of new data sources and data gathering techniques
Programs data science and analytics solutions in languages such as Python, R
Applies strong knowledge of statistics and mathematics including hypothesis testing, probability and regression for making informed decisions based on data
Researching, developing, and deploying machine learning models (supervised and unsupervised) to deliver predictive analytics
Maintaining and improving the machine learning pipelines, processes and products
Design/Develops data visualizations for effectively communicating data findings to both technical and non-technical audiences
Develop processes and tools to monitor and analyze model performance and data accuracy.
Provide best practices and guidance to other team members on data science projects.
Who You Are (Basic Qualifications)
3+ years of experience on the whole life cycle of machine learning (ML) model building for solving real-world complex problems.
Experience in Data Analysis techniques, EDA, Data Visualization to effectively communicate to stakeholders, clarify requirements and make effective suggestions to achieve business values.
Experience in Time Series Forecasting /Tree based models/and linear models.
3+ years of experience in SQL coding, python programming skills coupled with demonstrable experience using data science packages (numpy, pandas, sklearn, etc.)
Knowledge of bringing machine learning models from prototype to production.
What Will Put You Ahead
Experience working in HR/Recruiting/Talent analytics
Bachelor or Masters degree in a quantitative field such as Finance, Mathematics, Analytics, Data Science, Computer Science, Engineering or relevant work experience
At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.
Hiring Philosophy
All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.
Who We Are
As a Koch company, Koch Global Services (KGS) creates solutions spanning technology, human resources, finance, project management and anything else our businesses need. With locations in India, Mexico, Poland and the United States, our employees have the opportunity to make a global impact.
At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.
Our Benefits
Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.
Equal Opportunities
Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information: http://www.kochcareers.com/doc/Everify.pdf
Show more
Show less","Data Science, AI/ML, Python, R, SQL, Statistics, Mathematics, Data Analysis, EDA, Data Visualization, Machine Learning, Supervised Learning, Unsupervised Learning, Predictive Analytics, Model Monitoring, Model Performance, Data Accuracy, Time Series Forecasting, Tree Based Models, Linear Models, Numpy, Pandas, Sklearn, Data Science Packages, Production Deployment, HR Analytics, Talent Analytics, Tableau, Power BI","data science, aiml, python, r, sql, statistics, mathematics, data analysis, eda, data visualization, machine learning, supervised learning, unsupervised learning, predictive analytics, model monitoring, model performance, data accuracy, time series forecasting, tree based models, linear models, numpy, pandas, sklearn, data science packages, production deployment, hr analytics, talent analytics, tableau, power bi","aiml, data accuracy, data science, data science packages, dataanalytics, eda, hr analytics, linear models, machine learning, mathematics, model monitoring, model performance, numpy, pandas, powerbi, predictive analytics, production deployment, python, r, sklearn, sql, statistics, supervised learning, tableau, talent analytics, time series forecasting, tree based models, unsupervised learning, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Cicero, IL",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773086825,2023-12-17,Homer,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Statistical Analysis, Data Visualization, Data Pipelines, Agile ML Data Ops, Data Mining, Data Cleaning, Data Modeling, Data Platforms, Data Frameworks, Big Data, NLP, Tech Leadership, Cloud Computing, SQL, Python, Java, bash, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, NoSQL, ETL, Conversational AI, Recommender Systems, Microservices, Kafka, Storm, SparkStreaming, Machine Learning","data engineering, statistical analysis, data visualization, data pipelines, agile ml data ops, data mining, data cleaning, data modeling, data platforms, data frameworks, big data, nlp, tech leadership, cloud computing, sql, python, java, bash, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, etl, conversational ai, recommender systems, microservices, kafka, storm, sparkstreaming, machine learning","agile ml data ops, airflow, aws, azure, bash, big data, cloud computing, conversational ai, data cleaning, data engineering, data frameworks, data mining, data platforms, datamodeling, datapipeline, docker, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, microservices, nlp, nosql, python, recommender systems, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, tech leadership, visualization"
Inventory Optimization Analyst (Data Scientist)***Analyste optimisation d'inventaire,IPEX by Aliaxis,"Oakville, Ontario, Canada",https://ca.linkedin.com/jobs/view/inventory-optimization-analyst-data-scientist-analyste-optimisation-d-inventaire-at-ipex-by-aliaxis-3784371476,2023-12-17,Hamilton, Canada,Mid senior,Onsite,"IPEX is one of the North American leading providers of advanced plastic piping systems. Our mission is to shape a better tomorrow by connecting people with water and energy.
We currently have an exciting opportunity for an experienced Data Scientist to work in our Supply Chain Department as
Inventory Optimization Analyst
.
This role is based in our Oakville office, on a hybrid schedule, and reports to the
Director, End-To-End Supply Chain Planning
.
Do not miss the opportunity to join a broad family of people-centric professionals, thought leaders and rapid thinkers, entrepreneurs in spirit and status quo-fighters!
Job Summary
The Inventory Optimization Analyst ensures that the inventory planning models are optimized to achieve and exceed organization objectives for service to customers and inventory health, for a large portfolio of finished goods products.
The selected candidate acts as a focal point for root cause analysis on inventory levels above or below targets, using large datasets and data modelling, and collaborating with cross functional stakeholders to problem solve and bring inventory into optimal levels.
Principal Activities
Create models using inventory data to provide the minimum and maximum inventory targets within corporate inventory objectives
Provide regular reporting on the Corporate KPIs for Inventory Quality Ratio (IQR), and conducts root cause analysis with corrective action plans
Implement efficient workflows to engage key stakeholders as needed for action on inventory optimization
Prepare inventory lists for evaluation by stakeholders
Perform regular follow-ups with cross functional stakeholders to address inventories that do not meet the IQR objective.
Partner with our Supply Planners for root cause analysis and corrective actions identification
Collaborate with our Demand Planners to confirm demand, to validate any inventories that may be too low or in excess
Connect regularly with our Purchasing team on inventory planning parameters for purchased items
Work with our Inventory Deployment team on actions related to excess inventory by location
Provide expertise on best practice for inventory and data optimization, including data modelling, reporting, and analytics.
Monitor and report on progress, resolve bottlenecks, and escalate in a timely manner
Use good judgement to intervene as needed to achieve KPIs for inventory and fill rates.
Job Requirements
University degree with a specialty in Data Science, Supply Chain, Data Analytics, or Mathematics and Statistics discipline.
5+ year of relevant inventory optimization and data modelling experience, preferably within an industrial manufacturing or CPG environment; or other relevant transferable experience
Strong analytical skills, ability to administer, clean, interpret, and analyze large datasets to provide business insights for decision-making.
Proven experience with Power BI development (DAX) and Excel macros using VBA. Experience with data visualization and analysis using Power BI is highly desirable
Strong ability to explain data findings, including creating and delivering PowerPoint presentations, and facilitating working sessions
Proficiency in Microsoft Office and experience with process mapping using MS Visio
Familiarity with data modelling and data analysis
Experience with SAP Inventory Planning and optimization, is an asset
Knowledge of SQL databases and Python is a nice-to-have
Strong communication (written as well as spoken), interpersonal, and coordination skills
Factual approach to problem solving and challenges
Ability to work both independently and as a team player
High attention to detail with a process improvement mindset
Adept at working in a fast-moving environment and consistency meeting deadlines
IPEX is committed to providing accommodations for people with disabilities throughout the recruitment process and, upon request, will work with qualified job applicants to provide suitable accommodation in a manner that takes into account the applicant’s accessibility needs due to disability. Accommodation requests are available to candidates taking part in all aspects of the selection process for IPEX jobs. To request an accommodation, please contact HR at HR@ipexna.com
Show more
Show less","Data Science, Supply Chain, Data Analytics, Mathematics, Statistics, Inventory Optimization, Data Modelling, Power BI, DAX, Excel, VBA, PowerPoint, SAP, Inventory Planning, SQL, Python, Microsoft Office, MS Visio, Process Mapping","data science, supply chain, data analytics, mathematics, statistics, inventory optimization, data modelling, power bi, dax, excel, vba, powerpoint, sap, inventory planning, sql, python, microsoft office, ms visio, process mapping","data modelling, data science, dataanalytics, dax, excel, inventory optimization, inventory planning, mathematics, microsoft office, ms visio, powerbi, powerpoint, process mapping, python, sap, sql, statistics, supply chain, vba"
QC Data Review Scientist,Cambrex,"Charles City, IA",https://www.linkedin.com/jobs/view/qc-data-review-scientist-at-cambrex-3641370149,2023-12-17,Charles City,United States,Mid senior,Onsite,"Company Information
You Matter to Cambrex.
Cambrex is a leading global contract development and manufacturing organization (CDMO) that provides drug substance, drug product, and analytical services across the entire drug lifecycle. With more than 40 years of experience and a growing team of over 2,000 professionals servicing global clients, Cambrex is a trusted partner in branded and generic markets for API and dosage form development and manufacturing.
Your Work Matters.
At Cambrex, we strive to build a culture where all colleagues have the opportunity to:
engage in work that matters to our customers and the patients they serve
learn new skills and enjoy new experiences in an engaging and safe environment
strengthen connections with coworkers and the community
We’re committed to attracting and nurturing a passionate team of valued professionals in our fast-paced and growing company. We offer a competitive benefits package that includes healthcare, life insurance, planning for retirement, and more!
Your Future Matters.
Known for our customer-focused scientific and manufacturing excellence, as well as our strong commitment to quality and safety, we offer a range of career and growth opportunities across our global network of locations. Together with our customers, we aim to improve the quality of life for patients around the world. Start a career where You Matter by applying today!
Job Overview
The Data Review Scientist will be responsible for reviewing laboratory data and supporting documentation to ensure that they are compliant with SOPs, and compendial and CGMP requirements. This individual must have knowledge of instrument theory in order to thoroughly vet and review data and draw conclusions based on collected data.
Responsibilities
Reviews raw technical data provided by technicians, chemists and laboratory supervision to ensure accuracy, completeness and correct scientific conclusions have been drawn. Data may include Analytical Records, notebooks, logbooks, spectra, chromatograms, other instrument output and any other information supporting Analytical Records.
Identifies data inaccuracies, incomplete inferences and works directly with laboratory staff to provide guidance and direction on the correction of laboratory data packages.
Verifies and ensures that conclusions drawn on laboratory data are sound and accurate and representative of materials under examination.
Maintains and manages cGMP and controlled substance systems in all QC/PSG labs at defined levels of compliance in accordance with site approved procedures and protocols.
Maintains open communication within and across departments to ensure timely delivery and approval of laboratory documents. Works alternative schedules to support output and data review of laboratories based on business need.
All employees are required to adhere to DEA, EPA, FDA and cGMP regulations as they relate to the operation of the Company; and to adhere to all company safety rules and procedures. All employees are expected to report to work regularly and promptly. Other duties relating to departmental mission, not specifically detailed in this section may be assigned.
Education, Experience & Licensing Requirements
Bachelor’s degree in chemistry with three years’ experience in a pharmaceutical and cGMP environment highly preferred.
Experience in a laboratory environment highly preferred.
Knowledge of analytical instrumentation (GC, HPLC, FTIR, UV-VIS, etc.) required.
Strong chemistry knowledge with proven ability to handle various project loads is beneficial.
Cambrex Charles City offers an extraordinary opportunity, a competitive salary, and an exceptional benefits package including medical, dental, vision care and prescription, life, LTD, STD; 401(k) with employer match; tuition reimbursement; and the ‘Arthur I. Mendolia Scholarship Program.’ Relocation assistance will be offered for this position.
All requirements are subject to possible modifications to reasonably accommodate individuals with disabilities.
Cambrex is an Equal Opportunity / Affirmative Action employer and will consider all qualified applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
#CB
Show more
Show less","Data Review, Data Analysis, Data Interpretation, SOPs, cGMP, Controlled Substances, DEA, EPA, FDA, Analytical Instrumentation, GC, HPLC, FTIR, UVVIS, Chemistry","data review, data analysis, data interpretation, sops, cgmp, controlled substances, dea, epa, fda, analytical instrumentation, gc, hplc, ftir, uvvis, chemistry","analytical instrumentation, cgmp, chemistry, controlled substances, data interpretation, data review, dataanalytics, dea, epa, fda, ftir, gc, hplc, sops, uvvis"
QC Data Review Scientist,PharmiWeb.jobs: Global Life Science Jobs,"Charles City, IA",https://www.linkedin.com/jobs/view/qc-data-review-scientist-at-pharmiweb-jobs-global-life-science-jobs-3782946500,2023-12-17,Charles City,United States,Mid senior,Onsite,"Company Information
You Matter to Cambrex.
Cambrex is a leading global contract development and manufacturing organization (CDMO) that provides drug substance, drug product, and analytical services across the entire drug lifecycle. With more than 40 years of experience and a growing team of over 2,000 professionals servicing global clients, Cambrex is a trusted partner in branded and generic markets for API and dosage form development and manufacturing.
Your Work Matters.
At Cambrex, we strive to build a culture where all colleagues have the opportunity to:
engage in work that matters to our customers and the patients they serve
learn new skills and enjoy new experiences in an engaging and safe environment
strengthen connections with coworkers and the community
We’re committed to attracting and nurturing a passionate team of valued professionals in our fast-paced and growing company. We offer a competitive benefits package that includes healthcare, life insurance, planning for retirement, and more!
Your Future Matters.
Known for our customer-focused scientific and manufacturing excellence, as well as our strong commitment to quality and safety, we offer a range of career and growth opportunities across our global network of locations. Together with our customers, we aim to improve the quality of life for patients around the world. Start a career where You Matter by applying today!
Job Overview
The Data Review Scientist will be responsible for reviewing laboratory data and supporting documentation to ensure that they are compliant with SOPs, and compendial and CGMP requirements. This individual must have knowledge of instrument theory in order to thoroughly vet and review data and draw conclusions based on collected data.
Responsibilities
Reviews raw technical data provided by technicians, chemists and laboratory supervision to ensure accuracy, completeness and correct scientific conclusions have been drawn. Data may include Analytical Records, notebooks, logbooks, spectra, chromatograms, other instrument output and any other information supporting Analytical Records.
Identifies data inaccuracies, incomplete inferences and works directly with laboratory staff to provide guidance and direction on the correction of laboratory data packages.
Verifies and ensures that conclusions drawn on laboratory data are sound and accurate and representative of materials under examination.
Maintains and manages cGMP and controlled substance systems in all QC/PSG labs at defined levels of compliance in accordance with site approved procedures and protocols.
Maintains open communication within and across departments to ensure timely delivery and approval of laboratory documents. Works alternative schedules to support output and data review of laboratories based on business need.
All employees are required to adhere to DEA, EPA, FDA and cGMP regulations as they relate to the operation of the Company; and to adhere to all company safety rules and procedures. All employees are expected to report to work regularly and promptly. Other duties relating to departmental mission, not specifically detailed in this section may be assigned.
Education, Experience & Licensing Requirements
Bachelor’s degree in chemistry with three years’ experience in a pharmaceutical and cGMP environment highly preferred.
Experience in a laboratory environment highly preferred.
Knowledge of analytical instrumentation (GC, HPLC, FTIR, UV-VIS, etc.) required.
Strong chemistry knowledge with proven ability to handle various project loads is beneficial.
Cambrex Charles City offers an extraordinary opportunity, a competitive salary, and an exceptional benefits package including medical, dental, vision care and prescription, life, LTD, STD; 401(k) with employer match; tuition reimbursement; and the ‘Arthur I. Mendolia Scholarship Program.’ Relocation assistance will be offered for this position.
All requirements are subject to possible modifications to reasonably accommodate individuals with disabilities.
Cambrex is an Equal Opportunity / Affirmative Action employer and will consider all qualified applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
#CB
Show more
Show less","Chemistry, Analytical Instrumentation (GC HPLC FTIR UVVIS), Laboratory Environment, Good Manufacturing Practices (GMP), Standard Operating Procedures (SOPs), Data Accuracy, Data Completeness, Data Interpretation, Communication, CGMP and Controlled Substance Systems, Scientific Conclusions","chemistry, analytical instrumentation gc hplc ftir uvvis, laboratory environment, good manufacturing practices gmp, standard operating procedures sops, data accuracy, data completeness, data interpretation, communication, cgmp and controlled substance systems, scientific conclusions","analytical instrumentation gc hplc ftir uvvis, cgmp and controlled substance systems, chemistry, communication, data accuracy, data completeness, data interpretation, good manufacturing practices gmp, laboratory environment, scientific conclusions, standard operating procedures sops"
Sr. Data Engineer (Python/Spark),ASCENDING Inc.,"Rockville, MD",https://www.linkedin.com/jobs/view/sr-data-engineer-python-spark-at-ascending-inc-3787773630,2023-12-17,Yarmouth, Canada,Mid senior,Onsite,"Our client is a leading provider of financial and economic data to the investment banking industry. They are seeking an experienced Senior Data Engineer to join and growing team.
This role is only available for W2 or individual contracts. Please no C2C.
Hybrid, 1~2 days required onsite in Mclean, VA
Responsibilities:
Analyze system requirements and design responsive algorithms and solutions.
Build efficient ETL pipeline using Spark.
Use big data and cloud technologies to produce production quality code.
Engage in performance tuning and scalability engineering.
Work with team, peers and management to identify objectives and set priorities.
Perform related SDLC engineering activities like sprint planning and estimation.
Work effectively in small agile teams.
Provide creative solutions to problems.
Identify opportunities for improvement and execute.
Requirements:
4+ years of proven professional experience working in the IT industry.
AWS EMR experience.
Degree in Computer Science or related domains.
Candidate must have 3-5 years of demonstrated experience in building big data solutions on AWS cloud primarily with PySpark, preferably in Data Analytics space.
Minimum 3 - 5 years experience in application development using Python.
Minimum 3 - 5 years of experience working with Spark
Must have hands on experience developing data engineering solutions in Python using: S3, EMR, Glue, Athena, Kafka and notebooks.
Must have hands on development experience in building distributed Big Data solutions including ingestion, caching, processing, consumption, logging & monitoring.
Position requires strong technical communication skills.
Bonus:
Big Data and Open Source technical experience.
Agile (Scrum) methodology.
Experience developing SaaS application backends and APIs using a variety of tools.
Experience turning abstract business requirements into concrete technical plans.
Proficiency with algorithms (including time and space complexity analysis), data structures, and software architecture.
Must be a quick learner to evaluate and embrace new technologies in the Big data space.
Thanks for applying!
Powered by JazzHR
0CvKiM0dRw
Show more
Show less","AWS EMR, PySpark, Python, Spark, S3, EMR, Glue, Athena, Kafka, Notebooks, Agile, Scrum, Data engineering, Data structures, Software architecture","aws emr, pyspark, python, spark, s3, emr, glue, athena, kafka, notebooks, agile, scrum, data engineering, data structures, software architecture","agile, athena, aws emr, data engineering, data structures, emr, glue, kafka, notebooks, python, s3, scrum, software architecture, spark"
Senior Big Data Engineer -US,Zortech Solutions,"Jersey City, NJ",https://www.linkedin.com/jobs/view/senior-big-data-engineer-us-at-zortech-solutions-3780721621,2023-12-17,Yarmouth, Canada,Mid senior,Onsite,"Role: Senior Big Data Engineer
Location: New Jersey (hybrid)
Duration: 6+ Months
Job Description:
Please submit someone who can go for in person interview
Job Description:
We are seeking a highly skilled and experienced Senior Data Engineer to join our dynamic team. As a Senior Data Engineer, you will play a crucial role in designing, implementing, and maintaining data pipelines and infrastructure for our big data projects. Your expertise in Java, Python, Spark cluster management, data science, big data, REST API development, and knowledge of Databricks and Delta Lake will be essential in driving the success of our data initiatives.
Responsibilities:
Design, develop, and implement scalable data pipelines and ETL processes using Java, Python, and Spark.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and design efficient solutions.
Manage and optimize Spark clusters to ensure high performance and reliability.
Perform data exploration, data cleaning, and data transformation tasks to prepare data for analysis and modeling.
Develop and maintain data models and schemas to support data integration and analysis.
Implement data quality and validation checks to ensure accuracy and consistency of data.
Utilize REST API development skills to create and integrate data services and endpoints for seamless data access and consumption.
Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues.
Stay updated with the latest technologies and trends in big data, data engineering, data science, and REST API development, and provide recommendations for process improvements.
Mentor and guide junior team members, providing technical leadership and sharing best practices.
Qualifications:
Master's degree in Computer Science, Data Science, or a related field.
Minimum of 3 years of professional experience in data engineering, working with Java, Python, Spark, and big data technologies.
Strong programming skills in Java and Python, with expertise in building scalable and maintainable code.
Proven experience in Spark cluster management, optimization, and performance tuning.
Solid understanding of data science concepts and experience working with data scientists and analysts.
Proficiency in SQL and experience with relational databases (e.g., Snowflake, Delta Tables).
Experience in designing and developing REST APIs using frameworks such as Flask or Spring.
Familiarity with cloud-based data platforms (e.g.Azure)
Experience with data warehousing concepts and tools (e.g., Snowflake, BigQuery) is a plus.
Strong problem-solving and analytical skills, with the ability to tackle complex data engineering challenges.
Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.
If you are a highly motivated and skilled Senior Data Engineer with a passion for big data, data engineering, and REST API development, we would love to hear from you. Join our team and contribute to the success of our data-driven initiatives as we strive to make a significant impact in the industry.
Show more
Show less","Java, Python, Spark, Data science, Big data, REST API, Databricks, Delta Lake, ETL, Data pipelines, Hadoop, SQL, Data warehousing, Snowflake, Azure, BigQuery, Data modelling, REST APIs, Flask, Spring, AWS, Data visualization, Problemsolving, Analytical skills, Communication, Collaboration","java, python, spark, data science, big data, rest api, databricks, delta lake, etl, data pipelines, hadoop, sql, data warehousing, snowflake, azure, bigquery, data modelling, rest apis, flask, spring, aws, data visualization, problemsolving, analytical skills, communication, collaboration","analytical skills, aws, azure, big data, bigquery, collaboration, communication, data modelling, data science, databricks, datapipeline, datawarehouse, delta lake, etl, flask, hadoop, java, problemsolving, python, rest api, rest apis, snowflake, spark, spring, sql, visualization"
Azure Data Engineer (Junior to Mid level),BLN24,"Tysons Corner, VA",https://www.linkedin.com/jobs/view/azure-data-engineer-junior-to-mid-level-at-bln24-3787768914,2023-12-17,Yarmouth, Canada,Mid senior,Onsite,"BLN24 is an award-winning digital agency that supports the U.S. Federal Government in successfully achieving their mission and goals. Our service and solutions delivery start with understanding each client’s end-state, and then seamlessly integrating within each Agency’s organization to improve and enhance strategic and technical operations and deployments.
We are on the hunt for a talented junior to mid-level Azure Data Engineer
Tasks:
Will implement data ingestion pipelines from multiple data sources using Azure Data Factory, Azure Databricks and other ETL tools.
Will develop scalable and re-usable, self-service frameworks for data ingestion and processing.
Will design, build, and manage SQL Server databases in the Azure cloud.
Will perform data modelling and integrating data from various systems.
Will review and implement best practices for data manipulation.
Will create and support Azure Data Factory pipelines.
Will integrate end to end data pipelines to take data from data source to target data repositories ensuring the quality and consistency of data. Requirements:
Must have:
Azure Synapse
At least over 3 years of experience in Azure development.
Experience in cloud-based solutions.
Solid understanding and experience working with GIT.
Good understanding of Microsoft ETL tools (Azure Databricks, Azure Data Factory, Data Lake and SSIS).
Experience working with structured and unstructured data.
Ability to use ARM templates.
Experience in working with JSON.
Good understanding of Azure DevOps or Jira.
Knowledge of SQL.
Nice to have:
Programming skills for data analysis (especially PySpark, SparkSQL), or intent to learn it
Understanding of AAS (Azure Analysis Services).
Experience with Power BI.
Powered by JazzHR
0BCxpXzp0B
Show more
Show less","Azure Data Factory, Azure Databricks, SQL Server, SAS, PySpark, SparkSQL, JSON, ARM templates, GIT, Jira, Azure DevOps, Power BI, Azure Synapse, ETL, Data Lake, SSIS, Azure Analysis Services, Azure Data Factory pipelines","azure data factory, azure databricks, sql server, sas, pyspark, sparksql, json, arm templates, git, jira, azure devops, power bi, azure synapse, etl, data lake, ssis, azure analysis services, azure data factory pipelines","arm templates, azure analysis services, azure data factory, azure data factory pipelines, azure databricks, azure devops, azure synapse, data lake, etl, git, jira, json, powerbi, sas, spark, sparksql, sql server, ssis"
Data Engineer - IDELIC,Innovation Works,"Pittsburgh, PA",https://www.linkedin.com/jobs/view/data-engineer-idelic-at-innovation-works-3787726033,2023-12-17,Yarmouth, Canada,Mid senior,Onsite,"About Idelic
Idelic uses cutting-edge technology and beautifully designed interfaces to help predict and prevent trucking accidents and reduce driver turnover to ensure drivers get home safely each and every night. Our SaaS solutions are radically transforming the way in which the transportation industry manages safety through advanced Machine Learning (ML) and our driver management platform.
We believe that people do their best work while part of a culture that fosters inclusion, innovation, professional development, and teamwork. Together, we can fulfill our mission to make our roads and highways safer for everyone.
About Our Team
We are a venture-backed start-up company filled with people who are passionate about our product and seek to deliver the best experience for our clients. At Idelic, we’re committed to our mission, our customers, our teammates, and fostering a “work hard, play hard” culture.
Considering joining our team? You will be a part of an engaging, energetic, and entrepreneurial work environment headquartered in the heart of the technology boom within Pittsburgh, PA. We hire optimistic, results-oriented, innovative, and adaptable individuals with the desire to help our clients and one another succeed.
Overview Of The Role
Deploying a reliable and scalable machine learning project requires a lot of software engineering and devops work. Mathematical algorithms are typically a small portion of the overall deployment. The data engineer will assist data scientists and machine learning engineers with both coding and devops in order to build and deploy machine learning models.
You are not expected to have machine learning experience but you will be in charge of many of the tasks involved in running machine learning code in production. This will include implementing distributed systems such as microservices and task queues, as well as automating various data ingestion and preprocessing steps.
You should be comfortable with day-to-day operations and DBA tasks like launching EC2 instances, installing packages, writing shell scripts, and running SQL migrations.
What You’ll Do
Design and implement microservices to run machine learning models
Help implement the next version of a distributed task queue for data processing
Support data access layers and data ingestion for PostgreSQL databases
Assist with devops and automation prior to full production releases
Work on any task and help solve problems when needed — be humble and scrappy!
What You’ll Need To Succeed In The Role
3+ years of experience building backend architecture for distributed systems
Bachelor’s or equivalent degree in computer science, or a related field
Experience with Python, SQL
Comfortable with Linux, BASH, git
Experience working within AWS and AWS technologies (e.g. EC2, RDS, VPC, etc.)
Analytical and Problem Solving Skills
Proven ability to work in a collaborative and fast-paced environment
What Will Set You Apart
Experience deploying machine learning models in production
Familiarity with training machine learning models
Experience with containerization and Kubernetes
Experience with TensorFlow Serving
THINGS THAT MAKE IDELIC A GREAT PLACE TO WORK
Competitive Compensation Package Including Options
Medical, Dental, Vision and Life Insurance
401(k) with Company Matching Funds
Regular Company Outings and Events
Kickstarter Company Breakfast every Monday / Company Lunch Every Friday
A Dynamic and Supportive Environment
Professional Development Opportunities
Be Part of a Small Team (to Start)— Which Translates To You Having A Big Personal Impact
Please forward qualified resumes to:
TYPICAL PHYSICAL DEMANDS
The physical demands that are described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
While performing the duties of this job, the employee is regularly required to hear and see. The employee is regularly required to stand and sit. The employee is regularly required to practice manual dexterity sufficient to operate standard office equipment. Specific vision abilities required by this job include close vision and distant vision.
WORKING CONDITIONS:
While performing the duties of this job, the employee is exposed to standard office equipment. The noise level in the work environment is generally moderate. Occasionally called upon to work hours in excess of your normal daily schedule.
Idelic is an equal opportunity employer. Our success depends heavily on the effective utilization of qualified people, regardless of their race, ancestry, religion, color, sex, age, national origin, sexual orientation, gender identity, disability, veteran’s status, or any characteristic protected by law.
Powered by JazzHR
kWd37va4Zi
Show more
Show less","Python, SQL, AWS, EC2, RDS, VPC, PostgreSQL, Machine Learning, Microservices, Kuberentes, TensorFlow Serving, Shell scripting, Distributed Systems, Data Ingestion, Data Preprocessing, DevOps, Automation","python, sql, aws, ec2, rds, vpc, postgresql, machine learning, microservices, kuberentes, tensorflow serving, shell scripting, distributed systems, data ingestion, data preprocessing, devops, automation","automation, aws, data ingestion, data preprocessing, devops, distributed systems, ec2, kuberentes, machine learning, microservices, postgresql, python, rds, shell scripting, sql, tensorflow serving, vpc"
Data Engineer,CG Infinity,"Dallas, TX",https://www.linkedin.com/jobs/view/data-engineer-at-cg-infinity-3787911342,2023-12-17,Yarmouth, Canada,Mid senior,Onsite,"Job Title:
Data Engineer (Full Time Position) Dallas, TX
Get to Know Us:
CG Infinity, Inc. is a technology consulting firm that was founded in 1998. We offer solutions that are tailored to the needs of each individual client that we work with instead of offering standard, run-of-the-mill solutions to everyone. We work closely with our clients throughout the entire process and offer solutions for a myriad of challenges.
Our Culture:
Our people-first approach to technology offers best-in-class service and customer success rates. Here are some of the main services that we offer at CG Infinity: Salesforce Implementations, Customer Experience & CRM, Application Development & Integration, Production Support & QA, and Data Analytics & AI.
What You’ll Be Doing:
Design and develop the solution architecture for cloud data-enabled solutions and analytical platforms.
Research, analyze, and recommend technical approaches for solving our clients' complex data-related development and integration problems.
Engage effectively with client stakeholders and delivery teams to align technical solutions to the desired client outcomes.
Assist in designing multi-phased cloud data strategies, including crafting multi-phased implementation roadmaps.
Design and develop scalable data ingestion frameworks to transform a variety of datasets.
Serve as a subject matter expert in a cloud platform for larger the CG Infinity practice and contribute back to community.
Deliver on the technical scope of projects & demonstrate thought leadership at clients as well as internally at CG Infinity.
Gather technical requirements, assess client capabilities, and analyze findings to provide appropriate cloud solution recommendations and adoption strategy.
Deeply involved in designing and deploying end to end solutions with a cloud platform’s analytic services.
Lead the creation of technology roadmaps based on gathered and prioritized business needs.
Conduct and lead white-boarding sessions, workshops, and project meetings in support of data-enabled solutions.
Design & develop data models in support of BI/Analytics solutions.
Qualifications:
5+ years of data engineering and/or data warehousing experience
Banking and/or financial services industry experience highly preferred
3+ years of deep experience building cloud data solutions (Azure, AWS, GCP, Snowflake) and migrating from on-prem to cloud.
2+ years’ experience leading, managing and delivering complex cloud-based architecture engagements end-to-end with resources in multiple locations.
Hands-on experience with big data application development and/or with cloud data warehousing (e.g., Spark, Redshift, Snowflake, Azure SQL DW, Big Query)
Hands on experience with BI solutions (e.g., PowerBI, Looker, Tableau, or other format for reporting)
Experience implementing a variety of data warehousing concepts, methodologies, & best practices (e.g., Incremental, SCD, Star Schema, etc.)
Proficient in a relevant programming language for cloud platform e.g., Python/Java/C#/Unix as well as SQL
Working experience with version control platforms e.g., Git
Strong communication skills and a working knowledge of agile development, including DevOps concepts.
What Can We Offer You?
CG Infinity, Inc. offers an exceptionally strong benefits package that compares favorably with those offered by Fortune 500 companies. CG Infinity, Inc. has teamed with a highly regarded ASO to ensure a great choice for our benefits package. CG Infinity, Inc. employees have the flexibility to select benefits based on such factors as their personal preference, family situation, and financial objectives, along with our voluntary packages, such as additional Life as well as FSAs.
CG Infinity, Inc. also offers an excellent Safe Harbor 401k plan. Upon eligibility, CG Infinity, Inc. contributes an employer match of 100% of the first three percent and 50% of the fourth and fifth percent. All employees enrolled in the 401k retirement plan are 100% vested immediately.
Powered by JazzHR
YyO9Cpk062
Show more
Show less","Data engineering, Data warehousing, Cloud data solutions (Azure AWS GCP Snowflake), Big data application development, Cloud data warehousing (Spark Redshift Snowflake Azure SQL DW Big Query), Business intelligence (BI) solutions (PowerBI Looker Tableau), Data warehousing concepts, Methodologies, Best practices, Python, Java, C#, Unix, SQL, Git, Agile development, DevOps","data engineering, data warehousing, cloud data solutions azure aws gcp snowflake, big data application development, cloud data warehousing spark redshift snowflake azure sql dw big query, business intelligence bi solutions powerbi looker tableau, data warehousing concepts, methodologies, best practices, python, java, c, unix, sql, git, agile development, devops","agile development, best practices, big data application development, business intelligence bi solutions powerbi looker tableau, c, cloud data solutions azure aws gcp snowflake, cloud data warehousing spark redshift snowflake azure sql dw big query, data engineering, data warehousing concepts, datawarehouse, devops, git, java, methodologies, python, sql, unix"
"Big Data Engineer(Java, Azure, Spark) - Berkeley heights, NJ(hybrid)",Enexus Global Inc.,"Berkeley, NJ",https://www.linkedin.com/jobs/view/big-data-engineer-java-azure-spark-berkeley-heights-nj-hybrid-at-enexus-global-inc-3784226520,2023-12-17,Yarmouth, Canada,Mid senior,Onsite,"Role - Senior Data Engineer
Location - Berkeley heights, NJ(onsite)
Contract Type - W2/C2C/1099
Minimum Experience - 8+ Years
Responsibilities
Main Skills - Java, Spark, Azure, Big data
Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, Azure, and GitLab automation.
Collaborate with product and technology teams to design and validate the capabilities of the data platform
Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability
Provide technical support and usage guidance to the users of our platform's services.
Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services.
Qualifications
Experience building and optimizing data pipelines in a distributed environment
Experience supporting and working with cross-functional teams
5+ years of advanced working knowledge of SQL, Java, and Spark
5+ years of experience with using a broad range of Azure technologies
Experience using tools such as: Git/Bitbucket, CodePipeline
Experience with platform monitoring and alerts tools
Show more
Show less","Java, Spark, Azure, Big Data, Data pipelines, Distributed environment, SQL, Git, Bitbucket, CodePipeline, Platform monitoring, Alerts tools","java, spark, azure, big data, data pipelines, distributed environment, sql, git, bitbucket, codepipeline, platform monitoring, alerts tools","alerts tools, azure, big data, bitbucket, codepipeline, datapipeline, distributed environment, git, java, platform monitoring, spark, sql"
Software Engineer (Database Administrator),"G2 Ops, Inc.","Virginia Beach, VA",https://www.linkedin.com/jobs/view/software-engineer-database-administrator-at-g2-ops-inc-3787751011,2023-12-17,Norfolk,United States,Mid senior,Onsite,"Number of Roles Open:
1 Full Time
Location:
In office at Virginia Beach
Salary Range:
$100,000 - $150,000 + benefits (approximately $21,000 annual value)
Start Date:
10/16/23
Clearance Requirements:
Active DoD Secret Clearance
Role Requirements:
Knowledge of
database technologies such as MySQL, Cassandra, MongoDB, Derby, and Neo4j
Prefer certifications such as:
CompTIA Security+, CompTIA DataSys+, CompTIA Data+, AWS Cloud Practitioner, Datastax Administrator, Oracle Database Administration Certified Professional
Are you on the lookout for a company that values diversity, inclusivity, work satisfaction, and offers a collaborative team approach? If yes, then you should definitely check out G2 Ops! We're looking for a highly motivated
Software Engineer
to join our team, and we're sure you'll love it here.
At G2 Ops, we understand that there are multiple factors that influence your decision to choose a company to work for. Is it the pay, benefits, training, work satisfaction, or culture? We've got you covered on all fronts! Our compensation and benefits are
extremely competitive
, and we offer
100% company-paid insurance
for medical, dental, and vision for eligible employees and family members, along with a 401(K) Plan with discretionary employer matching.
What sets us apart is our culture.
We value diversity and inclusion, and we believe that
every team member is essential to our success.
You won't just be a payroll number or a cog in the machine at G2 Ops. We encourage cross-training in other areas that interest you and offer a flexible schedule to meet your needs and the needs of our customers.
Locations and WFH.
Join our vibrant and energetic team at G2 Ops and experience the ultimate work environment. Our offices are buzzing with
positivity and collaboration
, creating an incredible culture that we're proud of. We're thrilled to invite you to be a part of the excitement and fun! You may occasionally get the opportunity to work on-site directly with our customers, and teleworking is allowed with prior approval from the IT department and manager.
Let's talk salary.
Salary range is
$100,000 - $150,000
and is based on what we are asking you to do in the role. We'd like to start somewhere in that ballpark and go from there. With company standard annual performance reviews in place, plus on the spot awards for recognition; your performance will be rewarded and recognized, we promise!
Note:
ranges are
based on relevant qualifications
compared to Labor Categories (LCAT) - it has to check out.
Still not convinced?
Allow us to elaborate. For this opportunity we are seeking a highly motivated, team-oriented, and experienced
Software Engineer
with a penchant for automation. We’re looking a
leader
to take on tough problems and build complex, highly sophisticated cloud-native solutions.
What does this mean to you?
We’re looking for forward leaning technologists to strive to not only get the most out of Cameo Systems Modeler, Jira, Monte Carlo Simulation tools, MATLAB, Cassandra, GitHub, Microsoft Power BI, Java, and a host of other leading-edge engineering and software development tools but
integrate
them together for
streamlined and secure development.
Our ideal candidate will have a
Bachelor of Science Degree
in information systems, engineering, computer science, or related plus
about four (4) years of industry experience
. Our desired candidate will be skilled in operational analysis and use case definition, plus requirements management and analysis. A firm grasp of object-oriented programming languages such as Java, Python, C++, C#, VB.NET is preferred. We would also like candidates to have working knowledge of:
Web application frameworks such as HTML5, Java Server Faces, JavaScript, PHP, ASP.NET, Django, and Rails.
Relational, NoSQL and graph database frameworks such as Microsoft Access, Oracle, MySQL, Microsoft SQL Server, MongoDB, Cassandra, PostgreSQL, and Neo4J
To be successful,
you should be able to communicate clearly with stakeholders and engineering leadership teams to ensure requirements are clearly defined and customer expectations are being met. The ability to analyze new and complex program-related problems and create innovative solutions that integrate schedule, technology, methodology, tools, and financial aspects are critical components to this position. The ability to present the content of complex issues to a broad audience will ensure your success.
Lastly, as we are working with the DoD, we are beholden to some requirements. This role requires an
active DoD Secret Clearance
, with the ability to obtain TS/SCI without issue. Don’t let it stop you from applying if you don’t have a clearance. We’re willing to work with the right candidate to get a security clearance.
Quick Note:
We are seeking a full-time employee; the continuation of outside employment shall constitute a conflict with the Company’s interest, including performing work for a customer or competitor.
Congratulations
, you've made it all the way to the end of this job posting. We look forward to learning more about you!
Benefits
100% company-paid insurance for medical, dental, and vision for eligible employees and family members
100% company-paid insurance for life, short-term (STD) and long-term disability (LTD) for eligible employees
401(K) Plan with discretionary employer matching
10 paid holidays
Paid time off (PTO)
Educational assistance
Work/life balance
Family-oriented culture
Competitive salaries
About G2 Ops, Inc.
G2 Ops, Inc. is a small business with big capabilities in cyber security architectural analysis, model-based systems engineering (MBSE), and strategic consulting in support of both government and commercial clients across the globe. As a trusted and reliable government contractor, we deliver cyber security & systems engineering support for integrated DoD weapons, communications, intelligence, and other mission-critical systems. In the commercial space, we provide business solutions analysis, strategic planning, and training and development services to a variety of public and private sector businesses and organizations. Through innovative solutions, exceptional employees, top-tier analytical capabilities, and a customer-centered focus, G2 Ops has established a reputation for service excellence and innovation.
G2 Ops, Inc. is an Equal Opportunity Employer
Powered by JazzHR
uN5ck2bNHC
Show more
Show less","MySQL, Cassandra, MongoDB, Derby, Neo4j, CompTIA Security+, CompTIA DataSys+, CompTIA Data+, AWS Cloud Practitioner, Datastax Administrator, Oracle Database Administration Certified Professional, Oracle, Microsoft SQL Server, PostgreSQL, Django, Rails, HTML5, Java Server Faces, JavaScript, PHP, ASP.NET, Microsoft Access, Cameo Systems Modeler, Jira, Monte Carlo Simulation, MATLAB, GitHub, Microsoft Power BI, Java, Python, C++, C#, VB.NET","mysql, cassandra, mongodb, derby, neo4j, comptia security, comptia datasys, comptia data, aws cloud practitioner, datastax administrator, oracle database administration certified professional, oracle, microsoft sql server, postgresql, django, rails, html5, java server faces, javascript, php, aspnet, microsoft access, cameo systems modeler, jira, monte carlo simulation, matlab, github, microsoft power bi, java, python, c, c, vbnet","aspnet, aws cloud practitioner, c, cameo systems modeler, cassandra, comptia data, comptia datasys, comptia security, datastax administrator, derby, django, github, html5, java, java server faces, javascript, jira, matlab, microsoft access, microsoft power bi, microsoft sql server, mongodb, monte carlo simulation, mysql, neo4j, oracle, oracle database administration certified professional, php, postgresql, python, rails, vbnet"
Healthcare Data Analyst,Sentara Health,"Virginia Beach, VA",https://www.linkedin.com/jobs/view/healthcare-data-analyst-at-sentara-health-3785744227,2023-12-17,Norfolk,United States,Mid senior,Onsite,"City/State
Virginia Beach, VA
Overview
Work Shift
First (Days) (United States of America)
Be a part of an excellent healthcare organization that cares about our People, Quality, Patient Safety, Service, and Integrity. Join a team that has a mission to improve health every day and a vision to be the healthcare choice of the communities that we serve!
Sentara Health Plan is hiring for a HEALTHCARE DATA ANALYST
This is a Full-Time position, fully remote, with day shift hours and great benefits!
Work Location:
Remote opportunities available in the following states: Alabama, Delaware, Florida, Georgia, Idaho, Indiana, Kansas, Louisiana, Maine, Maryland, Minnesota, Nebraska, Nevada, New Hampshire, North Carolina, North Dakota, Ohio, Oklahoma, Pennsylvania, South Carolina, South Dakota, Tennessee, Texas, Utah, Virginia, Washington (state), West Virginia, Wisconsin, Wyoming
To apply, please go to www.sentaracareers.com and use the following as your Keyword Search:
JR-41184
Job Responsibilities
The Healthcare Data Analyst provides data analysis support to the customer by assisting with the development of reports and/or dashboards to monitor program and operational performance. Promotes self-service analytics for customer adoption, understanding and use of data. Supports design of programmatic analyses and reporting capabilities in addition to business requirement definition for new analyses and performs ad-hoc analysis as directed.
This position supports data quality and data stewardship functions to maintain data accuracy and identify new metrics. As a subject matter expert, the Healthcare Data Analyst uses analytic skills to offer data-driven conclusions and recommendations to business partners.
As an integrated health system, Sentara delivers analytics across a wide variety of domains, including payer functions, ambulatory, acute & post-acute care and support functions. The subject matter expertise required will vary based on the specific opening, but include the following:
Medical expense and trends
Quality including HEDIS reporting, interventions, and STAR ratings
Clinical gaps and adherence
Network Management contracting and pay-for-performance/value-based measures
State and Federal regulatory reporting
Employer groups reporting and client ad-hoc reporting related to clinical and financial services
Population health reporting & analysis
Risk, member/patient stratification, member/patient segmentation, quality, provider scoring, and cost savings
Ambulatory, acute & post-acute quality, clinical performance, and operations reporting & analysis
Quality & performance benchmarking
Research data & analytic support
Job Requirements
Required to have Bachelor's Level Degree
Required to have a minimum of 1 year of relevant experience
Required to have SQL and Tableau
Preferred Qualifications
Preferred to have experience in Healthcare
Benefits:
Sentara offers an attractive array of full-time benefits to include Medical, Dental, Vision, Paid Time Off, Sick, Tuition Reimbursement, a 401k/403B, 401a, Performance Plus Bonus, Career Advancement Opportunities, Work Perks, and more.
For information about our employee benefits, please visit: https://www.sentaracareers.com/explore-sentara/benefits/
Sentara Health Plans
is the health insurance division of Sentara Healthcare doing business as Optima Health and Virginia Premier.
Sentara Health Plans
provides health insurance coverage through a full suite of commercial products including consumer-driven, employee-owned and employer-sponsored plans, individual and family health plans, employee assistance plans and plans serving Medicare and Medicaid enrollees.
For applicants within Washington State, the following hiring range will be applied: $73,819.20 to $87,447.36 annually.
#Indeed
#Dice
#Monster
Talroo–IT
Indeed
Monster
Talroo
Talroo–Health Plan
Keywords: Data Analysis, SQL, Tableau, dashboard, Data Analysis, Data Quality, Data Stewardship, HEDIS, STAR, value-based measures, value based measures, ad-hoc reports, ad-hoc, ad-hoc reporting, population health, pop health, pop-health, ambulatory, acute, post-acute, data accuracy, payer, finance, financial, financial analysis, population, population management, Statistics, Alabama, Delaware, Florida, Georgia, Idaho, Indiana, Kansas, Louisiana, Maine, Maryland, Minnesota, Nebraska, Nevada, New Hampshire, North Carolina, North Dakota, Ohio, Oklahoma, Pennsylvania, South Carolina, South Dakota, Tennessee, Texas, Utah, Virginia, Washington, West Virginia, Wisconsin, Wyoming
Job Summary
The Healthcare Data Analyst provides data analysis support to the customer by assisting with the development of reports and/or dashboards to monitor program and operational performance. Promotes self-service analytics for customer adoption, understanding and use of data. Supports design of programmatic analyses and reporting capabilities in addition to business requirement definition for new analyses and performs ad-hoc analysis as directed. This position supports data quality and data stewardship functions to maintain data accuracy and identify new metrics. As a subject matter expert, the Healthcare Data Analyst uses analytic skills to offer data-driven conclusions and recommendations to business partners.
As an integrated health system, Sentara delivers analytics across a wide variety of domains, including payer functions, ambulatory, acute & post-acute care and support functions. The subject matter expertise required will vary based on the specific opening, but include the following:
Medical expense and trends
Quality including HEDIS reporting, interventions, and STAR ratings
Clinical gaps and adherence
Network Management contracting and pay-for-performance/value-based measures
State and Federal regulatory reporting
Employer groups reporting and client ad-hoc reporting related to clinical and financial services
Population health reporting & analysis
Risk, member/patient stratification, member/patient segmentation, quality, provider scoring, and cost savings
Ambulatory, acute & post-acute quality, clinical performance, and operations reporting & analysis
Quality & performance benchmarking
Research data & analytic support
Works directly with internal customers and external provider partners.
Knowledge of SQL and Tableau
Excellent Oral and Written Communication Skills
Problem Solving
Consultative Engagement Skills
Qualifications:
BLD - Bachelor's Level Degree (Required), MLD - Master's Level Degree
Data Analysis, Financial Analysis, Population Management, Statistics
Skills
Active Learning, Active Listening, Communication, Complex Problem Solving, Coordination, Critical Thinking, Reading Comprehension, Service Orientation, Time Management, Troubleshooting, Writing
Sentara Healthcare prides itself on the diversity and inclusiveness of its close to an almost 30,000-member workforce. Diversity, inclusion, and belonging is a guiding principle of the organization to ensure its workforce reflects the communities it serves.
Per Clinical Laboratory Improvement Amendments (CLIA), some clinical environments require proof of education; these regulations are posted at ecfr.gov for further information. In an effort to expedite this verification requirement, we encourage you to upload your diploma or transcript at time of application.
In support of our mission “to improve health every day,” this is a tobacco-free environment.
Show more
Show less","SQL, Tableau, Adhoc analysis, Dashboard, Data quality, Data stewardship, HEDIS, STAR, Valuebased measures, Population health, Population management, Ambulatory, Acute, Postacute, Payer, Finance, Financial analysis, Statistics, Data analysis, Problemsolving, Communication, Consultative engagement","sql, tableau, adhoc analysis, dashboard, data quality, data stewardship, hedis, star, valuebased measures, population health, population management, ambulatory, acute, postacute, payer, finance, financial analysis, statistics, data analysis, problemsolving, communication, consultative engagement","acute, adhoc analysis, ambulatory, communication, consultative engagement, dashboard, data quality, data stewardship, dataanalytics, finance, financial analysis, hedis, payer, population health, population management, postacute, problemsolving, sql, star, statistics, tableau, valuebased measures"
Senior Healthcare Data Analyst,Sentara Health,"Virginia Beach, VA",https://www.linkedin.com/jobs/view/senior-healthcare-data-analyst-at-sentara-health-3767310037,2023-12-17,Norfolk,United States,Mid senior,Onsite,"City/State
Virginia Beach, VA
Overview
Work Shift
First (Days) (United States of America)
Be a part of an excellent healthcare organization that cares about our People, Quality, Patient Safety, Service, and Integrity. Join a team that has a mission to improve health every day and a vision to be the healthcare choice of the communities that we serve!
Sentara Health Plan is hiring for a
Senior Healthcare Data Analyst.
This is a Full-Time position, fully remote, with day shift hours and great benefits!
Work Location:
Remote opportunities available in the following states -
Alabama, Delaware, Florida, Georgia, Idaho, Indiana, Kansas, Louisiana, Maine, Maryland, Minnesota, Nebraska, Nevada, New Hampshire, North Carolina, North Dakota, Ohio, Oklahoma, Pennsylvania, South Carolina, South Dakota, Tennessee, Texas, Utah, Virginia, Washington (state), West Virginia, Wisconsin, Wyoming
Job Responsibilities
Promotes self-service analytics for customer adoption, understanding and use of data.
Supports design of programmatic analyses and reporting capabilities in addition to business requirement definition for new analyses and performs ad-hoc analysis as directed.
This position supports data quality and data stewardship functions to maintain data accuracy and identify new metrics.
As a subject matter expert, the Healthcare Data Analyst Senior uses analytic skills to offer data-driven conclusions and recommendations to business partners.
As an integrated health system, Sentara delivers analytics across a wide variety of domains, including payer functions, ambulatory, acute & post-acute care and support functions.
The subject matter expertise required:
Population health reporting & analysis
Predictive analytics
Research data & analytic support
Health Outcomes Research
Nursing Research
Works directly with internal customers and external provider partners.
The Healthcare Data Analyst Senior on-boards new team members, and provides guidance and mentorship to junior analysts.
Job Requirements:
Required to have knowledge of SQL and Tableau
Required to have excellent Oral and Written Communication Skills
Required to have Problem Solving skills
Required to have Consultative Engagement Skills
Benefits
: Sentara offers an attractive array of full-time benefits to include Medical, Dental, Vision, Paid Time Off, Sick, Tuition Reimbursement, a 401k/403B, 401a, Performance Plus Bonus, Career Advancement Opportunities, Work Perks, and more.
Sentara Healthcare offers employees comprehensive health care and retirement benefits designed with you and your family's well-being in mind. Our benefits packages are designed to change with you by meeting your needs now and anticipating what comes next. You have a variety of options for medical, dental and vision insurance, life insurance, disability and voluntary benefits as well as Paid Time Off in the form of sick time, vacation time and paid parental leave. Team Members have the opportunity to earn an annual flat amount Bonus payment if established system and employee eligibility criteria is met.
Sentara Health Plans
is the health insurance division of Sentara Healthcare.
Sentara Health Plans
provides health insurance coverage through a full suite of commercial products including consumer-driven, employee-owned and employer-sponsored plans, individual and family health plans, employee assistance plans and plans serving Medicare and Medicaid enrollees.
With more than 30 years’ experience in the insurance business and 20 years’ experience serving Medicaid populations, we offer programs to support members with chronic illnesses, customized wellness programs, and integrated clinical and behavioral health services – all to help our members improve their health.
For applicants within Washington State, the following hiring range will be applied: $82,887.17 to $124,330.75 annually with a target hiring range of $82,887.17 to $103,608.96.
Our success is supported by a family-friendly culture that encourages community involvement and creates unlimited opportunities for development and growth.
To apply, please go to
www.sentaracareers.com
and use the following as your Keyword Search:
JR-45101
#Indeed
#Dice
Monster
Talroo–IT
Indeed
Monster
Talroo
Talroo–Health Plan
Keywords: Health plan, Healthcare, SQL, MS Excel, Excel, Microsoft Excel, Remote, Alabama, Delaware, Florida, Georgia, Indiana, Kansas, Louisiana, Maine, Maryland, Minnesota, Nebraska, Nevada, New Hampshire, North Carolina, North Dakota, Ohio, Oklahoma, Pennsylvania, South Carolina, South Dakota, Tennessee, Texas, Utah, Virginia, West Virginia, Wisconsin, Wyoming
Job Summary
The Healthcare Data Analyst Senior provides data analysis support to the customer by assisting with the development of reports and/or dashboards to monitor program and operational performance.
Qualifications:
BLD - Bachelor's Level Degree (Required), MLD - Master's Level Degree
Data Analysis, Financial Analysis, Population Management, Statistics
Skills
Active Learning, Active Listening, Communication, Complex Problem Solving, Coordination, Critical Thinking, Reading Comprehension, Service Orientation, Time Management, Troubleshooting, Writing
Sentara Healthcare prides itself on the diversity and inclusiveness of its close to an almost 30,000-member workforce. Diversity, inclusion, and belonging is a guiding principle of the organization to ensure its workforce reflects the communities it serves.
Per Clinical Laboratory Improvement Amendments (CLIA), some clinical environments require proof of education; these regulations are posted at ecfr.gov for further information. In an effort to expedite this verification requirement, we encourage you to upload your diploma or transcript at time of application.
In support of our mission “to improve health every day,” this is a tobacco-free environment.
Show more
Show less","SQL, Tableau, Population Health Analytics, Predictive Analytics, Research Data & Analytics, Health Outcomes Research, Nursing Research, Active Learning, Active Listening, Communication, Complex Problem Solving, Coordination, Critical Thinking, Reading Comprehension, Service Orientation, Time Management, Troubleshooting, Writing, Data Analysis, Financial Analysis, Population Management, Statistics","sql, tableau, population health analytics, predictive analytics, research data analytics, health outcomes research, nursing research, active learning, active listening, communication, complex problem solving, coordination, critical thinking, reading comprehension, service orientation, time management, troubleshooting, writing, data analysis, financial analysis, population management, statistics","active learning, active listening, communication, complex problem solving, coordination, critical thinking, dataanalytics, financial analysis, health outcomes research, nursing research, population health analytics, population management, predictive analytics, reading comprehension, research data analytics, service orientation, sql, statistics, tableau, time management, troubleshooting, writing"
Entry Level Data Analyst/Management Consultant - Nationwide (US Based Candidates Only),Arcadis,"Virginia Beach, VA",https://www.linkedin.com/jobs/view/entry-level-data-analyst-management-consultant-nationwide-us-based-candidates-only-at-arcadis-3701463927,2023-12-17,Norfolk,United States,Mid senior,Onsite,"Arcadis is the world's leading company delivering sustainable design, engineering, and consultancy solutions for natural and built assets.
We are more than 36,000 people, in over 70 countries, dedicated to improving quality of life. Everyone has an important role to play. With the power of many curious minds, together we can solve the world’s most complex challenges and deliver more impact together.
Role description:
Note: See below regarding the nature of this position being a prospecting position.
Arcadis is currently seeking Analysts and Junior Management Consultants to join our world-class Business Advisory practice nationwide.
We are looking for candidates who want to apply technical know-how, combined with business principles, to the water, wastewater, and stormwater industry. We want dedicated, creative, and energetic candidates interested in tackling challenges and developing sustainable solutions to address water issues like renewal and replacement of aging infrastructure, funding of capital improvements, water supply, workforce retention and development, and emergency preparedness. Collaborating with our experienced consulting professionals, you will support and contribute to project outcomes; interact, and work with clients, and develop your technical capabilities.
We are a People First company, industry thought leaders, and drivers and allies of utility innovation.
Our passion: to Improve Quality of Life.
Our approach: to delight our clients by developing successful long-term partnerships and supporting them to address existing and emerging challenges.
Arcadis provides multiple onboarding and development programs created for young professionals that support professional growth and help drive creativeness, innovation, and greater integration within our local, National and global teams.
Role accountabilities:
What will you do?
Assess, develop, and support a variety of management consultant projects including performing data analytics, financial analysis, operational and organizational assessments, condition assessments, vulnerability, and mitigation assessments, as well as planning and development for utilities, municipalities, and cities’ (primarily water/wastewater/stormwater utilities).
Utilize strong analytical skills and ability to apply logic to solve problems.
Support teams in tasks ranging from general fieldwork to technical office-based analysis.
Assist in technical writing which may include preparation of technical reports, business development support, presentations, and other audiovisual materials.
Work independently and as part of a team, with the flexibility to accommodate collaboration with team members across the U.S. and internationally.
Manage multiple concurrent projects with multiple deadlines, ensuring completion per project budgets and timelines.
What skills will you need?
Reliable, client-focused, and capable of working independently under the supervision of project managers.
Exceptional analytical and problem-solving skills, strong attention to detail, organization skills, and work ethic.
Self-motivated and team-oriented, with the ability to work successfully both independently and within a team.
Ability to balance and address new challenges as they arise and an eagerness to take ownership of tasks.
Knowledge of engineering concepts, theories, and practices related to water/wastewater/stormwater.
Drive to succeed and grow a career in the utility industry
Qualifications & Experience:
Required Qualifications:
Masters of Science degree in Civil or Environmental Engineering, or closely related STEM discipline; or business analytics/MBA, MS in data science or related business discipline.
For those with engineering degrees, ability to obtain the EIT within six months of start date
Preferred Qualifications:
Previous relevant consulting or utility experience, either internship or full-time.
Experience applying programming languages and analytics to problem-solving is a plus
SharePoint, Building Information Modeling (BIM), Power BI, Excel, PowerPoint, Visio, Change Management skills, and/or Augmented Reality experience
This is a general job posting and not tied to a specific current open position. Please make sure you create a search agent to be alerted of specific opportunities of interest. Candidates who submit their resume to this posting may be considered for all future openings as they arise.
Why Arcadis?
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills-based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You’ll do meaningful work, and no matter what role, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark, on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Join Arcadis. Create a Legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We are an equal opportunity and affirmative action employer. Women, minorities, people with disabilities and veterans are strongly encouraged to apply. We are dedicated to a policy of non-discrimination in employment on any basis including race, creed, color, religion, national origin, sex, age, disability, marital status, sexual orientation, gender identity, citizenship status, disability, veteran status, or any other basis prohibited by law.
Arcadis offers benefits for full time and part time positions. These benefits include medical, dental, and vision, EAP, 401K, STD, LTD, AD&D, life insurance, paid parental leave, reward & recognition program and optional benefits including wellbeing benefits, adoption assistance and tuition reimbursement. We offer seven paid holidays and potentially up to two floating holidays per calendar year depending on start date, and 15 days PTO that accrue per year. The salary range for this position is $52000 - 89700 / year.
#ANACollege
Show more
Show less","STEM, Engineering, Civil Engineering, Environmental Engineering, Business Analytics, Data Science, Programming Languages, Analytics, SharePoint, Building Information Modeling (BIM), Power BI, Excel, PowerPoint, Visio, Change Management, Augmented Reality, Microsoft Office Suite, Data Analysis, EIT, Project Management","stem, engineering, civil engineering, environmental engineering, business analytics, data science, programming languages, analytics, sharepoint, building information modeling bim, power bi, excel, powerpoint, visio, change management, augmented reality, microsoft office suite, data analysis, eit, project management","analytics, augmented reality, building information modeling bim, business analytics, change management, civil engineering, data science, dataanalytics, eit, engineering, environmental engineering, excel, microsoft office suite, powerbi, powerpoint, programming languages, project management, sharepoint, stem, visio"
Senior Healthcare Data Analyst (Remote),Sentara Health,"Virginia Beach, VA",https://www.linkedin.com/jobs/view/senior-healthcare-data-analyst-remote-at-sentara-health-3688831135,2023-12-17,Norfolk,United States,Mid senior,Remote,"City/State
Virginia Beach, VA
Overview
Work Shift
First (Days) (United States of America)
Sentara Health Plan is currently hiring a Senior Healthcare Data Analyst!
This is a Full-Time position with day shift hours and great benefits!
Work Location:
Remote opportunities available in the following states: Alabama, Delaware, Florida, Georgia, Idaho, Indiana, Kansas, Louisiana, Maine, Maryland, Minnesota, Nebraska, Nevada, New Hampshire, North Carolina, North Dakota, Ohio, Oklahoma, Pennsylvania, South Carolina, South Dakota, Tennessee, Texas, Utah, Virginia, Washington (state), West Virginia, Wisconsin, Wyoming
Job Responsibilities
The Healthcare Data Analyst Senior provides data analysis support to the customer by assisting with the development of reports and/or dashboards to monitor program and operational performance. Promotes self-service analytics for customer adoption, understanding and use of data. Supports design of programmatic analyses and reporting capabilities in addition to business requirement definition for new analyses and performs ad-hoc analysis as directed. This position supports data quality and data stewardship functions to maintain data accuracy and identify new metrics. As a subject matter expert, the Healthcare Data Analyst Senior uses analytic skills to offer data-driven conclusions and recommendations to business partners.
As an integrated health system, Sentara delivers analytics across a wide variety of domains, including payer functions, ambulatory, acute & post-acute care and support functions. The subject matter expertise required will vary based on the specific opening, but include the following:
Medical expense and trends
Quality including HEDIS reporting, interventions, and STAR ratings
Clinical gaps and adherence
Network Management contracting and pay-for-performance/value-based measures
State and Federal regulatory reporting
Employer groups reporting and client ad-hoc reporting related to clinical and financial services
Population health reporting & analysis
Risk, member/patient stratification, member/patient segmentation, quality, provider scoring, and cost savings
Predictive analytics
Ambulatory, acute & post-acute quality, clinical performance, and operations reporting & analysis
Quality & performance benchmarking
Research data & analytic support
Works directly with internal customers and external provider partners.
The Healthcare Data Analyst Senior on-boards new team members, and provides guidance and mentorship to junior analysts.
Job Requirements:
Required to have a Bachelor's degree.
Required to have 3+ years of Data Analysis experience
Required to have 1+ years of Healthcare experience
Required to have SQL, Tableau, Excel experience
Benefits
: Sentara offers an attractive array of full-time benefits to include Medical, Dental, Vision, Paid Time Off, Sick, Tuition Reimbursement, a 401k/403B, 401a, Performance Plus Bonus, Career Advancement Opportunities, Work Perks, and more.
Sentara Healthcare offers employees comprehensive health care and retirement benefits designed with you and your family's well-being in mind. Our benefits packages are designed to change with you by meeting your needs now and anticipating what comes next. You have a variety of options for medical, dental and vision insurance, life insurance, disability and voluntary benefits as well as Paid Time Off in the form of sick time, vacation time and paid parental leave. Team Members have the opportunity to earn an annual flat amount Bonus payment if established system and employee eligibility criteria is met.
For applicants within Washington State, the following hiring range will be applied: $82,887.17 to $124,330.752 annually with a target hiring range of $82,887.17 to $103,608.96
Sentara Health Plans
is the health insurance division of Sentara Healthcare doing business.
Sentara Health Plans
provides health insurance coverage through a full suite of commercial products including consumer-driven, employee-owned and employer-sponsored plans, individual and family health plans, employee assistance plans and plans serving Medicare and Medicaid enrollees.
With more than 30 years’ experience in the insurance business and 20 years’ experience serving Medicaid populations, we offer programs to support members with chronic illnesses, customized wellness programs, and integrated clinical and behavioral health services – all to help our members improve their health.
Our success is supported by a family-friendly culture that encourages community involvement and creates unlimited opportunities for development and growth.
Be a part of an excellent healthcare organization that cares about our People, Quality, Patient Safety, Service, and Integrity. Join a team that has a mission to improve health every day and a vision to be the healthcare choice of the communities that we serve!
To apply, please go to
www.sentaracareers.com
and use the following as your Keyword Search:
#Indeed
#Dice
Indeed
Monster
Talroo – Health Plan
Talroo
Keywords: HEDIS, Health plan, Health care, SQL, Tableau, Excel, MS Excel, Data Analysis, Power BI, Healthcare, Health Plan, Remote, Alabama, Delaware, Florida, Georgia, Idaho, Indiana, Kansas, Louisiana, Maine, Maryland, Minnesota, Nebraska, Nevada, New Hampshire, North Carolina, North Dakota, Ohio, Oklahoma, Pennsylvania, South Carolina, South Dakota, Tennessee, Texas, Utah, Virginia, Washington (state), West Virginia, Wisconsin, Wyoming
Job Summary
The Healthcare Data Analyst Senior provides data analysis support to the customer by assisting with the development of reports and/or dashboards to monitor program and operational performance. Promotes self-service analytics for customer adoption, understanding and use of data. Supports design of programmatic analyses and reporting capabilities in addition to business requirement definition for new analyses and performs ad-hoc analysis as directed. This position supports data quality and data stewardship functions to maintain data accuracy and identify new metrics. As a subject matter expert, the Healthcare Data Analyst Senior uses analytic skills to offer data-driven conclusions and recommendations to business partners.
As an integrated health system, Sentara delivers analytics across a wide variety of domains, including payer functions, ambulatory, acute & post-acute care and support functions. The subject matter expertise required will vary based on the specific opening, but include the following:
Medical expense and trends
Quality including HEDIS reporting, interventions, and STAR ratings
Clinical gaps and adherence
Network Management contracting and pay-for-performance/value-based measures
State and Federal regulatory reporting
Employer groups reporting and client ad-hoc reporting related to clinical and financial services
Population health reporting & analysis
Risk, member/patient stratification, member/patient segmentation, quality, provider scoring, and cost savings
Predictive analytics
Ambulatory, acute & post-acute quality, clinical performance, and operations reporting & analysis
Quality & performance benchmarking
Research data & analytic support
Works directly with internal customers and external provider partners.
The Healthcare Data Analyst Senior on-boards new team members, and provides guidance and mentorship to junior analysts.
Knowledge of SQL and Tableau
Excellent Oral and Written Communication Skills
Problem Solving
Consultative Engagement Skills
Qualifications:
BLD - Bachelor's Level Degree (Required), MLD - Master's Level Degree
Data Analysis, Financial Analysis, Population Management, Statistics
Skills
Active Learning, Active Listening, Communication, Complex Problem Solving, Coordination, Critical Thinking, Reading Comprehension, Service Orientation, Time Management, Troubleshooting, Writing
Sentara Healthcare prides itself on the diversity and inclusiveness of its close to an almost 30,000-member workforce. Diversity, inclusion, and belonging is a guiding principle of the organization to ensure its workforce reflects the communities it serves.
Per Clinical Laboratory Improvement Amendments (CLIA), some clinical environments require proof of education; these regulations are posted at ecfr.gov for further information. In an effort to expedite this verification requirement, we encourage you to upload your diploma or transcript at time of application.
In support of our mission “to improve health every day,” this is a tobacco-free environment.
Show more
Show less","Data Analysis, Tableau, SQL, Excel, Microsoft Excel, Power BI, HEDIS, Data Quality, Data Stewardship, Population Health, Risk Stratification, Predictive Analytics, Quality Benchmarking, Research Data, Analytic Support, Oral Communication, Written Communication, Problem Solving, Consultative Engagement, Active Learning, Active Listening, Communication, Complex Problem Solving, Coordination, Critical Thinking, Reading Comprehension, Service Orientation, Time Management, Troubleshooting, Writing","data analysis, tableau, sql, excel, microsoft excel, power bi, hedis, data quality, data stewardship, population health, risk stratification, predictive analytics, quality benchmarking, research data, analytic support, oral communication, written communication, problem solving, consultative engagement, active learning, active listening, communication, complex problem solving, coordination, critical thinking, reading comprehension, service orientation, time management, troubleshooting, writing","active learning, active listening, analytic support, communication, complex problem solving, consultative engagement, coordination, critical thinking, data quality, data stewardship, dataanalytics, excel, hedis, microsoft excel, oral communication, population health, powerbi, predictive analytics, problem solving, quality benchmarking, reading comprehension, research data, risk stratification, service orientation, sql, tableau, time management, troubleshooting, writing, written communication"
Healthcare Data Analyst,Dice,"Virginia Beach, VA",https://www.linkedin.com/jobs/view/healthcare-data-analyst-at-dice-3788785672,2023-12-17,Norfolk,United States,Mid senior,Remote,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Sentara Healthcare, is seeking the following. Apply via Dice today!
Be a part of an excellent healthcare organization that cares about our People, Quality, Patient Safety, Service, and Integrity. Join a team that has a mission to improve health every day and a vision to be the healthcare choice of the communities that we serve!
Sentara Health Plan is hiring for a HEALTHCARE DATA ANALYST
This is a Full-Time position, fully remote, with day shift hours and great benefits!
Work Location:
Remote opportunities available in the following states: Alabama, Delaware, Florida, Georgia, Idaho, Indiana, Kansas, Louisiana, Maine, Maryland, Minnesota, Nebraska, Nevada, New Hampshire, North Carolina, North Dakota, Ohio, Oklahoma, Pennsylvania, South Carolina, South Dakota, Tennessee, Texas, Utah, Virginia, Washington (state), West Virginia, Wisconsin, Wyoming
To apply, please go to and use the following as your Keyword Search:
JR-41184
Job Responsibilities
The Healthcare Data Analyst provides data analysis support to the customer by assisting with the development of reports and/or dashboards to monitor program and operational performance. Promotes self-service analytics for customer adoption, understanding and use of data. Supports design of programmatic analyses and reporting capabilities in addition to business requirement definition for new analyses and performs ad-hoc analysis as directed.
This position supports data quality and data stewardship functions to maintain data accuracy and identify new metrics. As a subject matter expert, the Healthcare Data Analyst uses analytic skills to offer data-driven conclusions and recommendations to business partners.
As an integrated health system, Sentara delivers analytics across a wide variety of domains, including payer functions, ambulatory, acute & post-acute care and support functions. The subject matter expertise required will vary based on the specific opening, but include the following:
Medical expense and trends
Quality including HEDIS reporting, interventions, and STAR ratings
Clinical gaps and adherence
Network Management contracting and pay-for-performance/value-based measures
State and Federal regulatory reporting
Employer groups reporting and client ad-hoc reporting related to clinical and financial services
Population health reporting & analysis
Risk, member/patient stratification, member/patient segmentation, quality, provider scoring, and cost savings
Ambulatory, acute & post-acute quality, clinical performance, and operations reporting & analysis
Quality & performance benchmarking
Research data & analytic support
Job Requirements
Required to have Bachelor's Level Degree
Required to have a minimum of 1 year of relevant experience
Required to have SQL and Tableau
Preferred Qualifications
Preferred to have experience in Healthcare
Benefits:
Sentara offers an attractive array of full-time benefits to include Medical, Dental, Vision, Paid Time Off, Sick, Tuition Reimbursement, a 401k/403B, 401a, Performance Plus Bonus, Career Advancement Opportunities, Work Perks, and more.
For information about our employee benefits, please visit:
Sentara Health Plans
is the health insurance division of Sentara Healthcare doing business as Optima Health and Virginia Premier.
Sentara Health Plans
provides health insurance coverage through a full suite of commercial products including consumer-driven, employee-owned and employer-sponsored plans, individual and family health plans, employee assistance plans and plans serving Medicare and Medicaid enrollees.
For applicants within Washington State, the following hiring range will be applied: $73,819.20 to $87,447.36 annually.
#Indeed
#Dice
#Monster
Talroo-IT
Indeed
Monster
Talroo
Talroo-Health Plan
Keywords: Data Analysis, SQL, Tableau, dashboard, Data Analysis, Data Quality, Data Stewardship, HEDIS, STAR, value-based measures, value based measures, ad-hoc reports, ad-hoc, ad-hoc reporting, population health, pop health, pop-health, ambulatory, acute, post-acute, data accuracy, payer, finance, financial, financial analysis, population, population management, Statistics, Alabama, Delaware, Florida, Georgia, Idaho, Indiana, Kansas, Louisiana, Maine, Maryland, Minnesota, Nebraska, Nevada, New Hampshire, North Carolina, North Dakota, Ohio, Oklahoma, Pennsylvania, South Carolina, South Dakota, Tennessee, Texas, Utah, Virginia, Washington, West Virginia, Wisconsin, Wyoming
The Healthcare Data Analyst provides data analysis support to the customer by assisting with the development of reports and/or dashboards to monitor program and operational performance. Promotes self-service analytics for customer adoption, understanding and use of data. Supports design of programmatic analyses and reporting capabilities in addition to business requirement definition for new analyses and performs ad-hoc analysis as directed. This position supports data quality and data stewardship functions to maintain data accuracy and identify new metrics. As a subject matter expert, the Healthcare Data Analyst uses analytic skills to offer data-driven conclusions and recommendations to business partners. As an integrated health system, Sentara delivers analytics across a wide variety of domains, including payer functions, ambulatory, acute & post-acute care and support functions. The subject matter expertise required will vary based on the specific opening, but include the following: Medical expense and trends Quality including HEDIS reporting, interventions, and STAR ratings Clinical gaps and adherence Network Management contracting and pay-for-performance/value-based measures State and Federal regulatory reporting Employer groups reporting and client ad-hoc reporting related to clinical and financial services Population health reporting & analysis Risk, member/patient stratification, member/patient segmentation, quality, provider scoring, and cost savings Ambulatory, acute & post-acute quality, clinical performance, and operations reporting & analysis Quality & performance benchmarking Research data & analytic support Works directly with internal customers and external provider partners. Knowledge of SQL and Tableau Excellent Oral and Written Communication Skills Problem Solving Consultative Engagement Skills
Bachelor's Level Degree
Master's Level Degree
Data Analysis 1 year
Financial Analysis 1 year
Population Management 1 year
Statistics 1 year
Time Management
Troubleshooting
Writing
Active Learning
Active Listening
Communication
Complex Problem Solving
Coordination
Critical Thinking
Reading Comprehension
Service Orientation
Show more
Show less","SQL, Tableau, Data Analysis, Data Quality, Data Stewardship, HEDIS, STAR, ValueBased Measures, AdHoc Reports, Population Health, Ambulatory, Acute, PostAcute, Payer, Finance, Population, Statistics","sql, tableau, data analysis, data quality, data stewardship, hedis, star, valuebased measures, adhoc reports, population health, ambulatory, acute, postacute, payer, finance, population, statistics","acute, adhoc reports, ambulatory, data quality, data stewardship, dataanalytics, finance, hedis, payer, population, population health, postacute, sql, star, statistics, tableau, valuebased measures"
Healthcare Data Analyst,Sentara Health,"Virginia Beach, VA",https://www.linkedin.com/jobs/view/healthcare-data-analyst-at-sentara-health-3734758222,2023-12-17,Norfolk,United States,Mid senior,Hybrid,"City/State
Virginia Beach, VA
Overview
Work Shift
First (Days) (United States of America)
Sentara Health is seeking a passionate Data professional to join our team as
Healthcare Data Analyst!
This position is 100% remote but candidates must have a current residence in one of the follow states or being willing to relocate: AL ,DE, FL, GA, ID, IN, KS, LA, ME, MD, MN, NE, NH, ND, NV, OK, OH,PA, SC, NC, SD, TN, TX, WA, VA, WV, WI
Job Description
The Healthcare Data Analyst provides data analysis support to the customer by assisting with the development of reports and/or dashboards to monitor program and operational performance. Promotes self-service analytics for customer adoption, understanding and use of data. Supports design of programmatic analyses and reporting capabilities in addition to business requirement definition for new analyses and performs ad-hoc analysis as directed. This position supports data quality and data stewardship functions to maintain data accuracy and identify new metrics. As a subject matter expert, the Healthcare Data Analyst uses analytic skills to offer data-driven conclusions and recommendations to business partners.
Qualifications & Skills :
Bachelor Level Degree
Knowledge of SQL and Tableau
Excellent Oral and Written Communication Skills
Problem Solving
Consultative Engagement Skills
Responsibilities
Responsible for developing and maintaining a program to improve data literacy across the enterprise, and help the organization use data to drive operational and clinical improvements. Works with operational and clinical leaders across the enterprise to understand business needs, guide leaders in the use of current key analytic models, dashboards, and strategic reporting tools. Develops a learning and development program within the analytics community, to promote the continuing development of skills within analytic teams.
To be successful in this role, individuals must display an in-depth understanding of both technical perspectives surrounding data management & data analytics, as well as the business drivers & imperatives for data-driven solutions. This is a role with shifting responsibilities across a wide array of functions – data literacy, data strategy, business and data analysis, and analytics consulting.
Domain knowledge in the following areas preferred. While all are not required, the ideal candidate will demonstrate understanding of the following: methods to support adult learning, data literacy, data exploration & visualization techniques; database structure and querying methods, healthcare electronic medical record systems and billing systems.
As the third-largest employer in Virginia, Sentara Health was named by Forbes Magazine as one of America's best large employers. We offer a variety of amenities to our employees, including, but not limited to:
Medical, Dental, and Vision Insurance
Paid Annual Leave, Sick Leave
Flexible Spending Accounts
Retirement funds with matching contribution
Supplemental insurance policies, including legal, Life Insurance and AD&D among others
Work Perks program including discounted movie and theme park tickets among other great deals
Opportunities for further advancement within our organization
Sentara employees strive to make our communities healthier places to live. We're setting the standard for medical excellence within a vibrant, creative, and highly productive workplace. For information about our employee benefits, please visit: Benefits – Sentara (sentaracareers.com)
Join our team! We are committed to quality healthcare, improving health every day, and provide the opportunity for training, development, and growth!
Note: Sentara Healthcare offers employees comprehensive health care and retirement benefits designed with you and your family's well-being in mind. Our benefits packages are designed to change with you by meeting your needs now and anticipating what comes next. You have a variety of options for medical, dental and vision insurance, life insurance, disability, and voluntary benefits as well as Paid Time Off in the form of sick time, vacation time and paid parental leave. Team Members have the opportunity to earn an annual flat amount Bonus payment if established system and employee eligibility criteria is met.
For applicants within Washington State, the following hiring range will be applied: $69,957 to $104,936.83 annually.
keywords: Talroo-IT, Indeed, Monster, Circa, LinkedIn, #Dice ""SQL"", ""Tableau"", ""Analytics"", ""Statistics""
Job Summary
The Healthcare Data Analyst provides data analysis support to the customer by assisting with the development of reports and/or dashboards to monitor program and operational performance. Promotes self-service analytics for customer adoption, understanding and use of data. Supports design of programmatic analyses and reporting capabilities in addition to business requirement definition for new analyses and performs ad-hoc analysis as directed. This position supports data quality and data stewardship functions to maintain data accuracy and identify new metrics. As a subject matter expert, the Healthcare Data Analyst uses analytic skills to offer data-driven conclusions and recommendations to business partners.
As an integrated health system, Sentara delivers analytics across a wide variety of domains, including payer functions, ambulatory, acute & post-acute care and support functions. The subject matter expertise required will vary based on the specific opening, but include the following:
Medical expense and trends
Quality including HEDIS reporting, interventions, and STAR ratings
Clinical gaps and adherence
Network Management contracting and pay-for-performance/value-based measures
State and Federal regulatory reporting
Employer groups reporting and client ad-hoc reporting related to clinical and financial services
Population health reporting & analysis
Risk, member/patient stratification, member/patient segmentation, quality, provider scoring, and cost savings
Ambulatory, acute & post-acute quality, clinical performance, and operations reporting & analysis
Quality & performance benchmarking
Research data & analytic support
Works directly with internal customers and external provider partners.
Knowledge of SQL and Tableau
Excellent Oral and Written Communication Skills
Problem Solving
Consultative Engagement Skills
Qualifications:
BLD - Bachelor's Level Degree (Required), MLD - Master's Level Degree
Data Analysis, Financial Analysis, Population Management, Statistics
Skills
Active Learning, Active Listening, Communication, Complex Problem Solving, Coordination, Critical Thinking, Reading Comprehension, Service Orientation, Time Management, Troubleshooting, Writing
Sentara Healthcare prides itself on the diversity and inclusiveness of its close to an almost 30,000-member workforce. Diversity, inclusion, and belonging is a guiding principle of the organization to ensure its workforce reflects the communities it serves.
Per Clinical Laboratory Improvement Amendments (CLIA), some clinical environments require proof of education; these regulations are posted at ecfr.gov for further information. In an effort to expedite this verification requirement, we encourage you to upload your diploma or transcript at time of application.
In support of our mission “to improve health every day,” this is a tobacco-free environment.
Show more
Show less","SQL, Tableau, Data Analysis, Statistics, Population Management, Active Learning, Active Listening, Communication, Complex Problem Solving, Coordination, Critical Thinking, Reading Comprehension, Service Orientation, Time Management, Troubleshooting, Writing","sql, tableau, data analysis, statistics, population management, active learning, active listening, communication, complex problem solving, coordination, critical thinking, reading comprehension, service orientation, time management, troubleshooting, writing","active learning, active listening, communication, complex problem solving, coordination, critical thinking, dataanalytics, population management, reading comprehension, service orientation, sql, statistics, tableau, time management, troubleshooting, writing"
"Data Analyst I -FCX/PCI - Raleigh, NC",Applied Industrial Technologies,"Raleigh, NC",https://www.linkedin.com/jobs/view/data-analyst-i-fcx-pci-raleigh-nc-at-applied-industrial-technologies-3751756082,2023-12-17,Whitman,United States,Associate,Hybrid,"PCI is seeking a Data Analyst for a new position needed to support our clients in the pharmaceutical industry. This is NOT an IT/Computer Engineering position, but rather a database support role in the Calibration and Metrology department. The work will be located in Durham and/or Raleigh. This is an hourly position with up to twenty-five percent travel.
Highlights Of Top Responsibilities And Required Skills
Relational database (CMMS) experience, Maximo, Procal, SAP, GAGEtrak, Blue Mountain RAM or similar software.
Familiar with database reporting
Involves extensive Data Entry
Extensive MS Office experience, as well as MS Teams.
The ability to prioritize duties based on customer requests
Basic administrative functions: shipping, scheduling and deliverable coordination.
Being prepared to demonstrate thoroughness with tasks and interactions with our technicians and customers.
Expectations
The Data Analysts are expected to take a proactive role in supporting the division and in providing client support. The DA is expected to build a high level of trust with internal and client personnel. This trust is developed through consistently upholding PCI Values and demonstrating Honesty, Integrity, Pride, Accountability, Teamwork, and Commitment.
Responsibilities& Assignments
Data Analyst I
Accurate and timely management of calibration databases and data entry at PCI and at client sites.
Completes work in a safe and correct manner, following PCI and client specific policies, procedures, and proposals.
Ensures precise and timely documentation.
Completes weekly timesheets with correct labor code charges.
Assists with scheduling and prioritizing calibrations based on client requests.
Creates and delivers PCI past due and calibration due reports and internal monthly and quarterly preventive maintenance (PMs).
Administrative support relative to shipping, scheduling, delivery coordination, etc.
Initiates client reports.
Constantly seeks innovative ways to deliver better value to clients in a highly professional, profitable manner.
Maintains a neat and professional PCI work area and projects a professional image at all times.
Always considers safety when executing tasks and projects. Ensures that PCI work areas are safe, neatly organized and professional on-site and in office.
Is attune to potential new opportunities for PCI when at a client site or interacting with a client and relays this lead back to Leader/Manager.
Skills Required
Data Analyst I
Accurate documentation, project follow through, and adherence to safe work practices.
High level of customer service, integrity, and ingenuity.
Effective communication skills with peers, manager and client.
Professional in appearance, communication, and follow through.
Organized and on target priorities.
Reliable (attendance, punctuality, meeting deadlines).
Comprehends the importance of financial discipline and operates with a focus on the company's bottom line.
Works well in a team environment and is open to suggestions of others.
Ability to work remotely under client direction with limited PCI management oversight.
Excellent working knowledge of MS Office Suite.
Report development software and relational database experience; preferably with calibration maintenance/management software expertise.
Experience& Education Required For Data Analyst I
A two-year degree in Computer Science, Business Administration, other related field, or equivalent military training and two years of applicable experience or the equivalent combination of education and experience. Pharmaceutical industry or quality system experience is preferred.
PCI is proud to serve pharmaceutical, biotechnology, medical device, and clinical research industries nationwide by providing calibration, commissioning and consulting solutions. At PCI, calibrations are performed by highly technical, cGMP/GLP-trained personnel who are knowledgeable in quality guidelines enforced by the FDA, EPA and ISO/IEC 17025:2017. PCI is an FCX Performance Company and a Subsidiary of
Applied Industrial Technologies (NYSE: AIT);
a leading industrial distributor that offers more than 6.5 million parts to serve the needs of MRO and OEM customers in virtually every industry.
Applied® provides engineering, design and systems integration for industrial and fluid power applications, as well as customized mechanical, fabricated rubber and fluid power shop services. Applied also offers storeroom services and inventory management solutions that provide added value to our customers.
We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, ancestry, gender, sexual orientation, gender identity, age, disability, protected veteran status, marital status, medical condition or any other characteristic protected by law.
Show more
Show less","Promax, Maximo, Data Entry, MS Office Suite, SAP, GAGEtrak, Blue Mountain RAM, Relational Databases, Report Development Software, MS Teams, Calibration, Scheduling, Deliverable Coordination, Shipping","promax, maximo, data entry, ms office suite, sap, gagetrak, blue mountain ram, relational databases, report development software, ms teams, calibration, scheduling, deliverable coordination, shipping","blue mountain ram, calibration, data entry, deliverable coordination, gagetrak, maximo, ms office suite, ms teams, promax, relational databases, report development software, sap, scheduling, shipping"
MuleSoft Enterprise Data Engineer,Schrödinger,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/mulesoft-enterprise-data-engineer-at-schr%C3%B6dinger-3719238398,2023-12-17,Nyack,United States,Associate,Hybrid,"We’re seeking a
MuleSoft Enterprise
Data Engineer to fill a temporary-to-permanent position
and join us in our mission to improve human health and quality of life through the development, distribution, and application of advanced computational methods!
Schrödinger is on the cutting edge of computer-aided drug discovery and materials science, collaborating with companies like Takeda, Nimbus, Pfizer, and Sanofi. We set the record for the largest and fastest cloud computing run, and our software suites continue to revolutionize the design of therapeutics and materials. WaterMap, Maestro, and LiveDesign are just a few examples of the programs we’ve created.
As a member of our Data team, you’ll play a critical role in the administration and improvement of our corporate data management system.
Who Will Love This Job
A data engineer with meaningful architecture experience
A creative problem-solver with an attention to detail
An excellent team player who wants to make a sizable impact working on a small, dedicated department
A lifelong student who’s eager to learn new and varied technologies and business processes on the job
A humanitarian who cares deeply about improving human health
What You’ll Do
Provide technical leadership towards architecting and delivering end to end solutions
Work with business partners for prioritization, impact assessment, and resolution
Collaborate with other teams for multi-functional initiatives
Design and build reusable components, frameworks and libraries at scale to support analytics products
Devise and implement product features in collaboration with business and technology partners
Develop architecture and design patterns to process and store high volume data sets
Identify and tackle issues concerning data management to improve data quality
Collaborate on the implementation of new data management projects and re-structure of the current data architecture
Build continuous integration, test-driven development and production deployment frameworks
Tackle data issues and perform root cause analysis to proactively resolve product and operational issues
What You Should Have
At least four years of relevant experience, with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT, and reporting/analytic tools
At least three years of experience in cloud environments like GCP and BigQuery
Experience developing custom features and solutions for ERP/CRM systems in similar roles, especially using Apex in Salesforce
Understanding of relational and non-relational SQL
Familiarity with building real-time streaming data pipelines
Background in pub/sub modes like Kafka
Experience in building lambda, kappa, microservice and batch architecture
Knowledge of CI/CD processes and source control tools such as GitHub and related dev processes
Experience with MuleSoft
Experience with NetSuite and/or Salesforce Lightning is a major plus
If you don’t consider yourself an expert in all of the fields
above
, that’s OK - we’re looking to hire an adaptable polymath who’s excited to learn on the job. Our team will be happy to help you get up to speed in any unfamiliar areas!
Pay And Perks
Schrödinger understands it’s people that make a company great. Because of this, we’re prepared to offer a competitive salary, stock options, and a wide range of benefits that include healthcare (with dental and vision), a 401k, pre-tax commuter benefits, a flexible work schedule, and a parental leave program. We have catered meals in the office every day, a company culture that is relaxed but engaged, and over a month of paid vacation time. Our Administrative and Human Resources departments also plan a myriad of fun company-wide events. New York is home to our largest office, but we have teams all over the world. Schrödinger is honored to have been selected as one of Crain's New York Best Places to Work in 2018 and 2019.
Sound exciting? Apply today and join us!
Estimated base salary range: $105,000 - $165,000. Actual compensation package is dependent on a number of factors, including, for example, experience, education, degrees held, market data, and business needs. If you have any questions regarding the compensation for this role, do not hesitate to reach out to a member of our Strategic Growth team.
As an equal opportunity employer, Schrödinger hires outstanding individuals into every position in the company. People who work with us have a high degree of engagement, a commitment to working effectively in teams, and a passion for the company's mission. We place the highest value on creating a safe environment where our employees can grow and contribute, and refuse to discriminate on the basis of race, color, religious belief, sex, age, disability, national origin, alienage or citizenship status, marital status, partnership status, caregiver status, sexual and reproductive health decisions, gender identity or expression, sexual orientation, or any other protected characteristic. To us, ""diversity"" isn't just a buzzword, but an important element of our core principles and key business practices. We believe that diverse companies innovate better and think more creatively than homogenous ones because they take into account a wide range of viewpoints. For us, greater diversity doesn't mean better headlines or public images - it means increased adaptability and profitability.
Show more
Show less","Data engineering, Architecture, Data warehouse, ETL/ELT, Reporting/analytic tools, Cloud environments, GCP, BigQuery, Apex, Salesforce, SQL, Realtime streaming data pipelines, Kafka, Lambda, Kappa, Microservice, Batch architecture, CI/CD processes, GitHub, MuleSoft, NetSuite, Salesforce Lightning","data engineering, architecture, data warehouse, etlelt, reportinganalytic tools, cloud environments, gcp, bigquery, apex, salesforce, sql, realtime streaming data pipelines, kafka, lambda, kappa, microservice, batch architecture, cicd processes, github, mulesoft, netsuite, salesforce lightning","apex, architecture, batch architecture, bigquery, cicd processes, cloud environments, data engineering, datawarehouse, etlelt, gcp, github, kafka, kappa, lambda, microservice, mulesoft, netsuite, realtime streaming data pipelines, reportinganalytic tools, salesforce, salesforce lightning, sql"
SAP Master Data Analyst,Sika,"Lyndhurst, NJ",https://www.linkedin.com/jobs/view/sap-master-data-analyst-at-sika-3761165239,2023-12-17,Nyack,United States,Associate,Hybrid,"Company Description
With over 100 years of experience, Sika is a worldwide innovation and sustainability leader in the development and production of systems and products for commercial and residential construction, as well as the marine, automotive, and renewable energy manufacturing industries.  Sika has offices in over 100 countries with over 300 manufacturing facilities and more than 33,500 employees worldwide. With annual sales of 11.5+ billion dollars in 2022, our commitment to quality, innovation, and the environment as well as putting our customer’s needs first, encompasses why Sika is the global leader in our industries.
Job Description
Broad Function and Purpose of Position: Consistent extension, release, update and deletion of SAP material master data on local level, according to defined business processes and rules. Manage the local data process for Sika US for 45+ plants and monitor local master data for errors and consistency per master data rules.
Creation of SAP material master data at local level for all new items, for all plants within Sika US according to Sika global and US local business rules.
Completion of mass changes to SAP material master data per local business requests.
Release of all new materials after checking completion of local data creation and costing.
Local deletion of materials per local business requests, involving multiple data checks in SAP, BOM deletion flags, material master deletion flags, and statuses, and submission of global deletion requests as required.
Local process owner for new material workflow process. Involves communication with local business for accurate / complete information in new item requests, as well as timely completion of requests by local responsible persons. Also involves training of local business in the workflow process as required.
Monitoring of local master data for errors through the use of web-based tool (DIC – data integrity centre). Resolve errors through correction of data, or communication to local responsible to drive the corrections.
Assist local business with extracts of master data / reports as needed (including quarterly SKU report to local management).
Assist with analysis and resolution of cross – functional data issues.
Assist local integration team with data related aspects of acquisition integrations and any other project participation as required.
Qualifications
Degree in Computer Sciences / Information Technology or equivalent work experience.
Experience in manufacturing, logistics, or material management in a technical-industrial environment.
A minimum of 3 years experience in data management (master data management) (preferred).
Knowledge/Experience with Microsoft products including Windows 10 and Office 365 (Outlook and Teams).
Proficient with Microsoft Excel.
Knowledge / Experience with SAP (preferred).
Ability to excel in a fast-paced and agile environment where critical thinking and strong problem-solving skills are required to be successful.
Effectively prioritize tasks and escalate problems to management when needed.
Ability to perform tasks with minimal supervision.
Execute tasks with high accuracy and reliability, with high attention to detail.
Willingness to travel.
Additional Information
Competitive Benefits: Health Insurance, 401k with company match, year-end profit-sharing bonus, paid time off, and paid holidays.
Meaningful Work: Sika products enhance our surroundings and the work every employee completes helps positively impact daily lives by making our world stronger, more durable, and more reliable – every day.
Company Culture: Sika centers work culture around entrepreneurship where individuals have the power to make decisions, learn from mistakes, and define their career.
Community Involvement: Sika takes active roles in our community and aims to support volunteer work and charitable endeavors across the United States through rebuilding and giving back.
Sustainability Initiatives: Sika is committed to sustainable development, reducing environmental impacts, and assuming social responsibility. The company supports energy efficient projects and implements numerous measures aimed to boost economic, social, and ecological sustainability.
Sika Corporation is committed to a work environment that supports, inspires, and respects all individuals that apply. As an equal opportunity employer Sika will consider all qualified applicants without discrimination on the basis of race, color, religion, sex, pregnancy, sexual orientation, gender identity, age, disability, national or ethnic origin, or other protected characteristics.
Show more
Show less","SAP, Material master data, Data management, Master data management, Microsoft products, Windows 10, Office 365, Outlook, Teams, Microsoft Excel, Entrepreneurship, Decision making, Problem solving, Critical thinking, Accuracy, Reliability, Attention to detail, Communication, Training, Analysis, Resolution, Integration, Acquisition","sap, material master data, data management, master data management, microsoft products, windows 10, office 365, outlook, teams, microsoft excel, entrepreneurship, decision making, problem solving, critical thinking, accuracy, reliability, attention to detail, communication, training, analysis, resolution, integration, acquisition","accuracy, acquisition, analysis, attention to detail, communication, critical thinking, data management, decision making, entrepreneurship, integration, master data management, material master data, microsoft excel, microsoft products, office 365, outlook, problem solving, reliability, resolution, sap, teams, training, windows 10"
Senior Lead Data Engineer (Remote-Eligible),Jobs for Humanity,"New York County, NY",https://www.linkedin.com/jobs/view/senior-lead-data-engineer-remote-eligible-at-jobs-for-humanity-3785525125,2023-12-17,Nyack,United States,Mid senior,Onsite,"Job Description
Job Title: Senior Lead Data Engineer (Remote-Eligible)
Do you enjoy working in technology and solving complex business problems? At Capital One, we are a diverse and inclusive group of individuals who love to create, innovate, and meet the needs of our customers. We are looking for passionate Data Engineers who are skilled in merging data with emerging technologies. As a Senior Lead Data Engineer at Capital One, you will play a vital role in driving a major transformation within our company.
What You'll Do
Manage and develop a Java-based pipeline and query tools using technologies such as Hive Metastore, AWS S3, Kafka, and ORC.
Create analytics tools to solve business problems that come with growth and international expansion.
Optimize configurations for analytics tools to support our growing business and organization.
Basic Qualifications
Bachelor's Degree.
At least 8 years of experience in application development (Internship experience does not apply).
At least 2 years of experience in big data technologies.
At least 1 year of experience with cloud computing (AWS, Microsoft Azure, Google Cloud).
Preferred Qualifications
9+ years of experience in application development including Python, Javascript, or Java.
4+ years of experience with AWS.
5+ years of experience with Distributed data/computing tools (Trino, Hive, Kafka, or Spark).
4+ years of experience working on real-time data and streaming applications.
4+ years of experience with NoSQL implementation (Cassandra).
4+ years of experience with UNIX/Linux, including basic commands and shell scripting.
2+ years of experience with Agile engineering practices.
Capital One is open to hiring remote employees for this opportunity.
Compensation
New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Data Engineer.
San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Data Engineer.
Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Data Engineer.
Please note that salaries for part-time roles will be prorated based on agreed-upon hours.
Benefits
Capital One offers a comprehensive set of health, financial, and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on employment status and management level.
Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. We do not discriminate based on sex, race, age, disability, genetic information, sexual orientation, gender identity, citizenship, veteran status, or any other basis prohibited by law. We also promote a drug-free workplace and consider applicants with a criminal history in compliance with applicable laws and regulations.
If you require accommodation during the application process, please contact Capital One Recruiting at (phone number removed) or email (url removed). All provided information will be kept confidential and used only to provide necessary accommodations.
For technical support or questions about our recruiting process, email (url removed).
Please note that Capital One Financial includes multiple entities. Positions posted in Canada are for Capital One Canada, positions in the UK are for Capital One Europe, and positions in the Philippines are for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Hive Metastore, AWS S3, Kafka, ORC, Python, Javascript, Trino, Hive, Spark, Cassandra, UNIX/Linux, Shell Scripting, Agile Engineering, NoSQL, AWS, Cloud Computing (AWS Microsoft Azure Google Cloud), Distributed Data/Computing Tools","java, hive metastore, aws s3, kafka, orc, python, javascript, trino, hive, spark, cassandra, unixlinux, shell scripting, agile engineering, nosql, aws, cloud computing aws microsoft azure google cloud, distributed datacomputing tools","agile engineering, aws, aws s3, cassandra, cloud computing aws microsoft azure google cloud, distributed datacomputing tools, hive, hive metastore, java, javascript, kafka, nosql, orc, python, shell scripting, spark, trino, unixlinux"
Senior Data Engineer,Bruin Group LLC,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-bruin-group-llc-3766135639,2023-12-17,Nyack,United States,Mid senior,Onsite,"*
NYC Office (3-4 days onsite) *
*GC & US Citizen's Only*
Senior Data Engineer at Bruin
New York, New York, United States
*Must be onsite 3-4 days in our NYC Office
At Bruin, we're deeply committed to developing innovative software solutions that address real-world challenges. We rely on our Senior Data Engineers to leverage their expertise in managing massive-scale data, providing actionable insights in real-time. As we aim to fortify our data capabilities and build a scalable data infrastructure, we are currently seeking an experienced Senior Data Engineer to play a pivotal role in designing and implementing robust solutions. Your expertise will be instrumental in crafting an efficient and scalable data architecture that empowers our organization to derive valuable insights seamlessly.
In this role, you will play a crucial part in shaping our data strategy, contributing fresh ideas, and demonstrating a unique and informed perspective. Collaboration with cross-functional teams will be key as you work towards developing robust, real-world solutions that enhance user experiences at every touchpoint.
If you are passionate about leveraging data to drive meaningful outcomes, possess a wealth of experience, and thrive in collaborative environments, we invite you to join us in this exciting journey of innovation and impact.
Responsibilities:
Database Scaling Strategy:
Develop and execute a strategic plan for scaling our database (Microsoft SQL Server, Postgres) infrastructure to accommodate the growing data and traffic demands of our enterprise applications.
Evaluate and select the most suitable technologies, architectures, and solutions for database scaling.
Database Architecture and Design:
Architect and design database systems that can efficiently store and retrieve large volumes of data while maintaining optimal performance.
Define data partitioning, sharding, and replication strategies to distribute data effectively.
Data Architecture and Strategy:
Define and implement the data engineering strategy, ensuring alignment with business objectives and future growth.
Design and maintain data architecture, including data lakes, data warehouses, and data pipelines.
Technical Leadership:
Provide technical leadership and mentorship to the data engineering team.
Collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to ensure data solutions meet their requirements.
Data Modeling and ETL:
Develop and optimize data models for efficient storage and retrieval.
Oversee the development of ETL (Extract, Transform, Load) processes to ingest, clean, and transform data from various sources.
Scalability and Performance:
Implement strategies for data scalability and performance optimization.
Monitor system performance, troubleshoot issues, and ensure data availability and reliability.
Data Security and Compliance:
Ensure data security and compliance with data protection regulations (e.g., GDPR, HIPAA) and industry best practices.
Implement access controls, encryption, and auditing mechanisms.
Data Quality and Governance:
Establish data quality standards and governance processes.
Implement data validation and cleansing routines to maintain data accuracy and integrity.
Tool Selection and Evaluation:
Evaluate and select appropriate data engineering tools, frameworks, and technologies.
Keep up-to-date with industry trends and emerging technologies.
Documentation and Best Practices:
Create and maintain documentation for data architecture, standards, and best practices.
Promote data engineering best practices across the organization.
Performance Optimization:
Identify and implement strategies for optimizing query performance and reducing data latency.
Troubleshoot and resolve performance bottlenecks.
Collaboration:
Collaborate with stakeholders to understand data requirements and deliver data solutions that meet business needs.
Present data architecture and solutions to technical and non-technical audiences.
Qualifications:
Bachelor's or Master's degree in Computer Science, Data Science, or a related field.
8+ years of experience in data engineering and architecture, with a track record of designing and implementing complex data solutions.
5+ years of experience working/designing/optimizing on MS SQL, stored procedures, SSIS.
Proficiency in data modeling, ETL processes, and data integration.
Strong knowledge of database technologies (e.g., SQL, NoSQL, distributed databases).
Expertise in data warehousing and data lake design.
Hands-on experience with cloud platforms (e.g., AWS, Azure, GCP) and containerization technologies (e.g., Docker, Kubernetes).
Familiarity with big data technologies (e.g., Hadoop, Spark) and data streaming (e.g., Kafka, Flink).
Excellent programming skills in languages such as Python, Java.
Strong problem-solving and communication skills.
Leadership and mentoring experience.
*The salary range reflected is a good faith estimate of base pay for the primary location of the position. Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $200,000 -$250,000 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, experience and abilities of the successful candidate. Your recruiter can share more about the specific salary range for the job location during the hiring process.
Show more
Show less","Database Scaling, Database Architecture, Data Architecture, Technical Leadership, Data Modeling, ETL, Scalability, Performance, Data Security, Data Quality, Governance, Tool Selection, Best Practices, Documentation, Collaboration, SQL, NoSQL, Hadoop, Spark, Python, Java, AWS, Azure, GCP, Docker, Kubernetes, Kafka, Flink","database scaling, database architecture, data architecture, technical leadership, data modeling, etl, scalability, performance, data security, data quality, governance, tool selection, best practices, documentation, collaboration, sql, nosql, hadoop, spark, python, java, aws, azure, gcp, docker, kubernetes, kafka, flink","aws, azure, best practices, collaboration, data architecture, data quality, data security, database architecture, database scaling, datamodeling, docker, documentation, etl, flink, gcp, governance, hadoop, java, kafka, kubernetes, nosql, performance, python, scalability, spark, sql, technical leadership, tool selection"
Data Engineer with Neo4j Graph,Genpact,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/data-engineer-with-neo4j-graph-at-genpact-3766869278,2023-12-17,Nyack,United States,Mid senior,Onsite,"Data Engineer with Neo4j Graph, NYC, NY, Fulltime
must have : Graph DB Neo 4J, Arango DB Neptune
Skills Required 8+ years of relevant work experience Strong with programming in Python to perform,
Batch data engineering on Apache Spark and populate downstream batch data stores (such as data mart) for BI use cases & to generate downstream feeds (ie., flat & wide tables or compressed files) for Data Science use cases Real-time service integration to process business events off Kafka and persist in operational MS SQL/MongoDB and/or Neo4j Graph data stores for fraud investigation.
Near real-time stream processing to derive features for ML model inference.
Strong with SQL Server & Hadoop based implementations Strong SQL & Stored Procedure skills Proficient with MongoDB and Neo4j Graph databases.
Good hands-on experience with at least one of the job scheduling tools like Autosys (Preferred), Control-M etc.
Experience of working in a Linux environment and can write Python/Shell scripts Strong data architecture and modeling experience.
Especially, in dimensional modeling for data mart design and development to support BI use cases Strong data analytics.
skills Strong oral and written communication skills Excellent interpersonal skills and professional approach Strong analytical and problem-solving skills Ability to learn quickly and pick up new techniques and/or technologies Experience in building & maintaining data solutions for BI & Data Science use cases Experience in Fraud detection and prevention business in Financial Services Skills Desired Experience with Azure (Databricks, Data Factory, Synapse, Azure Data Lake) and/or AWS (AWS S3, AWS Athena, AWS Glue) Data ecosystem & Snowflake Data Cloud Experience in building virtual data access layer using TIBCO Data Virtualization to support BI & Data Science use cases Experience of the full software development life cycle Experience of working in an Agile team Experience of working with version control systems Experience with bash scripting
Experience of working with Continuous Integration systems
Best Regards
Kavirala Sandeep Kumar
Talent Acquisition
E: Sandeep.Kavirala@genpact.com
Cell : 7
13-354-9580
www.Genpact.com
Show more
Show less","Neo4j Graph, ArangoDB, Neptune, Python, Apache Spark, Kafka, SQL Server, Hadoop, MongoDB, Autosys, ControlM, Azure, Databricks, Data Factory, Synapse, AWS, AWS S3, AWS Athena, AWS Glue, Data ecosystem, Snowflake Data Cloud, TIBCO Data Virtualization, bash scripting, Continuous Integration systems","neo4j graph, arangodb, neptune, python, apache spark, kafka, sql server, hadoop, mongodb, autosys, controlm, azure, databricks, data factory, synapse, aws, aws s3, aws athena, aws glue, data ecosystem, snowflake data cloud, tibco data virtualization, bash scripting, continuous integration systems","apache spark, arangodb, autosys, aws, aws athena, aws glue, aws s3, azure, bash scripting, continuous integration systems, controlm, data ecosystem, data factory, databricks, hadoop, kafka, mongodb, neo4j graph, neptune, python, snowflake data cloud, sql server, synapse, tibco data virtualization"
Sr. Data Analyst,Marshwinds International Incorporated,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/sr-data-analyst-at-marshwinds-international-incorporated-3783677414,2023-12-17,Nyack,United States,Mid senior,Remote,"Sr. Data Analyst
Remote -
US
Sponsorship Not Available -
Legal authorization to work in the USA is required. Our client is unable to sponsor individuals for employment visas, now or in the future, for this job opening.
Our client is a Global cloud communications leader that helps businesses accelerate their digital transformation through our fully programmable Unified Communications, Contact Center Applications, and Communications APIs.
As a Senior Data Analyst, you will be helping us on a journey towards a modern, self-serve analytics organization. You will get autonomy and ownership around large parts of how we interface with users, how they’re trained and around defining the boundary between “let me show you how to do this yourself” and “let me do this for you”.
Over the last couple of years, we’ve made significant investments into our data infrastructure and our data processing platform. This has improved the internal team processes, however, there is still room for improvement on enabling self-serve and using advanced analytics techniques to predict what will happen rather than what happened.
Where you will work:
Fully Remote Worker
- You will be home based in the USA as opposed to a designated Office and you will work 100% of the time from home.
Time Zone Requirements:
must be based on EST
in the United States
What you will do:
Work closely with Info Sec teams on upgrading their suite of reports to move to a self-service reporting environment driving efficiencies and cost savings
Use data mining techniques and analyze core KPI's generating insight to reach and improve on the target goals
Be our bridge and ambassador to the InfoSec team, championing for self-service data analytics and maintaining relationships with power users
Be responsible for coordinating our user facing “Data Academy” education initiative to make sure our users can make the most of the data and reporting that we have
Work closely with the rest of the BI & Data team to help make sure the data engineering work being done is well aligned to stakeholder needs
Self-service analytics environment: Given the size of the team vs the hundreds of users we support, a large part of your work would be focused on finding ways to scale analytics via empowering our users so they are able to do what they are trying to do independently (while having support and training, if they need it).
We’re using a modern analytics stack with Tableau as the visualization front-end and Snowflake as the DWH, where the majority of report development is done by our users, we also use Python, SQL and Airflow
Rather than a report mill, the BI & Data team is there to empower and help users get their data and analytics work done themselves
Required
7+ years of Data Analyst experience
Must have strong Python skills
Experience working independently with a wide range of stakeholders, from other analysts to senior management
Advanced knowledge of fundamental analytical, statistical techniques, attribution results, forecasting, simulation and data optimization
Strong technical capabilities with SQL, data warehouses, ETL processes, and data integration tools (Python, R coding/familiarity a plus)
Skilled in Tableau or other BI Tools
Show more
Show less","Data Analysis, Data Mining, KPI Analysis, Tableau, Snowflake, Python, SQL, Airflow, ETL, Data Warehouses, Data Integration","data analysis, data mining, kpi analysis, tableau, snowflake, python, sql, airflow, etl, data warehouses, data integration","airflow, data integration, data mining, data warehouses, dataanalytics, etl, kpi analysis, python, snowflake, sql, tableau"
Data Scientist,Curate Partners,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/data-scientist-at-curate-partners-3787308609,2023-12-17,Nyack,United States,Mid senior,Remote,"Job Description – Details:
Develops, validates and executes algorithms and predictive models to investigate problems, detect patterns and recommend solutions
Explores, examines and interprets large volumes of data in various forms
Performs analyses of structured and unstructured data to solve multiple and/or complex business problems utilizing advanced statistical techniques and mathematical analyses and broad knowledge of the organization and/or industry
Utilizing advanced statistical techniques and mathematical analyses
Develops data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs
Uses data visualization techniques to effectively communicate analytical results and support business decisions
Creates and evaluates the data needs of assigned projects and assures the integrity of the data
Explores existing data and recommends additional sources of data for improvements
Documents projects including business objectives, data gathering and processing, detailed set of results and analytical metrics
Required
:
1-3+ years of relevant analytic experience
Experience programming using R or Python
Experience in SAS or SQL
Show more
Show less","Predictive Modeling, Data Analysis, Data Visualization, Advanced Statistics, Data Mining, Machine Learning, Data Pipelines, Data Structures, R, Python, Business Intelligence, SAS, SQL","predictive modeling, data analysis, data visualization, advanced statistics, data mining, machine learning, data pipelines, data structures, r, python, business intelligence, sas, sql","advanced statistics, business intelligence, data mining, data structures, dataanalytics, datapipeline, machine learning, predictive modeling, python, r, sas, sql, visualization"
Senior Data Analyst Remote position,Avani Tech Solutions Private Limited,"Columbus, OH",https://www.linkedin.com/jobs/view/senior-data-analyst-remote-position-at-avani-tech-solutions-private-limited-3777997361,2023-12-17,Norwich,United States,Associate,Onsite,"Indotronix is seeking a Senior Data Analyst Remote position
Position:
Senior Data Analyst
Location:
Remote
Duration:
12 Months
Remote role. Candidates must be located in the client footprint to be considered. Potential to convert but conversion is NOT guaranteed and will depend on business needs - Please list if candidate is willing/able to convert on the resume.
Potential for rare occasional travel for all-hands meetings. Please review travel and expense policy with your candidate if they are selected.
Description
This position will be of the Interconnection Services Department. This position will be focused on supporting data governance, quality control, and audit compliance within Interconnection Services.
The Data Steward’s core functions include, but are not limited to,
Prepare reports by joining Interconnections Services (IS) owned applications and models to other internal applications and models for audit compliance, regulatory reporting, and executive reports.
Define data needs and record data definitions for IS owned applications and models.
Establish data governance and repeatable IT processes for IS application data needs.
Manage the data needs and integrity for IS application data.
Collabo*** with Data Management Services, IT, and Business Unit functions to establish and execute a IS data st***gy.
Identify the need for data from IS applications by other business units within client and establish data governance for the most critical data needs.
Identify and prioritize data needs and issues in IS processes for future IT projects, including the mapping of this information and working with the IT data teams for resolution.
Work with IT and other internal teams to ensure that new IS applications are developed with data governance as a high priority and an integral part of the projects.
Identify automation opportunities of data validation and governance for the above processes.
Manage data assets, data lineage, and data quality, and support sound data analysis that rationalizes the applicable data governance strategy.
Work with other client Data Stewards to develop and implement client enterprise-wide standards and processes for data governance.
Work with client Corpo*** Data Governance Group to populate metadata for IS applications in the client Metadata Repository.
Prepare reports for management to clearly demonst*** data governance needs, activities, projects, and results.
Essential Job Functions & Tasks
Under some supervision, performs data stewardship activities and analysis requiring technical knowledge in a safe, effective, efficient, and economic accomplishment of assigned objectives.
Data Governance
Strategy & Methodologies: Assist with operationalizing a business data domain specific data governance strategy with direction from lead data stewards.
Roadmap: Implement one or more part of a business data domain's data governance roadmap.
Data Quality
Policies, Standards & Scope: Execute and implement SOPs to enhance the quality of a business data domain's data as per defined scope and priorities.
Trust: Establish and build trust with others and work collaboratively across functions, levels, and departments towards shared business objectives.
Completeness: Execute and verify the processes that identify and enhance the completeness of a business data domain's data as per defined scope.
Accuracy: Execute and verify the processes that identify and enhance the Accuracy of a business data domain's data as per defined scope.
Consistency: Execute and verify the processes that identify and enhance the Consistency of a business data domain's data as per defined scope.
Integrity: Execute and verify the processes that identify and enhance the Integrity of a business data domain's data as per defined scope. Implement new business rules related to Integrity of relationships within the data.
Reasonability: Execute and verify the processes that identify and enhance the Reasonability of a business data domain's data as per defined scope.
Timeliness: Execute and verify the processes that identify and enhance the Timeliness of a business data domain's data as per defined scope.
Uniqueness/Deduplication: Execute and verify the processes that identify and enhance the Uniqueness/Deduplication of a business data domain's data as per defined scope.
Validity: Execute and verify the processes that identify and enhance the Validity of a business data domain's data as per defined scope.
Accessibility: Execute and verify the processes that identify and enhance the Accessibility of a business data domain's data as per defined scope
Data Stewardship Council
Actively participate in, and contribute to, the Data Stewardship Council.
Basic Qualifications
Education:
Bachelor's degree in computer science, information systems, business or related field of study; or associate degree in computer science or related field of study with 4 years of relevant work experience; or High school diploma/GED with 6 years of relevant work experience
Qualified Experience Includes
Data centric operations, experience with a good working knowledge of data governance practice within the utility industry.
Experience should be in defining and execution of data quality standards and associated business rules and data analysis.
Intermediate understanding of database structures.
Proficiency in writing queries in SQL, Oracle, SAS, .NET, and python to research and analyze data is preferred.
Experience with data cataloging and data analytics projects encouraged.
Knowledge and understanding of program automation / process streamlining utilizing .NET, Python, Matlab, or SAS is encouraged.
Additional Requirements
Lead the efforts, with little to no guidance, to troubleshoot complex issues that leverages strong analytical thinking and problem-solving skills.
Define and implement processes for data stewards to maintain metadata for a business data domain's data.
Use excellent oral and written communication in a clear, organized, and timely manner. Obtain and share information related to current issues that may influence departmental and corpo*** operations.
Demonst*** a positive attitude in the face of change. Work well in a changing environment, able to move forward in the presence of some degree of uncertainty.
Actively seek to understand the business, understand the business environment in which we operate. Be knowledgeable about the company, essential functions, and industry trends relative to your job.
Produces processes that certify that data is fit for intended business uses in operations, decision making and planning.
Demonst*** ability to manage time effectively and efficiently
Show capability of having organizational and planning skills, as well as having an aptitude for accuracy, attention to detail and ability to achieve goals
Team player with ability to work and collabo*** well with others. Ability to interface and work well with different levels of individual contributors and management is required
Solid data management, quality assurance and data flow skills
Proficient in writing SQL queries to research and analyze data
Intermediate understanding of database structures
Intermediate quantitative and analytical skills
Experience with data cataloging and data analytics projects preferred
Ability to create basic visual aids using the tools and applications available for a visual representation of data
Experience with data governance tools such as IBM Cloud Pak is preferred
Able to develop and present business documents, reports, and training concisely and effectively. Able to read the reactions of an audience and adjust the delivery style to fit the audience
Indotronix Commitment
: A Safe and Inclusive Workplace"" – Promoting a Culture of Inclusion, Respect, Equality, and Diversity: Ensuring Safety and Non-Discrimination.
We actively strive to attract, retain, and empower a diverse range of talented individuals, recognizing that diverse perspectives and experiences enhance our collective performance.
Breaking Barriers
: Your Potential Knows No Limits. Embrace Your Potential, Apply Today!""
Celebrating & Honouring Veteran Contributions:
Approximately 13% of our workforce are veterans (nearly twice the national average). This achievement underscores our deep commitment in fostering Opportunities for success to Civilian Careers
Recognition
- Indotronix has been recognized as one of the largest staffing companies in 2023 by Staffing Industry Analysts – a testament to our continued growth, commitment to excellence, and the trust our clients and candidates place in us.
Compliance
Indotronix upholds good corpo*** citizenship by complying with all applicable laws, including taxation, equal employment opportunity, statutory benefits, and data reporting. In 2022, we hired over 2,000 U.S.-based employees as consultants, contributing to workforce expansion and client service excellence.
Show more
Show less","Data Governance, Data Quality, SQL, Oracle, SAS, .NET, Python, Matlab, IBM Cloud Pak, Data Cataloging, Data Analytics, Database Structures, Data Stewardship","data governance, data quality, sql, oracle, sas, net, python, matlab, ibm cloud pak, data cataloging, data analytics, database structures, data stewardship","data cataloging, data governance, data quality, data stewardship, dataanalytics, database structures, ibm cloud pak, matlab, net, oracle, python, sas, sql"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Columbus, OH",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783191096,2023-12-17,Norwich,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Columbus-DataResearchAn.016
Show more
Show less","Generative AI, Python, JavaScript, JSON, Research, Data Science, Product Development, ObjectOriented Programming, OOP","generative ai, python, javascript, json, research, data science, product development, objectoriented programming, oop","data science, generative ai, javascript, json, objectoriented programming, oop, product development, python, research"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Columbus, OH",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783188499,2023-12-17,Norwich,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Columbus-DataScientist.003
Show more
Show less","Python, JavaScript, JSON, OOP, Generative AI, Data Analytics, Data Science, R Programming, Programming, Machine Learning, Natural Language Processing, Algorithm, Artificial Intelligence, Coaching, Machine Learning Model, Remote work, English communication, Communication","python, javascript, json, oop, generative ai, data analytics, data science, r programming, programming, machine learning, natural language processing, algorithm, artificial intelligence, coaching, machine learning model, remote work, english communication, communication","algorithm, artificial intelligence, coaching, communication, data science, dataanalytics, english communication, generative ai, javascript, json, machine learning, machine learning model, natural language processing, oop, programming, python, r programming, remote work"
Data Engineer,NR Consulting,"Columbus, OH",https://www.linkedin.com/jobs/view/data-engineer-at-nr-consulting-3768019176,2023-12-17,Norwich,United States,Mid senior,Onsite,"Job Requirements:
Responsibilities
The job function of a Java Spark Databricks AWS data engineer involves working with data engineering technologies and platforms to design, develop, and maintain data solutions on the AWS cloud platform. Here are some key responsibilities and tasks associated with this role:
Data Ingestion: Develop and implement processes to extract data from various sources, such as databases, APIs, and files, and load it into the data lake or data warehouse using Java, Spark, and AWS tools.
Data Transformation: Perform data cleansing, validation, and transformation using Spark and Java programming, ensuring data quality and consistency. Apply business rules and data processing techniques to prepare the data for analysis and consumption.
Data Pipeline Development: Design and build scalable data pipelines using AWS services like AWS Glue, AWS Data Pipeline, or Apache Airflow. Develop ETL (Extract, Transform, Load) processes to move and transform data between different systems and data stores.
Data Modeling: Create and maintain data models and schemas, including dimensional and relational models, to support data storage and retrieval requirements. Optimize data structures for performance and efficiency.
Performance Optimization: Fine-tune Spark applications and data processing workflows to improve performance and reduce processing time. Optimize resource utilization, data partitioning, and data caching strategies.
Data Security and Governance: Implement data security and access controls to ensure data privacy and compliance with regulatory requirements. Apply data governance practices to manage metadata, data lineage, and data cataloging.
Monitoring and Troubleshooting: Monitor data pipelines and Spark jobs for performance, errors, and issues. Troubleshoot and resolve data-related problems, such as data quality issues or performance bottlenecks.
Collaboration and Documentation: Collaborate with cross-functional teams, including data scientists, analysts, and business stakeholders, to understand data requirements and deliver data solutions. Document data pipelines, processes, and system configurations.
Cloud Infrastructure Management: Configure and manage AWS services like Amazon EMR (Elastic MapReduce), Amazon S3 (Simple Storage Service), and AWS Glue for data processing, storage, and management. Monitor and optimize cloud resources for cost efficiency.
Continuous Improvement: Stay updated with emerging technologies, industry trends, and best practices related to data engineering and cloud computing. Continuously enhance skills and knowledge to improve data engineering processes and solutions.
Show more
Show less","Java, Spark, Databricks, AWS, Data Ingestion, Data Transformation, Data Pipeline Development, Data Modeling, Performance Optimization, Data Security, Data Governance, Monitoring, Troubleshooting, Collaboration, Documentation, Cloud Infrastructure Management, Apache Airflow, AWS Glue, AWS Data Pipeline, ETL, Amazon EMR, Amazon S3","java, spark, databricks, aws, data ingestion, data transformation, data pipeline development, data modeling, performance optimization, data security, data governance, monitoring, troubleshooting, collaboration, documentation, cloud infrastructure management, apache airflow, aws glue, aws data pipeline, etl, amazon emr, amazon s3","amazon emr, amazon s3, apache airflow, aws, aws data pipeline, aws glue, cloud infrastructure management, collaboration, data governance, data ingestion, data pipeline development, data security, data transformation, databricks, datamodeling, documentation, etl, java, monitoring, performance optimization, spark, troubleshooting"
Sr. Database engineer - SQL/SAAS,"TekVivid, Inc","Columbus, OH",https://www.linkedin.com/jobs/view/sr-database-engineer-sql-saas-at-tekvivid-inc-3701081345,2023-12-17,Norwich,United States,Mid senior,Onsite,"Position: Sr. Database engineer - SQL/SAAS -Onsite
Location: Columbus, OH
Visa Accepted: H4EAD, L2EAD, J2EAD, GCEAD, GC, USC
JOB DESCRIPTION
Modifies existing software/application programs, which are typically more complex in nature, or writes new programs to support user and management needs. Designs, tests, debugs, documents, and implements those programs. Consults with users to design, modify, and explain program changes or to provide technical support. Resolves problems which occur in production systems. May serve as project leader in the development of automated systems or procedures. Provides direction and training to other team members.
Must Haves:
3+ years of experience with SQL server databases including strong knowledge in Stored Procedures, SSIS, and writing and tuning of queries
Knowledge of different SDLC methodologies including Waterfall, Agile, etc.
Demonstrate excellent communication skills
Demonstrate strong interpersonal skills, focus on customer service and the ability to work well independently or in teams
SQL Server close
Vendor Application Support - Releases, Patches, and Incident Support close
Windows Server
Experience with the following enterprise patterns\technologies\tools
Scheduling Tools and Code Management software.
Wants/Nice to Have:
Linux
XML
Zena
Thanks!
Vinodh Vuppalapati
|
Senior US IT Recruiter
E-Mail:
vinod@tekvividinc.com
Phone: 972-734-5182, Ext:-426
Show more
Show less","SQL, Stored Procedures, SSIS, Waterfall, Agile, Windows Server, Scheduling Tools, Code Management software, Linux, XML","sql, stored procedures, ssis, waterfall, agile, windows server, scheduling tools, code management software, linux, xml","agile, code management software, linux, scheduling tools, sql, ssis, stored procedures, waterfall, windows server, xml"
Senior Data Engineer/Lead,Cognizant,"Columbus, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-lead-at-cognizant-3782568714,2023-12-17,Norwich,United States,Mid senior,Onsite,"This in-person position is
open to any qualified applicant in the United States.
A qualified candidate must be local to Columbus, Ohio
,
or be open to relocation to this area.
Please note, this role is not able to offer visa transfer or sponsorship now or in the future*
Practice - AIA - Artificial Intelligence and Analytics
About AI & Analytics:
Artificial intelligence (AI) and the data it collects and analyzes will soon sit at the core of all intelligent, human-centric businesses. By decoding customer needs, preferences, and behaviors, our clients can understand exactly what services, products, and experiences their consumers need. Within AI & Analytics, we work to design the future—a future in which trial-and-error business decisions have been replaced by informed choices and data-supported strategies!
By applying AI and data science, we help leading companies to prototype, refine, validate, and scale their AI and analytics products and delivery models! Cognizant’s AIA practice takes insights that are buried in data and provides businesses with a clear way to transform how they source, interpret, and consume their information. Our clients need flexible data structures and a streamlined data architecture that quickly turns data resources into informative, meaningful intelligence.
Job Duties And Responsibilities
Contributes to technological decisions for business teams' future data and analysis needs
Support the business's daily operations inclusive troubleshooting of the business's data warehouse environment and job monitoring
Guide the business in identifying any new data needs and provide mechanisms for acquiring and reporting such information and addressing the actual needs
Brings together and maintains best practices that can be adopted in Delta Lake stacking and sharing across the business
Provides mentorship to the business in data analysis reporting data warehousing and business intelligence
Participate in efforts to design build and develop rapid Proof-of-Concept (POC) solutions and services
Work as a data engineer within the team that uses several Data Search and AWS technologies
Contribute to the exploration and understanding of new tools and techniques and propose improvements to the data pipeline
Build data pipelines using AWS services
Be a key team member in the design and development of the Marketing product team
Apply knowledge of basic principles methods and practices to simple and moderately sophisticated assignments
Proactively identify and implement opportunities to automate tasks and develop reusable frameworks
Adhere to standard methodologies for coding testing and designing reusable code/component
Participate in sprint planning meetings and provide estimations on technical implementation
Implement standardized automated operational and quality control processes to deliver accurate and timely data and reporting to meet or exceed SLAs
Collaborate with the other engineering team members to ensure all services are reliable maintainable and well-integrated into existing platforms
Review functional and technical designs to identify areas of risk and/or missing requirements
Position Qualifications
At least 8 years of work experience
Required skills: AWS Databricks
Working understanding of Agile Scrum Design Thinking and Lean Startup principles
Salary And Other Compensation
This position is also eligible for Cognizant’s discretionary annual incentive program and stock awards, based on performance and is subject to the terms of Cognizant’s applicable plans.
Benefits
: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:
Medical/Dental/Vision/Life Insurance
Paid holidays plus Paid Time Off
401(k) plan and contributions
Long-term/Short-term Disability
Paid Parental Leave
Employee Stock Purchase Plan
Disclaimer:
The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.
#CB #Ind123
Employee Status : Full Time Employee
Shift : Day Job
Travel : No
Job Posting : Dec 08 2023
About Cognizant
Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.
Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.
Show more
Show less","AWS, Databricks, Agile, Scrum, Design Thinking, Lean Startup, Delta Lake","aws, databricks, agile, scrum, design thinking, lean startup, delta lake","agile, aws, databricks, delta lake, design thinking, lean startup, scrum"
Staff Data Engineer,Recruiting from Scratch,"Columbus, OH",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744389931,2023-12-17,Norwich,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Kafka, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Storm, SparkStreaming, Data Modeling, Data Warehouses, ETL, Legal Compliance, Data Management Tools, Data Classification, Retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, tdd, pair programming, continuous integration, automated testing, deployment, storm, sparkstreaming, data modeling, data warehouses, etl, legal compliance, data management tools, data classification, retention","airflow, automated testing, continuous integration, data classification, data management tools, data warehouses, datamodeling, deployment, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, retention, snowflake, spark, sparkstreaming, storm, tdd"
Business Data Analyst I,DHL Supply Chain,"Lockbourne, OH",https://www.linkedin.com/jobs/view/business-data-analyst-i-at-dhl-supply-chain-3733086679,2023-12-17,Norwich,United States,Mid senior,Onsite,"Are you a passionate leader looking for autonomy and exciting career possibilities?Do you take an energetic and resourceful approach to problem-solving while bringing innovative ideas and analytics to life on behalf of your team and your customers?Do you enjoy effectively translating requirements into an efficient process and/or system solution? If so, DHL Supply Chain has the opportunity for you.
Job Description
To apply knowledge and analytics to develop and communicate timely, accurate, and actionable insight to the business through the use of modeling, visualization, and optimization. Responsible for the reporting, analyzing, and predicting of operational processes, performance, and Key Performance Indicators. Communication with site leadership, operations, and finance on efficiency, customer requirements, account specific issues, and insight into to the business, operations, and customer.
Applies hindsight, insight, and foresight techniques to communicate complex findings and recommendations to influence others to take action
Uses knowledge of business and data structure to discover and/or anticipate problems where data can be used to solve the problem
Uses spreadsheets, databases, and relevant software to provide ongoing analysis of operational activities
Applies data visualization for discovery and timely insights to decrease Cycle Time to Action (CTA)
Assists site operations in identifying areas for improving service levels, reducing operational costs, and providing other operational enhancements
Supports account start-up analysis and/or report implementation as needed
Develop standardized and ad hoc site and/or customer reporting
Streamlines and/or automates internal and external reporting
May investigate and recommend new technologies and information systems
May conduct feasibility analyses on various processes and equipment to increase efficiency of operations
Partners with Finance to develop financial models to analyze productivity and payroll; calculates cost benefits and business impact and proposes solutions
Develops predictive models to help drive decision making
Designs, develops, and implements data gathering and reporting methods and procedures for Operations
Responsible for tracking, planning, analysis, and forecasting of storage capacities, inventory levels, equipment and/or labor requirements
Coordinates with Operations Systems group to ensure technical issues and problems are being identified, addressed, and resolved in a timely manner
May coordinate with ILD group on issues related to modeling customer solutions, including providing data and relevant insight for customer pursuits
Responsible for assisting finance and senior leadership in modeling yearly labor budget based on operational and profile changes
Required Education And Experience
Undergraduate degree in business, logistics, mathematics, statistics, related field, or equivalent experience, required
1+ years of analytics experience, required
Our Organization has a business casual environment and focuses on teamwork, associate development, training, and continuous improvement. We offer competitive wages, excellent affordable insurance benefits (including health, dental, vision and life), 401K plan, paid vacation and holidays.
Our Organization is an equal opportunity employer.
Show more
Show less","Data Analytics, Data Visualization, Optimization, Reporting, Spreadsheets, Databases, Financial Modeling, Predictive Analytics, Data Gathering, Data Forecasting, Storage Capacity Management, Inventory Management, Equipment Management, Labor Requirement Forecasting, Technical Problem Solving","data analytics, data visualization, optimization, reporting, spreadsheets, databases, financial modeling, predictive analytics, data gathering, data forecasting, storage capacity management, inventory management, equipment management, labor requirement forecasting, technical problem solving","data forecasting, data gathering, dataanalytics, databases, equipment management, financial modeling, inventory management, labor requirement forecasting, optimization, predictive analytics, reporting, spreadsheets, storage capacity management, technical problem solving, visualization"
Data Center Architect,Ascendion,"Columbus, OH",https://www.linkedin.com/jobs/view/data-center-architect-at-ascendion-3774818583,2023-12-17,Norwich,United States,Mid senior,Remote,"About Ascendion
Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next.
Ascendion | Engineering to elevate life
We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:
Build the coolest tech for world’s leading brands
Solve complex problems – and learn new skills
Experience the power of transforming digital engineering for Fortune 500 clients
Master your craft with leading training programs and hands-on experience
Experience a community of change makers!
Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.
About the Role:
Job Title: AWS Data Center Project Architect
Daily Roles & Responsibilities:-
Travel for site assessments, internal design meetings, construction review, and interfacing with design consultants. Anticipated travel not to exceed 25%.
Communicate conceptual designs and create/maintain project documentation before, during, and after construction.
Maintenance of Basis of Design, prototype design, and template specifications for architectural elements.
Review and inform design RFPs.
Manage our external design consultants through the design and construction process.
Coordinate with internal and external Civil, Structural, Mechanical, Electrical, Controls, Cabling, and Security design engineers.
Effectively communicate design standards to internal and external project partners.
Manage multiple fast paced projects simultaneously.
Think outside of the box to find innovative solutions prior to and during the construction process to reduce costs without negative impacts on quality or reliability.
Requirements:-
Excellent communication skills and attention to detail.
NCARB recognized architecture license.
6+ years of design experience in commercial / industrial / other complex technical projects.
1+ years of Data Center or Mission Critical facility design experience.
1+ years leading sub-consultants and project teams.
Proficiency in building codes, regulations, and standards including IBC or equivalent.
Salary Range: The salary for this position is between $160,000 – $170,000 annually. Factors which may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.
Benefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertains to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]
Want to change the world? Let us know.
Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!
Show more
Show less","AWS, Data Center, Project Architect, NCARB architecture license, Commercial design, Industrial design, Complex technical projects, Data Center design, Mission Critical facility design, Subconsultant leadership, Project team leadership, Building codes, Regulations, Standards, IBC","aws, data center, project architect, ncarb architecture license, commercial design, industrial design, complex technical projects, data center design, mission critical facility design, subconsultant leadership, project team leadership, building codes, regulations, standards, ibc","aws, building codes, commercial design, complex technical projects, data center, data center design, ibc, industrial design, mission critical facility design, ncarb architecture license, project architect, project team leadership, regulations, standards, subconsultant leadership"
Senior Data Engineer,CoverMyMeds,"Columbus, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-at-covermymeds-3774889030,2023-12-17,Norwich,United States,Mid senior,Hybrid,"Current Need
We are looking for a
Sr. Data Engineer
to join the CoverMyMeds Data Warehouse team. This is a critical role focused on building high value data products and reducing existing technical debt.
Position Description
The primary role of the Sr. Data Engineer is to support and expand the data platforms that process, store, organize the data critical for the Data and Analytics team. This role will participate in the technical strategy and execution to provide trusted, stable, reliable, responsive, and secure solutions and proactively inform business partners on Data & Analytics platform and product health, and problem resolution. The Sr. Data Engineer will work collaboratively with our Data Systems Analysts as well as our Analytics and Technology partners to solve business problems and deliver solutions.
Key Responsibilities
Design and develop solutions integrating complex data across disparate systems for CMM customers.
Work with databases, files and unstructured data to identify, transport and quality test the data required to drive our data synchronization tasks to perform daily and incremental loads of data.
Develop data solutions in SQL and Cloud Data Platforms such as Snowflake or Databricks to cleanse, apply business logic and standardize the data according to business rules, enabling more effective data governance along with clear and efficient end-user reporting.
Design conceptual data models based on business reporting requirements, interacting with business partners to understand the business logic and end-use of the data.
Work with the application development teams to determine data flow in the source system and architect an appropriate flow into the data warehouse.
Contribute to the technical creation, architecture, design, and rollout of innovative tech, guiding operational excellence and business impact for CoverMyMeds and clients alike.
Work closely with peers and data warehouse analysts to support the development and implementation of innovative solutions and tools.
Contribute to the development of high-performance applications through writing efficient, testable, and reusable code.
Prototype and iterate on potential solutions, determining those with the most viability and impact.
Minimum Requirements.
Typically has 7+ years of relevant experience
Demonstrated expertise of database design and modeling.
Expert knowledge of BI Reporting and Data Discovery tools
Expert knowledge of Cloud technologies
Experience with business-critical applications.
Experience on large-scale implementation programs preferred.
Additional knowledge & Skills:
Excellent written and verbal communication skills; timely communication with clear expectations. An active listener and clear communicator; can lead by influence
Ability to find creative solutions to complex problems
Highly adept at working collaboratively across multiple business and technical functions to achieve results
Education
4-year degree in computer science or related field or equivalent experience
Candidates should be authorized to work in USA. Sponsorship is not available for this role.
At CoverMyMeds, we care about the well-being of the patients and communities we serve, and that starts with caring for our people. That’s why we have a Total Rewards package that includes comprehensive benefits to support physical, mental, and financial well-being. Our Total Rewards offerings serve the different needs of our diverse employee population and ensure they are the healthiest versions of themselves. For more information regarding benefits at CoverMyMeds, please click here.
As part of Total Rewards, we are proud to offer a competitive compensation package at CoverMyMeds. This is determined by several factors, including performance, experience and skills, equity, regular job market evaluations, and geographical markets. In addition to base pay, other compensation, such as an annual bonus or long-term incentive opportunities may be offered.
Our Base Pay Range for this position
$119,100 - $198,500
CoverMyMeds is an equal opportunity and affirmative action employer. We embrace diversity and are committed to creating an inclusive environment for all employees. Qualified applicants will be considered for employment without regard to race, religion, gender, gender identity, sexual orientation, national origin, age, disability or veteran status.
Show more
Show less","Data Warehouse, Data Platforms, SQL, Snowflake, Databricks, Cloud Technologies, Database Design, Data Modeling, BI Reporting, Data Discovery, Business Intelligence, Agile, Scalable, Data Governance, Data Quality, Data Synchronization, Data Integration, Data Architecture, Data Engineering, Business Logic, Problem Solving, Communication, Teamwork, Collaboration","data warehouse, data platforms, sql, snowflake, databricks, cloud technologies, database design, data modeling, bi reporting, data discovery, business intelligence, agile, scalable, data governance, data quality, data synchronization, data integration, data architecture, data engineering, business logic, problem solving, communication, teamwork, collaboration","agile, bi reporting, business intelligence, business logic, cloud technologies, collaboration, communication, data architecture, data discovery, data engineering, data governance, data integration, data platforms, data quality, data synchronization, database design, databricks, datamodeling, datawarehouse, problem solving, scalable, snowflake, sql, teamwork"
Senior Data Engineer,McKesson,"Columbus, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-at-mckesson-3780472546,2023-12-17,Norwich,United States,Mid senior,Hybrid,"Current Need
We are looking for a
Sr. Data Engineer
to join the CoverMyMeds Data Warehouse team. This is a critical role focused on building high value data products and reducing existing technical debt.
Position Description
The primary role of the Sr. Data Engineer is to support and expand the data platforms that process, store, organize the data critical for the Data and Analytics team. This role will participate in the technical strategy and execution to provide trusted, stable, reliable, responsive, and secure solutions and proactively inform business partners on Data & Analytics platform and product health, and problem resolution. The Sr. Data Engineer will work collaboratively with our Data Systems Analysts as well as our Analytics and Technology partners to solve business problems and deliver solutions.
Key Responsibilities
Design and develop solutions integrating complex data across disparate systems for CMM customers.
Work with databases, files and unstructured data to identify, transport and quality test the data required to drive our data synchronization tasks to perform daily and incremental loads of data.
Develop data solutions in SQL and Cloud Data Platforms such as Snowflake or Databricks to cleanse, apply business logic and standardize the data according to business rules, enabling more effective data governance along with clear and efficient end-user reporting.
Design conceptual data models based on business reporting requirements, interacting with business partners to understand the business logic and end-use of the data.
Work with the application development teams to determine data flow in the source system and architect an appropriate flow into the data warehouse.
Contribute to the technical creation, architecture, design, and rollout of innovative tech, guiding operational excellence and business impact for CoverMyMeds and clients alike.
Work closely with peers and data warehouse analysts to support the development and implementation of innovative solutions and tools.
Contribute to the development of high-performance applications through writing efficient, testable, and reusable code.
Prototype and iterate on potential solutions, determining those with the most viability and impact.
Minimum Requirements.
Typically has 7+ years of relevant experience
Demonstrated expertise of database design and modeling.
Expert knowledge of BI Reporting and Data Discovery tools
Expert knowledge of Cloud technologies
Experience with business-critical applications.
Experience on large-scale implementation programs preferred.
Additional knowledge & Skills:
Excellent written and verbal communication skills; timely communication with clear expectations. An active listener and clear communicator; can lead by influence
Ability to find creative solutions to complex problems
Highly adept at working collaboratively across multiple business and technical functions to achieve results
Education
4-year degree in computer science or related field or equivalent experience
Candidates should be authorized to work in USA. Sponsorship is not available for this role.
At CoverMyMeds, we care about the well-being of the patients and communities we serve, and that starts with caring for our people. That’s why we have a Total Rewards package that includes comprehensive benefits to support physical, mental, and financial well-being. Our Total Rewards offerings serve the different needs of our diverse employee population and ensure they are the healthiest versions of themselves. For more information regarding benefits at CoverMyMeds, please click here.
As part of Total Rewards, we are proud to offer a competitive compensation package at CoverMyMeds. This is determined by several factors, including performance, experience and skills, equity, regular job market evaluations, and geographical markets. In addition to base pay, other compensation, such as an annual bonus or long-term incentive opportunities may be offered.
Our Base Pay Range for this position
$119,100 - $198,500
CoverMyMeds is an equal opportunity and affirmative action employer. We embrace diversity and are committed to creating an inclusive environment for all employees. Qualified applicants will be considered for employment without regard to race, religion, gender, gender identity, sexual orientation, national origin, age, disability or veteran status.
Show more
Show less","Data Engineering, SQL, Snowflake, Databricks, Data Modeling, Business Intelligence Reporting, Data Discovery Tools, Cloud Technologies, Big Data, Data Warehousing, Data Governance, Data Analysis, Data Architecture, Data Integration, Data Quality, Data Migration, Data Transformation, Data Visualization, Data Security, Data Privacy, Agile Development, Software Development, Communication Skills, Teamwork, Problem Solving, Analytical Skills, Creativity, Attention to Detail, Computer Science, SQL","data engineering, sql, snowflake, databricks, data modeling, business intelligence reporting, data discovery tools, cloud technologies, big data, data warehousing, data governance, data analysis, data architecture, data integration, data quality, data migration, data transformation, data visualization, data security, data privacy, agile development, software development, communication skills, teamwork, problem solving, analytical skills, creativity, attention to detail, computer science, sql","agile development, analytical skills, attention to detail, big data, business intelligence reporting, cloud technologies, communication skills, computer science, creativity, data architecture, data discovery tools, data engineering, data governance, data integration, data migration, data privacy, data quality, data security, data transformation, dataanalytics, databricks, datamodeling, datawarehouse, problem solving, snowflake, software development, sql, teamwork, visualization"
Senior Cloud Data Engineer,BDO USA,"Columbus, OH",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765470282,2023-12-17,Norwich,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, C#, Python, Java, Scala, Tabular Modeling, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch and/or Streaming Data Ingestion, AI Algorithms/Machine Learning, Automation Tools, Computer Vision, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, c, python, java, scala, tabular modeling, git, devops, linux, data lake medallion architecture, batch andor streaming data ingestion, ai algorithmsmachine learning, automation tools, computer vision, sql, ssis, ssas, ssrs, pyspark, microsoft fabric, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql","ai algorithmsmachine learning, automation tools, batch andor streaming data ingestion, bicep, business intelligence, c, cloud data analytics, computer vision, data lake medallion architecture, data ops, dataanalytics, datamodeling, datawarehouse, dbt, delta, devops, git, java, linux, microsoft fabric, pandas, purview, python, scala, semantic model definition, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, tabular modeling, terraform"
Lead Data Analyst,Vernovis,"Columbus, Ohio Metropolitan Area",https://www.linkedin.com/jobs/view/lead-data-analyst-at-vernovis-3764337145,2023-12-17,Norwich,United States,Mid senior,Hybrid,"Job Title:
Lead Data Analyst
Location/ Work Structure
: Hybrid Onsite – Columbus, Ohio
Come join us:
Vernovis is a Total Talent Solutions company that specializes in Technology, Cybersecurity, Finance & Accounting functions. Our consultative and flexible approach to providing talent, to fill gaps and for project work, offers a variety of ways to engage, to meet your needs, and ensure you achieve your business objectives.
At Vernovis, we help IT, cybersecurity, accounting, and finance professionals achieve their career goals, matching them with innovative projects and dynamic direct hire opportunities in Ohio and across the Midwest.
Vernovis is currently partnering with a regional, multi-billion dollar company to place a Lead Data Analyst. This person will work closely with a variety of stakeholder groups, while scoping and guiding work for the enterprise data ingestion team.
What You'll Do:
Focus on, scope, monitor, and drive all data ingestion activities.
Work closely with Data Engineering team to focus on data ingestion
Partner with a variety of stakeholder groups to understand and execute their data needs
Partner with Architecture resources to create alignment between frameworks/architecture and the work being done by the engineers
Assist in the ingestion from a variety of sources (including SAP), into the enterprise Azure Data Lake
What Experience You'll Have:
3+ years of enterprise IT/Data experience
Experience working with Data Engineering and/or Analytics with a focus on ETL and/or Data ingestion from various source systems.
Experience working in Azure Data Lake and proficiency with Microsoft ADF preferred.
Experience leading / facilitating / planning work as a product owner or data lead preferred.
Show more
Show less","Data Analytics, Data Engineering, ETL, Data Ingestion, Azure Data Lake, Microsoft ADF","data analytics, data engineering, etl, data ingestion, azure data lake, microsoft adf","azure data lake, data engineering, data ingestion, dataanalytics, etl, microsoft adf"
Full Time Lead Data Integration Engineer/Architect,Jefferson Frank,"Columbus, OH",https://www.linkedin.com/jobs/view/full-time-lead-data-integration-engineer-architect-at-jefferson-frank-3738704719,2023-12-17,Norwich,United States,Mid senior,Hybrid,"Our TOP CLIENT has retained us for a Lead Data Integration Engineer/Architect. As the Lead Data Integration Engineer/Architect you will be part of a team responsible for delivering cloud data management solutions. You would be supporting solution architects, business analysts, and data engineers on system implementations and ensuring optimal data delivery. This role will lead a team of onshore and offshore/nearshore engineers responsible for building integrations and other required automation. In this role, you would Architect, design, develop, and implement small to large-scale integration solutions in the MuleSoft platform based on functional and technical requirements along with performing code reviews based on high engineering standards and writing unit and integration tests based on chosen DevOps frameworks. If you have a desire to work as part of a growing, fast-paced, and highly flexible team our client would love to speak with you!!!
Top Requirements
Up to 20% travel
Must be located in Texas, Iowa, Michigan, or Ohio
Experience creating and maintaining domain diagrams, architecture frameworks, design patterns and standards to support various work streams
Strong experience in the Application Integration Architecture, API and Microservices architecture, Solution Design, Development using SOA/EAI solutions, API Led Architectures, creation of API design specifications, and RAML creation
Experience integrating with Cloud/SaaS applications, APIs, SDK of packaged applications and legacy Ideally have Salesforce, MS Dynamics, and Data warehouse integration experience
Hands on experience on MuleSoft's CloudHub, DataWeave, Anypoint MQ and deploying/managing Mule flows to CloudHub
Experience setting up and configuring on-premise/cloud-based infrastructures
Experience in implementing security aspects including API security, authentication, authorization, message & transport level security
Experience in API Management tools using MuleSoft API Manager or others
Well versed in configuring VPC and dedicated load balancer on Anypoint platform
Good knowledge on DevOps stack (CI & CD) and other dependency management and build tools
Experience with High-Availability, Fault-Tolerance, Performance Testing and Tuning parameters
Well versed with agile methodologies and source control (Bitbucket, GitHub, ADO)
A desire to work as part of a growing, fast-paced, and highly flexible team
RESPOSIBILITIES
Leads one or more team members consisting of cross functional, global, and virtual groups; may need to supervise staff and assign responsibility to other team members
Architects, designs, develops, and implements small to large scale integration solutions in MuleSoft platform based on functional and technical requirements
Develops, evaluates, and influences effective and consistent productivity and teamwork to ensure the delivery of Legendary Customer Service (LCS)
Defines systems integrations, design patterns and development standards to support cross-functional, multi-system solutions that are scalable and flexible to meet current and future needs of the organization
Creates architectural deliverables that clearly communicate design and solution
Designs and develops automated solutions in accordance with MuleSoft and enterprise leading practices and design principles
Performs thorough code-reviews based on high engineering standards and writes unit and integration tests based on chosen DevOps frameworks
Provides guidance to junior resources on best practices and development techniques for automated processes
Works with Business Development Manager (BDM) (Salesperson) to identify new opportunities
Acts as trusted advisor and expert on MuleSoft platform promoting security and performance
Desired Experience
Experience in delivering enterprise complex systems integrations and intelligent automations required
Demonstrated hands-on experience with ESB platforms such as Talend, Workato, Boomi, MuleSoft, Informatica or similar products required
Strong working experience with SQL/PLSQL and relational databases such as Oracle, MS SQL Server, and NoSQL databases required
Established enterprise integration infrastructure, supporting ESB, messaging and SLA monitoring tools required
Experience with messaging infrastructure, preferably Azure Service Bus and with Storage like Azure Blobs or Data Lake preferred
Experience with ETL and Web Services based integrations with expert level knowledge of developing APIs using SOAP and REST architecture styles and data interchange formats like XML, JSON, etc. required
Preferred Certifications
Experience working in an Agile environment preferred
Active MuleSoft, Salesforce or Azure
MuleSoft Certified Developer and MuleSoft Certified Integration Architect
Benefits
Generous 401K match
Excellent benefits package
PTO/Holiday plan
Reach out to me directly if interested
E: s.murray@jeffersonfrank.com
P: 813-437-6913
Show more
Show less","Data Integration, Cloud Data Management, MuleSoft, DataWeave, Anypoint MQ, VPC, API Manager, Bitbucket, GitHub, ADO, Talend, Workato, Boomi, Informatica, SQL, PLSQL, Oracle, MS SQL Server, NoSQL, Azure Service Bus, Azure Blobs, Data Lake, ETL, SOAP, REST, XML, JSON","data integration, cloud data management, mulesoft, dataweave, anypoint mq, vpc, api manager, bitbucket, github, ado, talend, workato, boomi, informatica, sql, plsql, oracle, ms sql server, nosql, azure service bus, azure blobs, data lake, etl, soap, rest, xml, json","ado, anypoint mq, api manager, azure blobs, azure service bus, bitbucket, boomi, cloud data management, data integration, data lake, dataweave, etl, github, informatica, json, ms sql server, mulesoft, nosql, oracle, plsql, rest, soap, sql, talend, vpc, workato, xml"
Senior Data Analyst,SysMind,"Columbus, OH",https://www.linkedin.com/jobs/view/senior-data-analyst-at-sysmind-3782812938,2023-12-17,Norwich,United States,Mid senior,Hybrid,"8+ years of Data Analyst experience.
At least 3-4 years of previous work experience in the BFS domain (Risk operations experience preferred).
Self-motivated professional with ability to collaborate with the stakeholders through his/her interpersonal skills.
Should have excellent knowledge of MS Excel, MS Word and Visio.
Associate should be process oriented and must have an analytical mind.
Show more
Show less","Data Analysis, BFS Domain, MS Excel, MS Word, Visio","data analysis, bfs domain, ms excel, ms word, visio","bfs domain, dataanalytics, ms excel, ms word, visio"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Columbus, OH",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707752,2023-12-17,Norwich,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML/DL, Pandas, R, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Legal Compliance, Data Management Tools, Data Classification, Data Retention","data engineering, mldl, pandas, r, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, legal compliance, data management tools, data classification, data retention","airflow, applied machine learning, aws, azure, bash, data classification, data engineering, data management tools, data retention, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, legal compliance, mldl, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Columbus, OH",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773088675,2023-12-17,Norwich,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Pandas, R, Git, Airflow, KubeFlow, Kubernetes, Docker, Helm, Spark, PySpark, Snowflake, DynamoDB, Kafka, Storm, SparkStreaming, SQL, ETL, Data Engineering, Machine Learning, NLP, Natural Language Processing, LLMs","python, java, bash, sql, pandas, r, git, airflow, kubeflow, kubernetes, docker, helm, spark, pyspark, snowflake, dynamodb, kafka, storm, sparkstreaming, sql, etl, data engineering, machine learning, nlp, natural language processing, llms","airflow, bash, data engineering, docker, dynamodb, etl, git, helm, java, kafka, kubeflow, kubernetes, llms, machine learning, natural language processing, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Lead Customer & Agent Experience Data Analyst (hybrid/remote opportunity),Grange Insurance,"Columbus, OH",https://www.linkedin.com/jobs/view/lead-customer-agent-experience-data-analyst-hybrid-remote-opportunity-at-grange-insurance-3754667105,2023-12-17,Norwich,United States,Mid senior,Hybrid,"Summary:
This position is responsible for partnering with the key stakeholders across business units to identify opportunities for the success of digital, operational, and experience programs by generating business insights. Gathering and maintaining knowledge in a wide array of business areas, ultimately all relating back to how customers and agents interact with Grange and Integrity. Responsible for accelerating the pace of digital innovation at Grange Insurance with high-impact actionable insight generation through analytics. Providing answers to complex analytical questions using a wide array of tools and techniques. Building solutions and processes that turn disparate and sometimes messy data into interpretable, valuable, and meaningful insights. Turning data into fact-based conclusions and delivering the result to stakeholders in a way that is best suited to their needs to help drive data-driven decision-making. Partnering with the data warehouse, data science, business intelligence, dev ops, and data governance teams to inform improvements and requirements for how data is collected, stored, and accessed by analytics teams.
What You’ll Be Doing:
Lead collaboration with cross-functional partners to understand their business needs. Formulate and complete end-to-end analysis that includes data gathering, analysis, and ongoing scaled deliverables. Deliver effective presentations of findings and recommendations to multiple levels of leadership, creative visual displays of quantitative information.
Lead gathering and synthesizing data and other relevant information to develop fact-based conclusions and make business recommendations to improve customer and agent experience delivery. Employ a logical, systematic approach to problem solving and research using analytical techniques to achieve business objectives and drive change.
Lead strategy and execution of digital/web analytics and serve as the digital/web analytics expert for Grange.
Partner with Data Engineering and Data Warehouse team on requirements and enhancements to existing data pipelines, data sets, and data platforms. Partner with the Business Intelligence and Marketing teams to ensure synergies and a cohesive measurement system across operational/financial analytics, marketing analytics, digital experience analytics and customer and agent experience analytics.
Interacts with business stakeholders and cross-functional business partners. Owns specific relationships with business stakeholders and cross-functional business partners. Examples include: discussing and identifying business problems, collaboration on possible solutions, data needs, upcoming project requirements, and results.
Evaluate best method(s) to define, measure, investigate and track customer and agent experience performance and success measures. Iteratively prototype, build, and automate reports and share with stakeholders via established routine. Establish and automate post deployment reporting and analysis based on the success metrics, provide business and product teams with insights for post deployment performance.
Identify improvement opportunities, form hypothesis proposals, design and implement tests to drive strategy enhancement and optimization. Customer/agent data and analytics efforts align with and support team/business objectives and strategy.
Define, monitor and implement a customer & agent experience analytics strategy and associated data analysis. Ensure alignment of success measures with product and service delivery strategy.
Develop and maintain processes for extracting, cleaning, and organizing data for analysis including implementing a moderate amount of code (Python, R, SQL, some limited JavaScript) to enable completion of repetitive and complicated tasks and handle large amounts of data. Examples include data pulls from multiple sources, complex data cleaning, SQL data pulls, Google Tag Manager, predictive models (machine learning), expanded use of APIs (Google APIs, Snowflake), and process automation.
Support enterprise data governance/stewardship activities.
Determine and conduct appropriate exploratory data analysis techniques on candidate data sets. Use appropriate sampling techniques and thoroughly document assumptions, methodology, validation and testing.
Apply traditional and innovative analytical and modeling approaches to draw conclusions and make recommendations on business tactics and strategies.
Explore data using a variety of advanced statistical techniques to answer business questions or shape future model development. Understand and leverage segmentation such as Lifetime Value, Behavioral analysis and Model behavior.
Continuously learn the application of role-specific tools and techniques to solving analytical problems. Identify, leverage and develop expertise in emerging technologies and open source tools. Harness new mathematical techniques.
Guide, train, and mentor other analysts in data, applications, methods, and presentation.
What You’ll Bring To The Company:
BA/BS degree in Business, Finance, Economics, Technology or other relevant discipline, Master’s Degree a plus.
5 years of experience in analytics related to product development, digital, mobile, user experience, customer and agent experience, or ecommerce.
Experience with SAS, R, Python or a comparable data analysis tools
Experience with programming, working with databases, working with operational data
3 - 5 years of experience working with a variety of digital technology, reflected in a thorough understanding of the digital space.
Proven skills in data management and data extraction working with large data sets preferred.
Must have an aptitude for critical thinking and problem resolution
Enthusiastic and creative leader with the ability to inspire others.
High level of competency collaborating, inspiring and leading others across all levels of the organization.
Strong empathy for customers and passion for revenue and growth.
Strategic and innovative thinker.
Process-oriented mindset.
Pro-active and results orientated.
Candidate must have strong attention to detail.
Ability to “hit the ground running”
Expertise with analytics tools (i.e. Google Analytics, Sitecore Experience Analytics, Power BI / Tableau / Microstrategy or similar, Python and/or R, Excel)
Experience of analyzing complex data and large data volumes.
Comprehensive understanding of customer insights, technical capabilities, and experience design principles to translate business, channel and marketing objectives into integrated digital strategies.
Excellent organizational, verbal, presentation/facilitation and written communication skills.
Advanced knowledge of statistics and models.
About Us:
Grange Insurance Company, with $3.1 billion in assets and more than $1 billion in annual revenue, is an insurance provider founded in 1935 and based in Columbus, Ohio. Through its network of independent agents, Grange offers auto, home and business insurance protection. Grange Insurance Company and its affiliates serve policyholders in Georgia, Illinois, Indiana, Iowa, Kentucky, Michigan, Minnesota, Ohio, Pennsylvania, South Carolina, Tennessee, Virginia and Wisconsin- the 13-state Grange Enterprise holds an A.M. Best rating of ""A"" (Excellent).
Who We Are:
We are committed to an inclusive work environment where we welcome and value diversity and inclusion. We hire great talent from a wide variety of backgrounds, and our associates are our biggest strength. The diversity of our associates, their backgrounds, experiences, and individual differences are the foundation for our success. Our inclusive culture empowers all of us to “Be One Team”, “Deliver Excellence”, “Communicate Openly”, “Do the Right Thing”, and “Solve Creatively for Tomorrow”. We have active Associate Resource Groups and a Diversity and Inclusion Team, that focuses on professional development, networking, business value and community outreach; all which encourage and facilitate an environment that fosters learning, innovation, and growth. Together we use our individual experiences to learn from one another and grow as professionals and as humans.
We welcome the unique contributions that you bring from education, opinions, culture, beliefs, race, color, religion, age, sex, national origin, handicap, disability, sexual orientation, gender stereotyping, gender identity or expression, genetic information, ancestry, pregnancy, veteran status, and citizenship.
Grange Enterprise is proud to be part of the CEO Action for Diversity and Inclusion™, a national initiative of more than 1400 CEOs working for the advancement of diversity and inclusion within the workplace.
Show more
Show less","SAS, R, Python, SQL, Google Analytics, Sitecore Experience Analytics, Power BI, Tableau, Microstrategy, Google APIs, Snowflake, Excel, Machine learning, Predictive models, Big data, Data management, Data extraction, Data mining, Data analysis, Data visualization, Data governance, Data stewardship, Exploratory data analysis, Sampling techniques, Statistical techniques, Segmentation, Lifetime Value, Behavioral analysis, Model behavior, Critical thinking, Problem resolution, Leadership, Collaboration, Communication, Statistics, Models","sas, r, python, sql, google analytics, sitecore experience analytics, power bi, tableau, microstrategy, google apis, snowflake, excel, machine learning, predictive models, big data, data management, data extraction, data mining, data analysis, data visualization, data governance, data stewardship, exploratory data analysis, sampling techniques, statistical techniques, segmentation, lifetime value, behavioral analysis, model behavior, critical thinking, problem resolution, leadership, collaboration, communication, statistics, models","behavioral analysis, big data, collaboration, communication, critical thinking, data extraction, data governance, data management, data mining, data stewardship, dataanalytics, excel, exploratory data analysis, google analytics, google apis, leadership, lifetime value, machine learning, microstrategy, model behavior, models, powerbi, predictive models, problem resolution, python, r, sampling techniques, sas, segmentation, sitecore experience analytics, snowflake, sql, statistical techniques, statistics, tableau, visualization"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Chicago Heights, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742671960,2023-12-17,Chicago Heights,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, MLOps, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, IaC (Infrastructure as Code), Apache Beam, Kafka, ETL, Machine Learning, Data Engineering, CI/CD","python, mlops, spark, scala, pyspark, aws, azure, gcp, devops, iac infrastructure as code, apache beam, kafka, etl, machine learning, data engineering, cicd","apache beam, aws, azure, cicd, data engineering, devops, etl, gcp, iac infrastructure as code, kafka, machine learning, mlops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Flossmoor, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742678434,2023-12-17,Chicago Heights,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, MLOps, Data engineering, ETL, Data quality, Machine learning, DevOps, Cloud platforms (AWS Azure GCP), Spark, Scala, PySpark, Apache Beam, Kafka, CI/CD, Infrastructure as Code (IaC)","python, mlops, data engineering, etl, data quality, machine learning, devops, cloud platforms aws azure gcp, spark, scala, pyspark, apache beam, kafka, cicd, infrastructure as code iac","apache beam, cicd, cloud platforms aws azure gcp, data engineering, data quality, devops, etl, infrastructure as code iac, kafka, machine learning, mlops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Homewood, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742679366,2023-12-17,Chicago Heights,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Data engineering, ETL, Machine learning, MLOps, Data analysis, Cloud platforms (AWS Azure GCP), Software development, CI/CD, DevOps, IaC (Infrastructure as Code), ETL tools (Apache Beam Kafka Spark), Big data tools (Spark Scala PySpark), Communication, Collaboration, NFT marketplaces, VR imaging, AI/ML projects, Case studies, Community events, Online hackathons, Competitions","python, data engineering, etl, machine learning, mlops, data analysis, cloud platforms aws azure gcp, software development, cicd, devops, iac infrastructure as code, etl tools apache beam kafka spark, big data tools spark scala pyspark, communication, collaboration, nft marketplaces, vr imaging, aiml projects, case studies, community events, online hackathons, competitions","aiml projects, big data tools spark scala pyspark, case studies, cicd, cloud platforms aws azure gcp, collaboration, communication, community events, competitions, data engineering, dataanalytics, devops, etl, etl tools apache beam kafka spark, iac infrastructure as code, machine learning, mlops, nft marketplaces, online hackathons, python, software development, vr imaging"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Hazel Crest, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742673964,2023-12-17,Chicago Heights,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Machine learning, ETL, ELT, Data quality, Performance, CI/CD, DevOps, IaC, Apache Beam, Kafka, Spark, PySpark, AWS, Azure, GCP, NFT, VR, AI/ML","python, machine learning, etl, elt, data quality, performance, cicd, devops, iac, apache beam, kafka, spark, pyspark, aws, azure, gcp, nft, vr, aiml","aiml, apache beam, aws, azure, cicd, data quality, devops, elt, etl, gcp, iac, kafka, machine learning, nft, performance, python, spark, vr"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Lansing, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742679392,2023-12-17,Chicago Heights,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Machine Learning, ETL, ELT, Data Pipeline Monitoring, Troubleshooting, Debugging, MLOps, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Kafka, Apache Beam, Communication, Collaboration","python, machine learning, etl, elt, data pipeline monitoring, troubleshooting, debugging, mlops, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, kafka, apache beam, communication, collaboration","apache beam, aws, azure, cicd, collaboration, communication, data pipeline monitoring, debugging, devops, elt, etl, gcp, iac, kafka, machine learning, mlops, python, scala, spark, troubleshooting"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Markham, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742680310,2023-12-17,Chicago Heights,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Data Engineering, ETL, Machine Learning, Python, MLOps, Data Analysis, Spark, Scala, PySpark, Cloud Platforms, DevOps, CI/CD, IaC (Infrastructure as Code), ETL Tools, Apache Beam, Kafka, Communication Skills, Collaboration Skills","data engineering, etl, machine learning, python, mlops, data analysis, spark, scala, pyspark, cloud platforms, devops, cicd, iac infrastructure as code, etl tools, apache beam, kafka, communication skills, collaboration skills","apache beam, cicd, cloud platforms, collaboration skills, communication skills, data engineering, dataanalytics, devops, etl, etl tools, iac infrastructure as code, kafka, machine learning, mlops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Posen, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742673959,2023-12-17,Chicago Heights,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, MLOps, ETL, ELT, Machine Learning, Spark, Scala, PySpark, AWS, Azure, GCP, CI/CD, IaC (Infrastructure as Code), Apache Beam, Kafka, DevOps","python, mlops, etl, elt, machine learning, spark, scala, pyspark, aws, azure, gcp, cicd, iac infrastructure as code, apache beam, kafka, devops","apache beam, aws, azure, cicd, devops, elt, etl, gcp, iac infrastructure as code, kafka, machine learning, mlops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Dolton, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742674942,2023-12-17,Chicago Heights,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Machine Learning, ETL, ELT, Data Pipelines, Data Quality, Data Performance, Data Science, Collaboration, CI/CD, IaC (Infrastructure as Code), Apache Beam, Kafka, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, NFT marketplaces, VR imaging, AI/ML projects, Portfolio, Hackathons, Competitions","python, machine learning, etl, elt, data pipelines, data quality, data performance, data science, collaboration, cicd, iac infrastructure as code, apache beam, kafka, spark, scala, pyspark, aws, azure, gcp, devops, nft marketplaces, vr imaging, aiml projects, portfolio, hackathons, competitions","aiml projects, apache beam, aws, azure, cicd, collaboration, competitions, data performance, data quality, data science, datapipeline, devops, elt, etl, gcp, hackathons, iac infrastructure as code, kafka, machine learning, nft marketplaces, portfolio, python, scala, spark, vr imaging"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Riverdale, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742674944,2023-12-17,Chicago Heights,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Machine Learning, ETL, ELT, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Apache Beam, Kafka, NFT, VR, AI/ML","python, machine learning, etl, elt, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, apache beam, kafka, nft, vr, aiml","aiml, apache beam, aws, azure, cicd, devops, elt, etl, gcp, iac, kafka, machine learning, nft, python, scala, spark, vr"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Blue Island, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742678426,2023-12-17,Chicago Heights,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, MLOps, Spark, Scala, PySpark, AWS, Azure, GCP, CI/CD, IaC (Infrastructure as Code), Apache Beam, Kafka, ETL, ELT, Machine Learning, DevOps, NFT, VR, AI/ML, Git, Docker, Kubernetes","python, mlops, spark, scala, pyspark, aws, azure, gcp, cicd, iac infrastructure as code, apache beam, kafka, etl, elt, machine learning, devops, nft, vr, aiml, git, docker, kubernetes","aiml, apache beam, aws, azure, cicd, devops, docker, elt, etl, gcp, git, iac infrastructure as code, kafka, kubernetes, machine learning, mlops, nft, python, scala, spark, vr"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Calumet Park, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742674926,2023-12-17,Chicago Heights,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, MLOps, Spark, Scala, PySpark, Aws, Azure, GCP, IaC (Infrastructure as Code), Apache Beam, Kafka, ETL, ELT, Machine Learning","python, mlops, spark, scala, pyspark, aws, azure, gcp, iac infrastructure as code, apache beam, kafka, etl, elt, machine learning","apache beam, aws, azure, elt, etl, gcp, iac infrastructure as code, kafka, machine learning, mlops, python, scala, spark"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Pembroke, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3752007956,2023-12-17,Laurentian Hills, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data analysis, Data interpretation, Data visualization, Statistical techniques, Statistical modeling, Hypothesis testing, A/B testing, Data management, ETL processes, Data quality, Data integrity, Data accuracy, Data completeness, SQL, R, Python, Tableau, Power BI","data analysis, data interpretation, data visualization, statistical techniques, statistical modeling, hypothesis testing, ab testing, data management, etl processes, data quality, data integrity, data accuracy, data completeness, sql, r, python, tableau, power bi","ab testing, data accuracy, data completeness, data integrity, data interpretation, data management, data quality, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Data Analyst Part Time,Voxmediallc,"Drummondville, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-voxmediallc-3757202949,2023-12-17,Saint Albert, Canada,Mid senior,Onsite,"Summary:
Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, SQL, R, Python, Tableau, Power BI, Data Visualization, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL, Hadoop, Spark, Machine Learning, AI, Data Mining, Data Warehousing, Cloud Computing, Data Science","data analysis, sql, r, python, tableau, power bi, data visualization, statistical modeling, hypothesis testing, ab testing, data management, etl, hadoop, spark, machine learning, ai, data mining, data warehousing, cloud computing, data science","ab testing, ai, cloud computing, data management, data mining, data science, dataanalytics, datawarehouse, etl, hadoop, hypothesis testing, machine learning, powerbi, python, r, spark, sql, statistical modeling, tableau, visualization"
Business Data Analyst,Onyx CenterSource,European Economic Area,https://www.linkedin.com/jobs/view/business-data-analyst-at-onyx-centersource-3782250792,2023-12-17,Palatka,United States,Associate,Remote,"Reports to Manager of Business Data
The business analyst will support development and execution of enhanced reporting, analysis and analytical models used to enhance decision making for internal & external stakeholders. He or she will create deliverables to support customer facing analysis and consultation. The role will interface with strategic & key customers and will work collaboratively with both Client Management and the Commercial organization.
Location:
Europe
Responsibilities:
Understand company’s data set and quality to set a baseline & identify gaps to be filled
Gather market insight from customers on business needs/objectives to refine reporting, dashboard, and analytical deliverables for Business Intelligence offering
Deliver internal, client specific, and aggregate customer analysis
Review complex sets of information to identify opportunities, reach conclusions, & develop actionable recommendations
Participate in the on-going development of reporting, dashboards, scorecards and multi-dimensional data analysis solutions to provide ongoing insight to internal & external stakeholders
Perform segmentation and analysis of customer data to identify trends and create benchmarking comparisons
Continuously take a forward-looking view by researching, validating, and proposing new deliverables and analysis.
Support the BI leader to identify data requirements and enhancements to further enable analytical deliverables
Encourage the business to ‘think beyond’ and challenge the current practices
Develop analysis and materials to advance Onyx’s thought leadership in the industry
Develop strong relationships with commercial product owners to understand the business process and data.
Perform detailed data research and discovery to understand data sources, processes, and integrations that cause data quality issues
Work with Onyx’ BI and Data engineering team to build data pipeline and reports to support the analytical findings.
Minimum Qualifications:
Bachelor's Degree
2 to 5 years of analytics experience
Proven ability to leverage data, quantitative analysis and advanced analytics to deliver actionable insights, guide decisions and drive business results
Experience with SQL
Experience with Salesforce
Experience with AWS Data Lake
Experience with Python
Advanced skills in Microsoft Excel
Knowledge of Tableau / Microstrategy
Preferred Qualifications:
Experience in travel & hospitality highly valued
Specific experience in business analytics at a hotel chain and/or a large travel management company/travel consulting company is desired
Competencies:
Ability to translate quantitative data into actionable recommendations
Ability to translate business objectives into customer analysis, measurements, and recommendations
Identifying, analyzing, and interpreting trends or patterns in complex data sets
Analyzing data using statistical techniques and providing reports
Ability to identify and analyze critical issues, draw conclusions, and formulate recommended solutions while being attentive to details and data quality
Ability to communicate complex data concepts in ways that internal & external stakeholders will understand
Excellent verbal and written communication skills
Characteristics:
Analytical thinker
Curiosity
Strong communication and storytelling skills
Self/motivated & accountable
Achievement/goal orientated
Flexible/adaptable
Service-oriented
Collaborative
Rational/logical
Detail-oriented
Desire for continuous improvement
Show more
Show less","Data Analysis, Analytical Modeling, Data Quality Management, Market Insight Gathering, Customer Data Analysis, Segmentation, Data Integration, Business Analytics, Data Visualization, Python, SQL, Tableau, Microstrategy, Salesforce, AWS Data Lake, Microsoft Excel","data analysis, analytical modeling, data quality management, market insight gathering, customer data analysis, segmentation, data integration, business analytics, data visualization, python, sql, tableau, microstrategy, salesforce, aws data lake, microsoft excel","analytical modeling, aws data lake, business analytics, customer data analysis, data integration, data quality management, dataanalytics, market insight gathering, microsoft excel, microstrategy, python, salesforce, segmentation, sql, tableau, visualization"
Lead Data Engineer,Hudl,"Nebraska, United States",https://www.linkedin.com/jobs/view/lead-data-engineer-at-hudl-3727345101,2023-12-17,Nebraska,United States,Mid senior,Onsite,"Building a team starts with valuing the team. We hire the best of the best to ensure you’re working with people you can constantly learn from. You’re trusted to get your work done your way while testing the limits of what’s possible and what’s next. We work hard to provide a culture where everyone feels supported, and our employees feel it—their votes helped us become one of Newsweek's Top 100 Global Most Loved Workplaces in 2023 .
We also value sports. Not just because of the big wins and highlight-worthy plays, but because of the lasting impact sports can have: the lifelong lessons in teamwork and dedication; the influence of inspiring mentors; the priceless opportunities to play at the next level. Sports can change lives—that’s what we value.
Our team helps the world see sports differently through products that make it easier to capture video, analyze data, share highlights and do a lot more.
Ready to join us?
Your Role
We’re looking for a Lead Data Engineer to join our Data Quality & Infrastructure (DQ&I) team. Join us in shaping a culture where data is at everyone’s fingertips, quickly and intuitively. We currently support a 100TB data warehouse used by hundreds of people to make mission-critical decisions for our products and business. We’ll lean on your experience to guide projects that scale our systems and make data analysis more efficient across the company.
We'd like to hire someone for this role who lives near our office in
Nebraska
and
Lexington
, but we're also open to remote candidates in
Boston
,
Phoenix
,
Kansas City
,
Chicago
,
Austin
or
Dallas
.
Must-Haves
Technical backbone. To keep our ETL pipelines and data platform running smoothly, we need someone with at least seven years of experience using Python and AWS data services, like: DMS, Glue, Spectrum, Athena, RDS, DocumentDB and Redshift. We also use Airflow and Lambda, as well as designing and maintaining our own infrastructure as code (Terraform/Terragrunt/CodePipeline).
Problem-solving skills. You understand that finding solutions is a team effort and will help support other Engineers learn and develop.
Translation tactics. You translate real-world problems into elegant code solutions.
Leadership experience. You’ll set an example for others in planning, designing and delivering complex projects, while being the go-to person for all technical matters.
Communication skills. You’re always ready to fine-tune your style depending on the interaction, and you’re skilled at connecting with stakeholders from all areas of the company.
Nice-to-Haves
Collaborative. One of the Scrum ceremonies you appreciate most is proactively and thoughtfully creating tickets for the team’s backlog.
Hudl knowledge. You'll work on Hudl’s innovative data platform, which helps our users find and use our vast library of data to stay ahead of the competition.
Comfortable with ownership. You'll work in an autonomous squad that has the freedom and expertise to deliver the best solution.
Our Role
Champion work-life harmony . We’ll give you the flexibility you need in your work life (e.g., flexible vacation time, company-wide holidays and timeout (meeting-free) days, remote work options and more) so you can enjoy your personal life too.
Guarantee autonomy . We have an open, honest culture and we trust our people from day one. Your team will support you, but you’ll own your work and have the agency to try new ideas.
Encourage career growth . We’re lifelong learners who encourage professional development. We’ll give you tons of resources and opportunities to keep growing.
Provide an environment to help you succeed . We've invested in our offices, designing incredible spaces with our employees in mind. But whether you’re at the office or working remotely, we’ll provide you the tech stack and hardware to do your best work.
Support your mental and physical health . We care about our employees’ wellbeing. Our Employee Assistance Program, employee resource groups and fitness partner Peerfit have you covered.
Cover your medicalinsurance . We have multiple plans to pick from to ensure you’ll have the coverage you (and your dependents) want, including vision, dental, fertility healthcare and family forming benefits.
Contribute to your 401(K). Yep, that’s free money. We’ll match up to 4% of your own contribution.
Inclusion at Hudl
Hudl is an equal opportunity employer. Through our actions, behaviors and attitude, we’ll create an environment where everyone, no matter their differences, feels like they belong.
We offer resources to ensure our employees feel safe bringing their authentic selves to work, including employee resource groups and communities . But we recognize there’s ongoing work to be done, which is why we track our efforts and commitments in annual inclusion reports .
We also know imposter syndrome is real and the confidence gap can get in the way of meeting spectacular candidates. Please don’t hesitate to apply—we’d love to hear from you.
Privacy Policy
Hudl Applicant and Candidate Privacy Policy
Show more
Show less","Python, AWS, DMS, Glue, Spectrum, Athena, RDS, DocumentDB, Redshift, Airflow, Lambda, Terraform, Terragrunt, CodePipeline, ETL, Scrum, Hudl, Collaborative, Autonomy, Agile, Ownership, Leadership","python, aws, dms, glue, spectrum, athena, rds, documentdb, redshift, airflow, lambda, terraform, terragrunt, codepipeline, etl, scrum, hudl, collaborative, autonomy, agile, ownership, leadership","agile, airflow, athena, autonomy, aws, codepipeline, collaborative, dms, documentdb, etl, glue, hudl, lambda, leadership, ownership, python, rds, redshift, scrum, spectrum, terraform, terragrunt"
Data Architect (Staff Data Engineer),Optomi,"Norfolk, NE",https://www.linkedin.com/jobs/view/data-architect-staff-data-engineer-at-optomi-3773572899,2023-12-17,Nebraska,United States,Mid senior,Onsite,"Abbreviated Job Description- Please see important notes below
Summary:
Optomi, in partnership with a flagship enterprise in the American Steel industry, is inviting applicants for a Solutions Architect to help lead a mission-critical migration of data processing infrastructure to an Azure Data Factory-ADLS solution.
Candidates should be comfortable with hands-on & cutting-edge technical work, as well as decision-making autonomy, liaising with the business, and technical leadership of a small team.
Experience of the right candidate:
Preferably, at least a Bachelor of Science degree in a relevant field
5+ years of experience in designing, building, and implementing modern data integration and data warehouse solutions
Experience in designing solutions in Microsoft Azure (Azure Databricks/Azure Data Factory)
Experience in Data Warehouses, Data Lakes, and/or Data Lakehouses at scale
Extensive hands-on experience in implementing ETL/ELT solutions with data migration experience.
Important Note
:
Candidates will be required to relocate to one of several different offices disparately located around the country, in rural areas in Texas, South Carolina, Nebraska, Utah, Alabama, or Indiana.
Important Note
: Due to the nature of this opportunity- no 3rd parties or 3rd party candidates will be considered at this time; please do not apply if you require visa sponsorship or application on a C2C basis (Independent Consultants with their own llc can be considered)
Show more
Show less","Azure Data Factory, ADLS, Data Warehouses, Data Lakes, Data Lakehouses, ETL, ELT, Data migration, Azure Databricks, Microsoft Azure","azure data factory, adls, data warehouses, data lakes, data lakehouses, etl, elt, data migration, azure databricks, microsoft azure","adls, azure data factory, azure databricks, data lakehouses, data lakes, data migration, data warehouses, elt, etl, microsoft azure"
CLUB Data Analyst I,Cabela's,"Lincoln, NE",https://www.linkedin.com/jobs/view/club-data-analyst-i-at-cabela-s-3782763027,2023-12-17,Nebraska,United States,Mid senior,Onsite,"POSITION SUMMARY:
The CLUB Data Analyst I analyzes and executes cross-functional CLUB acquisition, activation and retention strategies. Researches CLUB credit risk and portfolio management activities, including credit policy, collections, fraud, operations and accounting. Leads marketing programs and executes new and existing activities that support overall marketing strategies.
ESSENTIAL FUNCTIONS:
Analyzes existing marketing strategies to identify opportunities to enhance CLUB acquisition, activation and retention.
Creates, reviews reports and analysis to determine how key business drivers can be manipulated through marketing programs and/or enhanced operations.
Researches trend on the CLUB credit card portfolio to provide insights into performance and guide business priorities.
Analyzes, develops, executes and validates marketing processes to support marketing strategies. Identifies and executes process improvements and maintains accurate documentation of processes. Ensures effective and efficient integration with other business units.
Retrieves information from a data warehouse using Structured Query Language (SQL) and other various sources including auditing and validating the data.
Creates clear and concise dashboards on KPI’s and metrics using BI software tools (DOMO).
Develops and maintains relationships with internal and external partners through regular, proactive communication.
ALL OTHER DUTIES AS ASSIGNED
EXPERIENCE/QUALIFICATIONS:
Minimum Degree Required: Bachelor’s Degree in Business, Finance, Statistics, Economics, or Math.
Experience: 1 year of analytics experience preferred
Knowledge, Skills, And Ability
Knowledge of qualitative and quantitative methodologies preferred.
Ability to handle large datasets using SQL, SAS, R or another similar programming language preferred.
Proficiency in Windows, Excel, Word and PowerPoint software. Experience in data visualization preferred.
Strong communication (written and verbal), mediation (conflict resolution) and facilitation skills
Ability to deliver accurate results within expected deadlines.
Self-starter; demonstrates personal initiative and willingly assumes responsibility and ownership.
Ability to build strong working relationships with internal and external customers.
Customer focused behavior.
Exemplary standards of integrity, personal work ethic and continuous involvement in self-education and development.
Ability to consistently exceed customer expectations and show commitment to Cabela's & Bass Pro Core Values.
TRAVEL REQUIREMENTS:
0-10%
PHYSICAL REQUIREMENTS:
Regular computer work, and sitting
Occasional walking, standing, and lifting up to 50lbs
INDEPENDENT JUDGEMENT
:
Performs duties within scope of general company policies, procedures, and objectives. Analyzes problems and performs needs assessments. Uses judgment in adapting broad guidelines to achieve desired result. Regular exercise of independent judgment within accepted practices. Makes recommendations that affect policies, procedures, and practices.
Full Time Benefits Summary:
Enjoy discounts on retail merchandise, our restaurants, world-class resorts and conservation attractions!
Medical
Dental
Vision
Health Savings Account
Flexible Spending Account
Voluntary benefits
401k Retirement Savings
Paid holidays
Paid vacation
Paid sick time
Bass Pro Cares Fund
And more!
Bass Pro Shops is an equal opportunity employer. Hiring decisions are administered without regard to race, color, creed, religion, sex, pregnancy, sexual orientation, gender identity, age, national origin, ancestry, citizenship status, disability, veteran status, genetic information, or any other basis protected by applicable federal, state or local law.
Reasonable Accommodations
Qualified individuals with known disabilities may be entitled to reasonable accommodation under the Americans with Disabilities Act and certain state or local laws.
If you need a reasonable accommodation for any part of the application process, please visit your nearest location or contact us at hrcompliance@basspro.com.
Cabela's
Show more
Show less","Business Intelligence Software (BI), Structured Query Language (SQL), SAS, R, Microsoft Excel, Microsoft Word, Microsoft PowerPoint, Windows, Data Visualization, Data Warehousing, Dashboards, Data Analysis, Data Mining, Machine Learning, Marketing Analytics, Customer Relationship Management (CRM), Business Process Management (BPM), Project Management, Risk Management, Communication, Mediation, Facilitation, Leadership, Teamwork, Problem Solving, Critical Thinking, Decision Making, Innovation, Creativity, Adaptability, Flexibility, Time Management, Stress Management","business intelligence software bi, structured query language sql, sas, r, microsoft excel, microsoft word, microsoft powerpoint, windows, data visualization, data warehousing, dashboards, data analysis, data mining, machine learning, marketing analytics, customer relationship management crm, business process management bpm, project management, risk management, communication, mediation, facilitation, leadership, teamwork, problem solving, critical thinking, decision making, innovation, creativity, adaptability, flexibility, time management, stress management","adaptability, business intelligence software bi, business process management bpm, communication, creativity, critical thinking, customer relationship management crm, dashboard, data mining, dataanalytics, datawarehouse, decision making, facilitation, flexibility, innovation, leadership, machine learning, marketing analytics, mediation, microsoft excel, microsoft powerpoint, microsoft word, problem solving, project management, r, risk management, sas, stress management, structured query language sql, teamwork, time management, visualization, windows"
BI DATA ANALYST,Skiltrek,"Newark, NE",https://www.linkedin.com/jobs/view/bi-data-analyst-at-skiltrek-3785514024,2023-12-17,Nebraska,United States,Mid senior,Hybrid,"Job Description
Skiltrek is looking for a BI Data Analyst to join their IT organization. The company is growing and so is the demand for BI solutions so they need this person to help support this effort. The successful candidate will build and maintain strong relationships with business partners, while supporting the company's analytic capabilities and ensuring delivery of high-quality analytic solutions. This role requires the person to have hands on experience in statistical analysis, predictive modeling, and customer analytics.
Required Skills & Experience
Minimum five years' experience as data analyst, business intelligence analyst, or equivalent roles
Experience working on data migration projects and tools
Strong experience in SQL and Tableau
Enterprise data models and data governance
ETL experience
Experience working directly with stakeholders, gathering requirements to understand the business
Nice To Have Skills & Experience
SAP Business Object experience
Database knowledge
About Us
Skiltrek is an award-winning IT staffing firm and the staffing partner of choice for many leading companies across the US.
At Skiltrek, we promise you the perfect opportunity of building technical excellence, understand business performance and nuances,
be abreast with the latest happenings in technology world and enjoy a satisfying work life balance.
Skiltrek is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to gender,
race, religion, national origin, ethnicity, disability, gender identity/expression, sexual orientation, veteran or military status, or any other category protected under the law.
Skiltrek is an equal opportunity employer; committed to a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Show more
Show less","Data Analysis, Statistical Analysis, Predictive Modeling, Customer Analytics, SQL, Tableau, Enterprise Data Models, Data Governance, ETL, SAP Business Object, Database","data analysis, statistical analysis, predictive modeling, customer analytics, sql, tableau, enterprise data models, data governance, etl, sap business object, database","customer analytics, data governance, dataanalytics, database, enterprise data models, etl, predictive modeling, sap business object, sql, statistical analysis, tableau"
Sr. Business Data Analyst,"The Buckle, Inc.","Kearney, NE",https://www.linkedin.com/jobs/view/sr-business-data-analyst-at-the-buckle-inc-3707166989,2023-12-17,Nebraska,United States,Mid senior,Hybrid,"Summary
The Senior Business Data Analyst is responsible for all phases of reporting, decision making and data analysis activity including design, data collection and presentation and auditing. The Teammate in this role will analyze information and processes and explain concepts to both technical and non-technical users.
Essential Duties And Responsibilities
This description intends to describe the general nature and level of work performed by Teammates assigned to this job. It is not intended to include all duties, responsibilities, and qualifications. To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Lead and oversee data analysis projects, working closely with stakeholders to define priorities and requirements.
Collaborate with cross-functional teams to ensure data accuracy and consistency throughout the organization.
Provide strategic recommendations and actionable insights based on data analysis to support business decision-making.
Drive business impact by using data to identify opportunities for growth, improve operational efficiency and reduce risk.
Craft compelling data stories by presenting data insights and findings to stakeholders, using effective data visualization techniques.
Act as a subject matter expert on data-related matters for the sales team, providing guidance and support to team members and other departments.
Foster a data-driven culture within the organization, promoting data literacy and encouraging data-based decision-making.
Fulfill mission statement by performing job duties with a high level of customer service while contributing to a positive team spirit.
Special projects and other duties as assigned.
Supervisory Responsibilities
This job has no supervisory responsibilities.
Education And/or Experience
Bachelor's degree from four-year college or university; and one to three years related experience; or equivalent combination of education and experience.
Physical Demands
The physical demands described here are representative of those that must be met by a Teammate to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
While performing the duties of this Job, the Teammate is regularly required to sit; use hands to finger, handle, or feel and talk or hear. The Teammate is occasionally required to stand; walk; reach with hands and arms and stoop, kneel, crouch, or crawl. The Teammate must frequently lift and/or move up to 25 pounds. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception and ability to adjust focus.
Work Environment
While performing the duties of this job, the Teammate regularly works in an office setting. The noise level in the work environment is usually moderate.
Equal Employment Opportunity
Buckle is committed to hiring and developing the most qualified Teammates from the available workforce in the communities we serve.
Equal employment opportunity has been, and will continue to be, a fundamental principle at Buckle, where employment is based upon personal capabilities and qualifications without discrimination and retaliation because of veteran status, uniformed service member status, race, color, national origin or ancestry, creed, religion, sex, sexual orientation, gender identity or expression, age, pregnancy (including childbirth, lactation, and related medical conditions), national origin or ancestry, physical or mental disability, genetic information (including characteristics and testing), or any other protected characteristic as established by applicable local, state, or federal law. For state specific information, refer to the Teammate Center.
Show more
Show less","Data analysis, Data collection, Data visualization, Data presentation, Data audits, Reporting, Decision making, Business intelligence, Data storytelling, Data literacy, Datadriven decisionmaking, Data mining, Data warehousing, Data modeling, Data integration, Data governance, Data quality management, Statistical analysis, Machine learning, Artificial intelligence, SQL, Python, R, Tableau, Power BI, Excel, Hadoop, Spark, Hive","data analysis, data collection, data visualization, data presentation, data audits, reporting, decision making, business intelligence, data storytelling, data literacy, datadriven decisionmaking, data mining, data warehousing, data modeling, data integration, data governance, data quality management, statistical analysis, machine learning, artificial intelligence, sql, python, r, tableau, power bi, excel, hadoop, spark, hive","artificial intelligence, business intelligence, data audits, data collection, data governance, data integration, data literacy, data mining, data presentation, data quality management, data storytelling, dataanalytics, datadriven decisionmaking, datamodeling, datawarehouse, decision making, excel, hadoop, hive, machine learning, powerbi, python, r, reporting, spark, sql, statistical analysis, tableau, visualization"
TSS Data Engineer Senior,General Dynamics Information Technology,"District of Columbia, United States",https://www.linkedin.com/jobs/view/tss-data-engineer-senior-at-general-dynamics-information-technology-3746124137,2023-12-17,Laurel,United States,Mid senior,Hybrid,"Job Description:
Type of Requisition:
Pipeline
Clearance Level Must Currently Possess:
None
Clearance Level Must Be Able To Obtain:
None
Suitability:
Public Trust/Other Required:
BI Full 6C (T4)
Job Family:
Data Science
Skills:
Job Qualifications:
Analytical Thinking, Design, Documentations, Software Program
Certifications:
Experience:
5 + years of related experience
US Citizenship Required:
Yes
Job Description:
Deliver simple solutions to complex problems as a Data Engineer Senior at GDIT. Here, you’ll tailor cutting-edge solutions to the unique requirements of our clients. With a career in application development, you’ll make the end user’s experience your priority and we’ll make your career growth ours.
At GDIT, people are our differentiator. As a Data Engineer Senior you will help ensure today is safe and tomorrow is smarter. Our work depends on Data Engineer Senior joining our team to help improve Aviation safety in collaboration with our stakeholders. Our customers deal with real-world safety problems and this candidate can help develop the required software designs and code to scale our system and improve user insights into safety issues.
HOW A DATA ENGINEER SENIOR WILL MAKE AN IMPACT:
Possesses and applies a comprehensive knowledge across key tasks and high impact assignments.
Plans and leads major technology assignments.
Evaluates performance results and functions as a technical expert across multiple project assignments.
May supervise others.
Codes, tests, debugs, implements, and documents low to highly complex programs.
Creates appropriate documentation in work assignments such as program code, and technical documentation.
Designs systems and programs to meet complex business needs.
Prepares detailed specifications from which programs are developed and coded.
Ensures programs meet standards and technical specifications; performs technical analysis and component delivery.
Gathers information from existing systems, analyzes program and time requirements.
Assists project manager in preparing time estimates and justification for assigned tasks.
Designs programs for projects or enhancements to existing programs.
Writes specifications for programs of low to advanced complexity.
Assists support and/or project personnel in resolving varying levels of complex program problems.
Works with client and management to resolve issues and validate programming requirements within their areas of responsibility.
Provides technical advice on complex programming.
Develops test plans to verify logic of new or modified programs.
Conducts quality assurance activities such as peer reviews.
Creates appropriate documentation in work assignments such as program code, and technical documentation.
Remains abreast of industry technical trends and new development to maintain current skills and remain current with industry standards.
Designs, develops, evaluates, plans and tests engineering specifications for software programs and applications
Education and Required Experience: Bachelor’s Degree + 5 years experience
Required Technical Skills: AWS Cloud Engineer
Security Clearance Level: Public Trust
Required Skills and Abilities: AWS Glue, AWS Lambda, Python, JIRA
Location: Remote
US Citizenship Required
Preferred Skills
Preferred Skills: AWS Athena, AWS CI/CD pipeline
Secret Clearance or ability to obtain a clearance
Home location of DMV
GDIT IS YOUR PLACE:
Full-flex work week to own your priorities at work and at home
401K with company match
Comprehensive health and wellness packages
Internal mobility team dedicated to helping you own your career
Professional growth opportunities including paid education and certifications
Cutting-edge technology you can learn from
Rest and recharge with paid vacation and holidays
Scheduled Weekly Hours:
40
Travel Required:
Less than 10%
Telecommuting Options:
Remote
Work Location:
USA DC Home Office (DCHOME)
Additional Work Locations:
We are GDIT. A global technology and professional services company that delivers consulting, technology and mission services to every major agency across the U.S. government, defense and intelligence community. Our 30,000 experts extract the power of technology to create immediate value and deliver solutions at the edge of innovation. We operate across 30 countries worldwide, offering leading capabilities in digital modernization, AI/ML, Cloud, Cyber and application development. Together with our clients, we strive to create a safer, smarter world by harnessing the power of deep expertise and advanced technology.
We connect people with the most impactful client missions, creating an unparalleled work experience that allows them to see their impact every day. We create opportunities for our people to lead and learn simultaneously. From securing our nation’s most sensitive systems, to enabling digital transformation and cloud adoption, our people are the ones who make change real.
GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class.
Show more
Show less","AWS, AWS Cloud Engineer, AWS Glue, AWS Lambda, AWS Athena, Python, JIRA, CI/CD, Analytical Thinking, Design, Documentations, Software Program, Coding, Testing, Debugging, Implementation, Technical Documentation, System Design, Program Design, Specification Writing, Technical Analysis, Quality Assurance, Peer Review, Technical Advice, Troubleshooting, Communication, Project Management, Time Estimation, Collaboration, Leadership, Problem Solving","aws, aws cloud engineer, aws glue, aws lambda, aws athena, python, jira, cicd, analytical thinking, design, documentations, software program, coding, testing, debugging, implementation, technical documentation, system design, program design, specification writing, technical analysis, quality assurance, peer review, technical advice, troubleshooting, communication, project management, time estimation, collaboration, leadership, problem solving","analytical thinking, aws, aws athena, aws cloud engineer, aws glue, aws lambda, cicd, coding, collaboration, communication, debugging, design, documentations, implementation, jira, leadership, peer review, problem solving, program design, project management, python, quality assurance, software program, specification writing, system design, technical advice, technical analysis, technical documentation, testing, time estimation, troubleshooting"
TTO 5/6 Database Engineer 2,Quevera,"Hanover, MD",https://www.linkedin.com/jobs/view/tto-5-6-database-engineer-2-at-quevera-3759994452,2023-12-17,Laurel,United States,Mid senior,Hybrid,"Job Description
Quevera is seeking a Database Engineer 2 to join an exciting, collaborative and innovative team. A place where you are positioned for More than Just a Job. Where leadership partners with you, seek to cultivate and support career development, encouraging growth from within while striving to foster a diverse and inclusive environment that improves individual and organizational performance.
Highlights Of Working For Quevera Are
Quevera employees voted Quevera as a TOP EMPLOYER in the Baltimore /DC area for 2020 (ranked #8 out of 150 companies) and 2022 (ranked #5 out of 150 companies).
Yearly $5,000 towards education/training.
Employees are in control of their career path through our Career Pathway Program.
Family and corporate events
Excellent health care coverage (100% paid premium option) and 401K matching (up to 4%).
And many more!
Q-Culture Video
Q-Careers
Duties And Responsibilities
The Database Engineer will provide technical expertise for database design, development, implementation, information storage and retrieval, data flow and analysis. Develop or maintain database structure to fit into the overall architecture of the system. Create new workflows to take over existing processes as needed as well as provide break/fix requests or updates. Lead development of database structures, database parser software, and database loading software. Support the development and test of various Python based ReST end points, microservices, and data model management capabilities utilizing Django and Flask frameworks to interact with data models such as mariaDB, MongoDB, and PostgreSQL and send data upon request, in JSON format, to UI front ends.
Required Experience
Python, Django or Flask, Database experience using MongoDB or MariaDB, ReST endpoint development, Micro service model
Desired Experience
Swagger, AWS, C2S or other cloud experience, Docker, Visual Studio Code or similar IDEs, JSON and/or XML serialization, Jira, Confluence, Git version control, Experience working in Agile environment
Quevera is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age or any other characteristic protected by law.
Show more
Show less","Python, Django, Flask, MongoDB, MariaDB, ReST, Microservices, Swagger, AWS, Docker, Visual Studio Code, Jira, Confluence, Git, Agile","python, django, flask, mongodb, mariadb, rest, microservices, swagger, aws, docker, visual studio code, jira, confluence, git, agile","agile, aws, confluence, django, docker, flask, git, jira, mariadb, microservices, mongodb, python, rest, swagger, visual studio code"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Washington, DC",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759711067,2023-12-17,Laurel,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Machine learning, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming, Relational databases, NoSQL databases, ETL, Data pipeline, NLP, Large language models, ML models, Automation workflows, Data enrichment, Data mining, Data classification, Retention","data engineering, machine learning, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming, relational databases, nosql databases, etl, data pipeline, nlp, large language models, ml models, automation workflows, data enrichment, data mining, data classification, retention","airflow, automation workflows, aws, azure, bash, data classification, data engineering, data enrichment, data mining, data pipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, large language models, machine learning, ml models, nlp, nosql databases, python, relational databases, retention, snowflake, spark, sparkstreaming, sql, storm"
Senior Data Platform Engineer,Motion Recruitment,"Washington, DC",https://www.linkedin.com/jobs/view/senior-data-platform-engineer-at-motion-recruitment-3779717412,2023-12-17,Laurel,United States,Mid senior,Hybrid,"The leader of audio entertainment and services is looking to fill a role is Washington, DC. The company is looking for a Senior Data Platform engineer. Ideally this candidate will have experience working with Python, AWS, ETL development, and Airflow.
Contract Duration: 10 months (possibility of converting to FT)
Required Skills & Experience
5+ years of professional experience
Python
AWS
ETL Development
Airflow
Desired Skills & Experience
Typescript
The Offer
Competitive Hourly Rate
You Will Receive The Following Benefits
Medical, Dental, and Vision Insurance
Vacation Time
Posted By:
Julianna Risi
Show more
Show less","Python, AWS, ETL Development, Apache Airflow, Typescript, Data Platform","python, aws, etl development, apache airflow, typescript, data platform","apache airflow, aws, data platform, etl development, python, typescript"
Lead Data Engineer,Total Wine & More,"Bethesda, MD",https://www.linkedin.com/jobs/view/lead-data-engineer-at-total-wine-more-3780556525,2023-12-17,Laurel,United States,Mid senior,Hybrid,"Lead Data Engineer
Total Wine & More is seeking a Lead Data Engineer with expertise in all aspects of data engineering, data management, and data analysis, to join our Data Services team in Bethesda, MD or Boca Raton, FL. We are embarking on a significant initiative to transform all aspects of data management services within Total Wine & More including enterprise data architecture, business intelligence, and data warehousing – leveraging the cloud to apply advanced analytics and data mining capabilities longer term. We are looking for an individual with experience in and who is passionate about data, data architecture, and data engineering to help execute the vision for data services at Total Wine & More. You will leverage advanced data engineering, cloud architecture, and programming skills to drive business value across all departments. You will work closely with the business, software development and support teams, infrastructure, and security staff and will enjoy learning and problem-solving in a fast-paced environment. You will report to the Principal Data Engineer.
You will:
In partnership with the Data Services leadership team, develop the vision for the next evolution of our Enterprise Data & Analytics Platform
Partner with Data Services product owners, data warehouse engineers, data scientists, and BI developers to support and deliver best-in-class reports, predictive models, datasets, and solutions
Create and manage batch and real time data pipelines
Manage and enhance our Kafka and Kubernetes implementations
Stay informed of new technologies and trends to guide the optimization of and continuous advancement of our platforms
Conduct exploratory data analysis to understand the patterns and potential business insights exhibited in the data
Be proactive and take ownership of the projects and tasks assigned
Coach junior team members and share knowledge with the broader team
Engage in implementing good security practices to handle sensitive information
You will have:
Bachelor's degree from four-year College or university in Computer Science, Technology, Business, or related field. Master’s or PhD preferred.
5+ years of experience in data engineering, data architecture, data warehousing or related fields
Proven track record in building impactful platforms and solutions using cloud-native tools
Experience using ETL/ELT tools – Airflow, Data Fusion, Dataprep, Informatica, Fivetran
Experience using Apache Kafka or Confluent Kafka or Google Pub/Sub
Experienced with general programing languages, such as Python, SQL, and SQL-like query languages for NoSQL databases
Experience using MapReduce or distributed computing technologies like PySpark, Apache Beam
Experience in leveraging the cloud in the development of data & analytics platforms
Familiarity with Dev-ops, Kubernetes, Docker
Experience with CI/CD including GitHub, Cloud Build, Jenkins
Passionate about data, data engineering, and platform development
Strong analytic skills, attention to detail, ability to multi-task, troubleshoot and problem solve along with the ability to make recommendations based on analysis are extremely important
Must possess excellent verbal, written, and presentation communication skills
We offer
Paid Time Off (PTO)
Generous store discounts
Health care plans (medical, prescription, dental, and vision)
401(k), HSA, FSA, Pretax commuter benefits
Disability & life insurance coverage
Paid parental leave
Pet insurance
Critical illness and accident insurance
Discounted home and auto insurance
College tuition assistance
Career development & product training
Consumer classes
& More!
Grow with us
Total Wine & More is the country's largest independent retailer of fine wine, beer and spirits, and we continue to grow our footprint year over year. Total Wine offers exciting and unique career opportunities across the country and in our corporate office. Our strength is our people. We have a commitment to training and career growth, all in an environment that values new ideas and teamwork. If you share our entrepreneurial spirit and a passion for providing best-in-class customer experience, take a moment to apply or learn more at
www.TotalWine.com/About-Us/Careers
!
Total Wine & More considers several factors when establishing compensation. Estimated salaries determined by third parties have not been validated by Total Wine & More. Total Wine & More is an equal opportunity employer and all qualified applicants will receive consideration for employment without discrimination based on race, color, religion, national origin, sex, sexual orientation, age, marital status, veteran status, disability, or any other characteristic protected by applicable law. Total Wine & More makes reasonable accommodations during all aspects of the employment process, including during the interview process. Total Wine & More is a Drug Free Workplace.
The information provided above indicates the general nature and level of work required of the position and is not a comprehensive list of all responsibilities or qualifications. Benefits list is only a highlight of some of the benefits offered to team members; eligibility for certain benefits apply.
About Total Wine & More
Total Wine & More is America‘s Wine Superstore®—the country‘s largest independent retailer of fine wine. We started in 1991 when brothers David and Robert Trone opened two wine stores in Delaware. Today, our typical store carries more than 8,000 wines from every wine-producing region in the world. In addition, Total Wine & More carries more than 2,500 beers, from America‘s most popular beers to hard-to-find microbrews and imports, and more than 3,000 different spirits from every price range and category.
Our strength is our people.  We are always looking for motivated, talented team members who are interested in working for a company with entrepreneurial spirit and a passion for providing best-in-class service. Our retail stores and corporate office (called the Store Support Center) provide opportunities for career growth and advancement. Offering competitive compensation and comprehensive benefits for qualifying positions, we strive to ensure that all Team Members feel that they are a part of the business, as they are valuable resources to our customers, co-workers, and communities.
Show more
Show less","Data Engineering, Data Management, Data Analysis, Cloud Architecture, Programming, Data Warehousing, ETL/ELT Tools, Apache Kafka, Confluent Kafka, Google Pub/Sub, Python, SQL, SQLlike Query Languages, MapReduce, PySpark, Apache Beam, Devops, Kubernetes, Docker, CI/CD, GitHub, Cloud Build, Jenkins","data engineering, data management, data analysis, cloud architecture, programming, data warehousing, etlelt tools, apache kafka, confluent kafka, google pubsub, python, sql, sqllike query languages, mapreduce, pyspark, apache beam, devops, kubernetes, docker, cicd, github, cloud build, jenkins","apache beam, apache kafka, cicd, cloud architecture, cloud build, confluent kafka, data engineering, data management, dataanalytics, datawarehouse, devops, docker, etlelt tools, github, google pubsub, jenkins, kubernetes, mapreduce, programming, python, spark, sql, sqllike query languages"
Senior Data Analyst - Hybrid,Stryker,"Portage, MI",https://www.linkedin.com/jobs/view/senior-data-analyst-hybrid-at-stryker-3707408724,2023-12-17,Climax,United States,Associate,Hybrid,"Why join Stryker?
We are proud to be named one of the World’s Best Workplaces and a Best Workplace for Diversity by Fortune Magazine! Learn more about our award-winning organization by visiting stryker.com
Our benefits include bonuses; commissions; healthcare; insurance benefits; retirement programs; stock based plans; family and parenting leaves; tuition reimbursement; wellness programs; onsite fitness centers and cafeterias; discount purchase programs; and service and performance awards – not to mention various social and recreational activities.
Know someone at Stryker?
Be sure to have them submit you as a referrral prior to applying for this position. Learn more about our employee referral program
Who We Want
Data translators.
Highly effective communicators who can transform data findings into recommendations to compose reports and executive level presentations.
Strategic thinkers.
People who enjoy analyzing data or trends for the purposes of planning, forecasting, advising, budgeting, reporting, or sales opportunities.
Collaborative partners.
People who build and leverage cross-functional relationships to bring together ideas, information, use cases, and industry analyses to develop best practices.
What You Will Do
We are looking for a highly motivated and experienced Senior PMO Analyst to join our team. The ideal candidate will have a strong understanding of project management principles and practices, as well as experience with data, analytics, and reporting.
As the Senior PMO Analyst, you will work with cross functional teams as you drive data integrity as you enter, review, and analyze data in the resource capacity platform. In this role, you will be responsible for supporting and maintaining the resource capacity platform associated with data collection, retrieval, accessibility, and usage for internal resource planning and activities. You will play a crucial role in ensuring data integrity, generating reports, and providing training to users. Additionally, you will have the opportunity to recommend changes in application development, maintenance, and system standards.
Other daily activities include but are not limited to:
Support and maintain resource capacity platform/tool for data collection and usage.
Partner with cross-functional teams SME’s on entering and ensuring clean and consistent data inputs into established tables, fields, and system databases.
Acts as the PMO point of contact for cross-functional users for all platform questions, request, and modifications.
Responsible for processing and validating platform changes in accordance with platform guidance and governance.
Partner with Lead PMO Analyst triage issues and determine if root cause is process, people, or system.
Set up user access and provide training on platform usage.
Create and maintain seamless end-user experience for all reporting needs.
Partner with Lead PMO Analyst to drive visibility, understanding, trust, and improvements within the resource capacity platform and reports.
Partner with cross-functional users to identify and resolve data gaps and issues.
Partner with the Lead PMO Analyst to ensure business readiness for minor and major releases and updates.
Build and produce reports using query and flexible reporting tools to meet the requirements of cross-functional users and resource managers.
Investigate reporting issues and provide expertise to resolve and follow-up in a timely manner.
Identify opportunities to gain efficiencies, automate, and improve data quality.
In addition to the activities listed above, the Senior PMO Analyst must be able to build strong relationships with stakeholders, communicate effectively with stakeholders, stay calm under pressure, think critically and make sound decisions.
The ideal candidate will also be highly motivated and experienced, passionate about data, analytics, project portfolio management software, and project management, able to work independently with minimal supervision, be able to understand and apply procedures and concepts of their discipline, pay attention to detail in making evaluative judgments based on the analysis of factual information.
What You Will Need
Bachelor's Degree is required
Minimum of 2 years of professional experience
Strong analytical and problem-solving skills.
Experience with data analysis tools such as Excel, PowerBI and Python.
Ability to communicate data insights to stakeholders.
Contribute to the development of the resource capacity platform, recommend improvements through data and reporting.
Assist in the development of the PMO tools and reports. Alert Lead PMO Analyst of variances or gaps.
Complete assigned tasks according to established timeline. Assist in the development of the resource capacity platform rollout and training timelines.
Participate in the development of PMO tools that support data quality.
Partner in the establishment of risk management plans and support effective risk analysis and risk mitigation planning for the PMO.
Provide support on rescheduling, and resource allocation in the program plan and support program gating processes.
Establish and maintain partnerships with internal and external PMO stakeholders.
Produce weekly and monthly PMO metric reports using dashboards and metrics to measure success.
Contributes data analysis expertise for short- and long-term strategic planning process activities.
Leads projects. May support a high complexity program.
Builds productive working relationships with departments that are key partners (e.g., R&D and Marketing).
About Stryker
Stryker is one of the world’s leading medical technology companies and, together with our customers, is driven to make healthcare better.
The company offers innovative products and services in Medical and Surgical, Neurotechnology, Orthopedics, and Spine that help improve patient and healthcare outcomes. Alongside its customers around the world, Stryker impacts more than 100 million patients annually.
More information is available at stryker.com
R503899_2
Show more
Show less","Data analysis, Project management, Reporting, SQL, PowerBI, Python, Excel, PMO tools, Jira, Risk management, Strategic planning, Data integrity, Resource capacity platform, Collaborative partners, Analytical skills, Problemsolving skills, Communication skills, Attention to detail, Time management skills, Leadership skills","data analysis, project management, reporting, sql, powerbi, python, excel, pmo tools, jira, risk management, strategic planning, data integrity, resource capacity platform, collaborative partners, analytical skills, problemsolving skills, communication skills, attention to detail, time management skills, leadership skills","analytical skills, attention to detail, collaborative partners, communication skills, data integrity, dataanalytics, excel, jira, leadership skills, pmo tools, powerbi, problemsolving skills, project management, python, reporting, resource capacity platform, risk management, sql, strategic planning, time management skills"
Analyst II Data Operations,Kellanova,"Battle Creek, MI",https://www.linkedin.com/jobs/view/analyst-ii-data-operations-at-kellanova-3770748488,2023-12-17,Climax,United States,Mid senior,Remote,"In this role as
Analyst II, Data Operations
, you will be a part of the Data Operations and Governance team, which works to ensure that all Kellanova data assets are credible and available with development of policy and processes, measuring compliance, and enforcing standards. Bring your owner’s mindset and play a critical role to support the efficiency of the operations team. In this role, you will work cross functionally to play a critical part in ensuring that our Master Data is certified, reliable, and easily accessible. This creates a strong foundation for agile analytics to harvest insights, which drive sustainable growth and profit. Your attention to detail, and pride for systems and data integrity will make you a great fit for our company and is a key ingredient to your success.
A Taste Of What You’ll Be Doing
Master Data Expertise-We are looking for someone who has a passion for data accuracy! Leveraging your strong attention to detail, you will be responsible for managing Master Data in SAP relative to all aspects of product and customer hierarchy setup and maintenance. Through accurate and timely setup and rapidly changing priorities, you will enable and maximize business effectiveness across the company. Examples of this type of work includes (but is not limited to): Updating customer/product hierarchy, creating source files, updating payment terms, shipping locations
Influencing Systems Consistency and Accuracy-You will be a significant contributor to ensuring our data requirements are met. Understanding WinShuttle, SAP and digital concierge and other technical capabilities will enable the team to drive fast, accurate setup. This position is heavily relied on to meet all aspects of SOX 404 requirements.
Communication –You will work closely with your teammates to ensure your success. Through effective communication and collaboration, you will be adding value to ensure we meet our business goals.
Your Recipe for Success
Bachelor’s Degree with some related work experience
Experience focused on business and systems related to SAP Master Data
Strong attention for detail
Strong verbal and written communication skills to convey complex and detailed information to different audiences.
Experience in the Data Governance space
Systems knowledge in technologies such as SAP, Winshuttle and Excel
What’s Next
After you apply, your application will be reviewed by a real recruiter–not a bot. This means it could take us a little while to get back with you so watch your inbox for updates. In the meantime, visit our
How We Hire page
to get insights into our hiring process and how to best prepare for a Kellanova interview.
If we can help you with a reasonable accommodation throughout the application or hiring process, please emailUSA.Recruitment@Kellanova.com
.
About Kellanova
Kellanova is a leading company in global snacking, international cereal and noodles, plant-based foods, and North America frozen breakfast, and a portfolio of iconic, world-class brands, including Pringles, Cheez-It, Pop-Tarts, Kellogg’s Rice Krispies Treats, MorningStar Farms, Incogmeato, Gardenburger, Nutri-Grain, RXBAR, and Eggo. We also steward a suite of beloved international cereal brands, including Kellogg’s, Frosties, Zucaritas, Special K, Krave, Miel Pops, Coco Pops, and Crunchy Nut, among others.
At Kellanova, we are committed to Equity, Diversity, and Inclusion (ED &I), uplifting each other and embracing our differences to achieve our common goals. Our focus on ED &I enables us to build a culture where all employees are inspired to share their passion, talents, ideas, and can bring their authentic selves to work. Learn morehere.
We’re proud to offer industry competitive
Total Health benefits
(Physical, Financial, Emotional, and Social) that vary depending on region and type of role. Be sure to ask your recruiter for more information!
The Finer Print
Kellanova is an Equal Opportunity Employer that strives to provide an inclusive work environment, a seat for everyone at the table, and embraces the diverse talent of its people. All qualified applicants will receive consideration for employment without regard to race, color, ethnicity, disability, religion, national origin, gender, gender identity, gender expression, marital status, sexual orientation, age, protected veteran status, or any other characteristic protected by law. For more information regarding our efforts to advance Equity, Diversity & Inclusion, please visit our websitehere.
Ready to Taste the Future of Food?
Kellanova Recruitment
Show more
Show less","SAP, WinShuttle, Excel, Data Governance, Master Data Management, SOX 404 compliance, Data integrity, Data accuracy","sap, winshuttle, excel, data governance, master data management, sox 404 compliance, data integrity, data accuracy","data accuracy, data governance, data integrity, excel, master data management, sap, sox 404 compliance, winshuttle"
Senior Cloud Data Engineer,BDO USA,"Kalamazoo, MI",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765473102,2023-12-17,Climax,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics Solutions, SQL, Data Definition Language (DDL), Data Manipulation Language (DML), Views, Functions, Stored Procedures, Performance Tuning, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps Deployment Technologies, Linux, Data Lake Medallion Architecture, Batch and/or Streaming Data Ingestion, AI Algorithms/Machine Learning, Automation Tools, Computer Vision Based AI Technologies, DataOps, Purview, Delta, Pandas, Spark SQL, Microsoft Fabric, dbt, Terraform, Bicep, Glue, Star Schema, Data Modeling, SSIS, SSAS, SSRS, PySpark, Linux, Terraform, Bicep","data analytics, business intelligence, artificial intelligence, application development, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics solutions, sql, data definition language ddl, data manipulation language dml, views, functions, stored procedures, performance tuning, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops deployment technologies, linux, data lake medallion architecture, batch andor streaming data ingestion, ai algorithmsmachine learning, automation tools, computer vision based ai technologies, dataops, purview, delta, pandas, spark sql, microsoft fabric, dbt, terraform, bicep, glue, star schema, data modeling, ssis, ssas, ssrs, pyspark, linux, terraform, bicep","ai algorithmsmachine learning, application development, artificial intelligence, automation tools, azure analysis services, batch andor streaming data ingestion, bicep, business intelligence, c, cloud data analytics solutions, computer vision based ai technologies, data definition language ddl, data lake medallion architecture, data manipulation language dml, dataanalytics, datamodeling, dataops, datawarehouse, dbt, delta, devops deployment technologies, functions, git, glue, java, linux, microsoft fabric, pandas, performance tuning, powerbi, purview, python, scala, semantic model definition, spark, spark sql, sql, ssas, ssis, ssrs, star schema, star schema construction, stored procedures, tabular modeling, terraform, views"
Junior Data Analyst Electronic Devices,Ernest Gordon Recruitment,"Stoke-on-Trent, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-analyst-electronic-devices-at-ernest-gordon-recruitment-3768235228,2023-12-17,Stoke-on-Trent, United Kingdom,Mid senior,Onsite,"Junior Data Analyst (Electronic Devices)
£25,000 - £30,000 + Flexitime + Training + Progression + Benefits
Llanelli, Wales
Are you a Junior Data Analyst looking to progress to Digital Forensics in a role offering mentoring on highly specialised software for a prestigious company working very closely with police forces nationwide?
In this role you will be forensically analysing digital electronic devices to aid either corporate, police or private investigations. Your responsibility will be to work very closely with other technician to uncover evidence and testify findings in court or at tribunals.
With ambitious growth and plans to diversify into consulting, this is a great opportunity to play a pivotal role in this companies success. They pride themselves with being the best in the business and are the constabularies preferred supplier for digital forensic services.
This role would suit a Junior Data Analyst looking to progress to Digital Forensics and is looking for a challenging but rewarding role helping to resolve criminal investigation.
This Role
Preserving, processing and analysing data using specialist forensic tools
Producing written reports and presenting findings in court when required
Review data sets against a remit
The Person
1 year + experience as a Data Analyst
Wants to Progress Digital Forensics
Job Refence
: BBBH10735
Digital Forensics, Digital, Forensics, Analyst, Forensic Analyst, Lab, Laboratory, Technician, Expert, ISO17025, Crime, Criminal, Computer Science, Gorseinon, Swansea, Dyfed, Llanelli
Electronics, Electrical, Mechanical, Engineer, Electronic, Trainee, Junior, Apprentice, Technician, Service, Mobile, Training, Development, Printers, Birmingham, West MidlandsIf you're interested in this role, click '
apply now
' to forward an up-to-date copy of your CV.We are an equal opportunities employer and welcome applications from all suitable candidates. The salary advertised is a guideline for this position. The offered renumeration will be dependent on the extent of your experience, qualifications, and skill set.Ernest Gordon Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workers. By applying for this job, you accept the T&C's, Privacy Policy and Disclaimers which can be found at our website.
Show more
Show less","Data Analysis, Digital Forensics, Forensic Software, Data Preservation, Data Processing, Data Analysis, Report Writing, Court Presentation, Data Review, ISO17025, Crime Analysis, Criminal Investigation, Computer Science","data analysis, digital forensics, forensic software, data preservation, data processing, data analysis, report writing, court presentation, data review, iso17025, crime analysis, criminal investigation, computer science","computer science, court presentation, crime analysis, criminal investigation, data preservation, data processing, data review, dataanalytics, digital forensics, forensic software, iso17025, report writing"
Senior Data Engineer,Nigel Frank International,"Stafford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-nigel-frank-international-3731260637,2023-12-17,Stoke-on-Trent, United Kingdom,Mid senior,Onsite,"I am working with a market leading manufacturer of specialist medical equipment who are looking for a Senior Data Engineer at the start of a number of exciting data driven projects. You would be joining at the start of a greenfield project where you will be responsible for the data migration to the brand new Azure Data Platform!
The overall goal of the business and ultimately this role is to deliver data to healthcare professionals to aid with rehabilitation. You will join a close-knit team, with talented professionals who specialise in a number of areas including software development and data science. You will become the expert for this team on all things data engineering and will take the lead on the creation, development and maintenance of the Azure data platform.
As part of this role, you will be responsible for some of the following areas.
Design, maintain and optimise on premise database solutions.
Develop and maintain robust ETL pipelines.
Work with other specialists within the team to make technical decisions that will benefit the overall business.
Take the lead of migration of data to the new Azure data platform.
This is a salaried role paying up to £65,000 per annum depending on experience and a company benefits package. This role would be largely a remote opportunity with occasional visits to the office in Stafford. These visits would be as and when needed in line with project requirements.
To be successful in the role you will have.
Strong knowledge of Databricks for data ingestion and transformation.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Strong coding experience with languages such as Python or C#.
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Databricks, Azure Data Factory, Synapse, Azure Data Lake, Python, C#, ETL, Azure Data Platform, Data engineering, SQL","databricks, azure data factory, synapse, azure data lake, python, c, etl, azure data platform, data engineering, sql","azure data factory, azure data lake, azure data platform, c, data engineering, databricks, etl, python, sql, synapse"
"Senior Cloud Data Engineer - GBP70,000",Nigel Frank International,"Stoke-on-Trent, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-cloud-data-engineer-gbp70-000-at-nigel-frank-international-3763118452,2023-12-17,Stoke-on-Trent, United Kingdom,Mid senior,Onsite,"Senior Cloud Data Engineer - £70,000
I am working with a market leading manufacturer of specialist medical equipment who are looking for a Senior Cloud Data Engineer at the start of a number of exciting data driven projects. You would be joining at the start of a greenfield project where you will be responsible for the data migration, development and maintenance of the brand new Azure Data Platform!
The overall goal of the business and ultimately this role is to deliver data to healthcare professionals to aid with rehabilitation of patients. You will join a close-knit team, with talented professionals who specialise in a number of areas including software development and data science. You will be the in house expert for the team on all things data engineering and will take the lead on the creation, development and maintenance of the Azure data platform.
As part of this role, you will be responsible for some of the following areas.
Design, maintain and optimize both on premise and cloud based database solutions.
Develop and maintain robust ETL pipelines.
Work with other specialists within the team to make technical decisions that will benefit the overall business.
Take the lead of migration of data to the new Azure data platform.
This is a salaried role paying up to £70,000 per annum depending on experience and a company benefits package. This role would be largely a remote opportunity with occasional visits to the office in Stafford. These visits would be as and when needed in line with project requirements.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Strong coding experience with languages such as Python or C#.
Experience working as a DBA or completing database administration tasks
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure, Azure Data Factory, Azure Synapse, Azure Data Lake, ETL, Python, C#, DBA, Database Administration","azure, azure data factory, azure synapse, azure data lake, etl, python, c, dba, database administration","azure, azure data factory, azure data lake, azure synapse, c, database administration, dba, etl, python"
Product Data Analyst - Telematics,Radius,"Crewe, England, United Kingdom",https://uk.linkedin.com/jobs/view/product-data-analyst-telematics-at-radius-3734563018,2023-12-17,Stoke-on-Trent, United Kingdom,Mid senior,Onsite,"Heard of us?
We’re an ambitious, forward-thinking global business who build transformative solutions for our customers to deliver best-in-class sustainable
mobility
,
connectivity
and
technology
solutions. We support our customers with a range of products and services to meet their needs.
Since 1990 our ambition has never wavered. From humble beginnings, our vision and drive has seen us venture into new markets with confidence and stay ahead of market trends. Our mission is to help businesses of all sizes adapt to the future and take advantage of the opportunities that change brings. Sustainability is at the core of our offering. With our leading e-mobility solutions, we’re committed to guiding businesses through the energy transition, building solutions for a more sustainable, connected future.
We have over 300,000 customers worldwide, over 50 offices across Europe, North America, Asia, Australasia and Africa, and over 2,800 staff primarily in the UK and Ireland.
The role…
The role of
Telematics Product Data Analyst
is really important to us.
In this position, you will be responsible for collecting, curating and analysing data on our core Vehicle Telematics platforms.
By utilising your all-round data experience, you will help us to better understand how our customers use our telematics products and how we should develop them in the future.
The continued growth of our Telematics business will be key to significant plans that we have over the next 3-5 years, so your work will provide significant business impact.
The role reports directly to our Chief Data Officer (CDO) and is based from our head office in
Crewe, Cheshire
.
Your responsibilities day to day will be…
Understanding our customers and Telematics products and how they interact, through our data.
Applying this understanding and knowledge of data to help product and service teams.
Designing key metrics to measure different aspects of our Telematics platform.
Building data pipelines and storage platforms for collated data.
Creating and maintaining new aggregated views and tables to simplify data querying.
Maintaining and creating new dashboards to track metrics and visualise insights.
Promoting data literacy across the product teams, organising and holding workshops.
Customising data collection: develop advanced tracking solutions using custom metrics to capture specific user behaviours or events that align with business objectives.
Data integrity & quality assurance: perform quality checks on collected data for accuracy.
Delivering key product and customer insights to the business to drive decision-making.
What do we expect of you?
You will bring previous experience in an analytical role, creating impactful solutions in a commercial / product led business.
Your background should include:
Strong experience with SQL, Python or other programming languages.
Proven ability to identify ways to improve data reliability, efficiency, and quality.
Evidence of strong mathematical and statistics knowledge.
Experience of ETL/ Pipeline building.
Ability to represent data when required via reporting and visualization.
Experience of Google Analytics (advantageous, not essential).
What can you expect of us?
Your impact on Radius will be rewarded with a competitive salary plus the opportunity to develop and progress your career in many directions.
You’ll also have access to our competitive benefits package.
This includes core company paid benefits such as a defined contribution pension scheme, performance bonus, enhanced maternity pay, cycle to work and electric car scheme, and fuel payment card scheme, plus a whole host of options to support your physical, mental, and financial wellbeing.
And of course, you will be part of a modern purpose-built space in Crewe that will provide you with an outstanding working environment (complete with onsite gym, café, games rooms and coffee bar style break out rooms).
Still curious?
If you feel we are a good match for each other, you can apply online now!
Radius is an equal opportunities employer. We are committed to welcome people regardless of age, disability, gender identity, race, faith or belief, sexual orientation or socioeconomic background.
We do not accept speculative agency CVs. Any CV received by Radius will be treated as a gift and not eligible for an agency fee. PSL agencies should only send CVs if authorised to do so by the Radius Talent Team.
Show more
Show less","Telematics, Data analysis, Data visualization, SQL, Python, Programming languages, Data reliability, Data efficiency, Data quality, Mathematical knowledge, Statistics, ETL/ Pipeline building, Reporting, Visualization, Google Analytics","telematics, data analysis, data visualization, sql, python, programming languages, data reliability, data efficiency, data quality, mathematical knowledge, statistics, etl pipeline building, reporting, visualization, google analytics","data efficiency, data quality, data reliability, dataanalytics, etl pipeline building, google analytics, mathematical knowledge, programming languages, python, reporting, sql, statistics, telematics, visualization"
Senior Data Engineer,Attractions.io,"Uttoxeter, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-attractions-io-3746567683,2023-12-17,Stoke-on-Trent, United Kingdom,Mid senior,Remote,"As a Senior Data Engineer, you will be responsible for designing, maintaining, and optimising our data systems to drive business value and play a key role in helping build the connected guest experience of the future at theme parks, zoos, resorts and other out-of-home venues.
Your expertise in data engineering will be critical to our growth and success, so we are excited to have you onboard as we expand and innovate in our industry. Your skills will help us build modern data architectures that will enable us to provide robust products that cater to our evolving client base.
Responsibilities
Create solutions to provide data in various forms to our wider platform, whether via databases, APIs, or streaming
Design and architect data solutions, ensuring we use the best approach to benefit from the data we hold
Supporting and facilitating data modelling and exploration
Collaborate with data scientists and software developers to ensure seamless integration of ML processes
Ensure data quality, security, and privacy are preserved in all data processing and storage processes
Be proactive in identifying and solving data processing issues
Continuously optimise our systems to enhance data processing and storage speed and accuracy
Requirements
5+ years experience in data engineering and related roles
Experience in building and maintaining data platforms with cloud-based architectures
Experience in AWS technologies such as Aurora, S3, Athena, Redshift, Kinesis
Experience in Data Warehouse and Data Lake design and implementation
Strong programming skills in Python and SQL
Strong analytical and problem-solving skills, with the ability to work with large and complex datasets
Familiarity with agile working practices
Understanding of software versioning and CI/CD pipelines
Excellent communication and collaboration skills
Knowledge of MLOps desirable
Benefits
Competitive salary and share option scheme
100% remote working with a remote work allowance
Flexible working hours
33 days paid holiday
High-end equipment
Quarterly company off-sites at fantastic attractions (our customers)
Show more
Show less","Data engineering, Data platforms, Cloud architectures, AWS technologies, Aurora, S3, Athena, Redshift, Kinesis, Data warehouse design, Data lake design, Python, SQL, Data modelling, Data exploration, Machine learning, Data quality, Data security, Data privacy, Data processing, Data storage, Agile working practices, Software versioning, CI/CD pipelines, Communication, Collaboration, MLOps","data engineering, data platforms, cloud architectures, aws technologies, aurora, s3, athena, redshift, kinesis, data warehouse design, data lake design, python, sql, data modelling, data exploration, machine learning, data quality, data security, data privacy, data processing, data storage, agile working practices, software versioning, cicd pipelines, communication, collaboration, mlops","agile working practices, athena, aurora, aws technologies, cicd pipelines, cloud architectures, collaboration, communication, data engineering, data exploration, data lake design, data modelling, data platforms, data privacy, data processing, data quality, data security, data storage, data warehouse design, kinesis, machine learning, mlops, python, redshift, s3, software versioning, sql"
Data Engineer - Telematics,Radius,"Crewe, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-telematics-at-radius-3672269183,2023-12-17,Stoke-on-Trent, United Kingdom,Mid senior,Hybrid,"Heard of us?
We’re an ambitious, forward-thinking global business who build transformative solutions for our customers to deliver best-in-class
fleet and connectivity solutions
.
We support our customers with a range of products and services to meet their needs.
Since 1990 our ambition has never wavered. From humble beginnings, our vision and drive has seen us venture into new markets with confidence and stay ahead of market trends. Our mission is to help businesses of all sizes adapt to the future and take advantage of the opportunities that change brings. Sustainability is at the core of our offering. With our leading e-mobility solutions, we’re committed to guiding businesses through the energy transition, building solutions for a more sustainable, connected future.
We have over 300,000 customers, over 50 offices worldwide, and over 2,800 staff primarily in the UK and Ireland.
The role …
The role of
Data Engineer
in our
telematics
business is important to us.
At Radius, we deal with a number of very large data sets, the largest of which is our detailed telemetry for the vehicles we track.
These vehicles cover hundreds of millions of miles per month and we record every significant piece of driver behaviour data we can capture during those journeys, as well as recording the location, speed and heading of the vehicle at regular intervals.
This data set created the need for new tools, practices, and processes. We are making use of data engineering, data analysis, data science, machine learning and interactive visualisations to gather insights from this data, to build better products and innovate for our customers.
We also use these same technologies to develop our product strategy internally. With numerous telematics platforms and back office systems from our acquisitions, we also have a need to create a single view across our internal process and customer lifecycle.
Technologies we use for our work with data include:
Cassandra
Apache Spark
ActiveMQ
Python
Pandas
AWS S3, Redshift, Lambda
Tableau
Key to our success in applying these technologies to our rapidly growing, innovative organisation, are the people we employ.
The role reports to our Chief Data Officer and is based from our head office in
Crewe, Cheshire.
What would your day to day look like?
Delivery of data pipelines to enable new data-driven products
Maintenance of existing data pipelines and jobs
Scaling of data platform to support growing data footprint of the business
Evaluation of new data sources and mapping onto existing data lake
Using applicable machine learning techniques to implement new data products and dashboards
Data analysis for specific purposes, as required to guide the business
What do we expect of you?
You will have a strong and proven commercial experience in a Data Engineering role, working with a similar tech stack to ours.
Your background will include working in a mid-large sized organisation, working with very large data sets.
What can you expect of us?
Your impact on Radius will be rewarded with the opportunity to develop and progress your career in many directions.
Not only will you have the chance to further your career development within Radius, you’ll have access to our competitive reward and benefits package.
This includes core company paid benefits such as a defined contribution pension scheme, performance bonus, enhanced maternity pay, cycle to work and electric car schemes, plus a whole host of options to support your physical, mental, and financial wellbeing.
And of course, you will be part of modern purpose-built space in Crewe that will provide you with an outstanding working environment (complete with onsite gym, canteen, games rooms and coffee bar style break out rooms).
Still curious?
If you feel we are a good match for each other, you can apply online now!
Radius is an equal opportunities employer. We are committed to welcome people regardless of age, disability, gender identity, race, faith or belief, sexual orientation or socioeconomic background.
We do not accept speculative agency CVs. Any CV received by Radius will be treated as a gift and not eligible for an agency fee. PSL agencies should only send CVs if authorised to do so by the Radius Talent Team.
Show more
Show less","Cassandra, Apache Spark, ActiveMQ, Python, Pandas, AWS S3, Redshift, Lambda, Tableau, Data Engineering, Data Analysis, Data Science, Machine Learning, Visualisation, Data Pipelines, Data Platforms, Data Sources, Data Lakes, Performance Bonus","cassandra, apache spark, activemq, python, pandas, aws s3, redshift, lambda, tableau, data engineering, data analysis, data science, machine learning, visualisation, data pipelines, data platforms, data sources, data lakes, performance bonus","activemq, apache spark, aws s3, cassandra, data engineering, data lakes, data platforms, data science, data sources, dataanalytics, datapipeline, lambda, machine learning, pandas, performance bonus, python, redshift, tableau, visualisation"
Data Analyst,Searchability,"Cannock, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-searchability-3775263817,2023-12-17,Stoke-on-Trent, United Kingdom,Mid senior,Hybrid,"Data Analyst
Location: Cannock, Staffordshire
Salary: £30,000-£40,000
Hybrid/On-Site
About The Company:
We supply data secure refurbishment and repair services for trad-in, consumer returns, excess consumer electronics. Working with some of the largest IT companies and manufacturers to provide a service that goes under the radar, avoiding any market conflict.
Role overview:
The ideal candidate will have a strong analytical mindset, excellent problem-solving skills, and a passion for turning data into actionable insights. As a Data Analyst, you will play a crucial role in analysing and interpreting complex data sets to inform strategic business decisions and drive continuous improvement.
Role requirements:
Collect and analyse large datasets to identify trends, patterns, and insights.
Develop and maintain reports and dashboards to communicate key performance indicators and metrics.
Collaborate with cross-functional teams to understand business requirements and provide data-driven solutions.
Clean, process, and validate data to ensure accuracy and reliability.
Conduct statistical analyses to support decision-making processes.
Stay updated on industry trends and emerging technologies to recommend innovative approaches to data analysis.
Communicate findings and insights effectively to both technical and non-technical stakeholders through presentations and reports.
Show more
Show less","Data Analysis, Data Interpretation, Statistical Analysis, Data Visualization, Report Generation, Data Cleansing, Data Validation, Business Intelligence, DataDriven Decision Making, Communication, Presentation Skills, Industry Knowledge, Emerging Technologies","data analysis, data interpretation, statistical analysis, data visualization, report generation, data cleansing, data validation, business intelligence, datadriven decision making, communication, presentation skills, industry knowledge, emerging technologies","business intelligence, communication, data interpretation, data validation, dataanalytics, datacleaning, datadriven decision making, emerging technologies, industry knowledge, presentation skills, report generation, statistical analysis, visualization"
"Senior Data Engineer - Hybrid - Up to GBP65,000",Nigel Frank International,"Uttoxeter, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-hybrid-up-to-gbp65-000-at-nigel-frank-international-3734505288,2023-12-17,Stoke-on-Trent, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer - Hybrid - Up to £65,000
I'm currently working with a market leading manufacturer of specialist medical equipment who are working with groundbreaking technology to deliver data to health care professionals. They are looking for a Senior Data Engineer to join at the start of an exciting greenfield project where you will be responsible for the data migration to their brand new azure data platform.
In this role you will design, maintain and optimise on premise database solutions. You will be developing and maintaining ETL pipelines. You will also take lead of the migration of data to the new Azure Data Platform. This role will involve working with other specialists within the team to make technical decisions that will benefit the overall business.
This is a salaried position of up to £65,000 depending on experience plus a company benefits package. They pride themselves on being an employee orientated business and as such, they strongly believe in providing a healthy work life balance. This role offers hybrid working with occasional travel to their Stafford office on average of 4 times per month.
I am looking for...
Strong experience with Databricks for data ingestion and transformation
Experience working with the Azure Stack - Data factory, Synapse, Data Lake
Experience designing and implementing ETL solutions
Strong coding experience with coding languages such as Python or C#
This is a unique opportunity for a Senior Data Engineer to work on an exciting greenfield project and be supported in achieving you career goals and aspirations.
If this is of interest then get in touch ASAP. Send across your CV to d.moore1@nigelfrank.com or alternatively, give me a call on 0191 338 7577
Key Skills: Microsoft, Azure, Databricks, Data Factory, Data Lake, SQL, Python, C#, ETL, Engineer, Engineering
Show more
Show less","Data Engineer, Data Migration, Data Platform, Azure Data Platform, Database Solutions, ETL Pipelines, Data Factory, Data Lake, Synapse, Python, C#, SQL, Databricks","data engineer, data migration, data platform, azure data platform, database solutions, etl pipelines, data factory, data lake, synapse, python, c, sql, databricks","azure data platform, c, data factory, data lake, data migration, data platform, database solutions, databricks, dataengineering, etl pipelines, python, sql, synapse"
Lead Data Engineer - Telematics,Radius,"Crewe, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-telematics-at-radius-3762802857,2023-12-17,Stoke-on-Trent, United Kingdom,Mid senior,Hybrid,"Heard of us?
We’re an ambitious, forward-thinking global business who build transformative solutions for our customers to deliver best-in-class
fleet and connectivity solutions
.
We support our customers with a range of products and services to meet their needs.
Since 1990 our ambition has never wavered. From humble beginnings, our vision and drive has seen us venture into new markets with confidence and stay ahead of market trends. Our mission is to help businesses of all sizes adapt to the future and take advantage of the opportunities that change brings. Sustainability is at the core of our offering. With our leading e-mobility solutions, we’re committed to guiding businesses through the energy transition, building solutions for a more sustainable, connected future.
We have over 300,000 customers, over 50 offices worldwide, and over 2,800 staff primarily in the UK and Ireland.
The role …
The role of
Telematics Lead Data Engineer
in our
telematics
business is important to us.
At Radius, we deal with a number of very large data sets, the largest of which is our detailed telemetry for the vehicles we track.
These vehicles cover hundreds of millions of miles per month and we record every significant piece of driver behaviour data we can capture during those journeys, as well as recording the location, speed and heading of the vehicle at regular intervals.
This data set created the need for new tools, practices, and processes. We are making use of data engineering, data analysis, data science, machine learning and interactive visualisations to gather insights from this data, to build better products and innovate for our customers.
We also use these same technologies to develop our product strategy internally. With numerous telematics platforms and back office systems from our acquisitions, we also have a need to create a single view across our internal process and customer lifecycle.
Technologies we use for our work with data include:
Cassandra
Apache Spark
ActiveMQ
Python
Pandas
AWS S3, Redshift, Lambda
Tableau
Key to our success in applying these technologies to our rapidly growing, innovative organisation, are the people we employ.
The role reports to our Chief Data Officer and is based from our head office in
Crewe, Cheshire.
What would your day to day look like?
Delivery of data pipelines to enable new data-driven products
Maintenance of existing data pipelines and jobs
Scaling of data platform to support growing data footprint of the business
Evaluation of new data sources and mapping onto existing data lake
Using applicable machine learning techniques to implement new data products and dashboards
Data analysis for specific purposes, as required to guide the business
What do we expect of you?
You will have a strong and proven commercial experience in a Senior / Lead Data Engineering role, working with a similar tech stack to ours.
Your background will include working in a mid-large sized organisation, working with very large data sets.
What can you expect of us?
Your impact on Radius will be rewarded with the opportunity to develop and progress your career in many directions.
Not only will you have the chance to further your career development within Radius, you’ll have access to our competitive reward and benefits package.
This includes core company paid benefits such as a defined contribution pension scheme, performance bonus, enhanced maternity pay, cycle to work and electric car schemes, plus a whole host of options to support your physical, mental, and financial wellbeing.
And of course, you will be part of modern purpose-built space in Crewe that will provide you with an outstanding working environment (complete with onsite gym, canteen, games rooms and coffee bar style break out rooms).
Still curious?
If you feel we are a good match for each other, you can apply online now!
Radius is an equal opportunities employer. We are committed to welcome people regardless of age, disability, gender identity, race, faith or belief, sexual orientation or socioeconomic background.
We do not accept speculative agency CVs. Any CV received by Radius will be treated as a gift and not eligible for an agency fee. PSL agencies should only send CVs if authorised to do so by the Radius Talent Team.
Show more
Show less","Telematics, Data Engineering, Data Analysis, Data Science, Machine Learning, Visualizations, Cassandra, Apache Spark, ActiveMQ, Python, Pandas, AWS S3, Redshift, Lambda, Tableau","telematics, data engineering, data analysis, data science, machine learning, visualizations, cassandra, apache spark, activemq, python, pandas, aws s3, redshift, lambda, tableau","activemq, apache spark, aws s3, cassandra, data engineering, data science, dataanalytics, lambda, machine learning, pandas, python, redshift, tableau, telematics, visualizations"
"Senior Data Engineer - Hybrid - Up to GBP65,000",Nigel Frank International,"Stafford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-hybrid-up-to-gbp65-000-at-nigel-frank-international-3734505289,2023-12-17,Stoke-on-Trent, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer - Hybrid - Up to £65,000
I'm currently working with a market leading manufacturer of specialist medical equipment who are working with groundbreaking technology to deliver data to health care professionals. They are looking for a Senior Data Engineer to join at the start of an exciting greenfield project where you will be responsible for the data migration to their brand new azure data platform.
In this role you will design, maintain and optimise on premise database solutions. You will be developing and maintaining ETL pipelines. You will also take lead of the migration of data to the new Azure Data Platform. This role will involve working with other specialists within the team to make technical decisions that will benefit the overall business.
This is a salaried position of up to £65,000 depending on experience plus a company benefits package. They pride themselves on being an employee orientated business and as such, they strongly believe in providing a healthy work life balance. This role offers hybrid working with occasional travel to their Stafford office on average of 4 times per month.
I am looking for...
Strong experience with Databricks for data ingestion and transformation
Experience working with the Azure Stack - Data factory, Synapse, Data Lake
Experience designing and implementing ETL solutions
Strong coding experience with coding languages such as Python or C#
This is a unique opportunity for a Senior Data Engineer to work on an exciting greenfield project and be supported in achieving you career goals and aspirations.
If this is of interest then get in touch ASAP. Send across your CV to d.moore1@nigelfrank.com or alternatively, give me a call on 0191 338 7577
Key Skills: Microsoft, Azure, Databricks, Data Factory, Data Lake, SQL, Python, C#, ETL, Engineer, Engineering
Show more
Show less","Databricks, Azure, Microsoft, Data Factory, Data Lake, SQL, Python, C#, ETL, Engineering","databricks, azure, microsoft, data factory, data lake, sql, python, c, etl, engineering","azure, c, data factory, data lake, databricks, engineering, etl, microsoft, python, sql"
Senior Data Analyst – Operations,THG,"Manchester Airport, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-%E2%80%93-operations-at-thg-3764230152,2023-12-17,Stoke-on-Trent, United Kingdom,Mid senior,Hybrid,"THG is a fast-moving, global technology business that specialises in taking brands direct to consumers. Our world-class proprietary tech and infrastructure powers our extensive portfolio of beauty, nutrition and lifestyle brands and is now helping drive exponential growth of our clients’ brands globally.
We’re powered by a global team of over 7,000 ambitious people around the world. Our culture is fast-paced and entrepreneurial, it’s this DNA that has supported our incredible growth.
We’re always looking for individuals that can bring fresh and innovative thinking to THG, and play a part in driving the group forward on its exciting journey. So, if you’re ready to take the next big step in your career, challenge yourself every day and evolve with the world around you, THG is ready for you.
Location: Icon 1, M90 5AA
About Central Functions
Central Functions is formed of the teams and people that serve the entire business. These people are integral to the smooth running of the business; ensuring everyone is paid on time, that materials are sourced and arrive promptly, that we meet our legal obligations and that our health, safety, and security is safeguarded.
About The Operations Team And The Role
The Operations Team drive the business through process improvement and timely interventions, to ensure that THG operates in the smoothest possible fashion. The team also provides insights via the collation and analysis of data to affect positive change within the wider business.
The Senior Data Analyst will be responsible for interpreting data to inform wider stakeholders of how the business is doing, providing an insight into processes and identifying where improvements could be made. The post holder will be suited to those who have a background in data analysis, BI analysis and MI analysis.
Responsibilities:
Analysing global operations KPIs – such as warehouse utilisation and allocation
Building bespoke automated reports/alerts to empower colleagues with data
Developing forecasting models to aid outbound demand planning
Working collaboratively across all of THG’s divisions and problem solving from a helicopter approach
Maintaining and owning existing data pipelines/dashboards and ensuring it’s accuracy
Lead ad-hoc projects to optimize operational processes
Perform root-cause analyses on key problem areas
Serve as a data expert to successfully support operational needs
Requirements:
Proficient with SQL – compulsory
Proficient in a data visualisation software (Tableau/PowerBI/etc.) – compulsory
Experience with programming languages (Python/R) – advantageous
3-4 years’ experience working in data analytics
Able to communicate effectively; using data to tell a story and drive business change
Benefits:
Competitive salary
Onsite Doctor
Employee discounts
Gym Discounts
Company bonus scheme
Company pension scheme
THG is proud to be a Disability Confident Committed employer. If you are invited to interview, please let us know if there are any reasonable adjustments we can make to the recruitment process that will enable you to perform to the best of your ability.
Because of the high volumes of applications our opportunities attract, it sometimes takes us time to review and consider them all. We endeavour to respond to every application we receive within 14 days. If you haven't heard from us within that time frame or should you have any specific questions about this or other applications for positions at THG please contact one of our Talent team to discuss further.
THG is committed to creating a diverse & inclusive environment and hence welcomes applications from all sections of the community.
Show more
Show less","SQL, Data Visualization, Tableau, PowerBI, Python, R, Data Analytics, BI Analysis, MI Analysis, Data Pipelines, Dashboards, Root Cause Analysis, Data Storytelling","sql, data visualization, tableau, powerbi, python, r, data analytics, bi analysis, mi analysis, data pipelines, dashboards, root cause analysis, data storytelling","bi analysis, dashboard, data storytelling, dataanalytics, datapipeline, mi analysis, powerbi, python, r, root cause analysis, sql, tableau, visualization"
Data Center Engineer,Nesco Resource,"Westborough, MA",https://www.linkedin.com/jobs/view/data-center-engineer-at-nesco-resource-3784332125,2023-12-17,Massachusetts,United States,Associate,Onsite,"Join our team at Nesco Resource, where we've partnered with a Third-Party Maintenance (TPM) company focused on providing IT Hardware Maintenance services to the Financial Services and Telecommunications industries, with a strong emphasis on data center support.
We're currently seeking an Onsite Data Center Support Technician for a data center in the greater Westborough or Grafton, Massachusetts area!
Both locations are overnight shifts
Our shifts may vary, encompassing both 8 and 12-hour schedules, so a bit of flexibility is needed.
If you're familiar with servers and blades from manufacturers like IBM, Dell, and Sun, you're the candidate we're looking for.
Your key responsibilities will include:
Rack and stack operations
Configuring and connecting new servers
Rebooting servers
Checking status lights
Managing inventories
Assisting with security escorts
Nesco Resource and affiliates (Lehigh G.I.T Inc, and Callos Resource, LLC) is an equal employment opportunity employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, or veteran status, or any other legally protected characteristics with respect to employment opportunities.
Show more
Show less","ThirdParty Maintenance (TPM), IT Hardware Maintenance, Data Center Support, Onsite Data Center Support Technician, Server Configuration, Server Rebooting, Server Status Monitoring, Inventory Management, Security Escorts","thirdparty maintenance tpm, it hardware maintenance, data center support, onsite data center support technician, server configuration, server rebooting, server status monitoring, inventory management, security escorts","data center support, inventory management, it hardware maintenance, onsite data center support technician, security escorts, server configuration, server rebooting, server status monitoring, thirdparty maintenance tpm"
Data Analyst-IL,ATC,"Boston, MA",https://www.linkedin.com/jobs/view/data-analyst-il-at-atc-3780615870,2023-12-17,Massachusetts,United States,Associate,Onsite,"Job Title: Business Analyst
We are looking for a skilled Business Analyst to join our team. As a Business Analyst, you will be responsible for analyzing business performance, identifying areas for improvement, and generating detailed reports to present to management.
Responsibilities:
Conduct research and analysis to identify trends, opportunities, and business insights
Develop detailed reports and presentations to share insights with stakeholders
Collaborate with cross-functional teams to improve business performance
Make recommendations to management based on findings
Use data to identify key operational and financial indicators of business success
Work closely with IT teams to ensure data integrity and accuracy
Requirements:
Bachelor’s degree in Business Administration, Information Technology or related field
Minimum of 3 years of experience in a business analyst role
Strong analytical and critical thinking skills
Proficiency in Microsoft Office Suite, specifically Excel and PowerPoint
Excellent communication and interpersonal skills
Ability to manage multiple projects simultaneously
Experience with Agile methodology is a plus
If you are a self-starter with a passion for finding trends and developing actionable insights, we encourage you to apply. This is an exciting opportunity to join a growing team and help shape the future of our organization. We offer competitive compensation, comprehensive benefits, and the chance to work with a dynamic group of professionals.
Show more
Show less","Business Analysis, Data Analysis, Data Visualization, Microsoft Office Suite, Excel, PowerPoint, Agile Methodology, Project Management, Communication, Critical Thinking","business analysis, data analysis, data visualization, microsoft office suite, excel, powerpoint, agile methodology, project management, communication, critical thinking","agile methodology, business analysis, communication, critical thinking, dataanalytics, excel, microsoft office suite, powerpoint, project management, visualization"
Bioinformatician / Data Scientist / Computational Biologist,Novartis Science,"Cambridge, MA",https://www.linkedin.com/jobs/view/bioinformatician-data-scientist-computational-biologist-at-novartis-science-3771700454,2023-12-17,Massachusetts,United States,Associate,Onsite,"About The Role
Location: Cambridge, MA
About this role:
250 genomics research projects every year are waiting for your data processing and analysis expertise!
The department of Discovery Sciences is looking for a highly motivated bioinformatician with expertise in the processing and analysis of sequencing data to support innovative drug discovery efforts. You will have the opportunity to work in a matrix team consisting of data analysts, members of a central sequencing facility, and disease area biologists. As a member of the data science community, you will also benefit from a global community of more than 200 experts with extensive knowledge and resources in data analysis, data engineering and machine learning.
Your Responsibilities Include:
Processing, quality control, and statistical analysis of sequencing data sets such as bulk RNASeq, single-cell and spatial transcriptomics, epigenetics, amplicon sequencing, CRISPR screens, whole-genome sequencing, long-read technologies
Maintenance and automation of various data workflows to provide complete IT solutions
Support projects with sequencing data expertise in diverse scientific fields such as gene and cell therapy, target discovery, genetics, drug safety, compound screening, etc.
Development of processing strategies for new types of data by combining existing public and commercial tools with innovative solutions
Translation of data into biological knowledge, presentation and discussion of results in project team meetings and planning of new experiments with partners such as disease area biologists and bioinformaticians
Close collaboration in a team of data analysts, computer scientists and sequencing experts, contribution to shaping and implementing our digital strategy and scientific processes
The pay range for this position at commencement of employment is expected to be between $130,400 and $195,600 per year; however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements, including a sign-on bonus, restricted stock units, and discretionary awards in addition to a full range of medical, financial, and/or other benefits (including 401(k) eligibility and various paid time off benefits, such as vacation, sick time, and parental leave), dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. If hired, employee will be in an “at-will position” and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors.
EEO Statement
We are Equal Opportunity Employers and take pride in maintaining a diverse environment. We do not discriminate in recruitment, hiring, training, promotion or other employment practices for reasons of race, color, religion, gender, national origin, age, sexual orientation, gender identity or expression, marital or veteran status, disability, or any other legally protected status. We are committed to building diverse teams, representative of the patients and communities we serve, and we strive to create an inclusive workplace that cultivates bold innovation through collaboration and empowers our people to unleash their full potential.
Accessibility and Reasonable Accommodations:
Individuals in need of a reasonable accommodation due to a medical condition or disability for any part of the application process, or to perform the essential functions of a position, please let us know the nature of your request, your contact information and the job requisition number in your message:
Novartis: e-mail us.reasonableaccommodations@novartis.com or call +1 (877)395-2339
Sandoz: e-mail reasonable.accommodations@sandoz.com or call: +1-609-422-4098
Role Requirements
What you’ll bring to the role:
Scientific background with a PhD degree in bioinformatics, computer science, computational biology, statistics, mathematics, physics, or related field, or an MS degree and 3+ years of related industry experience
Experience in the processing and analysis of next generation sequencing data
Demonstrated coding ability in Python, R/Bioconductor, Bash, Java, SQL, etc.
Familiarity with Unix and high-performance computing environments
Knowledge of fundamental concepts in molecular biology, cellular biology, statistics, and bioinformatics including biological databases and web tools
Expertise in statistical data analysis and machine learning, complemented by strong visualization and presentation skills
Team working ability, learning agility, scientific curiosity with a problem-solving mindset, organizational skills and an enthusiastic, collaborative spirit
Excellent communication and interpersonal skills to translate biologist/project team’s scientific questions into analytical strategies and methods
Fluency in English (oral and written)
Desirable Requirements:
Hands-on experience with Pacbio and Oxford Nanopore data and projects
Experience with other types of Omics data and data integration
Deep learning expertise
Cloud computing experience such as AWS
Web-based deployment of models and results (shiny, rest, flask, js)
Why Novartis: Our purpose is to reimagine medicine to improve and extend people’s lives and our vision is to become the most valued and trusted medicines company in the world. How can we achieve this? With our people. It is our associates that drive us each day to reach our ambitions. Be a part of this mission and join us! Learn more here: https://www.novartis.com/about/strategy/people-and-culture
You’ll receive: You can find everything you need to know about our benefits and rewards in the Novartis Life Handbook: https://www.novartis.com/careers/benefits-rewards
Commitment to Diversity and Inclusion / EEO:
The Novartis Group of Companies are Equal Opportunity Employers and take pride in maintaining a diverse environment. We do not discriminate in recruitment, hiring, training, promotion or other employment practices for reasons of race, color, religion, gender, national origin, age, sexual orientation, gender identity or expression, marital or veteran status, disability, or any other legally protected status. We are committed to building diverse teams, representative of the patients and communities we serve, and we strive to create an inclusive workplace that cultivates bold innovation through collaboration and empowers our people to unleash their full potential.
Join our Novartis Network: If this role is not suitable to your experience or career goals but you wish to stay connected to hear more about Novartis and our career opportunities, join the Novartis Network here: https://talentnetwork.novartis.com/network
Division
Novartis Institutes for BioMedical Research
Business Unit
DSc
Work Location
Cambridge, MA
Company/Legal Entity
NIBRI
Functional Area
Data Science
Job Type
Full Time
Employment Type
Regular
Shift Work
No
Early Talent
No
Show more
Show less","Bioinformatics, Genomics, Gene and cell therapy, Computational biology, Statistics, Machine learning, Python, R, Bioconductor, Bash, Java, SQL, Unix, Highperformance computing environments, Molecular biology, Cellular biology, Biological databases, Web tools, Data visualization, Presentation skills, Teamwork, Problemsolving, Organizational skills, Communication, Interpersonal skills, Pacbio, Oxford Nanopore, Omics data, Data integration, Deep learning, Cloud computing, AWS, Webbased deployment","bioinformatics, genomics, gene and cell therapy, computational biology, statistics, machine learning, python, r, bioconductor, bash, java, sql, unix, highperformance computing environments, molecular biology, cellular biology, biological databases, web tools, data visualization, presentation skills, teamwork, problemsolving, organizational skills, communication, interpersonal skills, pacbio, oxford nanopore, omics data, data integration, deep learning, cloud computing, aws, webbased deployment","aws, bash, bioconductor, bioinformatics, biological databases, cellular biology, cloud computing, communication, computational biology, data integration, deep learning, gene and cell therapy, genomics, highperformance computing environments, interpersonal skills, java, machine learning, molecular biology, omics data, organizational skills, oxford nanopore, pacbio, presentation skills, problemsolving, python, r, sql, statistics, teamwork, unix, visualization, web tools, webbased deployment"
Database Engineer,Allonnia,"Massachusetts, United States",https://www.linkedin.com/jobs/view/database-engineer-at-allonnia-3782264583,2023-12-17,Massachusetts,United States,Associate,Remote,"Company Description & Purpose
Allonnia’s purpose is the pursuit of imaginative solutions to solve the world’s toughest waste challenges through biology. Allonnia is leveraging the power of biotechnology and engineered systems to degrade or sequester pollutants and upcycle waste. Our ethos at Allonnia is to work in harmony with nature, combining biology and technology to fast forward time and bring Nature’s future solutions to the present day. Through this collaboration we will unlock the potential in waste and enable a world where nothing is wasted. Allonnia is backed by Battelle, Ginkgo Bioworks, Viking Global, General Atlantic, EVOK and Cascade Investments.
Mission of the Position
Allonnia is seeking a talented and highly motivated Database Engineer to play a pivotal role in managing and optimizing our database infrastructure for environmental microbiology data. As a Database Engineer, you will be responsible for furthering database design, architecture maintenance, data modeling, security, data migration, quality assurance, and anomaly detection. Your expertise will ensure the integrity, performance, and security of our database, and supporting Allonnia's ongoing mission to advance environmental solutions.
Key Criteria/Requirements
A Bachelor's or Master's degree in a relevant field, such as Database Engineering, Computer Science, or a related discipline.
Proven experience in database design, administration, and maintenance, particularly with complex data structures.
Proficient in data modeling and adept at crafting efficient database schemas
Skilled in working with both SQL and NoSQL databases, data lakes, and data warehouses.
Competent in cloud computing, with a preference for expertise in AWS services
Strong knowledge of database security practices, access control, and administration.
Experience in ETL processes and data migration.
Proficiency in data quality assurance and anomaly detection techniques.
Excellent problem-solving skills and the ability to work collaboratively with multidisciplinary teams.
Familiarity with environmental microbiology data and its unique requirements is a plus.
Excellent verbal and written communication skills capable of communicating with a variety of different team perspectives
Responsibilities & Measurable Accountabilities
Database Design and Architecture Maintenance and Fine Tuning
Maintain and enhance the database infrastructure to meet Allonnia's evolving data needs, with a focus on scalability, performance, and data integrity.
Manage database schemas, tables, indexes, storage, backup procedures, stored procedures, SQL queries, and data preprocessing, retrieval, and manipulation.
Create data models that efficiently serve Allonnia's Squadrons, enabling effective data storage and retrieval.
Data Security and Access Control and Administration
Implement robust security measures, including user authentication, authorization, and encryption, to safeguard sensitive data.
Enforce access control policies to ensure data is accessible only to authorized users.
Provide education and guidance on best practices for database usage.
Serve as the administrator of the database, ensuring compliance with data governance policies, maintaining up-to-date database documentation, and facilitating effective communication across teams.
Data Migration and ETL (Extract, Transform, Load) Processes
Develop and maintain ETL processes to efficiently move data between different systems, transform data into the desired format, and load it into the database.
Plan and execute data migrations during transitions between database systems or system upgrades.
Collaborate with the teams to establish best practices for structuring data and templates.
Data Quality Assurance and Anomaly Detection
Implement data validation and quality control procedures to ensure the accuracy and consistency of data stored in the database.
Monitor the health of the database, diagnose and resolve database-related issues, and proactively implement monitoring solutions.
Allonnia Core Values
Purposeful
Committing to work on the right things as individuals and as a company, making a difference every day
Entrepreneurial
Working with passion and curiosity to learn every day, creatively delivering results to the world
Transparent
Fostering a high trust environment that embraces constructive debate, achieving success together
Key Competencies
Analysis Skills
Creativity
Resourcefulness/Initiative
Organization/Planning
Team Player
Communications – Oral
Communications – Written
Tenacity
JOB CODE: 1000053
Show more
Show less","SQL, NoSQL, Data modeling, Data warehousing, Data security, Database administration, Cloud computing, AWS services, Data quality assurance, Anomaly detection, ETL processes, Data migration, Data preprocessing, Data retrieval, Data manipulation, Data governance","sql, nosql, data modeling, data warehousing, data security, database administration, cloud computing, aws services, data quality assurance, anomaly detection, etl processes, data migration, data preprocessing, data retrieval, data manipulation, data governance","anomaly detection, aws services, cloud computing, data governance, data manipulation, data migration, data preprocessing, data quality assurance, data retrieval, data security, database administration, datamodeling, datawarehouse, etl, nosql, sql"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Boston, MA",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783188342,2023-12-17,Massachusetts,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Boston-DataResearchAn.017
Show more
Show less","Python, JavaScript, JSON, Generative AI, OOP, Data science, R, English communication, Stakeholder management, Data analytics, EdTech, Machine learning, Technology, Algorithms, Education","python, javascript, json, generative ai, oop, data science, r, english communication, stakeholder management, data analytics, edtech, machine learning, technology, algorithms, education","algorithms, data science, dataanalytics, edtech, education, english communication, generative ai, javascript, json, machine learning, oop, python, r, stakeholder management, technology"
Principal Data Scientist,Fresenius Medical Care,"Lexington, MA",https://www.linkedin.com/jobs/view/principal-data-scientist-at-fresenius-medical-care-3782144256,2023-12-17,Massachusetts,United States,Associate,Remote,"PURPOSE AND SCOPE:
Supports FMCNA’s mission, vision, core values and customer service philosophy. Adheres to the FMCNA Compliance Program, including following all regulatory and division/company policy requirements. As Principal Data Scientist, you participate in the operational activities involved in the planning, conduct, and completion of data science projects in the FMCNA enterprise.
PRINCIPAL DUTIES AND RESPONSIBILITIES:
Conducts advanced data analysis and complex designs algorithm.
Identifies what data is available and relevant, including internal and external data sources, leveraging new data collection processes
Identifies and analyzes patterns in the volume of data supporting the initiative, the type of data (e.g., images, text, clickstream or metering data) and the speed or sudden variations in data collection.
Partners with the data stewards to define the data quality expectation in the context of the specific use case.
Recommends ongoing improvements to methods and algorithms that lead to findings, including new information.
Develop ideas into original research projects which can significantly improve clinical practice
Develop product and process improvement into an intellectual property format.
Develops technical specifications to implement data science workflow and engineering
Prototypes initial full-stack technical minimum viable product and partners with infrastructure and engineering to scale workloads for wider adoption
Analyze all data using various data analysis software and/or applying quantitative methods, resolve data discrepancies, draw conclusions, and provide/implement process or documents improvements.
Contribute to the definition and timely achievement of overall project goals. Make recommendations, and when appropriate, acts independently to resolve scientific problems encountered during experimental procedures to improve productivity of results. Ensure activities are consistent with project critical path and respond appropriately to changing priorities.
Present findings at international conferences and locally at group and departmental meetings.
Keep informed of trends and developments in area of scientific responsibility and improve competence by participating in educational activities.
Ability to work on issues that impact or address future concepts, products or technologies.
Ability to network with key decision makers.
EDUCATION:
Advanced Degree in epidemiology, public health, statistics, data analysis or similar preferred
Working knowledge in R, Python, SAS at a minimum
EXPERIENCE AND REQUIRED SKILLS
:
8 – 12 years’ related experience; or a Master’s degree with 6 years’ experience; or a PhD with 2 years’ experience; or equivalent directly related work experience.
Solid knowledge of statistical techniques.
The ability to come up with solutions to loosely defined problems by leveraging pattern detection over potentially large datasets.
Strong programming skills (such as Hadoop or other big data frameworks, Java), and statistical modeling (like SAS or R).
Experience using machine learning algorithms.
Proficiency in the use of statistical packages.
Excellent verbal and written communication skills required for interacting with physicians, nurses, management, and peers.
Ability to clearly summarize methodology and key points of program/report in technical documentation/specifications is also required.
Strong knowledge of outcomes research and analysis of health quality data.
Ability to work independently without day-to-day supervision or the use of templates.
Knowledge of clinical data and dialysis industry.
Show more
Show less","Statistical techniques, Pattern detection, Programming skills, Hadoop, Big data frameworks, Java, Statistical modeling, SAS, R, Machine learning algorithms, Statistical packages, Verbal communication skills, Written communication skills, Outcomes research, Health quality data analysis, Clinical data, Dialysis industry","statistical techniques, pattern detection, programming skills, hadoop, big data frameworks, java, statistical modeling, sas, r, machine learning algorithms, statistical packages, verbal communication skills, written communication skills, outcomes research, health quality data analysis, clinical data, dialysis industry","big data frameworks, clinical data, dialysis industry, hadoop, health quality data analysis, java, machine learning algorithms, outcomes research, pattern detection, programming skills, r, sas, statistical modeling, statistical packages, statistical techniques, verbal communication skills, written communication skills"
Data Engineer,Jobs for Humanity,"Worcester, MA",https://www.linkedin.com/jobs/view/data-engineer-at-jobs-for-humanity-3786350897,2023-12-17,Massachusetts,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
MassMutual is a company that is passionate about helping everyone achieve financial freedom. We believe in creating digital experiences that are inclusive and interactive for all of our customers. To help us with our mission, we are looking to hire data engineers for our Data Management and Engineering team.
What we're looking for:
We want a Data Engineer who is passionate about data and enjoys building data projects. You should be comfortable working with business partners to understand requirements and delivering robust solutions.
Someone who is excited to learn new technologies and work in the cloud.
A team player who is a strong communicator.
Objectives of the role:
Design, build, and maintain complex data jobs that provide business value.
Translate high-level business requirements into technical specifications.
Ingest data from different sources into our data lake and data warehouse.
Cleanse and enrich data while ensuring data quality.
Guide the future development of our data platform.
What we offer:
Develop re-usable tools that make delivering new projects easier.
Collaborate with other developers and provide mentorship.
Evaluate and recommend tools, technologies, processes, and reference architectures.
Work in an Agile development environment.
Basic Qualifications:
Bachelor's degree in computer science, engineering, or a related field.
At least 5 years of experience with data warehousing and analytics.
Strong knowledge of SQL programming and query optimization.
Good understanding of ETL/ELT methodologies and tools.
Experience with troubleshooting and root cause analysis.
Strong communication, problem-solving, organizational, and analytical skills.
Able to work independently and provide leadership to small teams.
Preferred Qualifications:
Master's degree in computer science, engineering, or a related field.
Experience working in a cloud environment (e.g., AWS).
Hands-on experience developing with Python.
Experience with data processing technologies such as Apache Spark or Kafka.
Knowledge of orchestration and scheduling tools (e.g., Apache Airflow).
Experience with data reporting and data cataloging tools (e.g., Microstrategy, Tableau, Looker, Alation).
We are an Equal Employment Opportunity employer. We welcome applicants from all backgrounds, including minorities, females, individuals with disabilities, LGBTQIA+, and veterans. If you need any accommodation during the application process, please let us know.
Show more
Show less","Data Warehousing, Analytics, SQL Programming, Query Optimization, ETL/ELT Methodologies, Troubleshooting, Root Cause Analysis, Apache Spark, Kafka, Apache Airflow, Python, Microstrategy, Tableau, Looker, Alation, AWS","data warehousing, analytics, sql programming, query optimization, etlelt methodologies, troubleshooting, root cause analysis, apache spark, kafka, apache airflow, python, microstrategy, tableau, looker, alation, aws","alation, analytics, apache airflow, apache spark, aws, datawarehouse, etlelt methodologies, kafka, looker, microstrategy, python, query optimization, root cause analysis, sql, tableau, troubleshooting"
Senior Database Engineer,Wood Mackenzie,"Boston, MA",https://www.linkedin.com/jobs/view/senior-database-engineer-at-wood-mackenzie-3775687618,2023-12-17,Massachusetts,United States,Mid senior,Onsite,"Company Description
Wood Mackenzie are the global research, analytics, and consultancy business powering the natural resources industry. For 50 years, we have been providing the quality data, analytics, and insights our customers rely on to inspire their decision making.
Our dedicated oil, gas & LNG, power & renewables, chemicals, metals & mining sector teams are located around the world and deliver a variety of projects based on our assessment and valuation of thousands of individual assets, companies, and economic indicators such as market supply, demand, and price trends.
We have over 2.500 employees in 30 locations, serving customers in nearly 80 countries. Together, we inspire and innovate the markets we serve – providing invaluable intelligence to help our customers overcome the toughest challenges, and make strategic decisions that will, ultimately, accelerate the world’s transition to a more sustainable future.
WoodMac.com
Wood Mackenzie brand video
Job Description
Wood Mackenzie is looking for a senior engineer specializing in database development to join our power & renewables engineering division. In this role you will be accountable for the continued development and support of products and data assets that drive our short-term power applications. These products focus on data presentation and power flow forecasting for the power transmission regions across the country and deal with a broad set of technologies, datasets and cross industry requirements. You will help drive the growth of these products through gained industry knowledge and client needs while adhering to development best practice and company software delivery frameworks.
Main Responsibilities
Create, maintain, and improve database data structures and functions to support business functions
Provide operational support for enterprise databases to ensure high availability of applications and data quality
Identify and transform industry data for the purpose of ingestion into Wood Mackenzie systems
Support internal and external customer on the use, processes, and data within short-term power databases and applications
Contribute to roadmap planning by working with stakeholders, identifying enhancement opportunities and technical support needs, and providing estimates for these initiatives
Collaborate with peers to provide specialized database engineering and industry insight and guidance
Qualifications
5+ years of experience with database development and data architecture
Demonstrated experience consuming, transforming, and managing large complex data sets through developing commercial applications with traditional relational (Postgres, Oracle, SQL, etc) databases
Demonstrated ability to understand industry-specific data and processes, and utilize them to drive the value of a product
Hands on expertise with agile methodologies and comprehensive understanding of modern patterns and practices
Experience with peer coaching and mentoring
Self-driven to manage your own time, balance priorities, and work well both independently and as part of a team
Strong technical and non-technical communication skills to effectively interact with various audience
Additional Information
“Wood Mackenzie is a place where we are committed to supporting our people to grow and thrive. We value different perspectives and aspire to create an inclusive environment which encourages diversity and fosters a sense of belonging.
Wood Mackenzie values everyone’s contribution and helps them reach their full potential while sustaining an organisational culture of health and well-being.
Our Core Values Are
Inclusive – we succeed together
Trusting – we choose to trust each other
Customer committed – we put customers at the heart of our decisions
Future Focused – we accelerate change
Curious – we turn knowledge into action
We understand the importance of bringing your whole self to work and to achieving balance between work, family and other life commitments. We are open to considering flexible working arrangements to enable the greatest spectrum of talent to contribute to Wood Mackenzie's success.
Hear what our team has to say about working with us:
https://www.woodmac.com/careers/our-people/
Wood Mackenzie takes your data privacy seriously, please click here to view our privacy notices:
Candidate Privacy Notice | Wood Mackenzie | Wood Mackenzie
/
Candidate Notice - California
”
Show more
Show less","Database development, Data architecture, PostgreSQL, Oracle, SQL, Agile methodologies, Peer coaching, Mentoring, Time management, Communication skills","database development, data architecture, postgresql, oracle, sql, agile methodologies, peer coaching, mentoring, time management, communication skills","agile methodologies, communication skills, data architecture, database development, mentoring, oracle, peer coaching, postgresql, sql, time management"
Data Engineer,L2L,"Boston, MA",https://www.linkedin.com/jobs/view/data-engineer-at-l2l-3766732881,2023-12-17,Massachusetts,United States,Mid senior,Remote,"Does the rapid evolution of technology excite you? Do you want to be at the forefront of digital transformation? Are you interested in joining a team dedicated to crafting a cutting-edge SaaS platform? If so, keep reading - this may be the perfect opportunity for you.
The Role:
L2L is on a Mission to Empower workers to manufacture better together. We started as a small team of manufacturing experts, and we created a no-code platform that is a cloud-based, open-environment system with customizable solutions in over 23 languages across a plethora of vertices.
We are experiencing rapid growth, and now is the time for us to add a talented data engineer to our team. This role comes with the ability to truly impact our product, partnering with our VP of Engineering, you will influence technology decisions, build out data pipelines, mentor and coach other teams on best practices around data. We are looking for someone who has hands-on experience writing Python code and SQL as well as working knowledge of Linux environments and tooling. Knowledge of Django and PostgreSQL is a plus, but not required.
Responsibilities:
Design, build, and maintain efficient and reliable data pipelines and data storage systems.
Collaborate with cross-functional teams to design and implement data-driven solutions.
Ensure data quality and integrity by implementing data validation and testing procedures.
Monitor and troubleshoot data pipelines and data storage systems to ensure optimal performance.
Assemble large, complex sets of data that meet non-functional and functional business requirements.
Identify, design and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Requirements:
Bachelor’s degree in Computer Science, Information Technology, or a related field.
5+ years of experience in data engineering.
Strong experience in writing Python code and SQL.
Experience with Django and PostgreSQL is a plus.
Experience with data warehousing and ETL processes.
Strong problem-solving skills and attention to detail Excellent communication and collaboration skills.
Preferred Qualifications:
Strong candidates will be self-starters with the ability to work through ambiguity. We highly value people who are humble, learn quickly, and deliver results. At L2L, we are focused on empowering our clients, and our team, by working together.
About L2L:
L2L is on a mission to empower workers to manufacture better together. How? By digitizing shop floor processes and presenting information to workers in ways that make plants more efficient, reliable and profitable.
L2L is a connected workforce platform that creates real-time visibility across all departments, enables the entire workforce to systematically identify and solve problems, and provides workers with the personalized information they need to do their best work. More than 225,000 frontline workers love how L2L gives them the confidence to perform at their best.
L2L is backed by M33 Growth, a growth-focused private equity firm in Boston Massachusetts that provides both capital and resources to develop already-great businesses into market leaders.
Show more
Show less","Python, SQL, Linux, Django, PostgreSQL, Data pipelines, Data storage systems, Data warehousing, ETL processes, Data validation, Data testing, Data quality, Data integrity, Infrastructure scalability, Data delivery optimization, Automation, Problemsolving, Attention to detail, Communication, Collaboration, Selfstarter, Ambiguity tolerance, Humility, Quick learning, Result delivery","python, sql, linux, django, postgresql, data pipelines, data storage systems, data warehousing, etl processes, data validation, data testing, data quality, data integrity, infrastructure scalability, data delivery optimization, automation, problemsolving, attention to detail, communication, collaboration, selfstarter, ambiguity tolerance, humility, quick learning, result delivery","ambiguity tolerance, attention to detail, automation, collaboration, communication, data delivery optimization, data integrity, data quality, data storage systems, data testing, data validation, datapipeline, datawarehouse, django, etl, humility, infrastructure scalability, linux, postgresql, problemsolving, python, quick learning, result delivery, selfstarter, sql"
Senior Data Engineer,ZoomInfo,"Waltham, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-zoominfo-3770691021,2023-12-17,Massachusetts,United States,Mid senior,Remote,"At ZoomInfo, we encourage creativity, value innovation, demand teamwork, expect accountability and cherish results. We value your take charge, take initiative, get stuff done attitude and will help you unlock your growth potential. One great choice can change everything. Thrive with us at ZoomInfo.
At
ZoomInfo
we encourage creativity, value innovation, demand teamwork, expect accountability and cherish results. If you are a take charge, take initiative, get stuff done individual we want to talk to you! We have high aspirations for the company and are looking for the right people to help fulfill the dream. We strive to continually improve every aspect of the company and use cutting edge technologies and processes to delight our customers and rapidly increase revenues.
As a Senior Data Engineer, you'll have a key role in building and designing the strategy of our finance analytics engineering team under the Enterprise Data Engineering group.
Our Technological Stack includes: Airflow, DBT, Python, Snowflake, AWS, GCP, Amplitude, Fivetran, and more.
What will you actually be doing?
Building, and continuously improving our data gathering, modeling, reporting capabilities and self-service data platforms.
Working closely with Data Engineers, Data Analysts, Data Scientists, Product Owners, and Domain Experts to identify data needs.
Required Experience:
Relevant Bachelor degree – preferably CS, Engineering/ Information Systems or other equivalent Software Engineering background.
8+ years of experience as a Data/BI engineer.
Strong SQL abilities and hands-on experience with SQL and no-SQL DBs, performing analysis and performance optimizations.
Hands-on experience in Python or equivalent programming language
Experience with data warehouse solutions (like BigQuery/ Redshift/ Snowflake)
Experience with data modeling, data catalog concepts, data formats, data pipelines/ETL design, implementation and maintenance.
Experience with AWS/GCP cloud services such as GCS/S3, Lambda/Cloud Function, EMR/Dataproc, Glue/Dataflow, Athena.
Experience with Airflow and DBT - Advantage.
Experience with data visualization tools and infrastructures (like Tableau/SiSense/Looker/other) - Advantage.
Experience with development practices – Agile, CI/CD, TDD - Advantage.
Experience with Infrastructure as Code practices - Terraform - Advantage
About Us:
For over a decade, ZoomInfo has helped companies achieve their most important objective: profitable growth. Backed by the world's most comprehensive B2B database, our platform puts sales and marketing professionals in position to identify, connect, and engage with qualified prospects.
Our mission is to provide every company with a 360-degree view of their ideal customer, empowering each phase of their go-to-market strategy and driving their ability to hit their number.
The US base salary range for this position is $133,600.00 to $170,000.00 variable compensation + benefits.
Actual compensation offered will be based on factors such as the candidate’s work location, qualifications, skills, experience and/or training. Your recruiter can share more information about the specific salary range for your desired work location during the hiring process.
We want our employees and their families to thrive. In addition to comprehensive benefits we offer holistic mind, body and lifestyle programs designed for overall well-being. Learn more about ZoomInfo benefits here.
About Us:
ZoomInfo (NASDAQ: ZI) is the trusted go-to-market platform for businesses to find, acquire, and grow their customers. It delivers accurate, real-time data, insights, and technology to more than 35,000 companies worldwide. Businesses use ZoomInfo to increase efficiency, consolidate technology stacks, and align their sales and marketing teams — all in one platform.
ZoomInfo may use a software-based assessment as part of the recruitment process. More information about this tool, including the results of the most recent bias audit, is available here.
ZoomInfo is proud to be an Equal Opportunity employer. We are committed to equal employment opportunities for applicants and employees regardless of sex, race, age, color, national origin, sexual orientation, gender identity, marital status, disability status, religion, protected military or veteran status, medical condition, or any other characteristic or status protected by applicable law. At ZoomInfo, we also consider qualified candidates with criminal histories, consistent with legal requirements.
Show more
Show less","Airflow, DBT, Python, Snowflake, AWS, GCP, Amplitude, Fivetran, SQL, NoSQL, BigQuery, Redshift, Apache Beam, Apache Airflow, Lambda, Cloud Function, EMR, Dataproc, Glue, Dataflow, Tableau, SIsense, Looker, Agile, CI/CD, TDD, Terraform","airflow, dbt, python, snowflake, aws, gcp, amplitude, fivetran, sql, nosql, bigquery, redshift, apache beam, apache airflow, lambda, cloud function, emr, dataproc, glue, dataflow, tableau, sisense, looker, agile, cicd, tdd, terraform","agile, airflow, amplitude, apache airflow, apache beam, aws, bigquery, cicd, cloud function, dataflow, dataproc, dbt, emr, fivetran, gcp, glue, lambda, looker, nosql, python, redshift, sisense, snowflake, sql, tableau, tdd, terraform"
Senior Data Engineer Consultant - Remote,Syrinx Consulting,"Massachusetts, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-consultant-remote-at-syrinx-consulting-3620301308,2023-12-17,Massachusetts,United States,Mid senior,Remote,"Senior Data Engineer Consultant
This is a Remote Role with a Syrinx Healthcare Tech Partner
U.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor at this time.
Python and SQL are must-haves
As a Senior Data Engineer Consultant, you'll be leading efforts in building scalable backend services and data pipelines that support hundred of millions of users and thousands of events per second. You will get to work across multiple microservices written in Python, Scala, and Ruby, using technologies such as AWS Lambda, Kinesis, SQS, RDS, Dynamo, Snowflake, Spark, HDFS, S3, ElasticSearch, ECS/Fargate, Athena, Presto, Cadence, Serverless Framework, Docker, Terraform, and more.
Development efforts of new services that deal with large scale data or high throughput data pipelines/streams
Architect and scale out our ETL framework to support back data processing in technologies such as Spark
Manage and implement scalable monitoring and escalation strategies across our systems
Act as a technical architect, elegantly separating domain models to ensure partner complexity doesn't leak into our app
Invest in infrastructure that ensures our small team can efficiently manage hundreds of integrations
Qualifications
8+ years software engineering experience
Python and SQL
Experience in a few of those is ideal
Experience in designing and implementing scalable applications/microservices
Experience creating robust RESTful APIs
Emphasis on clean, well-designed code
Deep understanding of Postgres, MySQL, and other relational databases
Experience working with large datasets/databases and scaling memory-intensive applications
Gritty mentality with a focus on shipping
Process-oriented executor; you can manage multiple projects concurrently and prioritize effectively
Show more
Show less","Python, SQL, Scala, Ruby, AWS Lambda, Kinesis, SQS, RDS, Dynamo, Snowflake, Spark, HDFS, S3, ElasticSearch, ECS/Fargate, Athena, Presto, Cadence, Serverless Framework, Docker, Terraform, ETL, RESTful APIs, Postgres, MySQL, Relational databases, Large datasets, Memoryintensive applications","python, sql, scala, ruby, aws lambda, kinesis, sqs, rds, dynamo, snowflake, spark, hdfs, s3, elasticsearch, ecsfargate, athena, presto, cadence, serverless framework, docker, terraform, etl, restful apis, postgres, mysql, relational databases, large datasets, memoryintensive applications","athena, aws lambda, cadence, docker, dynamo, ecsfargate, elasticsearch, etl, hdfs, kinesis, large datasets, memoryintensive applications, mysql, postgres, presto, python, rds, relational databases, restful apis, ruby, s3, scala, serverless framework, snowflake, spark, sql, sqs, terraform"
Senior Data Engineer (Azure),Energy Jobline,"Chatham, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-azure-at-energy-jobline-3772979435,2023-12-17,Massachusetts,United States,Mid senior,Hybrid,"Senior Data Engineer - Azure
A leading financial services corporation is currently recruiting a Senior Data Engineer with five years’ experience in ETL development coupled with strong capabilities in Azure cloud tools such as Azure Data Factory, Azure Synapse Analytics, Azure Data Lake, Azure Blob Storage, Azure Databricks, Azure Analysis service as our client rebuilds their technology estate moving to a cloud focused data driven environment leveraging the latest tools/technologies. Our client is looking to pay up to £67,000 + 15% bonus with on site presence occasionally in London or Chatham.
The ideal Senior Data Engineer will have experience migrating legacy applications into optimised data pipelines within a regulated environment.
Core Responsibilities
Leading solutions for data engineering
Maintain the integrity of both the design and the data that is held within the architecture
Champion and educate people in the development and use of data engineering best practices
Support the Head of Data Engineering and lead by example
Contribute to the development of database management services and associated processes relating to the delivery of data solutions
Provide requirements analysis, documentation, development, delivery and maintenance of data platforms.
Develop database requirements in a structured and logical manner ensuring delivery is aligned with business prioritisation and best practice
Design and deliver performance enhancements, application migration processes and version upgrades across a pipeline of BI environments.
Provide support for the scoping and delivery of BI capability to internal users.Experience requirements:
5+ years Data Engineering / ETL development experience
Strong capabilities in Azure Cloud Solutions (etc. Databricks)
Experience working within a regulated environment / finance / insurance / energy
5+ years data design experience in an MI / BI / Analytics environment
Excellent Data Warehouse with substantial experience in extracting, reporting and manipulating data from a data warehouse environment
Significant technical skills such as Transact SQL language, relational database skills
Evidence of delivering complex data platforms and solutions
Microsoft SQL Server 2019 certification£67,000/ 15% bonus / Flexible working / 28 Days Holiday / Medical Cover / Life Cover / 13% Pension / Flexible Benefits
Senior Data Engineer - Azure
Show more
Show less","Azure, Azure Data Factory, Azure Synapse Analytics, Azure Data Lake, Azure Blob Storage, Azure Databricks, Azure Analysis Services, ETL, Data engineering, Data pipelines, Data management, Data solutions, Database requirements, Database design, Database performance, Application migration, BI environments, BI capability, Transact SQL, Relational databases, Data warehouse, Data extraction, Data reporting, Data manipulation, Microsoft SQL Server 2019","azure, azure data factory, azure synapse analytics, azure data lake, azure blob storage, azure databricks, azure analysis services, etl, data engineering, data pipelines, data management, data solutions, database requirements, database design, database performance, application migration, bi environments, bi capability, transact sql, relational databases, data warehouse, data extraction, data reporting, data manipulation, microsoft sql server 2019","application migration, azure, azure analysis services, azure blob storage, azure data factory, azure data lake, azure databricks, azure synapse analytics, bi capability, bi environments, data engineering, data extraction, data management, data manipulation, data reporting, data solutions, database design, database performance, database requirements, datapipeline, datawarehouse, etl, microsoft sql server 2019, relational databases, transact sql"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Ohio, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739296982,2023-12-17,Lenox,United States,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","PostgreSQL, MySQL, Oracle, SQL, PL/SQL, pgPL/SQL, Python, Shell, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","postgresql, mysql, oracle, sql, plsql, pgplsql, python, shell, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, mysql, opensearch, oracle, pgplsql, plsql, postgresql, presto, python, redis, shell, snowflake, sql, terraform, trino"
"Data Quality Analyst (Marketing Data, Salesforce, MarTech, AdTech)",Sonitalent Corp,United States,https://www.linkedin.com/jobs/view/data-quality-analyst-marketing-data-salesforce-martech-adtech-at-sonitalent-corp-3697175627,2023-12-17,Lenox,United States,Associate,Remote,"Job Title: Data Quality Analyst (Marketing Data, Salesforce, MarTech, AdTech)
Location: REMOTE or Memphis, TN
Duration: 18 Months + Extensions (Long term contract.)
Job Description
We are looking for a
Data Quality Analyst
excited about data and marketing technology. The Quality Analyst will be responsible for reviewing and ensuring the integrity and robustness of data, customer profile,
Marketing-Tech, and Ad-Tech
, and implementations through rigorous testing and validation methodologies.
Notes
Experience with marketing data is preferred but not a deal breaker.
Experience with data QA is required.
Salesforce and/or Adobe MarTech stack experience is preferred but not required. (Salesforce would be more beneficial)
MarTech or AdTech background is preferred but not required
Automation experience is a nice to have. Selenium, uipath, etc
Requirements
Ability to design, execute, and maintain test cases, detect defects, collaborate with the development team to rectify issues, and provide feedback and insights to optimize ongoing development
Ability to interact well with team members and business users
Excellent written and verbal communications skills
Fast-paced adoption of new and emerging technologies
Understanding of software engineering principles and techniques
Salesforce and Adobe Marketing stacks, architecture and infrastructure
Experience with third party databases, libraries, interfaces, and internet protocols
Knowledge of LINUX, JavaScript, J2EE, Relational and Document Databases, JSON, Shell Scripting, automation
Show more
Show less","Data Quality Analyst, Marketing Data, Salesforce, MarTech, AdTech, Data QA, Selenium, UiPath, Test Cases, Defect Detection, Development Collaboration, Feedback and Insights, Team Collaboration, Business User Interaction, Written and Verbal Communication, New and Emerging Technologies, Software Engineering Principles, Salesforce Stack, Adobe Marketing Stack, Architecture and Infrastructure, Third Party Databases, Libraries, Interfaces, Internet Protocols, LINUX, JavaScript, J2EE, Relational Databases, Document Databases, JSON, Shell Scripting, Automation","data quality analyst, marketing data, salesforce, martech, adtech, data qa, selenium, uipath, test cases, defect detection, development collaboration, feedback and insights, team collaboration, business user interaction, written and verbal communication, new and emerging technologies, software engineering principles, salesforce stack, adobe marketing stack, architecture and infrastructure, third party databases, libraries, interfaces, internet protocols, linux, javascript, j2ee, relational databases, document databases, json, shell scripting, automation","adobe marketing stack, adtech, architecture and infrastructure, automation, business user interaction, data qa, data quality analyst, defect detection, development collaboration, document databases, feedback and insights, interfaces, internet protocols, j2ee, javascript, json, libraries, linux, marketing data, martech, new and emerging technologies, relational databases, salesforce, salesforce stack, selenium, shell scripting, software engineering principles, team collaboration, test cases, third party databases, uipath, written and verbal communication"
Senior Data Analyst,ASRC Federal,United States,https://www.linkedin.com/jobs/view/senior-data-analyst-at-asrc-federal-3111058198,2023-12-17,Pasco,United States,Associate,Remote,"Job Description
ASRC Federal Vistronix (ASRC Federal) is actively seeking a
senior data analyst
to join our Denver-based team and support defining business requirements for software to support for our federal customer. The successful candidate will work with business sponsors and other relevant stakeholders to define and document the business requirements for complex government software systems and advises on the design and build of relational databases for secure data storage or processing. The ability to work with both technical and non-technical audiences is very important, so having a solid understanding of the software development process is key. Most of the work will be performed virtually, but some work at the government site in Denver may occur in the future.
ASRC Federal is a technical, professional services company providing state-of-the-art solutions to government and commercial clients. Our services include custom-engineered solutions that integrate with the latest technology, resulting in advanced information technology systems; business and management consulting services to assess client; and strategic and tactical program expertise to support continuity and provide comprehensive oversight for mission-critical initiatives.
We partner with government and commercial agencies that require the development of systems, such as communication systems, asset management, network deployment and engineering services, power and energy management solutions, portal applications, command and control, and GIS to operate more efficiently and profitably.
As a leading IT consulting and strategic outsourcing leader, we are always looking for exceptionally bright and motivated people to join our team. We are thought leaders in our market space - providing comprehensive solutions to our clients, throughout the enterprise. ASRC Federal staff members enjoy a collaborative working environment and recognize our staff member’s contributions to the team’s success as well as individual professional accomplishments.  We offer competitive salaries and a comprehensive employee benefits package. If you are looking for an opportunity to use your skills in innovative ways, in an environment that promotes freethinking, presents positive challenges, and makes a real impact - ASRC Federal is the place for you!
Responsibilities
This position will be responsible for providing data analysis services is support of developing requirements for new and upgraded government systems.  Experience with DBMS design and system analysis, current operating systems software internals and data manipulation techniques and languages required to perform the specific job activities are key skillsets. Works closely with peers to evaluate the effects of systems tools or equipment changes on the database and to ensure continuing integration of the database. An understanding of Agile development lifecycle is important in defining Minimal Viable Data (MVD) specifications when migrating legacy applications to modern platforms.
Key Tasks
Perform business analysis to support the project managers in defining project scope and objectives as well as eliciting information and documenting functional requirements from product owners and business sponsors through oral and written communication.
Understand data standardization and data governance methodologies and develop strategy towards successful implementation of these methodologies.
Develop strategies for full-lifecycle data usage, including data acquisition, legacy data migration, warehouse implementation, and archive recovery.
Ensure the confidentiality, currency, accuracy, and integrity of the data.
Assist in cleaning and maintaining database by identifying old data.
Evaluate new data sources for adherence to the organization's quality standards and ease of integration.
Assesses the performance of the database for maximum effectiveness
.
Show more
Show less","Data analysis, DBMS design, System analysis, Operating systems software, Data manipulation techniques, Agile development lifecycle, Minimal Viable Data (MVD), Business analysis, Project scope, Functional requirements, Product owners, Data standardization, Data governance methodologies, Data usage, Data acquisition, Legacy data migration, Warehouse implementation, Archive recovery, Data confidentiality, Currency, Accuracy, Integrity, Database cleaning, Data maintenance, Data quality standards, Database performance assessment","data analysis, dbms design, system analysis, operating systems software, data manipulation techniques, agile development lifecycle, minimal viable data mvd, business analysis, project scope, functional requirements, product owners, data standardization, data governance methodologies, data usage, data acquisition, legacy data migration, warehouse implementation, archive recovery, data confidentiality, currency, accuracy, integrity, database cleaning, data maintenance, data quality standards, database performance assessment","accuracy, agile development lifecycle, archive recovery, business analysis, currency, data acquisition, data confidentiality, data governance methodologies, data maintenance, data manipulation techniques, data quality standards, data standardization, data usage, dataanalytics, database cleaning, database performance assessment, dbms design, functional requirements, integrity, legacy data migration, minimal viable data mvd, operating systems software, product owners, project scope, system analysis, warehouse implementation"
SQL Data Engineer,XCM,United Kingdom,https://uk.linkedin.com/jobs/view/sql-data-engineer-at-xcm-3779835954,2023-12-17,Blackpool, United Kingdom,Associate,Remote,"Primary Purpose
Use SQL development skills to develop data transformation and processing solutions for clients as part of implementations of “Horizon” our fast growing customer data platform CDP product.
Horizon comprises a number of sophisticated backend services which process very large volumes of data in real time. Much of this data ends up in SQL databases where further processes are undertaken including transformation and building of models to support analytics and data-science requirements.
This role is part of a team that specialise in providing solutions for clients in support of implementing horizon, there is a varied range of assignments and exposure to many different clients.
Key Responsibilities
Key tasks will also include the following:
Organising data in the right way for subsequent analytics etc
Development of data transformation logic using SQL
Involvement in deployment processes as part of delivery
Problem solving and troubleshooting.
Participation in team meetings and supporting colleagues and co-workers
Database Stack
Horizon uses a number of specialised database engines internally, we don’t expect you to be an expert in all of these at the outset, but over time you’d have the opportunity to become one.
These include:-
Clickhouse (primary OLAP data store for big data – terabyte scale tables with billions of records)
Microsoft SQL and PostgreSQL (primarily used for OLTP workloads)
RocksDB and Redis (used for key value lookups and specialised indexes)
Our broader technology stack includes Kafka / Kubernetes / C# / Javascript & Python
Experience & Qualities
The ideal candidate demonstrates a real interest in databases and database development and wants their work to include big and complex datasets using leading edge technology.
Keen to learn new subjects within chosen focus, and keen to keep up to date with new technology.
Experience with at least one of the primary database products listed.
Advantageous to have knowledge of database architecture, storage concepts, networking.
A working knowledge of JavaScript, Python or other languages is an advantage.
Attributes
Enthusiastic and Proactive
Excellent problem solving and investigation skills
Good stakeholder communications
Show more
Show less","SQL, Data Transformation, Databases, Big Data, OLAP, OLTP, Data Modeling, Analytics, Data Science, Troubleshooting, Clickhouse, Microsoft SQL, PostgreSQL, RocksDB, Redis, Kafka, Kubernetes, C#, Javascript, Python, JavaScript, Database Architecture, Storage Concepts, Networking","sql, data transformation, databases, big data, olap, oltp, data modeling, analytics, data science, troubleshooting, clickhouse, microsoft sql, postgresql, rocksdb, redis, kafka, kubernetes, c, javascript, python, javascript, database architecture, storage concepts, networking","analytics, big data, c, clickhouse, data science, data transformation, database architecture, databases, datamodeling, javascript, kafka, kubernetes, microsoft sql, networking, olap, oltp, postgresql, python, redis, rocksdb, sql, storage concepts, troubleshooting"
Senior Data Engineer,Mozilla,United Kingdom,https://uk.linkedin.com/jobs/view/senior-data-engineer-at-mozilla-3787098307,2023-12-17,Blackpool, United Kingdom,Associate,Remote,"Why Mozilla?
Mozilla Corporation is the non-profit-backed technology company that has shaped the internet for the better over the last 25 years. We make pioneering brands like Firefox, the privacy-minded web browser, and Pocket, a service for keeping up with the best content online. Now, with more than
225
million people around the world using our products each month, we’re shaping the next 25 years of technology. Our work focuses on diverse areas including AI, social media, security and more. And we’re doing this while never losing our focus on our core mission – to make the internet better for everyone.
The Mozilla Corporation is wholly owned by the non-profit 501(c) Mozilla Foundation. This means we aren’t beholden to any shareholders — only to our mission. Along with
60,000
+ volunteer contributors and collaborators all over the world, Mozillians design, build and distribute
open-source
software that enables people to enjoy the internet on their terms.
About The Team & Role
Now more than ever, the Internet is a utility that facilitates modern life. At Mozilla, we take this to heart, striving to build products that keep the Internet open, accessible, and secure for everyone. We handle terabytes of data every day from millions of users to guide our decision-making processes. We need your help to enable the future of Mozilla in a way that makes us proud!
As a Data Engineer At Mozilla, Your Primary Area Of Focus Will Be On Our Analytics Engineering Team. This Team Focuses On Modeling Our Data So That The Rest Of Mozilla Has Access To It, In The Appropriate Format, When They Need It, To Help Them Make Data Informed Decisions. This Team Is Also Tasked With Helping To Maintain And Make Improvements To Our Data Platform. Some Recent Improvements Include Introducing a Data Catalog, Building In Data Quality Checks Among Others. Check Out The Data@Mozilla Blog For More Details On Some Of Our Work. You Will
work with other data engineers to design and maintain scalable data models and ETL pipelines.
help design, build, and improve the infrastructure for ingesting, storing, and transforming data at a scale of tens of terabytes per day.
help design and build systems to monitor and analyze data from Mozilla’s products.
work with data scientists to answer questions and guide product decisions.
General Professional Requirements
Proficiency with one or more of the programming languages used by our teams (SQL and Python).
Strong software engineering fundamentals: modularity, abstraction, data structures, and algorithms.
Ability to work collaboratively with a distributed team.
Specific Skills/Experience
Our team requires skills in a variety of domains. You should have proficiency in one or more of the areas listed below, and be interested in learning about the others.
You have used data to answer specific questions and guide company decisions.
You have experience building modular and reusable ETL/ELT pipelines in distributed databases
You are opinionated about data models and how they should be implemented. You partner with others to map out a business process, profile available data, design and build flexible data models for analysis.
You have experience recommending / implementing new data collection to help improve the quality of data models.
You have experience with data infrastructure: databases, message queues, batch and stream processing
You have experience with highly scalable distributed systems hosted on cloud providers (e.g. Google Cloud Platform)
About Mozilla
Mozilla exists to build the Internet as a public resource accessible to all because we believe that open and free is better than closed and controlled. When you work at Mozilla, you give yourself a chance to make a difference in the lives of Web users everywhere. And you give us a chance to make a difference in your life every single day. Join us to work on the Web as the platform and help create more opportunity and innovation for everyone online.
Commitment to diversity, equity, inclusion, and belonging
Mozilla understands that valuing diverse creative practices and forms of knowledge are crucial to and enrich the company’s core mission. We encourage applications from everyone, including members of all equity-seeking communities, such as (but certainly not limited to) women, racialized and Indigenous persons, persons with disabilities, persons of all sexual orientations, gender identities, and expressions.
We will ensure that qualified individuals with disabilities are provided reasonable accommodations to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment, as appropriate. Please contact us at hiringaccommodation@mozilla.com to request accommodation.
We are an equal opportunity employer. We do not discriminate on the basis of race (including hairstyle and texture), religion (including religious grooming and dress practices), gender, gender identity, gender expression, color, national origin, pregnancy, ancestry, domestic partner status, disability, sexual orientation, age, genetic predisposition, medical condition, marital status, citizenship status, military or veteran status, or any other basis covered by applicable laws. Mozilla will not tolerate discrimination or harassment based on any of these characteristics or any other unlawful behavior, conduct, or purpose.
Group: D
Req ID: R2413
To learn more about our Hiring Range System, please click this link.
Hiring Ranges
Remote UK
£68,000—£99,000 GBP
Show more
Show less","Python, SQL, ETL/ELT pipelines, Data modeling, Data infrastructure, Databases, Message queues, Batch and stream processing, Distributed systems, Cloud computing, Data analysis, Data science, Software engineering, Modularity, Abstraction, Data structures, Algorithms, Distributed team collaboration","python, sql, etlelt pipelines, data modeling, data infrastructure, databases, message queues, batch and stream processing, distributed systems, cloud computing, data analysis, data science, software engineering, modularity, abstraction, data structures, algorithms, distributed team collaboration","abstraction, algorithms, batch and stream processing, cloud computing, data infrastructure, data science, data structures, dataanalytics, databases, datamodeling, distributed systems, distributed team collaboration, etlelt pipelines, message queues, modularity, python, software engineering, sql"
Data Scientist,Remit Resources,United Kingdom,https://uk.linkedin.com/jobs/view/data-scientist-at-remit-resources-3783975435,2023-12-17,Blackpool, United Kingdom,Associate,Remote,"Data Scientist needed for this rapidly expanding consultancy, predominantly focussed on the Insurance Sector. They are trusted partners for leading insurance companies, guiding them in the integration of Big Data, Machine Learning and AI. From optimising core pricing engines to integrating AI into client interactions. They are at the heart of industry transformation.
Experience is good, but it's drive and ambition that will set you apart. If you're keen to immerse yourself in the data science and machine learning space in a small, supportive consultancy with recognised industry leaders, this is an opportunity not to be missed. Their exponential growth trajectory offers a unique blend of learning and advancement opportunities.
Responsibilities are broad and include a working on a wide variety of client projects, usually as part of a team but occasionally independently. Activities will include coding, database and data pipeline design, devops processes and advisory consultancy. They don’t need you to have all of these skills, but they do expect the drive and ambition to develop them and will provide all of the support necessary to do this.
What they do need:
A degree in Mathematics or Computer Science.
A minimum of 2 years of professional experience (not necessarily in data science).
Proficiency in Python and a solid understanding of SQL Database design.
Machine learning experience is a bonus, but again, they provide comprehensive training
This is an opportunity to join a company at a pivotal moment in its trajectory, they are rapidly expanding and there’s real room for rapid advancement and career progression in an environment committed to providing all the support needed to flourish in your role.
To discuss in detail, please contact Richard Morgan at Remit Resources
Show more
Show less","Data Science, Machine Learning, Artificial Intelligence, Big Data, Python, SQL, Database Design, DevOps, Advisory Consultancy, Mathematics, Computer Science","data science, machine learning, artificial intelligence, big data, python, sql, database design, devops, advisory consultancy, mathematics, computer science","advisory consultancy, artificial intelligence, big data, computer science, data science, database design, devops, machine learning, mathematics, python, sql"
Research Data Support Analyst,Yale University,"New Haven, CT",https://www.linkedin.com/jobs/view/research-data-support-analyst-at-yale-university-3779438756,2023-12-17,East Haven,United States,Mid senior,Onsite,"University Job Title
Research Data Support Analyst
Bargaining Unit
None - Not included in the union (Yale Union Group)
Time Type
Full time
Duration Type
Regular
Compensation Grade
Administration & Operations
Compensation Grade Profile
Manager; Program Leader (P5)
Wage Ranges
Click here to see our Wage Ranges
Searchable Job Family
Research Res Support, Research/Support
Total # of hours to be worked:
37.5
Work Week
Standard (M-F equal number of hours per day)
Work Location
Central Campus
Worksite Address
205 Prospect Street
New Haven, CT 06511
Work Model
Hybrid
Position Focus
Provides professional consultation, research and technical support to social scientists engaged in data-intensive research projects across disciplines. Designs and implements statistical methods for descriptive and causal designs and learn new research methods as needed. Leads the design, development, and implementation of machine learning models and operations. Serve as liaison between researchers and technical organizations offering storage, compute, cloud or other data related services. Consult to faculty and researchers regarding their projects and needs related to data infrastructure and data-related processes. Discuss suggested project design solutions. Develop and implement machine learning models to solve research problems. Build and maintain data pipelines for data acquisition, preprocessing and feature engineering to ensure data flows efficiently to the machine learning models. Collaborate with Data Scientists and Software Engineers to implement statistical methods for descriptive and causal designs and learn new research methods as needed. Track model performance and detect anomalies and address issues that arise in production, such as model drift or degradation. Collaborate across teams to integrate AI algorithms into pipelines and products. Develop archiving and dissemination solutions for research across disciplines.
Essential Duties
Consult to faculty and researchers regarding their projects and needs related to data infrastructure and data-related processes. Discuss suggested project designs solutions. 2. Work with procurement and legal teams to assist researchers to locate and acquire data resources. 3. Aid with maintenance and management of data through all phases of research lifecycle; review progress and assure accuracy and compliance of data being acquired and stored. 4. Provide technical support and computing assistance to faculty and researchers in the university’s social science departments and schools. 5. Implement statistical methods for descriptive and causal designs and learn new research methods as needed. 6. Develop archiving and dissemination solutions for research across disciplines. 7. May perform other duties as assigned.
Required Education And Experience
Bachelor's Degree in a related field and four years of related experience in academic or scientific research support or an equivalent combination of education and experience.
Required Skill/Ability 1
Demonstrated ability with spatial analysis tools such as ArcGIS, Google Earth Engine, Mapbox, Leaflet, Carto.
Required Skill/Ability 2
Demonstrated ability with data visualization and reporting software such as D3.js, Tableau or PowerBI.
Required Skill/Ability 3
Demonstrated ability with cloud and High-Performance Computers for computation and storage.
Required Skill/Ability 4
Excellent interpersonal skills with demonstrated ability to effectively communicate with internal and external teams; ability to develop trust, cooperation, and mutual respect.
Preferred Education, Experience And Skills
Master's or PhD Degree in a related field and five years’ experience with R programming. Comfortable with Python, JavaScript, SQL, and other data processing and statistical modeling languages, and Git for versioning.
Drug Screen
No
Health Screening
No
Background Check Requirements
All candidates for employment will be subject to pre-employment background screening for this position, which may include motor vehicle, DOT certification, drug testing and credit checks based on the position description and job requirements. All offers are contingent upon the successful completion of the background check. For additional information on the background check requirements and process visit ""Learn about background checks"" under the Applicant Support Resources section of Careers on the It's Your Yale website.
COVID-19 Vaccine Requirement
Required
The University maintains policies pertaining to COVID-19. All faculty, staff, students, and trainees are required to comply with these policies, which may be found here:
https://covid19.yale.edu/health-guidelines
Posting Disclaimer
The intent of this job description is to provide a representative summary of the essential functions that will be required of the position and should not be construed as a declaration of specific duties and responsibilities of the particular position. Employees will be assigned specific job-related duties through their hiring departments.
EEO Statement
University policy is committed to affirmative action under law in employment of women, minority group members, individuals with disabilities, and protected veterans. Additionally, in accordance with Yale’s Policy Against Discrimination and Harassment, and as delineated by federal and Connecticut law, Yale does not discriminate in admissions, educational programs, or employment against any individual on account of that individual’s sex, sexual orientation, gender identity or expression, race, color, national or ethnic origin, religion, age, disability, status as a special disabled veteran, veteran of the Vietnam era or other covered veteran.
Inquiries concerning Yale’s Policy Against Discrimination and Harassment may be referred to the Office of Institutional Equity and Accessibility (OIEA).
Note
Yale University is a tobacco-free campus
Show more
Show less","ArcGIS, Google Earth Engine, Mapbox, Leaflet, Carto, D3.js, Tableau, PowerBI, Cloud Computing, HighPerformance Computing, R programming, Python, JavaScript, SQL, Spatial Analysis, Data Visualization, Reporting, Statistics, Machine Learning, Model Development, Data Pipelines, Data Infrastructure, Data Acquisition, Data Preprocessing, Feature Engineering, Model Integration, Model Performance, Anomalies Detection, Dissemination Solutions, Archiving Solutions, DataRelated Processes, Data Acquisition, Data Management, Interpersonal Skills, Communication Skills, Team Collaboration, Git","arcgis, google earth engine, mapbox, leaflet, carto, d3js, tableau, powerbi, cloud computing, highperformance computing, r programming, python, javascript, sql, spatial analysis, data visualization, reporting, statistics, machine learning, model development, data pipelines, data infrastructure, data acquisition, data preprocessing, feature engineering, model integration, model performance, anomalies detection, dissemination solutions, archiving solutions, datarelated processes, data acquisition, data management, interpersonal skills, communication skills, team collaboration, git","anomalies detection, arcgis, archiving solutions, carto, cloud computing, communication skills, d3js, data acquisition, data infrastructure, data management, data preprocessing, datapipeline, datarelated processes, dissemination solutions, feature engineering, git, google earth engine, highperformance computing, interpersonal skills, javascript, leaflet, machine learning, mapbox, model development, model integration, model performance, powerbi, python, r programming, reporting, spatial analysis, sql, statistics, tableau, team collaboration, visualization"
Senior Data Engineer,RMIT University,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-rmit-university-3779913000,2023-12-17,Melbourne, Australia,Mid senior,Onsite,"Overview
Full-time, Continuing position
Salary Level HEW 8 + 17% Superannuation and Flexible Working Arrangements
Based at the Melbourne CBD campus, and hybrid ways of working
About You
The Senior Data Engineer will have a primary focus on the preparation, maintenance and operation of pipelines that deliver large, complex datasets, that form the basis of master data management (MDM), analysis, dashboards, statistical analysis & modelling and data quality analysis. They will also deliver: standards and practices for data architecture and engineering; data system design, development and operation; conceptual, logical and physical data modelling; MDM models, integrations, user interfaces, workflows, etc.
The Senior Data Engineer will operate in a squad working with Data Engineers and others, supporting the team in delivering data driven solutions to a variety of communities across the University. The Senior Data Engineer will also be a key member of a guild focused on developing Data Architecture and Engineering processes and standards for the Data & Analytics team and other analytics teams.
To be successful in this position, you’ll have as a minimum:
Proven experience (3+ years) working with multiple data sets and distributed computing tools.
Demonstrated experience in the design, delivery and operation of data sets for education institutions.
Experience with agile (data) development and operations (DataOps) will be highly regarded.
Experience with cloud approaches and capabilities will be highly regarded.
Experience with Master Data Management (MDM) approaches and capabilities will be highly regarded.
Ability to design conceptual, logical and physical data models will be highly regarded.
Experience with Informatica, Snowflake, dbt, BitBucket and SageMaker will be highly regarded.
Extensive experience (5+ years) of data transformation languages such as SQL and Python.
Strong analytic capability, with the ability to integrate team efforts to achieve goals and overall program activities into the larger organisation.
Understanding of data analytics and data science practice particularly in sampling, experimentation, advanced statistical analysis, triangulation and database design – ensuring engineering deliverables are effective for subsequent use.
Strong communication and interactive skills – oral, written, visual – including for triaging conflict, navigating ideation barriers, mitigating risk, fostering thought diversity and liaising / negotiating with a range of people to achieve agreed outcomes.
Please Note: Appointment to this position is subject to passing a Working with Children and National Police Check.
About The Portfolio
Office of the Chief Data & Analytics Officer supports and enables RMIT’s business strategy by leading and driving the data and analytics agenda for the enterprise. This involves being accountable and responsible for:
Developing and implementing RMIT’s data and analytics strategy and ensuring ongoing alignment with university
Defining requirements and standards for data and analytics technologies to be delivered by Information Technology
Coordinating and governing data management activities for analytics and reporting
Developing and coordinating advanced analytics capabilities to resolve key university challenges.
To Apply
Please submit your
CV
and
covering letter
addressing your suitability for this position by clicking on the ‘
Apply
’ link at the top of this page.
For further information about this position, please see the Position Description hyperlinked below.
Position Description - Senior Data Engineer
Applications Close
20 Dec 2023 11.59 pm
RMIT is an equal opportunity employer committed to
being
a child safe organisation. We are dedicated to attracting, retaining and developing our people regardless of gender identity, ethnicity, sexual orientation, disability and age. Applications are encouraged from all sectors of the community and we strongly encourage applications from the Aboriginal and/or Torres Strait Isla
n
der community.
At RMIT, we are committed to supporting adjustments throughout the recruitment and selection process, as well as during employment. We actively support and encourage people with disability to apply to RMIT. To discuss adjustment requirements, please contact Kassie (Senior Talent Acquisition Advisor), via
talentsupport@rmit.edu.au
or visit our Careers page for more contact information - https://www.rmit.edu.au/careers.
We are a Circle Back Initiative Employer – we commit to respond to every applicant.
Show more
Show less","Data Engineering, Master Data Management (MDM), Data Architecture, Data Modeling, Data Quality Analysis, Data Pipelines, Data Warehousing, Cloud Computing, Data Analytics, Data Science, SQL, Python, Informatica, Snowflake, dbt, BitBucket, SageMaker, Agile Development, DataOps","data engineering, master data management mdm, data architecture, data modeling, data quality analysis, data pipelines, data warehousing, cloud computing, data analytics, data science, sql, python, informatica, snowflake, dbt, bitbucket, sagemaker, agile development, dataops","agile development, bitbucket, cloud computing, data architecture, data engineering, data quality analysis, data science, dataanalytics, datamodeling, dataops, datapipeline, datawarehouse, dbt, informatica, master data management mdm, python, sagemaker, snowflake, sql"
Lead Data Engineer,Mantel Group,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/lead-data-engineer-at-mantel-group-3752403511,2023-12-17,Melbourne, Australia,Mid senior,Hybrid,"About Us
Mantel Group is an Australian-owned technology consulting business with capabilities across Cloud, Digital, Data & Security. Since our inception in November 2017, we have experienced remarkable growth across Australia & New Zealand and are honoured to be recognised as a Great Place to Work for 4 years in a row!
We hire smart and talented people and get out of their way. As a principle-based organisation we have a flat structure with no hierarchy. By focusing on our five principles and not getting caught up in red tape, we trust you to get the job done!
Data
Working in data at Mantel Group means there’s opportunities to align to a particular cloud platform or remain cloud agnostic. Our teams are made up of awesome people who specialise in numerous areas of data so there’s endless opportunities for you including data engineering, machine learning, data analytics and data science.
About The Role
As a
Lead Data Engineer
, you will join a group of awesome data professionals helping to define what modern data engineering means in a lot of Australia's best known organisations. You will work with us to design and build leading edge data systems. The work we do is broad but always focused on delivering the best outcome for our clients. Your work might range from building a full data platform; building data pipelines to load large volumes of data using data devops principles; or helping with data architecture and strategy.
Here's a snapshot of what you'll be doing:
Make decisions across tech and system that allow teams to build better and more reliable systems
Enable technology and technical teams to deliver successful projects to clients
Design, propose and obtain acceptance of an overall architecture
Adopt new technologies while also using the expertise you have built on modern big data tools and cloud services
Have conversations with clients and potential clients about Mantel Group's offerings relative to the overall strategy
Creating proposals and walkthroughs with customers
Challenge existing processes and suggest improvements in constructive ways
Help shape the technology strategy for companies of various sizes
Represent Mantel Group within the data engineering community
Provide guidance and mentorship to others within the team through on-the-job teaching, hands-on training and other similar activities
We want to hear from you if you have:
Strong consulting experience, either with external clients or internally within an organisation
Demonstrable experience leading delivery of technical projects and project teams varying in size
Commercial data engineering experience across different industries and organisations
Modern Big Data experience with main focus on working in a cloud based environment (Azure, AWS or GCP)
Excellent SQL and/or Python skills, expertise in native development tools
Background in data engineering and/or software engineering
Love being on the tools, such as Spark, Snowflake, Databricks, Airflow, DBT, Kafka, Git, Terraform and/or others
Ability to engage with senior level stakeholders and having a broad influence in the internal and client’s teams
The ability to explain technical concepts regardless of the audience
Experience guiding and supporting teams, whether internal or on the client side
We value diversity in our people and the ideas they bring to our team. We look after our team and pride ourselves on our supportive and friendly culture. We’re looking for people who identify with our values whilst also bringing their own individual perspective on their work, choosing your own adventure in how you help us support our clients.
What you can expect from us:
We know you won’t have one job for life. At Mantel Group we believe in supporting our team to take their career in a direction that aligns with their passions. We have internal opportunities across Cloud, Data, Security and Digital.
You’ll get all the tools you need to hit the ground running including a new phone, laptop & swag.
We believe in unique experiences for all. Our My Deal program allows you to tailor your yearly plan, with the support of your Leader, to decide on what’s most important to you. That might be extra professional development, extra annual or parental leave, time to work on your side hustle, or something else completely different! One size does not fit all.
You’ll be genuinely supported by an organisation that cares about not only you but your family as well, Mantel Group offers Flexible Personal Leave options for those unplanned moments in life.
We support a flexible hybrid approach to working which is guided by our principles; we trust each other to “make good choices” about the best workplace locations for the requirements of the project, role and client. This can change based on our client needs.
Click ‘Apply’ to be considered for this role and our Talent team will be in touch.
Check out ‘how we hire’ to find out what’s in store if you’re successful and get to know us better by visiting our website and following Mantel Group on LinkedIn.
#EAST
Show more
Show less","Cloud, Azure, AWS, GCP, Data Engineering, Machine Learning, Data Analytics, Data Science, Spark, Snowflake, Databricks, Airflow, DBT, Kafka, Git, Terraform, SQL, Python, Data Science, Data Architecture, Data Platforms, Big Data, Software Engineering, Consulting","cloud, azure, aws, gcp, data engineering, machine learning, data analytics, data science, spark, snowflake, databricks, airflow, dbt, kafka, git, terraform, sql, python, data science, data architecture, data platforms, big data, software engineering, consulting","airflow, aws, azure, big data, cloud, consulting, data architecture, data engineering, data platforms, data science, dataanalytics, databricks, dbt, gcp, git, kafka, machine learning, python, snowflake, software engineering, spark, sql, terraform"
Data Modelling Analyst,iTech Solutions,"Eden Prairie, MN",https://www.linkedin.com/jobs/view/data-modelling-analyst-at-itech-solutions-3714398316,2023-12-17,Orono,United States,Associate,Onsite,"Job Role: Data Modelling Analyst
Location:
Eden Prarie, MN
Duration: Long term
Job Location
Eden Prarie, MN
Project Duration
6 Months
Role
Data Modelling Analyst
Job Description - In detail
Works directly with internal data SMEs (business or technology) to model data from source to target
Ability to design and maintain the logical and physical data models for:
Dimensional Data model (star schema)
Relational data model - Third normal form
Data Vault
Drive and document modeling and model review sessions.  Make model adjustments based on feedback
Experience in using one or more data modelling tools
Data analysis skills. Understanding of data transformation and processing
Understand data modeling concepts including normalization, star schema, data vault modeling
Understanding of RDBMS and non-RDBMS  technology
Ability to communicate the data models to technical and non-Technical audience.
Maintain a data dictionary
Attends scrum and other internal meetings
Experience Level
3+ Years of relevant experience
Show more
Show less","Data Modelling, Logical Data Model, Physical Data Model, Dimensional Data Model, Relational Data Model, Third Normal Form, Data Vault, Data Modelling Tools, Data Analysis, Data Transformation, Data Processing, Normalization, Star Schema, Data Vault Modeling, RDBMS, NonRDBMS, Scrum","data modelling, logical data model, physical data model, dimensional data model, relational data model, third normal form, data vault, data modelling tools, data analysis, data transformation, data processing, normalization, star schema, data vault modeling, rdbms, nonrdbms, scrum","data modelling, data modelling tools, data processing, data transformation, data vault, data vault modeling, dataanalytics, dimensional data model, logical data model, nonrdbms, normalization, physical data model, rdbms, relational data model, scrum, star schema, third normal form"
Data Engineer Remote,Avani Tech Solutions Private Limited,"Wayzata, MN",https://www.linkedin.com/jobs/view/data-engineer-remote-at-avani-tech-solutions-private-limited-3758748562,2023-12-17,Orono,United States,Mid senior,Onsite,"Pay: 75-78/hr
The Bio industrial Data & Analytics team seeks an Analytics Engineer with a business analyst mindset.
This person will work as part of a global team in an Agile framework to design, build and execute high-performing data models and analytical solutions (up to and including visualizations and/or datasets for business consumers).
We Need Someone
with a consulting mindset who will effectively engage with people from all business functions and help define the problem to solve.
who can ""show, not tell"" how to solve that problem with working prototypes of target data & analytics outputs.
who knows the WHAT and the HOW of good data management and modelling
Our Tech Stack
source systems = SAP (TC2), SalesForce ""LEAP"", and more!
data lake = Hadoop ecosystem (Client Data Platform)
data cataloging = Alation (Client Data Catalog)
data munging and modelling tools = SQL, Python, KNIME
end user analytics = Power BI
Genuine curiosity about the emerging bio-derived performance chemicals industry is a huge plus. You need to learn the business to help the business.
Show more
Show less","Agile, Business Analytics, Data Lake, Data Munging, Data Modeling, Hadoop, KNIME, Power BI, Python, SAP, Salesforce, SQL","agile, business analytics, data lake, data munging, data modeling, hadoop, knime, power bi, python, sap, salesforce, sql","agile, business analytics, data lake, data munging, datamodeling, hadoop, knime, powerbi, python, salesforce, sap, sql"
Sr Tech Lead-Data Engineer-Healthcare,Zortech Solutions,"Eden Prairie, MN",https://www.linkedin.com/jobs/view/sr-tech-lead-data-engineer-healthcare-at-zortech-solutions-3679268728,2023-12-17,Orono,United States,Mid senior,Onsite,"JOB TITLE
Sr Tech Lead-Data Engineer-Healthcare
LOCATION
Eden Prairie, MN (100% Onsite)
DURATION - 6+ Months
Job Description
12+ years of experience in Data Engineering technical field.
Lead the team with technical expertise on Teradata, DataStage and ETL techniques.
Apply SQL query optimization techniques to ensure performance of query changes.
Understand the business requirements and convert them into technical specifications.
Provide required technical support to PSL team for stakeholder coordination and availability.
Help resolving technical dependencies and remove impediments.
Provides a key role in reviewing Business Case, Risks & Mitigation Plan, Roll Back plans, Project Metrics.
Evaluate and communicate impact of the solution to address data issues.
Review impact analysis results
Show more
Show less","Data Engineering, Teradata, DataStage, ETL, SQL, Data analysis, Requirements analysis, Software development, Project management, Risk management","data engineering, teradata, datastage, etl, sql, data analysis, requirements analysis, software development, project management, risk management","data engineering, dataanalytics, datastage, etl, project management, requirements analysis, risk management, software development, sql, teradata"
Lead Data SAS Engineer -US,Zortech Solutions,"Eden Prairie, MN",https://www.linkedin.com/jobs/view/lead-data-sas-engineer-us-at-zortech-solutions-3707655072,2023-12-17,Orono,United States,Mid senior,Onsite,"Role: Lead Data SAS Engineer
Location: Eden Prairie, MN or Hartford CT-Day one onsite
Duration: 6+ Months
Job Description
Experience with Teradata SQL ,SAS Linux, and SAS Mainframes ,SAS coding skills, proficient in both SAS (UNIX) or SAS Mainframe, able to recognize which SAS components are SAS mainframe versus UNIX and requiring conversion
Understanding of available scheduling functions within SAS , Unix, and Teradata utilities
Basic healthcare knowledge, to understand the intent of each report being converted
Used to / able to communicate with business resources and document Pros and Cons of Various option to make changes to their system
Flexible toward the approach to solve customer problem depending on the situation , can offer solution approaches to address the concern.
Experience with testing system migration to Teradata and Experience working with Various consumption tools .
Can document test cases and test result and do a show and tell with business .
Automation experience utilizing Python or Shell ,airflow, TWS to SAS scheduling.
Should be able to drive and operate independently without much hand holding after Onboarding
Please share your updated Resume to pavan@zortechsolutions.ca
Show more
Show less","Teradata SQL, SAS Linux, SAS Mainframes, SAS coding, SAS, Unix, Teradata utilities, Healthcare knowledge, Python, Shell, Airflow, TWS, SAS scheduling","teradata sql, sas linux, sas mainframes, sas coding, sas, unix, teradata utilities, healthcare knowledge, python, shell, airflow, tws, sas scheduling","airflow, healthcare knowledge, python, sas, sas coding, sas linux, sas mainframes, sas scheduling, shell, teradata sql, teradata utilities, tws, unix"
Business Data Analyst,Insight Global,"Maple Grove, MN",https://www.linkedin.com/jobs/view/business-data-analyst-at-insight-global-3782261636,2023-12-17,Orono,United States,Mid senior,Hybrid,"Business/Data Analyst
Location: Local to Minneapolis, MN & willing to work onsite in Maple Grove
Duration: 12-month contract
Compensation
:
$40/hr to $50/hr.
Exact compensation may vary based on several factors, including skills, experience, and education.
Benefit packages for this role will start on the 31st day of employment and include medical, dental, and vision insurance, as well as HSA, FSA, and DCFSA account options, and 401k retirement account access with employer matching. Employees in this role are also entitled to paid sick leave and/or other paid time off as provided by applicable law.
Job description:
Insight Global is seeking a developing IT data and business analytics professional local to Minneapolis, MN to join the team of one of our largest medical device clients. In this role, you will work collaboratively across various Salesforce product teams to research issues, define processes, and establish best practices for the Salesforce.com platform. You must be able to confidently identify opportunities for business process improvements and establish effective, new processes. This role requires someone who is data driven, detail-oriented, and comfortable working with custom objects and any troubleshooting that may come with that. This is the ideal opportunity for someone to gain more hands-on, high-level experience with Salesforce.com who may be interested in solution architectures in their career.
Must haves:
Bachelor’s degree or some form of higher education/schooling required
3-5 years of work experience with delivery and support of software development solutions and environments
Understanding of the software development lifecycle and methodologies, including Agile
Understanding and experience providing software solution analysis, project management, and testing leadership
Ability to partner with business divisions and functions in delivery
Experience delivering and maintaining systems within a medical device or a highly regulated environment
Previous experience working in a cross-functional business environment
Ability to identify and implement process improvement opportunities
Strong written and verbal communication, with the ability to effectively communicate with leadership
Strong problem solving and analytical skills
Demonstrates courage in ensuring value-driven solutions are delivered
Ability to manage multiple projects with dependencies and deliver on time and budget
Proven team management, decision making and task prioritization skills
Daily Responsibilities:
Manage project teams and implementations with an agile/scrum approach
Appropriately communicate task status and manage project timelines/action items
Manage communications amongst technical staff, vendors, and internal partners
Escalate project risks/issues to management as required
Assure quality of business systems by implementing test plans and developing/executing test specifications
Work closely with development teams (internal and external) to assure timely solutions are delivered according to the specifications and implementation needs of the business
More about the Enterprise Salesforce Team:
We support 34 global product teams with varying goals as they deliver for 12,000 Salesforce users. These teams support Sales, Service, Marketing, eCommerce, educational initiatives and more. Previously these teams have gone unchecked and have created some fantastic functionality. Our efforts ensure they develop a scalable and sustainable functionality. Now is the time for us to ensure we reduce, reuse, and repurpose to capitalize on the investment.
Show more
Show less","Salesforce, Agile, Business Analysis, Data Analysis, Project Management, Software Development, Testing, Communication, Problem Solving, Analytical Skills, Team Management, Decision Making, Task Prioritization, Test Plans, Development, Implementation, Risk Management, Quality Assurance","salesforce, agile, business analysis, data analysis, project management, software development, testing, communication, problem solving, analytical skills, team management, decision making, task prioritization, test plans, development, implementation, risk management, quality assurance","agile, analytical skills, business analysis, communication, dataanalytics, decision making, development, implementation, problem solving, project management, quality assurance, risk management, salesforce, software development, task prioritization, team management, test plans, testing"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Philadelphia, PA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707751,2023-12-17,East Greenwich,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Git, Apache Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Snowflake, AWS, GCP, Azure, DynamoDB, NoSQL, ETL, Kafka, Storm, SparkStreaming, Machine learning, Data mining, Data cleaning, Data normalization, Data modeling, Data visualization, Pandas, R, Statistical analysis, NLP, Conversational AI, Recommender systems, Distributed systems, Microservices, Docker images, Streamprocessing systems","python, java, bash, sql, git, apache airflow, kubernetes, docker, helm, spark, pyspark, snowflake, aws, gcp, azure, dynamodb, nosql, etl, kafka, storm, sparkstreaming, machine learning, data mining, data cleaning, data normalization, data modeling, data visualization, pandas, r, statistical analysis, nlp, conversational ai, recommender systems, distributed systems, microservices, docker images, streamprocessing systems","apache airflow, aws, azure, bash, conversational ai, data cleaning, data mining, data normalization, datamodeling, distributed systems, docker, docker images, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, microservices, nlp, nosql, pandas, python, r, recommender systems, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, streamprocessing systems, visualization"
Business Data Analyst,River Edge Behavioral Health,"Macon, GA",https://www.linkedin.com/jobs/view/business-data-analyst-at-river-edge-behavioral-health-3698090498,2023-12-17,Fort Valley,United States,Mid senior,Onsite,"Data Analyst
About River Edge Behavioral Health
River Edge Behavioral Health exists to share the stories of more than 10,000 people with behavioral health, substance abuse, and intellectual and developmental disabilities and inspire our clients and the communities to make life better. River Edge is where caring professionals provide expert services to the people we serve by offering safe spaces for groups, coordinated care, evidenced-based treatments, inpatient stabilization, and substance abuse services.
At River Edge Behavioral Health, we aspire to continuously lead with our values and beliefs that enable individuals to develop their potential, bring their whole selves to the workplace, and engage in an environment of inclusion. At River Edge, we know you can choose to work anywhere, be we strive to be an employer of choice. We work every day to make your employee experience great. We believe in the skilled team members we hire and their ability to \""make life better\"" for someone on their recovery journey or struggling with behavioral health challenges.
River Edge Behavioral Health seeks a full-time energetic, enthusiastic Data Analyst. The ideal candidate will organize data related to sales numbers, market research, logistics, linguistics, or other behaviors.
Duties and Responsibilities
Work closely with program managers to understand and maintain focus on their analytics needs, including critical metrics and KPIs, and deliver actionable insights to relevant decision-makers.
Create and maintain rich interactive visualizations through data interpretation and analysis, with reporting components from multiple data sources.
Define and implement data acquisition and integration logic, selecting an appropriate combination of methods and tools within the defined technology stack to ensure optimal scalability and performance of the solution.
Develop and maintain databases by acquiring data from primary and secondary sources and build scripts to make our data evaluation process more flexible or scalable across datasets.
Demonstrate knowledge of and skill in adaptability, decision-making, customer service, interpersonal relations, oral communication, problem-solving, project management, quality management, teamwork, written communication, group presentations, group process facilitation, influence, results orientation, strategic thinking, team building.
Qualifications
Bachelor's degree in mathematics, computer science, economics, health information management or a related field.
Five (5) years of data analyst or related experience, including proficiency with analytical software or equivalent related education.
Proficiency in statistics, data analysis, and research methods
Provide 2 Professional references (Former employers only, no family members),
All applicants must pass a satisfactory background clearance and pre-employment drug test.
Additional Information
The Application Process
All qualified applicants will be considered.
This position is subject to close once a satisfactory candidate pool has been identified.
The hiring managers will contact only those selected for an interview.
Applicants who are not selected will receive notification via email.
Due to the volume of applications received, we cannot provide information on application status by phone or email.
Safe Working Environment
We at River Edge believe every employee has a right to a safe work environment. Therefore, we recommend full vaccination of all employees. Getting the vaccine, frequently washing your hands, sanitizing common areas, and wearing your mask are the most effective ways to fight.
Diversity and Inclusion
River Edge Behavioral Health is committed to creating a diverse and inclusive work environment and is proud to be an equal employment opportunity employer. All qualified applicants will receive consideration for employment regardless of race, color, religion, gender, gender identity or expression, sexual orientation, nationality, genetic makeup, disability, age, or veteran status.
Drug-Free Workplace
River Edge Behavioral Health is a drug-free workplace with a longstanding commitment to providing a safe, quality-oriented, and productive work environment. In compliance with the Drug-Free Workplace Act of 1998, all applicants must pass a satisfactory background clearance and pre-employment drug screen.
At-Will Workplace
Employment with River Edge Behavioral Health is at will. At-will means your employment relationship with River Edge Behavioral Health or Affordable Business Solution is for an indefinite period and is subject to termination by you or River Edge Behavioral Health, with or without cause, with or without notice, and at any time.
EEOC Statement
River Edge Behavioral Health is an Equal Opportunity Employer: River Edge Behavioral Heath recruits qualified candidates for positions in its service area. It is the policy of River Edge Behavioral Health to provide equal employment opportunities to all employees and applicants for employment and prohibits discrimination or harassment of any type without regard to race, color, sex, religion, national origin, age, disability, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.
Show more
Show less","Data Analysis, Data Interpretation, Statistics, Research Methods, Data Acquisition, Data Integration, Database Development, Scripting, Data Visualization, DecisionMaking, ProblemSolving, Project Management, Quality Management, Communication, Teamwork, Presentation Skills, Strategic Thinking, SQL, Python, R, Tableau, Power BI","data analysis, data interpretation, statistics, research methods, data acquisition, data integration, database development, scripting, data visualization, decisionmaking, problemsolving, project management, quality management, communication, teamwork, presentation skills, strategic thinking, sql, python, r, tableau, power bi","communication, data acquisition, data integration, data interpretation, dataanalytics, database development, decisionmaking, powerbi, presentation skills, problemsolving, project management, python, quality management, r, research methods, scripting, sql, statistics, strategic thinking, tableau, teamwork, visualization"
Data Analyst- External Manufacturing,Congo Brands,"Louisville, KY",https://www.linkedin.com/jobs/view/data-analyst-external-manufacturing-at-congo-brands-3767138288,2023-12-17,Westport,United States,Associate,Onsite,"Our Company
Congo Brands is at the epicenter of where consumer demand meets product development. Through brand partnerships with lead influencers, we are afforded an inside look at what consumers truly desire. Congo Brands is a health and wellness brand that sells supplements, fitness snacks, hydration drinks, energy drinks, and more through e-commerce and retail partnerships. Congo Brands houses some of the world’s most noted brands including Alani Nu, Prime Hydration, and 3D Energy. Congo Brands is on pace to become one of the world’s most noted consumer product companies, with revenue growth increasing exponentially year over year. Congo Brand’s goal is to be the go-to ‘Better for You Brand’ for everything from ready-to-drink beverages, snacks, supplements and more.
The Role:
We are seeking a detail-oriented and experienced Manufacturing Data Analyst with expertise in NetSuite to join our team. The successful candidate will play a critical role in maintaining the accuracy and integrity of our manufacturing master data, ensuring seamless and efficient production processes. You will be responsible for managing and analyzing master data within the ERP system, identifying opportunities for process improvements, and collaborating with cross-functional teams to support the organization's manufacturing operations and setting the stage to further enhance to the success of the organization.
Key Responsibilities:
Help optimize purchasing strategy by evaluating cost-savings and lead-time reduction opportunities
Manage and maintain accurate and up-to-date master data within the NetSuite ERP system, including bill of materials (BOMs), routings, work centers, and item master data. This involves regularly updating and reviewing the data to ensure accuracy, completeness, and consistency across the organization
Clean data of old and inactive items in preparation for the expanded use of supply chain planning tools. This will help us optimize our inventory levels and improve our overall planning capabilities
Conduct regular audits and reviews to ensure data accuracy, completeness, and consistency across the organization. This includes verifying the data against established standards and making necessary corrections or adjustments
Update purchase orders based on vendor order acknowledgements.
Collaborate with cross-functional teams, including production, planning, quality, and engineering, to gather requirements and input data into the system accurately. This involves actively engaging with various stakeholders to understand their needs and ensuring that the data entered aligns with their requirements
Identify, investigate, and resolve data discrepancies, working closely with relevant stakeholders to implement corrective actions. This requires a meticulous approach to analyze and address any inconsistencies or errors in the data, working collaboratively with the teams involved to find solutions
Support in documenting and managing product specification changes. This will ensure that our products information is up to date and readily available for reference
Develop and maintain standard operating procedures (SOPs) and best practices for master data management in the system. This involves documenting the processes and guidelines to ensure consistent and efficient management of master data, promoting uniformity and accuracy across the organization
Provide support and training to team members and end-users on master data management processes and the use of ERP. This includes assisting colleagues and users in understanding and utilizing the system effectively, offering guidance and troubleshooting assistance as needed
Collaborate with IT and other stakeholders to optimize functionality for manufacturing operations, including system enhancements, customizations, and integration with other systems. This involves actively working with IT and relevant teams to identify opportunities for improvement, customization, and integration to streamline manufacturing operations
Overall, the goal is to ensure that the ERP system's master data is accurate, up-to-date, and consistent, enabling efficient and effective operations across the organization
Requirements
Bachelor's degree in a relevant field, such as Supply Chain Management, Industrial Engineering, or Information Systems
Preferred Industries: Food and Beverage, Food production, Wine and Spirits, Dairy or Nutraceuticals
A minimum of 1 year of experience in master data management, preferably in a manufacturing environment
Demonstrated expertise in NetSuite/Industry leading ERP systems, with a strong understanding of manufacturing and supply chain modules
Knowledge of manufacturing processes, including BOMs, routings, work centers, and inventory management
Exceptional attention to detail and a commitment to data accuracy and integrity
Strong analytical, problem-solving, and decision-making skills
Excellent interpersonal and communication skills, with the ability to collaborate effectively with cross-functional teams
Proficiency in Microsoft Office Suite, particularly Excel, and experience with data analysis tools and techniques
Benefits
Competitive Salary
Health/Vision/Dental Benefits
15 days of PTO + 8 paid holidays
6 weeks paid parental leave
401K plan with employer match
Monthly cell phone stipend
AND SO MUCH MORE!
#PRIME
Congo Brands, LLC. is an Equal Opportunity Employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, veteran status, age, or any other classification protected by Federal, state, or local law.
Show more
Show less","Manufacturing Data Analyst, NetSuite ERP, Data Management, Supply Chain Planning, Master Data Management, Bill of Materials (BOMs), Routings, Work Centers, Item Master Data, Purchase Orders, Vendor Order Acknowledgements, Crossfunctional Collaboration, Data Discrepancies, Standard Operating Procedures (SOPs), Product Specification Changes, System Enhancements, Customizations, Microsoft Office Suite, Excel, Data Analysis Tools, ProblemSolving, DecisionMaking, Communication, Team Collaboration","manufacturing data analyst, netsuite erp, data management, supply chain planning, master data management, bill of materials boms, routings, work centers, item master data, purchase orders, vendor order acknowledgements, crossfunctional collaboration, data discrepancies, standard operating procedures sops, product specification changes, system enhancements, customizations, microsoft office suite, excel, data analysis tools, problemsolving, decisionmaking, communication, team collaboration","bill of materials boms, communication, crossfunctional collaboration, customizations, data analysis tools, data discrepancies, data management, decisionmaking, excel, item master data, manufacturing data analyst, master data management, microsoft office suite, netsuite erp, problemsolving, product specification changes, purchase orders, routings, standard operating procedures sops, supply chain planning, system enhancements, team collaboration, vendor order acknowledgements, work centers"
Actuarial Data Scientist,RealREPP,"Louisville, KY",https://www.linkedin.com/jobs/view/actuarial-data-scientist-at-realrepp-3761381688,2023-12-17,Westport,United States,Associate,Onsite,"RealREPP, a full service recruiting firm, is currently partnered with a direct seller insurance company with a national presence in recruiting for an Actuarial Data Scientist to work alongside actuaries and other data professionals to build and evaluate pricing and other enterprise models. You will navigate projects through the entire data science lifecycle, from problem definition to data exploration, data wrangling, modeling, analysis, and deployment into production. You will maintain a close partnership with IT to ensure that models can be deployed quickly and monitored on an ongoing basis.
This opportunity offers an excellent base salary + industry leading benefits package along with a collaborative and innovative company culture.
Requirements
Bachelor’s or master’s degree in data science, computer science, operations research, statistics, applied mathematics, or a related quantitative field. PhD or insurance designations such as ACAS desirable.
Minimum of 3 years’ experience in an insurance pricing role
Experience leading predictive modeling projects involving new rating structures
Demonstrate knowledge of when to apply predictive modeling techniques such as logistic regression, time series, neural networks, random forest, boosting, text mining, clustering, deep learning, optimization, etc.
Proficiency in analytical programming languages like SAS, R, Python, etc.
Experience exploring and manipulating large amounts of structured and unstructured data using SQL
Excellent communication and presentation skills, including storytelling and other methods to guide, inspire, and provide insights to non-technical stakeholders
If you are interested in this opportunity, please apply directly. All qualified candidates will be contacted directly.
*We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.*
Show more
Show less","Actuarial Data Science, Predictive Modeling, Rating Structures, Logistic Regression, Time Series, Neural Networks, Random Forest, Boosting, Text Mining, Clustering, Deep Learning, Optimization, SAS, R, Python, SQL, Data Exploration, Data Wrangling, Storytelling","actuarial data science, predictive modeling, rating structures, logistic regression, time series, neural networks, random forest, boosting, text mining, clustering, deep learning, optimization, sas, r, python, sql, data exploration, data wrangling, storytelling","actuarial data science, boosting, clustering, data exploration, data wrangling, deep learning, logistic regression, neural networks, optimization, predictive modeling, python, r, random forest, rating structures, sas, sql, storytelling, text mining, time series"
HR DATA ANALYST,BrightSpring Health Services,"Louisville, KY",https://www.linkedin.com/jobs/view/hr-data-analyst-at-brightspring-health-services-3703278664,2023-12-17,Westport,United States,Mid senior,Onsite,"Our Company
BrightSpring Health Services
Overview
Our support center is based in Louisville, Kentucky, though the team includes employees in multiple states. The team supports our mission and family of brands, across the country. We encourage you to Live Your Best Life and come work for the best. Apply today!
Responsibilities
Products owner for HR dashboard. Partner with IT to continue build of HR dashboard/reporting in Power BI
This includes metrics such as turnover, stability, retention, hires, terms, and headcount
Along with IT, manages the planning, development, and implementation of each Phase
Works with business stakeholders to identify, evaluate and prioritize requirements for each Phase (at a segment and Enterprise level)
Responsible for change management – gatekeeper for changes to the dashboard
Focuses on developing and maintaining reporting and analytics processes to support Company’s enterprise financial and human resources metrics
Works with large datasets from multiple sources
Responsible for ad hoc requests from multiple Financial and HR systems
Responsible for monthly metric data loaded into Oracle Fusion
Works with Senior Management to align on all HR Metric calculations (to include at a minimum Operations leadership, CHRO, and CFO)
Defines standard metric calculations for the Enterprise and each Segment
Creates a Data Dictionary with all terminology and calculation detail (to include source of data and specific calculation for each metric)
Assists and/or provides data to HR leaders to explain variances in various defined HR metrics
Creates customized metric reports for business partners and/or senior leaders, as needed
Runs weekly PAF report for FP&A and CFO and others as needed
Role will interface with senior operations leadership, general managers, and Executive leadership to present and explain trends in HR metrics
Qualifications
Bachelor’s Degree in Information Technology, Computer Science, Finance, or equivalent work experience
Understanding of HR/people data, project management, data governance, basic accounting, timekeeping, financial analysis, and reporting
Five plus years building business intelligence and analytics solutions
Experience with relational databases and SQL
Experience with financial systems, such as Oracle and Workday Planning
Experience with, and knowledge of, HR systems, such as ADP and iCIMS
Working knowledge of data visualization tools, such as Power BI, Oracle Business Intelligence, Tableau
Comprehensive knowledge of all Microsoft Office Applications, including Excel, Access, Word and Powerpoint
Microsoft Excel power user, including pivot tables, macros, array formulas, data tables, model building
Experience in working with large databases and data sources
Experience building and maintaining analytic data sets
Maintain strict confidentiality around compensation related data
Ability to work independently and manage multiple projects/tasks simultaneously
Reliable, accountable, accurate and self-motivated
About Our Line Of Business
BrightSpring Health Services is a leading provider of complementary home and community-based pharmacy and health services for complex populations in need of chronic and/or specialized care. Through the company’s pharmacy and provider services to seniors and specialty (including behavioral) populations, we provide comprehensive care and clinical services in 50 states to over 360,000 customers, clients and patients daily. The company’s services foster greater patient and family satisfaction, improve outcomes and reduce health care system costs, and are supported by industry-leading quality outcomes. For more information, visit www.brightspringhealth.com . Follow us on Facebook , Twitter and LinkedIn .
Show more
Show less","Microsoft Office Suite, SQL, Microsoft Excel, Data visualization, Power BI, Tableau, Oracle Business Intelligence, Oracle Fusion, Financial analysis, Project management, Data governance, Data modeling, Data sets, Microsoft Access, Microsoft Word, Microsoft Powerpoint, Macros, Array formulas, Data tables, Reporting, Pivot tables, HR Reporting","microsoft office suite, sql, microsoft excel, data visualization, power bi, tableau, oracle business intelligence, oracle fusion, financial analysis, project management, data governance, data modeling, data sets, microsoft access, microsoft word, microsoft powerpoint, macros, array formulas, data tables, reporting, pivot tables, hr reporting","array formulas, data governance, data sets, data tables, datamodeling, financial analysis, hr reporting, macros, microsoft access, microsoft excel, microsoft office suite, microsoft powerpoint, microsoft word, oracle business intelligence, oracle fusion, pivot tables, powerbi, project management, reporting, sql, tableau, visualization"
Principal Data Engineer – Cost of Quality,"GE Appliances, a Haier company","Louisville, KY",https://www.linkedin.com/jobs/view/principal-data-engineer-%E2%80%93-cost-of-quality-at-ge-appliances-a-haier-company-3741631916,2023-12-17,Westport,United States,Mid senior,Onsite,"At GE Appliances, a Haier company, we come together to make “good things, for life.” As the fastest-growing appliance company in the U.S., we’re powered by creators, thinkers and makers who believe that anything is possible and that there’s always a better way. We believe in the power of our people and in giving them the freedom to explore, discover and build good things, together.
The GE Appliances philosophy, backed by three simple commitments defines the way we work, invent, create, do business, and serve our communities:
we come together
,
we always look for a better way
, and
we create possibilities
.
Interested in joining us on our journey?
The Principal Data Engineer - Cost of Quality will have broad leadership responsibility for the strategy and implementation of robust analytical tools and intelligence to improve the efficiency of our Cost of Quality (COQ) resources. This role will work closely with cross-functional teams to equip them with tools to investigate data that can be used to develop and implement corrective actions and measure the effectiveness of those actions. The ideal candidate will have significant experience in data analysis, statistical methods, and quality analytics.
Position
Principal Data Engineer – Cost of Quality
Location
USA, Louisville, KY
How You'll Create Possibilities
Develop strategy to utilize AI (Artificial Intelligence) to enhance data analysis through insight generation. Drive the adoption of new AI tools by advocating the advantages and empowering users through training.
Enable and conduct exploratory, descriptive, and predictive analysis around cost of quality initiatives
Analyze and interpret data and historical trends, and report on and delve into findings as needed
Create data enriched visualizations associated with the measurement, analysis and reporting of COQ
Translate and present quantitative findings into practical data insights through the interpretation of data, trends, and other sources
Ensure Data Quality for COQ data through monitoring of data feeds and auditing of automated reports.
Identify and implement improvements to COQ data quality.
Create ad hoc analysis and reporting in response to internal data requests
Research competitors for benchmarking, as well as industry and cost of quality best practices
Own forecasting and analysis of COQ metrics while identifying and implementing enhancements to analysis.
Stay up to date on the latest data analysis and statistical methods.
What You'll Bring to Our Team
7+ year experience in high volume manufacturing environment working in manufacturing engineering or quality.
Experience with statistical methods and modeling.
Ability to create and manage data workflows using advanced Analytics tools like SAS, R, python or similar
Excellent communication and presentation skills with technical and business audiences
Ability to work independently and as part of a team.
Desired Skills:
Experience with Generative AI.
Experience with Cost of Quality metrics including Warranty costs, failure rates, etc.
Reliability experience
Experience with data visualization tools.
Familiar with databases like Google Big query and Oracle.
Our Culture
At GE Appliances, creativity meets passion and conversations lead to exceptional outcomes and experiences. We respect and value the unique backgrounds and experiences that everyone brings to GE Appliances. We believe a diverse workplace, where everyone is included and people can be their true and authentic selves, fosters creativity and innovation. We know our differences are our greatest strength. The very best innovations across every function of a company come from diverse teams. Our commitment to ensuring a safe and inclusive workplace where everyone is valued allows employees to perform at their best, every day. Diversity at GE Appliances helps us achieve zero distance to our owners, innovate smartly and connect to the communities and customers we serve. We encourage and support the ideas, aspirations, and the wellbeing of everyone - our employees and our communities
GE Appliances is a trust-based organization. It is important we offer our employees the flexibility they need to do their best work while balancing the needs of the business and individuals. When you join GE Appliances, you will have the opportunity to work with your leader to create a flexible work arrangement that balances the needs of the individual, team, and organization.
GE Appliances is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
GE Appliances participates in E-Verify and will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S
If you are an individual with a disability and need assistance or an accommodation to use our website or to apply, please send an e-mail to ask.recruiting@geappliances.com
Show more
Show less","AI, data analysis, dashboards, data visualization, statistical methods, quality analytics, Generative AI, SAS, R, Python, Google Big Query, Oracle, manufacturing engineering, quality management, data mining, data exploration, predictive analytics, reporting, data quality, data interpretation, data presentation, communication, teamwork, SQL, NOSQL","ai, data analysis, dashboards, data visualization, statistical methods, quality analytics, generative ai, sas, r, python, google big query, oracle, manufacturing engineering, quality management, data mining, data exploration, predictive analytics, reporting, data quality, data interpretation, data presentation, communication, teamwork, sql, nosql","ai, communication, dashboard, data exploration, data interpretation, data mining, data presentation, data quality, dataanalytics, generative ai, google big query, manufacturing engineering, nosql, oracle, predictive analytics, python, quality analytics, quality management, r, reporting, sas, sql, statistical methods, teamwork, visualization"
Senior Database Analyst - SG 20G,UPS,"Louisville, KY",https://www.linkedin.com/jobs/view/senior-database-analyst-sg-20g-at-ups-3669934384,2023-12-17,Westport,United States,Mid senior,Onsite,"Before you apply to a job, select your language preference from the options available at the top right of this page.
Explore your next opportunity at a Fortune Global 500 organization. Envision innovative possibilities, experience our rewarding culture, and work with talented teams that help you become better every day. We know what it takes to lead UPS into tomorrow—people with a unique combination of skill + passion. If you have the qualities and drive to lead yourself or teams, there are roles ready to cultivate your skills and take you to the next level.
Job Description:
Will consider candidates from the following I.T. Campus locations: KY, NJ, MD & GA
Job Summary
This position interacts with functional representatives, business analysts, data analysts, systems administration and programming, and Applications Development project teams to apply data operations requirements to support the effective storage and retrieval of data. He/She works independently on one or more highly complex database environments. This position assists with providing the definition of logical and physical data elements for promoting data sharing, reusing data objects, and ensuring a coherent data source strategy. He/She transforms logical data models into physical databases, applying techniques based on applications’ processing needs. The Senior Database Analyst investigates complex problems, monitors performance, contributes to back-up and recovery procedures, creates reports, and configures and installs database software.
Responsibilities:
Drives consistency and maintains standards and procedures for database environments to ensure enterprise compliance.
Implements and maintains the enterprise’s production and development databases.
Monitors and analyzes performance metrics, back-up, and recovery procedures and allocates database resources to achieve optimum database performance.
Minimum Qualifications:
Experienced with Database monitoring
SQL Server Administration experience
Experience Tuning SQL Server Stored Procedures
Windows Operating System
Excellent communication skills
Bachelor's Degree or International equivalent in Computer Science or related discipline - Preferred
Employee Type:
Permanent
UPS is committed to providing a workplace free of discrimination, harassment, and retaliation.
Other Criteria:
Employer will not sponsor visas for position. UPS is an equal opportunity employer. UPS does not discriminate on the basis of race/color/religion/sex/national origin/veteran/disability/age/sexual orientation/gender identity or any other characteristic protected by law.
Basic Qualifications:
Must be a U.S. Citizen or National of the U.S., an alien lawfully admitted for permanent residence, or an alien authorized to work in the U.S. for this employer.
Show more
Show less","SQL Server, Database Administration, SQL, Windows, Communication Skills, Logical Data Models, Physical Databases, Databases, Data Operations, Data Storage, Data Retrieval, Data Sharing, Data Reusing, Data Source Strategy, Data Monitoring, Performance Metrics, Backup and Recovery Procedures, Database Resources, Database Performance, Computer Science","sql server, database administration, sql, windows, communication skills, logical data models, physical databases, databases, data operations, data storage, data retrieval, data sharing, data reusing, data source strategy, data monitoring, performance metrics, backup and recovery procedures, database resources, database performance, computer science","backup and recovery procedures, communication skills, computer science, data monitoring, data operations, data retrieval, data reusing, data sharing, data source strategy, data storage, database administration, database performance, database resources, databases, logical data models, performance metrics, physical databases, sql, sql server, windows"
Senior Data Analyst,Heartland,"Jeffersonville, IN",https://www.linkedin.com/jobs/view/senior-data-analyst-at-heartland-3764335227,2023-12-17,Westport,United States,Mid senior,Onsite,"Every day, Heartland, a Global Payments Company, makes it possible for millions of people to move money between buyers and sellers using our products and unmatched services. Simply, we create meaningful technology centered experiences that enable our customers to prosper. If you want to work like an entrepreneur, support and serve entrepreneurs and bring your expertise to a dynamic team, then Heartland is for you. If it's in your nature to work with a passion to provide tangible solutions for everyone you interact with, then join us and let's see what we can do together.
The Senior Data Analyst will be primarily responsible for supporting the leaders of Heartland’s Operations teams with business process improvement, data analysis, and developing business intelligence (BI) tools. The Senior Data Analyst will work to gather, analyze, and translate data that has been collected regarding Heartland’s initiatives and production into a format to enable better business decisions.
Primary Responsibilities/Objectives:
Coordinate with leadership team to identify critical data to capture and report on
Consolidate and summarize data for business decision making and continuous improvements
Create graphical representations of data/analysis for easy consumption
Perform statistical analysis (correlation, regression, or time series analysis)
Inspect and cleanse data for accuracy and quality
Some data ETL/ELT responsibilities as needed
Other responsibilities as assigned
Core Competencies:
Strong understanding of reporting requirements and structural hierarchies for reporting in a collaborative environment. Query, analyze and transform complex data sets to optimize dashboard flexibility and performance.
Rationalize and identify use cases for developing data visualization, including identifying different personas for each use case.
The Senior Data Analyst will work with large amounts of data: facts, figures, and number crunching. The candidate will need to see through the data and analyze it to find conclusions. One needs to be strong in logic and mathematics.
Communication Skills: Senior Data Analyst will be expected to present their findings or translate the data into an understandable document. This candidate will need to write and speak clearly, easily communicating complex ideas. Needs to possess the ability to work in a team environment as well as individually.
Critical Thinking: Senior Data Analyst must look at the numbers, trends, and data and come to conclusions based on the findings.
Presentation Skills: Ability to organize data and supporting information into visual representations that allow consumers to easily dissect and make decisions based on data provided.
Attention to Detail: Data is precise. Data analysts have to make sure they are vigilant in their analysis to come to correct conclusions. Must make every effort not to only meet but also preempt customer needs.
Strong experience with hands-on building and deploying Tableau, Qlik, PowerBI or Spotfire data visualizations and dashboards
Strong understanding of data visualization concepts. Thoughtfully connects these concept business metrics and KPIs to achieve business goals.
Experience in relational database concepts with a solid knowledge of SQL Server
Google Suite Products: Advanced level with Google Sheets, Google Slides, and Google Docs
Education/Experience:
Position requires knowledge typically associated with a Bachelor’s Degree in Computer Science, MIS, Economics, Business, Mathematics, Statistics, related technical field, or equivalent work experience.
6+ years of applicable experience as a Data Analyst or similar role
Advanced Tableau experience building robust dashboards
Understanding of the Bankcard Industry or Commission Based Sales is a plus.
Preferred Qualities:
Motivated self-starter
Detail oriented
Cautious
Consistent
Tenacious
Ability to manage a project
Heartland Qualities:
Uncompromising honesty
Approachable and willing to offer helpful solutions
Ability to search relentlessly for better solutions
Must handle constructive feedback well
Heartland is an equal opportunity employer. Heartland, a Global Payments Company, provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex (including pregnancy), national origin, ancestry, age, marital status, sexual orientation, gender identity or expression, disability, veteran status, genetic information or any other basis protected by law. Those applicants requiring reasonable accommodation to the application and/or interview process should notify a representative of the Human Resources Department.
Show more
Show less","Tableau, Qlik, PowerBI, Spotfire, SQL Server, Google Suite, Data Visualization, Data Analysis, Data ETL/ELT, Business Intelligence, Relational Databases, Statistics, Mathematics, Data Warehousing, Data Mining, Data Modeling, Machine Learning, Artificial Intelligence, Cloud Computing, Agile Methodology, Project Management, Communication Skills, Critical Thinking, Presentation Skills, Attention to Detail","tableau, qlik, powerbi, spotfire, sql server, google suite, data visualization, data analysis, data etlelt, business intelligence, relational databases, statistics, mathematics, data warehousing, data mining, data modeling, machine learning, artificial intelligence, cloud computing, agile methodology, project management, communication skills, critical thinking, presentation skills, attention to detail","agile methodology, artificial intelligence, attention to detail, business intelligence, cloud computing, communication skills, critical thinking, data etlelt, data mining, dataanalytics, datamodeling, datawarehouse, google suite, machine learning, mathematics, powerbi, presentation skills, project management, qlik, relational databases, spotfire, sql server, statistics, tableau, visualization"
Senior HR Reporting and Data Analyst,American Commercial Barge Line (ACBL),"Jeffersonville, IN",https://www.linkedin.com/jobs/view/senior-hr-reporting-and-data-analyst-at-american-commercial-barge-line-acbl-3719529152,2023-12-17,Westport,United States,Mid senior,Onsite,"Job Title:
Senior HR Reporting and Data Analyst
Company:
American Commercial Barge Line
Location:
Jeffersonville, IN
Job Type:
Full-Time
If you are a data-driven professional with a passion for HR analytics and a desire to make a meaningful impact, we want to hear from you. American Commercial Barge Line is hiring a Senior HR Reporting and Data Analyst. You play a pivotal role in shaping our HR strategy by providing data-driven insights and recommendations. You will collaborate with cross-functional teams to design, develop, and maintain HR reporting and analytics solutions that drive informed decision-making and support our employees' success.
When you join ACBL..
American Commercial Barge Line (ACBL) is one of the largest and most diversified marine transportation companies in the U.S. Our legacy of providing the nation with the most economical, safest, and greenest mode of transportation dates all the way back to 1915, when we began moving coal on the Kentucky River.  We offer a wide range of career paths in both operations and support services. Whether you are interested in working on the river or in an office, we are always searching for the best of the best to join our ACBL team.
What you will be doing... Your IMPACT
Partner with internal stakeholders to maintain information systems, identify concerns and improve upon the quality of data, and ensure data is protected yet accessible.
Collaborate with Team Members in the analysis of data and delivery of people-friendly reporting to facilitate continuous improvement.
Develop and maintain key people-related metrics that measure the effectiveness of programs and initiatives.
Create and maintain complex reports and coach Team Members on understanding the data.
Document data definitions and data sources related to people reporting processes.
Stay up-to-date with industry best practices and emerging trends in HR analytics and reporting.
What we are looking for...
You will need to have:
Bachelor’s Degree in Human Resources, Business Administration, or a related field.
Previous experience in Data Management or Human Resource Information Systems.
Ability to gather and interpret data, as well as improve HRIS processes.
Ability to collaborate, provide technical support, and train staff.
Ability to analyze HRIS and HR performance metrics.
Understanding of database management and security.
Ability to document processes, as well as performing diagnostic tests and audits.
Ability to keep up with technical innovation and trends in HRIS Analysis.
Exceptional interpersonal and communication skills.
Even better if you have one or more of the following:
Experience with UKG is a plus.
Proficiency in data analysis and visualization tools (e.i. Power BI, Tableau).
Reasons you will love working at ACBL …
Positive and engaging team atmosphere
Competitive salary
Comprehensive health, dental, and vision insurance.
Onsite fitness area
401(k) retirement plan with employer match.
Professional development opportunities
Paid time off and holidays
Employee Assistance Program
Onsite cafeteria for breakfast and lunch options.
FLSA Status:
Exempt
Show more
Show less","HR Analytics, Data Analysis, Reporting, Data Visualization, Power BI, Tableau, HRIS, Database Management, Security, UKG, Communication, Data Interpretation","hr analytics, data analysis, reporting, data visualization, power bi, tableau, hris, database management, security, ukg, communication, data interpretation","communication, data interpretation, dataanalytics, database management, hr analytics, hris, powerbi, reporting, security, tableau, ukg, visualization"
Quality Assurance Data Analyst,Kentucky Lottery Corporation,"Louisville, KY",https://www.linkedin.com/jobs/view/quality-assurance-data-analyst-at-kentucky-lottery-corporation-3784571319,2023-12-17,Westport,United States,Mid senior,Onsite,"For over 30 years, the Kentucky Lottery has earned more than $6.8 billion for the Commonwealth of Kentucky. Our mission is fueling imagination and funding education for all Kentuckians. For nearly two decades, the largest chunk of proceeds – over $4.8 billion, has gone to funding programs to help Kentucky students stay home and attend college. We have continued to break records – earning over $1 billion in sales every fiscal year since ‘17 to continue to support these important programs.
As a company that was voted
Best Places to Work for 3 consecutive years (2021-2023)
, our team and company values are what makes the difference to college students all across Kentucky.
The
Kentucky Lottery’s Core Values
:
Integrity – We do the right thing.
Accountability – We are accountable to the Governor, the General Assembly, the people of the Commonwealth and to each other.
Social Responsibility – We are a good corporate citizen, giving back to the community.
Diversity, Equity & Inclusion – We value and respect our colleagues, our customers, and the communities we serve.
Teamwork – We believe in collaboration and the strength of people coming together to achieve something great.
Innovation – We embrace innovation, working to proactively see opportunities.
Fun – We have fun at work and enjoy a positive work environment.
We want to make winners out of our players, retailers, college students and our employees. We are looking for the best talent to join our winning team.
JOB SUMMARY
The Quality Assurance/Data Analyst – Operations is responsible for monitoring, analyzing and assessing the quality of field service and call center support received by the KLC lottery retail community. Analyzes and interprets complex data to provide actionable information to help improve services provided to retailers. Assists in developing comprehensive, proactive strategies and tactics for improving the level of service and increasing equipment availability. Assists in generating standard process measurement reporting to monitor and track performance measures.
ESSENTIAL DUTIES/RESPONSIBILITIES
Assists in monitoring, analyzing and assessing field service and call center contractual requirements under the supervision of the Vice President, Operations.
Analyzes daily field service data to identify discrepancies, anomalies, and non-compliance issues.
Investigates issues through the analysis of system data and call center recordings.
Identifies specific events for potential audits on service deliveries and customer interactions.
Communicates findings and collaborates with internal departments.
Assists in the development of retailer support performance improvement plan.
Provides actionable information that will assist the KLC field service contractor and the KLC in 1) reducing the number of retailer issues and questions, 2) reducing the number of dispatches required, and 3) reducing the intervals between the creation and resolution of issues.
Provides reports with high-value metrics that provide a clear and accurate delineation of the level of retailer support and equipment availability.
KNOWLEDGE/SKILLS/EXPERIENCE
Bachelor’s degree in Business, Analytics, Finance or related field required. Certification or coursework in Data Analysis, Auditing, or Contract Management preferred.
Three years of relevant experience preferred.
Ability to analyze data and documents.
Aptitude for thorough scrutiny and detailed analysis.
Excellent problem-solving skills and keen attention to detail.
Must possess excellent organizational skills.
Collaboration skills to work well in a team environment.
Effective verbal and written communication skills.
Proficient in data analytic tools such as Microsoft Excel and Python.
Proficient in Microsoft Office.
CORE VALUES
Must exhibit the KLC’s core values:
Integrity - We do the right thing.
Accountability - We are accountable to the Governor, the General Assembly and the people of the Commonwealth for the proper stewardship of the resources entrusted to us.
Social Responsibility - We are a good corporate citizen, giving back to our community not just through our proceeds but through our time and talents as well.
Diversity, Equity & Inclusion - We embrace differences and respect everybody, fostering a corporate culture where everyone is valued.
Teamwork - We believe in collaboration and the strength of people coming together to achieve something great.
Innovation - We embrace innovation, working to proactively seek opportunities.
Fun - We have fun at work. We create fun for players across the Commonwealth.
SUPERVISORY RESPONSIBILITIES
This position has no supervisor responsibilities.
WORKING CONDITIONS
Office environment; hybrid work schedule in accordance with KLC’s policies and procedures.
Travel: Minimal
PHYSICAL REQUIREMENTS
· Ability to lift up to 20 lbs.: Occasionally
· Standing or sitting: Frequently
· Moving: Frequently
· Reaches, writing, fingering, typing: Frequently
· Talking and/hearing: Continuous
· Seeing: must be able to read reports and use a computer: Continuous
· Sit for long periods of time: Frequently
BENEFITS
We offer a comprehensive employee benefits package including medical, dental, vision, wellness program, health savings account, flexible spending accounts, company paid life insurance, supplemental life insurance, company paid short-term and long-term disability, voluntary benefit plans, generous paid time off plans, retirement plans – including 401(k), 457(b), IRAs, and a money purchase retirement plan in which the company contributes a portion of your base pay.
We will only contact candidates who match the qualifications for this role and who are selected for the next steps in the talent acquisition process.
The Kentucky Lottery Corporation is an equal employment opportunity employer. Our mission is to build a diverse, equitable and inclusive environment where everyone is valued. We’re all winners when we embrace our differences.
The Kentucky Lottery Corporation is committed to the full inclusion of all qualified individuals through all aspects of employment, and will provide reasonable accommodations, upon request, to assist with the job application or interview process, or performing the essential functions of a job. If a reasonable accommodation is needed, please contact Human Resources.
Show more
Show less","Data Analysis, Auditing, Contract Management, Microsoft Excel, Python, Microsoft Office, Business, Analytics, Finance, Data Analytic tools","data analysis, auditing, contract management, microsoft excel, python, microsoft office, business, analytics, finance, data analytic tools","analytics, auditing, business, contract management, data analytic tools, dataanalytics, finance, microsoft excel, microsoft office, python"
Senior Data Engineer,PPL Corporation,"Louisville, KY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-ppl-corporation-3778931368,2023-12-17,Westport,United States,Mid senior,Hybrid,"Company Summary Statement
As one of the largest investor-owned utility companies in the United States, PPL Corporation (NYSE: PPL), is committed to creating long-term, sustainable value for our 3.5 million customers, our shareowners and the communities we serve. Our high-performing regulated utilities — PPL Electric Utilities, Louisville Gas and Electric, Kentucky Utilities and Rhode Island Energy — provide an outstanding experience for our customers, consistently ranking among the best utilities in the nation. PPL’s companies are also addressing challenges head-on by investing in new infrastructure and technology that is creating a smarter, more reliable and resilient energy grid. We are committed to doing our part to advance a cleaner energy future and drive innovation that enables us to achieve net-zero carbon emissions by 2050 while maintaining energy reliability and affordability for the customers and communities we serve. PPL is a positive force in the cities and towns where we do business, providing support for programs and organizations that empower the success of future generations by helping to build and maintain strong, diverse communities today.
Overview
Data Analysis:
The Data Engineer designs and implements processes and layouts for complex, large-scale data sets used for modeling, data mining, and research purposes. This person is expected to maintain a data knowledgebase (e.g., data dictionary).
Business Insights:
The Data Engineer empowers business partners to turn information into action by building tools, developing protocols, and defining metrics for forward-looking data collection and analysis. The Senior level performs assigned tasks under minimal guidance from supervisor.
NOTE: This position is Hybrid to any of our three office locations: Allentown, PA ; Providence, RI ; Louisville, KY
Responsibilities
Performs a variety of complex assignments (may lead some projects) and analyzes and solves complex problems in the below areas.
Data Design
Assist data architect in translating business requirements into design specifications
Create source mappings, business definitions, measures, dimensions, visualization designs, test plans, and test cases
Conduct data studies and data discovery around new data sources or new uses for existing data sources
Interpret and analyze data and generate reports as needed to understand flow of data
Apply quantitative methods to derive actionable insights and outcomes from data
Data Management:
Conduct data studies and data discovery around new data sources or new uses for existing data sources
Assist in documenting meta data information including data types, lengths, and conversion functions
Assist in analyzing and researching new and existing products, procedures, and/or workflows for data management
Execute activities pertaining to end-to-end data set identification, development, and implementation according to the guidance from Data Scientists
Support data collection, integration and retention requirements based on the input collected with the business
Gather and process data from the Transmission, Distribution, Customer Service, and EU Service functions, and transform them into data products
Implement statistical data quality procedures around new data sources
Data Integration:
Develop and modify functions, programs, routines, and stored procedures to export, transform, and load data
Resolve integration and data mapping issues and discrepancies as objectives change
Assist in resolving data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design strategies collaborating with subject matter experts (SMEs)
Master Data Management:
Execute on all aspects of MDM architecture (MDM data model, SAP MDM, MDM tools, MDM data integration, master data quality, MDM operations, master data security, )
Maintain data warehouse performance by identifying and resolving data conflicts
Adhere to data / information quality practices
Conduct data cleaning to rid the system of old, unused data, or duplicate data for better management and quicker access
Ensures quality of master data in key systems, as well as development and documentation of processes with other functional data owners to support ongoing maintenance and data integrity
Data Governance:
Adhere to data strategy, policies, controls, and programs to ensure the enterprise data is accurate, complete, secure, and reliable
May be assigned an Electric Utilities emergency and storm role. This is a special assignment that comes into play during storms and other emergencies when the company needs to restore power or respond to other issues affecting customer service. This role may necessitate the need to work after-hours, outside of your normal schedule.
The company reserves the right to determine if this position will be assigned to work on-site, remotely, or a combination of both. Assigned work location may change. In the case of remote work, physical presence in the office/on-site may be required to engage in face-to-face interaction and coordination of work among direct reports and co-workers.
Qualifications
Bachelor's degree and 5 years of related work experience OR 8 years of related work experience
Awareness of industry leading data quality and data protection management practices
Awareness of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Awareness of data related government regulatory requirements and emerging trends and issues
Proven ability to prioritize and execute tasks
Highly self-motivated and directed with attention to detail
Easily adapts to changing circumstances
Understands business goals and strategic priorities
Preferred Qualifications
:
Experience supporting fast-changing business organizations
Show more
Show less","Data Analysis, Business Insights, Data Design, Data Management, Data Integration, Master Data Management, Data Governance, Data Discovery, Data Quality, Data Architecture, Data Mining, Data Visualization, Data Collection, Data Processing, Data Transformation, Data Warehousing, Data Security, Data Protection, SAP MDM, MDM Tools, Data Dictionary, Data Knowledgebase, Data Policies, Data Controls, Data Programs, Data Regulations, Data Trends, Task Prioritization, Execution, Attention to Detail, Adaptability, Strategic Thinking","data analysis, business insights, data design, data management, data integration, master data management, data governance, data discovery, data quality, data architecture, data mining, data visualization, data collection, data processing, data transformation, data warehousing, data security, data protection, sap mdm, mdm tools, data dictionary, data knowledgebase, data policies, data controls, data programs, data regulations, data trends, task prioritization, execution, attention to detail, adaptability, strategic thinking","adaptability, attention to detail, business insights, data architecture, data collection, data controls, data design, data dictionary, data discovery, data governance, data integration, data knowledgebase, data management, data mining, data policies, data processing, data programs, data protection, data quality, data regulations, data security, data transformation, data trends, dataanalytics, datawarehouse, execution, master data management, mdm tools, sap mdm, strategic thinking, task prioritization, visualization"
"BI/Data Engineer Los Angeles, CA",Executive Staff Recruiters / ESR Healthcare,"Los Angeles, CA",https://www.linkedin.com/jobs/view/bi-data-engineer-los-angeles-ca-at-executive-staff-recruiters-esr-healthcare-3644022453,2023-12-17,El Monte,United States,Mid senior,Hybrid,"Company Profile
esrhealthcare.com.mysmartjobboard.com
BI/Data Engineer
>
> Location: Los Angeles, CA onsite at least for 2-3 days.
>
> Position: 2 Nos
>
> Duration: 6 – 12 Months+
>
> Only local candidates to CA– 2/3 days onsite is a must.
>
> JD
>
>
Minimum 5-7 years of experience
>
>
Data warehouse basics, Experience in requirements gathering
> and writing functional & Design Specs
>
>
ETL development using functional specs/design specs and
> inputs from business user feedback
>
>
Experience using ETL tools (Wherescape, Informatica, Talend
> etc ) and MS SQL Stored Procedures
>
>
Data validation experience (ERP source to DW target facts
> and dimensions)
>
>
BI tool Experience (Tableau)
>
>
Building and automating data flows from external sources
> (Cloud data sources such as HR datasets, TMS, etc)
>
>
Working with a distributed team environment.
>
> Regards...!!!!
>
> Aravind
Powered by Webbtree
Show more
Show less","Data warehouse, ETL development, ETL tools, MS SQL Stored Procedures, Data validation, BI tool (Tableau), Data flows, Cloud data sources","data warehouse, etl development, etl tools, ms sql stored procedures, data validation, bi tool tableau, data flows, cloud data sources","bi tool tableau, cloud data sources, data flows, data validation, datawarehouse, etl development, etl tools, ms sql stored procedures"
Database Engineer,JBA International,"Los Angeles, CA",https://www.linkedin.com/jobs/view/database-engineer-at-jba-international-3578845415,2023-12-17,El Monte,United States,Mid senior,Hybrid,"Job Summary:
The Database Engineer, Senior will work closely with the engineering team members to design, develop and enhance Microsoft SQL Server and PostgreSQL database solutions.
Duties & Responsibilities
Strong SQL, experience writing, tuning queries based on application requirements and performing debugging on database scripts and programs, as well as resolving conflicts.
Highly skilled problem-solver and communicator that is fluent in most data manipulation languages.
Design and code a high volume of SQL Queries, stored procedures, and SSIS packages
Actively participate as part of matrixed product development teams providing advice on design and tuning of database objects, queries, and overall data architecture.
Use of tools and utilities to monitor, load and unload data, generate, and edit test data
Effectively plan and organize daily work following priorities, ensuring timely completion of projects and user support
Work with the different Technology and business team members to ensure that the associated compute resources are allocated to the databases and to ensure high availability and optimum performance.
Provide trend analysis to the service management team to enable them to make informed decisions regarding resource management.
Develops a continually growing knowledge of Company's internal business practices, processes and the daily IT operational needs of the users
Provide query and performance expertise in support of code development
Monitor long running transactions and optimize query executions with index tuning and optimized T-SQL coding technique
Problem escalation to development team and third parties as appropriate.
Implementation and release of database changes as submitted by the development team
Documenting technical environments and processes as necessary
Participate in the on-call rotation
Competencies
Perform all work and activities with honesty and integrity.
Take personal responsibility for productivity, quality and timeliness of work.
Effectively communicate (and listen) clearly, professionally, politely and persuasively in all situations; respond well and in a reasonable, timely manner.
Challenge conventional practices and use creativity and information to lead, innovate, problem solve, and implement ideas to contribute to the growth of the organization.
Support and meet company/department goals and core values.
Collaborate with co-workers to achieve common goals.
Problem Solving/Analysis.
Qualifications & Requirements
In-depth SQL knowledge is required, including advanced tuning skills.
Expertise in Microsoft SQL Server or PostgreSQL.
Experience with clustering and log shipping.
Strong communication skills and the ability to share ideas and work well in small teams.
Possesses excellent problem-solving capabilities.
Independent and self-motivated.
Bachelor's degree in computer science, engineering, business, or the equivalent is preferred and 5 plus years of database development experience is preferred.
Experience with Microsoft SQL Server 2008 and SSIS is required.
Knowledge of other programming languages is a plus.
Show more
Show less","SQL, PostgreSQL, Data manipulation languages, SSIS, TSQL, Microsoft SQL Server 2008","sql, postgresql, data manipulation languages, ssis, tsql, microsoft sql server 2008","data manipulation languages, microsoft sql server 2008, postgresql, sql, ssis, tsql"
Business Data Engineer,Latham & Watkins,"Los Angeles, CA",https://www.linkedin.com/jobs/view/business-data-engineer-at-latham-watkins-3729110776,2023-12-17,El Monte,United States,Mid senior,Hybrid,"About Latham & Watkins
Latham & Watkins is a global law firm consistently ranked among the top firms in the world. The success of our firm is largely determined by our commitment to hire and develop the very best and brightest, creating a team that provides our clients with the highest quality of work and service. We are driven by our core values: respect, innovation, and collaboration.
About The Role
The Business Data Engineer is an integral part of Latham’s Technology team. This role will be responsible for developing, researching, and implementing data analysis projects including but not limited to data reporting, data visualizations, data design, and ad-hoc research projects , while w orking with firm management to further Enterprise Analytics and Business Intelligence visualization and reporting capabilities . This role will be located in our Global Services Office, located in downtown Los Angeles. Please note that this role may be eligible for a flexible working schedule that allows for a hybrid and in-office presence.
Responsibilities & Qualifications
Other key responsibilities include:
Working with senior management on research projects to design, develop, and deploy advanced data modeling systems
Analyzing and verifying internal consistency, completeness, and accuracy of data systems and making required adjustments
Acting as a resource to the firm on various data related questions for staff and other users; responding to user questions and issues in a timely manner and assisting with solutions
Working with vendors in order to resolve complex issues
Staying abreast of current and new development processes, tools, technologies, and market trends as they relate to systems development and implementation in support of the legal industry
We’d love to hear from you if you:
Possess strong development experience with Business Intelligence Tools (such as Tableau, SAP Business Objects, Microsoft Power BI, SAS)
Display strong skills in dashboard and report development, including but not limited to the ability to create meaningful dashboards to customers' satisfaction
Demonstrate strong SQL skills including but not limited to data profiling skills to identify and understand anomalies
And have:
A Bachelor’s degree, preferably in Computer Science, Data Science, Math, Economics, or a related field
A minimum of seven (7) years of experience with professional BI development
Benefits & Additional Information
Successful candidates will not only be provided with an outstanding career opportunity and welcoming environment, but will also be provided with a generous total compensation package with bonuses awarded in recognition of both individual and firm performance. Eligible employees can participate in Latham’s comprehensive benefit program which includes:
Healthcare, life and disability insurance
A generous 401k plan
At least 11 paid holidays per year, and a PTO program that accrues 23 days during the first year of employment and grows with tenure
Well-being programs (e.g. mental health services, mindfulness and resiliency, medical resources, well-being events, and more)
Professional Development programs
Employee discounts
And more!
Additionally, we have a range of diversity programming including Global Affinity Groups. These groups provide a firmwide platform to share experiences and advice as well as an opportunity to participate in a supportive network with common interests to help make life at the firm even better.
Latham & Watkins is committed to diversity, equal opportunity, sustainability, and pro bono legal services. We draw from a remarkable wealth of talent to create one of the world's leading law firms, and advance these commitments through the work of our Global Citizenship department. Our lawyers, paralegals, and professional staff worldwide comprise a rich mixture of different races, ethnic backgrounds, religions, sexual orientations, cultures, and primary languages. Our diversity makes us who we are.
Latham & Watkins LLP will consider qualified applicants with criminal histories in a manner consistent with the City of Los Angeles Fair Chance Initiative for Hiring Ordinance (FCIHO) . Please click the link below to review the Ordinance.
Please click here to review your rights under U.S. employment laws.
Pay Range
USD $110,000.00 - USD $130,000.00 /Yr.
Show more
Show less","Business Intelligence Tools, Tableau, SAP Business Objects, Microsoft Power BI, SAS, SQL, Data Visualizations, Data Design, Data Reporting, Data Analysis, Data Modeling","business intelligence tools, tableau, sap business objects, microsoft power bi, sas, sql, data visualizations, data design, data reporting, data analysis, data modeling","business intelligence tools, data design, data reporting, data visualizations, dataanalytics, datamodeling, microsoft power bi, sap business objects, sas, sql, tableau"
Data Analyst,Ashdown People,"Parramatta, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-analyst-at-ashdown-people-3780870384,2023-12-17,Camden, Australia,Mid senior,Onsite,"Data Analyst | 6 Month Contract | Up to $943.50 p/d inc. Super
$750 - $850 p/d + super
6 Month Contract
Parramatta
About the Company
This NSW Government agency is a major service provider who strives to deliver a more consistent and efficient experience within government. This government agency uses data and behavioural insights to drive improvements with government and providing services to NSW. We are looking for someone to start ASAP for a 6 month contract paying $750 - $850 p/d + Super. This role is located in Parramatta.
About The Team
This newly established team is composed of individuals who share a common mindset, utilising data analytics to make a positive impact on the residents of NSW. You need to be hands-on and happy to dive deep into data. This role requires you to get into the details of biodiversity data analysis!
About The Role
The primary goal of this role is to facilitate the scaling, transformation, and implementation of data-driven solutions within the framework of a large government scheme. The successful candidate will contribute to data-centric process changes, ensure the scheme's biodiversity objectives are met, and collaborate with various stakeholders to analyse and interpret biodiversity data effectively.
Key Responsibilities Include
Utilise data analytics to manage relevant stakeholders, gaining ongoing support for the scheme, and soliciting input to improve projects.
Develop and execute data-driven plans to ensure the scheme scales effectively, establishing data governance structures, and priorities.
Collaborate with a versatile, IT-dominated team to address data-centric process changes and implement necessary adjustments.
Conduct data-driven security/cyber reviews and implement measures to enhance data security.
Advise on the right way to apply data standards and methods, ensuring compliance with security and data analysis standards.
Manage varied business stakeholders across a multidisciplinary organisation.
Establish, monitor, and manage strategic relationships with key data analytics service providers, providing the necessary expertise and capacity required to meet the objectives of the programme and business requirements.
Ensure data analysis delivery time, quality, and risk management objectives are met at a programme level, at all phases of the project lifecycles.
Oversee the preparation and implementation of communications and change management plans and strategies relevant to the programme of works.
About You
Tertiary qualifications in Data Analytics, ICT, or a related area.
Previous experience in a data analyst role, preferably within a government context.
Familiarity with biodiversity data, trading platforms, or related systems is a plus.
Understanding of business processes and their alignment with data analytics initiatives.
Demonstrated capability in large-scale data analysis projects with extensive end-to-end experience in data planning and implementation.
Benefits of contracting through Ashdown People
5th largest supplier to the NSW Government
Get paid weekly
Rated 4.9/5 on Google Reviews with over 1100 5⭐ reviews for candidate feedback this year
Offer Insurances for ABN Contractors
Consistent communication & transparency
Detailed onboarding & support
Please apply if you are interested. Please note, only shortlisted candidates will be contacted. If you have any questions, please contact ryan.wilson@ashdownpeople.com.au or call 0482 096 055.
Show more
Show less","Data Analytics, ICT, Biodiversity data, Trading platforms, Data governance, Data security, Data standards, Business stakeholders, Data analysis service providers, Programme and business requirements, Communications and change management, Data planning, Data implementation","data analytics, ict, biodiversity data, trading platforms, data governance, data security, data standards, business stakeholders, data analysis service providers, programme and business requirements, communications and change management, data planning, data implementation","biodiversity data, business stakeholders, communications and change management, data analysis service providers, data governance, data implementation, data planning, data security, data standards, dataanalytics, ict, programme and business requirements, trading platforms"
Property Data Analyst,Bromford,"West Midlands, England, United Kingdom",https://uk.linkedin.com/jobs/view/property-data-analyst-at-bromford-3771943221,2023-12-17,Swindon, United Kingdom,Associate,Onsite,"We are Bromford, where property data transforms into community connections. If you are motivated leveraging technology and intelligence to elevate service and enrich people’s lives and, you would you like to join a leading Housing Association who has been certified as a
Great Place to Work
, read on!
We seek a passionate Property Data Analyst to provide safe, secure housing for our customers. This is a perfect role for detail-driven professionals who enjoy optimising systems and processes. You’ll be working within our Homes, Investment and Compliance team who are responsible for the management and ongoing maintenance of our social and affordable housing. This role will help keep our business on track, making the difference in our ability to deliver our essential services and support to customers, technical knowledge of housing and data systems and ensure the most effective use of data to drive and monitor performance.
The role will also support the effective delivery of the stock condition survey and data collation programme. This is a data-driven opportunity where you will be responsible for assisting in the implementation of systems and effectively upkeeping our records and performance. Your work will support our future investment decisions and provide property asset information to customers and colleagues.
This role is available as a 12-month fixed term contract and subject to a basic DBS check.
To succeed in this position, you will need:
Extensive experience of performance monitoring and reporting
Demonstrable knowledge of data management systems
Experience of delivering an asset-based business support service
Experience of working with and implementing integrated application and data systems
Flexible working attitude
Customer focused with the ability to communicate effectively
We offer location flexibility with offices in Wolverhampton (WV10) and Tewkesbury (GL20) plus remote work options. If you have extensive monitoring/reporting experience, housing data skills, and a flexible approach, here is a chance to directly impact 100,000 plus lives for the better.
Apply before 7 December 2023 for a chance to join our rapidly growing team of property experts.
About Us
We are a housing association- one that owns and provides 46,000 homes for people who can't access market housing; has individual relationships with more than 110,000 customers; has a strong balance sheet and plans to build 11,000 homes by 2030. all of this is only possible because of our 1,800 dedicated colleagues.
We take a simple view that nothing is more important to any individual or a family than their home. It's a matter of social justice that everyone should have a home that is safe, secure, and affordable. We exist to provide such homes. With the right home, people can achieve great things, not only for themselves but for wider society too.
We provide quality, affordable homes. But we care about the people who live in them too. We want each of them to be able to achieve their goals. This will be different for each customer. Put simply, we want people to thrive.
Diversity Statement
We are committed to providing a culture where our customers, our colleagues and our partners feel valued for being unique. We empower individuals to reach their full potential within an open, fair and supportive environment. In return we expect everyone to be respectful, collaborate effectively and embrace diversity. By listening, learning and acting we aim to constantly evolve our ambitions for equality, diversity and inclusion to ensure we can always achieve our purpose.
Great Place To Work Certified
Bromford have been certified as a Great Place To Work (Nov 22 – Nov 23). The 2022 Great Place to Work Trust Index Survey highlighted that our employees believe we have an amazing company culture and that 89% of our employees where made to feel welcome when they joined us. We are also proud to have been named on ‘UK’s Best Workplaces for Wellbeing’ list, proving that colleague health and wellbeing remains a top priority and that our people are truly at the heart of everything we do.
Benefits
Flexible benefits
Learning and development
Private medical cover
Work-life balance
Pay and financial wellbeing
Health and wellbeing
Documents
Show more
Show less","Data Management Systems, Performance Monitoring, Reporting, AssetBased Business Support, Integrated Application and Data Systems, Data Analysis, Housing Data Systems","data management systems, performance monitoring, reporting, assetbased business support, integrated application and data systems, data analysis, housing data systems","assetbased business support, data management systems, dataanalytics, housing data systems, integrated application and data systems, performance monitoring, reporting"
Data Cabling Engineer,Digital Waffle,"Wolverhampton, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-at-digital-waffle-3736915016,2023-12-17,Swindon, United Kingdom,Mid senior,Onsite,"Position: Data Cabling Engineer (Temp/Contract/Freelance)
Department: Information Technology / Network Infrastructure
Location: Wolverhampton, West Midlands, UK
Job Summary: We are seeking a skilled and dedicated Data Cabling Engineer to join our Information Technology team. The successful candidate will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Jake on jake@digitalwaffle.co.uk
Show more
Show less","Data Cabling, Cat6, Cat6a, Copper Cables, TIA/EIA, ISO/IEC, Network Topologies, Protocols, Schematics, RJ45, Patch Panels, Keystone Jacks, Outlets, Cable Tester, Cable Certifier, Tone Generator, Probe, Cable Stripper, Cable Cutter, PunchDown Tool, Tape Measure, Level, Cable Ties, Velcro Straps, Cable Clips, Mounts, Power Drill, Bits, Screwdrivers, Wall Anchors, Safety Glasses, Work Gloves, Tool Bag, Pen, Notepad, Mobile Device, SteelToed Boots, Hard Hat, Cable Fish Tape, Cable Lubricant, Cable Toner","data cabling, cat6, cat6a, copper cables, tiaeia, isoiec, network topologies, protocols, schematics, rj45, patch panels, keystone jacks, outlets, cable tester, cable certifier, tone generator, probe, cable stripper, cable cutter, punchdown tool, tape measure, level, cable ties, velcro straps, cable clips, mounts, power drill, bits, screwdrivers, wall anchors, safety glasses, work gloves, tool bag, pen, notepad, mobile device, steeltoed boots, hard hat, cable fish tape, cable lubricant, cable toner","bits, cable certifier, cable clips, cable cutter, cable fish tape, cable lubricant, cable stripper, cable tester, cable ties, cable toner, cat6, cat6a, copper cables, data cabling, hard hat, isoiec, keystone jacks, level, mobile device, mounts, network topologies, notepad, outlets, patch panels, pen, power drill, probe, protocols, punchdown tool, rj45, safety glasses, schematics, screwdrivers, steeltoed boots, tape measure, tiaeia, tone generator, tool bag, velcro straps, wall anchors, work gloves"
Senior BI Developer / Data Engineer,Energy Jobline,"Wolverhampton, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-bi-developer-data-engineer-at-energy-jobline-3773338811,2023-12-17,Swindon, United Kingdom,Mid senior,Onsite,"A leading financial services corporation is currently recruiting a Senior BI Developer / Data Engineer with strong capabilities in SQL Server Backend 2016+, SSIS, SSRS and Azure to work alongside a number of data professionals within data driven organisation. The selected Senior BI Developer / Data Engineer must be able to demonstrate recent project experience within on-premise and cloud environments to be eligible for this exciting opportunity. Our client is looking to pay up to £67,000 + 15% bonus with an excellent benefits package to be based in Chatham, Wolverhampton or London on occasions.
This is a truly exciting position where you will be part of a team responsible for shaping our client data ecosystem transition into the latest cloud technologies and trends.
Core Responsibilities
Leading solutions for BI / Data engineering
Maintain the integrity of both the design and the data that is held within the architecture
Champion and educate people in the development and use of data engineering best practices
Support the Head of Data Engineering and lead by example
Contribute to the development of database management services and associated processes relating to the delivery of data solutions
Provide requirements analysis, documentation, development, delivery and maintenance of data platforms.
Develop database requirements in a structured and logical manner ensuring delivery is aligned with business prioritisation and best practice
Design and deliver performance enhancements, application migration processes and version upgrades across a pipeline of BI environments.
Provide support for the scoping and delivery of BI capability to internal users.
Experience Requirements
5 years Data Engineering / ETL development experience (must have)
Experience working within a regulated environment (must have)
5 years data design experience in an MI / BI / Analytics environment (Kimball, lake house, data lake) (must have)
Excellent Data Warehouse with substantial experience in extracting, reporting and manipulating data from a data warehouse environment (must have)
Significant technical skills such as Transact SQL language, relational database (must have).
Evidence of delivering complex data platforms and solutions (must have)
Experience with cloud data platforms (Microsoft Azure) (nice to have)
Microsoft SQL Server 2019 certification (nice to have)
£67,000/ 15% bonus / Flexible working / 28 Days Holiday / Medical Cover / Life Cover / 13% Pension / Flexible Benefits
Senior BI Developer / Data Engineer
Show more
Show less","SQL Server Backend 2016+, SSIS, SSRS, Azure, Cloud computing, Database management, Data engineering, ETL development, Data design, Data warehouse, Data manipulation, Transact SQL, Relational database, Kimball, Lake house, Data lake, Microsoft Azure, Microsoft SQL Server 2019","sql server backend 2016, ssis, ssrs, azure, cloud computing, database management, data engineering, etl development, data design, data warehouse, data manipulation, transact sql, relational database, kimball, lake house, data lake, microsoft azure, microsoft sql server 2019","azure, cloud computing, data design, data engineering, data lake, data manipulation, database management, datawarehouse, etl development, kimball, lake house, microsoft azure, microsoft sql server 2019, relational database, sql server backend 2016, ssis, ssrs, transact sql"
"Senior Cloud Data Engineer - GBP70,000",Nigel Frank International,"Telford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-cloud-data-engineer-gbp70-000-at-nigel-frank-international-3763121062,2023-12-17,Swindon, United Kingdom,Mid senior,Onsite,"Senior Cloud Data Engineer - £70,000
I am working with a market leading manufacturer of specialist medical equipment who are looking for a Senior Cloud Data Engineer at the start of a number of exciting data driven projects. You would be joining at the start of a greenfield project where you will be responsible for the data migration, development and maintenance of the brand new Azure Data Platform!
The overall goal of the business and ultimately this role is to deliver data to healthcare professionals to aid with rehabilitation of patients. You will join a close-knit team, with talented professionals who specialise in a number of areas including software development and data science. You will be the in house expert for the team on all things data engineering and will take the lead on the creation, development and maintenance of the Azure data platform.
As part of this role, you will be responsible for some of the following areas.
Design, maintain and optimize both on premise and cloud based database solutions.
Develop and maintain robust ETL pipelines.
Work with other specialists within the team to make technical decisions that will benefit the overall business.
Take the lead of migration of data to the new Azure data platform.
This is a salaried role paying up to £70,000 per annum depending on experience and a company benefits package. This role would be largely a remote opportunity with occasional visits to the office in Stafford. These visits would be as and when needed in line with project requirements.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Strong coding experience with languages such as Python or C#.
Experience working as a DBA or completing database administration tasks
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Cloud Computing, Azure, Azure Data Factory, Synapse, Azure Data Lake, ETL, Python, C#, Database Administration","cloud computing, azure, azure data factory, synapse, azure data lake, etl, python, c, database administration","azure, azure data factory, azure data lake, c, cloud computing, database administration, etl, python, synapse"
Data Engineer,Practicus,"Hindlip, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-practicus-3769246069,2023-12-17,Swindon, United Kingdom,Mid senior,Hybrid,"Data Engineer - Initial 6 Month Contract - Hybrid
We are currently seeking an experienced Data Engineer to join a Government Institution based near Worcester. This role offers an exciting potential to deliver critical oversight on the organisations data environment, guiding them through planned migrations on an end-to-end basis.
They are looking for someone willing to roll their sleeves up for an initial 6-months to get stuck into the team, who is happy working in environments where they take a lot of autonomy over their own work. There is a great opportunity to put your own stamp on this role, identifying opportunities of improvement and potential to stay in the role for a longer duration.
Responsibilities
Liaising with analysts, SMEs and other stakeholders to understand data requirements
Supporting a migration to 365
Working autonomously and supporting the developers
Upskilling and developing junior members of the team
Experience
Critical and confidential data set experience beneficial
Stakeholder management and communication experience essential
High quality of verbal and written ability
Keen attention to detail
Tech stack experience with tools such as Power BI, Azure SQL, 365, ETL tools etc
Logistics
Initial 6 month contract with high potential to extend
Outside IR35
1-2 days a week in the office hybrid working
If you, or anyone you know, would like to discuss this opportunity, then please apply or contact with a CV to phil.hellmuth@practicus.com
Show more
Show less","Data Engineering, Data Migration, Power BI, Azure SQL, ETL tools, Stakeholder Management, Communication, Verbal Ability, Written Ability, Attention to Detail, Hybrid Working","data engineering, data migration, power bi, azure sql, etl tools, stakeholder management, communication, verbal ability, written ability, attention to detail, hybrid working","attention to detail, azure sql, communication, data engineering, data migration, etl tools, hybrid working, powerbi, stakeholder management, verbal ability, written ability"
Data Analyst,Peaple Talent,"Wolverhampton, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-peaple-talent-3781860419,2023-12-17,Swindon, United Kingdom,Mid senior,Hybrid,"🏘️ Data Analyst | £40,000 - £45,000 | Wolverhampton 👨‍👩‍👧‍👦
Peaple Talent have partnered exclusively with one of the UK's largest charity housing associations. They're not just about housing; but about using data to create exceptional living experiences for those who are unable to access the housing market. You'll be helping thousands of families and individuals to access safe & securing housing across the UK.
Are you a BI & reporting professional with a proven track record of boosting an organisation's reporting strategy? We're on the lookout for someone who can bring their expertise to support our self-service reporting environment, gather and craft report requirements, delve into BI development, and hold a strong grasp of database and reporting tech.
Your mission?
Craft and enhance Business Intelligence Reports
Encourage the adoption of BI tools and concepts across the organization.
Serve as the main point of contact for various business areas and earn their trust in translating their needs into requirements.
Develop an in-depth understanding of different areas of the business, becoming their go-to for reporting and requirements.
To create or recommend reporting solutions to support business areas
Practical/Technical Knowledge we are looking for:
Data Visualisation experience in Power Bi
Analytical and problem-solving skills
Strong SQL or Excel skills with the ability to learn other analytic tools.
Exposure to building and maintaining ETL pipelines
Knowledge of DAX highly desired
Desirable but not essential:
Awareness of Azure Data Lake, Data Factory, Data Bricks or other Azure tooling.
Package and working arrangements:
💰£40,000-£45,000
📍Remote working with once/twice a month in office visits for team meetings
If you think you'd be a good fit for this role, don't hesitate to apply now!
Show more
Show less","Power BI, SQL, Excel, DAX, ETL pipelines, Azure Data Lake, Data Factory, Data Bricks","power bi, sql, excel, dax, etl pipelines, azure data lake, data factory, data bricks","azure data lake, data bricks, data factory, dax, etl pipelines, excel, powerbi, sql"
"Senior Data Engineer - Hybrid - Up to GBP65,000",Nigel Frank International,"Telford, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-hybrid-up-to-gbp65-000-at-nigel-frank-international-3734503656,2023-12-17,Swindon, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer - Hybrid - Up to £65,000
I'm currently working with a market leading manufacturer of specialist medical equipment who are working with groundbreaking technology to deliver data to health care professionals. They are looking for a Senior Data Engineer to join at the start of an exciting greenfield project where you will be responsible for the data migration to their brand new azure data platform.
In this role you will design, maintain and optimise on premise database solutions. You will be developing and maintaining ETL pipelines. You will also take lead of the migration of data to the new Azure Data Platform. This role will involve working with other specialists within the team to make technical decisions that will benefit the overall business.
This is a salaried position of up to £65,000 depending on experience plus a company benefits package. They pride themselves on being an employee orientated business and as such, they strongly believe in providing a healthy work life balance. This role offers hybrid working with occasional travel to their Stafford office on average of 4 times per month.
I am looking for...
Strong experience with Databricks for data ingestion and transformation
Experience working with the Azure Stack - Data factory, Synapse, Data Lake
Experience designing and implementing ETL solutions
Strong coding experience with coding languages such as Python or C#
This is a unique opportunity for a Senior Data Engineer to work on an exciting greenfield project and be supported in achieving you career goals and aspirations.
If this is of interest then get in touch ASAP. Send across your CV to d.moore1@nigelfrank.com or alternatively, give me a call on 0191 338 7577
Key Skills: Microsoft, Azure, Databricks, Data Factory, Data Lake, SQL, Python, C#, ETL, Engineer, Engineering
Show more
Show less","Databricks, Azure Data Factory, Azure Synapse, Azure Data Lake, Python, C#, ETL, SQL, Database, Data Migration","databricks, azure data factory, azure synapse, azure data lake, python, c, etl, sql, database, data migration","azure data factory, azure data lake, azure synapse, c, data migration, database, databricks, etl, python, sql"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3733884702,2023-12-17,Swindon, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader in the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on Databricks. Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too.
You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration too. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks.
There is great room for progression within this role, and support and training will be provided as well.
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, Azure Data Factory, SQL, Python, PySpark","databricks, azure data factory, sql, python, pyspark","azure data factory, databricks, python, spark, sql"
Senior Azure Data Developer – UK Wide,WSP in the UK,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-azure-data-developer-%E2%80%93-uk-wide-at-wsp-in-the-uk-3780778390,2023-12-17,Swindon, United Kingdom,Mid senior,Hybrid,"We are WSP - Join us and make your career future ready!
In today’s world it’s important to work for a company that has clear purpose, giving back to communities and supporting what is truly important in the world.
When considering a career move it’s vital to work for a business that is aligned to your values and goals, a place where you can belong. See what WSP stands for in 2024 and beyond…
To find out more about our
Intelligent Transportation Services (ITS)
business click on the following link and discover what awaits you at WSP: Intelligent transportation services (ITS) | WSP
Your new role, what's involved?
Join our Intelligent Infrastructure discipline and make a real-world difference by helping to deliver crucial projects for public sector clients, which include examples from using big data to help the DfT provide better rail services, to helping road safety officers visualise accident data geospatially, or gathering vital transport planning data using AI-enabled traffic cameras that distinguish and count the different modes of transport.
Work on amazing projects, large and small, to deliver tangible value for our clients through technology enabled change
Engage with clients and technical stakeholders to understand requirements and solve problems
Estimate, Design, Build and Deliver digital solutions using industry best practice
Carry out peer reviews and provide direction and guidance to other team members
Input to the continuous improvement of practice and standards within the team
Provide contributions to the future technical strategy of the team
Help to support and maintain new and legacy software systems, ensuring timely and effective resolutions while maintaining clear communication with stakeholders.
You will benefit from Personal Development Reviews that will align your personal development plan to your own career goals, and those of the business, to help grow and develop your skillset and elevate your position within the business over time.
YOUR TEAM
This role sits within the Intelligent Infrastructure business and will form a key part of the growing Data and Architecture Capability, which is our centre of excellence for digital skills. The Capability team provides a community of colleagues working in and around technology to help you deliver effectively and support your professional development through training, mentoring and opportunities to really stretch your capabilities.
You’ll work as part of an Agile team with specialist developers, testers and analysts to design, develop, deliver and maintain software applications and systems using the latest technologies, helping to make people’s journeys greener, safer, cheaper, and faster to deliver innovative solutions across the property and infrastructure sectors.
We operate a hybrid working policy and expect colleagues to spend time on client sites and collaborating with colleagues face-to-face, as well as working from a local office. We have offices across the UK.
We'd love to hear from you if you have:
Excellent analytical skills and a proven ability to think creatively to solve problems.
Significant experience in the development of data solutions within Azure, utilising functionality such as Azure Data Factory / Lake, Synapse Analytics, Logic and Function Apps and Azure SQL
In depth knowledge of data transformations, pipelines, structured / unstructured data and data ingestion from various sources such as blob, Excel, API.
Strong experience of working in an Agile delivery team and producing and deploying applications in a production environment through a CI/CD pipeline.
Experience with building enterprise-level web applications or services, predominantly using ASP .Net and Azure.
Strong leadership skills, demonstrating the ability to effectively lead and collaborate with other developers to achieve project goals and deliver high-quality solutions.
Experience of working within a DevOps environment
Experience working with clients and customers and excellent communications skills.
A degree in an analytical discipline such as engineering, mathematics, computer science, economics, social sciences, or equivalent industry experience.
Desirable Skills And Experience
Non-Azure cloud platforms (AWS / Google)
Wider programming experience, such a Python, R, PHP, etc
Associated Microsoft (or other) certificates and qualifications
Data Analytics using AI/ML
Visual analytics tools, e.g. Power BI / Tableau / etc.
Good general understanding of data governance/quality/privacy practices
Experience of NOSQL data stores such as MongoDB / CosmosDB
Experience of applying Software Architecture Patterns.
Infrastructure as code technologies and cloud technologies and distributed systems
Business Analysis, Solutions Architecture
Experience in both Cloud and On-Premise server infrastructure
Experience of applying Design Patterns to complex problems and architecting solutions
What's in it for you?
Work-life balance?
WSP recognises that work is only one part of your life and making time for other things is important – whether that’s for your families, friends, or yourself.
Our hybrid working policy allows the flexibility to work from the comfort of your own home as well as collaborating in our contemporary offices across the UK.
Inclusivity & Diversity?
We want our people to achieve rewarding careers, bringing their whole selves to work. We celebrate integrity and treat people with respect, supporting each other and embracing diversity to create a culture of inclusion and belonging at WSP.
Our employee resource groups VIBE (LGBTQ+ employees), CREED (Championing Racial Equality and Ethnic Diversity) and our Gender Balance Group, in tandem with WSP’s Neurodiverse Community Group, WSP Connect Group (visible and non-visible disabilities) help us promote the right environment for you to reach your full potential.
Health & Wellbeing?
We are committed to supporting our people, giving you the tools to make improvements to your health and wellbeing through our Thrive programme.
Med24 gives you and your family unrestricted telephone access to an NHS doctor where you can call day or night or have a face-to-face video consultation.
Flex your time?
For improved work life balance, WSP offers the “WSP Hour” which enables you to take one hour per day to do as you wish and make up the time earlier or later that day. We also offer part time and flexible working arrangements plus the option to flex your bank holiday entitlement to suit you.
Your development?
We appreciate that development and training is important to you and that’s why we have a supportive environment that invests in your development, whether that’s chartership, training or mentoring.
Apply now and be the future of WSP!
#WeAreWSP
Here at WSP we positively encourage applications from suitably qualified and eligible candidates regardless of sex, race, disability, age, sexual orientation, gender reassignment, religion or belief, marital status, pregnancy or maternity/paternity. As a Disability Confident leader, we will interview all disabled applicants who meet the essential criteria, please let us know if you require any workplace adjustments in support of your application.
Please note WSP reserves the right to close the vacancy before the advertised closing date.
Show more
Show less","Azure, Azure Data Factory, Azure Lake, Synapse Analytics, Logic Apps, Function Apps, Azure SQL, Data transformations, Pipelines, Structured data, Unstructured data, Data ingestion, Agile, .Net, ASP .Net, DevOps, AI, ML, Power BI, Tableau, Data governance, Data quality, Data privacy, MongoDB, CosmosDB, Software architecture patterns, Infrastructure as code, Cloud technologies, Distributed systems, Business Analysis, Solutions architecture, Design patterns, AWS, Google, Python, R, PHP","azure, azure data factory, azure lake, synapse analytics, logic apps, function apps, azure sql, data transformations, pipelines, structured data, unstructured data, data ingestion, agile, net, asp net, devops, ai, ml, power bi, tableau, data governance, data quality, data privacy, mongodb, cosmosdb, software architecture patterns, infrastructure as code, cloud technologies, distributed systems, business analysis, solutions architecture, design patterns, aws, google, python, r, php","agile, ai, asp net, aws, azure, azure data factory, azure lake, azure sql, business analysis, cloud technologies, cosmosdb, data governance, data ingestion, data privacy, data quality, data transformations, design patterns, devops, distributed systems, function apps, google, infrastructure as code, logic apps, ml, mongodb, net, php, pipelines, powerbi, python, r, software architecture patterns, solutions architecture, structured data, synapse analytics, tableau, unstructured data"
Business Data Analyst I,Liberty University,"Lynchburg, VA",https://www.linkedin.com/jobs/view/business-data-analyst-i-at-liberty-university-3726215385,2023-12-17,Virginia,United States,Associate,Onsite,"Job Description Summary
The Business Data Analyst will work as part of the Analytics and Decision Support department. The primary purpose of this position will be to enable the University through data and information for improved decision making. This position will work closely with other business data analysts to develop technical requirements and comprehensive solutions to extract and transform data from multiple source systems for business use. The ideal candidate will understand the importance of balancing customer requirements/feedback while also performing data analysis using various tools and techniques.
Essential Functions And Responsibilities
Analyze and advise management of workflow issues and data integrity problems and offer recommendations on resolution. Document business process, research best practices and support development of solutions for current business operations.
Understands University systems and data sets. Able to summarize and describe raw data sets for departmental consumption. Support validation processes that profile data attributes against their intended meaning.
Performs analysis of data that aligns to organizational goals and relays business impacts of findings through sound data driven recommendations.
Works closely with the business units with a high degree of customer service and provides input on functional business topics.
Assists the business unit in streamlining business processes using new or current technology, including form and reporting development. Adhering to internal development and publication standards.
Provides operational support to assigned business units systems, including assistance with technical issues.
Provides regular updates to customers on business process improvement and new technology activities.
Qualifications, Credentials, And Competencies
A bachelor's degree and one year of SQL experience, as well as Excel, Access, Argos, Tableau and SSRS experience required. Effective communication both verbally and in writing. Ability to intuitively reason, analyze information and events, and apply judgment in order to solve problems of both a routine and complex nature. Excellent computer and organizational skills. Proof of a valid Virginia driver’s license, an acceptable DMV record, and liability insurance is required.
Disclaimer Liberty University’s hiring practices and EEO Statement are fully in compliance with both federal and state law. Federal law creates an exception to the “religion” component of the employment discrimination laws for religious organizations (including educational institutions), and permits them to give employment preference to members of their own religion. Liberty University is in that category.
Show more
Show less","Business Data Analysis, Decision Support, Data Extraction, Data Transformation, SQL, Excel, Access, Argos, Tableau, SSRS, Data Integrity, Data Analysis, Business Process Improvement, Technology Adoption, Communication, Problem Solving, Organizational Skills","business data analysis, decision support, data extraction, data transformation, sql, excel, access, argos, tableau, ssrs, data integrity, data analysis, business process improvement, technology adoption, communication, problem solving, organizational skills","access, argos, business data analysis, business process improvement, communication, data extraction, data integrity, data transformation, dataanalytics, decision support, excel, organizational skills, problem solving, sql, ssrs, tableau, technology adoption"
Data Engineer – ETL Developer,Guidehouse,"Arlington, VA",https://www.linkedin.com/jobs/view/data-engineer-%E2%80%93-etl-developer-at-guidehouse-3733268402,2023-12-17,Virginia,United States,Mid senior,Onsite,"Job Family
Data Engineering & Architecture Consulting
Travel Required
None
Clearance Required
Active Secret
What You Will Do
Our consultants focusing on data engineering and architecture help clients maximize the value of their data. These high performing individuals works with clients to implement the full spectrum of data analytics and data science services, from data querying and wrangling to data engineering, to data visualization, dashboarding, and business intelligence (BI), to predictive analytics, machine learning (ML), and artificial intelligence (AI). Our services enable our clients to define their information strategy, enable mission critical insights and data-driven decision making, reduce cost and complexity, increase trust, and improve operational effectiveness.
Provide the full spectrum of data science services to support the mission of National Security clients.
Engage clients to define requirements, elicit feedback, and communicate results.
Collaborate with other data science and analytics professionals across our Defense & Security segment and provide data science / data analytics / business intelligence services on cross-functional teams delivering other consulting services as well.
Work as a data engineer and ETL (Extract, Transform, Load) developer for various data types and file types in collaboration with cross-functional teams.
Manage ETL functions and develop, build, test, and maintain scalable data pipeline architectures and tools.
Work with data integration and management tools and databases.
Gather requirements, design, implement, and test database systems.
Design and develop databases with Structured Query Language (SQL) and develop complex queries involving joins, subqueries, aggregation, indices, common table expressions, and query optimization.
Move and manipulate data of different types and file types using Python.
Provide and prepare data to enable data science and machine learning.
Demonstrate strong understanding of relational databases, columnar data warehouses, data lakes, NoSQL, and other storage types.
Work in cloud-based databases and ETL architecture.
Work with Azure and Databricks.
Design and build distributed systems for scalability and security.
Write technical process documentation.
Communicate/present ideas, methods, and results to clients, and explain technical topics to non-technical audiences.
Operate successfully on remote projects as well as hybrid on-site projects in the DC metro area.
What You Will Need
An ACTIVE and CURRENT SECRET federal security clearance
B.S. in Computer Science, Information Systems, or similar analytical and computational discipline.
3+ years of relevant professional experience.
Proficiency designing and developing databases with Structured Query Language (SQL) and performing complex queries.
Proficiency developing complex data pipelines and extract – transform – load (ETL) processes.
Proficiency moving and manipulating data of different types and file types using Python.
Strong understanding of different types of data storage.
Experience working in cloud-based databases and architecture.
Familiarity with Azure and Databricks.
Ability to support remote projects and hybrid on-site projects in the DC metro area.
What Would Be Nice To Have
An ACTIVE and CURRENT TOP SECRET federal security clearance
M.S. in Computer Science, Information Systems, or similar analytical and computational discipline.
6+ years of relevant professional experience.
Experience architecting data solutions in Azure.
Experience using Databricks.
Familiarity with DevOps and container technologies (Docker/Kubernetes).
Ability to implement basic automation and CI/CD.
Experience with micro services architecture and API gateways.
Technical expertise with data models, data mining, data cleansing, and segmentation techniques.
Experience with source code version control technologies such as Git.
Ability to code and debug stored procedures.
Experience supporting Federal National Security organizations.
What We Offer
Guidehouse offers a comprehensive, total rewards package that includes competitive compensation and a flexible benefits package that reflects our commitment to creating a diverse and supportive workplace.
Benefits Include
Medical, Rx, Dental & Vision Insurance
Personal and Family Sick Time & Company Paid Holidays
Position may be eligible for a discretionary variable incentive bonus
Parental Leave and Adoption Assistance
401(k) Retirement Plan
Basic Life & Supplemental Life
Health Savings Account, Dental/Vision & Dependent Care Flexible Spending Accounts
Short-Term & Long-Term Disability
Student Loan PayDown
Tuition Reimbursement, Personal Development & Learning Opportunities
Skills Development & Certifications
Employee Referral Program
Corporate Sponsored Events & Community Outreach
Emergency Back-Up Childcare Program
Mobility Stipend
About Guidehouse
Guidehouse is an Equal Employment Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, national origin, ancestry, citizenship status, military status, protected veteran status, religion, creed, physical or mental disability, medical condition, marital status, sex, sexual orientation, gender, gender identity or expression, age, genetic information, or any other basis protected by law, ordinance, or regulation.
Guidehouse will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable law or ordinance including the Fair Chance Ordinance of Los Angeles and San Francisco.
If you have visited our website for information about employment opportunities, or to apply for a position, and you require an accommodation, please contact Guidehouse Recruiting at 1-571-633-1711 or via email at RecruitingAccommodation@guidehouse.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodation.
Guidehouse does not accept unsolicited resumes through or from search firms or staffing agencies. All unsolicited resumes will be considered the property of Guidehouse and Guidehouse will not be obligated to pay a placement fee.
Show more
Show less","Data Engineering, Data Science, Data Analytics, Business Intelligence, Predictive Analytics, Machine Learning, Artificial Intelligence, Data Pipeline Architectures, Data Integration, Data Management, Databases, Database Systems, Structured Query Language (SQL), Python, Data Storage, Cloudbased Databases, ETL Architecture, Azure, Databricks, Distributed Systems, Technical Documentation, Remote Projects, Hybrid Projects, Computer Science, Information Systems, DevOps, Container Technologies, Docker, Kubernetes, Automation, CI/CD, Microservices Architecture, API Gateways, Data Models, Data Mining, Data Cleansing, Segmentation Techniques, Source Code Version Control, Git, Stored Procedures, National Security Organizations","data engineering, data science, data analytics, business intelligence, predictive analytics, machine learning, artificial intelligence, data pipeline architectures, data integration, data management, databases, database systems, structured query language sql, python, data storage, cloudbased databases, etl architecture, azure, databricks, distributed systems, technical documentation, remote projects, hybrid projects, computer science, information systems, devops, container technologies, docker, kubernetes, automation, cicd, microservices architecture, api gateways, data models, data mining, data cleansing, segmentation techniques, source code version control, git, stored procedures, national security organizations","api gateways, artificial intelligence, automation, azure, business intelligence, cicd, cloudbased databases, computer science, container technologies, data engineering, data integration, data management, data mining, data models, data pipeline architectures, data science, data storage, dataanalytics, database systems, databases, databricks, datacleaning, devops, distributed systems, docker, etl architecture, git, hybrid projects, information systems, kubernetes, machine learning, microservices architecture, national security organizations, predictive analytics, python, remote projects, segmentation techniques, source code version control, stored procedures, structured query language sql, technical documentation"
Sr. Data Engineer (Python/AWS),Software Technology Inc.,"Reston, VA",https://www.linkedin.com/jobs/view/sr-%C2%A0data%C2%A0engineer-python-aws-at-software-technology-inc-3752617091,2023-12-17,Virginia,United States,Mid senior,Onsite,"Hii ,
This is Vamshi ,from Software Technology We have a job opening with our client for position
Sr. Data Engineer (Python/AWS)
If you are available and looking for any new opportunities, please send me your updated resume for below position ASAP.
Job Title:
Sr. Data Engineer (Python/AWS)
Location:
Reston, Virginia, ( Local or Near By States)
(Onsite)
Duration: Long-term
Description
We are seeking a Python Developer with AWS For a large financial service client.
Your Future Duties And Responsibilities
In this role you will be focused on AWS Development and Architecture. We are looking for hands-on professionals who can build good technical solutions and then roll up their sleeves to implement the solution.
Required Qualifications To Be Successful In This Role.
8+ years of IT Data Engineering experiences.
4+ years of experience with Python (must have)
Must have strong experience with SQL, Databases (Oracle, PostgreSQL, Aurora)
2+ years of experience with Data engineering (EMR, PySpark, Redshift, Glue)
Strong experience with data migration, cloud migration, and ETL.
Strong experience with AWS Lambda, Fargate, SNS, Elastic Beanstalk, ECS, and CloudWatch.
Experience with enterprise data lakes, data warehouses, data marts, and big data.
Excellent communication skills to ask questions, clarify requirements, and engage with the team and stakeholders.
Strong logic, reasoning, and critical thinking skills to solve problems as they arise.
Must be an independent problem solver who can evaluate a situation and build solution options.
Strong Python, Strong working AWS experience, Databases (Oracle, PostgreSQL, Aurora) Data engineering (EMR, PySpark, Redshift, Glue), Serverless experience (Lambda, step functions), containerization (ECS with Fargate)
Desired Skillset
SAS knowledge
DevOps knowledge (Jenkins, Bitbucket, Terraform/UCD/CloudFormation)
Testing Automation
Educational Requirements
Bachelor's degree in computer science, Information Systems or related field
AWS Certification(s) desired
Thanks,
Vamshi Thangadpalli
Technical Recruiter
Direct:
404-777-9837 |
Fax:
866-608-6686
Email:
vamshi.t@stiorg.com |
Web:
www.stiorg.com
https://www.linkedin.com/in/vamshi-thangadpalli-3a0415251/
100 Overlook Center, Suite 200
Princeton, NJ 08540.
Show more
Show less","Python, AWS, SQL, Databases (Oracle PostgreSQL Aurora), Data engineering (EMR PySpark Redshift Glue), Data migration, Cloud migration, ETL, AWS Lambda, Fargate, SNS, Elastic Beanstalk, ECS, CloudWatch, Data lakes, Data warehouses, Data marts, Big data, Communication skills, Problemsolving skills, Critical thinking skills, SAS, DevOps, Jenkins, Bitbucket, Terraform, UCD, CloudFormation, Testing Automation, Computer science, Information Systems","python, aws, sql, databases oracle postgresql aurora, data engineering emr pyspark redshift glue, data migration, cloud migration, etl, aws lambda, fargate, sns, elastic beanstalk, ecs, cloudwatch, data lakes, data warehouses, data marts, big data, communication skills, problemsolving skills, critical thinking skills, sas, devops, jenkins, bitbucket, terraform, ucd, cloudformation, testing automation, computer science, information systems","aws, aws lambda, big data, bitbucket, cloud migration, cloudformation, cloudwatch, communication skills, computer science, critical thinking skills, data engineering emr pyspark redshift glue, data lakes, data marts, data migration, data warehouses, databases oracle postgresql aurora, devops, ecs, elastic beanstalk, etl, fargate, information systems, jenkins, problemsolving skills, python, sas, sns, sql, terraform, testing automation, ucd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Portland, OR",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773089679,2023-12-17,Newberg,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, bash, SQL, Pandas, R, Git, Airflow, Kubeflow, Snowflake, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning, Kubernetes, Docker, Kafka, Storm, SparkStreaming, Data governance, Data risk, Data compliance","python, java, bash, sql, pandas, r, git, airflow, kubeflow, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, kubernetes, docker, kafka, storm, sparkstreaming, data governance, data risk, data compliance","airflow, aws, azure, bash, data compliance, data governance, data risk, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Portland, OR",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759710398,2023-12-17,Newberg,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine Learning","python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning","airflow, aws, azure, bash, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, python, snowflake, spark, sparkstreaming, sql, storm"
Data Analyst Part Time,Voxmediallc,"Saint-Laurent, Manitoba, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-voxmediallc-3757202947,2023-12-17,Manitoba, Canada,Mid senior,Onsite,"Summary:
Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Data Interpretation, Advanced Statistical Techniques, SQL, R, Python, Data Management, ETL Processes, DataDriven DecisionMaking, Performance Metrics, Reporting, A/B Testing, Experimentation, Data Quality, Data Integrity, Data Accuracy, Tableau, Power BI, Data Visualization, CrossFunctional Teams, Communication, Collaboration","data analysis, data interpretation, advanced statistical techniques, sql, r, python, data management, etl processes, datadriven decisionmaking, performance metrics, reporting, ab testing, experimentation, data quality, data integrity, data accuracy, tableau, power bi, data visualization, crossfunctional teams, communication, collaboration","ab testing, advanced statistical techniques, collaboration, communication, crossfunctional teams, data accuracy, data integrity, data interpretation, data management, data quality, dataanalytics, datadriven decisionmaking, etl, experimentation, performance metrics, powerbi, python, r, reporting, sql, tableau, visualization"
Staff Data Engineer,Recruiting from Scratch,"Victoria, Manitoba, Canada",https://ca.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744395377,2023-12-17,Manitoba, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Deployment, Streamprocessing systems, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data warehouses, ETL, Legal compliance, Data management tools, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, legal compliance, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Victoria, Manitoba, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748828486,2023-12-17,Manitoba, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, TDD, Continuous Delivery, ETL, S3, Snowflake, Kafka, Spark, R&D, Python, SQL, Agile, Pair Programming, Continuous Integration, Streamprocessing systems, Data Warehouse, Data classification","data engineering, tdd, continuous delivery, etl, s3, snowflake, kafka, spark, rd, python, sql, agile, pair programming, continuous integration, streamprocessing systems, data warehouse, data classification","agile, continuous delivery, continuous integration, data classification, data engineering, datawarehouse, etl, kafka, pair programming, python, rd, s3, snowflake, spark, sql, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Victoria, Manitoba, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744398281,2023-12-17,Manitoba, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","TDD, Automation, Continuous Delivery, Data Engineering, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Agile Engineering, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, ETL Pipelines, Data Management Tools, Data Classification, Data Retention","tdd, automation, continuous delivery, data engineering, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, etl pipelines, data management tools, data classification, data retention","agile engineering, airflow, automated testing, automation, continuous delivery, continuous integration, data classification, data engineering, data management tools, data retention, data warehouses, deployment, dimensional data modeling, docker, etl, etl pipelines, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Customer Service Representative/Data Analyst,Drmartens,"Thompson, Manitoba, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-at-drmartens-3757205747,2023-12-17,Manitoba, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
Powered by Webbtree
Show more
Show less","Data analysis, Statistical techniques, SQL, R, Python, Datadriven decisionmaking, Performance metrics, Reports, Models, Algorithms, Business optimization, A/B testing, Data quality, Data integrity, Data collection, Data cleansing, Data manipulation, Datadriven reports, Data visualization, Tableau, Power BI, Statistical modeling, Hypothesis testing, ETL processes","data analysis, statistical techniques, sql, r, python, datadriven decisionmaking, performance metrics, reports, models, algorithms, business optimization, ab testing, data quality, data integrity, data collection, data cleansing, data manipulation, datadriven reports, data visualization, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, algorithms, business optimization, data collection, data integrity, data manipulation, data quality, dataanalytics, datacleaning, datadriven decisionmaking, datadriven reports, etl, hypothesis testing, models, performance metrics, powerbi, python, r, reports, sql, statistical modeling, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk/Part Time,Rodtookjing,"Victoria, Manitoba, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-rodtookjing-3741439516,2023-12-17,Manitoba, Canada,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk/Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","SQL, R, Python, Tableau, Power BI, Data analysis, Data visualization, Statistical modeling, Hypothesis testing, A/B testing, Data management, ETL processes","sql, r, python, tableau, power bi, data analysis, data visualization, statistical modeling, hypothesis testing, ab testing, data management, etl processes","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
"Job Opening for Data Warehouse Architect/Engineer - Omaha, NE",Steneral Consulting,"Omaha, NE",https://www.linkedin.com/jobs/view/job-opening-for-data-warehouse-architect-engineer-omaha-ne-at-steneral-consulting-3644953896,2023-12-17,Council Bluffs,United States,Mid senior,Onsite,"Hi,
Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""+1 3026017375"".
Job Title:- DATA WAREHOUSE ARCHITECT/ENGINEER
Work Location:-
NEBRASKA,OMAHA
Duration: 6+month
Work Authorization:- Citizen, GC
Interview Mode: Phone/Skype
INTERVIEW PROCESS : 2 ROUNDS OF INTERVIEWS ONE REMOTE INTERVIEW AND ONE ONSITE THEY MIGHT EVEN JUST CALL FOR ONSITE INTERVIEW
job Title: Data Warehouse Architect
Job Duties
Design and implement data architecture solutions, including data modeling, data integration, and data management
Consolidate existing data sources into a new data warehouse
Either validate or invalidate the existing data structures in place
Help determine analytics and ETL tools
Design efficient data pipelines to transform raw data sources into, reliable components ensuring data quality, efficient processing, and timely delivery of accurate and trusted data
Design data models for optimal storage and retrieval to meet critical business requirements
Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs
Maintain high standards of engineering excellence
Define and enforce data governance policies and advocate for best practices
What They Are Looking For/Requirements
5+ years experience working with data warehousing
Previous experience designing and implementing a new data warehouse (at least two projects)
Experience with SQL Server
Strong Data Modeling skills
Experience with Star and Snowflake Data Warehouse Schemas
Experience with multiple ETL tools such as SSIS, Informatica, DataStage, etc...
Experience with multiple BI tools such as SSAS, Congnos or Business Objects
Experience with collecting data from tradition systems as well as shop floor systems and IOT devices
Kirti Rani
Associate Talent Acquisition -North America
Desk: +1 3026017375
kirti@steneral.com
In my absence please reach out to Mr. Harish Sharma at harish@steneral.com &
3027216151
Show more
Show less","Data Architecture, Data Modeling, Data Integration, Data Management, Data Warehouse, SQL Server, Star Schema, Snowflake Schema, ETL Tools, SSIS, Informatica, DataStage, BI Tools, SSAS, Congnos, Business Objects, Data Governance, Data Quality, Data Delivery, Data Privacy, Data Security","data architecture, data modeling, data integration, data management, data warehouse, sql server, star schema, snowflake schema, etl tools, ssis, informatica, datastage, bi tools, ssas, congnos, business objects, data governance, data quality, data delivery, data privacy, data security","bi tools, business objects, congnos, data architecture, data delivery, data governance, data integration, data management, data privacy, data quality, data security, datamodeling, datastage, datawarehouse, etl tools, informatica, snowflake schema, sql server, ssas, ssis, star schema"
Data Lake Lead Developer,iTech Solutions,"Omaha, NE",https://www.linkedin.com/jobs/view/data-lake-lead-developer-at-itech-solutions-3714395979,2023-12-17,Council Bluffs,United States,Mid senior,Onsite,"Job Title: Data Lake Lead Developer
Work Location: Omaha, NE
6 months Contract
Responsibilities:
Designing and implementing highly performant data ingestion pipelines from multiple sources using Apache Spark and/or Azure Databricks ensuring that the infrastructure is highly-available and secure.
Develop scalable and re-usable frameworks for ingestion of data from centralized and federated hubs
Architect and build security compliant user management framework for multi-tenant big data platform
Performance-tune Apache Spark applications to optimize cluster configurations
Set up and maintain infrastructure for RESTful microservices layer
Integrate the end to end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times
Collaborate across internal functional groups and external vendors to incorporate emerging/Client big data tools
Help maintain and support the platform on a day-to-day basis including on-boarding new members
Work closely with the product management and development teams to rapidly translate the understanding of customer data and requirements to product and solutions
Enable ETL/ELT solutions and perform DB tuning, table partitioning, shell scripting, drive prototypes and POCs
Qualifications:
Bachelor's Degree in Computer Science
Minimum 8 years of experience as a technology leader with strong knowledge of Data Management principles and dimensional modelling concepts
Advanced applied knowledge of Spark - a must.
Proven knowledge of designing, developing and deploying federated data architecture - highly desirable
Hands on experience designing and delivering solutions using with Microsoft Azure data services (Azure Data Lake, Azure Data Factory, Azure Client, Azure SQL, Azure Datawarehouse, Azure DataBricks, Azure Cosmos DB) -- highly desirable
Experience with event based / streaming technologies to ingest and process data (Apache KAFKA, Nifi, Azure Event Hub) - highly desirable
Experience with OS non-relational/NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J) - desirable
Familiarity Open Source big data products Hadoop (incl. Hive, Pig, Impala) - desirable
Broad based architecture acumen: Database architecture, ETL, SOA, Cloud, etc - desirable
Ability to work successfully in a distributed team environment
Show more
Show less","Apache Spark, Azure Databricks, RESTful, Apache Kafka, Hive, Pig, Impala, MongoDB, Cassandra, Neo4J, Hadoop, SQL, Azure Data Lake, Azure Data Factory, Azure Datawarehouse, Azure DataBricks, Azure Cosmos DB, Azure Event Hub, NoSQL, ETL, ELT, SOA, Cloud","apache spark, azure databricks, restful, apache kafka, hive, pig, impala, mongodb, cassandra, neo4j, hadoop, sql, azure data lake, azure data factory, azure datawarehouse, azure databricks, azure cosmos db, azure event hub, nosql, etl, elt, soa, cloud","apache kafka, apache spark, azure cosmos db, azure data factory, azure data lake, azure databricks, azure datawarehouse, azure event hub, cassandra, cloud, elt, etl, hadoop, hive, impala, mongodb, neo4j, nosql, pig, restful, soa, sql"
Staff Data Engineer,Recruiting from Scratch,"Omaha, NE",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744394396,2023-12-17,Council Bluffs,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Data Science, Data Engineering, SQL, Snowflake, Airflow, Kafka, Kubernetes, Docker, Helm, pySpark, Spark, Storm, SparkStreaming, Data Warehouses, ETL, TDD, Pair Programming, Continuous Integration, Automated Testing, Tech Stack, Data Governance, Data Management Tools, Data Classification, Data Retention, Distributed Databases","python, data science, data engineering, sql, snowflake, airflow, kafka, kubernetes, docker, helm, pyspark, spark, storm, sparkstreaming, data warehouses, etl, tdd, pair programming, continuous integration, automated testing, tech stack, data governance, data management tools, data classification, data retention, distributed databases","airflow, automated testing, continuous integration, data classification, data engineering, data governance, data management tools, data retention, data science, data warehouses, distributed databases, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd, tech stack"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Omaha, NE",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744395033,2023-12-17,Council Bluffs,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Science, Business Intelligence, Python, Snowflake, Big Data Technologies, Relational Databases, SQL, Agile Engineering Practices, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, ETL Pipelines, Legal Compliance, Data Management Tools, Data Classification, Data Retention","data engineering, data science, business intelligence, python, snowflake, big data technologies, relational databases, sql, agile engineering practices, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, etl pipelines, legal compliance, data management tools, data classification, data retention","agile engineering practices, big data technologies, business intelligence, data classification, data engineering, data management tools, data retention, data science, dimensional data modeling, etl pipelines, kafka, legal compliance, python, relational databases, schema design, snowflake, sparkstreaming, sql, storm, streamprocessing systems"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Omaha, NE",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748830443,2023-12-17,Council Bluffs,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Kafka, Storm, SparkStreaming, ETL, Dimensional data modeling, Schema design, Data Warehouses","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, etl, dimensional data modeling, schema design, data warehouses","airflow, business intelligence, data engineering, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, python, schema design, snowflake, spark, sparkstreaming, sql, storm"
Data Engineer - Scala(U.S. remote),Railroad19,"Omaha, NE",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782295416,2023-12-17,Council Bluffs,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, Restful APIs, AWS, EMR clusters, S3, Relational databases, Nonrelational databases","scala 212, spark 24, restful apis, aws, emr clusters, s3, relational databases, nonrelational databases","aws, emr clusters, nonrelational databases, relational databases, restful apis, s3, scala 212, spark 24"
Senior Civil Engineer - Data Center (Remote),Olsson,"Omaha, NE",https://www.linkedin.com/jobs/view/senior-civil-engineer-data-center-remote-at-olsson-3784204616,2023-12-17,Council Bluffs,United States,Mid senior,Remote,"Company Description
We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As an experienced engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Qualifications
You are passionate about:
Working collaboratively with others
Having ownership in the work you do
Using your talents to positively affect communities
Solving problems
Providing excellence in client service
You bring to the team:
Strong communication skills
Ability to contribute and work well on a team
Bachelor's Degree in civil engineering
At least 6 years of related civil engineering experience
Proficient in Civil 3D software
Must be a registered professional engineer
Additional Information
Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
Engage in work that has a positive impact in communities
Receive an excellent 401(k) match
Participate in a wellness program promoting balanced lifestyles
Benefit from a bonus system that rewards performance
Have the possibility for flexible work arrangements
Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
Show more
Show less","Civil engineering, Proficient in Civil 3D software, Project management, Design engineering, Planning and design documents, Design calculations, Team and client standards, Quality assurance/quality control, Communication skills, Teamwork, Bachelor's Degree in civil engineering, 6+ years of related civil engineering experience","civil engineering, proficient in civil 3d software, project management, design engineering, planning and design documents, design calculations, team and client standards, quality assurancequality control, communication skills, teamwork, bachelors degree in civil engineering, 6 years of related civil engineering experience","6 years of related civil engineering experience, bachelors degree in civil engineering, civil engineering, communication skills, design calculations, design engineering, planning and design documents, proficient in civil 3d software, project management, quality assurancequality control, team and client standards, teamwork"
Experienced Civil Engineer - Data Center (Remote),RemoteWorker US,"Omaha, NE",https://www.linkedin.com/jobs/view/experienced-civil-engineer-data-center-remote-at-remoteworker-us-3787519955,2023-12-17,Council Bluffs,United States,Mid senior,Remote,"Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As an engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Show more
Show less","Data center design, Project management, Design engineering, Planning and design documents, Design calculations, Team and client standards, Quality assurance/quality control, Advisor on complex projects, Coordination with teams staff clients and consultants, Observation at job sites, Client meetings","data center design, project management, design engineering, planning and design documents, design calculations, team and client standards, quality assurancequality control, advisor on complex projects, coordination with teams staff clients and consultants, observation at job sites, client meetings","advisor on complex projects, client meetings, coordination with teams staff clients and consultants, data center design, design calculations, design engineering, observation at job sites, planning and design documents, project management, quality assurancequality control, team and client standards"
Experienced Civil Engineer - Data Center (Remote),Olsson,"Omaha, NE",https://www.linkedin.com/jobs/view/experienced-civil-engineer-data-center-remote-at-olsson-3784206329,2023-12-17,Council Bluffs,United States,Mid senior,Remote,"Company Description
We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As an engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Qualifications
You are passionate about:
Working collaboratively with others
Having ownership in the work you do
Using your talents to positively affect communities
Solving problems
Providing excellence in client service
You bring to the team:
Strong communication skills
Ability to contribute and work well on a team
Bachelor's Degree in civil engineering
At least 6 years of related civil engineering experience
Proficient in Civil 3D software
Must be a registered professional engineer
Additional Information
Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
Engage in work that has a positive impact in communities
Receive an excellent 401(k) match
Participate in a wellness program promoting balanced lifestyles
Benefit from a bonus system that rewards performance
Have the possibility for flexible work arrangements
Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
Show more
Show less","Civil 3D, AutoCAD, GIS, Surveying, Hydrology, Hydraulics, Stormwater management, Grading and drainage, Earthwork, Structural engineering, Mechanical engineering, Electrical engineering, BIM (Building Information Modeling), Project management, Design management, Quality assurance/quality control, Communication, Teamwork, Problemsolving, Client service, Registered professional engineer","civil 3d, autocad, gis, surveying, hydrology, hydraulics, stormwater management, grading and drainage, earthwork, structural engineering, mechanical engineering, electrical engineering, bim building information modeling, project management, design management, quality assurancequality control, communication, teamwork, problemsolving, client service, registered professional engineer","autocad, bim building information modeling, civil 3d, client service, communication, design management, earthwork, electrical engineering, gis, grading and drainage, hydraulics, hydrology, mechanical engineering, problemsolving, project management, quality assurancequality control, registered professional engineer, stormwater management, structural engineering, surveying, teamwork"
Licensed Civil Engineer - Data Center (Remote),Olsson,"Omaha, NE",https://www.linkedin.com/jobs/view/licensed-civil-engineer-data-center-remote-at-olsson-3784200966,2023-12-17,Council Bluffs,United States,Mid senior,Remote,"Company Description
We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As a Licensed Civil Engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Qualifications
You are passionate about:
Working collaboratively with others
Having ownership in the work you do
Using your talents to positively affect communities
Solving problems
Providing excellence in client service
You bring to the team:
Strong communication skills
Ability to contribute and work well on a team
Bachelor's Degree in civil engineering
At least 6 years of related civil engineering experience
Proficient in Civil 3D software
Must be a registered professional engineer
Additional Information
Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
Engage in work that has a positive impact in communities
Receive an excellent 401(k) match
Participate in a wellness program promoting balanced lifestyles
Benefit from a bonus system that rewards performance
Have the possibility for flexible work arrangements
Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
Show more
Show less","Civil 3D, Civil Engineering, Bachelor's Degree in Civil Engineering, Registered Professional Engineer, Project Management, Design Engineering, Quality Assurance/Quality Control","civil 3d, civil engineering, bachelors degree in civil engineering, registered professional engineer, project management, design engineering, quality assurancequality control","bachelors degree in civil engineering, civil 3d, civil engineering, design engineering, project management, quality assurancequality control, registered professional engineer"
Salesforce CRMA Data Architect / Data Engineer - Analyst,Silverline,"Omaha, NE",https://www.linkedin.com/jobs/view/salesforce-crma-data-architect-data-engineer-analyst-at-silverline-3778733469,2023-12-17,Council Bluffs,United States,Mid senior,Remote,"Silverline Company Overview:
As part of Mphasis, Silverline tailors digital transformation solutions to meet your specific needs by leveraging insights acquired through 10+ years in the business. We've completed thousands of Salesforce engagements informed by real-world expertise gained across Media and Entertainment, Financial Services and Healthcare industries. From strategic planning and implementation to managed services, we guide clients through every phase of their journey, enabling continuous value with the Salesforce platform. We also offer CalendarAnything, a popular scheduling application on the AppExchange, as well as industry-proven accelerators.
Silverline Value Proposition:
Silverline employees are a diverse group of global professionals who are passionate about creating rewarding experiences for our teams, our clients, and the world we live in. Silverline's culture of collaboration, inclusion, and career growth plays a vital role in our success. Come be a part of our team and join the fun!
Role Overview:
In this position, you will help solve business-related needs and operationalize customer data by effectively embedding the proper visualizations, analytics, insights, and predictive and prescriptive recommendations into the day-to-day tasks, workflows, and experiences for the various users/personas across the organization.
Primary Responsibilities:
Work with clients to communicate data analytics dependencies and needs
Lead data analytics requirements gathering and design workshops
Create and own Solution Design document for the data analytics efforts as well as the backlog of user stories. Create a Solution Design document detailing,
Connected Data
Data Flows and/or Recipes
Data-driven reports/lenses, dashboards in CRMA
Salesforce Analytics Query Language (SAQL)
Visualizations
Work with the client to obtain access to source systems/files and/or to generate appropriate extracts
Create and work with a reusable data analytics approach
Capture metrics on success, failures, and mitigation steps
Technical leadership, setting best practices for data analytics solution build, testing, and iterative refinement
Manage technical scope and client expectations
Work closely with and on occasion lead a team of developers on projects
Required Qualifications:
3+ years of experience building Salesforce CRM Analytics solutions and strategies
2+ years of experience across a wide variety of industries including Financial Services and/or Healthcare
Experience working with Salesforce, and Snowflake data in Salesforce CRMA
Expertise with at least one of the following database technologies and familiarity with the others: relational, columnar, and NoSQL (SQL, Oracle, MySQL)
Preferred Qualifications:
3+ years of experience in Tableau, both architecting and implementing solutions
Data integration tool experience: Python, AWS, Informatica, Tableau, Jitterbit
Certifications:
Salesforce Certified Einstein Analytics and Discovery Consultant
Salesforce Certified Administrator
EEO & Pay transparency Statement:
Silverline provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. Compensation for this role is expected to range from $100,000 - $140,000 in base salary plus performance-based bonus. Actual compensation offered will be determined by, and commensurate with, candidate experience as evaluated during the interview process.
Show more
Show less","Salesforce CRM Analytics, Salesforce CRMA, SAQL, Snowflake, SQL, Oracle, MySQL, Tableau, Python, AWS, Informatica, Jitterbit, Einstein Analytics and Discovery Consultant, Salesforce Certified Administrator","salesforce crm analytics, salesforce crma, saql, snowflake, sql, oracle, mysql, tableau, python, aws, informatica, jitterbit, einstein analytics and discovery consultant, salesforce certified administrator","aws, einstein analytics and discovery consultant, informatica, jitterbit, mysql, oracle, python, salesforce certified administrator, salesforce crm analytics, salesforce crma, saql, snowflake, sql, tableau"
Data Manager/Engineer,The Garrett Group,"Bellevue, NE",https://www.linkedin.com/jobs/view/data-manager-engineer-at-the-garrett-group-3763983339,2023-12-17,Council Bluffs,United States,Mid senior,Hybrid,"Are you ready to make a significant impact on critical decision-making processes, drive operational efficiency, and enhance performance in a dynamic environment? The Garrett Group (TGG) invites you to join our team as a Data Manager/Engineer, where you'll play a pivotal role in supporting USSTRATCOM's mission by applying advanced analytical techniques to address complex challenges.
Required Experience Includes 5 Years Of Experience In
Managing the effective development and use of data systems to include:
Collection, organization, storage, protection, and analysis of data systems;
Implementation of backup and restoral procedures, and working practices;
Ensuring compliance with data architecture and data management standards and procedures defined in the Data Management Framework
Developing, constructing, testing, and maintaining architectures, such as databases and large-scale processing systems
Deriving trends in data sets and developing algorithms to help make raw data more useful to the enterprise
Experience with SQL database design and multiple programming language to prepare data for analytical or operational use
Capability to communicate requirements and data trends across departments and the enterprise
Additional Experience Desired
Strong background in Information Technology and systems, encompassing aspects like networking and cloud storage
Familiarity with Microsoft Access and Oracle databases
Competence with Microsoft Excel Power BI
Understanding of Data Ontology, particularly Basic Formal Ontology (BFO), are additional assets.
Experience with Informatica or other data Extract, Transform, Load (ETL) tools and processes is also beneficial.
At The Garrett Group, we prioritize the well-being of our team members and offer a comprehensive benefits package, including healthcare coverage, financial security through employer-paid life insurance, educational support with tuition assistance, and financial planning opportunities through our 401K plan. We value your dedication and talent, and our benefits package reflects our commitment to your overall well-being and professional success.
The Garrett Group is an Equal Opportunity/Affirmative Action Employer, and we encourage applications from all qualified candidates, regardless of various factors such as race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Job Posted by ApplicantPro
Show more
Show less","Data Management, Data Engineering, Data Analytics, Data Architecture, Data Visualization, SQL, Programming Languages, Data Ontology, Basic Formal Ontology (BFO), Informatica, Data Extraction, Data Transformation, Data Loading (ETL), Networking, Cloud Storage, Microsoft Access, Oracle Databases, Microsoft Excel Power BI","data management, data engineering, data analytics, data architecture, data visualization, sql, programming languages, data ontology, basic formal ontology bfo, informatica, data extraction, data transformation, data loading etl, networking, cloud storage, microsoft access, oracle databases, microsoft excel power bi","basic formal ontology bfo, cloud storage, data architecture, data engineering, data extraction, data loading etl, data management, data ontology, data transformation, dataanalytics, informatica, microsoft access, microsoft excel power bi, networking, oracle databases, programming languages, sql, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Omaha, NE",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707760,2023-12-17,Council Bluffs,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, ML Data OPs, Data pipelines, ML models, Data mining, Data cleaning, Data normalization, Data modeling, Data platforms, Data processing, Data governance, Python, Java, bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, NoSQL, DynamoDB, ETL, Conversational AI APIs, Recommender systems, Distributed systems, Microservices, Kafka, Storm, SparkStreaming, Machine learning, Legal compliance, Data classification, Data retention","data engineering, ml data ops, data pipelines, ml models, data mining, data cleaning, data normalization, data modeling, data platforms, data processing, data governance, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, dynamodb, etl, conversational ai apis, recommender systems, distributed systems, microservices, kafka, storm, sparkstreaming, machine learning, legal compliance, data classification, data retention","airflow, aws, azure, bash, conversational ai apis, data classification, data cleaning, data engineering, data governance, data mining, data normalization, data platforms, data processing, data retention, datamodeling, datapipeline, distributed systems, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, legal compliance, machine learning, microservices, ml data ops, ml models, nosql, python, recommender systems, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Omaha, NE",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773086809,2023-12-17,Council Bluffs,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Conversational AI APIs, Recommender Systems, Microservices, Kubernetes, Docker, ML Data OPs, LLMs, NLP, Data Visualization, Pandas, R, Statistical Analysis, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Data Enrichment, Data Governance, Data Compliance, Data Management, Data Classification, Data Retention, Agile","data engineering, machine learning, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, conversational ai apis, recommender systems, microservices, kubernetes, docker, ml data ops, llms, nlp, data visualization, pandas, r, statistical analysis, data mining, data cleaning, data normalization, data modeling, data enrichment, data governance, data compliance, data management, data classification, data retention, agile","agile, airflow, aws, azure, bash, conversational ai apis, data classification, data cleaning, data compliance, data engineering, data enrichment, data governance, data management, data mining, data normalization, data retention, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, llms, machine learning, microservices, ml data ops, nlp, pandas, python, r, recommender systems, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Data Analyst,Motion Recruitment,"Northfield, NJ",https://www.linkedin.com/jobs/view/data-analyst-at-motion-recruitment-3774629909,2023-12-17,Ocean City,United States,Mid senior,Onsite,"A SaaS company in the Greater Philadelphia area is on lookout for an eager Data Analyst to join their collaborative adn rowing team. As a key member of our data-driven company, you will play a crucial role in uncovering trends, making data-informed decisions, and contributing to our mission of enhancing municipal functions through data-driven insights.
Required Skills & Experience
Data Analytics experience
SQL or any relational databaes
Python
ETL
Data Migration etc
Collaborative, detail-oriented, and self-driven.
Desired Skills & Experience
Data Anlytics
Interest in sturtcing raw data
Experience with finance or budgeting
High Level Communicator
Tech Stack
SQL
Python
ETL
Monarch
Benefits
Health, Dental, Vision Insurance.
401(k) with 3% matching.
21 days PTO.
10% bonus.
Posted By:
Moira Shakin
Show more
Show less","Data Analytics, SQL, Python, ETL, Data Migration, Monarch","data analytics, sql, python, etl, data migration, monarch","data migration, dataanalytics, etl, monarch, python, sql"
Data Scientist,Quick Release_,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-scientist-at-quick-release-3758373397,2023-12-17,Werribee, Australia,Mid senior,Onsite,"Quick Release (QR_) is a subsidiary of Alten Group located in Australia. Alten Group is a worldwide leader in Engineering services (46,000 employees) with operations in 30 different countries.
We work in key areas such as Aerospace and Defence, Automotive, Rail, Energy and Life Science.
To support our projects with our customers in Australia QR_ is currently expending its activity and we are looking for a Data Scientist to join our team.
Requirements
Quick Release is looking for a Data Scientist to join our team and support the growth of our organisation in the defence industry. We are looking a highly motivate worker that enjoys new challenges and solving problems on the fly in a dynamic environment. The role requires autonomy and leadership in an exciting new area with opportunities of growth and ownership.
Responsibilities:
Provide support to multiple customers through scoping, advisory and delivery type deployments
Delivery solutions to existing systems that improve data quality
Provide analysis of existing data to leverage time and cost efficiencies in product development
Design and implement reporting structures that deliver useful metrics in product development
Create process documentation and assist in the training of new process developments
Support the systems engineering team through product data management
Key stakeholder management and project governance
Experience
5+ years of experiences, in the defence industry ideal
Experience working with integrated systems
Some coding skills (python, VBA etc)
Experience working with complex data systems and data flow
Agile methodologies, i.e. SCRUM, SAFE
Benefits
Join a team that listens to your input
A light-hearted culture
Join a fast-paced team with opportunities to build your career
Internal training and personal development
Yearly away and team bonding trips
Show more
Show less","Data Science, Python, VBA, Agile methodologies, SCRUM, SAFE, Integrated systems, Complex data systems, Product data management, Stakeholder management, Project governance","data science, python, vba, agile methodologies, scrum, safe, integrated systems, complex data systems, product data management, stakeholder management, project governance","agile methodologies, complex data systems, data science, integrated systems, product data management, project governance, python, safe, scrum, stakeholder management, vba"
People Operations Data Analyst,The Center for Discovery,"Harris, NY",https://www.linkedin.com/jobs/view/people-operations-data-analyst-at-the-center-for-discovery-3606181283,2023-12-17,Rock Hill,United States,Mid senior,Hybrid,"Responsibilities
The Center for Discovery is currently seeking a Data Analyst for The Department of People Operations. This position serves a key role in providing organizational leadership, leveraging analysis to make well-informed data-driven workforce decisions. The Data Analyst will work to produce dashboards and status reports, providing information in real time. The candidate must possess strong analytic, communication and interpersonal skills, ability to work independently and communicate effectively with staff of all levels, and the ability to work in a fast-paced environment while managing multiple projects. With latitude for independent initiative and judgement, the People Operations Data Analyst responsibilities/duties will include (but are not limited to):
 Ability to create new reports and modify existing reports
 Create effective data visualizations
 Manage data confidentiality
 Communicate findings effectively with department leaders
 Analyze data to find new ways to improve operations
 Work collaboratively to identify problems and find solutions
Qualifications
 Bachelor’s degree or Master’s in Human Resources, Statistics, Analytics or related field, preferred
 Minimum of 2 years in an Analyst role
 PHR, SPHR certification, a plus
 HRIS System Experience Required
 Skilled in using statistics to provide solutions
 Strong organizational and problem-solving skills with impeccable multi-tasking abilities
 Flexible to the daily changing needs and handle obstacles with compassion and resolve
Show more
Show less","Data Analyst, HRIS System, Statistics, Effective Data Visualization, Data Confidentiality, Communication, Problem Solving, Multitasking, Organizational Skills","data analyst, hris system, statistics, effective data visualization, data confidentiality, communication, problem solving, multitasking, organizational skills","communication, data confidentiality, dataanalytics, effective data visualization, hris system, multitasking, organizational skills, problem solving, statistics"
Customer Service Representative/Data Analyst/Data Entry Clerk,Newyorkuniversity,"Timmins, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-newyorkuniversity-3755590272,2023-12-17,Timmins, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
Powered by Webbtree
Show more
Show less","Data Analysis, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Visualization, Data Management, ETL Processes","data analysis, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data visualization, data management, etl processes","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Marketing Data Analyst,Sandhills Global,"Lincoln, NE",https://www.linkedin.com/jobs/view/marketing-data-analyst-at-sandhills-global-3594991412,2023-12-17,Lincoln,United States,Mid senior,Onsite,"QUALIFICATIONS:
Bachelors or Masters Degree in Business Administration, Marketing, Psychology, Communications preferred or any other relevant program
Knowledge of the products and customers of the Auction and Heavy Equipment industries preferred
Experience with web analytics reporting tools such as Google Analytics preferred
Excellent oral and written communication skills
Able to work independently and communicate within a team environment
Detail oriented and analytical skills
Results oriented
Flexible and open to change
Stable and progressive work history
Professional appearance and demeanor
Ability to travel by air and ground -- valid driver’s license required
Strong computer skills
Able to work with minimal supervision
Now offering a 4.5 day workweek!
DESCRIPTION: The Marketing Data Analyst will be responsible for maintaining Google Tag Manager and on-going updates to tracking on our websites, setting up analytics for new and acquired websites, building reports in Google Analytics and Google Data Studio, pulling data from ads/marketing platforms, and analyzing all marketing data for the purpose of optimizing campaigns, improving our websites, and communication to stakeholders. This position will also be responsible for monitoring the integrity and compliance of the data we collect and use for marketing purposes.
Experience with web analytics reporting tools, such as Google Analytics, is preferred. The Marketing Data Analyst must have a good understanding of the market that they are assigned to and the goals and objectives of the department. It will be their responsibility to manage the data collected for the industry that they are assigned to. This will require good communication and organizational skills.
This position could require some travel. This will include overnight and weekend stays to industry events and customer facilities. They must help encourage a team environment to help assure the success of the publications and web sites. Other duties may be assigned determined by the needs of the industry.
Show more
Show less","Google Analytics, Google Tag Manager, Google Data Studio, Web Analytics, Data Analysis, Data Interpretation, Data Visualization, Data Integrity, Compliance, Communication, Organizational Skills, Team Work","google analytics, google tag manager, google data studio, web analytics, data analysis, data interpretation, data visualization, data integrity, compliance, communication, organizational skills, team work","communication, compliance, data integrity, data interpretation, dataanalytics, google analytics, google data studio, google tag manager, organizational skills, team work, visualization, web analytics"
Senior Civil Engineer - Data Center (Remote),Olsson,"Lincoln, NE",https://www.linkedin.com/jobs/view/senior-civil-engineer-data-center-remote-at-olsson-3784202753,2023-12-17,Lincoln,United States,Mid senior,Remote,"Company Description
We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As an experienced engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Qualifications
You are passionate about:
Working collaboratively with others
Having ownership in the work you do
Using your talents to positively affect communities
Solving problems
Providing excellence in client service
You bring to the team:
Strong communication skills
Ability to contribute and work well on a team
Bachelor's Degree in civil engineering
At least 6 years of related civil engineering experience
Proficient in Civil 3D software
Must be a registered professional engineer
Additional Information
Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
Engage in work that has a positive impact in communities
Receive an excellent 401(k) match
Participate in a wellness program promoting balanced lifestyles
Benefit from a bonus system that rewards performance
Have the possibility for flexible work arrangements
Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
Show more
Show less","Civil 3D, Civil engineering, Design, Planning, Quality assurance, Quality control","civil 3d, civil engineering, design, planning, quality assurance, quality control","civil 3d, civil engineering, design, planning, quality assurance, quality control"
Experienced Civil Engineer - Data Center (Remote),Olsson,"Lincoln, NE",https://www.linkedin.com/jobs/view/experienced-civil-engineer-data-center-remote-at-olsson-3784205419,2023-12-17,Lincoln,United States,Mid senior,Remote,"Company Description
We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As an engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Qualifications
You are passionate about:
Working collaboratively with others
Having ownership in the work you do
Using your talents to positively affect communities
Solving problems
Providing excellence in client service
You bring to the team:
Strong communication skills
Ability to contribute and work well on a team
Bachelor's Degree in civil engineering
At least 6 years of related civil engineering experience
Proficient in Civil 3D software
Must be a registered professional engineer
Additional Information
Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
Engage in work that has a positive impact in communities
Receive an excellent 401(k) match
Participate in a wellness program promoting balanced lifestyles
Benefit from a bonus system that rewards performance
Have the possibility for flexible work arrangements
Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
Show more
Show less","Civil Engineering, Civil 3D, Project Management, Design Calculations, Quality Assurance, Quality Control, AutoCAD, Proficient Communication, Teamwork, Problem Solving, Engineering Standards, Technical Writing, Microsoft Office Suite, Google Products","civil engineering, civil 3d, project management, design calculations, quality assurance, quality control, autocad, proficient communication, teamwork, problem solving, engineering standards, technical writing, microsoft office suite, google products","autocad, civil 3d, civil engineering, design calculations, engineering standards, google products, microsoft office suite, problem solving, proficient communication, project management, quality assurance, quality control, teamwork, technical writing"
Licensed Civil Engineer - Data Center (Remote),Olsson,"Lincoln, NE",https://www.linkedin.com/jobs/view/licensed-civil-engineer-data-center-remote-at-olsson-3784205416,2023-12-17,Lincoln,United States,Mid senior,Remote,"Company Description
We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As a Licensed Civil Engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Qualifications
You are passionate about:
Working collaboratively with others
Having ownership in the work you do
Using your talents to positively affect communities
Solving problems
Providing excellence in client service
You bring to the team:
Strong communication skills
Ability to contribute and work well on a team
Bachelor's Degree in civil engineering
At least 6 years of related civil engineering experience
Proficient in Civil 3D software
Must be a registered professional engineer
Additional Information
Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
Engage in work that has a positive impact in communities
Receive an excellent 401(k) match
Participate in a wellness program promoting balanced lifestyles
Benefit from a bonus system that rewards performance
Have the possibility for flexible work arrangements
Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
Show more
Show less","Civil 3D, Microsoft Office Suite, Engineering, Scheduling, Problem Solving, AutoCAD, Design, Leadership, Communication, Project Management, Staad Pro, Budgeting, Project Planning, Project Execution, Bridge Design, Civil Site Design, Survey, GIS, CAD, Management","civil 3d, microsoft office suite, engineering, scheduling, problem solving, autocad, design, leadership, communication, project management, staad pro, budgeting, project planning, project execution, bridge design, civil site design, survey, gis, cad, management","autocad, bridge design, budgeting, cad, civil 3d, civil site design, communication, design, engineering, gis, leadership, management, microsoft office suite, problem solving, project execution, project management, project planning, scheduling, staad pro, survey"
Senior Data Quality Engineer,The Hartford,"Hartford, CT",https://www.linkedin.com/jobs/view/senior-data-quality-engineer-at-the-hartford-3758743031,2023-12-17,Wethersfield,United States,Mid senior,Onsite,"IT Quality Analyst - QI08EE
Sr Analyst SDET - QI07DE
We’re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. Working here means having every opportunity to achieve your goals – and to help others accomplish theirs, too. Join our team as we help shape the future.
Group Benefits – Enterprise Data Services is undergoing a transformation to Cloud and we are looking for a Senior Data Quality Engineer to bring business focus to QA practices. As a Quality Engineer, you will play a pivotal role to establish frameworks for data warehouse & data lake testing, automated testing, Service virtualization, Enterprise data and data quality assurance in addition to helping the organization move towards Continuous testing practices. The individual will work with multiple vendors, IT leaders, and business teams to align automation priorities with IT strategies. Champion the usage and adoption of automated testing and related tools (in-house and new external tools) across the organization. Help defining quality engineering standards, best practices, provide technical recommendations to continuously improve testing efficiencies, mentor the scrum QA teams to adopt defined practices, and report KPIs. Partner with the Scrum Masters, Data Engineers and business leads to increase test coverage and optimize automated test executions time to achieve faster release readiness, support post implementation reviews, driving for root cause to improve test processes and key quality metrics.
Responsibilities:
Accountable for overall test planning and test execution of data & ETL pipelines and downstream reporting
Thorough understanding of DevOps & DataOps practices and make advancements to shift left the QA pipelines in overall CICD process.
Contribute to the quality engineering activities throughout the SDLC phases - requirements, design, development, and testing.
Thorough understanding Data masking, Test data management & Environment engineering practices.
Advanced knowledge of SQL and hands on experience with various on-prem and on-cloud databases like Oracle, Postgres, Redshift & Snowflake etc.
Lead initiatives for Performance testing and Security testing to build quality into the data products, reports, dashboards & APIs.
Experience with testing of reports and dashboards on standard toolsets like ThoughtSpot, Tableau, Micro-Strategy etc.
Identify industry accepted tools and technology for implementation that align with business goals, application landscape and cloud first approach.
Develop automated Test Data Management capabilities across the program ecosystem.
Provide technical leadership to Scrum QA engineers to adopt shift left approach in testing.
Design, develop, review, and maintain test artifacts – test cases, frameworks, code library, common capabilities, etc. adhering to the quality engineering best practices and meet expected quality standards.
Be passionate about new technologies and help bring in outside- in view to teams on the ground to drive transformation.
Evaluate efficiency of existing QA practice and recommend for improvements.
Display critical thinking skills by identifying and resolving risks and issues in proactive manner.
Qualifications:
Must be authorized to work in the U.S.
BS in Computer Science or equivalent experience
5+ years of experience in Quality Engineering
3+ years of experience in leading Quality engineering teams
Knowledge on any Cloud tech stack
Advanced knowledge of SQL as it pertains to data & analytics and reporting
Experience with any scripting or programing language – Python, JavaScript etc.
Experience with Test automation & DevOps tools
Strong experience in risk-based approach for implementing End to End test strategies
Knowledge of Agile Scrum/SAFE methodology
Effectively use collaboration tools like Rally, Jira etc.
Nice to Have:
Experience with AppScan, Checkmarx, CA DevTest
Experience with any of the reporting tools –Tableau, Business Objects, Micro-Strategy
Insurance & Financial services domain knowledge
Knowledge of AWS cloud technology – Code Build, Code Pipeline, Containers
Additional Details:
Must be Authorized to work in the United States
The Hartford is proud to offer a hybrid work location model that is designed to support flexibility.
This partial remote position requires in office presence Tuesday, Wednesday & Thursday with remote work flexibility Monday, and Friday.
Office Locations include Hartford, CT, San Antonio, TX, Lake Mary, FL, Phoenix, AZ, Aurora, IL, New York City, NY and Danbury, CT
Compensation
The listed annualized base pay range is primarily based on analysis of similar positions in the external market. Actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. The base pay is just one component of The Hartford’s total compensation package for employees. Other rewards may include short-term or annual bonuses, long-term incentives, and on-the-spot recognition. The annualized base pay range for this role is:
$98,800 - $148,200
The posted salary range reflects our ability to hire at different position titles and levels depending on background and experience.
Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age
About Us | Culture & Employee Insights | Diversity, Equity and Inclusion | Benefits
Show more
Show less","Data Quality Engineer, Cloud, Data Warehouse Testing, Data Lake Testing, Automated Testing, Service Virtualization, Enterprise Data, Data Quality Assurance, Continuous Testing, DevOps, DataOps, SDLC, Data Masking, Test Data Management, Environment Engineering, SQL, Oracle, Postgres, Redshift, Snowflake, Performance Testing, Security Testing, ThoughtSpot, Tableau, MicroStrategy, Python, JavaScript, Rally, Jira, AppScan, Checkmarx, CA DevTest, AWS, Code Build, Code Pipeline, Containers, Agile Scrum, SAFE methodology","data quality engineer, cloud, data warehouse testing, data lake testing, automated testing, service virtualization, enterprise data, data quality assurance, continuous testing, devops, dataops, sdlc, data masking, test data management, environment engineering, sql, oracle, postgres, redshift, snowflake, performance testing, security testing, thoughtspot, tableau, microstrategy, python, javascript, rally, jira, appscan, checkmarx, ca devtest, aws, code build, code pipeline, containers, agile scrum, safe methodology","agile scrum, appscan, automated testing, aws, ca devtest, checkmarx, cloud, code build, code pipeline, containers, continuous testing, data lake testing, data masking, data quality assurance, data quality engineer, data warehouse testing, dataops, devops, enterprise data, environment engineering, javascript, jira, microstrategy, oracle, performance testing, postgres, python, rally, redshift, safe methodology, sdlc, security testing, service virtualization, snowflake, sql, tableau, test data management, thoughtspot"
Data Science Data Engineer,ForMotiv,"Boston, MA",https://www.linkedin.com/jobs/view/data-science-data-engineer-at-formotiv%E2%80%8B-3648724030,2023-12-17,Nahant,United States,Mid senior,Onsite,"We’re looking for someone to join our data science team as a data engineer. We’re a
startup (albeit 5 years old) and this is a hands-on position that bridges the data science
and engineering teams. We’re actively creating turnkey behavioral data science
products, and looking for a data engineer to own and drive data pipeline and integration
efforts. This is a rare opportunity to get in on the ground-floor of a rapidly growing
international startup. As our data science engineer, you will report to the CDO and work
directly with the product, technology and data science teams.
Our tech stack runs entirely within AWS (and AWS Data experience is required), and
leverages serverless, container-based facilities to create a highly performant and highly
reliable service-based architecture.
Responsibilities
Build, support, optimize, monitor and integrate ForMotiv’s python and data infrastructure technologies with the AWS infrastructure
Design, implement and maintain common data plumbing services, such as data acquisition, data processing and data storage using tools such as Lake Formation, SageMaker, Glue, RedShift, S3, SQS, Airflow, EMR, Athena and more.
Assist data scientists and engineers in automating the movement and processing of data in a real-time and batch Machine Learning environment.
Work with our DevOps team to create and maintain monitoring and alerting for traditional DevOps data services and for Machine Learning models and pipelines.
Qualifications
4 year computer science or related degree (highly recommended, but may be waived with relevant experience)
3+ years of experience designing high-volume, high-speed data storage, distributed processing and data analytics systems working primarily with cloud technologies at a reputable company with references
Strong mathematics and problem-solving skills
3+ years of AWS data automation experience using tools such as Python,Airflow, Spark, Hadoop/EMR, Sagemaker, S3, Glue, etc.
Familiarity with BI tools, data structures and general data visualization.
Experience with software development processes and tools such as Scrum, Kanban, Git and Jira.
Python experience
Expertise in interacting with multiple data storage types and products, such as S3,Postgres, Mariadb, MongoDB, Redshift, Snowflake, HDFS, etc.
Familiarity with mlops concerns, including monitoring and alerting tools such as datadog,Logstash, CloudWatch, Prometheus, Grafana and similar.
Self-motivated, proactive, organized and a willingness to learn (and be taught)
You’re comfortable in a fast-paced, unstructured environment
Eagerness to wear multiple hats and lend a hand to your teammates when called upon
Perks
Stock options with vested interest in growing the business
Ground floor access to venture-backed technology start-up with global presence
The chance to contribute to our growing culture and join a winning team
Grow and advance in any number of potential roles including management
The 5 Cultural Characteristics we look for
Curiosity: You look at each day as an opportunity to learn something new, not as an opportunity to prove what you already know.
Work ethic: You look at each day as an opportunity to get better for yourself and your team
Empathy: You care how others feel around you, and you have the capacity to understand their points of view.
Teachable: You know your strengths and weaknesses and you aren’t afraid to ask for help; everybody you work with is smarter than you about something; willing to learn from everybody and willing to accept that you can always improve the way YOU work.
Integrity: You have the judgment to do the right thing even when it’s not in your own self-interest
Show more
Show less","AWS, Python, Data pipeline, Data integration, Data acquisition, Data processing, Data storage, Data analytics, Serverless, Containerbased facilities, Airflow, EMR, Athena, S3, SQS, Redshift, SageMaker, Glue, Spark, Hadoop, Postgres, Mariadb, MongoDB, Snowflake, HDFS, Datadog, Logstash, CloudWatch, Prometheus, Grafana, Scrum, Kanban, Git, Jira","aws, python, data pipeline, data integration, data acquisition, data processing, data storage, data analytics, serverless, containerbased facilities, airflow, emr, athena, s3, sqs, redshift, sagemaker, glue, spark, hadoop, postgres, mariadb, mongodb, snowflake, hdfs, datadog, logstash, cloudwatch, prometheus, grafana, scrum, kanban, git, jira","airflow, athena, aws, cloudwatch, containerbased facilities, data acquisition, data integration, data pipeline, data processing, data storage, dataanalytics, datadog, emr, git, glue, grafana, hadoop, hdfs, jira, kanban, logstash, mariadb, mongodb, postgres, prometheus, python, redshift, s3, sagemaker, scrum, serverless, snowflake, spark, sqs"
"Data Engineer (AWS, Python, SQL)",Adroit Software Inc.,"Boston, MA",https://www.linkedin.com/jobs/view/data-engineer-aws-python-sql-at-adroit-software-inc-3667477815,2023-12-17,Nahant,United States,Mid senior,Onsite,"For a financial client we need Data Engineer (AWS, Python, SQL). This position is based in Boston, MA and Durham, NC . We are Primarily looking for W2 Candidates and not looking for Third Party Candidates.
The Expertise And Skills You Bring
Has Bachelor's or Primary's Degree in a technology related field (e.g. Engineering, Computer Science, etc.).
5+ years of experience in implementing data solutions in data analytics space
1-2 years of experience in developing data applications in Cloud (AWS)
Extensive experience in Object Oriented Programming (Java, Scala, Python), data movement technologies (ETL/ELT), Messaging Technologies (ActiveMQ, Kafka), Relational and NoSQL databases (Cassandra, Spark, Elastic search, Graph database), Hadoop ecosystem (Hadoop, Hive, Sqoop, Flume, HBase), API and in-memory technologies.
Strong knowledge of developing highly scalable distributed systems using Open source technologies
Experience in Java, Python, Unix scripting or related programming languages.
Understanding of Machine Learning is preferable.
Experience with DevOps, Continuous Integration and Continuous Delivery (Maven, Jenkins, Stash, Ansible, Chef, Docker)
Solid experience in Agile methodologies (Kanban and SCRUM)
Cloud experience (AWS preferred)
You have strong technical design and analysis skill
You the ability to deal with ambiguity and work in fast paced environment
Your deep experience supporting critically important applications quickly
You have excellent communication skills, both through written and verbal channels
You have excellent collaboration skills to work with multiple teams in the organization
Your ability to understand and adapt to changing business priorities and technology advancements
Knowledge and technology trends in implementing of data ecosystem
Strategizing and critical problem-solving skills
Show more
Show less","AWS, Python, SQL, Java, Scala, ETL/ELT, ActiveMQ, Kafka, Cassandra, Spark, Elastic search, Graph database, Hadoop, Hive, Sqoop, Flume, HBase, API, Inmemory technologies, Unix scripting, Machine Learning, DevOps, Continuous Integration, Continuous Delivery, Maven, Jenkins, Stash, Ansible, Chef, Docker, Agile methodologies, Kanban, SCRUM, Technical design, Analysis, Ambiguity, Fast paced environment, Critical applications, Communication, Collaboration, Business priorities, Technology advancements, Data ecosystem, Strategizing, Problemsolving","aws, python, sql, java, scala, etlelt, activemq, kafka, cassandra, spark, elastic search, graph database, hadoop, hive, sqoop, flume, hbase, api, inmemory technologies, unix scripting, machine learning, devops, continuous integration, continuous delivery, maven, jenkins, stash, ansible, chef, docker, agile methodologies, kanban, scrum, technical design, analysis, ambiguity, fast paced environment, critical applications, communication, collaboration, business priorities, technology advancements, data ecosystem, strategizing, problemsolving","activemq, agile methodologies, ambiguity, analysis, ansible, api, aws, business priorities, cassandra, chef, collaboration, communication, continuous delivery, continuous integration, critical applications, data ecosystem, devops, docker, elastic search, etlelt, fast paced environment, flume, graph database, hadoop, hbase, hive, inmemory technologies, java, jenkins, kafka, kanban, machine learning, maven, problemsolving, python, scala, scrum, spark, sql, sqoop, stash, strategizing, technical design, technology advancements, unix scripting"
Sr Data Engineer (Python/GCP),BGSF,Nashville Metropolitan Area,https://www.linkedin.com/jobs/view/sr-data-engineer-python-gcp-at-bgsf-3783324403,2023-12-17,Excelsior Springs,United States,Mid senior,Onsite,"Zycron has joined with a large healthcare client in search of a Data Engineer.
No subcontractors or agencies please
Hybrid position- Must be in Nashville, Tennesse area or willing be willing to relocate to Nashville, TN area.
If not, local travel will be required.
The Data Engineer will be responsible for designing and implementing data pipelines, data transformation processes, and data integration solutions. This role plays a crucial part in ensuring the availability and accessibility of high-quality data for analytical and business intelligence purposes.
Essential Duties and Responsibilities:
Data Pipeline Development: Design, build, and maintain scalable data pipelines to collect, process, and transform data from various sources.
Data Integration: Develop and maintain data integration solutions that facilitate the flow of data between different systems and platforms.
Data Transformation: Implement ETL (Extract, Transform, Load) processes to ensure data accuracy and consistency.
Data Quality: Monitor and improve data quality by implementing data validation and cleansing processes.
Database Management: Assist in managing and optimizing database systems for performance and scalability.
Collaboration: Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.
Documentation: Maintain documentation for data pipelines, processes, and best practices.
Minimum Job Requirements
Bachelor's degree in Computer Science, Information Technology, or a related field.
Proven experience as a Data Engineer or similar role.
Proficiency in programming languages such as Python, Java, or Scala.
Strong SQL skills.
Experience with data integration tools and technologies.
Knowledge of Big Data frameworks (e.g., Hadoop, Spark) is a plus.
Familiarity with cloud-based data solutions (Google Cloud Platform).
Problem-solving and analytical skills.
Job ID Number: 821929
(Please reference in call or email)
Only candidates with backgrounds who match our client's requested experience will be contacted. Do not take this as a poor reflection on your experience, just a decision for the specific needs of our client's project/job. We look forward to working with you.
Show more
Show less","Data Engineering, Data Pipelines, Data Integration, Data Transformation, ETL, Data Quality, Database Management, Data Validation, Data Cleansing, Cloud Computing, Python, Java, Scala, SQL, Hadoop, Spark, Google Cloud Platform","data engineering, data pipelines, data integration, data transformation, etl, data quality, database management, data validation, data cleansing, cloud computing, python, java, scala, sql, hadoop, spark, google cloud platform","cloud computing, data engineering, data integration, data quality, data transformation, data validation, database management, datacleaning, datapipeline, etl, google cloud platform, hadoop, java, python, scala, spark, sql"
Senior Data Analyst,FUJIFILM Diosynth Biotechnologies,"College Station, TX",https://www.linkedin.com/jobs/view/senior-data-analyst-at-fujifilm-diosynth-biotechnologies-3773361365,2023-12-17,College Park,United States,Mid senior,Onsite,"The work we do at FDB has never been more important—and we are looking for talented candidates to join us. We are growing our locations, our capabilities, and our teams, and looking for passionate, mission-driven people like you who want to make a real difference in people’s lives. Join FDB and help create the next vaccine, cure, or gene therapy in partnership with some of the most innovative biopharma companies across the globe. We are proud to cultivate a culture that will fuel your passion, energy and drive - what FDB call Genki.
College Station, Texas may be a small, university town, but the lively cultural scene and local amenities make it a great place for families as well as those who want the ease of small-town life and the convenience of living close to the vibrant pulse of big cities. Eighty-seven percent of Texas' population lives within a 180-mile radius, so we are in the center of it all in Texas. And our site is nestled in the hub of innovation, representing a source of pride for the area.
Summary:
Works under the supervision of the Data Analytics Manager or other data governance leadership appropriate for the scope of work. This role is responsible for data analytics and additional support of performance measures, ongoing measurement, data collection, reporting, data visualizations and information dissemination. Responsible for structuring the strategic design and maintenance of business intelligence applications. Identifies, researches, and resolves technical problems. Ensures that the use of data and business intelligence applications enhances business operations and decision making capabilities. Contributes to complex aspects of a project. Work is generally independent and collaborative in nature. Engages in data exploration exercises with a variety of complex business intelligence tools, requiring knowledge of relational database structures. Collaborates with other departments.
External US
Essential Functions:
Develop and execute on data analytics projects as assigned by the Data Analytics Manager
Utilize company resources across departments and sites to curate data
Analyze data for trends and patterns
Generate reports and dashboards and transfer to super users
Train and provide technical support to super users
Coordinate with other departments and sites to develop and implement new data collection models
Perform data profiling to identify anomalies
All other duties as assigned
Required Skills & Abilities:
Create data models using tools using software applications such as Python, SAS, Alteryx, or equivalents for visualization in Tableau, Smartsheet, Power BI, or equivalent
Translate data into dynamic dashboards
Knowledge of databases such as Microsoft SQL, MySQL, PostgreSQL, or equivalent
Working understanding of relational database design
Working understanding of statistics and mathematical models
Familiarity with data collection software and protocol
Experience integrating new software and programs to data services
Ability to prioritize multiple projects while still achieving deadlines
Excellent analytical and forecasting ability
Strong written and verbal communication skills
Proficient understanding of current data protection and privacy laws
Working Conditions & Physical Requirements:
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is regularly required to:
Experience prolonged sitting, some bending, stooping, and stretching.
Use hand-eye coordination and manual dexterity sufficient to operate a keyboard, photocopier, telephone, calculator, and other office equipment is required.
Minimum Qualifications:
Bachelor’s degree preferably in Data or Computer Science, Engineering, Mathematics, Statistics, or other scientific field with 4 years of experience
Preferred Qualifications:
Advanced degree and/or certifications in data science, analytics, or computer science
Join us! FDB is advancing tomorrow’s medicine, impassioning employees to chase the impossible and continually expand their potential. We are a company of emboldened goal seekers – driven by an innate desire to better ourselves, our families, our workplace, our company, our community and the world at large.
We are an equal opportunity and affirmative action employer.  All qualified applicants will receive consideration without regard to race, color, national origin, sex, gender identity, sexual orientation, religion, disability, protected veteran status or any other characteristic protected by applicable federal, state or local law. If an accommodation to the application process is needed, please email FDBTHR@fujifilm.com or call 979-431-3500.
To all agencies: Please, no phone calls or emails to any employee of FUJIFILM about this requisition. All resumes submitted by search firms/employment agencies to any employee at FUJIFILM via-email, the internet or in any form and/or method will be deemed the sole property of FUJIFILM, unless such search firms/employment agencies were engaged by FUJIFILM for this requisition and a valid agreement with FUJIFILM is in place. In the event a candidate who was submitted outside of the FUJIFILM agency engagement process is hired, no fee or payment of any kind will be paid.
Show more
Show less","Python, SAS, Alteryx, SQL, MySQL, PostgreSQL, Tableau, Smartsheet, Power BI, Data analytics, Data collection, Data visualization, Data modeling, Data profiling, Data protection, Machine learning, Statistics, Mathematical models","python, sas, alteryx, sql, mysql, postgresql, tableau, smartsheet, power bi, data analytics, data collection, data visualization, data modeling, data profiling, data protection, machine learning, statistics, mathematical models","alteryx, data collection, data profiling, data protection, dataanalytics, datamodeling, machine learning, mathematical models, mysql, postgresql, powerbi, python, sas, smartsheet, sql, statistics, tableau, visualization"
Lead Data Analyst,Texas A&M University,"College Station, TX",https://www.linkedin.com/jobs/view/lead-data-analyst-at-texas-a-m-university-3784937672,2023-12-17,College Park,United States,Mid senior,Onsite,"Job Title
Lead Data Analyst
Agency
Texas A&M University
Department
VP for Planning, Assessment & Strategy
Proposed Minimum Salary
Commensurate
Job Location
College Station, Texas
Job Type
Staff
Job Description
Our Commitment
Texas A&M University is committed to enriching the learning and working environment by promoting a culture that respects all perspectives, talents & identities. Embracing varying opinions and perspectives strengthens our core values which are: Respect, Excellence, Leadership, Loyalty, Integrity, and Selfless Service.
Who We Are
Planning, Assessment, and Strategy provides leadership related to the strategic and operational plans and priorities of the University, as well as assessing, recommending, and advising on university structures and communication strategies. Additionally, the group advises on access, affordability, continuous improvement, and growth initiatives.
What We Want
The Lead Data Analyst, under general direction, serves as technical lead and lead programmer for a variety of data processes. Provides technical oversight for the application of and compliance with technical standards. May coordinate the technical activities of a systems analysis and development team. Completes reports and summaries for management and users including data requests, data sets for State and Federal agencies, data sets and analyses for university administration and committees, project status reports, problem reports, and progress summaries. Documents all developed processes and reporting methodologies.
What you need to know
Compensation Will Be Commensurate To Selected Hire’s Experience
A cover letter and resume are strongly recommended
Required Education & Experience:
Bachelor’s degree or equivalent combination of education and experience
Five years with SQL scripting or similar programming background
Required Knowledge, Skills, and Abilities:
Ability to write complex scripts in SQL to load recurring data extracts into databases, and to manipulate and merge data from various sources
Requires heavy use of SQL tools for handling data
Ability to interpret data requests, determine most appropriate tool for writing the report, and prepare end product for customer
Ability to create reports based on user specifications
Ability to multi-task and work cooperatively with others
Preferred Qualifications:
Master’s degree in information technology, Business Analytics, Data Science, Programming, or similar field
Eight years experience in information technology or institutional research
Experience with Informatica, Tableau, and Cognos a plus
Responsibilities:
Technical Lead on Data Project (Student Reports/Faculty Reports/Texas Higher Education Coordinating Board Reports) - Act independently as a technical lead on various data projects. Analyze, review, document and develop complex IT processes and data structures to meet Reporting needs for the University administration as well as for State and Federal agencies. Acquire, load, and merge data from various campus sources to produce required State and Federal reports. Run processes during key data acquisition dates, make modifications to scripts, and document scripts as data profile and requirements change. Work with University administration and committees to provide data needed for strategic initiatives.
Coordination with Others to Implement Systems - Work with end users or agencies to make sure that the systems developed meet the requirements of the processes. Meet with data owners to make sure reports generated by processes accurately reflect source data. Also, work with data owners to find solutions to data quality issues and to monitor progress of changes. Maintains current knowledge of State and Federal requirements for reporting to ensure reports generated by the department are correct.
Ad Hoc Reporting - Assist in the response to requests for special data from both within the University and outside the university. Determine appropriate tool and data source for the request.
Why Texas A&M University?
We are a prestigious university with strong traditions, Core Values, and a community of caring and collaboration.  Amenities associated with a major university, such as sporting and cultural events, state-of-the-art recreation facilities, the Bush Library and Museum, and much more await you.  Experience all that a big city has to offer but with a reasonable cost-of-living and no long commutes.
Health, dental, vision, life and long-term disability insurance with Texas A&M contributing to employee health and basic life premiums
12-15 days of annual paid holidays
Up to eight hours of paid sick leave and at least eight hours of paid vacation each month
Automatically enrollment in the Teacher Retirement System of Texas
Health and Wellness: Free exercise programs and release time
Professional Development: All employees have access to free LinkedIn Learning training, webinars, and limited financial support to attend conferences, workshops, and more
Employee Tuition Assistance and Educational Release time for completing a degree while a Texas A&M employee
Instructions to Applicants: Applications received by the School of Nursing must have all job application data entered and must include the following attachments: resume, cover letter and list of references. Failure to provide all job application data and attachments could result in an invalid submission and a rejected application. We encourage all applicants to upload a resume or use a LinkedIn profile to pre-populate the online application.
All positions are security-sensitive. Applicants are subject to a criminal history investigation, and employment is contingent upon the institution’s verification of credentials and/or other information required by the institution’s procedures, including the completion of the criminal history check.
Equal Opportunity/Affirmative Action/Veterans/Disability Employer.
Show more
Show less","SQL, SQL scripting, Data extraction, Data manipulation, Data merging, Data analysis, Reporting, Data visualization, Informatica, Tableau, Cognos, Data quality management, Data governance, Data project management, Systems analysis and design, Project management, Communication, Teamwork, Problemsolving, Critical thinking, Attention to detail, Accuracy, Time management, Multitasking, Flexibility, Adaptability, Initiative, Selfmotivation, Leadership, Mentoring, Training","sql, sql scripting, data extraction, data manipulation, data merging, data analysis, reporting, data visualization, informatica, tableau, cognos, data quality management, data governance, data project management, systems analysis and design, project management, communication, teamwork, problemsolving, critical thinking, attention to detail, accuracy, time management, multitasking, flexibility, adaptability, initiative, selfmotivation, leadership, mentoring, training","accuracy, adaptability, attention to detail, cognos, communication, critical thinking, data extraction, data governance, data manipulation, data merging, data project management, data quality management, dataanalytics, flexibility, informatica, initiative, leadership, mentoring, multitasking, problemsolving, project management, reporting, selfmotivation, sql, sql scripting, systems analysis and design, tableau, teamwork, time management, training, visualization"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Everett, WA",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783187247,2023-12-17,Arlington Heights,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Everett-DataResearchAn.019
Show more
Show less","Generative AI, Python, JavaScript, JSON, R, OOP, Git, Research, Data Analysis, Technical Writing, Communication, Problem Solving, Time Management, Project Management, Data Science, Education, Technology, Remote Work, Independent Contractor, EdTech","generative ai, python, javascript, json, r, oop, git, research, data analysis, technical writing, communication, problem solving, time management, project management, data science, education, technology, remote work, independent contractor, edtech","communication, data science, dataanalytics, edtech, education, generative ai, git, independent contractor, javascript, json, oop, problem solving, project management, python, r, remote work, research, technical writing, technology, time management"
"Senior Software Engineer(Object Oriented ABAP,CDS Views,HANA Views,SQL,SAP Data Services)",Cube Hub Inc.,"Johnston, IA",https://www.linkedin.com/jobs/view/senior-software-engineer-object-oriented-abap-cds-views-hana-views-sql-sap-data-services-at-cube-hub-inc-3673272322,2023-12-17,West Des Moines,United States,Mid senior,Onsite,"Description
Senior Software Engineer
4.5 months contract with possibility to extend;
Remote/hybrid
John Deere Financial is seeking an
SAP Software Engineer
to deliver SAP technical solutions by writing code and developing CDS Views, as well as performing unit testing. The position will be working on the Data Services team assisting in delivery of solutions for the John Deere Wholesale Financial Services function of the Order Management program.
The position requires
expert knowledge
and
hands on experience working in SAP Finance scenarios and SAP BP Model
. The position will also be a member of an Agile delivery team and will participate in Agile ceremonies such as daily stand-up, sprint planning, demo, retro, etc.
Required technical skills include (3 years+):
Object Oriented ABAP
CDS Views
HANA Views
SAP Data Services
SQL and Query knowledge
Additional Desired Skills Include
Knowledge on SAP Banking Services
Palantir and Data Migration experience
Experience is primary in this role, but a 4-year degree will also be accepted with some experience (no recent grads)
Show more
Show less","SAP, ABAP, CDS Views, HANA Views, SAP Data Services, SQL, Agile, Palantir, Data Migration","sap, abap, cds views, hana views, sap data services, sql, agile, palantir, data migration","abap, agile, cds views, data migration, hana views, palantir, sap, sap data services, sql"
Database Engineer,A2Zxperts,"Des Moines, IA",https://www.linkedin.com/jobs/view/database-engineer-at-a2zxperts-3652277541,2023-12-17,West Des Moines,United States,Mid senior,Onsite,"Job Title:
Database Engineer
Job Location:
Des Moines, IA
Job Type:
Hybrid
Our direct client is looking for an experienced database administrator who will be responsible for leading the efforts to ensure the performance, availability, and security of Oracle, MySQL, and MS SQL databases. This position requires someone to work in Des Moines.
This position will be the lead DBA and lead all database upgrades and projects for Oracle and SQL. This position's goal is the deployment of sustainable and modern database technologies, on prem and in cloud based. The candidate will have full control over the database technologies used as well as creating a future roadmap for all database technologies at the agency. This position will reside on the systems engineering team with a dotted line to the application architect and development teams. Day-to-day 50% operations and 50% planning and designing future state. This role will also handle orchestrating upgrades, backups, and provisioning of database instances. You will also work in tandem with the other teams in the information technology division, preparing documentations and specifications as needed.
Requirements
Bachelor's degree in Computer Engineering or Computer Science or equivalent work experience - Required 5 Years
Familiarity with Oracle database design, coding, and documentation. - Required 5 Years
Knowledge of database backup procedures, recovery systems, and SQL. - Required 5 Years
Knowledge of programming languages and API. - Required 5 Years
Excellent Communication And Problem-solving Skills. - Required 5 Years
Proficient installing, configuring, maintaining, and upgrading Oracle and SQL databases - Required 5 Years
Proficient installing, configuring, maintaining, and upgrading Linux based database servers - Required 5 Years
Altering storage structures to meet the evolving needs of the company. - Required 5 Years
Setting up database user accounts - Required 5 Years
Experience interacting with users and customers regularly - Required 5 Years
Training users on how to access the information in the database. - Required 5 Years
Finding and debugging malfunctioning programs affecting the database integrity. - Required 5 Years
Experience with monitoring Database capacity and performance - Required 5 Years
Experience with applying security patches and subsequent testing - Required 5 Years
Ability to refresh or clone databases from production to development - Required 5 Years
Experience using stored Procedures, Functions, Triggers, and packages - Required 5 Years
Experience provisioning Microsoft SQL server instances - Required 5 Years
Experience with Microsoft SQL server clusters - Required 5 Years
Experience with AppDynamics or other performance monitoring tool - Required 5 Years
Experience provisioning MySQL instances - Desired 5 Years
Experience with using a SIEM / Log aggregator - Desired 5 Years
Experience backing up and cloning databases using NetApp Snap Center - Desired 5 Years
Understanding of MySQL's underlying storage engines - Desired 5 Years
Experience writing XML scripts per the customer requirements - Desired 5 Years
Familiarity with other SQL/NoSQL databases such as PostgreSQL, MongoDB, - Desired 5 Years
Proficient understanding of code versioning tools such as {{Git / Mercurial / SVN}} - Desired 5 Years
Experience with MariaDB and associated technologies - Desired 5 Years
Experience with Azure SQL always on - Desired 5 Years
Experience DB2 database technologies - Desired 5 Years
Experience working in a DevOps environment - install and configure Sonar, Git, Jenkins - Desired 5 Years
Experience administrating Microsoft Windows Servers - Desired 5 Year
Experience with Alfresco Enterprise Content Management solutions - Desired 5 Years
Experience working with Atlassian products (JIRA, Confluence, Bit Bucket) - Desired 5 years of experience.
working with Business Objects (SAP) - Desired 5 Years
Note: ""Proficient installing, configuring, maintaining and upgrading Linux based database servers - Required 5 Years""
Also, when screening, we need to ensure that Oracle and SQL experience is emphasized.
For more details, please reach out to pd@a2zxperts.com
Show more
Show less","Oracle, MySQL, SQL, SQL Server, Linux, AppDynamics, PostgreSQL, MongoDB, Git, MariaDB, Azure SQL, DB2, DevOps, Sonar, Jenkins, Windows Server, Alfresco, Jira, Confluence, Bitbucket, Business Objects","oracle, mysql, sql, sql server, linux, appdynamics, postgresql, mongodb, git, mariadb, azure sql, db2, devops, sonar, jenkins, windows server, alfresco, jira, confluence, bitbucket, business objects","alfresco, appdynamics, azure sql, bitbucket, business objects, confluence, db2, devops, git, jenkins, jira, linux, mariadb, mongodb, mysql, oracle, postgresql, sonar, sql, sql server, windows server"
Sr. Data Analyst,Viant Technology,"Irvine, CA",https://www.linkedin.com/jobs/view/sr-data-analyst-at-viant-technology-3788853177,2023-12-17,Orange,United States,Associate,Hybrid,"What You’ll Do
Come help us build Viant’s industry leading advanced measurement capabilities. Help marketers understand the impact of their digital media spend through comprehensive, yet simple to read, dashboards. Viant’s Advanced Reporting suite of reports is all about bridging the gap between back-end technical data know-how & business application. Be a part of the process end-to-end; from concept, to scoping, developing, and release of next generation closed-loop measurement offerings.
THE DAY-TO-DAY
Work in synergy with the Product Management team to develop and automate measurement offerings, including the utilization of data science methodologies to enhance insights for pre, mid, and post-campaign deliverables
Write complex SQL queries to build statements, functions, and workflows to support application development, reporting and data extraction processes
Actively refine and optimize existing ETL processes, applying advanced techniques to enhance efficiency and reduce computational & storage costs
Employ statistical methods for identifying, explaining, and rectifying data discrepancies
Design, develop, test, debug and deploy data workflows using various technologies
Solve complex performance problems, architectural challenges, and production issues
Recommend and implement enhancements in processes, utilizing data-driven insights and modelling techniques to meet evolving business requirements
Build and maintain data infrastructure that powers production dashboards while ensuring system designs meet quality standards
GREAT TO HAVE
B.S. degree in Data Science, Computer Science, Statistics, Mathematics, or data related field.
Professional SQL experience with ability to create and optimize queries
Utilization of GBQ preferred
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Technical writing experience in relevant areas; including queries, reports, and presentations
Ability to solve complex problems, understand intricate data patterns and provide solutions based on data-driven insights
Experience in statistical analysis (such as distributions, statistical testing, regression, etc.) through SQL or Excel
Ability to work in a fast-paced, dynamic environment while juggling multiple projects and priorities
Who We Are
Viant ® (NASDAQ: DSP) is a leading advertising software company that enables marketers to plan, execute and measure omnichannel ad campaigns through a cloud-based platform. Viant’s self-service Demand Side Platform, Adelphic®, powers programmatic advertising across Connected TV, Linear TV, mobile, desktop, audio, gaming and digital out-of-home channels. In 2022, Viant was recognized as a Leader in the DSP category , earned Great Place to Work® certification and Co-Founders Tim and Chris Vanderhook were named EY Entrepreneurs of the Year. To learn more, please visit viantinc.com .
LIFE AT VIANT
Investing in our employee’s professional growth is important to us, but so is investing in their well-being. That’s why Viant was voted one of the best places to work and some of our favorite employee benefits include fully paid health insurance, paid parental leave and unlimited PTO and more.
$75,000 - $100,000 a year
In accordance with California law, the range provided is Viant’s reasonable estimate of the compensation for this role. Final title and compensation for the position will be based on several factors including work experience and education.
Not the right position for you? Check out our other opportunities!
Viant Careers
About Viant
Viant® (NASDAQ: DSP) is a leading people-based advertising technology company that enables marketers to plan, execute and measure omnichannel ad campaigns through a cloud-based platform. Viant’s self-service demand side platform (“DSP”) powers programmatic advertising across Connected TV, Linear TV, mobile, desktop, audio, gaming and digital out-of-home channels. As an organization committed to sustainability, Viant’s Adricity® carbon reduction program helps clients achieve their sustainability goals. In 2023, Viant was recognized by G2 as a Leader in the DSP category and as the Best Software in Marketing & Advertising, earned Great Place to Work® certification, and became a founding member of Ad Net Zero. Viant’s Co-Founders Tim and Chris Vanderhook were also recently named EY Entrepreneurs of the Year.
Based in Irvine, CA, Viant has more than 334 employees in 10 offices around the U.S. To learn more, please visit viantinc.com .
Viant is an equal opportunity employer and makes employment decisions on the basis of merit. Viant prohibits unlawful discrimination against employees or applicants based on race (including traits historically associated with race, such as hair texture and protective hairstyles), religion, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, reproductive health decision making, gender, gender identity, gender expression, age, military status, veteran status, uniformed service member status, sexual orientation, transgender identity, citizenship status, pregnancy, or any other consideration made unlawful by federal, state, or local laws. Viant also prohibits unlawful discrimination based on the perception that anyone has any of those characteristics, or is associated with a person who has or is perceived as having any of those characteristics.
By clicking “Apply for this Job” and providing any information, I accept the Viant California Personnel Privacy Notice.
Show more
Show less","SQL, Data science, Data analysis, Data extraction, ETL, Statistical methods, Data engineering, System design, Datadriven insights, Modelling techniques, GBQ, Python, Java, Agile, Communication, Problemsolving, Analytical skills, Attention to detail, Accuracy, Technical writing, Distributed systems","sql, data science, data analysis, data extraction, etl, statistical methods, data engineering, system design, datadriven insights, modelling techniques, gbq, python, java, agile, communication, problemsolving, analytical skills, attention to detail, accuracy, technical writing, distributed systems","accuracy, agile, analytical skills, attention to detail, communication, data engineering, data extraction, data science, dataanalytics, datadriven insights, distributed systems, etl, gbq, java, modelling techniques, problemsolving, python, sql, statistical methods, system design, technical writing"
Data Integration Solutions Architect,Irvine Technology Corporation,"Irvine, CA",https://www.linkedin.com/jobs/view/data-integration-solutions-architect-at-irvine-technology-corporation-3780049443,2023-12-17,Laguna Beach,United States,Mid senior,Hybrid,"Data Integration Solutions Architect
Data Integration Solutions Architect (Hybrid)
We have an immediate need for a Data Integration Solutions Architect to join a real estate company for a full-time, permanent position that is onsite in Irvine, CA. The ideal candidate can be hands-on in the development of Snowflake Cloud, SQL, and data integration using tools like Boomi or Matillion.
Location: Irvine, CA (Hybrid- 1 day remote, 4 days onsite)
This Job Pays: $175,000 – 211,000 annually with 10% annual bonus
What You Will Do:
Translates business and technical requirements into an architectural blueprint to achieve business objectives, and documents all integration solution architecture design and analysis work.
Oversee the assessment, development, and execution of a solutions architecture for a set of designated business applications or technological components. This entails aligning with integration strategies, capabilities, value streams, prerequisites, and corporate integration guidelines.
Produces architectural design documentation to provide guidance and context for solution development across a spectrum of products, services, projects, and systems, encompassing applications, technologies, processes, and information.
Assess existing and future solutions requirements on integration architecture, ETL, data modeling, data sourcing, and data quality.
Partner with Sr. Director, Enterprise Data Integration to help guide technology modernization strategy.
Perform code reviews and ensure alignment to industry standards.
Manage release deployments and performance testing.
What Gets You The Job:
Bachelor's degree in Computer Science, Information Systems, Engineering or Data Analytics
12+ years of hands-on experience as an Integration Developer, Data Engineer and/or Architect using integration tools such as Matillion, Informatica, Boomi etc.
8+ years proven data modeling skills - must have demonstrable experience with ETL/ELT designs for loading data into the ODS , Dimensions and Facts, designing models for data warehousing and analytics use-cases (e.g. from operational data store to semantic models)
5+ years of Data engineering experience in API integrations
5+ years of experience with Snowflake and other Cloud Data Warehousing / Data Lake
Highly proficient in SQL and python for automation and data manipulations
Capability to conduct performance analysis, troubleshooting and remediation
Please send your resume to
Hannah Xu, Senior Technical Recruiter
for immediate consideration.
Irvine Technology Corporation (ITC)
is a leading provider of technology and staffing solutions for IT, Security, Engineering, and Interactive Design disciplines servicing startups to enterprise clients, nationally. We pride ourselves in the ability to introduce you to our intimate network of business and technology leaders – bringing you opportunity coupled with personal growth, and professional development!
Join us. Let us catapult your career!
Irvine Technology Corporation provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Irvine Technology Corporation complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.
Show more
Show less","Data Integration, Snowflake Cloud, SQL, Boomi, Matillion, ETL, Data Modeling, ODS, Dimensions, Facts, Data Warehousing, Analytics, API Integrations, Cloud Data Warehousing, Data Lake, Python, Performance Analysis, Troubleshooting, Remediation","data integration, snowflake cloud, sql, boomi, matillion, etl, data modeling, ods, dimensions, facts, data warehousing, analytics, api integrations, cloud data warehousing, data lake, python, performance analysis, troubleshooting, remediation","analytics, api integrations, boomi, cloud data warehousing, data integration, data lake, datamodeling, datawarehouse, dimensions, etl, facts, matillion, ods, performance analysis, python, remediation, snowflake cloud, sql, troubleshooting"
Data Analyst 3,Experis,"Mishawaka, IN",https://www.linkedin.com/jobs/view/data-analyst-3-at-experis-3759662315,2023-12-17,Elkhart,United States,Mid senior,Onsite,"Our client is seeking a
Data Analyst
to join their team.
Data Analyst
Duration: 12+ months
Location:
Mishawaka, Indiana (Onsite)
Position Description
Role & Responsibilities:
Primary focus will be to manage all project objectives associated with the Kinaxis Project/Implementation through completion.
Manage and improve inventory performance, measured in Days-of-Supply (DOS) and Inventory COGs, for the Mishawaka site.
Management of purchased raw material, WIP and some finished goods inventories.
Ensure availability of all production critical materials.
Continue the Plan-for-Every Part (PFEP) for all regularly used materials with, among others, the following major elements:
Planning Strategy (i.e. Kanban, MRP)
Optimal safety stock and target inventory levels
Replenishment practices and cost optimized re-order strategies
Integrate and maintain all material planning parameters in SAP ERP-system.
Identify inventory risks, such as obsolescence, expiry and improvement opportunities, including raw material buying and rounding value improvements. Create and drive projects to completion fpr the Materials Management group.
Implement replenishment pull-systems (i.e. Kanban, Heijunka, Vendor Managed Inventory) where possible.
Work closely with the following departments on execution of inventory improvement strategies:
Tactical/Operational and Strategic Procurement
Warehousing
Production Planning
Global Planning
Finance
Regularly report inventory performance and improvement strategies to management and peers, in some cases weekly/monthly, while developing dashboards for communication.
Participate in the monthly Sales, Inventory and Operations Planning meeting.
Become local expert to assist buyers in execution of Good Receipts/Invoice Receipt clearing
Required Qualifications
Bachelor degree or college diploma in Economics, Business Administration or closely related field is required.
Relevant professional certification, such as APICS-CPIM, combined with 3-4 years of related experience (Purchasing, Material Planning, Logistics) could be considered in lieu of certifications.
Experience with SAP or similarly structured major integrated ERP-system a must
Experience with Kinaxis would be
Excellent working knowledge of Microsoft Excel and PowerPoint is required.
Experience with Project and database programs would be a plus
Must be familiar with critical elements of statistical risk analysis, such as Standard Deviations, Service Levels, Trend Analysis, Cumulative Yield Analysis.
If this is a role that interests you and you’d like to learn more, click apply now and a recruiter will be in touch with you to discuss this great opportunity. We look forward to speaking with you!
About ManpowerGroup, Parent Company of:
Manpower, Experis, Talent Solutions, and Jefferson Wells
ManpowerGroup® (NYSE: MAN), the leading global workforce solutions company, helps organizations transform in a fast-changing world of work by sourcing, assessing, developing, and managing the talent that enables them to win. We develop innovative solutions for hundreds of thousands of organizations every year, providing them with skilled talent while finding meaningful, sustainable employment for millions of people across a wide range of industries and skills. Our expert family of brands –
Manpower, Experis, Talent Solutions, and Jefferson Wells
–
creates substantial value for candidates and clients across more than 75 countries and territories and has done so for over 70 years. We are recognized consistently for our diversity - as a best place to work for Women, Inclusion, Equality and Disability and in 2022 ManpowerGroup was named one of the World's Most Ethical Companies for the 13th year - all confirming our position as the brand of choice for in-demand talent.
Show more
Show less","Project Management, Data Analysis, Inventory Management, Materials Planning, Supply Chain Management, SAP ERP, Kinaxis, Microsoft Excel, Microsoft PowerPoint, Kanban, MRP, Heijunka, Vendor Managed Inventory, Warehousing, Production Planning, Global Planning, Finance, Sales, Operations Planning, Statistical Risk Analysis, APICSCPIM","project management, data analysis, inventory management, materials planning, supply chain management, sap erp, kinaxis, microsoft excel, microsoft powerpoint, kanban, mrp, heijunka, vendor managed inventory, warehousing, production planning, global planning, finance, sales, operations planning, statistical risk analysis, apicscpim","apicscpim, dataanalytics, datawarehouse, finance, global planning, heijunka, inventory management, kanban, kinaxis, materials planning, microsoft excel, microsoft powerpoint, mrp, operations planning, production planning, project management, sales, sap erp, statistical risk analysis, supply chain management, vendor managed inventory"
Data Engineer - Scala(U.S. remote),Railroad19,"Tulsa, OK",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782292599,2023-12-17,Sun Valley,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, Restful API, AWS, EMR, S3, SQL, NoSQL","scala, spark, restful api, aws, emr, s3, sql, nosql","aws, emr, nosql, restful api, s3, scala, spark, sql"
"Data Conversion Developer, Senior Associate",PwC,"Tulsa, OK",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749939439,2023-12-17,Sun Valley,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Azure Data Engineer Associate, Databricks Certified Data Engineer Associate, Maximo, PowerPlant, SQL, Java, Python, PySpark, Scala, Data conversion, Data Integration, ETL, Data cleansing, Automation Scripts, Web services (SOAP RESTful APIs), XML, JSON, DataBricks, AWS Glue, SSIS, Oracle, Microsoft SQL Server, IBM DB2","azure data engineer associate, databricks certified data engineer associate, maximo, powerplant, sql, java, python, pyspark, scala, data conversion, data integration, etl, data cleansing, automation scripts, web services soap restful apis, xml, json, databricks, aws glue, ssis, oracle, microsoft sql server, ibm db2","automation scripts, aws glue, azure data engineer associate, data conversion, data integration, databricks, databricks certified data engineer associate, datacleaning, etl, ibm db2, java, json, maximo, microsoft sql server, oracle, powerplant, python, scala, spark, sql, ssis, web services soap restful apis, xml"
Senior Cloud Data Engineer,BDO USA,"Tulsa, OK",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765471244,2023-12-17,Sun Valley,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, SQL, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Azure, AWS, C#, Python, Java, Scala, Tabular modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch and/or streaming data ingestion into a data lake, AI Algorithms/Machine Learning, Automation tools, Computer Vision based AI technologies, DataOps, Data Pipeline, Data Modeling, SSIS, SSAS, SSRS, PySpark, dbt, Terraform, Bicep, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, artificial intelligence, application development, sql, data warehousing, data modeling, semantic model definition, star schema construction, azure, aws, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch andor streaming data ingestion into a data lake, ai algorithmsmachine learning, automation tools, computer vision based ai technologies, dataops, data pipeline, data modeling, ssis, ssas, ssrs, pyspark, dbt, terraform, bicep, purview, delta, pandas, spark sql","ai algorithmsmachine learning, application development, artificial intelligence, automation tools, aws, azure, azure analysis services, batch andor streaming data ingestion into a data lake, bicep, business intelligence, c, computer vision based ai technologies, data lake medallion architecture, data pipeline, dataanalytics, datamodeling, dataops, datawarehouse, dbt, delta, devops, git, java, linux, microsoft fabric, pandas, powerbi, purview, python, scala, semantic model definition, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, tabular modeling, terraform"
IT Auditor/Data Analyst III,Williams,"Tulsa, OK",https://www.linkedin.com/jobs/view/it-auditor-data-analyst-iii-at-williams-3782544482,2023-12-17,Sun Valley,United States,Mid senior,Hybrid,"As an IT Auditor III, you will evaluate, test, and monitor IT controls. You will be a subject matter authority for IT audit engagements and data analytics. You will lead efforts to improve processes, systems, or business behavior and provide training for other team members.
Your work will challenge you, and with our Core Values to guide you, you’ll quickly learn and grow with us.
Responsibilities/Expectations:
Supports the internal audit plan by implementing work papers, walkthroughs, internal control testing, substantive procedures, and other special reviews/consulting engagements for IT projects and field operations using traditional and/or agile methodologies
Monitors and review job progress and workpapers to ensure accurate and timely completion of audit projects
Provides verbal and written recommendations to management on process and internal controls improvements by preparing audit reports and ensuring audit findings and recommendations are clear, effective, and sustainable
Validates all exceptions and action plans with process owners and management and perform periodic follow-up through action plan completion
Advises the internal audit department on current and emerging data analytics standard processes, tools, and techniques and champion their adoption; Prepares and provides ad-hoc analyses and reports on business transactional data
Other duties as assigned
Education/Years of Experience:
Required: High School Diploma or GED and a minimum of four (4) years' IT, Information Security, IT Audit or related industry experience
Preferred: Bachelor's degree; Certified Internal Auditor (CIA) or similar professional accreditation and six (6) years' IT, Information Security, IT Audit or related industry experience
Preferred: CISA Certification
Other Requirements:
Demonstrates strong analytical, written communication, interpersonal, and presentation skills. Prioritizes work when given multiple projects and works with limited supervision
Possess advanced skills in Excel, PowerPoint, Visio, and Word. Maintains proficiency in these tools as technology changes. Exhibits strong relationship and documentation skills, including process walkthroughs, narratives/flow charts, and transactional testing
Demonstrates strong problem-solving, written and verbal communication, and learning agility skills. Knowledge of operating systems, networking technologies, virtualization technologies, IT service management, cybersecurity frameworks, and/or programming/scripting. Exhibits knowledge of COSO, NIST, or other enterprise risk frameworks
Preferred: Possesses experience in PowerBI or similar data analytics platforms. Exhibit project management skills
Show more
Show less","IT Security, IT Audit, Data Analytics, Process Improvement, Training, Work Papers, Walkthroughs, Substantive Procedures, Special Reviews, Consulting Engagements, Agile Methodologies, Audit Reports, Action Plans, Process Owners, Data Analytics Standards, Data Analytics Techniques, Excel, PowerPoint, Visio, Word, Relationship Management, Documentation Skills, Process Walkthroughs, Narratives, Flow Charts, Transactional Testing, Problem Solving, Communication, Learning Agility, Operating Systems, Networking Technologies, Virtualization Technologies, IT Service Management, Cybersecurity Frameworks, Programming, Scripting, COSO, NIST, Enterprise Risk Frameworks, PowerBI, Project Management","it security, it audit, data analytics, process improvement, training, work papers, walkthroughs, substantive procedures, special reviews, consulting engagements, agile methodologies, audit reports, action plans, process owners, data analytics standards, data analytics techniques, excel, powerpoint, visio, word, relationship management, documentation skills, process walkthroughs, narratives, flow charts, transactional testing, problem solving, communication, learning agility, operating systems, networking technologies, virtualization technologies, it service management, cybersecurity frameworks, programming, scripting, coso, nist, enterprise risk frameworks, powerbi, project management","action plans, agile methodologies, audit reports, communication, consulting engagements, coso, cybersecurity frameworks, data analytics standards, data analytics techniques, dataanalytics, documentation skills, enterprise risk frameworks, excel, flow charts, it audit, it security, it service management, learning agility, narratives, networking technologies, nist, operating systems, powerbi, powerpoint, problem solving, process improvement, process owners, process walkthroughs, programming, project management, relationship management, scripting, special reviews, substantive procedures, training, transactional testing, virtualization technologies, visio, walkthroughs, word, work papers"
Strategic Buyer/Data Analyst,BluePrint Automation (BPA),"Chesterfield, VA",https://www.linkedin.com/jobs/view/strategic-buyer-data-analyst-at-blueprint-automation-bpa-3686840515,2023-12-17,Livingston,United States,Mid senior,Onsite,"Data Analyst / Buyer
BluePrint Automation (BPA) is the world leader in innovative packaging automation solutions. Our mission is to deliver solutions that offer true flexibility in real world production environments. Backed by thousands of successful installations in over 30 countries, BPA has three full-service manufacturing facilities in the USA and Europe, two additional facilities in Hangzhou China and São Paulo Brazil and eight additional subsidiaries throughout the world for sales and service. We know it is our reputation that makes us successful, starting from initial contact through project management, including our 24/7 after sales service and support. We remain committed to being your long-term partner for innovative end-of-line turn-key packaging automation solutions for primary and secondary packages. We are currently seeking a Data Analyst / Buyer for our South Chesterfield, VA location.
Responsibilities
The Date Analyst / Buyer is a hands-on role responsible for ensuring uninterrupted and timely delivery of materials to Operations and our customers by evaluating the longer-term needs of the organization and recommending / executing strategic purchases. Additionally, the role has responsibility for:
Identifying long-lead and/or “challenging” items to procure against on-going demand
Evaluate and work close with production scheduling and aftermarket teams to maintain planning parameters to meet inventory expectations.
Evaluate and optimize stocking levels for vendor managed inventories against on-going demand
Identify critical value suppliers and negotiate long-term contracts that set out prices, quality standards, cost-reduction targets, delivery timescales, and commercial terms and conditions. Conduct annual reviews against performance metrics and develop plan for improvement if necessary. Partner with suppliers to identify opportunities for better business relationships
Negotiate with key suppliers regarding price, lead time, ordering quantities, shipping terms, and delivery expectations to achieve ongoing cost containment and improvement.
Maintain accuracy of ERP system data to ensure reliable business operations
Find sources and negotiate pricing for new products and resource existing products when needed.
Perform other duties as assigned.
Education / Experience & Qualifications
Minimum Qualifications
include the following:
Minimum of a bachelor’s degree (or equivalent) with a preferred field of study in either procurement, business, or engineering. However, a combination of significant experience in related fields combined with education will be taken into consideration.
Demonstrated critical thinking, project management, written and verbal communications skills.
Proficient computer skills in MS Office programs.
2 Years (minimum) experience/skills related to data analytics including ability to define problems, collect data, establish facts, and draw valid conclusions in mathematical or diagram form and deal with several abstract and concrete variables.
Strong MS- Excel skills for data management and analysis (pivot tables, lookups, filtering, etc.)
2 Years (minimum) experience working with SQL or similar database structures (Access, dBase, Oracle, etc.)
2 Years (minimum) industry experience related to procurement and/or data analytics
Strong relationship skills with excellent time management and follow up skills.
Must be self-directed and detail oriented with an inquisitive nature and the desire to solve problems.
Strong written and verbal communication skills.
Strong work ethic with a positive, “can-do"" attitude.
While not required, these
Additional Skills / Qualifications
will be beneficial to the role:
Knowledge / experience working in a manufacturing environment related to equipment construction
Experience working with Kinetic ERP system
BPA offers an opportunity to grow and develop your career in an environment that provides a fulfilling workplace for employees, creates an environment for continuous learning, and embraces the ideas and diversity of others. In addition, we offer a robust benefits package including a majority employer paid employee health, dental, vision, life and disability; 20 days’ PTO (pro rata) during the first year of employment, as well as 401(k) with a 6% match.
All qualified applicants will receive consideration for employment without regard to sex, pregnancy, race, color, creed, religion, national origin, age, disability status, protected veteran status, marital status, sexual orientation, gender identity or expression, or any other legally protected status.
BPA prides itself on the quality of its employees and as such, candidates who receive an employment offer will be required to successfully pass a drug screen and a background check. At this time, BPA will not sponsor a new applicant for employment authorization for this position.
BPA is an Equal Opportunity Employer, Minorities/Female/Disabled/Veteran.
Job Type: Full-time
Show more
Show less","MSExcel, SQL, Procurement, Data analytics, Data mining, ERP (Kinetic)","msexcel, sql, procurement, data analytics, data mining, erp kinetic","data mining, dataanalytics, erp kinetic, msexcel, procurement, sql"
Data Engineer Data Quality - Contract - 23-00210,Atlantic Partners Corporation,"Bethlehem, PA",https://www.linkedin.com/jobs/view/data-engineer-data-quality-contract-23-00210-at-atlantic-partners-corporation-3636421291,2023-12-17,Allentown,United States,Mid senior,Onsite,"Job Description:
The company is seeking an experienced Data Engineer to be part of our Data and Analytics organization. You will be playing a key role in building and delivering best-in-class data and analytics solutions aimed at creating value and impact for the organization and our customers. As a member of the data engineering team, you will help developing and delivery of Data Products with quality backed by best-in-class engineering. You will collaborate with analytics partners, business partners and IT partners to enable the solutions.
The Qualified Candidate will:
Architect, build, and maintain scalable and reliable data pipelines including robust data quality as part of data pipeline which can be consumed by analytics and BI layer.
Design, develop and implement low-latency, high-availability, and performant data applications and recommend & implement innovative engineering solutions.
Design, develop, test and debug code in Python, SQL, PySpark, bash scripting as per company standards.
Design and implement data quality framework and apply it to critical data pipelines to make the data layer robust and trustworthy for downstream consumers.
Design and develop orchestration layer for data pipelines which are written in SQL, Python and PySpark.
Apply and provide guidance on software engineering techniques like design patterns, code refactoring, framework design, code reusability, code versioning, performance optimization, and continuous build and Integration (CI/CD) to make the data analytics team robust and efficient.
Performing all job functions consistent with company policies and procedures, including those which govern handling PHI and PII.
Work closely with various IT and business teams to understand systems opportunities and constraints for maximally utilizing company Enterprise Data Infrastructure.
Develop relationships with business team members by being proactive, displaying an increasing understanding of the business processes and by recommending innovative solutions.
Communicate project output in terms of customer value, business objectives, and product opportunity.
The Qualified Candidate has:
5&plus; years of experience with Bachelors / master's degree in computer science, Engineering, Applied mathematics or related field.
Extensive hands-on development experience in Python, SQL and Bash.
Extensive Experience in performance optimization of data pipelines.
Extensive hands-on experience working with cloud data warehouse and data lake platforms like Databricks, Redshift or Snowflake.
Familiarity with building and deploying scalable data pipelines to develop and deploy Data Solutions using Python, SQL, PySpark.
Extensive experience in all stages of software development and expertise in applying software engineering best practices.
Experience in developing and implementing Data Quality framework either home grown or using any open-source frameworks like Great Expectations, Soda, Deequ.
Extensive experience in developing end-to-end orchestration layer for data pipelines using frameworks like Apache Airflow, Prefect, Databricks Workflow.
Familiar with RESTful Webservices (REST APIs) to be able to integrate with other services.
Familiarity with API Gateways like APIGEE to secure webservice endpoints.
Familiarity with concurrency and parallelism.
Familiarity with Data pipelines and Client development cycle.
Experience in creating and configuring continuous integration/continuous deployment using pipelines to build and deploy applications in various environments and use best practices for DevOps to migrate code to Production environment.
Ability to investigate and repair application defects regardless of component: front-end, business logic, middleware, or database to improve code quality, consistency, delays and identify any bottlenecks or gaps in the implementation.
Ability to write unit tests in python using unit test library like pytest.
Additional Qualifications (nice to have):
Experience in using and implementing data observability platforms like Monte Carlo Data, Metaplane, Soda, bigeye or any other similar products.
Expertise in debugging issues in Cloud environment by monitoring logs on the VM or use AWS features like Cloudwatch.
Experience with DevOps tech stack like Jenkins and Terraform.
Experience working with concept of Observability in software world and experience with tools like Splunk, Zenoss, Datadog or similar.
Ability to learn and adopt to new concepts and frameworks and create proof of concept using newer technologies.
Ability to use agile methodology throughout the development lifecycle and provide update on regular basis, escalating issues or delays in a timely manner.
Show more
Show less","Data Engineering, Data Analytics, Data Pipelines, Data Quality, Data Applications, Python, SQL, PySpark, Bash Scripting, RESTful Webservices (REST APIs), API Gateways, Apache Airflow, Prefect, Databricks Workflow, DevOps, Unit Testing, Jenkins, Terraform, Observability, Splunk, Zenoss, Datadog, Agile Methodology, Continuous Integration/Continuous Deployment (CI/CD), Data Observability Platforms, Monte Carlo Data, Metaplane, Soda, Bigeye","data engineering, data analytics, data pipelines, data quality, data applications, python, sql, pyspark, bash scripting, restful webservices rest apis, api gateways, apache airflow, prefect, databricks workflow, devops, unit testing, jenkins, terraform, observability, splunk, zenoss, datadog, agile methodology, continuous integrationcontinuous deployment cicd, data observability platforms, monte carlo data, metaplane, soda, bigeye","agile methodology, apache airflow, api gateways, bash scripting, bigeye, continuous integrationcontinuous deployment cicd, data applications, data engineering, data observability platforms, data quality, dataanalytics, databricks workflow, datadog, datapipeline, devops, jenkins, metaplane, monte carlo data, observability, prefect, python, restful webservices rest apis, soda, spark, splunk, sql, terraform, unit testing, zenoss"
Data Analyst #: 23-05460,HireTalent - Diversity Staffing & Recruiting Firm,"Hatfield, PA",https://www.linkedin.com/jobs/view/data-analyst-%23-23-05460-at-hiretalent-diversity-staffing-recruiting-firm-3764250012,2023-12-17,Allentown,United States,Mid senior,Onsite,"Job Description
We are seeking a Data Analyst in which you will serve the business as an Information Systems Lead and take ownership of a cross systems perspective of client's operational data.
You will ensure that the data is gathered and managed properly in multiple Oracle ERP functions and several other core systems including plant automation; data acquisition; color computing; time accounting; research & development electronic records; and product management.
Responsibilities
Initiating and conducting broadly defined technical programs related to data integration and information flow across multiple platforms in a manufacturing environment.
Leading the design, implementation, and maintenance of software systems integrated with upper-level manufacturing and business data systems and lower-level control systems used to manufacture Penn Color products.
Maintaining data systems to reliably provide information necessary for decision making by production, engineering, and other support roles for real-time and historical data analytics.
Supporting development and adoption of company best practices for software and process technologies to enable manufacturing and supply chain improvements.
Communicating project status and issues to the project stakeholders to ensure timely resolution and achievement of project milestones.
Championing self-development and providing high-level trouble shooting capabilities and tactical support for day-to-day manufacturing and supply chain teams.
Keeps immediate supervisor well-informed of activities and recommends corrective actions.
Other duties and responsibilities as assigned.
Qualifications
Bachelor’s degree or higher from a four-year engineering degree in Computer Science, Computer Engineering, Software Engineering, or Electrical Engineering from an accredited institution.
Ten (10) combined years of experience Systems Engineering or similar manufacturing experience.
Manufacturing Plant Data Analysis Experience is a must.
Passion for experience in developing SMART Manufacturing, Data Automation, and approaches to contribute to the digital transformation of Penn Color’s manufacturing and supply chain processes.
Efficient database skills.
Experience with MES (Manufacturing Execution System) integration.
Knowledge of PLCs, SCADA environments, Ethernet network communication, and Windows operating systems.
Working experience in IT systems integration; manufacturing IT systems’ web and mobile application development.
Understanding of deployment aspects of upper-level manufacturing and business data systems (Oracle EBS, legacy, and other enterprise systems).
Experience with the Rockwell or Siemens automation and data acquisition platforms.
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with project stakeholders.
Ability to understand and translate business requirements into data and technical requirements.
Team player with the ability to successfully meet project milestones in a matrix environment.
A readiness and eagerness to learn other technologies, as needed.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender, identity, national origin, disability, or protected veteran status.
Show more
Show less","Data Analysis, Oracle ERP, Plant Automation, Data Acquisition, Color Computing, Time Accounting, Electronic Records, Product Management, Software Development, Manufacturing, Supply Chain, Project Management, Communication, Troubleshooting, MES (Manufacturing Execution System), PLCs, SCADA, Ethernet, Windows, Web Development, Oracle EBS, Rockwell, Siemens","data analysis, oracle erp, plant automation, data acquisition, color computing, time accounting, electronic records, product management, software development, manufacturing, supply chain, project management, communication, troubleshooting, mes manufacturing execution system, plcs, scada, ethernet, windows, web development, oracle ebs, rockwell, siemens","color computing, communication, data acquisition, dataanalytics, electronic records, ethernet, manufacturing, mes manufacturing execution system, oracle ebs, oracle erp, plant automation, plcs, product management, project management, rockwell, scada, siemens, software development, supply chain, time accounting, troubleshooting, web development, windows"
Data Analyst #: 23-05691,HireTalent - Diversity Staffing & Recruiting Firm,"Hatfield, PA",https://www.linkedin.com/jobs/view/data-analyst-%23-23-05691-at-hiretalent-diversity-staffing-recruiting-firm-3764245423,2023-12-17,Allentown,United States,Mid senior,Onsite,"Job Description
We are seeking a Data Analyst in which you will serve the business as an Information Systems Lead and take ownership of a cross systems perspective of Penn Color operational data.
You will ensure that the data is gathered and managed properly in multiple Oracle ERP functions and several other core systems including plant automation; data acquisition; color computing; time accounting; research & development electronic records; and product management.
Responsibilities
Initiating and conducting broadly defined technical programs related to data integration and information flow across multiple platforms in a manufacturing environment.
Leading the design, implementation, and maintenance of software systems integrated with upper-level manufacturing and business data systems and lower-level control systems used to manufacture clients' products.
Maintaining data systems to reliably provide information necessary for decision-making by production, engineering, and other support roles for real-time and historical data analytics.
Supporting development and adoption of company best practices for software and process technologies to enable manufacturing and supply chain improvements.
Communicating project status and issues to the project stakeholders to ensure timely resolution and achievement of project milestones.
Championing self-development and providing high-level trouble shooting capabilities and tactical support for day-to-day manufacturing and supply chain teams.
Keeps immediate supervisor well-informed of activities and recommends corrective actions.
Other duties and responsibilities as assigned.
Qualifications
Bachelor’s degree or higher from a four-year engineering degree in Computer Science, Computer Engineering, Software Engineering, or Electrical Engineering from an accredited institution.
Ten (10) combined years of experience Systems Engineering or similar manufacturing experience.
Manufacturing Plant Data Analysis Experience is a must.
Passion for experience in developing SMART Manufacturing, Data Automation, and approaches to contribute to the digital transformation of Penn Color’s manufacturing and supply chain processes.
Efficient database skills.
Experience with MES (Manufacturing Execution System) integration.
Knowledge of PLCs, SCADA environments, Ethernet network communication, and Windows operating systems.
Working experience in IT systems integration; manufacturing IT systems’ web and mobile application development.
Understanding of deployment aspects of upper-level manufacturing and business data systems (Oracle EBS, legacy, and other enterprise systems).
Experience with the Rockwell or Siemens automation and data acquisition platforms.
Excellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with project stakeholders.
Ability to understand and translate business requirements into data and technical requirements.
Team player with the ability to successfully meet project milestones in a matrix environment.
A readiness and eagerness to learn other technologies, as needed.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender, identity, national origin, disability, or protected veteran status.
Show more
Show less","Data Analysis, Data Integration, System Engineering, Manufacturing Plant Data Analysis, Oracle ERP, MES (Manufacturing Execution System) Integration, PLCs, SCADA, Ethernet, Windows, IT Systems Integration, Manufacturing IT Systems, Web and Mobile Application Development, Rockwell, Siemens, Verbal Communication, Written Communication, Team Player, Oracle EBS, Legacy Systems","data analysis, data integration, system engineering, manufacturing plant data analysis, oracle erp, mes manufacturing execution system integration, plcs, scada, ethernet, windows, it systems integration, manufacturing it systems, web and mobile application development, rockwell, siemens, verbal communication, written communication, team player, oracle ebs, legacy systems","data integration, dataanalytics, ethernet, it systems integration, legacy systems, manufacturing it systems, manufacturing plant data analysis, mes manufacturing execution system integration, oracle ebs, oracle erp, plcs, rockwell, scada, siemens, system engineering, team player, verbal communication, web and mobile application development, windows, written communication"
Senior Master Data Analyst,B. Braun Medical Inc. (US),"Allentown, PA",https://www.linkedin.com/jobs/view/senior-master-data-analyst-at-b-braun-medical-inc-us-3766321104,2023-12-17,Allentown,United States,Mid senior,Onsite,"Overview
About B. Braun
B. Braun Medical Inc., a leader in infusion therapy and pain management, develops, manufactures, and markets innovative medical products and services to the healthcare industry. Other key product areas include nutrition, pharmacy admixture and compounding, ostomy and wound care, and dialysis. The company is committed to eliminating preventable treatment errors and enhancing patient, clinician and environmental safety. B. Braun Medical is headquartered in Bethlehem, Pa., and is part of the B. Braun Group of Companies in the U.S., which includes B. Braun Interventional Systems, Aesculap ® and CAPS ® .
Globally, the B. Braun Group of Companies employs more than 64,000 employees in 64 countries. Guided by its Sharing Expertise ® philosophy, B. Braun continuously exchanges knowledge with customers, partners and clinicians to address the critical issues of improving care and lowering costs. To learn more about B. Braun Medical, visit www.BBraunUSA.com .
Responsibilities
Position Summary:
The Sr. Master Data Analyst will have the responsibility for the process of generating and maintaining the master data elements used within SAP ERP system. This person will interface with various functional areas to ensure that accurate master data is being generated, entered into the system and maintained as changes to the business occur; and will also work together with functional departments to ensure that Master Data can be reported as required to meet the Regulatory and business needs. This person will assist the functional areas in generating, entering and maintain the master data content. This person will function as a requestor for central master server information; review and evaluate master data for accuracy and compliance to corporate standards prior to submission to the CMS global transfer point. The Sr. Master Data Analyst will report directly to the Master Data Manager.
Responsibilities: Essential Duties
Responsibility for the process of generating and maintaining the master data elements used within SAP ERP system. Interface with various functional areas to ensure that accurate master data is being generated, entered into the system and maintained as changes to the business occur.
Work with functional departments to ensure that Master Data can be reported as required to meet the Regulatory and business needs; assist in generating, entering and maintain the master data content.
Function as a requestor for central master server information; review and evaluate master data for accuracy and compliance to corporate standards prior to submission to the CMS global transfer point.
The job function listed is not exhaustive and shall also include any responsibilities as assigned by the Supervisor from time to time.
Expertise: Knowledge & Skills
Requires full working knowledge of relevant business practices and procedures in professional field. Uses standard theories, principles and concepts and integrates them to propose a course of action.
Work under minimal supervision. Relies on experience and judgement to plan and accomplish assigned goals. May periodically assist in orienting, training, assigning and checking the work of lower level employees. Referring only complex problems and issues
Judgement is required in resolving all day-to-day problems
Interacts with internal and/or external clients and customers to negotiate and interpret information on projects and unit operations. May consult with senior management.
The targeted range for this role takes into account a range of factors that are considered when making compensation and hiring decisions; included but not limited to: skill sets; experience and training; licensure and certifications; and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. Compensation decisions are dependent on the facts and circumstances of each case. The range provided is a reasonable estimate.
#ID
#MSL
Target Based Range
$69,740 - $87,180
Qualifications
Expertise: Qualifications - Experience/Training/Education/Etc
Required
Bachelor's degree required
04-06 years related experience required.
Applicable industry/professional certification required.
Secrecy and invention agreement and non-compete agreement
Ability to work non-standard schedule as needed
Desired
N/A
While performing the duties of this job, the employee is regularly required to sit and talk or hear. The employee frequently is required to use hands to handle or feel and reach with hands and arms. The employee is occasionally required to stand and walk. The employee must occasionally lift and/or move up to 20 pounds.
Additional Information
Responsibilities: Other Duties:
The preceding functions have been provided as examples of the types of work performed by employees assigned to this position. To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed in this description are representative of the knowledge, skill, and/or ability required. Management reserves the right to add, modify, change or rescind the work assignments of different positions due to reasonable accommodation or other reasons.
Physical Demands
While performing the duties of this job, the employee is expected to:
Light work - Exerting up to 20 lbs of force occasionally, and/or up to 10 pounds of force frequently, and/or a negligible amount of force constantly to move objects.
Lifting, Carrying, Pushing, Pulling And Reaching
Occasionally: Reaching upward and downward, Push/pull, Stand, Visual Acuity with or without corrective lenses
Frequently: Sit
Constantly: N/A
Activities
Occasionally: Push/pull, Reaching upward and downward, Seeing - depth perception, color vision, field of vision/peripheral, Standing, Walking
Frequently: Finger feeling, Hearing - ordinary, fine distinction, loud (hearing protection required), Sitting, Talking - ordinary, loud/quick
Constantly: N/A
Environmental Conditions
Occasionally: Proximity to moving parts, Exposure to toxic or caustic chemicals (in most areas)
Frequently: N/A
Constantly: N/A
Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Noise Intensity: Low
Occasionally: N/A
Frequently: N/A
Constantly: Office environment
What We Offer
B. Braun offers an excellent benefits package, which includes healthcare, a 401(k) plan, and tuition reimbursement. To learn more about B. Braun and our safety healthcare products or view a listing of our employment opportunities, please visit us on the internet at www.bbraunusa.com.
Through its “Sharing Expertise®” initiative, B. Braun promotes best practices for continuous improvement of healthcare products and services.
Notices
Equal Opportunity Employer Veterans/Disabled
Show more
Show less","SAP, Master Data, SAP ERP, CMS","sap, master data, sap erp, cms","cms, master data, sap, sap erp"
Lead Data Engineer (Data Integration),BOMBARDIER,"Dorval, Quebec, Canada",https://ca.linkedin.com/jobs/view/lead-data-engineer-data-integration-at-bombardier-3775821910,2023-12-17,Chambly, Canada,Mid senior,Hybrid,"Bombardier is a global leader, creating innovative and game-changing planes. Our products and services provide world-class transportation experiences that set new standards in passenger comfort, energy, efficiency, reliability and safety. We are a global organization focused on working together with a team spirit.
In your role, you will
Lead/Administer the enterprise data platform (DWH, Data Lake).
Create and maintain optimal/reliable data pipeline architecture to meet business needs.
Define and operate the infrastructure required for optimal extraction, transformation and loading (ETL) of data from a wide variety of data sources using SQL and ‘big data’ technologies as needed (Hadoop, MapReduce, Hive, Spark, Kafka, Pig, data streaming, NoSQL, SQL, programming)
Design and implement life cycle management processes (DevOps) to enable continuous integration and continuous deployment (CICD) of data systems.
Integrate data from various resources (including external data sources and IoT) and manage the big data as a key enterprise asset.
Create and maintain backend data solutions for data analysts and data scientists. Assist them in unlocking insight from enterprise data.
Identify, design, and implement internal process and framework (e.g. elimination of manual processes, optimizing data delivery, evolving data infrastructure capabilities, etc.)
Work with stakeholders including product, data and architecture SME to assist with data-related technical issues and support their data infrastructure needs.
Ensure compliance to data architecture and security requirements.
Identify data quality issue and make recommendation for addressing root causes.
Setup observability/monitoring to measure reliability of the data pipelines and act quickly (e.g. operational support) in case of incident.
As our ideal candidate
You hold a bachelor’s degree in computer science, Statistics, Informatics, Information Systems or another quantitative field.
You have 10 years of experience in a Data Engineer / Data Specialist role .
You have experience coaching/leading a small team (technical leadership).
You have knowledge of Agile / SCRUM project delivery, DevOps and CICD practices.
You have advanced knowledge of SQL, query authoring and relational databases.
You have experience optimizing ‘big data’ pipelines (storage, file format, partitioning, spark, python, streaming)
You are efficient at performing root cause analysis to address issues and applying long-term fix.
You have experience designing and building data transformation, data structures, metadata framework and automated workload management.
You have experience in data protection measures, data privacy and collaborating with Cyber team
You have good knowledge of Azure data services (Azure Data Factory, Synapse, Azure Data Lake Storage, Event Hub, Polybase, Databricks, Delta lake, Cognitive Services, …)
You have good knowledge of Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
You have good people skills and are a team player, motivated by developing other people.
You are a good communicator, who can simplify complex technical issue for non-technical people. You can work in an environment with a mix of French and English languages.
Bombardier is an equal opportunity employer and encourages persons of any race, religion, ethnicity, gender identity, sexual orientation, age immigration status, disability or other applicable legally protected Characteristics to apply.
We thank all applicants for their interest, however, only those under consideration will be contacted. Please continue to monitor our website and apply for additional positions for which you are qualified and may be of interest to you.
Join us at https://bombardier.com/en/careers/career-opportunities
Your ideas move people.
Job
Lead Data Engineer (Data Integration)
Primary Location
Administrative Centre (CA)
Organization
Aerospace Canada
Shift
Employee Status
Non-Employee
Show more
Show less","Data Warehousing (DWH), Data Lake, Data Pipeline Architecture, Extraction Transformation Loading (ETL), Hadoop, MapReduce, Hive, Spark, Kafka, Pig, NoSQL, DevOps, Data Integration, Big Data, Data Analytics, Data Science, Data Quality, Observability, Monitoring, Data Security, Azure Data Services, Azure Data Factory, Synapse, Azure Data Lake Storage, Event Hub, Polybase, Databricks, Delta Lake, Cognitive Services, ObjectOriented Languages, Python, Java, C++, Scala","data warehousing dwh, data lake, data pipeline architecture, extraction transformation loading etl, hadoop, mapreduce, hive, spark, kafka, pig, nosql, devops, data integration, big data, data analytics, data science, data quality, observability, monitoring, data security, azure data services, azure data factory, synapse, azure data lake storage, event hub, polybase, databricks, delta lake, cognitive services, objectoriented languages, python, java, c, scala","azure data factory, azure data lake storage, azure data services, big data, c, cognitive services, data integration, data lake, data pipeline architecture, data quality, data science, data security, data warehousing dwh, dataanalytics, databricks, delta lake, devops, event hub, extraction transformation loading etl, hadoop, hive, java, kafka, mapreduce, monitoring, nosql, objectoriented languages, observability, pig, polybase, python, scala, spark, synapse"
Data Engineer (Remote Option),Balsam Brands,"Boise, ID",https://www.linkedin.com/jobs/view/data-engineer-remote-option-at-balsam-brands-3777334421,2023-12-17,Idaho City,United States,Mid senior,Remote,"Job Description
As Data Engineer, you will be responsible for designing and developing robust and scalable data warehousing solutions. The Data Engineer will be responsible for building data solutions based on the business requirements. Data solutions may involve retrieval, transformation, storage, and delivery of the data. The Data Engineer must follow standards and implement best practices while writing code and provide production support for the enterprise data warehouse. Our ideal candidate is a skillful data wrangler who enjoys building data solutions from the ground up and optimizing their performance.
This full-time position reports to the Manager of Data Engineering and can work remote from any U.S. state where Balsam Brands is currently setup as an employer, which includes: CA, CO, FL, GA, ID, IL, IN, KS, KY, MD, MA, MO, NJ, NC, OH, OR, PA, TN, TX, VA, and WA. This role can also work locally in our Redwood City, CA or Boise, ID office location. Our local teams work in a hybrid model, which currently includes Tuesday and Wednesday in-office.
To ensure sufficient overlap with functional and cross-functional team members globally, some flexibility with this role's regular work schedule will be required. Most of our teams have overlap with early morning and/or early evening PST. Specific scheduling needs for this role will be discussed in the initial interview.
What You’ll Do
Be accountable for building and maintaining the data infrastructure for the organization
Collaborate with systems analysts and cross functional partners to understand data requirements
Champion data warehouse, create denormalized data foundation layer and normalized data marts
Define strategies to capture all data sources and impact of business process changes on data coming from those sources
Work on all aspects of the data warehouse/BI environment including architecture, design, development, automation, caching and performance tuning
Continually explore new technologies like Big Data, Artificial Intelligence, Generative AI, Machine Learning, and Predictive Data Modeling
What You Bring To The Table
5+ years of professional experience in the data engineering field
Demonstrated history of designing and building schemas, tables, views, and data pipelines
Experience in cloud technologies like Azure, AWS
Experience in Azure Data Factory (ADF) or equivalent ETL tool
Knowledge and experience of working with SQL and relational databases like SQL Server, Oracle, Postgres and MySQL
Ability to understand and tell the story embedded in the data at the core of our business
Ability to communicate with non-technical audience from a variety of business functions
Strong knowledge of coding standards, best practices and data governance
Travel for remote team members:
At Balsam Brands, we believe that time spent together, in-person, collaborating and building relationships is important to who we are. For our newest remote Brandits, we will arrange travel to one of our local offices within your first three months of employment so you can meet and train with your new team in-person. You may also get to travel an additional 1 – 2 times a year for events such as team retreats, offsites, or learning and development opportunities.
Notes:
This is a full-time, permanent position with benefits. Please only apply if you are able to live and work full-time in one of the states listed in this posting. State locations and specifics are subject to change as our hiring requirements shift.
About Us:
Balsam Brands is a global, eCommerce retailer with roots in holiday and home décor. We strive for excellence in everything we do and present a unique opportunity for those seeking to have a meaningful impact in a people-first company that values relationship building, authenticity, and doing the right thing. We have steadily growing teams in Boise, the Bay Area, Dublin, and the Philippines.
The company's mission is to create joy together. We empower our team and partners to love what they do, provide products and experiences that inspire meaningful moments with family and friends, and give back to our families and communities in impactful ways. When you join Balsam Brands, you'll find a culture of caring people doing challenging work and building a welcoming workplace.
Check out our flagship brand, Balsam Hill: www.balsamhill.com
Balsam Brands in Forbes: https://bit.ly/balsambrandsforbes
Balsam Brands on LinkedIn: http://www.linkedin.com/company/balsam-brands/
Glassdoor: https://bit.ly/balsambrands-glassdoor
Benefits
At Balsam Brands, we strive to offer a competitive compensation and benefits package. For permanent, full-time team members, our current package includes:
Competitive compensation, including a cash-based incentive plan; salary is reviewed yearly and may be adjusted as part of the normal compensation review process
Comprehensive Medical, Dental, and Vision coverage, with 100% of monthly premiums covered for team members, and 85%+ employer-paid premiums for other coverage tiers that include dependents
Up to $2,000 annual funding toward HSA accounts
Medical, transit, dependent care FSA
Infertility coverage offered on all medical plans
Generous parental leave program and flexible return options
Company-paid life and AD&D insurance
Company-paid short and long-term disability insurance
401(k) with dollar-for-dollar company match up to $4,000 per calendar year
Employee Assistance Program (EAP) and other mental health and wellness perks
Paid holidays, annual shutdown week, PTO, and volunteer time-off (VTO) packages
Paid 5-week sabbatical leave after 10 years of employment
Annual continuous learning benefit up to $1,000 per person, per fiscal year
Up to $300 flexible reimbursement to support setup of new team member's work-from-home environment
Generous team member merchandise discount
Valuable extras: identity theft protection, subsidized parking, monthly wellness, pet insurance, accident & critical illness insurance
The base pay range for this position is: $111,000 to $132,000. Where an individual falls within that range will vary based on several factors including geographic location and may vary depending on candidate qualifications and experience, applicable skills, and other job-related factors. We benchmark our pay ranges against current external data sources and regularly review compensation for our team members. Balsam Brands is committed to providing our team members with an internally fair, externally competitive, and fiscally prudent total compensation package administered in a simple and consistent manner.
At Balsam Brands, we strive to build a diverse, equitable, and inclusive team to fulfill our purpose to create joy together. Balsam Brands is proud to be an equal opportunity employer. We encourage people from all backgrounds, ages, abilities, and experiences to apply. We do not discriminate on the basis of race, ethnicity, religion, national origin, citizenship, marital or family status, disability, sexual orientation, gender identity or expression, pregnancy or caregiver status, veteran status, or any other legally protected status. We will ensure that individuals with disabilities are provided reasonable accommodations to participate in the job application and interview process, to perform essential job functions, and to receive other benefits and privileges of employment.
#DICE
Additional Information
All your information will be kept confidential according to EEO guidelines.
Show more
Show less","Data Engineering, Building Data Infrastructure, Data Warehousing, Data Pipelines, SQL, Relational Databases, SQL Server, Oracle, Postgres, MySQL, Big Data, Artificial Intelligence, Generative AI, Machine Learning, Predictive Data Modeling, Azure, AWS, Azure Data Factory (ADF), ETL Tools, Coding Standards, Data Governance, Data Analysis, Data Visualization, Business Intelligence (BI), Data Storytelling, Communication Skills","data engineering, building data infrastructure, data warehousing, data pipelines, sql, relational databases, sql server, oracle, postgres, mysql, big data, artificial intelligence, generative ai, machine learning, predictive data modeling, azure, aws, azure data factory adf, etl tools, coding standards, data governance, data analysis, data visualization, business intelligence bi, data storytelling, communication skills","artificial intelligence, aws, azure, azure data factory adf, big data, building data infrastructure, business intelligence bi, coding standards, communication skills, data engineering, data governance, data storytelling, dataanalytics, datapipeline, datawarehouse, etl tools, generative ai, machine learning, mysql, oracle, postgres, predictive data modeling, relational databases, sql, sql server, visualization"
Java developer - Big Data / Glasgow,Ampstek,"Glasgow, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/java-developer-big-data-glasgow-at-ampstek-3667471838,2023-12-17,Motherwell, United Kingdom,Mid senior,Onsite,"Hi Professionals,
This is Jason Mathew from Ampstek
New Requirement
Role Java developer - Big Data
Location- Glasgow, Scotland ( Hybrid )
Type- Full time / Fixed Term Contract
Onsite 3 days a week is mandate
Requirement
Must Have: 8+ years of experience working in Java technology
Strong experience with Core Java Proficient in Spring Boot, Microservices Experience with Bigdata and Spark Expectation
Core Java - Threading, Collection, Threadpool
Event driven design
Microservice Architecture
Explain Architecture of previous couple of projects
Java 8 - Hashmap new implementation (Good to have)
hanks & Regards
Jason Mathew | IT
Recruiter
|
Europe & UK
E-mail- jason.mathew@ampstek.com
Tel - +48 (22)1857586
LinkedIn :
https://www.linkedin.com/in/pravin-raja-jason-mathew-4340aa101/
Ampstek Services Limited
Kemp House, 152-160, City Road
London-EC1V 2NX
Website :
https://www.ampstek.com/
Show more
Show less","Java, Spring Boot, Microservices, Big Data, Spark, Hadoop, Java 8, Multithreading, Thread Pool, EventDriven Design, Microservice Architecture, HashMap","java, spring boot, microservices, big data, spark, hadoop, java 8, multithreading, thread pool, eventdriven design, microservice architecture, hashmap","big data, eventdriven design, hadoop, hashmap, java, java 8, microservice architecture, microservices, multithreading, spark, spring boot, thread pool"
Data Analyst I,Children's National Hospital,"Silver Spring, MD",https://www.linkedin.com/jobs/view/data-analyst-i-at-children-s-national-hospital-3780835816,2023-12-17,Clarksburg,United States,Mid senior,Onsite,"A Data Analyst plays a crucial role in analyzing and interpreting data to provide valuable insights that drive informed decision-making. You'll work with diverse datasets, collaborating closely with teams to understand business needs and present findings effectively. Additionally, you'll be responsible for crafting clear and concise reports that communicate your analysis outcomes.
Collect, input, verify, correct, and analyze data to measure key performance indicator actual versus business objectives.
Collect, clean, and transform data for analysis
Analyze data to identify trends, patterns, and opportunities
Create visualizations, reports, and dashboards to present insights
Collaborate with teams to define data requirements and priorities
Assist in the development of data-driven strategies and initiatives
Craft well-structured reports that effectively communicate analysis results
Minimum Education
Bachelor's Degree Bachelor's degree in a relevant field (e.g., Computer Science, Statistics, Mathematics) (Required)
Minimum Work Experience
0 - 3 years relevant work experience. (Required)
Required Skills/Knowledge
Work involves some problem solving with assistance and guidance in understanding and applying Children’s National’s policies and procedures.
Attention to detail critical.
Ability to collect, organize, and display data in spreadsheet format.
Follow-through skills necessary to get information from operational business owners and vendors to have data errors/omissions corrected.
Relationship management skills strongly desired.
Strong written and verbal communication skills to interact with management.
Be curious and proactive in exploring data to uncover meaningful insights
Communicate complex findings in a clear and understandable manner
Collaborate effectively with cross-functional teams
Adapt to changing priorities and thrive in a dynamic environment
Primary Location
Maryland-Silver Spring
Work Locations
Inventa Towers
Job
Information Technology
Organization
Operations
Position Status
R (Regular)
Shift
Day
Work Schedule
Monday - Friday
Job Posting
Dec 7, 2023, 2:52:35 AM
Show more
Show less","Data Analysis, Data Interpretation, Data Visualization, Data Reporting, Data Cleaning, Data Transformation, Trend Analysis, Pattern Recognition, Opportunity Identification, Visualization Software, Reporting Software, Dashboard Software, Data Requirements Gathering, Data Prioritization, DataDriven Strategy Development, DataDriven Initiative Development, Communication, Problem Solving, Attention to Detail, Spreadsheet Software, Data Error Correction, Relationship Management, Curiosity, Proactiveness, Insight Discovery, Complex Findings Communication, CrossFunctional Team Collaboration, Adaptability, Dynamic Environment Thriving","data analysis, data interpretation, data visualization, data reporting, data cleaning, data transformation, trend analysis, pattern recognition, opportunity identification, visualization software, reporting software, dashboard software, data requirements gathering, data prioritization, datadriven strategy development, datadriven initiative development, communication, problem solving, attention to detail, spreadsheet software, data error correction, relationship management, curiosity, proactiveness, insight discovery, complex findings communication, crossfunctional team collaboration, adaptability, dynamic environment thriving","adaptability, attention to detail, communication, complex findings communication, crossfunctional team collaboration, curiosity, dashboard software, data cleaning, data error correction, data interpretation, data prioritization, data reporting, data requirements gathering, data transformation, dataanalytics, datadriven initiative development, datadriven strategy development, dynamic environment thriving, insight discovery, opportunity identification, pattern recognition, proactiveness, problem solving, relationship management, reporting software, spreadsheet software, trend analysis, visualization, visualization software"
Data Scientist/Machine Learning Engineer,"iBovi - Staffing, Consulting and Recruitment Services","Reston, VA",https://www.linkedin.com/jobs/view/data-scientist-machine-learning-engineer-at-ibovi-staffing-consulting-and-recruitment-services-3686546146,2023-12-17,Clarksburg,United States,Mid senior,Onsite,"Role: Data Scientist/Machine Learning Engineer
Location: Reston, VA (Once/Twice in a month as on needed basis)
Duration: 6 months (Extension/Permanent Possible)
Job Description
R Apps are built on Shiny
Moving these apps to Cloud
These are Data Science apps.
Work with business teams, capture requirements, and migrate to cloud.
Understand what tools will be used in AWS.
CI/CD mechanism to deploy it to cloud.
How they will access the date
What IDEs will be used, etc.
How we take this framework and deploy it to cloud
Preparation of knowledge guide - Documentations, user guides, what workflow is there, troubleshooting issues.
Must Haves
R Shiny Development
CI/CD
RStudio Connect needed.
SageMaker needed.
AWS exp needed S3, Lambda, Glue, EC2, RDS, etc.
Containers Docker/Kubernetes
Preferred
Python background desired
Java background desired
Show more
Show less","R Shiny Development, CI/CD, RStudio Connect, SageMaker, AWS (S3 Lambda Glue EC2 RDS etc.), Containers, Docker, Kubernetes, Python, Java","r shiny development, cicd, rstudio connect, sagemaker, aws s3 lambda glue ec2 rds etc, containers, docker, kubernetes, python, java","aws s3 lambda glue ec2 rds etc, cicd, containers, docker, java, kubernetes, python, r shiny development, rstudio connect, sagemaker"
Senior Data Engineer,Professional Diversity Network,"Vienna, VA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-professional-diversity-network-3784624526,2023-12-17,Clarksburg,United States,Mid senior,Onsite,"Locations: VA - McLean, United States of America, McLean, Virginia
Senior Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking
Data Engineers
who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you'll have the opportunity to be on the forefront of driving a major transformation within Capital One.
What You'll Do
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems
Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment
Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications
Bachelor's Degree
At least 4 years of experience in application development (Internship experience does not apply)
At least 1 year of experience in big data technologies
Preferred Qualifications
5+ years of experience in application development including Python, SQL, or Java
2+ years of experience with a public cloud (AWS/Microsoft Azure/ Google Cloud)
3+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, or MySQL)
2+ year experience working on real-time data and streaming applications
2+ years of data warehousing experience (Redshift or Snowflake)
3+ years of experience with UNIX/Linux including basic commands and shell scripting
2+ years of experience with Agile engineering practices
At this time, Capital One will not sponsor a new applicant for employment authorization for this position.
Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
This role is expected to accept applications for a minimum of 5 business days.
No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City's Fair Chance Act; Philadelphia's Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
PDN-98702d7a-2852-406a-9683-37f22ddf4ea4
Show more
Show less","Agile, Java, Scala, Python, RDBMS, NoSQL, Cloud Computing, Redshift, Snowflake, Linux, Unix, SQL, Redshift, Snowsage, Data Warehouse, Kafka, Spark, EMR, Hadoop, Hive, MapReduce","agile, java, scala, python, rdbms, nosql, cloud computing, redshift, snowflake, linux, unix, sql, redshift, snowsage, data warehouse, kafka, spark, emr, hadoop, hive, mapreduce","agile, cloud computing, datawarehouse, emr, hadoop, hive, java, kafka, linux, mapreduce, nosql, python, rdbms, redshift, scala, snowflake, snowsage, spark, sql, unix"
Sr. Data engineer (10+ candidate),Diverse Lynx,"Providence, VA",https://www.linkedin.com/jobs/view/sr-data-engineer-10%2B-candidate-at-diverse-lynx-3758887579,2023-12-17,Clarksburg,United States,Mid senior,Hybrid,"Hello
Hope you are doing well today !
We are currently hiring for “Sr. data engineer ” for our client who can join the project asap . Please check the Jd once which are mention below and if you are okay with the Jd please reply with you
updated resume , LinkedIn id , Current
location
ASAP .
Role :-
Sr. Data engineer
Location :- Remote )
Type & duration :- contract , 6-12months
JD:-
Senior Data Engineer
8&plus; years of total IT experience
Minimum 5&plus; years of development experience in Azure
Must have “Data Warehouse / Data Lake” development experience.
Must have “Azure Data Factory (ADF) & Azure SQL DB”
Must have “Azure Data Bricks” experience using Python or Spark or Scala
Nice to have “Data Modelling” & “Azure Synapse” experience.
Nice to Azure Client experience
Nice to have “Power BI” experience.
Nice to have Azure Data Engineer Certifications
Passion for Data Quality with an ability to integrate these capabilities into the deliverables.
Prior use of Big Data components and the ability to rationalize and align their fit for a business case.
Experience in working with different data sources - flat files, XML, JSON, Avro files and databases.
Knowledge of Jenkins for continuous integration and End-to-End automation for application build and deployments.
Ability to integrate into a project team environment and contribute to project planning activities.
Experience in developing implementation plans and schedules and preparing documentation for the jobs according to the business requirements.
Lead ambiguous and complex situations to clear measurable plans.
Proven experience and ability to work with people across the organization and skilled at managing cross-functional relationships and communicating with leadership across multiple organizations.
Proven capabilities for strong written and oral communication skill with the ability to synthesize, simplify and explain complex problems to different audiences.
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Azure, Data Warehouse, Data Lake, Azure Data Factory (ADF), Azure SQL DB, Azure Data Bricks, Python, Spark, Scala, Data Modelling, Azure Synapse, Azure Client, Power BI, Azure Data Engineer Certifications, Data Quality, Big Data, Jenkins, Continuous integration, EndtoEnd automation, Project planning, Implementation plans, Schedules, Documentation, Leadership, Communication, Diverse Lynx LLC, Equal Employment Opportunity","azure, data warehouse, data lake, azure data factory adf, azure sql db, azure data bricks, python, spark, scala, data modelling, azure synapse, azure client, power bi, azure data engineer certifications, data quality, big data, jenkins, continuous integration, endtoend automation, project planning, implementation plans, schedules, documentation, leadership, communication, diverse lynx llc, equal employment opportunity","azure, azure client, azure data bricks, azure data engineer certifications, azure data factory adf, azure sql db, azure synapse, big data, communication, continuous integration, data lake, data modelling, data quality, datawarehouse, diverse lynx llc, documentation, endtoend automation, equal employment opportunity, implementation plans, jenkins, leadership, powerbi, project planning, python, scala, schedules, spark"
Senior Microsoft SQL Database Engineer,"M.C. Dean, Inc.","Tysons Corner, VA",https://www.linkedin.com/jobs/view/senior-microsoft-sql-database-engineer-at-m-c-dean-inc-3744708642,2023-12-17,Clarksburg,United States,Mid senior,Hybrid,"Position Summary
M.C. Dean is looking for a Senior MS SQL Database Engineer to be responsible for solution design, testing, and implementation. The ideal candidate will collaborate cross functionally to implement new solutions and solve complex challenges. This candidate will also serve as a mentor and a coach in developing employees to maximize their potential, meet their career objectives, and contribute to the success of M.C. Dean and customer outcomes.
Responsibilities
Produce documentation for global initiatives following those through to implementation via collaboration with project and support teams
Lead and mentor programmers, designers, technologists, technicians and other engineering or scientific personnel
Provide recommendations on new database technologies and architectures
Manage customer satisfaction through effectively communicating and managing customer expectations
Develop a software system testing and perform risk analysis (e.g., threat, vulnerability, and probability of occurrence)
Implement and document new and existing processes and standards for communication and consistency
Analyze application architecture and performance metrics, connectivity, and design in the context of industry best practices
Apply cybersecurity functions (e.g., encryption, access control, and identity management) to reduce exploitation opportunities
Identify security implications and apply methodologies within centralized and decentralized environments across the enterprise
Design and develop secure applications (e.g., Enterprise PKI, Federated Identity server, Enterprise AV solution)
Perform configuration management, problem management, capacity management, and financial management for databases and data management systems
Translate security requirements into application design elements including documenting the elements of the software attack surfaces, conducting threat modeling, and defining any specific security criteria
Perform integrated quality assurance testing, secure programing, and risk analysis
Maintain database management systems software
Requirements:
Education Requirements:
5-7 Years of Experience with a Master’s Degree in Information Technology, Risk Management, Cybersecurity
7-9 Years of Experience with a Bachelor's Degree in Information Technology, Risk Management, Cybersecurity
Meet the KSATS listed under Work Role ID: 421 (NIST: OM-DA-001)
Must possess one of the following certifications:
CCNA-Security
GICSP
GSEC
Security +
Must be able to obtain a TS/SCI
Desired Qualifications:
Experience in the following tools:
Microsoft SQL Server 2019
Microsoft SQL Server Management Studio 19
Microsoft SQL Server Reporting Services
Microsoft Visual Studio 2022
Azure Repos
Experience and Knowledge in:
Performing deployments and upgrades
Troubleshooting and resolving system outages and incidents
Cybersecurity principles and methods that apply to software development
Security architecture concepts and enterprise architecture reference models (e.g., Zackman, Federal Enterprise Architecture [FEA])
Information technology (IT) risk management policies, requirements, and procedures
Network protocols such as TCP/IP, Dynamic Host Configuration, Domain Name System (DNS), and directory services.
Abilities:
Exposure to computer screens for an extended period of time.
Sitting for extended periods of time.
Reach by extending hands or arms in any direction.
Have finger dexterity in order to manipulate objects with fingers rather than whole hands or arms, for example, using a keyboard.
Listen to and understand information and ideas presented through spoken words and sentences.
Communicate information and ideas in speaking so others will understand.
Read and understand information and ideas presented in writing.
Apply general rules to specific problems to produce answers that make sense.
Identify and understand the speech of another person.
Applicants for this position may be required to obtain or provide proof of flu shots or of other vaccinations depending on customer requirements and nature of the position or demonstrate a valid basis for exception.
EOE Minorities/Females/Protected Veterans/Disabled
VEVRAA Contractor
Show more
Show less","MS SQL Server, Microsoft SQL Server Management Studio, Microsoft SQL Server Reporting Services, Microsoft Visual Studio, Azure Repos, Cyber security, Network protocols, Enterprise Architecture Reference Models, Risk Management, Threat Modeling","ms sql server, microsoft sql server management studio, microsoft sql server reporting services, microsoft visual studio, azure repos, cyber security, network protocols, enterprise architecture reference models, risk management, threat modeling","azure repos, cyber security, enterprise architecture reference models, microsoft sql server management studio, microsoft sql server reporting services, microsoft visual studio, ms sql server, network protocols, risk management, threat modeling"
Senior Microsoft SQL Database Engineer,"M.C. Dean, Inc.","Tysons Corner, VA",https://www.linkedin.com/jobs/view/senior-microsoft-sql-database-engineer-at-m-c-dean-inc-3744713103,2023-12-17,Clarksburg,United States,Mid senior,Hybrid,"Position Summary
M.C. Dean is looking for a Senior MS SQL Database Engineer to be responsible for solution design, testing, and implementation. The ideal candidate will collaborate cross functionally to implement new solutions and solve complex challenges. This candidate will also serve as a mentor and a coach in developing employees to maximize their potential, meet their career objectives, and contribute to the success of M.C. Dean and customer outcomes.
Responsibilities
Produce documentation for global initiatives following those through to implementation via collaboration with project and support teams
Lead and mentor programmers, designers, technologists, technicians and other engineering or scientific personnel
Provide recommendations on new database technologies and architectures
Manage customer satisfaction through effectively communicating and managing customer expectations
Develop a software system testing and perform risk analysis (e.g., threat, vulnerability, and probability of occurrence)
Implement and document new and existing processes and standards for communication and consistency
Analyze application architecture and performance metrics, connectivity, and design in the context of industry best practices
Apply cybersecurity functions (e.g., encryption, access control, and identity management) to reduce exploitation opportunities
Identify security implications and apply methodologies within centralized and decentralized environments across the enterprise
Design and develop secure applications (e.g., Enterprise PKI, Federated Identity server, Enterprise AV solution)
Perform configuration management, problem management, capacity management, and financial management for databases and data management systems
Translate security requirements into application design elements including documenting the elements of the software attack surfaces, conducting threat modeling, and defining any specific security criteria
Perform integrated quality assurance testing, secure programing, and risk analysis
Maintain database management systems software
Requirements:
Education Requirements:
7-9 Years of Experience with a Bachelor's Degree in Information Technology, Risk Management, Cybersecurity
This position will require the ability to obtain a clearance in the future; US citizenship is required due to DOD clearance eligibility.
Desired Qualifications:
Experience in the following tools:
Microsoft SQL Server 2019
Microsoft SQL Server Management Studio 19
Microsoft SQL Server Reporting Services
Microsoft Visual Studio 2022
Azure Repos
Experience and Knowledge in:
Performing deployments and upgrades
Troubleshooting and resolving system outages and incidents
Cybersecurity principles and methods that apply to software development
Security architecture concepts and enterprise architecture reference models (e.g., Zackman, Federal Enterprise Architecture [FEA])
Information technology (IT) risk management policies, requirements, and procedures
Network protocols such as TCP/IP, Dynamic Host Configuration, Domain Name System (DNS), and directory services.
Abilities:
Exposure to computer screens for an extended period of time.
Sitting for extended periods of time.
Reach by extending hands or arms in any direction.
Have finger dexterity in order to manipulate objects with fingers rather than whole hands or arms, for example, using a keyboard.
Listen to and understand information and ideas presented through spoken words and sentences.
Communicate information and ideas in speaking so others will understand.
Read and understand information and ideas presented in writing.
Apply general rules to specific problems to produce answers that make sense.
Identify and understand the speech of another person.
Applicants for this position may be required to obtain or provide proof of flu shots or of other vaccinations depending on customer requirements and nature of the position or demonstrate a valid basis for exception.
EOE Minorities/Females/Protected Veterans/Disabled
VEVRAA Contractor
Show more
Show less","Microsoft SQL Server, Microsoft SQL Server Management Studio, Microsoft SQL Server Reporting Services, Microsoft Visual Studio, Azure Repos, TCP/IP, Dynamic Host Configuration, Domain Name System (DNS), Directory Services, Cybersecurity, Risk Management, Software Development, Security Architecture, Enterprise Architecture, Information Technology (IT), Network Protocols","microsoft sql server, microsoft sql server management studio, microsoft sql server reporting services, microsoft visual studio, azure repos, tcpip, dynamic host configuration, domain name system dns, directory services, cybersecurity, risk management, software development, security architecture, enterprise architecture, information technology it, network protocols","azure repos, cybersecurity, directory services, domain name system dns, dynamic host configuration, enterprise architecture, information technology it, microsoft sql server, microsoft sql server management studio, microsoft sql server reporting services, microsoft visual studio, network protocols, risk management, security architecture, software development, tcpip"
Lead Data Engineer,ByteHire,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-bytehire-3784827736,2023-12-17,York, United Kingdom,Mid senior,Onsite,"Reference:DT-259p
Title: Lead Data Engineer
Job Type:Permanent
Salary:£80,000 - £100,000
Location:FullyRemote, (Must be located close to Leeds)
The Client
Industry: Global Aviation and Fuel Management technology business.
Purpose: Rationalise and support the use of fuel management for sustainability. This will add environmental impacts and financial impacts to the platforms customers.
The Role
Data Warehousing And Modelling
Design and implement scalable data warehousing solutions that cater to the client's analytics and reporting needs.
Regularly review and optimise the data model to accommodate evolving business requirements.
Ensure data accuracy and integrity throughout all data warehouses.
Data Pipelines Design And Operation
Develop, construct, test, and maintain architectures, such as Data Warehouse, Pipeline Orchestration, Data Transformation tooling systems.
Collaborate with data analytics engineers to produce optimised ETL/ELT processes and data pipelines.
Monitor and troubleshoot data ingestion and processing issues, ensuring timely and accurate data availability.
Data Infrastructure Observability And Monitoring
Implement monitoring tools and frameworks to oversee the health of the data infrastructure.
Establish alerting systems for potential issues and discrepancies in the data infrastructure.
Perform regular system audits to detect anomalies and ensure system resilience.
Development And Deployment Process Management
Lead the design and implementation of CI/CD processes for data pipelines and infrastructure.
Guide the team on best practices related to version control, testing, and deployment.
Collaborate with cross-functional teams to ensure smooth deployment and scaling of data solutions.
BI Reporting System Implementation And Maintenance
Partner with business stakeholders to design and roll out business intelligence solutions that meet their needs.
Maintain and optimise reporting systems, ensuring data accuracy and timely delivery.
Essential Skills
Data Architecture
Software and data architecture skills, understanding the bigger picture
Experienced in communicating with management and stakeholders around requirements
Python
Working with files, at scale
Working with data, at scale
Familiarity with Cloud Services APIs and SDKs
Unit Testing
DBT
Data warehousing, data modelling, data transformation, data quality
Databases / Storage
Strong SQL skills needed here (BigQuery and MongoDB)
Distributed Data Warehousing
Query Optimisation
Data Modelling
Role and User management
BigQuery monitoring and alerting
Apache Airflow
Good experience creating DAGS
Airflow monitoring and alerting
BI
Apache Superset (or Tableau or PowerBI)
Desirable Skills
GCP / Google Cloud
GKE (Google Kubernetes Engine)
Google Cloud Run (serverless)
Google Cloud Storage
Azure
Azure Container Apps
AKS (Azure Kubernetes Service)
Azure Blob Storage
Cloud
Kubernetes and Docker
Experienced in CI/CD, with Terraform for infrastructure provisioning
API
OpenAPI specification
Show more
Show less","Data Warehousing, Data Modelling, Data Pipelines, Data Infrastructure Observability, DevOps, Data Integration, ETL/ELT, Machine Learning, Data Analytics, Business Intelligence, Python, DBT, SQL, BigQuery, MongoDB, Apache Airflow, Apache Superset, Tableau, PowerBI, Kubernetes, Docker, Terraform, OpenAPI","data warehousing, data modelling, data pipelines, data infrastructure observability, devops, data integration, etlelt, machine learning, data analytics, business intelligence, python, dbt, sql, bigquery, mongodb, apache airflow, apache superset, tableau, powerbi, kubernetes, docker, terraform, openapi","apache airflow, apache superset, bigquery, business intelligence, data infrastructure observability, data integration, data modelling, dataanalytics, datapipeline, datawarehouse, dbt, devops, docker, etlelt, kubernetes, machine learning, mongodb, openapi, powerbi, python, sql, tableau, terraform"
Data Centre Engineer - PART TIME,Salute Mission Critical,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-centre-engineer-part-time-at-salute-mission-critical-3783946777,2023-12-17,York, United Kingdom,Mid senior,Onsite,"Job Openings
Data Centre Engineer - PART TIME
Apply To Position
Use My Indeed Resume
Apply Using LinkedIn
Are you Data Center Facility Engineer looking for a great opportunity but only available part time? Salute has several great opportunities in our key locations - supporting critical facilities - working on a 1 in 3 week call out rota.
Salute is a global leader in delivering data center services – with established operations in the US and with a rapidly growing client base in major cities across Europe.
The Data Center On call Engineer will work on agreed shifts and will be responsible for responding to out of office hours incidents on Data Centre electrical, mechanical, and fire/life safety equipment systems – and escalating as required.
Your role
Be part of an out of hours on-call rotation team to respond and investigate faults on the Data Centres mechanical and electrical building services.
Implement corrective actions to resolve faults and/or implement Emergency Operating Procedures to mitigate the impact of faults.
Follow site processes including the call out of specialist engineering support, escalation and communication to the NOC team
Support and supervise specialist engineers during fault rectification
Skills & Experience
It is essential that you live up to around 30 minutes from our data center and have access to transport. In addition we are looking for the following:
Experience in the building systems in Data Centers: with technical understanding and knowledge of the electrical and mechanical systems including Feeders, Transformers, Generators, Switchgear, UPS systems, ATS/STS units, PDU/PMM units, Chillers, Air handling units, CRAC units and building management systems (BMS)
Excellent observation skills and problem-solving ability
Good verbal and written communication skills in English
Electrical low voltage switching qualification/experience preferred
The role is an on-call position and based in Leeds, and will include business travel as needed to meet business demands but will likely be nominal.
At Salute, we don’t have employees. We have team members. It’s our culture, and it’s a significant driver of the success we're able to deliver for our clients. This team-oriented culture is defined by transparent communication, collaborative development and deployment of procedures and best practices, a customer service mindset both internally and externally, and a strong commitment to safety.
Apply today and find out more about our training, competitive salaries, and career opportunities.
Apply To Position
Use My Indeed Resume
Apply Using LinkedIn
Show more
Show less","Data Center Engineering, Electrical Power Systems, Mechanical Systems, Transformers, Generators, Switchgear, UPS Systems, ATS/STS Units, PDU/PMM Units, Chillers, Air Handling Units, CRAC Units, Building Management Systems (BMS), Electrical Low Voltage Switching, ProblemSolving, Communication, English","data center engineering, electrical power systems, mechanical systems, transformers, generators, switchgear, ups systems, atssts units, pdupmm units, chillers, air handling units, crac units, building management systems bms, electrical low voltage switching, problemsolving, communication, english","air handling units, atssts units, building management systems bms, chillers, communication, crac units, data center engineering, electrical low voltage switching, electrical power systems, english, generators, mechanical systems, pdupmm units, problemsolving, switchgear, transformers, ups systems"
Senior Data & Analytics Engineer,Jet2.com and Jet2holidays,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analytics-engineer-at-jet2-com-and-jet2holidays-3766945800,2023-12-17,York, United Kingdom,Mid senior,Onsite,"Jet2.com
and
Jet2Holidays
are always looking for great people to join our award-winning team of colleagues. We have an exciting opportunity for an experienced
Senior Data & Analytics Engineer
to join our
central data team
at
Jet2.com
and
Jet2Holidays
as we continue our journey towards becoming a fully data-driven organisation and embed a data-first culture across our business.
Reporting to one of our
Lead Data & Analytics Engineers
, the
Senior Data & Analytics Engineer
will work as part of a
multi-disciplinary, agile, data delivery team focused on the delivery of innovative, robust, and efficient data solutions
to complex business problems. You will work alongside a team of passionate data professionals including other data and analytics engineers, test engineers, data scientists and data visualisation specialists.
Our priority is the delivery of
high-quality, well-modelled, clean, and trustworthy data assets
for use across the business. Our data teams also work hard to support all our data assets and ensure the business realise maximum benefit and return on investment from them.
What Do We Need You to Do?
Mentoring: help support more junior colleagues providing advice and guidance as required to help with their development and the delivery of the team objectives.
Data Delivery: be responsible for delivery of complex data solutions including the ingest of data from a wide variety of data sources into our analytics platforms (typically cloud-based but some work on our on-premise data analytics platforms), transformation and cleansing of data and modelling of data into our enterprise data warehouse for consumption by both technical users and non-technical business users via centrally developed reporting and visualisation or self-service platforms.
Data Culture: drive a data-first culture both within the data team and across the business by supporting continual learning and development within your team and data enablement activity across the wider business. You will demonstrate a passion for data and encourage a similar passion within your team. As part of a data-first culture you may also be involved in supporting production data assets (not on a first-line basis).
What do we need you to be?
Able to demonstrate strong written and verbal communication skills and be comfortable communicating and building relationships with stakeholders at all levels.
Experienced working in an Agile delivery environment, ideally using Scrum and\or Kanban.
Able to demonstrate strong proficiency in at least some of the following technical areas (cross-training and upskilling supported for the right individual where necessary)
SQL (mandatory): a strong understanding of SQL and be comfortable reading and writing complex SQL queries ideally across multiple platforms.
Cloud Platforms (highly desirable): experience working with key services on either GCP (preferred), AWS or Azure. Key services include cloud storage, containerisation, event-driven services, orchestration, cloud functions and basic security/user management.
Data Warehousing (highly desirable): experience working on a data warehouse solution irrespective of underlying technology. Experience using cloud data warehouse technology would also be beneficial - Snowflake (preferred), Google BigQuery, AWS Redshift or Azure Synapse.
Data Pipeline (highly desirable): Demonstrable experience working with data from a wide variety of data sources including different database platforms, flat files, API’s and event-driven data feeds. Experience building complex data transformations ideally using dbt. Experience working with large data volumes, near real-time or event-driven data would be an advantage. Knowledge of programming languages such as Python would be beneficial.
Additional Desirable Technical And Other Skills
CI\CD & Automation (desirable): Any experience developing or supporting data CI\CD pipelines regardless of tooling would be beneficial. We use Microsoft Azure DevOps to run most of our CI\CD pipelines. We also rely heavily on Infrastructure as Code for cloud infrastructure deployment so any experience with technology such as Terraform would be beneficial in this respect.
Data Visualisation (desirable): Although we have dedicated data visualisation specialists within the team, any knowledge of, or experience with, data visualisation platforms such as Tableau (preferred), Power BI, Looker or Quicksight would be beneficial.
Although not required for this role, any prior experience acting as a mentor for more junior members of a team would be beneficial.
What Can We Offer You?
We have been recognised as one of the
Top 50 Best Places to Work in the UK
on
Glassdoor
and offer our valued colleagues a range of benefits including: -
Competitive salary, with annual pay review
Contributory pension scheme
34 days holiday including Bank Holidays per annum (pro rata dependent upon contracted hours)
3 x salary life assurance
Generous Discretionary Profit Share Scheme
Colleague discounts on Jet2holidays and Jet2.com holidays and flights
Cycle to Work Scheme
Access to Mental Health First Aiders
Many retail discounts on – travel and leisure, health, and wellbeing, eating out, shopping and lifestyle
Mon - Fri 08:30 - 17:30 (discretionary 15:00 Friday finish)
Remote Working, Office-based or Hybrid
We offer an
excellent remuneration package
with
fantastic opportunities for progression
in a growing business.
This is a great opportunity to be part of an exciting forward-thinking business. We operate scheduled leisure flights to holiday destinations in the Mediterranean, the Canary Islands and to European Leisure Cities from our 10 UK bases.
Help us to send our all-important customers on holiday with
Jet2.com
and
Jet2holidays!!
Show more
Show less","SQL, Cloud Platforms (GCP AWS Azure), Data Warehousing (Snowflake Google BigQuery AWS Redshift Azure Synapse), Data Pipeline (dbt Python), CI/CD & Automation (Microsoft Azure DevOps Terraform), Data Visualization (Tableau Power BI Looker Quicksight), Agile (Scrum Kanban), Communication, Mentoring, Problem Solving, DataDriven Culture","sql, cloud platforms gcp aws azure, data warehousing snowflake google bigquery aws redshift azure synapse, data pipeline dbt python, cicd automation microsoft azure devops terraform, data visualization tableau power bi looker quicksight, agile scrum kanban, communication, mentoring, problem solving, datadriven culture","agile scrum kanban, cicd automation microsoft azure devops terraform, cloud platforms gcp aws azure, communication, data pipeline dbt python, data visualization tableau power bi looker quicksight, data warehousing snowflake google bigquery aws redshift azure synapse, datadriven culture, mentoring, problem solving, sql"
Senior Data Analyst,In Technology Group,Greater Leeds Area,https://uk.linkedin.com/jobs/view/senior-data-analyst-at-in-technology-group-3768701924,2023-12-17,York, United Kingdom,Mid senior,Onsite,"Senior Data Analyst – £45,000 – Ilkley (Hybrid Options)
My client, an award nominated Data Analytics provider based in Ilkley are looking to acquire a new Senior Data Analyst for their highly successful team. This role will see you working within an expanding team and collaborating with industry giants on exciting Data Analysis, Data Management, and Category Projects. This position is perfect for a candidate who is keen to use their initiative, is driven, and always eager to develop their skillset further.
This role will be based onsite (at least initially) at my clients site in Ilkley, with hybrid working to be discussed post probatory period.
Responsibilities:
Develop and maintain analytical models and data sets to support business decisions
Utilise data from multiple sources to identify trends and patterns in order to provide insights
Implement data mining techniques to analyse and interpret patterns from data
Develop and maintain reports and dashboards for executives and users
Collaborate with stakeholders to define data requirements
Design and develop data visualisations to communicate insights
Provide technical guidance and training for other data analysts
Develop data models to enable the analysis of complex data
Analyse large volumes of data to identify trends and patterns
Help other or mentor junior team members
Provide in depth customer or buying insights aiding business intelligence / sales teams and improving performance
Manage you own end-to-end reporting projects
Requirements:
4+ years professional experience – desirable areas would be Data, Insights, Research, Marketing, or Commercial Analysis
Advanced experience with SQL, Power BI, & DAX
Transferable skills for the wider team e.g. Python, R, Tableau, Qlik
Knowledge of ETL Processes
Cloud exposure Azure/AWS is a bonus
Client facing or Customer facing skills/experience is highly desirable
Benefits:
Excellent development and growth opportunities
Progression plans
Team building events / Days out
Client trips to New York, Hong Kong, Paris, London, and more
Chance to attend the Annual Trade show in Cannes, South of France
Exceptional workplace culture
If the above role is of interest and you’ve got the required experience, apply with your most up to date CV for immediate consideration and interview!
Alternatively, if you would like to find out more about the position, reach out to me on 0113 526 6892 or at ethan.chesterfield@intechnologygroup.com
Show more
Show less","SQL, Power BI, DAX, Python, R, Tableau, Qlik, Data visualization, Business Intelligence, ETL Processes, Cloud computing, Azure, AWS, Machine Learning, Data modeling, Data mining","sql, power bi, dax, python, r, tableau, qlik, data visualization, business intelligence, etl processes, cloud computing, azure, aws, machine learning, data modeling, data mining","aws, azure, business intelligence, cloud computing, data mining, datamodeling, dax, etl, machine learning, powerbi, python, qlik, r, sql, tableau, visualization"
"Senior Cloud Data Engineer - GBP70,000",Nigel Frank International,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-cloud-data-engineer-gbp70-000-at-nigel-frank-international-3763118451,2023-12-17,York, United Kingdom,Mid senior,Onsite,"Senior Cloud Data Engineer - £70,000
I am working with a market leading manufacturer of specialist medical equipment who are looking for a Senior Cloud Data Engineer at the start of a number of exciting data driven projects. You would be joining at the start of a greenfield project where you will be responsible for the data migration, development and maintenance of the brand new Azure Data Platform!
The overall goal of the business and ultimately this role is to deliver data to healthcare professionals to aid with rehabilitation of patients. You will join a close-knit team, with talented professionals who specialise in a number of areas including software development and data science. You will be the in house expert for the team on all things data engineering and will take the lead on the creation, development and maintenance of the Azure data platform.
As part of this role, you will be responsible for some of the following areas.
Design, maintain and optimize both on premise and cloud based database solutions.
Develop and maintain robust ETL pipelines.
Work with other specialists within the team to make technical decisions that will benefit the overall business.
Take the lead of migration of data to the new Azure data platform.
This is a salaried role paying up to £70,000 per annum depending on experience and a company benefits package. This role would be largely a remote opportunity with occasional visits to the office in Stafford. These visits would be as and when needed in line with project requirements.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Strong coding experience with languages such as Python or C#.
Experience working as a DBA or completing database administration tasks
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure Data Factory, Synapse, Azure Data Lake, ETL, Python, C#, Database Administration, Azure tech stack, Azure Data Platform, SQLBits, Power Platform World Tour, London Power BI User Group, Newcastle Power BI User Group, Newcastle Data Platform and Cloud User Group","azure data factory, synapse, azure data lake, etl, python, c, database administration, azure tech stack, azure data platform, sqlbits, power platform world tour, london power bi user group, newcastle power bi user group, newcastle data platform and cloud user group","azure data factory, azure data lake, azure data platform, azure tech stack, c, database administration, etl, london power bi user group, newcastle data platform and cloud user group, newcastle power bi user group, power platform world tour, python, sqlbits, synapse"
Lead Customer Data Engineer,Harnham,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-customer-data-engineer-at-harnham-3778031120,2023-12-17,York, United Kingdom,Mid senior,Onsite,"LEAD DATA ENGINEER
LEEDS, 2 DAYS IN THE OFFICE PER WEEK
UP TO £85,000 + BENEFITS
PERMANENT
Are you an expert at using Pyspark and Databricks? Do you have a passion for not only honing your technical skills, but also sharing your expertise with eager up-and-comers in the data engineering world? This role is a fantastic opportunity to work with a well-known company and improve your managerial capabilities.
THE COMPANY
This company is a leading retailer that is optimising its loyalty program and enhancing customer personalisation through predictive analytics and machine learning. To help champion this within the business, they're expanding their data engineering team and are seeking a lead engineer to manage a team of 2.
THE ROLE
Manage a team of 2 data engineers.
Working closely with product owners, DS's and stakeholders.
Develop and productionise robust data products which integrate into ML and customer impact systems.
Implement CI/CD pipelines.
Developing and writing code for data products which will then drive decision making.
YOUR SKILLS AND EXPERIENCE
Experience productionising CI/CD pipelines (MLOps or DevOps).
Experience managing, mentoring or coaching a small team of data engineers.
Advanced knowledge and commercial experience using Pyspark and Databricks.
Cloud Experience, preferably with Azure.
Strong communication skills.
THE BENEFITS
A salary of up to £85,000.
Flexible working - 3 days WFH a week.
Working in a data driven team that is expanding and focused on collaboration.
10% employee discount
THE PROCESS
1st Stage - 30 minute CV run through with a team member.
Tech test at home.
2nd Stage - Chat around Tech test and case study within this.
HOW TO APPLY
Please register your interest by sending your CV to Riccardo via the apply link on this page or contact me to hear more about the role:
07488889389
Show more
Show less","Pyspark, Databricks, CI/CD pipelines, MLOps, DevOps, Azure, Cloud, Machine learning, Predictive analytics, Customer personalization, Data engineering, Data products, Coding, Communication, Management, Mentoring, Coaching","pyspark, databricks, cicd pipelines, mlops, devops, azure, cloud, machine learning, predictive analytics, customer personalization, data engineering, data products, coding, communication, management, mentoring, coaching","azure, cicd pipelines, cloud, coaching, coding, communication, customer personalization, data engineering, data products, databricks, devops, machine learning, management, mentoring, mlops, predictive analytics, spark"
Data Analyst Sr,Stericycle,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-sr-at-stericycle-3768316744,2023-12-17,York, United Kingdom,Mid senior,Onsite,"About Us
At Stericycle, we deliver solutions and drive innovations that protect the environment, people, and public health. This includes working to create a more sustainable, shared future. Our innovative solutions make a difference in people’s lives, communities, and our planet by protecting their health and well-being.
Operating from 20 sites across the UK and Ireland, Stericycle and its Shred-it brand are both widely recognized as the UK’s leading healthcare waste specialist & information security solutions provider. Built on unrivaled knowledge and expertise, we are the clear leader in the delivery of safe, compliant, and sustainable solutions to an ever-increasing customer base.
Join us on our mission to protect health and well-being in a safe, responsible and sustainable way.
Position Purpose
To provide data and application support for existing and future systems, initiatives, acquisitions and upgrades. To facilitate mass internal production system changes. To develop and maintain service level agreements for escalated support requests pertaining to Data Management Support. In line with business requirements, maintain data integrity, governance and security for existing and future systems. For existing and future applications, act as an intermediary between all Group businesses and the IT Department. Supporting the data integrity of the Data Warehouse by identifying and revising source data issues. To use SQL and Business Intelligence tools to produce analysis of Data based on in house Information Systems. Assessing and assisting in the implementation of new or upgraded software and assisting with strategic decisions on new systems.
Key Job Activities
Serve as an escalation point for data and application related support requests.
To participate in the maintenance of all Database Systems.
Generating reports from single or multiple systems.
Coordinate and perform mass data changes by developing queries to solve data related. discrepancies and assist the business to perform mass change.
Perform data integrity gap analysis of key problem areas to assist in root cause analysis.
Audit data on a regular basis to ensure data integrity and quality.
Develop documentation and how to guides to support established processes and service level agreements.
Develop reports as required to help business users understand and monitor data quality issues.
Education
Bachelors or master’s in computer science or any other Information Technology related discipline.
Experience (EMEAA)
2+ years of required experience
Targeted experience with financial systems, transportation logistics, warehousing, and/or telephony systems are desirable.
Proficient in MS Office (Excel, Access, Word etc.)
Demonstrating an advanced knowledge and understanding of SQL
Microsoft SQL Server Management Studio
Use of SSIS for task automation
Use of Information Steward for Data Quality Assessment (Desirable)
Use of SSRS to build reports (desirable)
Use of middleware/ETL tools e.g., Informatica, MuleSoft, DB Amp etc (Desirable
Source Control/Microsoft Team Foundation Server (Desirable)
Benefits
Certifications and/or Licenses:
Stericycle Offers You
Contributory Pension Scheme
Life Insurance
Cycle to Work Scheme
Access to SteriCares, our employee support fund
Stericycle University – Our online library of self-development & learning
Annual performance related pay review.
Referral Scheme (Earn by introducing people in your network to the Stericycle family)
Flu voucher
Eye Test voucher
And more…
Disclaimer
The above description is meant to provide a summary of the nature and level of work being performed; it should not be construed as an exhaustive list of all responsibilities, duties and requirements of the job. This document does not create an employment contract, implied or otherwise. Stericycle will consider requests for workplace accommodations for protected physical or mental limitations in accordance with its human resources policies and local laws. To the extent permissible under local law, and consistent with business necessity, Stericycle reserves the right to modify the content formally or informally, either verbally or in writing, at any time with or without advance notice.
Show more
Show less","Data Management, SQL, Business Intelligence, Data Quality Assessment, SSIS, SSRS, Informatica, MuleSoft, DB Amp, ETL tools, Microsoft Team Foundation Server, Microsoft SQL Server Management Studio","data management, sql, business intelligence, data quality assessment, ssis, ssrs, informatica, mulesoft, db amp, etl tools, microsoft team foundation server, microsoft sql server management studio","business intelligence, data management, data quality assessment, db amp, etl tools, informatica, microsoft sql server management studio, microsoft team foundation server, mulesoft, sql, ssis, ssrs"
HR Data Analyst,SYSTRA,"York, England, United Kingdom",https://uk.linkedin.com/jobs/view/hr-data-analyst-at-systra-3784783985,2023-12-17,York, United Kingdom,Mid senior,Onsite,"Around the world, SYSTRA’s specialists plan, design, integrate, test, commission, project manage and deliver mass transit and mobility solutions that are relied on by more than 50 million people every day.
For more than 60 years, the Group has been committed to helping cities and regions contribute to their development by creating, improving and modernising their transport infrastructure with sustainability, accessibility and innovation at the heart of our designs. With over 10,300 colleagues globally and around 1000 in the UK & Ireland we are growing significantly and seeking out the very best talent to join the SYSTRA signature team and be part of leading the way in infrastructure design.
Due to our continued growth, we have a great opportunity for a HR Data Analyst to join our team in the York office. In this role you will be Responsible for the quality of HR systems data and HR reporting activities.
You will develop the HR analytics dashboard and produce a suite of monthly HR metrics for the business, as well as respond to business wide requests for people data.
The ideal candidate will have a strong working knowledge of GDPR governance and will ensure all HR data is protected and aligned across our systems to accurately reflect business position. You will also support key stakeholders with analytical data to support strategic decision making.
Main Duties
Maintenance of data in HR systems
Regularly carry out data cleansing activities to maintain integrity of data
Create and produce monthly HR metrics
Review and develop metrics packs to ensure data meets changing business requirements
Produce in-depth and meaningful people reports
Interpret complex data and track trends
Support audit activities
Work closely with HR and Payroll specialists to support data and system activities
Support annual data updates, e.g. pay review, bonus, holiday entitlements
Ad-hoc system updates and data requests as required
Requirements
Used to working with sensitive data and the guidelines that support it
Analytical with the ability to manipulate, present and analyse data
Used to auditing and reconciling data across systems
Able to produce and present key metrics that are engaging and clear
Advanced user of business systems such as Word, Excel, PowerPoint
Used to working with databases, trackers and electronic files
Work accurately, with attention to detail
A strong communicator – both written and verbal
Used to organising your own time and working to deadlines
Flexible with the ability to juggle tasks to meet changing priorities
Familiar with HR Operations activities including onboarding, payroll and employee benefits
Why SYSTRA?
Our three core values: Excellence, Connected Teams and Bold Leadership are kept at the heart of everything we do. We strive for the highest levels of technical excellence, achieving the best results through teamwork, both locally and internationally, and reward innovative thinking through encouraging all colleagues to think as leaders. We offer clear and well supported pathways for career development and qualification attainment, a competitive remuneration package including a bonus and private healthcare, an electric car scheme and a broad suite of flexible benefits to suit your lifestyle.
Flexibility
Because we value who you are as much as what you do, we want you to feel supported at work. We offer hybrid working, balancing the benefits of working from home with time in our modern, well-equipped offices for that crucial in-person contact, team development and collaborative working. We also recognise that life doesn’t always run to the same schedule for everyone, so we have a dedicated flexible working policy to support you in tailoring your work life to reach your full potential, for more details on this, please get in touch.
Diversity & Inclusion
We provide a warm welcome and encourage applications from a diverse range of people who can enrich our company and offer perspectives which will drive better solutions in a truly inclusive environment. Our employee led working groups and clearly defined strategy help us to keep at the leading edge of important issues and keep us accountable. We want our colleagues to feel comfortable to bring their whole selves to work and continually seek new ideas and contributions to ensure we continue to grow and develop in this space.
Wellbeing
It’s no surprise that people do their best work when they feel physically and mentally supported. Our SYSTRA Wellness Programme offers a wide range of support; from Wellness champions, to free health checks, healthy eating workshops and regular seminars, alongside access to a wide range of external support. We offer two paid days for charity work and an ever-growing social calendar.
About
Apply and find out more about how Systra can support you in your career journey. If you require any adjustments or financial assistance to support you in your application or interview process please email:
reasonableadjustments@systra.com where your request will be treated confidentially and with respect. We pledge to offer an interview to any candidates with a disability who apply for a role and meet the minimum criteria.
As we are always looking to expand our team, we would still be interested in hearing from you if you fulfil most, but not all, of the relevant criteria for the role – you could still be just what we are looking for.
Show more
Show less","Data Analysis, HR Systems, HR Metrics, Reporting, Data Cleansing, Analytics, GDPR Governance, Data Manipulation, Data Presentation, Auditing, Reconciling, Business Systems, Databases, Trackers, Electronic Files, Onboarding, Payroll, Employee Benefits, Hybrid Working, Flexible Working, Diversity & Inclusion, SYSTRA Wellness Programme","data analysis, hr systems, hr metrics, reporting, data cleansing, analytics, gdpr governance, data manipulation, data presentation, auditing, reconciling, business systems, databases, trackers, electronic files, onboarding, payroll, employee benefits, hybrid working, flexible working, diversity inclusion, systra wellness programme","analytics, auditing, business systems, data manipulation, data presentation, dataanalytics, databases, datacleaning, diversity inclusion, electronic files, employee benefits, flexible working, gdpr governance, hr metrics, hr systems, hybrid working, onboarding, payroll, reconciling, reporting, systra wellness programme, trackers"
Data Architect,Harnham,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-architect-at-harnham-3781939789,2023-12-17,York, United Kingdom,Mid senior,Onsite,"To Apply for this Job Click Here
DATA ARCHITECT
REMOTE FIRST - 1X A MONTH IN THE UK (MIDLANDS/NORTHERN OFFICES)
£60,000-£70,000 +BONUS +BENEFITS
The Company
This organisation is a leader in the property sector, having large developments all across the UK and have a developing data team. You will play a key role in developing key data models for various business stakeholders.
THE ROLE
Responsibilities
As a Data Architect, you will sit in the Data Engineering team and report to the Senior Data Engineer. Some of your main responsibilities will be:
Work closely with data engineers and other technical professionals to ensure the data is modelled effectively within Azure
Align architecture principles to what the business needs are - ensuring the alignment of data models
Converse with various stakeholders across the business to understand business requirements for the data models
Work within Azure DataFactory, SQL, ETL and Synapes
Your Skills And Experience
A successful Data Architect will have:
Previous experience building data models but business purposes
Experience within Azure DataFactory would be ideal, SQL, and ETL is a must
Understanding and experience working with non-technical stakeholders and understanding the requirements from stakeholders
3+ years experience in this space
Educated to a degree level or above in a STEM subject.
The Benefits
A salary of £60,000-£70,000.
Very flexible working - remote first
Comprehensive bonus and benefits package.
INTERVIEW PROCESS
1 STAGE CONVERSATION WITH HIRING MANAGER
2ND STAGE CONVERSATION WITH STAKEHOLDERS FOR CULTURE FIT
How To Apply
Please register your interest by sending your CV to Lydia via the apply link on this page.
To Apply for this Job Click Here
Show more
Show less","Data Modeling, SQL, Azure DataFactory, ETL, Synapse, Stakeholder Engagement, Data Engineering, Azure, Business Intelligence, Cloud Computing, Data Architecture","data modeling, sql, azure datafactory, etl, synapse, stakeholder engagement, data engineering, azure, business intelligence, cloud computing, data architecture","azure, azure datafactory, business intelligence, cloud computing, data architecture, data engineering, datamodeling, etl, sql, stakeholder engagement, synapse"
Associate/Principal Mechanical Engineer - Data Centres,Hydrock,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/associate-principal-mechanical-engineer-data-centres-at-hydrock-3752873504,2023-12-17,York, United Kingdom,Mid senior,Onsite,"Job Advert
We have a fantastic opportunity for a Principal or Associate Mechanical Engineer to join our established Data Centre specialist team within our successful national MEP division.
You will have the opportunity to develop your consulting engineering expertise with a clear plan for progression, honing your skills within commercial management, client service and people leadership skills.
The MEP Team
We excel in delivering MEP engineering design services from project inception to construction completion, emphasizing early involvement, collaboration with the design team and tailored solutions. We serve a wide range of sectors including commercial, industrial, leisure, retail, manufacturing, logistics, heritage and residential, for end-user clients, investors, developers, architects, project managers or engineer-led teams. You can check out MEP’s project portfolio here.
Responsibilities Of The Role
Undertake the design of data centre specific mechanical systems from RIBA 1 through to RIBA 4c. Be fully aware of the current technical trends in the sector and be able to apply these to new projects.
Lead projects (or elements of large projects) and take responsibility for the technical delivery and financial profitability of the project.
As the lead discipline for data centre projects, you will also manage sub-disciplines and external contractors, assisted by the team’s design managers.
Be an expert in the sector and support with the training/upskilling of the Hydrock MEP team.
Assist in the marketing of the team externally and be able to generate fee income for new and existing clients.
You’ll be set up for success if you have:
Proven experience as a Principal or Associate Mechanical Engineer with a strong track record in the UK leading mechanical (ideally M&E) delivery on a variety of Data Centre building service projects.
Relevant mechanical engineering qualification at HNC, HND, Bachelors or Masters level.
Ideally a chartered engineer or working towards membership with a relevant institution.
Sound knowledge of BIM processes, Revit, IES and other design software.
Working knowledge of current building regulations with a strong understanding and passion to apply sustainable design principles.
A passion for sharing knowledge and developing junior members of the team.
Experience of winning work, managing teams, and undertaking business operations such as marketing, commercial and financial responsibilities (Associate level only).
What's Great About Hydrock 'in a Nutshell'
We are a British-owned integrated multi-disciplinary engineering consultancy of over 900 staff in 21 offices across the UK. Our driving motivation is to be a ‘Force for Good’, as it is our aim to improve the quality of people’s lives from our employees to our clients, through to the communities we work in and our planet as a whole through the work we do.
From the buildings that surround you, the roads and bridges you cross, all related to the infrastructure that we create, we aim to offer the most sustainable possibilities to shape the places, communities and society that we live in through meeting our client’s needs. Through the path we are on to delivering a green future, the result is something that everyone can be proud of.
Our welcoming and friendly culture is something we are proud of and has gained us recognition with 9 years in the Top 100 Best Companies to Work For list. Check out some of our incredible projects which have been awarded: 2022 Net Zero Award for Bay Technology Centre, Integration and Collaborative Working Award for YGG Tan-y-Lan primary school, The Deaf Academy awarded for its Universal Design at the 2022 Civic Trust Awards and Bristol’s iconic waterfront Wapping Wharf Living making a double win at the Bristol Property Awards!
To top off, here’s our 2022 wrap up video!
What We Can Offer You
Inspiring and supportive colleagues
Reward for progression and hard work
An opportunity to develop your soft skills, as well your technical skills
Competitive starting salary
Excellent health benefits 25 days of holiday (buy/sell up to five days), accrue 1 day extra every 2 years, with bonus holidays too!
An earlier finish on Friday (4pm!)
An opportunity to give back: “Day off for good cause” (on a workday)
A huge range of flexible benefits, including climate perks and an EV car leasing scheme
Our biggest event: Challenge Day!
A place to feel included
We champion diversity, equity and inclusion. As an Equal Opportunities Employer, we commit to supporting our employees and ensure we create a safe environment that nurtures you to perform at your best. Offering our people flexibility is an important factor in achieving this aim.
We consider all application individually with the required qualifications and knowledge without regard to any of the protected characteristics. We would like to provide everyone with a fair selection, assessment and employment experience so we ask with are made aware of any physical or neurodiverse condition within your application for which appropriate reasonable adjustments can be made by us.
Looking for the next steps?
Once you have completed your application through our careers site, we aim to review and respond to you as soon as your application’s been reviewed.
If shortlisted, a member of our Recruitment Team will call you for an initial pre-screen by phone (typically 30 minutes) to help us assess your motivations and interest in the position and Hydrock.
If you progress following this telephone pre-screen, you will be invited to attend a formal interview by video conference (Microsoft Teams) or in our offices.
For the latest updates and news, connect with us on our LinkedIn page!
Department
MEP
Contract type
Permanent
Hours
37.5 Hours
Salary
Competitive
Show more
Show less","BIM processes, Revit, IES, Project management, Team leadership, M&E design, Data center design, Commercial management, Client service, Sustainable design, Building regulations, AutoCAD, Financial management, Marketing, Business operations","bim processes, revit, ies, project management, team leadership, me design, data center design, commercial management, client service, sustainable design, building regulations, autocad, financial management, marketing, business operations","autocad, bim processes, building regulations, business operations, client service, commercial management, data center design, financial management, ies, marketing, me design, project management, revit, sustainable design, team leadership"
"Senior Data Engineer (Python, Informatica, ETL) REMOTE | £60k",Energy Jobline,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-python-informatica-etl-remote-%C2%A360k-at-energy-jobline-3773892977,2023-12-17,York, United Kingdom,Mid senior,Remote,"Senior Data Engineer (Python, Informatica, ETL)
Huge Data Transformation programme
No1 leader in the UK within their industry
Fully Remote UK | Up to £60k
Are you a Data Engineer who is absolutely at the top of their game.. Looking to lead data strategy and shape a data landscape for a well-known UK brand working on large projects on a truly enterprise-scale?
Keep reading....
Who are my client?
Well, they're a massively well-recognised brand here within the UK but they're now also making great waves into the US market too. They are utterly THRIVING and are classed the Number1 brand in the UK, in regards to their sector. Yep, they also have been named Number 1 Place to work in the UK a couple of times too. Trust me - This is a brand you want to work for.
They're also a private owned firm - The environment here is very fast, lean and they work with a ""fail fast"" approach where they are always open to trying new ideas and explore better ways of working.
You simply couldn't join at a better time. These guys are going through a HUGE transformation which will significantly reshape their Data strategy, taking it to the very next level…
And this is where you come in.
Who are they looking for?
What they are looking for is simply - A Senior, battle-scarred Data Engineer who can take the reigns on their Data Strategy and know exactly what to do, to evolve and improve it. It goes without saying this is a very autonomous role where you will be supported by x1 other Data enthusiast. So, this individual will will be expected to wear many hats within this role..
Firstly, you'll know Python well and you'll be equipped to make changes at an architectural level too. You will also have full ownership of their ETL pipelines ensuring a really smooth flow of data through its transformations. Ideally you'll be a Cloud enthusiast too and know AWS, with a good understanding of its data services - like Glue and S3.
These guys mean business and they are investing heavily into their Data offering by leveraging industry-leading tools. They have chosen the Enterprise Cloud Data Management Informatica to support their new Data Platform, so we'd absolutely love for you to have some knowledge of this tool.
You'll also manage their Data Warehouse and orchestrate data extraction from a wide range of sources and drive data analysis and report generation - to ensure you provide the wider business with solid, valuable business insights.
As an individual?
You will live and breathe Data Engineering and be up-to-speed on all the latest going-ons in the technology world! You'll thrive from working in a busy and fast paced environment… Where requirements can often change at a moments notice.
I am looking for the type of Data Engineer who is technically inquisitive and has a genuine thirst for learning. After all, my client will encourage and support technical training/certifications in areas that excite you and will allow you to do your job better- They genuinely want to get the best out of their staff and this is what makes this company such a fabulous place to work.
You'll also back yourself - in regards to your ideas and process. You'll be confident in presenting business insights and new tech ideas to the business, ensuring your voice is both heard and valued.
You'll join a large, wider talented Dev and Change team - Scope for learning here is vast - They have a really substantial estate so opportunity to put own stamp on your work is MASSIVE!
Cracking role, cracking team and working for a true market-leader. We can offer up to £55,000 plus £5k bonus- all dependant on your ability.
You can work fully remotely anywhere within the UK.
Call me now on (phone number removed) for immediate consideration.
Modis International Ltd acts as an employment agency for permanent recruitment and an employment business for the supply of temporary workers in the UK. Modis Europe Ltd provide a variety of international solutions that connect clients to the best talent in the world. For all positions based in Switzerland, Modis Europe Ltd works with its licensed Swiss partner Accurity GmbH to ensure that candidate applications are handled in accordance with Swiss law.
Both Modis International Ltd and Modis Europe Ltd are Equal Opportunities Employers.
By applying for this role your details will be submitted to Modis International Ltd and/ or Modis Europe Ltd. Our Candidate Privacy Information Statement which explains how we will use your information is available on the Modis website
Show more
Show less","Python, AWS, Data Engineering, Data Strategy, ETL, Informatica, Cloud Data Management, Data Warehouse, Data Analysis, Business Insights, Data Extraction, Report Generation, Glue, S3","python, aws, data engineering, data strategy, etl, informatica, cloud data management, data warehouse, data analysis, business insights, data extraction, report generation, glue, s3","aws, business insights, cloud data management, data engineering, data extraction, data strategy, dataanalytics, datawarehouse, etl, glue, informatica, python, report generation, s3"
Senior Data Analyst (Data & Reporting),NG Bailey,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-data-reporting-at-ng-bailey-3783147699,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"NG Bailey
are currently seeking a
Senior Analyst
to join our
Facilities Services
team in
Leeds
. Reporting to the Energy & Asset Management Director you will lead the development of business reporting and analytics to support Facilities Services transition into becoming a data led organisation. The role will lead strategic projects that focus on optimising business performance by leveraging the power of data analytics in day-to-day operations and senior management reporting.
Extensive stakeholder engagement will be required across IT, operations, and management teams to develop effective data visualisations that lead users to insightful action.
This is a hybrid role based at our Leeds office.
Core Responsibilities Include
Experience in delivering project programmes on time, and in line with stakeholder expectations
Ability to prioritise conflicting priorities and work under pressure to deliver projects on time
Self-motivated, with strong problem-solving skills
Develop informative Power BI dashboards that deliver the highest quality data and insight to drive better decision-making across the business.
Create optimal solutions to visualise and report analytic outputs to a diverse and senior level of stakeholders
Interested in how the Facilities Services business works and a drive to improve performance. Ability to understand business requirements and transform these ensuring the product is relevant & effective
Drive thought leadership around data visualisation and storytelling
Effective line management of a small, agile, team.
Thought leadership on data management, management information and business intelligence.
Excellent team working and willingness to proactively engage people and teams to deliver objectives
Ideal applicants will have solid understanding of
Azure Cloud Services
,
SQL
, Microsoft
Power BI
and have experience
deploying AI/ML Algorithms
and with
data integration
,
cleansing
and
quality
assurance
. Strong
stakeholder management
,
project management
and skills inpresenting/reportingtosenior stakeholders will also be required in this role.
Benefits
Competitive Salary
25 Days holidays
Pension with leading provider
Private healthcare
Discounts
About Us
We are one of the leading independent engineering and services businesses in the UK. Founded in 1921, with a turnover of £500m and 3000 employees, we are proud of our history of developing great people through our investment in training.
Working across a variety of sectors within the building and infrastructure industry, our innovative, responsible and forward thinking approach allows us to work on fantastic ground-breaking projects, providing solutions using the latest tools and technologies.
Progression is something we value and we’ll make sure that when you join us you have a clearly defined development path, supported by regular reviews, training and ongoing support to enable you to be the best you can be.
Apply now
Show more
Show less","Azure Cloud Services, SQL, Power BI, AI/ML Algorithms, Data integration, Data cleansing, Data quality assurance, Stakeholder management, Project management, Presenting, Reporting","azure cloud services, sql, power bi, aiml algorithms, data integration, data cleansing, data quality assurance, stakeholder management, project management, presenting, reporting","aiml algorithms, azure cloud services, data integration, data quality assurance, datacleaning, powerbi, presenting, project management, reporting, sql, stakeholder management"
Lead Data Cabling Engineer,Hamilton Barnes 🌳,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-cabling-engineer-at-hamilton-barnes-%F0%9F%8C%B3-3776945414,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"Role
Do you want the opportunity to work for a national award-winning Managed Service Provider ? Well, this is your chance!
This Leading Electrical service Provider are looking for a Lead Data cabling engineer to come & join their ever-growing team.
You will be testing, installing, and terminating Cat6/ Cat6a data cables. This will include completing fixed wire testing and fault finding. Furthermore, you will be installing patch panels and working with fibre cables.
Take the chance to work with an established Electrical company that gives you the opportunity to progress.
If this role sounds like the next step in your career, then please apply here!
Responsibilities
Be happy to travel UK Wide
Daily Cat5/ Cat6/ Cat6a data cabling
Running cables from cabinets to end point.
Installing patch panels
Terminating & testing cables
Maintenance of various IT Equipment
Fault finding and Fixed wire testing
Experience And Skills
Data cabling Cat5/ Cat6 Cabling
fixed wire testing
Fault finding
Cary an ECS Card
Fibre cabling experience is beneficial but not essential
Salary Details
Basic Salary up to £34,000
Vehicle and fuel card
Paid accommodation
Holiday package
Pension scheme
Overtime available
Show more
Show less","Data Cabling, CAT5, CAT6, CAT6a, Fixed Wire Testing, Fault Finding, Patch Panels, Fibre Cabling, Maintenance of IT Equipment","data cabling, cat5, cat6, cat6a, fixed wire testing, fault finding, patch panels, fibre cabling, maintenance of it equipment","cat5, cat6, cat6a, data cabling, fault finding, fibre cabling, fixed wire testing, maintenance of it equipment, patch panels"
Ground Operations Executive - Data Analysis & Administration,Jet2.com and Jet2holidays,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/ground-operations-executive-data-analysis-administration-at-jet2-com-and-jet2holidays-3787321046,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"We have a fantastic opportunity to join our Central Ground Operations Team at our Head Office at Leeds Bradford Airport on a Permanent basis!
As a
Ground Operations Executive – Data Analysis & Administration
, you will play a pivotal role in supporting our Ground Operations team’s technical aspects, with a strong emphasis on data analysis, interpretation, and reporting. Reporting to the
Ground Operations Supervisor – Technical
, you will be responsible for ensuring the efficient administrative and data-related functions of our Central Ground Operations team. Your contributions will be instrumental in maintaining our high standards of safety, compliance, and punctuality, as well as ensuring we continuously deliver a
VIP Customer Experience
throughout the Ground Operations network.
What will I do in the role?
Data Management - which will include collecting, processing and maintaining operational data from various sources and developing and maintaining reporting tools to track operational metrics and KPI’s. Conduct regular data quality checks and ensure accuracy and consistency
of data.
Data Analysis and Reporting - Analysing Operational and Safety related data. Preparing comprehensive dashboards and ad hoc reports and providing actionable insights based on data analysis. Prepare presentations to communicate findings to the management team.
Continuous Process Efficiency Improvement - Identifying opportunities to improve and propose improvements to enhance departmental efficiency. Identify opportunities to improve data management and administrative workflows, using process automation technology.
Administration Support - Coordinate and manage the departmental SharePoint site along with various administrative tasks. Supporting in scheduling meetings and assisting with departmental travel arrangements and other logistical requirements and/or requests.
System Maintenance - Control and management of the departmental systems. Oversight and review of the departmental self-monitoring tools and the issuance of any associated documentation. Ensuring clear and effective communications to all stations via the creation and maintenance of departmental documentation and newsletters.
Other duties including the drafting and issuing of GOIs and other departmental instructions to make sure our stations are informed of policies and procedures.
What Skills and Experience do I need?
The successful candidate must have proven experience in data analysis, interpretation, and reporting. You will possess the capability to identify trends within operational data and draw meaningful insights. You will have excellent attention to detail with a focus on accuracy and adherence. Proficiency in data analysis tools and software, such as Tableau, Power BI, or equivalent is required. Excellent communication and interpersonal skills are required to effectively interact with internal and external stakeholders across different cultures and language barriers, whilst using the correct communication channels. Whilst not mandatory, the ability to speak a foreign language, ideally Spanish, would be a distinct advantage.
Help us send our all-important customers on holiday!!
Show more
Show less","Data Management, Data Analysis, Reporting, Data Quality Control, Tableau, Power BI, Process Efficiency Improvement, SharePoint, System Maintenance, Documentation, Communication, Interpersonal Skills, Foreign Language (Spanish)","data management, data analysis, reporting, data quality control, tableau, power bi, process efficiency improvement, sharepoint, system maintenance, documentation, communication, interpersonal skills, foreign language spanish","communication, data management, data quality control, dataanalytics, documentation, foreign language spanish, interpersonal skills, powerbi, process efficiency improvement, reporting, sharepoint, system maintenance, tableau"
Senior Data Engineer,Direct Line Group,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-direct-line-group-3766784434,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"About Us
Fuelled by our passion for using data to solve customer problems, we’re working to be the top-tier data-driven organisation in the UK’s General Insurance market. With Analysts, Engineers and Developers working together in an agile environment, our Data Enabling Tribe is at the heart of our mission. Building a new, enterprise-scale, cloud-hosted, secure and trusted analytics platform, we’re using the economics of big data, cloud elasticity, machine learning and AI automation. And we’re turning information into business insights to provide even greater customer support.
The Data Chapter
Our team is at the heart of this mission, and we are looking for people passionate about using data to solve customer problems and making the organisation data driven. We are working with our teams to build our new enterprise-scale, cloud hosted, secure and trusted analytics platform. The platform uses the economics of big data, cloud elasticity, machine learning (ML)/Artificial Intelligence (AI) automation and to turn information into business insights so that we can support our customers better.
Who You’ll Be Working With
You will be working closely with data engineers, analysts, and other stakeholders to understand their data requirements and provide the necessary data infrastructure and support. You will be reporting to the Data Chapter Lead.
What you’ll need:
Proven experience as a Senior Data Engineer with expertise in Python, PySpark, AWS Glue.
Extensive experience in designing ETL (Extract, Transform, Load) processes for data ingestion and integration.
Proficiency in Python and Spark for data processing & optimization techniques.
Hands-on experience in dbt models
Hands-on experience in orchestrations tools such as Airflow
Strong experience with AWS services, including but not limited to S3, Glue, Lambda, EC2, and IAM.
Hands-on experience with CI/CD tools and practices, such as Jenkins & Git
Experience with data warehousing solutions (e.g., Redshift, Snowflake) and data lake architecture.
Strong problem-solving skills, attention to detail, and the ability to work in a fast-paced, collaborative environment.
Excellent communication and teamwork skills, with the ability to explain complex technical concepts to non-technical stakeholders.
What you'll be doing:
As a PySpark & AWS Glue Data Engineer at DLG you will be responsible for the below:
Data Pipeline Development:
Design, develop, and maintain robust data pipelines using Python/ PySpark and AWS Glue to extract, transform, and load data from various sources into our data lake or warehouse. C reate and maintain dbt models and transformations to ensure data consistency, accuracy, and integrity .
Orchestration:
Create, configure, and maintain Apache Airflow workflows to automate data pipelines and other operational processes.
Define dependencies and schedules for tasks within workflows to ensure smooth execution.
Data Quality Assurance:
Implement data quality checks and validation processes to maintain data integrity, accuracy, and consistency.
AWS Infrastructure Management:
Manage and maintain AWS infrastructure resources, including EC2 instances, Lambda functions, Glue jobs, and data storage solutions.
Continuous Integration and Deployment (CI/CD):
Implement and maintain CI/CD pipelines to automate and streamline the deployment of data pipelines, code changes, and infrastructure updates.
Monitoring and Optimisation:
Monitor the performance of data pipelines using Spark UI and otherwise, AWS resources, and data storage, and proactively optimise for efficiency and scalability.
Performance Tuning:
Continuously optimize data pipelines, SQL queries, and dbt models to improve data processing efficiency.
Security and Compliance:
Implement security best practices and ensure compliance with data protection regulations, working with the security and compliance teams.
Documentation:
Maintain detailed documentation for data pipelines, infrastructure,
configurations, and DevOps processes.
What we’ll give you in return:
We wouldn’t be where we are today without our people and the wide variety of perspectives and life experiences they bring. That’s why we offer excellent benefits to suit your lifestyle and a flexible working model combining the best parts of home and office-working, varying with the nature of your role. Core benefits include:
9% employer contributed pension
50% off home, motor and pet insurance plus free travel insurance and Green Flag breakdown cover
Up to 10% annual bonus
25 days holiday (rising by 1 each year to 28) + bank holidays and option to buy or sell up to 5 days
Additional optional Health and Dental insurance
EV car scheme allows all colleagues to lease a brand new electric or plug-in hybrid car in a tax efficient way.
Buy as you earn share scheme
Employee discounts and cashback
Plus many more
Ways of Working
Our mixed model way of working offers a 'best of both worlds' approach combining the best parts of home and office-working, offering flexibility for everyone. How much you'll be in the office depends on your role, and we'll consider the flexible working options that work best for you.
Read our flexible working approach here.
We recognise and embrace people that work in different ways so if you need any reasonable adjustments within this recruitment process, please reach out to us and we can discuss how we can support you with this.
There’s no-one else like you. No-one with the exact same mix of strengths, quirks, skills and thoughts. That’s why you could belong here. As part of a team of brilliant individuals, in a place that empowers you to be the best you can be. We’re proud of who we are, of what we do, and what every single one of us brings.
Join us
. Help us keep innovating and putting customers at the heart of everything. To be an insurance company of the future. When we work together, we can all achieve great things. Inspiring, challenging, and supporting each other to aim higher.
Together we’re one of a kind.
Show more
Show less","Python, PySpark, AWS Glue, ETL, dbt, Airflow, AWS, Redshift, Snowflake, Spark UI, CI/CD, Apache Airflow, EC2, Lambda, IAM, Jenkins, Git, S3","python, pyspark, aws glue, etl, dbt, airflow, aws, redshift, snowflake, spark ui, cicd, apache airflow, ec2, lambda, iam, jenkins, git, s3","airflow, apache airflow, aws, aws glue, cicd, dbt, ec2, etl, git, iam, jenkins, lambda, python, redshift, s3, snowflake, spark, spark ui"
Senior Data Engineer,hackajob,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-hackajob-3759174898,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"hackajob is a matching platform partnering with Hippo Digital helping them to hire the best talent and build the future.
As a Senior Data Engineer, you will be working as part of a multidisciplinary team to build solutions that make data accessible to enable solutions to be optimised by using an evidence-based approach. Engaging with our clients you will design and implement data solutions ensuring solutions are integrated with internal systems and business processes.
Your role in a nutshell:
Act as an SME within the Hippo squad to lead, design and implement data solutions that meet business requirements.
Help architects realise data system design such as Meshes, Warehouses and Event based systems
Implement data flows to connect operational systems, data for analytics and business intelligence (BI) systems
Re-engineer, develop and optimise code to ensure processes perform optimally
Build great relationships with your team and stakeholders
Work with the Hippo community to share best practice to ensure high standards
Involved in the recruitment and on-boarding of other engineers, and support other consultants in their professional development
Promote Hippo’s Engineering Herd internally and externally (for example through writing Blogs, Workshops, Seminars or Conferences)
Skills and experience that you need:
Proven track record in Python
Proficiency in R, Bash, Java/.NET and/or PowerShell desirable
Experience in at least one Cloud Platform (AWS, GCP or Azure)
Broad knowledge across cloud architectures, networking and distributed computing systems
Experience of data system design such as Data Lakes, Data Meshes and Data Warehouses.
Experience of a wide range of data sources, SQL, NoSQL and Graph.
A proven track record of infrastructure delivery on any data platform (Snowflake, Elastic, Redshift, Data Bricks, Splunk, etc).
Strong and demonstrable experience writing regular expressions and/or JSON parsing, etc.
Strong experience in log processing (Cribl, Splunk, Elastic, Apache NiFi etc.)
Expertise in the production of dashboard/insight delivery
Be able to demonstrate a reasonable level of security awareness (An understanding of basic security best practices, OAuth, MFA, TLS, etc.)
Experience in the processing of large datasets
A firm understanding of data modelling and normalisation concepts
Good estimation skills for times, latencies and costs.
Experience working within multidisciplinary teams desirable
Teamwork and presentation skills, experience of mentor led working practices
What makes us great
As well as a competitive salary which we’re transparent about from the outset, you can also expect a range of benefits:
Contributory pension scheme (Hippo 6% with employee contributions of 2%)
25 days holiday plus UK public holidays
Perkbox access for a wide range of discounts
Critical illness cover
Life assurance and death in service cover
Volunteer days
Cycle-to-work scheme for the avid cyclists
Salary sacrifice electric vehicles scheme
Season ticket loans
Financial and general wellbeing sessions
Flexible benefits scheme with options of:
private health cover
private dental cover
additional company pension contributions
additional holidays (up to an extra 2 days)
wellbeing contribution
charity contributions
tree planting
How to apply:
Click
""Apply""
to get to the application page and click on
""Get Matched""
Create a profile by filling out the form so the company can consider you for the role
Show more
Show less","Python, R, Bash, Java, .NET, PowerShell, AWS, GCP, Azure, Data Lakes, Data Meshes, Data Warehouses, SQL, NoSQL, Graph, Snowflake, Elastic, Redshift, Data Bricks, Splunk, Log processing, Cribl, Apache NiFi, Dashboard delivery, Data modelling, Normalisation, Multidisciplinary teams, Teamwork, Presentation skills, Mentor led working practices","python, r, bash, java, net, powershell, aws, gcp, azure, data lakes, data meshes, data warehouses, sql, nosql, graph, snowflake, elastic, redshift, data bricks, splunk, log processing, cribl, apache nifi, dashboard delivery, data modelling, normalisation, multidisciplinary teams, teamwork, presentation skills, mentor led working practices","apache nifi, aws, azure, bash, cribl, dashboard delivery, data bricks, data lakes, data meshes, data modelling, data warehouses, elastic, gcp, graph, java, log processing, mentor led working practices, multidisciplinary teams, net, normalisation, nosql, powershell, presentation skills, python, r, redshift, snowflake, splunk, sql, teamwork"
Senior Data Analyst,SPG Resourcing,"West Yorkshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-spg-resourcing-3766073798,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"My client, a leading provider of comprehensive automation software to regulated industries worldwide, seeks a Senior Data Analyst to join its Projects and Analytics Team. This role focuses on leveraging Data intelligence to enhance their product portfolio, ensuring the delivery of cutting-edge financial solutions to clients.
Key Responsibilities
Identification of areas for product enhancement and optimization
Utilization of statistical models for accurate planning and forecasting
Collaboration with stakeholders to manage deadlines and expectations
Transformation of diverse datasets into actionable business insights through comprehensive reporting.
Execution of solution testing of full reconciliation
Evaluation of business model strengths, weaknesses, and exposure, with recommendations for strategic directions
Continuous professional development and utilization of relevant resources and networks.
Requirements
Required Qualifications:
4 to 5 years’ experience in Data Analysis
Proficiency in T-SQL query writing
Experience in crafting advanced reports using SSRS, Power BI, Tableau, or similar tools
Advanced skills in Microsoft Office applications, especially Excel, Access, and SQL languages.
Preferred experience in SSIS/SSAS and VBA
Advantageous to have experience in financial services, specifically debt recovery or insolvency.
Strong analytical and reporting capabilities, presenting Data clearly and concisely.
Adaptable and flexible work style.
Excellent written and verbal communication skills, fostering string relationships with colleagues and clients
Ability to work autonomously and collaboratively in a team environment
Effective workload management, prioritization, and performance under pressure to meet tight deadlines
Show more
Show less","Data Analysis, TSQL, SSRS, Power BI, Tableau, Microsoft Office, Excel, Access, SQL, Data intelligence, Statistical models, Business insights, Solution testing, Business model evaluation, Financial services, Debt recovery, Insolvency, Reporting, Communication, Teamwork, Workload management","data analysis, tsql, ssrs, power bi, tableau, microsoft office, excel, access, sql, data intelligence, statistical models, business insights, solution testing, business model evaluation, financial services, debt recovery, insolvency, reporting, communication, teamwork, workload management","access, business insights, business model evaluation, communication, data intelligence, dataanalytics, debt recovery, excel, financial services, insolvency, microsoft office, powerbi, reporting, solution testing, sql, ssrs, statistical models, tableau, teamwork, tsql, workload management"
Solutions Architect - Data,Jet2.com and Jet2holidays,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/solutions-architect-data-at-jet2-com-and-jet2holidays-3535287072,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"At
Jet2.com
and
Jet2holidays
we’re working together to deliver an amazing journey, literally! We work together to really drive forward a ‘Customer First’ ethos, creating unforgettable package holidays and flights. We couldn’t do it without our wonderful people.
As our new
Solutions Architect - Data
, you’ll have a major impact on helping us achieve our goal. You’ll become a key member of our data architecture forum and wider architecture forum which helps guide technology choices and building blocks across the whole of our logical architecture function. You’ll work with our architecture colleagues across data and applications to ensure that simple, effective and future-focused designs are created, with data at the very heart of everything that we do.
You’re probably now asking: What’s in it for me? As our
Solutions Architect - Data
, you’ll have access to a wide range of benefits including:
26 days holiday (plus Bank Holidays)
Access to a generous discretionary profit share scheme
Colleague discounts on Jet2holidays and Jet2.com flights
Don’t just take our word for it check our reviews on
Glassdoor
!
What You’ll Be Doing
Become a domain expert within your pillar, understanding how the business operates within that area, and building relationships to bridge technical and business areas, becoming a trusted technical ally to the business pillar
Be involved with new initiatives as they are defined from the business to understand what a high-level solution might look like
Identify areas where we need to define building blocks or new technology, and allowing high-level estimates to be defined
Be able to evaluate technology options and build POCs to support your designs
What You’ll Have
Have general knowledge of cloud native computing, public hyperscalers (Azure, AWS or GCP)
Have sound knowledge on modern data solutions such as data lakes, data platforms, data streaming, and data security best practices
Show an interest in (new) technologies, methodologies, and concepts such as data governance & management, data mesh, data vault 2.0, delta lake, DWH automation, event streaming
Experience in a technical role with extensive exposure to data; data architect, data engineer, data analyst, software engineer/testing, DevOps engineer, data scientist, or similar relevant experience
Join us as we redefine travel experiences and create memories for millions of passengers. At
Jet2.com
and
Jet2holidays
, your potential has no limits. Apply today and let your career take flight!
'
Show more
Show less","Cloud native computing, Azure, AWS, GCP, Data lakes, Data platforms, Data streaming, Data security, Data governance, Data management, Data mesh, Data vault, Delta lake, DWH automation, Event streaming, Data architect, Data engineer, Data analyst, Software engineer, Testing, DevOps engineer, Data scientist","cloud native computing, azure, aws, gcp, data lakes, data platforms, data streaming, data security, data governance, data management, data mesh, data vault, delta lake, dwh automation, event streaming, data architect, data engineer, data analyst, software engineer, testing, devops engineer, data scientist","aws, azure, cloud native computing, data architect, data governance, data lakes, data management, data mesh, data platforms, data scientist, data security, data streaming, data vault, dataanalytics, dataengineering, delta lake, devops engineer, dwh automation, event streaming, gcp, software engineer, testing"
Senior Data Analyst,G.Digital,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-g-digital-3770947448,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"Senior Data Analyst | Leeds (Hybrid) | £35-58k + 10% bonus
If you're a Data Analyst who likes working on end-to-end projects that have a big impact on the company then you'll want to read on...
The business
This global software house has a shedload of products that big players in the financial services industry use on a day-to-day to drive value in their own businesses, while also providing consultancy in the space too.
Over the past 20 years they've gone from being based solely in the UK to:
Being based out of 12 countries
Won countless awards
300+ people strong
Acquired several businesses
Now have over 20 products in their software suite
The role:
All of the above means they need a Senior Data Analyst who can proactively work with stakeholders and clients to identify areas that data can add real value.
From conception, you'll work with stakeholders to understand their challenges and then pull together the data you need, work your magic to find useful insight, then report back to them with your findings and tell a meaningful story with data.
You'll be in a dead social team based in central Leeds and would love someone equally personable.
What you need:
Several years of experience in an Analytics role
Dashboarding experience (Power BI would be useful)
Strong SQL
Strong communicator
Commercially minded
Strong relationship builder
Any experience with finance data would be useful but not a deal breaker
What's on offer:
Brilliant L&D opportunities - the current team lead started as an analyst 4 years ago and got promotion after promotion
Bonus
Private health
Leeds city centre office - dead social team with incentives and social events
Either apply or drop me an inmail/email if you have any Q's - z.q-lomax@g-digital.co.uk
Senior Data Analyst | Leeds (Hybrid) | £35-58k + 10% bonus
Show more
Show less","Data Analytics, Reporting, Data Interpretation, Storytelling, SQL, Power BI, Dashboarding, Stakeholder Collaboration, Finance Data","data analytics, reporting, data interpretation, storytelling, sql, power bi, dashboarding, stakeholder collaboration, finance data","dashboard, data interpretation, dataanalytics, finance data, powerbi, reporting, sql, stakeholder collaboration, storytelling"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3733886589,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader in the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on Databricks. Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too.
You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration too. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks.
There is great room for progression within this role, and support and training will be provided as well.
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, SQL, Python, PySpark, Azure Data Factory","databricks, sql, python, pyspark, azure data factory","azure data factory, databricks, python, spark, sql"
Data Engineer,BJSS,"York, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-3440131186,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
About the Role
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","Software Engineering, Data Engineering, Python, Cloud Computing, AWS, Azure, GCP, DataOps, Data Platforms, CI/CD, ObjectOriented Programming, Unit Testing, Relational Databases, NonRelational Databases, Data Storage, Data Processing, Parallel Computing, Workflow Orchestration, SQL, NoSQL","software engineering, data engineering, python, cloud computing, aws, azure, gcp, dataops, data platforms, cicd, objectoriented programming, unit testing, relational databases, nonrelational databases, data storage, data processing, parallel computing, workflow orchestration, sql, nosql","aws, azure, cicd, cloud computing, data engineering, data platforms, data processing, data storage, dataops, gcp, nonrelational databases, nosql, objectoriented programming, parallel computing, python, relational databases, software engineering, sql, unit testing, workflow orchestration"
Data Engineer,BJSS,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-3253821510,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
About the Role
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","Software Engineering, Python, Data Processing, Cloud Data Services, Parallel Computing, ObjectOriented Programming, Logging, Monitoring, Code Versioning, Error Handling, DataOps, CI/CD Tooling, Git, AWS, Azure, GCP, Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion","software engineering, python, data processing, cloud data services, parallel computing, objectoriented programming, logging, monitoring, code versioning, error handling, dataops, cicd tooling, git, aws, azure, gcp, databricks, data factory, synapse, kafka, redshift, glue, athena, bigquery, s3, cloud data fusion","athena, aws, azure, bigquery, cicd tooling, cloud data fusion, cloud data services, code versioning, data factory, data processing, databricks, dataops, error handling, gcp, git, glue, kafka, logging, monitoring, objectoriented programming, parallel computing, python, redshift, s3, software engineering, synapse"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3728586040,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader in the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on Databricks. Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too.
You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration too. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks.
There is great room for progression within this role, and support and training will be provided as well.
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, SQL, Python, PySpark, Azure Data Factory, Data Engineering, Data Pipelines, Big Data, Cloud Computing, Data Warehouse, Data Architecture, Software Development, Programming, Data Integration, ETL","databricks, sql, python, pyspark, azure data factory, data engineering, data pipelines, big data, cloud computing, data warehouse, data architecture, software development, programming, data integration, etl","azure data factory, big data, cloud computing, data architecture, data engineering, data integration, databricks, datapipeline, datawarehouse, etl, programming, python, software development, spark, sql"
Lead Data Engineer,Harnham,"Leeds, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-harnham-3781944404,2023-12-17,York, United Kingdom,Mid senior,Hybrid,"To Apply for this Job Click Here
LEAD DATA ENGINEER
LEEDS, 2 DAYS IN THE OFFICE PER WEEK
UP TO £75,000 + BENEFITS
PERMANENT
Are you an expert at using Pyspark and Databricks? Do you have a passion for not only honing your technical skills but also sharing your expertise with eager up-and-comers in the data engineering world? This role is a fantastic opportunity to work with a well-known company and improve your managerial capabilities.
The Company
This company is a leading retailer that is optimising its loyalty program and enhancing customer personalisation through predictive analytics and machine learning. To help champion this within the business, they're expanding their data engineering team and are seeking a lead engineer to manage a team of 2.
THE ROLE
Manage a team of 2 data engineers.
Working closely with product owners, DS's, and stakeholders.
Develop and productionise robust data products that integrate into ML and customer impact systems.
Implement CI/CD pipelines.
Developing and writing code for data products which will then drive decision making.
Your Skills And Experience
Experience productionising CI/CD pipelines (MLOps or DevOps).
Experience managing, mentoring, or coaching a small team of data engineers.
Advanced knowledge and commercial experience using Pyspark and Databricks.
Cloud Experience, preferably with Azure.
Strong communication skills.
The Benefits
A salary of up to £75,000.
Flexible working - 3 days WFH a week.
Working in a data-driven team that is expanding and focused on collaboration.
10% employee discount
THE PROCESS
1st Stage - 30 minute CV run-through with a team member.
Tech test at home.
2nd Stage - Chat around Tech test and case study within this.
How To Apply
Please register your interest by sending your CV to Riccardo via the apply link on this page or contact me to hear more about the role:
07488889389
To Apply for this Job Click Here
Show more
Show less","Python, PySpark, Databricks, Azure, Machine Learning, Predictive Analytics, Customer Personalization, CI/CD Pipelines, DevOps, MLOps, Data Products, Decision Making, Cloud Computing, Communication Skills, Data Engineering, Team Management, Mentoring, Coaching","python, pyspark, databricks, azure, machine learning, predictive analytics, customer personalization, cicd pipelines, devops, mlops, data products, decision making, cloud computing, communication skills, data engineering, team management, mentoring, coaching","azure, cicd pipelines, cloud computing, coaching, communication skills, customer personalization, data engineering, data products, databricks, decision making, devops, machine learning, mentoring, mlops, predictive analytics, python, spark, team management"
Healthcare Data Analyst,Milliman,"San Diego, CA",https://www.linkedin.com/jobs/view/healthcare-data-analyst-at-milliman-3699405298,2023-12-17,Coronado,United States,Mid senior,Onsite,"About Milliman
Independent for over 75 years, Milliman delivers market-leading services and solutions to clients worldwide. Today, we are helping companies take on some of the world’s most critical and complex issues, including retirement funding and healthcare financing, risk management and regulatory compliance, data analytics and business transformation.
Through a team of professionals ranging from actuaries to clinicians, technology specialists to plan administrators, we offer unparalleled expertise in employee benefits, investment consulting, healthcare, life insurance and financial services, and property and casualty insurance.
Milliman's San Diego Healthcare Technology practice is looking for a Healthcare Analyst to support data warehouse projects, analytical services, claims analysis, and creating client reports. The ideal candidate has strong critical thinking and technical skills to contribute to a variety of projects. The analyst will gain experience using a wide variety of qualitative and quantitative healthcare data to support client needs.
Job Responsibilities
Key responsibilities will include the following:
Work on data loading for data warehouse projects
Provide analytic services, primarily but not exclusively associated with the analysis of healthcare claims data
Help develop and refine analytic algorithms to analyze claims
Conduct analyses for clients and draft reports explaining the results
Conduct projects as determined by clients or consultants
Be able to work independently and but also participate actively as a member of various project teams, fostering the sharing of relevant information and helping to build consensus collaboratively
Experience Desired
The ideal candidate should demonstrate experience using the following skills to design and develop analytical solutions:
2 to 5 Years of Experience and knowledge of healthcare data and analytics
Strong technical capabilities, especially with Microsoft tools and technologies; this includes using Microsoft business analytics tools, and perhaps delivering solutions that include:
Microsoft Office components
Experience with SQL
Experience with Azure Databricks
Experience with Microsoft Visual Studio
Strong experience with database tools and technologies
Applicants’ Requirements/Qualifications
Strong SQL, Azure Databricks, Visual Studio experience
Proficiency with basic statistics and management reporting
Knowledge of, and experience with, healthcare claims data and their nomenclatures
Milliman Benefits
At Milliman, we focus on creating an environment that recognizes – and meets – the personal and professional needs of the individual. We offer a competitive benefits package which includes:
Medical, dental and vision coverage for employees and their dependents, including domestic partners
A 401(k) plan with matching program, and profit sharing contribution
Employee Assistance Program (EAP)
A discretionary bonus program
Paid Time Off (PTO) starts accruing on the first day of work and can be used for any reason; full-time employees will accrue 15 days of PTO per year, and employees working less than a full-time schedule will accrue PTO at a prorated amount based on hours worked
Family building benefits, including adoption and fertility assistance and paid parental leave up to 12 weeks for employees who have worked for Milliman for at least 12 months and have worked at least 1,250 hours in the preceding 12-month period
A minimum of 8 paid holidays
Milliman covers 100% of the premiums for life insurance, AD&D, and both short-term and long-term disability coverage
Flexible spending accounts allow employees to set aside pre-tax dollars to pay for dependent care, transportation, and applicable medical needs
Location
This position will be based out of the Milliman office in San Diego. Applicants must be willing to work onsite in the Milliman office.
Compensation
The salary range for this role is $62,000 to $99,200, depending on a combination of factors, including, but not limited to, education, relevant work experience, qualifications, skills, certifications, location, etc.
Milliman is an Equal Opportunity Employer
All qualified applicants will receive consideration for employment, without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","Healthcare data analytics, Microsoft Office components, SQL, Azure Databricks, Microsoft Visual Studio, Database tools and technologies, Basic statistics, Management reporting, Healthcare claims data, Nomenclatures","healthcare data analytics, microsoft office components, sql, azure databricks, microsoft visual studio, database tools and technologies, basic statistics, management reporting, healthcare claims data, nomenclatures","azure databricks, basic statistics, database tools and technologies, healthcare claims data, healthcare data analytics, management reporting, microsoft office components, microsoft visual studio, nomenclatures, sql"
Database Engineer/Administrator,TCWGlobal (formerly TargetCW),"San Diego, CA",https://www.linkedin.com/jobs/view/database-engineer-administrator-at-tcwglobal-formerly-targetcw-3752372901,2023-12-17,Coronado,United States,Mid senior,Onsite,"Title
: Database Administrator
Category
: Full-Time
Status
: Salary, Exempt
Reports to
: AWS Solutions Architect
Start Date
: ASAP
Range
: $113,200 - $130,300
Position Summary
Get ready for a life-changing work experience! Welcome to TCWGlobal, the place where hard work meets endless fun. Not only are we the reigning champion of the ""Best Place to Work in San Diego"" for several years, but we've also claimed the title of ""Best Place to Work in Southern California""! That's right, we're on a roll and always raising the bar.
At TCWGlobal, we believe in the power of community and giving back. That's why we give every team member two whole days a year, with pay, to go out and make a difference in the world. We're not just building a business here; we're building a better future.
We are a team of experienced HR professionals offering payrolling, staffing, pre-screening, international services, and overall HR-related solutions for temporary workers and contractors. Our dedication to customer service and focus on utilizing technology for streamlining processes is our core philosophy. So, if you're ready to join a team of unstoppable go-getters, a company culture that encourages growth, and a company that truly cares about its employees and the world, then TCWGlobal is the place for you!
We are seeking an experienced, flexible, and team-oriented
Database Administrator.
This individual
will be
responsible for designing and implementing databases, creating new databases, and/or adjusting the function and capacity of existing databases. The Database Administrator will design efficient database engine processes for sorting and accessing information, using programming knowledge to build data pipelines and integrate new information into existing databases. This individual will be responsible for handling the general management of database(s). The Database Administrator will oversee user account access issues, data integrity, and the overall upkeep of database operations. They will help troubleshoot problems and supervise the flow of tasks within the database(s).
Key Responsibilities
· Administer and maintain database security, integrity, and optimization
· Develop new databases/applications and maintain/enhance the functionality and performance of existing database/SaaS programs.
· Monitor databases and related systems to ensure optimized performance and capacity issues.
· Establish and maintain backup and recovery processes
· Maintain associated non-referential data
· Review database and user reports, as well as system information.
· Perform debugging procedures on database scripts and programs and resolve conflicts.
· Adhere to best practices in securely storing, backing up, and archiving data.
· Document processes related to database design, configuration, and performance.
· Keep abreast of developments and best practices in database engineering.
Qualifications & Skills
4-7 years of experience in database administration
BS, MS, and/or PhD degree in computer science, math, or a related technical field
Thorough understanding of database performance monitoring and analysis tools
Expertise in database server specification, performance tuning, analysis, and optimization
Experience with AWS development required
Experience with MySQL, DynamoDB, Amazon RDS, Aurora, S3 required
Strong experience in database disaster recovery planning and recovery
Demonstrated ability to solve complex database administration problems
Ability to work with Developers, QA Engineers, and different operations teams to help them identify, analyze, and resolve database performance issues
Team-player, flexible and able to work in a fast-paced environment
Salary & Benefits
This position offers full-time benefits, including a comprehensive benefits package. Salary will be commensurate with education and experience.
Additional compensation may be earned by exceeding sales and service goals.
Application Information
Visit
https://www.tcwglobal.com/join-the-family
to learn more about our amazing team!
TCWGlobal is an equal opportunity employer. We do not discriminate based on age, ethnicity, gender, nationality, religious belief, or sexual orientation.
Show more
Show less","Database Administration, MySQL, DynamoDB, Amazon RDS, Aurora, S3, AWS, Database Security, Database Optimization, Database Performance Tuning, Database Disaster Recovery, Database Server Specification, Database Analysis, Database Design, Database Configuration, Data Pipelines, Programming Knowledge, Debugging Procedures, Database Scripts, Data Integrity, User Account Access","database administration, mysql, dynamodb, amazon rds, aurora, s3, aws, database security, database optimization, database performance tuning, database disaster recovery, database server specification, database analysis, database design, database configuration, data pipelines, programming knowledge, debugging procedures, database scripts, data integrity, user account access","amazon rds, aurora, aws, data integrity, database administration, database analysis, database configuration, database design, database disaster recovery, database optimization, database performance tuning, database scripts, database security, database server specification, datapipeline, debugging procedures, dynamodb, mysql, programming knowledge, s3, user account access"
Senior Data Scientist,Hilco Global,"Northbrook, IL",https://www.linkedin.com/jobs/view/senior-data-scientist-at-hilco-global-3731743147,2023-12-17,Hannibal,United States,Mid senior,Onsite,"Job Description:
Responsibilities:
The Data Scientist will report directly to the VP, Director of CRM. This role requires curiosity for leveraging Hilco Global and OPCO data to streamline business intelligence, sales enablement, and data accessibility. Effective collaboration in a team environment with the VP, Director of CRM, CRM Analyst, and other divisions as necessary to achieve data-driven solutions that transform Hilco and strengthen its data utility and accessibility for empowered decision-making. To foster a data intelligence practice that systemizes solutions through leading and lagging feedback loops.
This job requires to be on-site 5 days a week in Northbrook, IL***
Data Strategy
Advise service-level agreement (SLA) vendor partners on CRM database management, needs, and strategic guidance, which empower and position the CRM platform and sales enablement for success.
Create and execute a comprehensive, linear process of early opportunity identification, opportunity predictability to the financial forecasting model, which identifies inputs and outputs, and business financial impact, which informs business health.
Develop, maintain, and utilize the firm's data, data integrations, and data science tools to help produce business insights and support project initiatives.
Remain current in emerging automation, artificial intelligence, and machine learning trends that can streamline systems and address business challenges.
Dashboard Management:
Develop, manage, and hand off OPCO-specific ad-hoc, pipeline reports, and dashboards.
Design business intelligence dashboards with user experience (UX) best practices that synthesize and visualize data for an executive-level, decision-orientated audience.
Leverage disparate data sources such as CRM platform, Workday, etc., through API's and SQL for OPCO activity dashboards.
Data Research:
Mining pre-stressed, watch-list-worthy OPCO opportunity-target companies through headlines, financial, and secondary data providers. Leveraging various data providers: CapitalIQ, Pitchbook, SugarCRM, Ibis World, Statista, Credit Cloud, and ZoomInfo.
Cross-reference pre-stressed pre-opportunities with CRM platform business development activity, point of contact, date, and connections.
Identify, troubleshoot, and address data discrepancies and uphold the highest integrity and ethics.
Pull ad-hoc queries on industry data and financials.
Collaboration:
Be a role player to indirect reports, junior analysts, or support staff to maintain existing dashboards, reports, and aspiring data scientists.
Develop and maintain strong relationships with key clients at all levels within the firm and with key vendors.
Collaborating with and serving various Hilco Global and operating companies (OPCOs) business functions.
Additional Duties And Responsibilities As Required.
Characteristics:
Strong work ethic and interpersonal skills, emphasizing teamwork, collaboration, initiative, and integrity.
Detail-oriented and capable of working through large amounts of data efficiently.
Hands-on, resourceful, and proactive professional, able to work autonomously and operate as part of a strong, collaborative team.
Solid interpersonal communication skills and ability to work with employees at all levels of the organization.
Strong research abilities, ability to source company, contact data, and hunt down key information.
Excellent problem-solving skills.
Highly organized and detail-oriented individual who thrives in dynamic and fast-paced working environments.
Ability to prioritize and manage multiple projects/processes at one time.
A strong team player comfortable working in matrix environments.
Strong verbal and written communication skills.
Skills/Qualifications:
Master’s in a highly quantitative field (e.g., mathematics, statistics, computer science, or related fields) preferred.
Experience leading large data science projects, passionate about solving business challenges through data science.
Experience leading directly/indirectly other data scientists or aspiring data scientists.
Demonstrated research ability a plus.
Understanding of data partners such as PitchBook, Capital IQ, Credit Cloud, Bloomberg terminal, ZoomInfo, etc., is a plus.
Experience in a CRM (SugarCRM preferred), such as Salesforce, HubSpot, Microsoft, etc.
Ability to quickly pick up new data languages, emerging tools, and technology technologies.
Proficiency in PowerBI, SQL, MySQL is required, and related programming languages are a plus.
Experience in predictive modeling, data science, and analysis in both batch and streaming settings.
Experience in discrepancies detection, supervised/unsupervised learning, time-series data, and natural language processing.
Proficient with data visualization and translating complex problems into actionable insights.
Ability to maintain high confidentiality of personally identifiable information.
Proficient in Microsoft Excel, PowerPoint, Word, and Outlook.
Hilco Global is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Show more
Show less","Data Science, Business Intelligence, Sales Enablement, Data Accessibility, Collaboration, DataDriven Solutions, Data Integration, Artificial Intelligence, Machine Learning, Dashboard Management, Data Research, Data Mining, Data Analytics, Data Integrity, ProblemSolving, Communication, Teamwork, Initiative, Research, SQL, MySQL, PowerBI, Predictive Modeling, Natural Language Processing, Data Visualization, Microsoft Office Suite","data science, business intelligence, sales enablement, data accessibility, collaboration, datadriven solutions, data integration, artificial intelligence, machine learning, dashboard management, data research, data mining, data analytics, data integrity, problemsolving, communication, teamwork, initiative, research, sql, mysql, powerbi, predictive modeling, natural language processing, data visualization, microsoft office suite","artificial intelligence, business intelligence, collaboration, communication, dashboard management, data accessibility, data integration, data integrity, data mining, data research, data science, dataanalytics, datadriven solutions, initiative, machine learning, microsoft office suite, mysql, natural language processing, powerbi, predictive modeling, problemsolving, research, sales enablement, sql, teamwork, visualization"
Senior Data Engineer,Lever Middleware Test Company 2,"Ottawa, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-lever-middleware-test-company-2-3787334807,2023-12-17,Bonavista, Canada,Mid senior,Remote,"a Bit About Us
Lever, an Employ brand, has a vision of connecting human potential to meaningful work. Hiring is the most strategic challenge that every company faces, and Lever is one of the most recognized brands in talent acquisition software. As the innovation leader in our space, we are searching for great people to join us and push further.
With an overall gender ratio of 50:50 women and men, Lever is also fiercely committed to building a team culture that celebrates diversity and inclusion. We have been certified as a “Great Place to Work”, along with “Best Company Outlook 2022,” and “Forbes Best Startup Employers 2022,” and the list keeps growing! Our people are Lever’s biggest competitive advantage. For more information on our team culture, visit https://inside.lever.co/.
THE CHALLENGE
Lever is on a mission to provide the best analytics feature set in the world of Talent software. We’ve made good progress in the last 2 years, but we still have room to improve.
This is where you come in. You will be the technical lead for the engineering team that is evolving Lever’s analytics features. You will help Lever’s customers recruit more efficiently and more predictably by providing them with a reliable and accurate data set to analyze.
This position will focus on maintaining and improving our data pipelines while consulting with partners in product management and product design on building user facing analytics features. As part of that collaboration, you will be encouraged to use your expertise and conduct technical research to help inform the direction we take with our analytics products.
THE OPPORTUNITY
One of Lever’s top priorities is to make our analytics features a reason that people buy and stay with Lever. As the lead engineer on the Analytics team you will have the opportunity to have a big impact on the success of Lever’s product and Lever’s business. You’ll work collaboratively with partners on the analytics team to drive the implementation of features, and be a champion of best practices for writing well-tested, well-organized code.
In addition to working on a high impact product area, as part of your daily work you will have the opportunity to build expertise in multiple Business Intelligence tools. You will be able to exercise and strengthen your automation and optimization skills. You will have the opportunity to learn about state of the art data infrastructure systems like Kafka and Snowflake, and you will be able to leverage observability tools like Datadog and Kibana to observe our systems.
You will also have the opportunity to lead the development of user facing analytics features, depending on your preferences and prior experience.
THE TEAM
Lever’s reporting engineering team combines ETL engineers, product engineers, data analysts, designers, and product managers to build, define, and design analytics products for customers. You will join the team as the lead software engineer building and maintaining advanced analytics products for Lever’s customers.
You will collaborate closely with our brilliant data analysts who create dashboards in the product, and scrappy Talent Analytics Consultants provide custom analysis services for customers. You will use your data and engineering knowledge to help guide the product and design team that strives to build analytics features to connect human potential to meaningful work.
THE SKILL SET
6+ years of software engineering experience
Experience with data pipelines and both the common pitfalls and best practices for maintaining them is a plus
Professional experience with relational databases that speak SQL (Postgres, MySql, MSSql, Oracle, Snowflake, etc)
An understanding of Business Intelligence tools, like Tableau or Looker, and the problems they solve
Knowledge of Looker is a plus, a desire to learn Looker is a must
A desire to teach others with the communication skills needed to effectively share knowledge
A healthy appetite for being a technical mentor for other engineers
This role includes a lot of technical freedom, so the ability to research technical solutions beyond those that already exist in our code base is a plus
Empathy for users of the Lever product and internal users that work at Lever
WITHIN 1 MONTH, YOU'LL
Complete Ramp Camp, our onboarding program designed to get you up to speed on our business, vision, and team.
Learn about the technology behind Lever’s analytics features (Lever Talent Intelligence, Data Warehouse Sync, and Visual Insights)
Participate in the reporting team’s ceremonies and get to know your teammates
Meet internal users of Business Intelligence tools
Deploy your first code change
WITHIN 3 MONTHS, YOU'LL
Learn about Lever’s product and how the analytics feature fit into the full picture
Build familiarity with Lever’s application data model in Mongo, how it aligns with data in SQL, and how the our data pipelines perform the translation
Understand the existing observability tools and monitoring we have in place for our data pipelines
Participate in Lever’s analytics product development process and ship your first feature to customers
Improve tools that reporting engineers use to maintain data systems
WITHIN 6 MONTHS, YOU'LL
Find and fill gaps in observability and automated testing for reporting features
Engage in your first on-call rotation for the product to help diagnose and resolve production problems and customer issues
Collaborate with other product engineering teams to incorporate data generated by new features into our analytics products
Identify and fix large areas of improvement in our data pipelines and schema management tool set
Contribute new ideas to the long-term roadmap of the reporting team
WITHIN 12 MONTHS, YOU'LL
Help the data analysts improve performance of pre-built Looker dashboards with optimizations and pre-calculated aggregations
Improve the experience of data analysts by providing them with better data environments to use for development
Automate continuous data quality and end-to-end service testing of data pipelines
Incorporate custom access control rules into the data available to end users of Lever
Is this role not an exact fit?
Sign up to stay in touch, we'll let you know when we have new positions on the team.
The Lever Story
Lever, an Employ brand, is a leading Talent Acquisition Suite that makes it easy for talent teams to reach their hiring goals and to connect companies with top talent. Lever is the only platform that provides all talent acquisition leaders with complete ATS and robust CRM capabilities in one product, LeverTRM. The LeverTRM features allow leaders to scale and grow their people pipeline, build authentic and long-lasting relationships, and source the right people to hire. Lever Analytics provides customized reports with data visualization, see offers completed and interview feedback, and more, to inform strategic decisions between hiring managers and executives alike.
Our platform also enables companies to hire with inclusivity in mind, helping eliminate any hiring bias. Lever supports the hiring needs of over 5,000 companies around the globe including the teams at Netflix, Spotify, Atlassian, KPMG, and Nielsen. For more information, visit https://www.lever.co.
California residents applying for positions at Lever can see our privacy policy here.
#BI-REMOTE
Show more
Show less","Software Engineering, Data Pipelines, SQL, Relational Databases, Business Intelligence Tools, Tableau, Looker, Communication Skills, Technical Mentoring, Technical Research, Empathy, Mongo, Observability Tools, Monitoring, Data Quality, Endtoend Service Testing, Access Control, ETL, Kafka, Snowflake, Datadog, Kibana","software engineering, data pipelines, sql, relational databases, business intelligence tools, tableau, looker, communication skills, technical mentoring, technical research, empathy, mongo, observability tools, monitoring, data quality, endtoend service testing, access control, etl, kafka, snowflake, datadog, kibana","access control, business intelligence tools, communication skills, data quality, datadog, datapipeline, empathy, endtoend service testing, etl, kafka, kibana, looker, mongo, monitoring, observability tools, relational databases, snowflake, software engineering, sql, tableau, technical mentoring, technical research"
Senior Data Analytics Engineer,When I Work,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-analytics-engineer-at-when-i-work-3785765718,2023-12-17,Bonavista, Canada,Mid senior,Remote,"When I Work is a remote first company. We are open to hiring candidates in the continental US and Ontario, Canada. If an onsite location is important to you in your search, you are welcome to work from our Minneapolis HQ office.
Who We Are
We help hourly teams get shift done.
At When I Work, everything we do starts with a mission to make shift work awesome. We deliver on that mission by making every piece of hourly workforce management - scheduling, time tracking, shift trading, team messaging, and more - easy and straightforward for managers and employees alike.
The Data and RevOps team at When I Work is a group of inquisitive and driven individuals who love solving problems using data. We have built a best-in-class data environment and fuel insights throughout the organization on our product and customers. We work collaboratively together, invest in our processes and tooling, and move slow to move fast. We focus on projects that will have a big impact to the company and work to enable anyone to be a savvy data user.
What You'll Do
Over the last few years we have been building out a best-in-class data environment that we've used to transform When I Work into a data-driven company. You will be a key contributor extracting value from this environment as well as develop and sustain data projects and analysis that will have significant impact to our company and our users.
Proactively identify opportunities to build out solutions or use internally stored data to provide value to the business
Design and implement scalable ETLs
Own end to end design and implementation of analytical project scope to turn raw data into actionable insight
Work closely with internal teams across the business (Product, GTM, etc.) to understand their analytical needs and help empower them to find success in their initiatives through data
Clearly articulate analytical findings to both technical and non-technical stakeholders
Work with data visualization tools to help end users answer pertinent questions
Make use of statistical modeling / machine learning in order to advance business outcomes
Who You Are
As a Senior Data Analytics Engineer, you are eager to use data to solve business problems and guide data-driven decision making across an entire organization. You enjoy the end to end process of data analytics. You are a maker at heart who enjoys being in the code while also uncovering new product and business opportunities through whatever methods and tools are right for the task. You enjoy collaborating with a team, but you also have the ability to work independently when needed to get things done. Above all, you are broadly curious, driven to learn and a motivated problem solver who wants to help tackle the new and interesting challenges that we encounter as a fast-growing startup.
Experience And Skills Needed
3+ years of experience in data science or data analytics
You have strong programming fundamentals
You are comfortable with agile, collaborative coding processes
You can distinguish signal from noise in complex data sets
You have experience with Python or another, similar programming language
You have solid, hygienic SQL skills
You have experience modeling data to support both consistent and generic usage patterns
You have significant experience with both structured and unstructured data
Experience modeling data within OLAP relational data stores
You have familiarity with cloud computing environments and infrastructure
You have experience taking technical analytics work and presenting in a form that non-technical users are able to glean the significance of your findings
You practice empathy and kindness, and you look to help others
What Would Be Awesome To Have
Advanced Python and data package (Numpy, Pandas, etc.) skills
You are a proponent of DevOps and enthusiastic about DataOps
You are comfortable with different data modeling techniques (Star Schema, Snowflake, DV2, etc.)
Have experience with a data warehouse platform (Redshift, Snowflake, etc.)
Experience with S3 based data lake and parquet file format
What's In It For You
Professional development allowance
Dental Care & Extended Healthcare Coverage paid by When I Work
Enhanced paid parental leave
Paid vacation and holidays
Flexible work environment
RRSP Match
Remote first culture including home office set-up stipend and ongoing telecommuter stipend
Casual dress code
Dynamic and dedicated team
We believe actions speak louder than words. Every encounter with our people and products should be memorable and helpful. Challenges are exciting, failure is how we learn, and we all have an entrepreneurial spirit. Building an inclusive and equitable workplace isn't lip service. We invest our time and our money in organizations that are not only working to diversify the current jobscape, but also investing in the future of talent. We're motivated by a strong, innovative, and passionate work culture and we're constantly searching for ways to improve and get shift done.
Whether you're a perfect match or not, if it sounds like a good fit, we encourage you to apply.
The tech industry is notorious for its lack of diverse representation, and we're aware of the research showing that historically underrepresented groups are less likely to apply to a job if they don't believe that they meet all of the criteria. Are you hesitant to submit an application because you're not sure if you check every box? Apply anyway! We would love to hear from you and figure out what you can add to the culture here at When I Work.
We'd love to talk to you! Please submit the following to apply:
Resume (including months/years of employment for each position).
Cover letter including:
an overview of your existing experience
a convincing reason why you'd like to work at When I Work.
Must already be authorized to work in the United States or Canada on a full-time basis for any employer.
Show more
Show less","Data analytics, Data science, Python, SQL, Data modeling, Data visualization, Statistical modeling, Machine learning, Cloud computing, DevOps, DataOps, Star Schema, Snowflake, DV2, Data warehouse, Redshift, S3, Parquet","data analytics, data science, python, sql, data modeling, data visualization, statistical modeling, machine learning, cloud computing, devops, dataops, star schema, snowflake, dv2, data warehouse, redshift, s3, parquet","cloud computing, data science, dataanalytics, datamodeling, dataops, datawarehouse, devops, dv2, machine learning, parquet, python, redshift, s3, snowflake, sql, star schema, statistical modeling, visualization"
Senior Data Engineer,When I Work,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-when-i-work-3758250588,2023-12-17,Bonavista, Canada,Mid senior,Remote,"When I Work is a remote first company. We are open to hiring candidates in the continental US and Ontario, Canada. If an onsite location is important to you in your search, you are welcome to work from our Minneapolis HQ office.
Who We Are
We help hourly teams get shift done.
At When I Work, everything we do starts with a mission to make shift work awesome. We deliver on that mission by making every piece of hourly workforce management - scheduling, time tracking, shift trading, team messaging, and more - easy and straightforward for managers and employees alike.
The Data and RevOps team at When I Work is a group of inquisitive and driven individuals who love solving problems using data. We have built a best-in-class data environment and fuel insights throughout the organization on our product and customers. We work collaboratively together, invest in our processes and tooling, and move slow to move fast. We focus on projects that will have a big impact to the company and work to enable anyone to be a savvy data user.
What You'll Do
Over the last few years we have been building out a best-in-class data environment that we've used to transform When I Work into a data-driven company. You will be a key contributor in continuing to grow and mature this environment as well as develop and sustain data projects that will have significant impact on our company and our users.
Proactively identify opportunities to improve and update data platform infrastructure and research new technologies and strategies
Design, build and implement tools aimed at allowing business users to collect and analyze data in an efficient and effective way
Design, build and maintain integrations between our internal data platform and 3rd party tools utilized throughout the company
Develop and manage ETLs and data pipelines
Create data products for consumption by internal When I Work team members
Be part of a team that owns all aspects of its service delivery -- from cloud infrastructure, to application code, to operations
Our Technology Stack
We use a lot of different technologies to get the job done, and each member of our team brings their own mix of technology experience. If you have familiarity with even a few of these (or equivalents), you could make a valuable contribution: Python, Go, SQL, Terraform, Jupyter, Git, GitLab, Spark, Flink, Presto, Kafka, MySQL, NoSQL, Kubernetes, DBT, Prefect, Airflow, lots of AWS(EC2, EKS, Lambda, S3, RDS, DynamoDB, Aurora, Redshift, Athena, EMR, CloudSearch, Kinesis, API Gateway).
Who You Are
You are a programmer who is excited by data and its endless possible use cases. Someone that enjoys creating tools and infrastructure to empower your peers. Collaboration and teamwork are a must, but you also have the ability to work independently when needed to get things done. Above all, you are driven to learn and a motivated problem solver who wants to help tackle the new and interesting challenges that we encounter as a fast-growing startup.
Experience And Skills Needed
3+ years of experience in data engineering
You have strong programming fundamentals
You are comfortable with agile software processes
You have experience with multiple programming languages (Python, SQL, etc.)
Comfortable working with APIs/Webhooks
You have significant experience working with structured and unstructured data
You have significant experience with cloud computing environments and infrastructure
You are a proponent of DevOps and enthusiastic about DataOps
You practice empathy and kindness, and you look to help others
What's In It For You
Professional development allowance
Dental Care & Extended Healthcare Coverage paid by When I Work
Enhanced paid parental leave
Paid vacation and holidays
Flexible work environment
RRSP Match
Remote first culture including home office set-up stipend and ongoing telecommuter stipend
Casual dress code
Dynamic and dedicated team
We believe actions speak louder than words. Every encounter with our people and products should be memorable and helpful. Challenges are exciting, failure is how we learn, and we all have an entrepreneurial spirit. Building an inclusive and equitable workplace isn't lip service. We invest our time and our money in organizations that are not only working to diversify the current jobscape, but also investing in the future of talent. We're motivated by a strong, innovative, and passionate work culture and we're constantly searching for ways to improve and get shift done.
Whether you're a perfect match or not, if it sounds like a good fit, we encourage you to apply.
The tech industry is notorious for its lack of diverse representation, and we're aware of the research showing that historically underrepresented groups are less likely to apply to a job if they don't believe that they meet all of the criteria. Are you hesitant to submit an application because you're not sure if you check every box? Apply anyway! We would love to hear from you and figure out what you can add to the culture here at When I Work.
We'd love to talk to you! Please submit the following to apply:
Resume (including months/years of employment for each position).
Cover letter including:
an overview of your existing experience
a convincing reason why you'd like to work at When I Work.
Must already be authorized to work in the United States or Canada on a full-time basis for any employer.
Show more
Show less","Python, Go, SQL, Terraform, Jupyter, Git, GitLab, Spark, Flink, Presto, Kafka, MySQL, NoSQL, Kubernetes, DBT, Prefect, Airflow, AWS, EC2, EKS, Lambda, S3, RDS, DynamoDB, Aurora, Redshift, Athena, EMR, CloudSearch, Kinesis, API Gateway, Agile software processes, DevOps, DataOps, Cloud computing environments, Infrastructure","python, go, sql, terraform, jupyter, git, gitlab, spark, flink, presto, kafka, mysql, nosql, kubernetes, dbt, prefect, airflow, aws, ec2, eks, lambda, s3, rds, dynamodb, aurora, redshift, athena, emr, cloudsearch, kinesis, api gateway, agile software processes, devops, dataops, cloud computing environments, infrastructure","agile software processes, airflow, api gateway, athena, aurora, aws, cloud computing environments, cloudsearch, dataops, dbt, devops, dynamodb, ec2, eks, emr, flink, git, gitlab, go, infrastructure, jupyter, kafka, kinesis, kubernetes, lambda, mysql, nosql, prefect, presto, python, rds, redshift, s3, spark, sql, terraform"
Senior Data Engineer,FutureFit AI,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-futurefit-ai-3741585293,2023-12-17,Bonavista, Canada,Mid senior,Remote,"FutureFit AI is looking for a contract Senior Data Engineer to join our team. We have a culture of high trust, high impact, high velocity and a gritty determination to win. If you are passionate about development, like to have fun, do it right, and get things done we would love to hear from you!
An important note: Data shows that men on average apply for a role if they meet 3/10 requirements while women often only do so if it’s 10/10. We work hard to be clear and specific about what our roles include and demand and encourage you to apply if you see a strong (but not necessarily perfect) fit between you and the opportunity.
About You:
Technology - You are a passionate data engineer, eager to learn, solve problems and share your knowledge with others
Data - Strong architecture and design around various data paradigms (document stores, data lakes/warehouses and relational) and databases (Mongo, Elastic, Postgres, Redshift, etc.)
Data Query - Experience writing highly performant data queries for large/complex data sets across relational and NoSQL databases
Data Pipelines - Experience in architecture and implementation of ETL processes, streaming data, data process orchestration (Airflow)
BI - Adept at connecting BI platforms (Looker) to data warehouses to provide critical insights via dashboards
MLOps - Experience with model versioning, deployment, monitoring and serving models via REST APIs
Understanding - You are eager to understand the business problem and collaborate with product to build a solution that meets both technology and business requirements
Communication - You can share your perspective clearly and constructively, you are comfortable challenging ideas and collaborating to find the best approach
Grit - You like to solve challenging problems, are not afraid of the unknown, eager to learn and take a systematic approach to solving problems
AI - You are interested in integrating advances in AI into FutureFit AI products and the development process to drive improvements in velocity, quality and engineer experience
Values - You are a good person who shares our values of excellence, transparency, and humanity
The Product’s Technology Stack:
Front End - React, TypeScript, Tailwind, Storybook
Back End - Node.js, Nest.js, Python, Flask, GraphQL, Postgres, MongoDB, RedShift, Airflow
AWS - CloudFront, Cognito, ECS, RDS, S3, SQS, Elasticsearch, VPC, Lambda
Location:
Most days we work from home but everyone comes to the office at least once a week for face to face collaboration and team building. The office is located at 325 Front St West (a short walk from Union Station).
Our Company:
In looking at a job posting, it’s often hard to get a basic picture of the company profile (size, stage, structure, etc.) which is why we are sharing it with you upfront. This helps you quickly decide and helps us focus any time we spend together on going beyond the basics:
Funding
: We have raised one round of funding led by JP Morgan which fueled a significant growth trajectory for us and we have a safe financial runway to execute against.
Problem Domain:
Future of Work / Workforce Development - important that the problem domain interests you even if you haven’t worked in the space before.
Customers
: We partner with workforce development agencies, government agencies, and employers/enterprises.
Structure
: We are organized around the following key departments: Growth, Customer Success, Product, Engineering, Data, People & Culture, Finance & Operations.
Team
: We are a team of 30-50 across US and Canada with main hubs in New York and Toronto. This role will be based in Toronto.
Core Principles
: Be Curious, Drive to Outcomes, Raise the Bar, Speed Matters, Own It, Put We Over Me
About FutureFit AI
At FutureFit AI, we’re on a mission to unlock pathways between talent and opportunity using the power of AI. We focus on personalized, AI-powered career guidance for job seekers, emphasizing skills over extensive resumes, and partner with workforce development partners, governments, and employers to level access to opportunity.
FutureFit AI is a growing, venture-backed company focused on using technology to improve the lives and outcomes for people going through career transitions. We’re a small, driven team, united by our commitment to the job seekers and workforce ecosystems we serve. We're not just building a company; we're shaping the future of work.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Please contact us to request an accommodation.
©
FutureFit AI All rights reserved, we are proud to be an equal opportunity workplace. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, religion, color, gender identity, sexual orientation, age, disability, veteran status, or other applicable legally protected characteristics. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Show more
Show less","React, TypeScript, Tailwind, Storybook, Node.js, Nest.js, Python, Flask, GraphQL, SQL, NoSQL, MongoDB, PostgreSQL, Airflow, Data Query, AWS CloudFront, Cognito, ECS, RDS, S3, SQS, Elasticsearch, VPC, Lambda, Document Stores, Data Lakes/Warehouses, Relational Databases, ETL Processes, Streaming Data, Data Process Orchestration, BI Platforms, Dashboards, MLOps, Model Versioning, Deployment, Monitoring, REST APIs","react, typescript, tailwind, storybook, nodejs, nestjs, python, flask, graphql, sql, nosql, mongodb, postgresql, airflow, data query, aws cloudfront, cognito, ecs, rds, s3, sqs, elasticsearch, vpc, lambda, document stores, data lakeswarehouses, relational databases, etl processes, streaming data, data process orchestration, bi platforms, dashboards, mlops, model versioning, deployment, monitoring, rest apis","airflow, aws cloudfront, bi platforms, cognito, dashboard, data lakeswarehouses, data process orchestration, data query, deployment, document stores, ecs, elasticsearch, etl, flask, graphql, lambda, mlops, model versioning, mongodb, monitoring, nestjs, nodejs, nosql, postgresql, python, rds, react, relational databases, rest apis, s3, sql, sqs, storybook, streaming data, tailwind, typescript, vpc"
Senior Data Engineer,Alpaca,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-alpaca-3711172915,2023-12-17,Bonavista, Canada,Mid senior,Remote,"Your Role:
We are looking for a Senior Data Engineer who will design and develop the data management layer for our platform. Data engineering at Alpaca spans across financial transactions, customer data, API logs, system metrics, analytics augmented data and third-party systems that affect both internal and external user's decision making. As our business grows, the complexity and volume of data will progressively grow, from billions of data points in a month today.
The team is 100% distributed and remote.
Things You Get To Do:
Design and oversee the key forward and reverse ETL patterns to expose the data to relevant stakeholders
Expand the key elements of the Alpaca Data Lakehouse architecture
Empower the data analytics team deploy data intensive applications
Work closely with sales, marketing, product, operations to address the needs for key data flows
Operate the system and handle the production issues in a timely manner
Who You Are (Must-Haves):
Proven experience of data engineering at scale.
5+ years experience in software engineering, proficiency in at least one programming language, strong working knowledge of Python
3+ years experience in building data warehouse/data lake platform.
Experience with 3rd party data integration projects
Experience with ETL technologies like Airflow, Airbyte, DBT.
Strong knowledge of cloud native technologies like Kubernetes, Docker, Helm, Prometheus
Strong hands on experience in relational database systems
Production experience with streaming systems like Kafka Experience with infrastructure, Devops and IaaC
Deep knowledge around problems and solutions around the distributed system, storage, transactions, query processing
Experience handling high volume customer data, metrics and transactions, including SQL, BI tools
Exposure to big data distributed file systems
Experience with data security best practices including handling of sensitive data such as personally identifiable information (PII).
You may be asked to be on-call to assist with engineering projects that are timely in nature
Desire and ability to work in a fast paced environment, excitement about adapting your solutions to the changing needs of business
How We Take Care of You:
Competitive Salary & Stock Options
Benefits: Health benefits start on day 1. In the US this includes Medical, Dental, Vision. In Canada, this includes supplemental health care. Internationally, this includes a stipend value to offset medical costs.
New Hire Home-Office Setup: One-time USD $500
Monthly Stipend: USD $150 per month via a Brex Card
Work with awesome hard working people, super smart and cool clients and innovative partners from around the world
Alpaca is proud to be an equal opportunity workplace dedicated to pursuing and hiring a diverse workforce.
Recruitment Privacy Policy
Show more
Show less","Data Engineering, Data Lakehouse Architecture, Data Analytics, ETL, Airflow, Airbyte, DBT, Cloud Native Technologies, Kubernetes, Docker, Helm, Prometheus, Relational Database Systems, Kafka, Infrastructure, DevOps, Infrastructure as Code, Distributed Systems, Storage, Transactions, Query Processing, SQL, BI Tools, Big Data Distributed File Systems, Data Security Best Practices, Sensitive Data, Personally Identifiable Information (PII), Fast Paced Environment","data engineering, data lakehouse architecture, data analytics, etl, airflow, airbyte, dbt, cloud native technologies, kubernetes, docker, helm, prometheus, relational database systems, kafka, infrastructure, devops, infrastructure as code, distributed systems, storage, transactions, query processing, sql, bi tools, big data distributed file systems, data security best practices, sensitive data, personally identifiable information pii, fast paced environment","airbyte, airflow, bi tools, big data distributed file systems, cloud native technologies, data engineering, data lakehouse architecture, data security best practices, dataanalytics, dbt, devops, distributed systems, docker, etl, fast paced environment, helm, infrastructure, infrastructure as code, kafka, kubernetes, personally identifiable information pii, prometheus, query processing, relational database systems, sensitive data, sql, storage, transactions"
Sr. Data Engineer,Terminal,Canada,https://ca.linkedin.com/jobs/view/sr-data-engineer-at-terminal-3764865777,2023-12-17,Bonavista, Canada,Mid senior,Remote,"About Plato Systems
Plato unlocks the power of digital transformation and industrial automation through its integrated Spatial Intelligence (TM) system and platform. Plato Deep Fusion perception technology tracks activity patterns of people, equipment, and assets in industrial environments – making it the world’s first tagless activity tracking system. Plato provides a reliable, frictionless, and scalable automation solution that delivers actionable KPIs so businesses can improve safety, optimize process efficiency, and increase productivity.
About The Role
Deep Domain Engagement: A fervent willingness to understand Plato Systems' domain. Success in this role hinges on grasping the nuances and intricacies of our domain to offer relevant and impactful data solutions.
Data Visualization & Reporting: Create and sustain automated reporting, dashboards, and consistent analysis to bolster data-driven decisions within and across teams.
Generate Business Insights: Identify and articulate key business insights, utilizing statistical analysis
What You'll Do
Develop and maintain ETL pipelines: Create ETL data pipelines using SQL, and PySpark, while ensuring data quality. Familiarity with Databricks is preferred.
What You'll Bring
Prior experience working with processing (normalizing, aggregating, aligning, correlating, ...) time-series data e.g. from sensors, machines, processes
Demonstrated experience with working on production of data products, including handling data quality issues, orchestration and automation, & testing
Excellent SW development practices
Show more
Show less","Plato Deep Fusion, Spatial Intelligence, ETL pipelines, SQL, PySpark, Databricks, Timeseries data, Data products, Data quality, Orchestration, Automation, Testing, SW development practices","plato deep fusion, spatial intelligence, etl pipelines, sql, pyspark, databricks, timeseries data, data products, data quality, orchestration, automation, testing, sw development practices","automation, data products, data quality, databricks, etl pipelines, orchestration, plato deep fusion, spark, spatial intelligence, sql, sw development practices, testing, timeseries data"
Staff Data Engineer (Databricks),Affinity.co,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/staff-data-engineer-databricks-at-affinity-co-3783147711,2023-12-17,Bonavista, Canada,Mid senior,Remote,"Affinity stitches together billions of data points from massive datasets to create a powerful, accurate representation of the world's professional relationship graph. Based on this data, we offer our users the insights and visibility they need to nurture and tap into their team's network of opportunities.
Reporting to the Director of Data and AI, you'll support creating the magic that underlies Affinity's industry-leading relationship intelligence model as the key technical leader of Affinity's Data Platform team.
In this role, you'll leverage your past experiences and deep understanding of data warehousing and data lake concepts to help shape and execute Affinity's roadmap. You'll champion engineering best practices, delivery velocity, and act as a technical mentor for other engineers on the team. You'll play a significant role in defining the future of how businesses around the world use their relationships.
What You'll Be Doing
Design scalable and reliable data pipelines to consume, integrate and analyze large volumes of complex data from different sources to support the growing needs of our business.
Help define our data roadmap. You'll collaborate with our team of data engineers, machine learning engineers, product, and business leaders to help to answer these questions and more.
Build frameworks for measuring and monitoring data quality and integrity.
Establish CI/CD processes, test frameworks, and infrastructure-as-code tooling.
Implement, and build data solutions using Spark, Python, Databricks, and the AWS ecosystem (S3, Redshift, EMR, Athena, Glue).
Mentor, coach, and inspire the engineers on the team.
Identify and fill gaps in the team, and create the processes necessary for the teams' success.
Qualifications
Don't meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every qualification. At Affinity, we are dedicated to building a diverse, inclusive, and authentic workplace, so if you're excited about this role, but your past experience doesn't perfectly align with the qualifications above, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Required:
You have 8+ years of experience working in data engineering, with at least 3+ years of acting as a senior team lead or staff engineer, leading complex, sometimes ambiguous engineering projects across team boundaries.
You have extensive hands-on experience in building scalable data platforms and reliable data pipelines using technologies such as Spark, Hadoop, Databricks, AWS SQS, AWS Kinesis, and/or Kafka.
You have experience working with large, multi-terabyte datasets and are comfortable with high-scale data ingestion, transformation, and distributed processing tools such as Apache Spark (Scala or Python).
Experience with AWS, DBX or related cloud technologies.
You're comfortable with the building blocks of modern back-end systems, such as horizontally scalable data infrastructure, event-driven architecture, and beyond and can clearly articulate the pros/cons of different approaches, while also providing a recommended solution based on the current context.
You have familiarity with databases and analytics technologies in the industry, including Data Warehousing, Data Lakes, ETL and Relational Databases.
You have experience mentoring and helping the engineers around you grow.
You have experience partnering with product and machine learning teams on large, strategic data projects and routine partner work.
You take pride in delivering exceptionally high quality work in terms of data accuracy, performance, and reliability.
Nice to have:
Experience leveraging machine learning to improve the quality of ingested data.
You have worked with multiple third party data vendors and have experience in conflict resolution approaches.
How we work:
Our culture is a key part of how we operate as well as our hiring process:
We iterate quickly. As such, you must be comfortable embracing ambiguity, be able to cut through it, and deliver incremental value to our customers each sprint.
We are candid, transparent, and speak our minds while simultaneously caring personally with each person we interact with.
We make data driven decisions and make the best decision for the moment based on the information available.
Join us in enabling every professional on the planet to succeed by harnessing the power of their relationships.
If you'd want to learn more about our values click here.
What you'll enjoy at Affinity:
We live our values as playmakers, obsessed with learning, care personally about our colleagues and clients, are radically open-minded, and take pride in everything we do.
Health Care coverage and flexible personal & sick days. We want our team to be happy and healthy :)
We provide an annual budget for you to spend on education and offer a comprehensive L&D program – after all, one of our core values is that we're #obsessedwithlearning!
We support our employee's overall health and well-being and reimburse monthly for things such as; Transportation, Home Internet, Meals, and Wellness memberships/equipment.
Virtual team building and socials. Keeping people connected is essential.
Please note that the role compensation details below reflect the base salary only and do not include any variable pay, equity, or benefits. This represents the salary range that Affinity believes, in good faith, at the time of this posting, that it will pay for the posted job.
A reasonable estimate of the current range is
$193,500 to $255,500 CAD
. Within the range, individual pay is determined by factors such as job-related skills, experience, and relevant education or training.
About Affinity
We have raised over $120M and are backed by some of Silicon Valley's best firms, with over 2,700 customers worldwide on our platform. We are proud to have a 4.0 Star Glassdoor rating and recently ranked; Inc.'s Best Workplaces of 2022 and Great Places to Work 2022. Passionate about helping dealmakers in the world's biggest relationship-driven industries to find, manage, and close the most important deals; our Relationship Intelligence platform uses the data exhaust of trillions of interactions between Investment Bankers, Venture Capitalists, Consultants, and other strategic dealmakers with their networks to deliver automated relationship insights that drive over 450,000 deals every month.
We use E-Verify
Our company uses E-Verify to confirm the employment eligibility of all newly hired employees. To learn more about E-Verify, including your rights and responsibilities, please visit www.dhs.gov/E-Verify.
Show more
Show less","Spark, Python, Databricks, AWS, S3, Redshift, EMR, Athena, Glue, Data warehousing, Data lake, ETL, Relational databases, Machine learning, Hadoop, AWS SQS, AWS Kinesis, Kafka, Eventdriven architecture, Apache Spark, Scala","spark, python, databricks, aws, s3, redshift, emr, athena, glue, data warehousing, data lake, etl, relational databases, machine learning, hadoop, aws sqs, aws kinesis, kafka, eventdriven architecture, apache spark, scala","apache spark, athena, aws, aws kinesis, aws sqs, data lake, databricks, datawarehouse, emr, etl, eventdriven architecture, glue, hadoop, kafka, machine learning, python, redshift, relational databases, s3, scala, spark"
Data Engineer with Fivetran developer,Sequoia Connect,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-with-fivetran-developer-at-sequoia-connect-3736841183,2023-12-17,Bonavista, Canada,Mid senior,Remote,"Our Client is a leading Modernization and digital engineering Company. Our Client provides modernization services using its proprietary Platformation™ approach. It specializes in cloud and data modernization, Microsoft Dynamics Modernization, Digital contact center setup and management, managed cloud services, and digital transformation services.
Founded in 1986, the company has since grown to have a presence in North America, Europe, and Asia-Pacific. Our Client has partnerships with leading technology providers such as Microsoft, Amazon, and Google, among others, and has received several awards and recognition for its services.
Focusing on innovation and customer-centricity, Our Client aims to help businesses accelerate their modernization and digital transformation journeys. The Group operates in key industries that drive economic growth, enjoying leadership in the construction, financial, and technology industries.
We are currently searching for a
Data Engineer with Fivetran developer
:
Requirements:
Min 5+ years of experience is required.
Experience with Snowflake.
Experience with SQL.
Experience with Fivetran.
Languages:
Advanced Oral English.
Note:
Fully remote
If you meet these qualifications and are pursuing new challenges, Start your application to join an award-winning employer. Explore all our job openings | Sequoia Career’s Page: https://www.sequoiags.com/careers/.
Show more
Show less","Cloud, Data Modernization, Microsoft Dynamics Modernization, Digital Contact Center Management, Managed Cloud Services, Digital Transformation, Platformation, Snowflake, SQL, Fivetran, Advanced Oral English","cloud, data modernization, microsoft dynamics modernization, digital contact center management, managed cloud services, digital transformation, platformation, snowflake, sql, fivetran, advanced oral english","advanced oral english, cloud, data modernization, digital contact center management, digital transformation, fivetran, managed cloud services, microsoft dynamics modernization, platformation, snowflake, sql"
Data Engineer,Randstad Canada,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-randstad-canada-3783997719,2023-12-17,Bonavista, Canada,Mid senior,Hybrid,"Data Engineer V (Contract Position)
Number of Positions: 1 Filled: 0 Duration: 12 months
Location: Toronto, ON, CA
Must be eligible to work in Canada
Hybrid position, a few days/w onsite in downtown Toronto
MUST HAVE:
1.) Experience with data integration, data storage, ETL – 5+ years
2.) Experience with Java – 5 years
3.) Experience with Python scripting – 2 years
4.) Experience with Messaging Systems and Paradigms (e.g. Solace, Kafka, etc.) – 2 years
5.) Experience with Enterprise integrations with vendor applications – 2 years
6.) Experience with UNIX, Linux as the operating systems - 2 years
NICE TO HAVE
1.) Experience with the following preferred:
2.) Implementation experience for the NICE Actimize Platform, NASDAQ SMARTS
3.) Understanding of Workflow Engines (e.g. Drools)
4.) Detailed knowledge of how reference data is used by Front, Middle, and Back Office applications
5.) Detailed understanding of transaction lifecycle: like order-execution or payment-confirmation life cycles
6.) Understanding of DevOps Tools and the Agile Methodology
7.) Compliance (AML, Surveillance and Supervision)
8.) Capital Markets (Familiarity with Asset Classes and Products)
Show more
Show less","Data Integration, Data Storage, ETL, Java, Python, Messaging Systems, Paradigms, Enterprise Integrations, UNIX, Linux, NICE Actimize Platform, NASDAQ SMARTS, Workflow Engines, Drools, Reference Data, Front Office Applications, Middle Office Applications, Back Office Applications, Transaction Lifecycle, DevOps Tools, Agile Methodology, Compliance, AML, Surveillance, Supervision, Capital Markets, Asset Classes, Products","data integration, data storage, etl, java, python, messaging systems, paradigms, enterprise integrations, unix, linux, nice actimize platform, nasdaq smarts, workflow engines, drools, reference data, front office applications, middle office applications, back office applications, transaction lifecycle, devops tools, agile methodology, compliance, aml, surveillance, supervision, capital markets, asset classes, products","agile methodology, aml, asset classes, back office applications, capital markets, compliance, data integration, data storage, devops tools, drools, enterprise integrations, etl, front office applications, java, linux, messaging systems, middle office applications, nasdaq smarts, nice actimize platform, paradigms, products, python, reference data, supervision, surveillance, transaction lifecycle, unix, workflow engines"
"Senior Associate, IT, Data Engineer",MUFG Investor Services,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-associate-it-data-engineer-at-mufg-investor-services-3759962154,2023-12-17,Bonavista, Canada,Mid senior,Hybrid,"Company Description
MUFG Investor Services provides asset servicing solutions to the global investment management industry. Leveraging the financial and intellectual capital of MUFG – one of the largest banks in the world with $2.8 trillion in assets – we provide clients access to a range of leading solutions from fund administration, middle-office outsourcing, custody, foreign exchange, trustee services and depository to securities lending and other banking services.
With a diverse and dynamic network of offices across the globe, MUFG Investor Services provides challenging and rewarding careers. We achieve this by offering continuous learning and development, collaborative team work environment, promotion of work-life integration, and exposure to a wide variety of work.
Imagine your future at MUFG Investor Services where you can grow professionally, in a diverse and inclusive workplace that rewards your contribution.
Job Description
The successful candidate will be part of a team which will be responsible for the correct, timely and efficient flow of data from various source systems to various end points.
Key Responsibilities
Develop data flows based on the requirement by executing the necessary steps of data transformation, data validation and mapping using nifi, SQL and Python.
Develop necessary components to establish connection with various sources of data and destinations via API, SQL, S3, SFTP and any other communication protocol.
Maintain and apply changes on existing flows running on nifi ETL tool.
Perform thorough testing and validation to support the accuracy of data transformations, data verification and delivery.
Develop unitest and regression tests.
Prepare release plans and coordinate with the stakeholders and release engineers to ensure the proper execution.
Provide post-implementation support and troubleshoot any potential issues.
Provide 2nd and 3rd level support to operation teams and resolve issues in a timely manner.
Prepare documentation of existing and new data flows as well as clients’ procedures.
Qualifications
Bachelor’s degree in Computer Science or a related technical field or equivalent experience.
3 - 5 years of commercial experience ETL tools (experience with Apache NiFi will be considered an advantage).
Experience with programming languages such as Python will be considered as advantage.
Experience in data-related process controls.
Experience working with a team of developers using agile methodologies to achieve planned process deliverables.
High level of accuracy and attention to detail.
Demonstrate commitment to providing customer-focused quality service.
Excellent verbal and written communication in English.
Good teamwork and collaboration skills.
Additional Information
At MUFG Investor Services, we are exceptionally proud of our approach to Hybrid Working. It enables the flexibility to thrive from wherever our employees work and, stay connected to their team and our culture. When we make Hybrid Working plans, we get to know the individual and pride ourselves in underpinning all our decisions with fairness and consistency.
MUFG Investor Services provides all of its employees with an extremely attractive compensation package. In addition to base salary, there is a group medical insurance scheme, group pension scheme, reimbursement of professional subscriptions, paid holidays and assistance towards gym memberships.
We thank all candidates for applying; however, only those proceeding to the interview stage will be contacted. If you are contacted for a job opportunity, please advise us of any accommodations needed to ensure fair and equitable access throughout the recruitment and selection process. All accommodation information provided will be treated as confidential and used only to provide an accessible candidate experience.
MUFG is an equal opportunity employer.
Show more
Show less","ETL, NIFI, SQL, Python, API, S3, SFTP, Unit Testing, Regression Testing, Data Transformation, Data Validation, Data Mapping, Data Controls, Agile Methodologies, Documentation, Hybrid Working","etl, nifi, sql, python, api, s3, sftp, unit testing, regression testing, data transformation, data validation, data mapping, data controls, agile methodologies, documentation, hybrid working","agile methodologies, api, data controls, data mapping, data transformation, data validation, documentation, etl, hybrid working, nifi, python, regression testing, s3, sftp, sql, unit testing"
Data Engineer,Vaco,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-vaco-3778831671,2023-12-17,Bonavista, Canada,Mid senior,Hybrid,"We're hiring a seasoned Data Engineer for our dynamic e-training company. If you have over 8 years of experience in crafting scalable data pipelines on cloud platforms like AWS, GCP, and Azure, we want to hear from you.
Responsibilities:
Design and build scalable data pipelines.
Collaborate with cross-functional teams.
Develop and maintain automated data workflows.
Optimize pipelines for performance and reliability.
Stay updated on emerging data engineering trends.
Requirements:
Bachelor's or Master's degree in Computer Science or related field.
Strong experience with cloud platforms.
Proficient in big data technologies (Hadoop, Spark, Kafka).
Solid understanding of software engineering best practices.
Excellent communication and collaboration skills.
Familiarity with agile methodologies.
Preferred Qualifications:
Exposure to containerization (Docker, Kubernetes).
Knowledge of data analytics and visualization tools (Power BI, Tableau).
Familiarity with machine learning and AI technologies.
Understanding of DevOps and CI/CD best practices.
Experience in regulated industries is a plus.
Show more
Show less","Data Engineering, Cloud Platforms (AWS GCP Azure), Scalable Data Pipelines, Big Data Technologies (Hadoop Spark Kafka), Software Engineering Best Practices, Agile Methodologies, Data Analytics, Data Visualization (Power BI Tableau), Machine Learning, AI Technologies, DevOps, CI/CD Best Practices, Containerization (Docker Kubernetes)","data engineering, cloud platforms aws gcp azure, scalable data pipelines, big data technologies hadoop spark kafka, software engineering best practices, agile methodologies, data analytics, data visualization power bi tableau, machine learning, ai technologies, devops, cicd best practices, containerization docker kubernetes","agile methodologies, ai technologies, big data technologies hadoop spark kafka, cicd best practices, cloud platforms aws gcp azure, containerization docker kubernetes, data engineering, data visualization power bi tableau, dataanalytics, devops, machine learning, scalable data pipelines, software engineering best practices"
Azure Data Engineer,Tata Consultancy Services,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/azure-data-engineer-at-tata-consultancy-services-3766971267,2023-12-17,Bonavista, Canada,Mid senior,Hybrid,"About TCS:
TCS operates on a global scale, with a diverse talent base of more than 600,000 associates representing 153 nationalities across 55 countries. TCS has been recognized as a Global Top Employer by the Top Employers Institute - one of only eight companies worldwide to have achieved this status. Our organizational structure is domain-led and designed to offer businesses a single window into industry-specific solutions. Our agile industry units have embedded capabilities to enable rapid responses that provide a competitive edge to our customers. This, coupled with a unique Global Network Delivery Model™ (GNDM™), is recognized as the current benchmark of excellence in technology deployment. We have made significant investments in digital technology, horizontal, and vertical platforms, allowing us to successfully serve our clients for over 50 years.
Required Skills:
Skills and Responsibilities:
• Support and develop solutions in both on premise and cloud environments.
• Develop schemas, tables, stored procedures, and deployment packages.
• Provide database support to cross-functional teams across the company.
• Conduct testing and analysis.
• Collaborate with senior team members to design migration solutions, including development of specifications to ensure quality.
• Create and maintain data mapping documentation.
• Experience building data pipelines in Azure Data Factory.
• Investigate and resolve data issues.
• Implement, maintain, enhance data governance, quality and related policies, aligning with organizations standards and frameworks.
• Knowledge of Data warehouse, Data marts and Data Modelling
• Knowledge of Microsoft Azure data factory, data bricks and synapse.
• Experience working in SQL and Python.
Thank you for your interest in TCS. Candidates that meet the qualification for this position will be contacted within a 2-week period. We invite you to continue to apply for other opportunities that match your profile.
Tata Consultancy Services Canada Inc. is committed to meeting the accessibility needs of all individuals in accordance with the Accessibility for Ontarians with Disabilities Act (AODA) and the Ontario Human Rights Code (OHRC). Should you require accommodations during the recruitment and selection process, please inform Human Resource.
Show more
Show less","Database Management, Data Modelling, Azure Data Factory, SQL, Python, Data Governance, Data Pipelines, Data Quality, Data Warehouse, Data Marts","database management, data modelling, azure data factory, sql, python, data governance, data pipelines, data quality, data warehouse, data marts","azure data factory, data governance, data marts, data modelling, data quality, database management, datapipeline, datawarehouse, python, sql"
Senior Backend Engineer- Bigdata,Capgemini Engineering,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-backend-engineer-bigdata-at-capgemini-engineering-3761840352,2023-12-17,Bonavista, Canada,Mid senior,Hybrid,"Life at Capgemini
Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer:
Flexible work
Healthcare including dental, vision, mental health, and well-being programs
Financial well-being programs such as 401(k) and Employee Share Ownership Plan
Paid time off and paid holidays
Paid parental leave
Family building benefits like adoption assistance, surrogacy, and cryopreservation
Social well-being benefits like subsidized back-up child/elder care and tutoring
Mentoring, coaching and learning programs
Employee Resource Groups
Disaster Relief
About Capgemini Engineering
World leader in engineering and R&D services, Capgemini Engineering combines its broad industry knowledge and cutting-edge technologies in digital and software to support the convergence of the physical and digital worlds. Coupled with the capabilities of the rest of the Group, it helps clients to accelerate their journey towards Intelligent Industry. Capgemini Engineering has more than 55,000 engineer and scientist team members in over 30 countries across sectors including Aeronautics, Space, Defense, Naval, Automotive, Rail, Infrastructure & Transportation, Energy, Utilities & Chemicals, Life Sciences, Communications, Semiconductor & Electronics, Industrial & Consumer, Software & Internet.
Capgemini Engineering is an integral part of the Capgemini Group, a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided every day by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse organization of over 360,000 team members in more than 50 countries. With its strong 55-year heritage and deep industry expertise, Capgemini is trusted by its clients to address the entire breadth of their business needs, from strategy and design to operations, fueled by the fast evolving and innovative world of cloud, data, AI, connectivity, software, digital engineering and platforms. The Group reported in 2022 global revenues of €22 billion.
Get the Future You Want | www.capgemini.com
Job Title
: Senior Backend Engineer
Location
: Vancouver, BC or Toronto, ON (Hybrid)
Purpose of the job:
As Senior Engineer, you will work on one of the world's largest social media platform which deals with few hundred millions of users. You will give as part of R&D self-organized team working in a complicated, innovative environment for our client doing design, research, building proof of concepts and production ready product.
Investigate, build, and implement the solutions for existing technical challenges, including building visual processing application.
RESPONSIBILITIES
You will acquire tasks from the project lead or Team Lead (TL), prepares functional and design specifications, and approves them with all partners.
Ensures that assigned area/areas are delivered within set deadlines and required quality objectives.
Provides estimations, agreed on task duration with the manager, and chips in to project plan of an assigned area.
Developing standard methodologies for data coding to ensure consistency within the system.
Updating and optimizing local and metadata models.
Understand the business drivers and analytical use cases and translate these into data products
Design, implement, and maintain pipelines that produce business effectively data optimally and efficiently using cloud technology
Reports about area readiness/quality, and raise red flags in crisis situations which are beyond responsibilities.
Responsible for resolving crisis situations within responsibilities.
Suggests technical and functional improvements to supply to the product.
Collaborates with other teams.
Required Skills:
We are looking for someone with proven track record as a Bigdata software backend development.
Shown Python coding skills with proven ability.
Established relational DB systems experience (MySQL)
Experience with distributed systems
Ability to coordinate and document solutions
A passion for streamlining systems and processes to make the difficult trivial.
Effective communication (oral & written), collaboration, and communication skills
Would be a plus:
Experience with AWS (S3)
Experience with Kafka.
Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.
Show more
Show less","Python, MySQL, Distributed Systems, AWS, Kafka, Data Coding Methodologies, Relational Database Systems, Bigdata Software Backend Development","python, mysql, distributed systems, aws, kafka, data coding methodologies, relational database systems, bigdata software backend development","aws, bigdata software backend development, data coding methodologies, distributed systems, kafka, mysql, python, relational database systems"
"Data Engineer, Data Platform",Grammarly,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3689961990,2023-12-17,Bonavista, Canada,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Python, Scala, Java, Relational databases, AWS, Azure, GCE, Apache Kafka, Git, Data Lakehouse, Data engineering, APIs, Internal tools, System design, Admin sites","python, scala, java, relational databases, aws, azure, gce, apache kafka, git, data lakehouse, data engineering, apis, internal tools, system design, admin sites","admin sites, apache kafka, apis, aws, azure, data engineering, data lakehouse, gce, git, internal tools, java, python, relational databases, scala, system design"
Data Analyst,Invafresh,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-at-invafresh-3778653371,2023-12-17,Bonavista, Canada,Mid senior,Hybrid,"WHO WE ARE:
Based in Mississauga, Canada, and deployed globally, Invafresh is a leading technology company specializing in FreshologyTM software solutions for supermarket & boutique fresh food grocers around the world. In business since 1988, Invafresh continues to invest in its people and technology. Today, Invafresh is an organization of experienced and retail-savvy Freshologists equipped with industry specific solutions that supermarket chains can leverage to succeed in the field of fresh operations.
Our fresh food retail operations solutions are deployed collectively in over 25,000 grocery stores worldwide, actively supporting the in-store execution of fresh operations. We enable our customers to achieve and maintain a leadership position in their fresh food offering by providing data integrity and decision support tools needed to enhance sales, reduce shrink and inventories, and improve overall margins while positively impacting the planet by reducing food waste throughout the supply chain of fresh food retail. We help our customers achieve success in the highly competitive grocery market.
POSITION DETAILS:
We are seeking a highly motivated and detail-oriented Data Analyst to join our team. As a Data Analyst, you will be responsible for interpreting data, analyzing results, and providing insights to guide business decisions. You will work closely with cross-functional teams to gather, organize, and transform data into meaningful reports and visualizations that drive actionable insights.
RESPONSIBILITIES:
Data Collection and Preparation:
Collaborate with various teams to identify data sources and collect relevant data.
Clean, transform, and preprocess data to ensure accuracy, consistency, and suitability for analysis.
Data Analysis:
Utilize statistical techniques to analyze large datasets and extract meaningful insights.
Identify trends, patterns, and correlations in the data to support business objectives.
Perform exploratory data analysis to discover hidden insights.
Reporting and Visualization:
Develop clear and informative dashboards, charts, and visualizations to present insights to stakeholders.
Create recurring reports and automate data visualization processes where possible.
Interpretation and Insight Generation:
Translate complex data into understandable insights that drive decision-making.
Collaborate with business teams to understand their needs and provide data-driven recommendations.
Data Quality and Integrity:
Monitor and ensure data quality and accuracy through regular audits and validation procedures.
Identify and address data discrepancies or anomalies.
Collaboration:
Work closely with cross-functional teams, including business analysts, managers, and data scientists, to understand requirements and provide analytical support.
Communicate findings and insights effectively to both technical and non-technical stakeholders.
Continuous Improvement:
Stay up-to-date with industry trends, best practices, and emerging technologies in data analysis.
Proactively suggest process improvements and ways to enhance data analysis efficiency
QUALIFICATIONS:
Bachelor's degree in a related field such as Data Science, Statistics, Computer Science, Economics, or Mathematics. A Master's degree is a plus.
Proven experience as a Data Analyst or similar role.
Proficiency in data analysis tools such as Python, R, or SQL.
Experience with data visualization tool Power BI
Strong analytical thinking and problem-solving skills.
Attention to detail and a strong sense of data accuracy.
Excellent communication skills to convey complex findings in a clear and understandable manner. Ability to work collaboratively in a team-oriented environment.
Familiarity with data warehousing concepts and data modeling is a plus.
WHY INVAFRESH?
Competitive compensation and flex hours and location
We take great pride in our first-class team and culture. Open-door approach at all levels.
Entrepreneurial and rapid growth.
The premium and most progressive product on the market. Focus on continuous innovation.
Work hard/Play hard mentality; we thrive on change and continuously strive for excellence.
Real-time coaching and feedback, growth opportunities, and a supportive and collaborative team
Show more
Show less","Data Analysis, Data Collection, Data Preparation, Data Visualization, Reporting, Data Interpretation, Data Quality, Data Integrity, Data Modeling, Data Warehousing, Statistical Analysis, Exploratory Data Analysis, Python, R, SQL, Power BI","data analysis, data collection, data preparation, data visualization, reporting, data interpretation, data quality, data integrity, data modeling, data warehousing, statistical analysis, exploratory data analysis, python, r, sql, power bi","data collection, data integrity, data interpretation, data preparation, data quality, dataanalytics, datamodeling, datawarehouse, exploratory data analysis, powerbi, python, r, reporting, sql, statistical analysis, visualization"
Cloud Data Engineer,PrecisionERP Incorporated,"Ontario, Canada",https://ca.linkedin.com/jobs/view/cloud-data-engineer-at-precisionerp-incorporated-3776929436,2023-12-17,Bonavista, Canada,Mid senior,Hybrid,"PrecisionIT is seeking an Intermediate level Cloud Data Engineer who has worked on ingesting data to Azure environments. This is for an initial 1-year hybrid style contract with our client based in London, ON.
The resource is expected to work onsite 3 days/week in either London or Toronto, ON.
Mandatory Experience: Azure, ADF, ADB, Control-M, Synapse
Show more
Show less","Cloud Data Engineering, Azure, Azure Data Factory (ADF), Azure DataBricks (ADB), ControlM, Synapse","cloud data engineering, azure, azure data factory adf, azure databricks adb, controlm, synapse","azure, azure data factory adf, azure databricks adb, cloud data engineering, controlm, synapse"
Big Data Developer,Iris Software Inc.,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/big-data-developer-at-iris-software-inc-3774821411,2023-12-17,Bonavista, Canada,Mid senior,Hybrid,"Iris's direct client, one of the Top 5 Bank in Canada, is looking to hire a
Big Data Developer
for a long term opportunity at
Toronto, ON (Hybrid Position)
.
Our Client is a Canadian multinational financial services company and the largest bank in Canada by market capitalization. The bank serves over 17 million clients and has more than 89,000 employees worldwide. Bank is serving individual consumers, small and middle market businesses and large corporations with a full range of banking, investing, asset management and other financial and risk-management products and services.
Position: BigData Developer
Location: Toronto, ON (Hybrid - 3 Days in Office/week)
Duration: Long Term Open Ended Contract
Must have:
Over 8 years of hands-on development experience.
Experienced in Java/J2EE, Scala, Shell Script, C, Python, C#.
Experienced in one or more of DB2, Oracle, MySQL, MS SQL Server, No-SQL (HBase, Vertica, Casandra).
Experienced in Hadoop, Spark, MapReduce, Hive, Kafka, Knox, Oozie, Solr, Elasticsearch.
Focus on Kafka, API, APGEE, Micro-services, general application build.
About Iris Software Inc.
With 4,000+ associates and offices in India, U.S.A. and Canada, Iris Software delivers technology services and solutions that help clients complete fast, far-reaching digital transformations and achieve their business goals. A strategic partner to Fortune 500 and other top companies in financial services and many other industries, Iris provides a value-driven approach - a unique blend of highly-skilled specialists, software engineering expertise, cutting-edge technology, and flexible engagement models. High customer satisfaction has translated into long-standing relationships and preferred-partner status with many of our clients, who rely on our 30+ years of technical and domain expertise to future-proof their enterprises. Associates of Iris work on mission-critical applications supported by a workplace culture that has won numerous awards in the last few years, including Certified Great Place to Work in India; Top 25 GPW in IT & IT-BPM; Ambition Box Best Place to Work, #3 in IT/ITES; and Top Workplace NJ-USA.
Show more
Show less","Big Data Development, Java/J2EE, Scala, Shell Script, C, Python, C#, DB2, Oracle, MySQL, MS SQL Server, NoSQL, HBase, Vertica, Casandra, Hadoop, Spark, MapReduce, Hive, Kafka, Knox, Oozie, Solr, Elasticsearch, API, APGEE, Microservices","big data development, javaj2ee, scala, shell script, c, python, c, db2, oracle, mysql, ms sql server, nosql, hbase, vertica, casandra, hadoop, spark, mapreduce, hive, kafka, knox, oozie, solr, elasticsearch, api, apgee, microservices","apgee, api, big data development, c, casandra, db2, elasticsearch, hadoop, hbase, hive, javaj2ee, kafka, knox, mapreduce, microservices, ms sql server, mysql, nosql, oozie, oracle, python, scala, shell script, solr, spark, vertica"
AWS Data Engineer,First Derivative,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/aws-data-engineer-at-first-derivative-3784376480,2023-12-17,Bonavista, Canada,Mid senior,Hybrid,"Who are we?
First Derivatives (FD) is a leading software and services company, with world-leading intellectual property in ultra-high-performance analytics (KX) across industries, and extensive domain expertise and capabilities in capital markets systems and technology (managed services and consulting).
AWS Data Engineer:
Required Skills
10+ years of professional experience with strong Python & Pyspark coding skill, AWS + AWS Glue.
AWS technologies: Lambda, SNS, SQS, S3, Glue, EMR, IAM.
Proven track record working in data engineering, business intelligence, or a similar role
Experience in ETL orchestration and workflow management tools like Airflow, or Oozie
Guide in Database fundamentals, SQL and distributed computing
Excellent communication skills and experience working with technical and non-technical teams.
Experience in AWS (EC2/S2/IAM)
Experience in Databricks & Snowflake
Proficiency in programming languages such as Java
KX technology is designed to capture and analyze data to make real-time decisions in a world where data volumes generated by markets and machines are increasing exponentially, and existing technologies fail due to technological or commercial limitations.
KX is widely adopted throughout the financial industry, and is poised for accelerated growth across high-tech manufacturing, automotive, oil and gas, utilities and telecommunications.
Managed Services and Consulting
FD provides a range of services worldwide to its clients in the capital markets sector, including many of the world’s leading banks, focused on supporting mission-critical systems as well as helping them to achieve and maintain regulatory compliance.
FD provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Show more
Show less","Python, Pyspark, AWS, AWS Glue, Lambda, SNS, SQS, S3, Glue, EMR, IAM, Airflow, Oozie, SQL, Distributed computing, Java, Databricks, Snowflake, EC2, S2, KX","python, pyspark, aws, aws glue, lambda, sns, sqs, s3, glue, emr, iam, airflow, oozie, sql, distributed computing, java, databricks, snowflake, ec2, s2, kx","airflow, aws, aws glue, databricks, distributed computing, ec2, emr, glue, iam, java, kx, lambda, oozie, python, s2, s3, snowflake, sns, spark, sql, sqs"
Big Data Developer,Iris Software Inc.,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/big-data-developer-at-iris-software-inc-3778822465,2023-12-17,Bonavista, Canada,Mid senior,Hybrid,"Greetings
!
Iris's client, one of the world's prominent financial services company is looking to hire a strong
Big Data Developer
for a long term contract opportunity.
Position: Big Data Developer
Location: Toronto ON (Hybrid)
Responsibilities
:
Design, develop, and maintain robust software solutions to meet the needs of our organization.
Collaborate with cross-functional teams to understand business requirements and translate them into technical specifications.
Utilize your extensive experience in Java/J2EE, Scala, and Shell Script, C, Python, and C # to contribute to the development of cutting-edge applications.
Work with various database technologies, including DB2, Oracle, MySQL, MS SQL Server, and No-SQL databases such as HBase, Vertica, and Cassandra.
Demonstrate proficiency in big data technologies such as Hadoop, Spark, MapReduce, Hive, Kafka, Knox, Oozie, Solr, and Elasticsearch.
Focus on Kafka, API, APGEE, Micro-services, and contribute to the overall architecture and development of our applications.
Stay informed about industry best practices, emerging technologies, and trends to ensure the continuous improvement of our software solutions.
Requirements
:
· Over 10 + years of hands-on development experience.
· Strong programming skills in Java/J2EE, Scala, Shell Script, C, Python, and C#.
Extensive experience with databases, including DB2, Oracle, MySQL, MS SQL Server, and No-SQL databases (HBase, Vertica, Cassandra).
Proficiency in big data technologies, including Hadoop, Spark, MapReduce, Hive, Kafka, Knox, Oozie, Solr, and Elasticsearch.
Expertise in Kafka, API, APGEE, Micro-services, and general application build.
Strong problem-solving skills and the ability to work collaboratively in a team environment.
Excellent communication skills, with the ability to convey complex technical concepts to both technical and non-technical stakeholders.
About Iris Software Inc.
With 4,000+ associates and offices in India, U.S.A. and Canada, Iris Software delivers technology services and solutions that help clients complete fast, far-reaching digital transformations and achieve their business goals. A strategic partner to Fortune 500 and other top companies in financial services and many other industries, Iris provides a value-driven approach - a unique blend of highly-skilled specialists, software engineering expertise, cutting-edge technology, and flexible engagement models. High customer satisfaction has translated into long-standing relationships and preferred-partner status with many of our clients, who rely on our 30+ years of technical and domain expertise to future-proof their enterprises. Associates of Iris work on mission-critical applications supported by a workplace culture that has won numerous awards in the last few years, including Certified Great Place to Work in India; Top 25 GPW in IT & IT-BPM; Ambition Box Best Place to Work, #3 in IT/ITES; and Top Workplace NJ-USA.
Show more
Show less","Java, Java/J2EE, Scala, Shell Script, C, Python, C#, DB2, Oracle, MySQL, MS SQL Server, NoSQL, HBase, Vertica, Cassandra, Hadoop, Spark, MapReduce, Hive, Kafka, Knox, Solr, Elasticsearch, Microservices, APIs, APGEE","java, javaj2ee, scala, shell script, c, python, c, db2, oracle, mysql, ms sql server, nosql, hbase, vertica, cassandra, hadoop, spark, mapreduce, hive, kafka, knox, solr, elasticsearch, microservices, apis, apgee","apgee, apis, c, cassandra, db2, elasticsearch, hadoop, hbase, hive, java, javaj2ee, kafka, knox, mapreduce, microservices, ms sql server, mysql, nosql, oracle, python, scala, shell script, solr, spark, vertica"
Data and Communications Analyst (Program Analyst),"Tygart Technology, Inc.","Clarksburg, WV",https://www.linkedin.com/jobs/view/data-and-communications-analyst-program-analyst-at-tygart-technology-inc-3681080709,2023-12-17,West Virginia,United States,Mid senior,Onsite,"Tygart is currently seeking a
Data and Communication Analyst (Program Analyst)
to support a Department of Justice (DOJ) Program Office. This is an onsite position in Clarksburg, WV and requires that the candidate have
a current Top Secret
security clearance.
Responsibilities Include
Support the program with various research and analytical projects, white paper development, presentations, and other project-oriented tasks.
Ability to develop a communications strategy and produce creative, effective, and engaging communications content.
Manage, track, prioritize, and schedule multiple projects simultaneously, while meeting all deadlines.
Provide necessary professional skills, including planning, selecting, collecting, analyzing, interpreting, and communicating for all phases of data processing; system and software development lifecycle phases; and data analysis.
Collaborate with other CJIS units on projects, initiatives, and events.
Recognize patterns of customer needs, problems, and potential systemic solutions.
Work with all levels of the organization. Interactions may include presentations, workshop facilitation, and outreach.
Proficiency in preparing and/or reviewing technical and programmatic documentation to ensure consistency with outreach documentation and ensuring impacts are identified, understood, and communicated is required.
Shall work with all levels of the organization. Interactions may include the delivery of presentations, workshop facilitation, and other forms of outreach.
Recommend innovative avenues of communication for external and internal audiences.
Assist with system messaging as needed; this includes system maintenance, outages, and notifications Shall manage and perform print production updates, while maintaining master production schedule.
Provide functional expertise in the following aspects of creative content production: creative content strategy development; content development; and content delivery.
Shall perform extensive analysis and research to create case analysis studies and reports some of which will involve use of MS PowerBI.
Qualifications:
A minimum of 5 years of experience in the development and management of user manuals, standard operating procedures, outreach materials, bulletins, presentations, reports, and instructional materials
Expertise in creative writing and marketing materials, including, but not limited to creating informative emails, articles, success stories, brochures, flyers, newsletters, presentations, etc. is required.
Excellent analytical, critical thinking, judgement, and organizational skills are required.
Must have excellent verbal, written, and interpersonal communication skills.
Must have experience working with MS Tools, specifically PowerPoint, Excel, and Visio software.
Ability to develop a communications strategy and produce creative, effective, and engaging communications content.
Desired: Proficiency with the application of copyright procedures and laws, and Section 508 of the Rehabilitation Act of 1973 requirements and standards.
Show more
Show less","Data analysis, PowerPoint, Excel, Visio, MS PowerBI, System analysis, Data processing, Project management, Communication strategy, Technical writing, Marketing materials, Creative writing, Copyright procedures, Section 508 of the Rehabilitation Act of 1973, User manuals, Standard operating procedures, Outreach materials, Bulletins, Presentations, Reports, Instructional materials","data analysis, powerpoint, excel, visio, ms powerbi, system analysis, data processing, project management, communication strategy, technical writing, marketing materials, creative writing, copyright procedures, section 508 of the rehabilitation act of 1973, user manuals, standard operating procedures, outreach materials, bulletins, presentations, reports, instructional materials","bulletins, communication strategy, copyright procedures, creative writing, data processing, dataanalytics, excel, instructional materials, marketing materials, ms powerbi, outreach materials, powerpoint, presentations, project management, reports, section 508 of the rehabilitation act of 1973, standard operating procedures, system analysis, technical writing, user manuals, visio"
Data and Communications Analyst (Program Analyst) with Security Clearance,ClearanceJobs,"Clarksburg, WV",https://www.linkedin.com/jobs/view/data-and-communications-analyst-program-analyst-with-security-clearance-at-clearancejobs-3753478921,2023-12-17,West Virginia,United States,Mid senior,Onsite,"Tygart is currently seeking a Data and Communication Analyst (Program Analyst) to support a Department of Justice (DOJ) Program Office. This is an onsite position in Clarksburg, WV and requires that the candidate have a current Top Secret security clearance. Responsibilities Include: Support the program with various research and analytical projects, white paper development, presentations, and other project-oriented tasks.
Ability to develop a communications strategy and produce creative, effective, and engaging communications content.
Manage, track, prioritize, and schedule multiple projects simultaneously, while meeting all deadlines.
Provide necessary professional skills, including planning, selecting, collecting, analyzing, interpreting, and communicating for all phases of data processing; system and software development lifecycle phases; and data analysis.
Collaborate with other CJIS units on projects, initiatives, and events.
Recognize patterns of customer needs, problems, and potential systemic solutions.
Work with all levels of the organization. Interactions may include presentations, workshop facilitation, and outreach.
Proficiency in preparing and/or reviewing technical and programmatic documentation to ensure consistency with outreach documentation and ensuring impacts are identified, understood, and communicated is required.
Shall work with all levels of the organization. Interactions may include the delivery of presentations, workshop facilitation, and other forms of outreach.
Recommend innovative avenues of communication for external and internal audiences.
Assist with system messaging as needed; this includes system maintenance, outages, and notifications Shall manage and perform print production updates, while maintaining master production schedule.
Provide functional expertise in the following aspects of creative content production: creative content strategy development; content development; and content delivery. Shall perform extensive analysis and research to create case analysis studies and reports some of which will involve use of MS PowerBI.
Qualifications
A minimum of 5 years of experience in the development and management of user manuals, standard operating procedures, outreach materials, bulletins, presentations, reports, and instructional materials
Expertise in creative writing and marketing materials, including, but not limited to creating informative emails, articles, success stories, brochures, flyers, newsletters, presentations, etc. is required.
Excellent analytical, critical thinking, judgement, and organizational skills are required. Must have excellent verbal, written, and interpersonal communication skills.
Must have experience working with MS Tools, specifically PowerPoint, Excel, and Visio software.
Ability to develop a communications strategy and produce creative, effective, and engaging communications content.
Desired: Proficiency with the application of copyright procedures and laws, and Section 508 of the Rehabilitation Act of 1973 requirements and standards.
Show more
Show less","Data Analysis, Communication Strategy, Project Management, Technical Writing, Outreach, Creative Content Production, Case Analysis, User Manuals, Standard Operating Procedures, Marketing Materials, MS Tools (PowerPoint Excel Visio), MS PowerBI, Copyright Procedures, Section 508 of the Rehabilitation Act of 1973","data analysis, communication strategy, project management, technical writing, outreach, creative content production, case analysis, user manuals, standard operating procedures, marketing materials, ms tools powerpoint excel visio, ms powerbi, copyright procedures, section 508 of the rehabilitation act of 1973","case analysis, communication strategy, copyright procedures, creative content production, dataanalytics, marketing materials, ms powerbi, ms tools powerpoint excel visio, outreach, project management, section 508 of the rehabilitation act of 1973, standard operating procedures, technical writing, user manuals"
Data Engineer - SME III (Remote) with Security Clearance,ClearanceJobs,"Clarksburg, WV",https://www.linkedin.com/jobs/view/data-engineer-sme-iii-remote-with-security-clearance-at-clearancejobs-3775645635,2023-12-17,West Virginia,United States,Mid senior,Remote,"Description Work Where it Matters Tuva, an Akima company, is not just another federal IT contractor. As an Alaska Native Corporation (ANC), our mission and purpose extend beyond our exciting federal projects as we support our shareholder communities in Alaska. At Tuva, the work you do every day makes a difference in the lives of our 15,000 Iñupiat shareholders, a group of Alaska natives from one of the most remote and harshest environments in the United States. For our shareholders, Tuva provides support and employment opportunities and contributes to the survival of a culture that has thrived above the Arctic Circle for more than 10,000 years. For our government customers, Tuva helps optimize IT systems, tools, and methods, as well as mission support services and specialized technologies. As a Tuva employee, you will be surrounded by a challenging, yet supportive work environment that is committed to innovation and diversity, two of our most important values. You will also have access to our comprehensive benefits and competitive pay in addition to growth opportunities and excellent retirement options. Job Summary: The NCIC is a computerized database of documented criminal justice information consisting of 21 files. All of the existing NCIC and new N3G functionality will be migrated to, and developed on, an approved public cloud, such as Amazon Web Services (AWS) GovCloud, requiring real-time communication and data exchange between the mainframe and the public cloud for a transitional period of time. TUVA seeks to hire a Data Engineer to support SMEs services to be provided to our government client related to Data Engineering and to provide new test data to support large scale performance testing and the functionality of the NCIC database system being transitioned to the cloud. Job Responsibilities:
The Data Engineer shall work under the direction of the Government Task Lead. The Data Engineer will work with their Government counterparts on Sprint Teams through the Agile development of the N3G system.
The data engineer will be responsible for all aspects of data profiling, data design, data management, and production of test data. Minimum Qualifications:
A minimum of six (6) years of data management, database administration or equivalent experience.
A minimum of four (4) years of data engineering, data analysis, or equivalent experience
Possess a Bachelor of Arts (B.A.) or Bachelor of Science (B.S.) degree from an accredited institution or equivalent experience related to the scope and objectives of the contract.
Top Secret Security Clearance.
In addition to the education and experience requirements, candidates shall possess superior verbal and written communication skills with the ability to work without supervision. Candidates shall also possess excellent analytical, interpersonal, and presentation skills. Desired Qualifications:
Prior work experience with the federal government.
Prior work experience with either Amazon Web Services (AWS).
Familiarity with Agile development. We are an equal opportunity employer. All applicants will receive consideration for employment, without regard to race, color, religion, creed, national origin, gender or gender-identity, age, marital status, sexual orientation, veteran status, disability, pregnancy or parental status, or any other basis prohibited by law. If you are an individual with a disability, and would like to request a reasonable accommodation for any part of the employment process, please contact us at or 571-353-7053 (information about job applications status is not available at this contact information). \#CJP-1234 Job: Information Technology Travel: No Organization: TUVA Clearance: TS Shift: Day Job Work Type: Hybrid Req ID: TUV03083
Show more
Show less","Data Engineering, Data Analysis, Data Profiling, Data Design, Data Management, Data Migration, Performance Testing, Data Architecture, Database Administration, Big Data, Cloud Computing, Amazon Web Services (AWS) GovCloud, Agile Development, JIRA, Confluence, Sprint Teams, Jira, Git, SQL, NoSQL","data engineering, data analysis, data profiling, data design, data management, data migration, performance testing, data architecture, database administration, big data, cloud computing, amazon web services aws govcloud, agile development, jira, confluence, sprint teams, jira, git, sql, nosql","agile development, amazon web services aws govcloud, big data, cloud computing, confluence, data architecture, data design, data engineering, data management, data migration, data profiling, dataanalytics, database administration, git, jira, nosql, performance testing, sprint teams, sql"
Data Engineer - SME III (Remote),Akima,"Clarksburg, WV",https://www.linkedin.com/jobs/view/data-engineer-sme-iii-remote-at-akima-3770477987,2023-12-17,West Virginia,United States,Mid senior,Hybrid,"Work Where it Matters
Tuva, an Akima company, is not just another federal IT contractor. As an Alaska Native Corporation (ANC), our mission and purpose extend beyond our exciting federal projects as we support our shareholder communities in Alaska.
At Tuva, the work you do every day makes a difference in the lives of our 15,000 Iñupiat shareholders, a group of Alaska natives from one of the most remote and harshest environments in the United States.
For our shareholders,
Tuva provides support and employment opportunities and contributes to the survival of a culture that has thrived above the Arctic Circle for more than 10,000 years.
For our government customers,
Tuva helps optimize IT systems, tools, and methods, as well as mission support services and specialized technologies.
As a Tuva employee,
you will be surrounded by a challenging, yet supportive work environment that is committed to innovation and diversity, two of our most important values. You will also have access to our comprehensive benefits and competitive pay in addition to growth opportunities and excellent retirement options.
Job Summary:
The NCIC is a computerized database of documented criminal justice information consisting of 21 files. All of the existing NCIC and new N3G functionality will be migrated to, and developed on, an approved public cloud, such as Amazon Web Services (AWS) GovCloud, requiring real-time communication and data exchange between the mainframe and the public cloud for a transitional period of time.
TUVA seeks to hire a Data Engineer to support SMEs services to be provided to our government client related to Data Engineering and to provide new test data to support large scale performance testing and the functionality of the NCIC database system being transitioned to the cloud.
Job Responsibilities:
The Data Engineer shall work under the direction of the Government Task Lead. The Data Engineer will work with their Government counterparts on Sprint Teams through the Agile development of the N3G system.
The data engineer will be responsible for all aspects of data profiling, data design, data management, and production of test data.
Minimum Qualifications:
A minimum of six (6) years of data management, database administration or equivalent experience.
A minimum of four (4) years of data engineering, data analysis, or equivalent experience
Possess a Bachelor of Arts (B.A.) or Bachelor of Science (B.S.) degree from an accredited institution or equivalent experience related to the scope and objectives of the contract.
Top Secret Security Clearance.
In addition to the education and experience requirements, candidates shall possess superior verbal and written communication skills with the ability to work without supervision. Candidates shall also possess excellent analytical, interpersonal, and presentation skills.
Desired Qualifications:
Prior work experience with the federal government.
Prior work experience with either Amazon Web Services (AWS).
Familiarity with Agile development.
We are an equal opportunity employer. All applicants will receive consideration for employment, without regard to race, color, religion, creed, national origin, gender or gender-identity, age, marital status, sexual orientation, veteran status, disability, pregnancy or parental status, or any other basis prohibited by law. If you are an individual with a disability, and would like to request a reasonable accommodation for any part of the employment process, please contact us at job-assist@akima.com or 571-353-7053 (information about job applications status is not available at this contact information).
\#CJP-1234
Job:
Information Technology
Travel:
No
Organization:
TUVA
Clearance:
TS
Shift:
Day Job
Work Type:
Hybrid
Req ID:
TUV03083
Show more
Show less","Data Engineering, Data Analysis, Data Profiling, Data Design, Data Management, Database Administration, Amazon Web Services (AWS), Agile Development, Top Secret Security Clearance, Verbal Communication, Written Communication, Analytical Skills, Interpersonal Skills, Presentation Skills","data engineering, data analysis, data profiling, data design, data management, database administration, amazon web services aws, agile development, top secret security clearance, verbal communication, written communication, analytical skills, interpersonal skills, presentation skills","agile development, amazon web services aws, analytical skills, data design, data engineering, data management, data profiling, dataanalytics, database administration, interpersonal skills, presentation skills, top secret security clearance, verbal communication, written communication"
Data Analyst Lead,Zortech Solutions,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-lead%C2%A0-at-zortech-solutions-3727075199,2023-12-17,Mississauga, Canada,Mid senior,Onsite,"Job Title: Data Analyst Lead
Duration: Contract
Location: Toronto, ON
Job Description
Basic Function Data Analyst the Business Intelligence Data Analyst is responsible for supporting the Business Intelligence team and the business users through the analysis of quantitative & qualitative data.
This position will participate in the analysis required for the design and development of the enterprise data warehouse and related reporting structures and interfaces.
This role requires the ability to translate business problems into solutions/approaches driven by data analysis, business acumen, creativity, and clear, concise, and effective communication.
Responsibilities
1.Work with Business Users to define and document explicit data requirements for implementing subject areas in the Operational Data Store and Enterprise Data Warehouse.
2.Work with a team of data analysts to provide input and oversight over database designs / models / processes.
3.Perform in-depth data analysis by writing complex PL/SQL queries on various source systems, data marts and data warehouse to find trends and anomalies in the data in support of the requirements for various strategic and/or tactical projects.
4.Serve as Subject Matter Expert on Operational Data Stores, Data Warehouse, DataMart has; guide the development design activities with assumptions and data dependencies.
5.Identify data quality issues and concerns and support the data governance initiative by being involved in various data initiatives.
6.Conduct on-going analysis of the business, operating practices, and data flows within and between various systems and applications.
7.Keeps informed of existing and evolving best practice, industry standards and technologies.
8.Fosters and maintains good relationships with colleagues to meet expected customer service levels. 9.Maintain all appropriate training and continuing education requirements for both internal programs and external licenses and certifications.
10.Develop and maintain positive, productive, and professional relationships with key business collaborates to meet expected customer service levels.
11.Complies fully with all Bank Operational policies and procedures as well as all regulatory requirements (e.g., Bank Secrecy Act, Know Your Client, Community Reinvestment Act, Fair Lending Practices, Code of Conduct, etc.).
Job Skills And Knowledge
Minimum of 5 years of experience working with Information technologies in a senior role required.
Minimum of 5 years of experience as a Data Analyst, preferably in a data-intensive financial company.
Minimum of 5 years of experience in advanced PL/SQL and Oracle queries and analysis.
Experience in ETL and BI tools* Proficient in SQL, writing queries, scripts, and stored procedures.
Understanding of database models
Prior hands-on experience in using ETL tools, Business Objects, or other related BI tools.
Prior hands-on experience in using Erwin a plus.
Prior experience in the Financial Services industry a plus.
Excellent analytical skills
Able to communicate clearly and professionally with all levels of an organization.
Strong interpersonal, written, and verbal communication skills with demonstrated ability to work in a team environment.
Ability to express complex technical concepts in business terms.
Show more
Show less","SQL, PL/SQL, Oracle, ETL, Business Objects, Erwin, Data Warehouse, Data Analysis, Data Mart, Data Governance, Financial Services, Analytical Skills, Communication Skills, Complex Technical Concepts","sql, plsql, oracle, etl, business objects, erwin, data warehouse, data analysis, data mart, data governance, financial services, analytical skills, communication skills, complex technical concepts","analytical skills, business objects, communication skills, complex technical concepts, data governance, data mart, dataanalytics, datawarehouse, erwin, etl, financial services, oracle, plsql, sql"
Data Engineer Consultant,Apex Systems,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-consultant-at-apex-systems-3785823938,2023-12-17,Mississauga, Canada,Mid senior,Onsite,"Job#: 2006945
Job Description:
Data Engineer Consultant
Apex Systems is a global IT services provider, and our staffing practice has an opening for a Data Engineer Consultant with ETL, data management, and data processes experience to place at our client, a Big Five Bank.
Client:
Big Five Bank
Terms:
8-month contract. Possibility of extension or conversion.
Location:
Hybrid, downtown Toronto, 1 day a week in the office on Thursday.
Hours:
37.5 hours/week Monday-Friday, 9:00 am - 5:00 pm
Application Process:
It is best to apply via the medium on which you are seeing this posting. If you encounter technical difficulties submitting your resume, please send a Word version of your resume to Ashley at [email protected]
Job Description:
The Data Engineer is responsible for the development of highly scalable ETL and data flow applications adhering to software development best practices.
This position requires advanced SQL scripting who understands data visualization.
The person will design Data Warehouses in modern column-oriented database systems and develop data reporting solutions.
The position will be part of the IT Analytics team and will work closely with Architects, Scrum master, other members of the team and our business partners in a highly collaborative environment.
Job Responsibilities:
Develop ETL processes delivering high-quality code, following coding best practices.
Create and implement complex analytical data models that support reporting and analytics business requirements.
Prepare test scripts and perform functional unit, integration, and regression testing. Coordinate and drive user acceptance testing?
Support and troubleshoot issues/bugs in data pipeline and SQLs for analytics
Analyze the use of data, design and create data marts based on requirements.
Design and build reporting solutions.
Experiment with new/emerging technologies in the space and conduct POCs
Work with Functional/Business resources on day-to-day basis to understand business requirements and processes.
Work with Technical Lead/Architects to identify solutions/designs based on business requirements provided.
Provide status reports to Manager/Leads during task execution
Key Qualifications:
Bachelors degree in Computer Science, Engineering, or related field
4 years minimum of experience in data engineering with 2-3 years of hands on ETL experience
Experience working with enabling data solutions, analyzing business applications, strong technology/tools knowledge (Oracle, ThoughtSpot, Azure) and also related industry practices, Alteryx, SAS, Informatica
Strong knowledge and experience working with relation data bases/systems: advance SQL, user/group administration, access provisioning
Nice to Have:
Azure fundamentals
Experience in Banking/Finance industry with data management operations, processes, and governance
Soft Skills:
Presentation/interactive skills sufficient to convey both factual and conceptual information
Consulting skills (e.g., capable of promoting the benefits of data enablement function, considerable networking) – responsible for convincing the business group to adopt new data approaches, services, and/or products.
Creativity skills sufficient to resolve complex problems and/or identify innovative alternatives or opportunities where established procedures sometimes do not exist.
This a great opportunity to join a Big Five Bank and continue your career in the financial domain. Be a part of a great work environment with a very well organized team and colleagues who will help you succeed.
This is a position that impacts the bank enterprise wide with great opportunity for career growth within the bank.
If you are not a 99% match to the above, and want to be considered for other opportunities at our enterprise clients, register for our Talent Network where you can receive job alerts about new opportunities that match your interests.
EEO Employer
Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at [email protected] or 844-463-6178 .
Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.
Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.
4400 Cox Road
Suite 200
Glen Allen, Virginia 23060
Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at [email protected] (Do not submit resumes or solicit consultants to this email address). UnitedHealthcare creates and publishes the Transparency in Coverage Machine-Readable Files on behalf of Apex Systems.
Show more
Show less","SQL, ETL, Data visualization, Data warehouse, Columnoriented database, Data reporting, Data modeling, Unit testing, Integration testing, Regression testing, User acceptance testing, Data pipeline, Data mart, Azure fundamentals, Banking/Finance industry, Oracle, ThoughtSpot, Azure, Alteryx, SAS, Informatica, Relational databases, User/group administration, Access provisioning","sql, etl, data visualization, data warehouse, columnoriented database, data reporting, data modeling, unit testing, integration testing, regression testing, user acceptance testing, data pipeline, data mart, azure fundamentals, bankingfinance industry, oracle, thoughtspot, azure, alteryx, sas, informatica, relational databases, usergroup administration, access provisioning","access provisioning, alteryx, azure, azure fundamentals, bankingfinance industry, columnoriented database, data mart, data pipeline, data reporting, datamodeling, datawarehouse, etl, informatica, integration testing, oracle, regression testing, relational databases, sas, sql, thoughtspot, unit testing, user acceptance testing, usergroup administration, visualization"
(5461) Data Analyst,"Merit321, Launching Careers","Fort Gordon, GA",https://www.linkedin.com/jobs/view/5461-data-analyst-at-merit321-launching-careers-3768040413,2023-12-17,Martinez,United States,Associate,Onsite,"Position:
(5461) Data Analyst
Location:
Ft. Gordon, GA (Hybrid)
Clearance:
Active Secret Clearance
Our client is seeking an experienced Data Analyst to join our team. The Data Analyst will collect and analyze data across the customers mission space to make informed decisions. Data will involve structured and unstructured data from information advantage activities in support of multi-domain operations. Work is performed in a hybrid role with some on-customer site support.
Essential Job Responsibilities
Regular activities consist of interpreting data, analyzing results using statistical techniques, and providing reports.
Develop and implement data exploration techniques, data analytics, and other strategies that optimize data efficiency and quality.
Identify, analyze, and interpret trends or patterns in complex data sets.
Support initiatives for data integrity and normalization, including data sources, security, and metadata.
Providing technical expertise in data storage structures, data mining, and data cleansing.
Demonstrated ability to work well independently with little input, and as a part of a team.
Minimum Qualifications
5+ years of experience in cyber security operations related fields and a Bachelors in related field or 3 years experience with Masters; or High School Diploma and 9 years experience.
Excellent verbal, written, analytical, and presentation skills.
Excellent work ethic and a high commitment to quality.
Proven working experience as a Data Analyst or Business Data Analyst
Ability to work with stakeholders to assess potential risks.
Ability to translate business requirements into non-technical, Client terms.
High-level experience in methodologies and processes for managing large-scale databases.
Demonstrated experience in handling large data sets and relational databases.
Technical expertise regarding data models, database design development, data mining and segmentation techniques
Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, or ETL frameworks)
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Adept at queries, report writing and presenting findings
Must be able to work a hybrid role with some on customer site support
Desired Skills (Optional)
Prior experience interfacing and consulting with customers is a big plus.
Experience in data mining and sentiment analysis of social media datasets.
EEO
It is the policy of Merit321 to provide equal opportunity in recruiting, hiring, training, and promoting individuals in all job categories without regard to race, color, religion, national origin, gender, age, disability, genetic information, veteran status, sexual orientation, gender identity, or any other protected class or category as may be defined by federal, state, or local laws or regulations.
Show more
Show less","Data Analysis, Statistical Techniques, Data Exploration, Data Analytics, Data Mining, Data Cleansing, Data Normalization, Data Integrity, Database Design, Programming, XML, JavaScript, ETL Frameworks, Business Objects, SQL, Queries, Report Writing, Data Modeling","data analysis, statistical techniques, data exploration, data analytics, data mining, data cleansing, data normalization, data integrity, database design, programming, xml, javascript, etl frameworks, business objects, sql, queries, report writing, data modeling","business objects, data exploration, data integrity, data mining, data normalization, dataanalytics, database design, datacleaning, datamodeling, etl frameworks, javascript, programming, queries, report writing, sql, statistical techniques, xml"
Future Opportunity- Data Engineering Consultant,Avanade,"Augusta, GA",https://www.linkedin.com/jobs/view/future-opportunity-data-engineering-consultant-at-avanade-3781783099,2023-12-17,Martinez,United States,Associate,Onsite,"Job Description
RECRUITING FOR THE FUTURE
This posting is for a future job opportunity.
To support the needs of our clients, Avanade is actively recruiting and interviewing for the role outlined below, as we continue to build on another year of growth in our business. Now, with more than 60,000 employees around the globe, we are positioning our organization to effectively prepare for the future. What does this mean for you? We encourage you to apply and interview for this role without the need to decide now. This allows you to connect with leaders and hiring managers at a pace that works for you and when roles become available, you will be the first know.
Data Engineering Consultant
Our talented Data and AI Practice is made up of globally recognized experts- and there's room for more analytical and ambitious data professionals. If you're passionate about helping clients make better data-driven decisions to tackle their most complex business issues. let's talk. Take your skills to a new level and launch a career where you can can truly do what matters.
Come join us
As a Data Engineer within our Data and AI practice, you’ll design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.
Together we do what matters.
What You'll Do
Use your knowledge to plan and deliver data warehouses and storage
Take part in crafting and running bespoke data services for individual projects
Stay up to date with business best practice in using and retrieving data
Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining
Customize storage and extraction, metadata, and information repositories
Build and use effective metrics and monitoring processes
Help to develop business intelligence tools
Craft and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
Skills And Experiences
Strong knowledge of Python, Spark, and T-SQL
Microsoft Fabric/Synapse, Purview and Azure Databricks, PowerBI
Database, storage, collection and aggregation models, techniques, and technologies – and how to apply them in business
Ability to use technology to aim business problems using one or more Microsoft Analytics services for building data pipelines, data streams, and system integration
About You
Characteristics that can spell success for this role:
Advanced knowledge in entity and relationship extraction from unstructured data
Understanding of database table indexing, with emphasis on clustered column store tables
Proficient in data handling, understanding, analysis and interpretation of results
Experience with SQL technologies: Databricks (Spark), OR Azure Synapse
Data security: Must be able to demonstrate an understanding of data security at rest and in transit
Scale: Deliver performance at scale, and accurately anticipate system enhancement needs
Data integrity: Data manipulation, error identification and handling, and modeling
Enjoy your career
Some of the best things about working at Avanade
People-first culture that supports innovation and encourages people to move forward
Turn your ideas to human impact by cultivating a team that will help clients unlock what’s next
Opportunities to innovate in the Microsoft platform while charting your own career path
On-the-job growth through hackathons, innovation contests, and other expressions of our passion for innovating with purpose
Real-time access to technical and skilled resources globally
Find out more about some of our benefits here .
A great place to work
As you bring your skills and abilities to Avanade, you’ll get distinctive experiences, limitless learning, and ambitious growth in return. As we continue to build our diverse and inclusive culture, we become even more innovative and creative, helping us better serve our clients and our communities. You’ll join a community of smart, supportive collaborators to lift, mentor, and guide you, but to also lean on your expertise . You get a company purpose-built for business-critical, leading-edge technology solutions, committed to improving the way humans work, interact, and live. It’s all here, so take a closer look!
We work hard to provide an inclusive, diverse culture with a deep sense of belonging for all our employees. Visit our Inclusion & Diversity page.
Create a future for our people that focuses on
Expanding your thinking
Experimenting courageously
Learning and pivoting
Inspire greatness in our people by
Empowering every voice
Encouraging boldness
Celebrating progress
Accelerate the impact of our people by
Amazing the client
Prioritizing what matters
Acting as one
Learn more
To learn more about the types of projects our Data & AI team works on check out these case studies:
thyssenkrupp Materials Services uses data to help strike a delicate operational balance
What matters to SSE Renewables is innovating for a sustainable future
Hachette UK enables employees with machine learning-driven contract searches
Marston Holdings identifies significant cloud savings
Interested in knowing what’s going on inside Avanade? Check out our blogs:
Avanade Insights – exchange ideas that drive tomorrow’s innovation
Inside Avanade – explore what life is like working at Avanade
Compensation for roles at Avanade varies depending on a wide array of factors including but not limited to the specific office location, role, skill set and level of experience. As required by local law, Avanade provides a reasonable range of compensation for roles that may be hired in New York as set forth below and information on benefits offered is here.
Range of Starting Pay for role:
New York: $121,500 – $ 143,000
About Avanade
Avanade is the leading provider of innovative digital, cloud and advisory services, industry solutions and design-led experiences across the Microsoft ecosystem. Every day, our 59,000 professionals in 26 countries make a genuine human impact for our clients, their employees and their customers.
We have been recognized as Microsoft’s Global SI Partner of the Year more than any other company. With the most Microsoft certifications (60,000+) and 18 (out of 18) Gold-level Microsoft competencies, we are uniquely positioned to help businesses grow and solve their toughest challenges.
We are a people first company, committed to providing an inclusive workplace where employees feel comfortable being their authentic selves. As a responsible business, we are building a sustainable world and helping people from underrepresented communities fulfil their potential.
Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com .
Show more
Show less","Python, Spark, TSQL, Microsoft Fabric/Synapse, Purview, Azure Databricks, PowerBI, Data pipelines, Data streams, System integration, Entity extraction, Relationship extraction, Database indexing, Clustered column store tables, Data handling, Data understanding, Data analysis, Data interpretation, SQL technologies, Data security, Data manipulation, Error identification, Error handling, Data modeling, SQL, Databricks, Azure Synapse","python, spark, tsql, microsoft fabricsynapse, purview, azure databricks, powerbi, data pipelines, data streams, system integration, entity extraction, relationship extraction, database indexing, clustered column store tables, data handling, data understanding, data analysis, data interpretation, sql technologies, data security, data manipulation, error identification, error handling, data modeling, sql, databricks, azure synapse","azure databricks, azure synapse, clustered column store tables, data handling, data interpretation, data manipulation, data security, data streams, data understanding, dataanalytics, database indexing, databricks, datamodeling, datapipeline, entity extraction, error handling, error identification, microsoft fabricsynapse, powerbi, purview, python, relationship extraction, spark, sql, sql technologies, system integration, tsql"
Senior Data Engineer,"Geo Owl - GIS, Geospatial, and UAS Experts","Fort Gordon, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-geo-owl-gis-geospatial-and-uas-experts-3787920827,2023-12-17,Martinez,United States,Mid senior,Onsite,"Geo Owl is currently looking for a motivated and qualified
Senior Data Engineer
to support our Department of Defense contract opportunity. To be qualified, you need knowledge of Army structure and defense level intelligence, intelligence collection, fusion, analysis, production, and dissemination for intelligence databases and products, and meet the requirements listed below. If interested, apply now, or contact one of our recruiters.
Location:
Fort Gordon, GA
Clearance:
TS/SCI
Requirements:
Must meet all the requirements listed below.
Excellent written & oral communication, research, and analytic skills
Expert ability to manage personnel, requirements, and coordination of projects
Expert capabilities to research, create, develop, and deliver professional briefings, multimedia presentations, and written reports
Experience utilizing programming languages such as SAS, R, Java, C, MATLAB, ScaLa, or Python; experience accelerating large data transactions across industry-leading GPU architectures to answer analytic questions
Experience with assessments, enterprise data integration, governance, and metrics, including the application of metadata management techniques and ability to interrogate databases efficiently using SQL
Experience with tradecraft and publication; ability to coordinate and support cross-community meetings and working groups; assimilate large volumes of information, and independently produce reports using data science focused libraries such as Pandas, Scikit, TensorFlow and Gensim to answer analytical questions
Desired Requirements:
Knowledge of Army structure and defense level intelligence operations: intelligence collection, fusion, analysis, production, and dissemination for intelligence databases and products
Knowledge and experience with intelligence operations and in assisting with drafting expert assessments across operations priorities on behalf of the stakeholder
Specialized training from any intelligence collection and analysis school or certification to include GEOINT Professional Certification (GPC-F, GPC_IA-II, GPC_GA-II, GPC_IS-II, etc)
Knowledge and understanding of the National System for GEOINT (NSG) and Intelligence Community; knowledge of private sector data science/analytics, machine learning, and data visualization communities
Education Requirements:
MA or MS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 2 years CURRENT Intelligence Analysis experience; OR
BA or BS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 5 years CURRENT Intelligence Analysis experience; OR
Undergraduate degree with graduate/professional certificate in Data Science, Data Analytics, Informatics, Statistics, or related field AND at least 10 years of Intelligence Analysis experience
Benefits:
Health Insurance (Geo Owl pays 80%+ of the premium).
401k matching.
Dental, Vision, and other supplemental insurance plans available.
Company-paid short-term and long-term disability and life insurance.
Peer-to-Peer spot bonuses.
120 hours of PTO per year plus federal holidays.
Joining the Geo Owl Team | What to Expect
At Geo Owl, we highly value our team members. We offer challenging but rewarding opportunities for those who want to work hard to provide a great experience for the customer and strive to reach their professional goals. As a member of the Geo Owl family, you will be working alongside people who share this work ethic and are aiming to be the best partner for our customer.
We are all proud to be a part of this company and we want you to be too.
Our Mission
Provide high quality solutions to our mission partners in the United States through our expert analysts.
Be recognized as the best at what we do by our customers.
Be a team our team members are proud and excited to be a part of.
Continually strive for excellence and seek to tackle the most difficult challenges our industry has to offer.
About Us
Geo Owl is a premiere provider of Full-Motion Video (FMV), Geospatial, ISR, Intelligence and IT services to the Department of Defense and Intelligence Community. We are vitalized by our engaged team of professionals that truly value each other and the important missions we support.
Equal Opportunities
Geo Owl is an equal opportunity employer and does not discriminate on the basis of race, color, religion, creed, sex, age, sexual orientation, national origin, disability, marital status, military status, genetic predisposition, or any other basis protected by law.
To stay up to date about new career opportunities:
Follow us on
Follow us on
Follow us on
Powered by JazzHR
I8pR3c9K0u
Show more
Show less","Java, C, MATLAB, ScaLa, Python, SAS, R, SQL, Metadata management, TensorFlow, Gensim, Pandas, Scikit, GPU, Intelligence collection, Fusion, Analysis, Production, Dissemination, Intelligence databases, Intelligence products, Assessments, Enterprise data integration, Governance, Metrics, Tradecraft, Publication, GEOINT Professional Certification, National System for GEOINT (NSG), Intelligence Community, Data science, Analytics, Machine learning, Data visualization","java, c, matlab, scala, python, sas, r, sql, metadata management, tensorflow, gensim, pandas, scikit, gpu, intelligence collection, fusion, analysis, production, dissemination, intelligence databases, intelligence products, assessments, enterprise data integration, governance, metrics, tradecraft, publication, geoint professional certification, national system for geoint nsg, intelligence community, data science, analytics, machine learning, data visualization","analysis, analytics, assessments, c, data science, dissemination, enterprise data integration, fusion, gensim, geoint professional certification, governance, gpu, intelligence collection, intelligence community, intelligence databases, intelligence products, java, machine learning, matlab, metadata management, metrics, national system for geoint nsg, pandas, production, publication, python, r, sas, scala, scikit, sql, tensorflow, tradecraft, visualization"
Data Analyst (Hybrid),Cyberjin,"Augusta, GA",https://www.linkedin.com/jobs/view/data-analyst-hybrid-at-cyberjin-3787916640,2023-12-17,Martinez,United States,Mid senior,Onsite,"Looking for an experienced Data Analyst to join our team. The Data Analyst will collect and analyze data across the customer’s mission space to make informed decisions. Data will involve structured and unstructured data from information advantage activities in support of multi-domain operations. Work is performed in a hybrid role with some on customer site support.
Essential Job Responsibilities
Regular activities consist of interpreting data, analyzing results using statistical techniques, and providing reports.
Develop and implement data exploration techniques, data analytics, and other strategies that optimize data efficiency and quality.
Identify, analyze, and interpret trends or patterns in complex data sets.
Support initiatives for data integrity and normalization, including data sources, security, and metadata.
Providing technical expertise in data storage structures, data mining, and data cleansing.
Demonstrated ability to work well independently with little input, and as a part of a team.
Other duties as assigned.
Minimum Qualifications
Security Clearance - Current Secret level clearance is required and therefore all candidates must be a U.S. Citizen. Can also consider candidates with a TS/SCI clearance.
5+ years of experience in cyber security operations related fields and a Bachelors in related field or 3 years experience with Masters; or High School Diploma and 9 years experience.
Excellent verbal, written, analytical, and presentation skills.
Excellent work ethic and a high commitment to quality.
Proven working experience as a Data Analyst or Business Data Analyst
Ability to work with stakeholders to assess potential risks.
Ability to translate business requirements into non-technical, lay terms.
High-level experience in methodologies and processes for managing large-scale databases.
Demonstrated experience in handling large data sets and relational databases.
Technical expertise regarding data models, database design development, data mining and segmentation techniques
Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, or ETL frameworks)
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Adept at queries, report writing and presenting findings
Must be able to work a hybrid role with some on customer site support
Preferred Requirements
Prior experience interfacing and consulting with customers is a big plus.
Experience in data mining and sentiment analysis of social media datasets.
Powered by JazzHR
wkzwsvNUMd
Show more
Show less","Data Analytics, Statistical Techniques, Data Exploration, Data Integrity, Data Normalization, Data Storage Structures, Data Mining, Data Cleansing, SQL, XML, JavaScript, ETL Frameworks, Business Objects, Relational Databases, Data Models, Database Design Development, Data Mining Techniques, Segmentation Techniques, Social Media Data Mining, Sentiment Analysis","data analytics, statistical techniques, data exploration, data integrity, data normalization, data storage structures, data mining, data cleansing, sql, xml, javascript, etl frameworks, business objects, relational databases, data models, database design development, data mining techniques, segmentation techniques, social media data mining, sentiment analysis","business objects, data exploration, data integrity, data mining, data mining techniques, data models, data normalization, data storage structures, dataanalytics, database design development, datacleaning, etl frameworks, javascript, relational databases, segmentation techniques, sentiment analysis, social media data mining, sql, statistical techniques, xml"
Data Analyst (Hybrid),Cyberjin,"Augusta, GA",https://www.linkedin.com/jobs/view/data-analyst-hybrid-at-cyberjin-3787911795,2023-12-17,Martinez,United States,Mid senior,Onsite,"Looking for an experienced Data Analyst to join our team. The Data Analyst will collect and analyze data across the customer’s mission space to make informed decisions. Data will involve structured and unstructured data from information advantage activities in support of multi-domain operations. Work is performed in a hybrid role with some on customer site support.
Essential Job Responsibilities
Regular activities consist of interpreting data, analyzing results using statistical techniques, and providing reports.
Develop and implement data exploration techniques, data analytics, and other strategies that optimize data efficiency and quality.
Identify, analyze, and interpret trends or patterns in complex data sets.
Support initiatives for data integrity and normalization, including data sources, security, and metadata.
Providing technical expertise in data storage structures, data mining, and data cleansing.
Demonstrated ability to work well independently with little input, and as a part of a team.
Other duties as assigned.
Minimum Qualifications
Security Clearance - Current Secret level clearance is required and therefore all candidates must be a U.S. Citizen. Can also consider candidates with a TS/SCI clearance.
5+ years of experience in cyber security operations related fields and a Bachelors in related field or 3 years experience with Masters; or High School Diploma and 9 years experience.
Excellent verbal, written, analytical, and presentation skills.
Excellent work ethic and a high commitment to quality.
Proven working experience as a Data Analyst or Business Data Analyst
Ability to work with stakeholders to assess potential risks.
Ability to translate business requirements into non-technical, lay terms.
High-level experience in methodologies and processes for managing large-scale databases.
Demonstrated experience in handling large data sets and relational databases.
Technical expertise regarding data models, database design development, data mining and segmentation techniques
Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, or ETL frameworks)
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Adept at queries, report writing and presenting findings
Must be able to work a hybrid role with some on customer site support
Preferred Requirements
Prior experience interfacing and consulting with customers is a big plus.
Experience in data mining and sentiment analysis of social media datasets.
Powered by JazzHR
buHmhxhJSj
Show more
Show less","Data Analysis, Data Analytics, Statistical Techniques, Data Exploration, Data Mining, Data Integrity, Data Normalization, Data Storage Structures, Data Cleansing, Data Visualization, Reporting Packages, Databases, Programming, Data Models, Database Design, Data Segmentation, ETL Frameworks, SQL, XML, JavaScript, Queries, Report Writing, Presentation Skills","data analysis, data analytics, statistical techniques, data exploration, data mining, data integrity, data normalization, data storage structures, data cleansing, data visualization, reporting packages, databases, programming, data models, database design, data segmentation, etl frameworks, sql, xml, javascript, queries, report writing, presentation skills","data exploration, data integrity, data mining, data models, data normalization, data segmentation, data storage structures, dataanalytics, database design, databases, datacleaning, etl frameworks, javascript, presentation skills, programming, queries, report writing, reporting packages, sql, statistical techniques, visualization, xml"
Data Engineer Senior with Security Clearance,ClearanceJobs,"Fort Gordon, GA",https://www.linkedin.com/jobs/view/data-engineer-senior-with-security-clearance-at-clearancejobs-3753471671,2023-12-17,Martinez,United States,Mid senior,Onsite,"Geo Owl is seeking a Data Engineer with knowledge of Army structure and defense level intelligence,
intelligence collection, fusion, analysis, production, and dissemination for intelligence databases and products. Clearance:
TS/SCI Clearance is required. Locations:
Ft. Gordon, GA POSITION REQUIREMENTS Minimum Education Qualifications:
MA or MS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 2 years CURRENT Intelligence Analysis experience; OR
BA or BS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 5 years CURRENT
Intelligence Analysis experience; OR
Undergraduate degree with graduate/professional certificate in Data Science, Data Analytics, Informatics,
Statistics, or related field AND at least 10 years of Intelligence Analysis experience Minimum Qualifications:
Excellent written & oral communication, research, and analytic skills
Expert ability to manage personnel, requirements, and coordination of projects
Expert capabilities to research, create, develop, and deliver professional briefings, multimedia presentations, and written reports
Experience utilizing programming languages such as SAS, R, Java, C, MATLAB, ScaLa, or Python; experience accelerating large data transactions across industry-leading GPU architectures to answer analytic questions
Experience with assessments, enterprise data integration, governance, and metrics, including the application of metadata management techniques and ability to interrogate databases efficiently using SQL
Experience with tradecraft and publication; ability to coordinate and support cross-community meetings and working groups; assimilate large volumes of information, and independently produce reports using data science focused libraries such as Pandas, Scikit, TensorFlow and Gensim to answer analytical questions Desired Experience:
Knowledge of Army structure and defense level intelligence operations: intelligence collection, fusion,
analysis, production, and dissemination for intelligence databases and products
Knowledge and experience with intelligence operations and in assisting with drafting expert assessments
across operations priorities on behalf of the stakeholder
Specialized training from any intelligence collection and analysis school or certification to include GEOINT Professional Certification (GPC-F, GPC_IA-II, GPC_GA-II, GPC_IS-II, etc)
Knowledge and understanding of the National System for GEOINT (NSG) and Intelligence Community;
knowledge of private sector data science/analytics, machine learning, and data visualization communities
Show more
Show less","SAS, R, Java, C, MATLAB, ScaLa, Python, Pandas, Scikit, TensorFlow, Gensim, SQL, GEOINT Professional Certification, National System for GEOINT (NSG)","sas, r, java, c, matlab, scala, python, pandas, scikit, tensorflow, gensim, sql, geoint professional certification, national system for geoint nsg","c, gensim, geoint professional certification, java, matlab, national system for geoint nsg, pandas, python, r, sas, scala, scikit, sql, tensorflow"
Data Analyst (Hybrid) - 16671 with Security Clearance,ClearanceJobs,"Augusta, GA",https://www.linkedin.com/jobs/view/data-analyst-hybrid-16671-with-security-clearance-at-clearancejobs-3753453593,2023-12-17,Martinez,United States,Mid senior,Onsite,"Benefits
Enlighten, honored as a Top Workplace from the Baltimore Sun, is a leader in big data solution development and deployment, with expertise in cloud-based services, software and systems engineering, cyber capabilities, and data science. Enlighten provides continued innovation and proactivity in meeting our customers' greatest challenges. We recognize that the most effective environment for your projects doesn't always look the same. Our hybrid work approach ensures that you can make lasting relationships with your team and collaborate in-person to get the job done-while having the flexibility to work from home when needed to achieve focused results. Why Enlighten? At Enlighten, our team's unwavering work ethic, top talent and celebration of innovative ideas have helped us thrive. We know that our employees are essential to our company's success, so we seek to take care of you as much as you take care of us. Here are a few highlights of our benefits package:
100% paid employee premium for healthcare, vision and dental plans.
10% 401k benefit.
Generous PTO + 10 paid holidays.
Education/training allowances. Job Description Enlighten is looking for an experienced Data Analyst to join our team. The Data Analyst will collect and analyze data across the customer's mission space to make informed decisions. Data will involve structured and unstructured data from information advantage activities in support of multi-domain operations. Work is performed in a hybrid role with some on customer site support. Essential Job Responsibilities * Regular activities consist of interpreting data, analyzing results using statistical techniques, and providing reports.
Develop and implement data exploration techniques, data analytics, and other strategies that optimize data efficiency and quality.
Identify, analyze, and interpret trends or patterns in complex data sets.
Support initiatives for data integrity and normalization, including data sources, security, and metadata.
Providing technical expertise in data storage structures, data mining, and data cleansing.
Demonstrated ability to work well independently with little input, and as a part of a team.
Other duties as assigned.
Minimum Qualifications * Security Clearance - Current Secret level clearance is required and therefore all candidates must be a U.S. Citizen. Can also consider candidates with a TS/SCI clearance.
5+ years of experience in cyber security operations related fields and a Bachelors in related field or 3 years experience with Masters; or High School Diploma and 9 years experience.
Excellent verbal, written, analytical, and presentation skills.
Excellent work ethic and a high commitment to quality.
Proven working experience as a Data Analyst or Business Data Analyst
Ability to work with stakeholders to assess potential risks.
Ability to translate business requirements into non-technical, lay terms.
High-level experience in methodologies and processes for managing large-scale databases.
Demonstrated experience in handling large data sets and relational databases.
Technical expertise regarding data models, database design development, data mining and segmentation techniques
Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, or ETL frameworks)
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Adept at queries, report writing and presenting findings
Must be able to work a hybrid role with some on customer site support
Preferred Requirements * Prior experience interfacing and consulting with customers is a big plus.
Experience in data mining and sentiment analysis of social media datasets.
We have many more additional great benefits/perks that you can find on our website at www.eitccorp.com [eitccorp.com]. Enlighten, an HII Company, is an Equal Opportunity/Veterans and Disabled Employer. U.S. citizenship may be required for certain positions. HII Is committed to cultivating an inclusive company culture to promote collaboration and enhance creativity by hiring a diverse work force.
Show more
Show less","Data Analysis, Data Mining, SQL, Java, Data Structures, Data Storage, Data Cleaning, Data Modeling, Database Design, Database Development, Data Warehousing, Data Integration, Big Data, Data Visualization, Reporting, Business Intelligence, Statistical Techniques, Data Security, Data Integrity, Data Normalization, Data Exploration, Data Analytics, Data Transformation, Data Quality, Data Governance, Data Management, Data Architecture, Data Visualization, Software Development, Programming Languages, Data Science, Machine Learning, Artificial Intelligence, Cloud Computing, Cyber Security, Data Mining, Sentiment Analysis","data analysis, data mining, sql, java, data structures, data storage, data cleaning, data modeling, database design, database development, data warehousing, data integration, big data, data visualization, reporting, business intelligence, statistical techniques, data security, data integrity, data normalization, data exploration, data analytics, data transformation, data quality, data governance, data management, data architecture, data visualization, software development, programming languages, data science, machine learning, artificial intelligence, cloud computing, cyber security, data mining, sentiment analysis","artificial intelligence, big data, business intelligence, cloud computing, cyber security, data architecture, data cleaning, data exploration, data governance, data integration, data integrity, data management, data mining, data normalization, data quality, data science, data security, data storage, data structures, data transformation, dataanalytics, database design, database development, datamodeling, datawarehouse, java, machine learning, programming languages, reporting, sentiment analysis, software development, sql, statistical techniques, visualization"
Data Analyst (Hybrid) - 16671 with Security Clearance,ClearanceJobs,"Augusta, GA",https://www.linkedin.com/jobs/view/data-analyst-hybrid-16671-with-security-clearance-at-clearancejobs-3753450202,2023-12-17,Martinez,United States,Mid senior,Onsite,"Benefits
Enlighten, honored as a Top Workplace from the Baltimore Sun, is a leader in big data solution development and deployment, with expertise in cloud-based services, software and systems engineering, cyber capabilities, and data science. Enlighten provides continued innovation and proactivity in meeting our customers' greatest challenges. We recognize that the most effective environment for your projects doesn't always look the same. Our hybrid work approach ensures that you can make lasting relationships with your team and collaborate in-person to get the job done-while having the flexibility to work from home when needed to achieve focused results. Why Enlighten? At Enlighten, our team's unwavering work ethic, top talent and celebration of innovative ideas have helped us thrive. We know that our employees are essential to our company's success, so we seek to take care of you as much as you take care of us. Here are a few highlights of our benefits package:
100% paid employee premium for healthcare, vision and dental plans.
10% 401k benefit.
Generous PTO + 10 paid holidays.
Education/training allowances. Job Description Enlighten is looking for an experienced Data Analyst to join our team. The Data Analyst will collect and analyze data across the customer's mission space to make informed decisions. Data will involve structured and unstructured data from information advantage activities in support of multi-domain operations. Work is performed in a hybrid role with some on customer site support. Essential Job Responsibilities * Regular activities consist of interpreting data, analyzing results using statistical techniques, and providing reports.
Develop and implement data exploration techniques, data analytics, and other strategies that optimize data efficiency and quality.
Identify, analyze, and interpret trends or patterns in complex data sets.
Support initiatives for data integrity and normalization, including data sources, security, and metadata.
Providing technical expertise in data storage structures, data mining, and data cleansing.
Demonstrated ability to work well independently with little input, and as a part of a team.
Other duties as assigned.
Minimum Qualifications * Security Clearance - Current Secret level clearance is required and therefore all candidates must be a U.S. Citizen. Can also consider candidates with a TS/SCI clearance.
5+ years of experience in cyber security operations related fields and a Bachelors in related field or 3 years experience with Masters; or High School Diploma and 9 years experience.
Excellent verbal, written, analytical, and presentation skills.
Excellent work ethic and a high commitment to quality.
Proven working experience as a Data Analyst or Business Data Analyst
Ability to work with stakeholders to assess potential risks.
Ability to translate business requirements into non-technical, lay terms.
High-level experience in methodologies and processes for managing large-scale databases.
Demonstrated experience in handling large data sets and relational databases.
Technical expertise regarding data models, database design development, data mining and segmentation techniques
Strong knowledge of and experience with reporting packages (Business Objects etc), databases (SQL etc), programming (XML, Javascript, or ETL frameworks)
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Adept at queries, report writing and presenting findings
Must be able to work a hybrid role with some on customer site support
Preferred Requirements * Prior experience interfacing and consulting with customers is a big plus.
Experience in data mining and sentiment analysis of social media datasets.
We have many more additional great benefits/perks that you can find on our website at www.eitccorp.com [eitccorp.com]. Enlighten, an HII Company, is an Equal Opportunity/Veterans and Disabled Employer. U.S. citizenship may be required for certain positions. HII Is committed to cultivating an inclusive company culture to promote collaboration and enhance creativity by hiring a diverse work force.
Show more
Show less","Data Analysis, Statistical Techniques, Data Exploration, Data Analytics, Data Efficiency, Data Quality, Data Integrity, Data Normalization, Data Storage Structures, Data Mining, Data Cleansing, Big Data, CloudBased Services, Software Engineering, Cyber Capabilities, Data Science, Business Objects, SQL, XML, Javascript, ETL Frameworks, Programming, Databases, Data Models, Database Design Development, Data Mining Techniques, Data Segmentation Techniques, Reporting Packages, Queries, Report Writing, Data Presentation, Relational Databases","data analysis, statistical techniques, data exploration, data analytics, data efficiency, data quality, data integrity, data normalization, data storage structures, data mining, data cleansing, big data, cloudbased services, software engineering, cyber capabilities, data science, business objects, sql, xml, javascript, etl frameworks, programming, databases, data models, database design development, data mining techniques, data segmentation techniques, reporting packages, queries, report writing, data presentation, relational databases","big data, business objects, cloudbased services, cyber capabilities, data efficiency, data exploration, data integrity, data mining, data mining techniques, data models, data normalization, data presentation, data quality, data science, data segmentation techniques, data storage structures, dataanalytics, database design development, databases, datacleaning, etl frameworks, javascript, programming, queries, relational databases, report writing, reporting packages, software engineering, sql, statistical techniques, xml"
Lead Mechanical Engineer (Mission Critical/Data Center),WSP in the U.S.,"Augusta, GA",https://www.linkedin.com/jobs/view/lead-mechanical-engineer-mission-critical-data-center-at-wsp-in-the-u-s-3757436532,2023-12-17,Martinez,United States,Mid senior,Hybrid,"Who We Are
At WSP, we are driven by inspiring future-ready pioneers to innovate. We’re looking to grow our teams with people who are ready to collaborate in building communities and expanding our skylines. To do this, we hire candidates of all experiences, skillsets, backgrounds and walks of life. We actively foster a work environment and culture where inclusion and diversity is part of our fundamental structure. This is delivered behaviorally, through our policies, trainings, local partnerships with professional diverse organizations, internal networks and most importantly with the support and sponsorship of our leaders who help drive our commitment to an inclusive, diverse, welcoming and equitable work environment. Anything is within our reach and yours as a WSP employee. Come join us and help shape the future!
Great people. Great places. Great projects. kW Mission Critical Engineering, a WSP company, is a high-performance, fast-paced consulting engineering firm designing data centers and mission critical environments across the globe. We hire smart, responsive, team players to work in collaborative and mentoring office settings. Our mechanical, electrical, plumbing, fire protection, controls, telecommunications, and security building system designs keep many of the world’s top Fortune 100 financial, technology, enterprise, hyperscale, and colocation companies up and running 24 hours a day, 365 days a year.
We work on innovative, award-winning, large-scale projects. We travel to construction sites to see our designs being built. As part of WSP, we are able to offer our employees increased professional development and career opportunities in addition to kW MCE’s office culture which is consistently recognized as one of the “Best Places to Work.” Join our great people at our great places designing great projects.
This Opportunity
kW Mission Critical Engineering, a member of WSP USA, is currently initiating a search for a
Lead Mechanical Engineer
that can be located out of our
kW office
out of
Atlanta, GA office (115 Perimeter Center Place, Suite 640 Atlanta GA 30346)
As a Mechanical Engineer with us, you will design complex cooling and HVAC systems including air distribution systems, chiller plants, and alternative energy solutions for mission critical facilities.
Your Impact
Independently support the project team during design and construction stages of projects
Design air distribution, hydronic and automated temperature controls systems
Integrate complex mechanical engineering requirements into facility designs
Work within multi-discipline project teams to develop drawing and specification documents for issuance to architects, contractors and building owners
Attend client meetings and contribute to discussions
Collaborate and coordinate with internal project discipline team members and external vendors and manufacturers
Communicate mechanical engineering concepts and decisions to clients and stakeholders
Interact regularly with clients, which includes maintaining current relationships and developing new relationships
Mentor and train junior engineers
Track and coordinate all mechanical disciplines: HVAC, Energy, Controls, Fire Protection, Fire Alarm, Plumbing, Fuel Oil Storage / Management / Distribution
Provide oversight of all aspects of mechanical design, review systems, drawings prior to issuance
Perform Computational Fluid Dynamic (CFD) evaluations for existing and new facilities, both internal and external
Select and schedule major equipment
Develop project specifications
Survey and evaluate existing conditions
Perform construction administration tasks
Who You Are
The ideal candidate has familiarity with Building Information Modeling using Revit, has strong communication skills, and an interest in liaising with internal and external design, client, and construction team members. Candidate should be willing to travel to client sites.
Required Qualifications
Bachelor’s degree in Mechanical Engineering or Architectural Engineering with mechanical building systems emphasis
7+ years of experience in designing mechanical systems for the high performing, commercial, industrial or mission critical/data center buildings
Excellent interpersonal skills, teamwork, and written and verbal communication skills
Proficiency with applicable software including AutoCAD, Revit, Trane Trace and Pipeflow
Knowledge of building, mechanical and energy codes
Preferred Qualifications:
EIT or Registered Professional Engineer (PE), if eligible
Experience with the analysis and modeling of interior and exterior Computational Fluid Dynamics of airflow
Mission Critical/Data Center experience
Experience with international projects
Additional Requirements
To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform essential job functions.
Additional Details
Travel Required: 20%
Job Status: Regular
Employee Type: Full
Primary Location: ATLANTA - PERIMETER CENTER
All locations: US-GA-Atlanta, US-GA-Augusta, US-GA-College Park, US-GA-Kennesaw, US-GA-Marietta
About WSP
WSP USA is the U.S. operating company of WSP, one of the world's leading engineering and professional services firms. Dedicated to serving local communities, we are engineers, planners, technical experts, strategic advisors and construction management professionals. WSP USA designs lasting solutions in the buildings, transportation, energy, water and environment markets. With more than 15,000 employees in over 300 offices across the U.S., we partner with our clients to help communities prosper. www.wsp.com
WSP provides a flexible and agile workplace model while meeting client needs. Employees are also afforded a comprehensive suite of benefits including medical, dental, vision, disability, life, and retirement savings focused on providing health and financial stability throughout the employee’s career.
At WSP, we want to give our employees the challenges they seek to grow their careers and knowledge base. Your daily contributions to your team will be essential in meeting client objectives, goals and challenges. Are you ready to get started?
WSP USA (and all of its U.S. companies) is an Equal Opportunity Employer Race/Age/Color/Religion/Sex/Sexual Orientation/Gender Identity/National Origin/Disability or Protected Veteran Status.
The selected candidate must be authorized to work in the United States.
NOTICE TO THIRD PARTY AGENCIES:
WSP does not accept unsolicited resumes from recruiters, employment agencies, or other staffing services. Unsolicited resumes include any resume or hiring document sent to WSP in the absence of a signed Service Agreement where WSP has expressly requested recruitment/staffing services specific to the position at hand. Any unsolicited resumes, including those submitted to hiring managers or other business leaders, will become the property of WSP and WSP will have the right to hire that candidate without reservation – no fee or other compensation will be owed or paid to the recruiter, employment agency, or other staffing service.
Show more
Show less","Revit, AutoCAD, Trane Trace, Pipeflow, Computational Fluid Dynamics, HVAC, Mechanical Engineering, Building Information Modeling, Energy Codes","revit, autocad, trane trace, pipeflow, computational fluid dynamics, hvac, mechanical engineering, building information modeling, energy codes","autocad, building information modeling, computational fluid dynamics, energy codes, hvac, mechanical engineering, pipeflow, revit, trane trace"
Senior Data Engineer,Alium,Atlanta Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-alium-3782007609,2023-12-17,Duluth,United States,Mid senior,Hybrid,"Data Engineer - Kafka, Databricks, Python, Java, MSK, S3
This is a long-term contract position (up to 2 years working hybrid)
The ideal candidate will have proven experience in data pipeline development and large-scale data processing, with a strong focus on using platforms like
Databricks and Apache Spark
.
The role plays a crucial part in optimizing our data architecture, ensuring seamless data flow, and supporting data-driven decision-making processes.
Responsibilities:
Develop, construct, test, and maintain end-to-end data pipeline solutions encompassing data ingestion, extraction, transformation, enrichment, loading, validation, processing, monitoring, maintenance, governance, and security.
Utilize Databricks and Apache Spark for large-scale data processing and analytics, data engineering, data science, and machine learning tasks.
Ensure seamless data flow between various systems to support data-driven decision making.
Utilize knowledge of Messaging Platforms like Kafka, Amazon MSK & TIBCO EMS or IBM MQ Series to optimize data architecture.
Work with both SQL and NoSQL databases like Teradata, MS SQL, DB2, Cassandra, and AWS S3.
Apply knowledge of distributed data storage systems, data lakes, and data warehousing concepts to enhance our data infrastructure.
Use programming languages like Python or Java for data manipulation and scripting.
Collaborate effectively with cross-functional teams, translating complex data-related challenges into clear tasks and solutions.
Qualifications:
8 years experience as a Data Engineer or similar role, with a strong focus on data pipeline development.
Demonstrable hands-on experience with Databricks and Apache Spark for large-scale data processing and analytics.
Strong with Messaging Platforms like Kafka, Amazon MSK & TIBCO EMS or IBM MQ Series.
Experience with SQL and NoSQL databases like Teradata, MS SQL, DB2, Cassandra, and AWS S3.
Strong knowledge of distributed data storage systems, data lakes, and data warehousing concepts.
Excellent programming skills in languages like Python or Java.
Excellent problem-solving and analytical skills, with the ability to handle complex data-related challenges.
Strong communication and collaboration skills.
Bachelor’s degree in Computer Science, Information Technology, or a related field, or equivalent work experience
Show more
Show less","Data Engineering, Kafka, Databricks, Apache Spark, Python, Java, AWS MSK, S3, Data Pipeline Development, LargeScale Data Processing, Data Architecture, DataDriven DecisionMaking, Data Ingestion, Data Extraction, Data Transformation, Data Enrichment, Data Loading, Validation, Processing, Monitoring, Maintenance, Governance, Security, SQL, NoSQL, Teradata, MS SQL, DB2, Cassandra, Distributed Data Storage Systems, Data Lakes, Data Warehousing, ProblemSolving, Analytical Skills","data engineering, kafka, databricks, apache spark, python, java, aws msk, s3, data pipeline development, largescale data processing, data architecture, datadriven decisionmaking, data ingestion, data extraction, data transformation, data enrichment, data loading, validation, processing, monitoring, maintenance, governance, security, sql, nosql, teradata, ms sql, db2, cassandra, distributed data storage systems, data lakes, data warehousing, problemsolving, analytical skills","analytical skills, apache spark, aws msk, cassandra, data architecture, data engineering, data enrichment, data extraction, data ingestion, data lakes, data loading, data pipeline development, data transformation, databricks, datadriven decisionmaking, datawarehouse, db2, distributed data storage systems, governance, java, kafka, largescale data processing, maintenance, monitoring, ms sql, nosql, problemsolving, processing, python, s3, security, sql, teradata, validation"
Lead Data Engineer,Aptonet Inc,Atlanta Metropolitan Area,https://www.linkedin.com/jobs/view/lead-data-engineer-at-aptonet-inc-3768080769,2023-12-17,Duluth,United States,Mid senior,Hybrid,"Lead Data Engineer
We are currently seeking a Lead Data Engineer experienced with
cloud conversion/migration
, with a preference for Teradata
to AWS migration
The Engineer must have Knowledge of
Databricks.
Must have demonstrable hands-on
experience in creating/designing/implementing end-to-end data pipeline solutions. (Data ingestion, Data extraction, Data transformation, Data enrichment, Data loading, Data validation, Data processing, Data monitoring and maintenance, Data governance and security)
In-depth understanding of ETL and data ingestion processes used in large-scale data warehouses
Position Overview:
The Data Engineer is a skilled professional responsible for creating end-to-end data pipelines, managing data ingestion, transformation, enrichment, loading, validation, monitoring, and maintenance processes. This role plays a crucial part in building and optimizing data architecture, ensuring seamless data flow between various systems, and supporting data-driven decision-making. The Data Engineer should possess expertise in cloud migration, Databricks, ETL processes, and data governance, particularly for large-scale data warehouses.
Key Responsibilities:
End-to-End Data Pipelines
: Design, implement, and maintain robust data pipelines that cover the entire data lifecycle, from data ingestion to data loading, and ensure the efficient flow of data through the entire process.
Data Ingestion:
Develop and manage data ingestion processes to collect data from different sources, such as databases, APIs, logs, and other data repositories, ensuring data is collected in a reliable and scalable manner.
Data Transformation:
Perform data transformations and data enrichment to convert raw data into a structured and usable format, preparing it for storage and analysis.
Data Loading:
Implement data loading procedures to efficiently store processed data into data warehouses, data lakes, or other storage systems, ensuring data quality and integrity.
Data Validation
: Design and implement data validation mechanisms to verify data accuracy, consistency, and completeness, ensuring the data is reliable for analytical purposes.
Data Processing:
Utilize various data processing technologies and frameworks to handle large-scale data volumes efficiently and effectively.
Data Monitoring and Maintenance:
Establish monitoring processes to ensure data pipelines run smoothly, and proactively address any issues that arise. Regularly maintain and optimize data pipelines for better performance and scalability.
Cloud Migration
: Plan and execute cloud migration strategies, transferring data and applications to cloud-based platforms while ensuring security, scalability, and cost-efficiency.
Databricks Expertise
: Leverage Databricks and Apache Spark to build scalable and high-performance data processing workflows, providing insights and analytics on large datasets.
ETL and Data Ingestion Processes
: Understand and implement ETL (Extract, Transform, Load) processes used in large-scale data warehouses, ensuring efficient data movement and integration.
Data Governance and Security
: Implement data governance best practices and security measures to protect sensitive data and ensure compliance with data privacy regulations.
Qualifications and Skills:
Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
Proven experience as a Data Engineer or similar role with a strong focus on data pipeline development.
Expertise in creating end-to-end data pipelines, including data ingestion, extraction, transformation, loading, and validation.
Proficiency in cloud migration, particularly with cloud platforms like AWS, Azure, or Google Cloud.
Experience with Databricks and Apache Spark for large-scale data processing and analytics.
In-depth knowledge of ETL processes and best practices used in large-scale data warehouses.
Knowledge of big data platforms such as Hadoop, Spark, HBase, and Teradata
SQL Databases: Oracle DB, DB2 - On-Prem and AWS (preferred)
Strong programming skills in languages like Python, SQL, or Java for data manipulation and scripting.
Knowledge of distributed data storage systems, data lakes, and data warehousing concepts.
Excellent problem-solving and analytical skills, with an ability to handle complex data-related challenges.
Strong communication and collaboration skills to work effectively with cross-functional teams.
Nice to have
Precisely Connect
Fivetran Transformations
Prophecy
As a Data Engineer, the individual plays a pivotal role in designing and implementing robust data pipelines and facilitating smooth data integration, empowering organizations to harness the power of data for informed decision-making and business growth.
Benefits (employee contribution):
Health insurance
Health savings account
Dental Insurance
Vision insurance
Flexible spending accounts
Life insurance
Retirement plan
All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","Cloud Migration, Teradata, AWS, Databricks, Data Pipeline Development, Data Ingestion, Data Extraction, Data Transformation, Data Loading, Data Validation, Data Processing, Data Monitoring, Data Maintenance, Data Governance, Data Security, SQL Databases, Python, SQL, Java, Hadoop, Spark, HBase, Oracle DB, DB2, AWS, Precisely Connect, Fivetran Transformations, Prophecy","cloud migration, teradata, aws, databricks, data pipeline development, data ingestion, data extraction, data transformation, data loading, data validation, data processing, data monitoring, data maintenance, data governance, data security, sql databases, python, sql, java, hadoop, spark, hbase, oracle db, db2, aws, precisely connect, fivetran transformations, prophecy","aws, cloud migration, data extraction, data governance, data ingestion, data loading, data maintenance, data monitoring, data pipeline development, data processing, data security, data transformation, data validation, databricks, db2, fivetran transformations, hadoop, hbase, java, oracle db, precisely connect, prophecy, python, spark, sql, sql databases, teradata"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Florida, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739500787,2023-12-17,Beverly Hills,United States,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","PostgreSQL, MySQL, Oracle, SQL, PL/SQL, pgPL/SQL, Python, Shell, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","postgresql, mysql, oracle, sql, plsql, pgplsql, python, shell, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, mysql, opensearch, oracle, pgplsql, plsql, postgresql, presto, python, redis, shell, snowflake, sql, terraform, trino"
Senior Data Engineer,Professional Diversity Network,"Florida, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-professional-diversity-network-3788311479,2023-12-17,Beverly Hills,United States,Mid senior,Onsite,"Responsibilities
The Senior Data Engineer responsibilites include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of Seattle Children's Enterprise Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatement, etc.
Requirements
Required Education/Experience: - Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competency - Minimum of five (5) years technology industry or related experience, including items such as: - Build highly scalable, scaled-out architectures on large scale database platforms - Experience working in a complex data infrastructure environment - Five (5) years of experience in a data engineering role - Extensive and in depth data pipeline development experience with industry standard data integration tools - Advanced competency in SQL with ability to perform query optimization in large scale database platforms - Experience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support. - Experience with any industry standard tool for Source Control and Project Management - Experience wrting test cases and test scripts for data quality assurance - Experience creating stored procedures and functions - Experience developing dimensional data model with any industry standard tool. Required Credentials: - N/A Preferred: - Experience in Healthcare or related industry - Experience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plus - Experience productizing/automating predictive models that use R, SAS, Python, SPSS, etc. - Continuous delivery and deployment automation for analytic solutions - Familiarity with Agile framework and test driven development methodology for analytic solutions - API development - Data visualization and/or dashboard development
Min to Max Hourly Salary
$59.18 - $88.77 /hr
Min to Max Annual Salary
$123,094.00 - $184,640.00 /yr
Salary Information
This compensation range was calculated based on full-time employment (2080 hours worked per calendar year). Offers are determined by multiple factors including equity, skills, experience, and expertise, and may vary within the range provided.
The union pay ranges can be found on the Seattle Children's website here: WSNA - UFCW
Seattle Children's offers annual incentive pay based upon performance that is commensurate with the level of the position.
Disclaimer for Out of State Applicants
This compensation range is specific to Seattle, positions located outside of Seattle may be compensated differently depending on various factors
.
Benefits Information
Seattle Children's offers a generous benefit package, including medical, dental, and vision plans, 403(b), life insurance, paid time off, tuition reimbursement, and more. Click here for more information.
About Us
Hope. Care. Cure. These three simple words capture what we do at Seattle Children's -- to help every child live the healthiest and most fulfilling life possible. Are you ready to engage with a mission-driven organization that is life-changing to many, and touches the hearts of all? #HOPECARECURE
Our founding promise to the community is as valid today as it was over a century ago: we will care for all children in our region, regardless of the families' ability to pay. Together, we deliver superior patient care, advance new discoveries and treatments through pediatric research, and serve as the pediatric and adolescent, academic medical center for Washington, Alaska, Montana and Idaho - the largest region of any children's hospital in the country.
In 2022, U.S. News & World Report once again ranked Seattle Children's among the nation's best children's hospitals - for the 30th year in a row. For more than a decade, Seattle Children's has been nationally ranked in all 10 specialty areas evaluated by U.S. News & World Report. We are honored to be the top-ranked pediatric hospital in Washington and the Pacific Northwest.
As a Magnet designated institution, and classified among America's best large employers by Forbes, we recognize the importance of hiring and developing great talent to provide best-in-class care to the patients and families we serve. Our organizational DNA takes form in our core values: Compassion, Excellence, Integrity, Collaboration, Equity and Innovation. Whether it's delivering frontline care to our patients in a kind and caring manner, practicing the highest standards of quality and safety, or being relentlessly curious as we work towards eradicating childhood diseases, these values are the fabric of our culture and community. The future starts here.
Our Commitment to Diversity
Our community welcomes diverse experiences, backgrounds, and thoughts as this is what drives our spirit of inquiry and allows us to better connect with our increasingly diverse patients and families. Our organization recruits, employs, trains, compensates, and promotes without regard to race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
The people who work at Seattle Children's are members of a community that seeks to respect and celebrate all the qualities that make each of us unique. Each of us is empowered to be ourselves within this community, which cultivates and promotes equity, diversity, and inclusion at all levels.
Seattle Children's is proud to be an Equal Opportunity Workplace and Affirmative Action Employer.
PDN-9a3eeb2f-7a3d-4f98-a030-81f150c2e4f9
Show more
Show less","Data Pipeline Development, Data Engineering, SQL, Agile Framework, Test Driven Development, API Development, Data Visualization, Dashboard Development, Netezza, DataStage, BitBucket, Jira, Confluence, R, SAS, Python, SPSS, Continuous Delivery, Deployment Automation, Source Control, Project Management, Dimensional Data Modeling","data pipeline development, data engineering, sql, agile framework, test driven development, api development, data visualization, dashboard development, netezza, datastage, bitbucket, jira, confluence, r, sas, python, spss, continuous delivery, deployment automation, source control, project management, dimensional data modeling","agile framework, api development, bitbucket, confluence, continuous delivery, dashboard development, data engineering, data pipeline development, datastage, deployment automation, dimensional data modeling, jira, netezza, project management, python, r, sas, source control, spss, sql, test driven development, visualization"
InformationTechnology - Data Engineer II #: 23-06983,HireTalent - Diversity Staffing & Recruiting Firm,"Florida, United States",https://www.linkedin.com/jobs/view/informationtechnology-data-engineer-ii-%23-23-06983-at-hiretalent-diversity-staffing-recruiting-firm-3774595951,2023-12-17,Beverly Hills,United States,Mid senior,Onsite,"Job Description: Job Summary: Develops and operationalizes data pipelines to make data available for consumption (reports and advanced analytics), including data ingestion, data transformation, data validation / quality, data pipeline optimization, and orchestration. Engages with the DevSecOps Engineer during continuous integration and continuous deployment. Education and Experience:
A Bachelor's degree in a quantitative or business field (e.g., statistics, mathematics, engineering, computer science).
Requires 2 – 4 years of related experience. Essential Functions:
Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning, and data destruction (scripts, programs, automation, assisted by automation, etc.)
Designs, develops, implements, tests, documents, and operates large-scale, high-volume, high-performance data structures for business intelligence analytics
Designs, develops, and maintains real-time processing applications and real-time data pipelines
Ensure quality of technical solutions as data moves across ***'s environments
Provides insight into the changing data environment, data processing, data storage, and utilization requirements for the company and offers suggestions for solutions
Develops, constructs, tests, and maintains architectures using programming language and tools
Identifies ways to improve data reliability, efficiency, and quality; use data to Client tasks that can be automated Comments for Vendors: Will need to have virtual interviews for all candidates! Same requirements/team as req #107125, 107131, 107132, 107133. Please do not cross submit candidates.
Show more
Show less","Data pipelining, Data ingestion, Data transformation, Data validation, Data quality, Data pipeline optimization, Data orchestration, Data management, Data staging, Data preparation, Data provisioning, Data destruction, Data structures, Business intelligence analytics, Realtime processing, Realtime data pipelines, Programming languages, Automation tools, Data reliability, Data efficiency, Data quality","data pipelining, data ingestion, data transformation, data validation, data quality, data pipeline optimization, data orchestration, data management, data staging, data preparation, data provisioning, data destruction, data structures, business intelligence analytics, realtime processing, realtime data pipelines, programming languages, automation tools, data reliability, data efficiency, data quality","automation tools, business intelligence analytics, data destruction, data efficiency, data ingestion, data management, data orchestration, data pipeline optimization, data preparation, data provisioning, data quality, data reliability, data staging, data structures, data transformation, data validation, datapipeline, programming languages, realtime data pipelines, realtime processing"
InformationTechnology - Data Engineer II #: 23-06982,HireTalent - Diversity Staffing & Recruiting Firm,"Florida, United States",https://www.linkedin.com/jobs/view/informationtechnology-data-engineer-ii-%23-23-06982-at-hiretalent-diversity-staffing-recruiting-firm-3774597911,2023-12-17,Beverly Hills,United States,Mid senior,Onsite,"Job Description: Job Summary: Develops and operationalizes data pipelines to make data available for consumption (reports and advanced analytics), including data ingestion, data transformation, data validation / quality, data pipeline optimization, and orchestration. Engages with the DevSecOps Engineer during continuous integration and continuous deployment. Education and Experience:
A Bachelor's degree in a quantitative or business field (e.g., statistics, mathematics, engineering, computer science).
Requires 2 – 4 years of related experience. Essential Functions:
Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning, and data destruction (scripts, programs, automation, assisted by automation, etc.)
Designs, develops, implements, tests, documents, and operates large-scale, high-volume, high-performance data structures for business intelligence analytics
Designs, develops, and maintains real-time processing applications and real-time data pipelines
Ensure quality of technical solutions as data moves across ***'s environments
Provides insight into the changing data environment, data processing, data storage, and utilization requirements for the company and offers suggestions for solutions
Develops, constructs, tests, and maintains architectures using programming language and tools
Identifies ways to improve data reliability, efficiency, and quality; use data to Client tasks that can be automated Comments for Vendors: Will need to have virtual interviews for all candidates!. Same requirements/team as req #107125, 107131, 107132, 107133. Please do not cross submit candidates.
Show more
Show less","Data pipelines, Data ingestion, Data transformation, Data validation, Data quality, Data pipeline optimization, Orchestration, DevSecOps, Continuous integration, Continuous deployment, Data management, Data staging, Data preparation, Data provisioning, Data destruction, Data structures, Business intelligence, Realtime processing, Realtime data pipelines, Data reliability, Data efficiency, Data quality, Programming languages, Tools, Data architecture","data pipelines, data ingestion, data transformation, data validation, data quality, data pipeline optimization, orchestration, devsecops, continuous integration, continuous deployment, data management, data staging, data preparation, data provisioning, data destruction, data structures, business intelligence, realtime processing, realtime data pipelines, data reliability, data efficiency, data quality, programming languages, tools, data architecture","business intelligence, continuous deployment, continuous integration, data architecture, data destruction, data efficiency, data ingestion, data management, data pipeline optimization, data preparation, data provisioning, data quality, data reliability, data staging, data structures, data transformation, data validation, datapipeline, devsecops, orchestration, programming languages, realtime data pipelines, realtime processing, tools"
InformationTechnology - Data Engineer II #: 23-06984,HireTalent - Diversity Staffing & Recruiting Firm,"Florida, United States",https://www.linkedin.com/jobs/view/informationtechnology-data-engineer-ii-%23-23-06984-at-hiretalent-diversity-staffing-recruiting-firm-3774599118,2023-12-17,Beverly Hills,United States,Mid senior,Onsite,"Job Description: Job Summary: Develops and operationalizes data pipelines to make data available for consumption (reports and advanced analytics), including data ingestion, data transformation, data validation / quality, data pipeline optimization, and orchestration. Engages with the DevSecOps Engineer during continuous integration and continuous deployment. Education and Experience:
A Bachelor's degree in a quantitative or business field (e.g., statistics, mathematics, engineering, computer science).
Requires 2 – 4 years of related experience. Essential Functions:
Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning, and data destruction (scripts, programs, automation, assisted by automation, etc.)
Designs, develops, implements, tests, documents, and operates large-scale, high-volume, high-performance data structures for business intelligence analytics
Designs, develops, and maintains real-time processing applications and real-time data pipelines
Ensure quality of technical solutions as data moves across ***'s environments
Provides insight into the changing data environment, data processing, data storage, and utilization requirements for the company and offers suggestions for solutions
Develops, constructs, tests, and maintains architectures using programming language and tools
Identifies ways to improve data reliability, efficiency, and quality; use data to Client tasks that can be automated Comments for Vendors: Will need to have virtual interviews for all candidates! Same requirements/team as req #107125, 107131, 107132, 107133. Please do not cross submit candidates.
Show more
Show less","Data pipelines, Data ingestion, Data transformation, Data validation, Data quality, Data pipeline optimization, Orchestration, Continuous integration, Continuous deployment, Data staging, Data preparation, Data provisioning, Data destruction, Data structures, Business intelligence analytics, Realtime processing, Realtime data pipelines, Data reliability, Data efficiency, Data quality, Architectures, Programming languages, Tools","data pipelines, data ingestion, data transformation, data validation, data quality, data pipeline optimization, orchestration, continuous integration, continuous deployment, data staging, data preparation, data provisioning, data destruction, data structures, business intelligence analytics, realtime processing, realtime data pipelines, data reliability, data efficiency, data quality, architectures, programming languages, tools","architectures, business intelligence analytics, continuous deployment, continuous integration, data destruction, data efficiency, data ingestion, data pipeline optimization, data preparation, data provisioning, data quality, data reliability, data staging, data structures, data transformation, data validation, datapipeline, orchestration, programming languages, realtime data pipelines, realtime processing, tools"
Principal Data Engineer,Reperio Human Capital,"Florida, United States",https://www.linkedin.com/jobs/view/principal-data-engineer-at-reperio-human-capital-3697277051,2023-12-17,Beverly Hills,United States,Mid senior,Onsite,"Apply Now
Salary
$80-120
Location
Florida, USA
Type
Contract
Start
ASAP
Principal Data Engineer
84145
Desired skills:
Data Engineer, AWS, GCP, Azure, SnowFlake, Python
Job Overview
:
I am looking for a Lead Data Engineer with strong cloud expertise to join our team on a W2 contract basis. As the Lead Data Engineer, you will play a critical role in driving the design, development, and implementation of our cloud-based data solutions. You will lead a team of data engineers and collaborate closely with cross-functional stakeholders to ensure the delivery of robust and scalable data pipelines. This is an exciting opportunity to contribute your technical expertise and leadership skills to the success of our data engineering initiatives.
Responsibilities:
Lead a team of data engineers in designing, building, and maintaining scalable data pipelines and ETL processes within the cloud environment.
Collaborate with cross-functional teams to gather data requirements and translate them into efficient and reliable data engineering solutions.
Architect and implement data models, schemas, and database designs that align with business needs and best practices.
Optimize and fine-tune data pipelines for performance, scalability, and cost efficiency in the cloud environment.
Implement data quality checks and ensure data consistency, accuracy, and integrity throughout the data processing lifecycle.
Stay up to date with emerging cloud technologies and recommend their adoption to enhance data engineering processes and capabilities.
Provide technical guidance, mentorship, and support to the data engineering team, fostering a collaborative and high-performing environment.
Collaborate with stakeholders to define data governance policies, standards, and best practices for data management and security.
Monitor and troubleshoot data pipeline issues, implement solutions, and ensure high availability and reliability of data processing workflows.
Collaborate with data scientists, analysts, and business stakeholders to understand their data needs and enable effective data-driven decision-making.
Requirements:
Bachelor's degree in Computer Science, Engineering, or a related field.
Proven experience as a Lead Data Engineer, overseeing the design and implementation of cloud-based data solutions.
Strong expertise in cloud platforms such as AWS, Azure, or GCP, including their data services and infrastructure.
Extensive experience with data pipeline orchestration tools (e.g., Apache Airflow, AWS Step Functions).
Proficiency in programming languages such as Python, Java, or Scala for data processing and manipulation.
Solid understanding of data warehousing concepts, data modeling, and database technologies.
Experience with big data technologies (e.g., Hadoop, Spark) and distributed computing frameworks.
Excellent problem-solving, analytical thinking, and communication skills.
Leadership abilities with a track record of effectively leading and mentoring a team.
Ability to manage multiple priorities, meet deadlines, and thrive in a fast-paced, dynamic environment.
Duration
: This is a W2 contract position with an expected duration of 12 months.
Reperio Human Capital acts as an Employment Agency and an Employment Business.
Apply Now
Gareth Irvine
is recruiting for this role.
Get in touch with Gareth Irvine for more information: +1 813 902 2716
View profile
Show more
Show less","Data Engineer, AWS, GCP, Azure, SnowFlake, Python, Apache Airflow, AWS Step Functions, Java, Scala, Hadoop, Spark, Data Warehousing, Data Modeling, Database Technologies, Big Data, Distributed Computing, Cloud Computing, Data Pipelines, ETL, Data Quality, Data Security, Data Governance, DataDriven DecisionMaking, Leadership, Mentorship, ProblemSolving, Analytical Thinking, Communication Skills, Project Management, Time Management, Adaptability, FastPaced Environment","data engineer, aws, gcp, azure, snowflake, python, apache airflow, aws step functions, java, scala, hadoop, spark, data warehousing, data modeling, database technologies, big data, distributed computing, cloud computing, data pipelines, etl, data quality, data security, data governance, datadriven decisionmaking, leadership, mentorship, problemsolving, analytical thinking, communication skills, project management, time management, adaptability, fastpaced environment","adaptability, analytical thinking, apache airflow, aws, aws step functions, azure, big data, cloud computing, communication skills, data governance, data quality, data security, database technologies, datadriven decisionmaking, dataengineering, datamodeling, datapipeline, datawarehouse, distributed computing, etl, fastpaced environment, gcp, hadoop, java, leadership, mentorship, problemsolving, project management, python, scala, snowflake, spark, time management"
"Staff Software Engineer, Data Storage",DApp360 Workforce,"Florida, United States",https://www.linkedin.com/jobs/view/staff-software-engineer-data-storage-at-dapp360-workforce-3766899357,2023-12-17,Beverly Hills,United States,Mid senior,Onsite,"DApp360 Workforce is recruiting for an experienced Staff Software Engineer, Data Storage for the largest marketplace for non-fungible tokens, or NFTs. When hiring candidates, the protocol looks for signals that a candidate will thrive in their culture, where they default to trust, embrace feedback, grow rapidly, and love the work. They also know how critical it is to celebrate and support our differences. Employing a team rich in diverse thoughts, experiences and opinions enables their employees, their product and their community to flourish. They are dedicated to equal employment opportunities regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. To help facilitate this, they support remote, hybrid or onsite work at either New York City, San Francisco or the Silicon Valley for the majority of our opportunities They are seeking a data storage expert to deliver a highly scalable and distributed system as the foundation of our OpenSea product experience. Responsibilities
Help scale, architect, and implement our next-generation storage infrastructure including but not limited to using Postgres, Dynamo, Redis, and Elasticsearch
Work closely with product engineering teams in advancing their critical initiatives
Support data in transient and event streaming infrastructure and define paved paths
Mentor and train other team members
Desired Experience
Designed and operated large-scale Postgres databases that support millions of users and a high volume of transactions
Excited to dive deep into database specifics to diagnose and resolve scalability bottlenecks
Ability to apply a world-class understanding of storage architecture to critical product initiatives
Passion for teaching and mentoring high-growth engineers
Proficiency with at least one programming language, preferably Python
Full working experience with other distributed systems at scale (e.g. event streaming, DynamoDB, ElasticSearch) is a plus
The base salary for this full-time position, which spans across multiple internal levels depending on qualifications, ranges between $210,000 to $340,000 plus benefits & equity.
Show more
Show less","Software Engineering, Data Storage, Postgres, DynamoDB, Redis, Elasticsearch, Event Streaming, Paved Paths, Database Architecture, Highgrowth Engineers, Python, Distributed Systems","software engineering, data storage, postgres, dynamodb, redis, elasticsearch, event streaming, paved paths, database architecture, highgrowth engineers, python, distributed systems","data storage, database architecture, distributed systems, dynamodb, elasticsearch, event streaming, highgrowth engineers, paved paths, postgres, python, redis, software engineering"
Senior Data Engineer (W2),Reperio Human Capital,"Florida, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-w2-at-reperio-human-capital-3787595097,2023-12-17,Beverly Hills,United States,Mid senior,Onsite,"Apply Now
Salary
$70-80
Location
Florida, USA
Type
Contract
Start
ASAP
Senior Data Engineer (W2)
89952
Desired skills:
AWS, GCP, Azure, Data, Big Data, Data Engineering
**US CITIZEN OR GREEN CARD HOLDERS ONLY **PLEASE NOTE THIS ROLE CANNOT ACCEPT C2C, C2H, WORK PERMITS OR VISA TRANSFERS. NO OUTSIDE ASSISTANCE NEEDED **
Are you a passionate and experienced Cloud Data Engineer looking for an exciting opportunity to shape the future of data-driven decision-making? Join my client's dynamic team and play a pivotal role in designing, implementing, and optimizing their cutting-edge data solutions.
The client are a leading software company dedicated to leveraging data to drive innovation and enhance their services. They are at the forefront of technology adoption, and they are seeking a Cloud Data Engineer to help them revolutionize the way they manage, process, and analyze vast amounts of data.
Key Responsibilities:
Design, implement, and maintain scalable data pipelines and ETL processes using AWS services such as Glue, EMR, Lambda, and Step Functions.
Collaborate with cross-functional teams to understand data requirements and translate them into efficient data processing solutions.
Optimize data storage, processing, and retrieval techniques to ensure high performance and cost-effectiveness.
Develop and manage data models, data sets, and data catalogues, ensuring data accuracy and integrity.
Monitor data processing activities, troubleshoot issues, and ensure data quality throughout the pipeline.
Stay up-to-date with the latest AWS services and data engineering best practices to continuously enhance our data infrastructure.
Qualifications:
Bachelor's or Master's degree in Computer Science, Data Science, or a related field.
Proven experience as a Big Data Engineer with a strong focus on AWS services and technologies.
Proficiency in designing and implementing data pipelines using AWS tools such as Glue, EMR, S3, Lambda, and Redshift.
Strong programming skills in languages such as Python, Java, or Scala for data processing and ETL tasks.
Solid understanding of data modelling, database design, and data warehousing concepts.
Experience with real-time data streaming technologies (Kinesis, Kafka) is a plus.
AWS certification(s) in relevant domains is highly desirable.
Excellent problem-solving skills and ability to work collaboratively in a team environment.
Strong communication skills to effectively convey technical concepts to both technical and non-technical stakeholders.
Reperio Human Capital acts as an Employment Agency and an Employment Business.
Apply Now
Gareth Irvine
is recruiting for this role.
Get in touch with Gareth Irvine for more information: +1 813 902 2716
View profile
Show more
Show less","AWS, GCP, Azure, Data, Big Data, Data Engineering, Glue, EMR, Lambda, Step Functions, Python, Java, Scala, Redshift, Kinesis, Kafka, Data modeling, Database design, Data warehousing, ETL, Problemsolving, Communication","aws, gcp, azure, data, big data, data engineering, glue, emr, lambda, step functions, python, java, scala, redshift, kinesis, kafka, data modeling, database design, data warehousing, etl, problemsolving, communication","aws, azure, big data, communication, data, data engineering, database design, datamodeling, datawarehouse, emr, etl, gcp, glue, java, kafka, kinesis, lambda, problemsolving, python, redshift, scala, step functions"
Data Engineer,LHH,"Florida, United States",https://www.linkedin.com/jobs/view/data-engineer-at-lhh-3763836763,2023-12-17,Beverly Hills,United States,Mid senior,Remote,"DATA ENGINEER level II. and III. --- Full Time Permanent W2 --- MUST HAVE:
Data marts
Data modeling
Database Redshift
AWS (S3 bucket, DMS, Lambda)
Python programming
SQL (MySQL, Postgres, Redshift)
Apache Airflow
Sisense for reporting
ETL (Airflow, Boomi) tools
BI Tools is a must (such as Periscope, Sisense, Tableau)
A minimum of 5 years of Data engineering experience is required
Software Engineering skills
Manage the full lifecycle of our data warehousing needs
Build data pipelines and ETLs for loading source system data into the data warehouse for further reporting and analysis
Build data pipelines and ETLs for loading source system data into the data warehouse for further reporting and analysis
Build, analyze and manage reports and dashboards for business stakeholders
Show more
Show less","Data Marts, Data Modeling, Redshift, AWS, S3 Bucket, DMS, Lambda, Python, SQL, MySQL, Postgres, Apache Airflow, Sisense, ETL, Boomi, BI Tools, Periscope, Tableau, Data Engineering, Software Engineering, Data Warehousing, Data Pipelines, Data Analysis, Reporting, Dashboards","data marts, data modeling, redshift, aws, s3 bucket, dms, lambda, python, sql, mysql, postgres, apache airflow, sisense, etl, boomi, bi tools, periscope, tableau, data engineering, software engineering, data warehousing, data pipelines, data analysis, reporting, dashboards","apache airflow, aws, bi tools, boomi, dashboard, data engineering, data marts, dataanalytics, datamodeling, datapipeline, datawarehouse, dms, etl, lambda, mysql, periscope, postgres, python, redshift, reporting, s3 bucket, sisense, software engineering, sql, tableau"
Data Engineer (PL/SQL: Postgres OR Oracle),Conexus Recruiting,"Florida, United States",https://www.linkedin.com/jobs/view/data-engineer-pl-sql-postgres-or-oracle-at-conexus-recruiting-3769588581,2023-12-17,Beverly Hills,United States,Mid senior,Remote,"Conexus has been engaged with a leading music industry leader who provides services for covering music rights licensing, administration and royalty accounting for customers providing worldwide music related services. They are actively searching for someone with
6+years of experience writing procedures using PL/PgSQL OR PL/SQL.
TERM: Direct Hire/Permanent Placement
Logistics: Remote
Eligible States: California, Connecticut, Florida, Montana, Nebraska, & Tennessee only
US Citizen OR Green Card Holders only
Qualifications:
6+ years of overall experience in database development/data engineering supporting
Postgres
Must have advanced experience in Postgres (SQL & PL/PgSQL) or Oracle (SQL & PL/SQL) and ability to write stored procedures.
Knowledge in programming languages such as Java, Python, and/or Sparks.
Working knowledge of performance tuning, optimization, stored procedures, etc.
Show more
Show less","PL/PgSQL, PL/SQL, Postgres, Java, Python, Sparks, Stored procedures, Performance tuning, Optimization","plpgsql, plsql, postgres, java, python, sparks, stored procedures, performance tuning, optimization","java, optimization, performance tuning, plpgsql, plsql, postgres, python, sparks, stored procedures"
Data Engineer (U.S. remote),Railroad19,"Florida, United States",https://www.linkedin.com/jobs/view/data-engineer-u-s-remote-at-railroad19-3769044536,2023-12-17,Beverly Hills,United States,Mid senior,Remote,"#Hiringnow
We are actively hiring (Data Engineers-Scala, AWS, Spark)
We seek a
Data Engineer
to be a strong technical resource on a dynamic and growing team of engineers. Our ideal candidate is passionate about creating well-architecture solutions containing thoroughly tested code. The ability to communicate effectively and create relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12+ development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
Bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required
$120,000 - $160,000 a year
Salary is commensurate with experience.-
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
Show more
Show less","Scala, AWS, Spark, Python, Java, C++, SQL, NoSQL, Hadoop, HDFS, EMR, S3, EC2, Restful APIs, Unit Testing, Software Development, Software Engineering, Problem Solving, Analytical Skills, Communication Skills, Teamwork, Initiative, SelfDirection, Technical Documentation","scala, aws, spark, python, java, c, sql, nosql, hadoop, hdfs, emr, s3, ec2, restful apis, unit testing, software development, software engineering, problem solving, analytical skills, communication skills, teamwork, initiative, selfdirection, technical documentation","analytical skills, aws, c, communication skills, ec2, emr, hadoop, hdfs, initiative, java, nosql, problem solving, python, restful apis, s3, scala, selfdirection, software development, software engineering, spark, sql, teamwork, technical documentation, unit testing"
"Senior Data Engineer, Platform Engineering",DApp360 Workforce,"Florida, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-platform-engineering-at-dapp360-workforce-3766878904,2023-12-17,Beverly Hills,United States,Mid senior,Remote,"DApp360 Workforce is recruiting for an experienced Senior Data Engineer, Platform Engineering. The Data Platform team builds and maintains the data processing and analytic pipelines that democratize access to data across our organization.
Data Engineers Have Several Key Responsibilities
Create and maintain optimal data pipeline architecture to extract, transform, and load data from a wide variety of data sources using SQL and Azure big data technologies
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Keep our data separated and secure across national boundaries through multiple data centers and Azure regions
Create Data Tools For Analytics And Data Scientist Team Members That Assist Them In Building And Optimizing Our Product Into An Innovative Industry Leader. We Expect To See
5+ years experience building and optimizing big data pipelines, architectures, and data sets
5+ years working SQL knowledge and experience working with relational databases, query authoring (SQL), as well as working familiarity with a variety of database platforms
5+ years working with Python
Strong analytic skills related to working with free-form text
5+ years experience building processes supporting data transformation, data structures, metadata, dependencies, and workload management
A track record of leading and mentoring less experienced developers. You are eager to teach others and invested in the growth of your team.
Self-motivating, self-directing, and a great communicator (written and oral). You thrive in an environment that grants you a lot of autonomy to explore creative solutions.
Excellent problem solving skills. You excel at analyzing and solving problems using technology.
Living and working within GMT-7:00 (US) to GMT+2:00 (Europe) time zones. We like to see (but not required):
Experience with Microsoft technologies and Azure cloud services for building and operating data pipelines
Experience working remotely and/or working with teams that are distributed geographically.
Experience with Agile methodologies such as Scrum, XP, or Kanban. Certification is a plus, but not a requirement.
An active Stack Overflow profile, open source code, example projects that you're proud of (whether open source or worked on at a previous job), or any other evidence of your passion for building great software.
Knowledge of how Stack Overflow works from our blog, podcasts, and other public artifacts. Ideas about how to evolve the platform and increase our impact on the developer community are even better.
Experience with leveraging cloud-native technologies and techniques to build product ecosystems Base salary will range from: $150k - $175k USD What Youll Get in Return:
Competitive Base Salary
Generous paid vacation
Generous parental leave (16 weeks at 100% pay), family care leave, and unlimited sick days
Equity (RSUs) for all employees at all levels
Industry-leading health benefits that are applicable per country of residence for all our full-time employees
Company-paid Life Insurance
Health & wellness stipend
Home Internet stipend
Professional allocation for your growth and development
Home office allowance of $2,000 (for remote employees) with an additional $450 allowance on each anniversary date
Company-paid access to Calm, Bravely, LinkedIn Learning, MyAcademy and Overdrive One of the most popular websites in the world - a community-based space focused on increasing productivity, decreasing cycle times, accelerating time to market, and protecting institutional knowledge. Innovation is at the heart of everything done, as well as embracing collaboration, transparency, and believing in leading with empathy; creating an environment where everyone knows they belong. The company embraces the unique contributions and points of view of all contributers to their success.
As
Senior Data Engineer, Platform Engineering your cross-functional team will optimize flow and collection of all product and business data, enabling data at scale. Youll ensure that the data delivery architecture is optimized and consistent, enabling software engineers, database architects, data analysts, and data scientists to do their best work.
Show more
Show less","Azure, Agile, SQL, Python, Stack Overflow, Big Data, Data pipelines, Analytics, Data Scientist, Machine Learning, Data transformation, Data structures, Metadata, Dependencies, Workload management, Cloudnative technologies, Product ecosystems, RSU, Scrum, XP, Kanban","azure, agile, sql, python, stack overflow, big data, data pipelines, analytics, data scientist, machine learning, data transformation, data structures, metadata, dependencies, workload management, cloudnative technologies, product ecosystems, rsu, scrum, xp, kanban","agile, analytics, azure, big data, cloudnative technologies, data scientist, data structures, data transformation, datapipeline, dependencies, kanban, machine learning, metadata, product ecosystems, python, rsu, scrum, sql, stack overflow, workload management, xp"
Big Data Software Engineer IV (Remote),Availity,"Florida, United States",https://www.linkedin.com/jobs/view/big-data-software-engineer-iv-remote-at-availity-3780561531,2023-12-17,Beverly Hills,United States,Mid senior,Remote,"Availity delivers revenue cycle and related business solutions for health care professionals who want to build healthy, thriving organizations. Availity has the powerful tools, actionable insights and expansive network reach that medical businesses need to get an edge in an industry constantly redefined by change.
At Availity, we're not just another Healthcare Technology company; we're pioneers reshaping the future of healthcare! With our headquarters in vibrant Jacksonville, FL, and an exciting office in Bangalore, India, along with an exceptional remote workforce across the United States, we're a global team united by a powerful mission.
We're on a mission to bring the focus back to what truly matters – patient care. As the leading healthcare engagement platform, we're the heartbeat of an industry that impacts millions. With over 2 million providers connected to health plans, and processing over 13 billion transactions annually, our influence is continually expanding.
Join our energetic, dynamic, and forward-thinking team where your ideas are celebrated, innovation is encouraged, and every contribution counts. We're transforming the healthcare landscape, solving communication challenges, and creating connections that empower the nation's premier healthcare ecosystem.
Reporting to the Manager, Applications Development the Software Engineer IV will work on a dedicated team of engineers developing, enhancing, and maintaining Availity’s Covered California/Symphony product.
Sponsorship, in any form, is not available for this position.
Location: Remote US
Why Work On This Team
This team supports a high transactional platform that directly impacts patient experience
This team is working to continually improve process and enhance platform capabilities
What You Will Be Doing
Lead the design, development, and maintenance of complex, high-traffic web applications and services with a heavy emphasis on AWS backend technologies
Implement and optimize backend infrastructure utilizing AWS services such as EC2, S3, Lambda, RDS, DynamoDB, API Gateway, and others
Develop reusable, scalable backend components and APIs to support frontend features and functionality
Design and implement data storage and retrieval mechanisms, ensuring high performance, security, and reliability
Mentor and provide technical guidance to junior engineers, fostering a culture of continuous learning and best practices
Conduct code reviews, identify areas for improvement, and drive the adoption of best-in-class software engineering practices
Participate in the evaluation and introduction of new technologies, frameworks, and tools to enhance the development process
Participating in weekly planning and refinement meetings
Utilizing JIRA for project planning and tracking
Collaborating with the Product Owner on a regular basis
Solving healthcare problems and improving processes
Implementing a backlog of items to fulfill a focused roadmap
Working on a team following Agile Scrum principles
Requirements
Bachelor’s degree preferably Computer Science, Engineering, or other quantitative fields
8+ years of professional experience in software engineering, with a focus on full stack development and AWS backend services
Proven expertise in building and scaling web applications using AWS cloud services and infrastructure
Proficiency in programming languages such as Scala, JavaScript (Node.js), Python, Java, or others commonly used in full stack development
Extensive experience with frontend technologies such as HTML, CSS, JavaScript frameworks (e.g., React, Angular, Vue), and responsive design principles
Solid understanding of relational and NoSQL databases, as well as data modeling and optimization techniques
Excellent grasp of software architecture, design patterns, and principles of microservices and serverless computing
Strong knowledge of CI/CD processes, containerization (e.g., Docker, Kubernetes), and infrastructure as code (e.g., CloudFormation, Terraform)
Ability to troubleshoot and optimize backend performance, security, and scalability issues in a production environment
Effective communication skills and the ability to articulate complex technical concepts to a diverse audience
5+ years of experience working with large-scale data and developing SQL queries
3+ years of experience with RESTful APIs and web services
Experience with RedShift and Airflow is a plus
Experience in the healthcare industry or another highly regulated field is a plus
AWS certifications (e.g., AWS Certified Solutions Architect, AWS Certified Developer) are a plus
Availity Culture And Benefits
Availity is a certified “Great Place to Work”, a “Best Workplaces for Technology Companies”, a “Best Workplaces for Women” and a “Best Workplaces for Millennials”!
Culture is important to us and there are many ways for you to make your mark here!
We have several Diversity & Inclusion teams and various ways to engage with fellow Availity associates. “Availadies”, “Beyond Black”, “HOLA”, “Availity Pride”, “VetAvaility” a Young Professionals Group and “She Can Code IT” a group for women in tech are some of the groups you can get involved in.
Availity is a culture of continuous learning. We have many resources and experts in our tech stack and in our industry that can help get you there too!
We offer a competitive salary, bonus structure, generous HSA company contribution, healthcare, vision, dental benefits and a 401k match program that you can take advantage of on day one!
We offer unlimited PTO for salaried associates + 9 paid holidays. Hourly associates start at 19 days of PTO and go up from there with all the same holiday benefits.
Interested in wellness? We allow our associates to reimburse up to $250/year for gym memberships, participation in racing events, weight management programs, etc.
Interested in furthering your education? We offer education reimbursement!
Availity offers Paid Parental Leave for both moms and dads, both birth parents and adoptive parents.
Want to work for an organization that gives back to the community? You’re at the right place! Availity partners with various organizations, both locally and nationally, to raise awareness, funds and morale as our staff members volunteer their time and funds to engage the organizations campaign.
Next Steps In Process
After you apply, you will receive text/email messages thanking you for applying and then you will continue to receive more text/email messages alerting you as to where you are in the recruitment process.
Interview process:
Recruiter resume review
Manager resume review
Recruiter video interview
Manager video interview
Panel video interview
Engineering leadership video interview
Availity is an equal opportunity employer and makes decisions in employment matters without regard to race, religious creed, color, age, sex, sexual orientation, gender identity, gender expression, genetic information, national origin, religion, marital status, medical condition, disability, military service, pregnancy, childbirth and related medical conditions, or any other classification protected by federal, state, and local laws and ordinances.
Availity is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.
NOTICE: Federal law requires all employers to verify the identity and employment eligibility of all persons hired to work in the United States. When required by state law or federal regulation, Availity uses I-9, Employment Eligibility Verification in conjunction with E-Verify to determine employment eligibility. Learn more about E-Verify at
http://www.dhs.gov/e-verify
.
Click the links below to view Federal Employment Notices.
Family & Medical Leave Act Equal Employment Law Poster Pay Transparency Employee Polygraph Protection Act IER Right to Work Poster Important Notice about Employee Rights to Organize and Bargain Collectively with Their Employers
Show more
Show less","AWS, EC2, S3, Lambda, RDS, DynamoDB, API Gateway, Scala, JavaScript (Node.js), Python, Java, HTML, CSS, React, Angular, Vue, SQL, NoSQL, CI/CD, Docker, Kubernetes, CloudFormation, Terraform, RedShift, Airflow, RESTful APIs, Healthcare industry, AWS Certified Solutions Architect, AWS Certified Developer","aws, ec2, s3, lambda, rds, dynamodb, api gateway, scala, javascript nodejs, python, java, html, css, react, angular, vue, sql, nosql, cicd, docker, kubernetes, cloudformation, terraform, redshift, airflow, restful apis, healthcare industry, aws certified solutions architect, aws certified developer","airflow, angular, api gateway, aws, aws certified developer, aws certified solutions architect, cicd, cloudformation, css, docker, dynamodb, ec2, healthcare industry, html, java, javascript nodejs, kubernetes, lambda, nosql, python, rds, react, redshift, restful apis, s3, scala, sql, terraform, vue"
Data Scientist (3536),NextPath Career Partners,"Florida, United States",https://www.linkedin.com/jobs/view/data-scientist-3536-at-nextpath-career-partners-3781906104,2023-12-17,Beverly Hills,United States,Mid senior,Remote,"NextPath Career Partners
is currently seeking a
Data Scientist
to join our client’s team in
Tampa, FL.
This is a
remote, contract to hire
position. Candidate must reside in Florida, Georgia, North Carolina, Virginia, Hawaii, Texas, Colorado, or Tennessee.
SALARY:
$75-85/hour, 145-160K (depending on experience)
Requirements
Power BI with ML integration
Working in Azure environments including Azure ML
Familiarity with DevOps and CI/CD as well as Agile tools and processes including Git, and Azure DevOps
Minimum of a bachelor’s degree in computer science, Applied Mathematics or related technical field
Responsibilities
Demonstrated experience in machine learning techniques, probabilistic reasoning, data science, and/or optimization
Proven ability in creating explainable models and implementing advanced algorithms into production
Experience in implementing supervised and unsupervised machine learning techniques, analysis of variance (ANOVA) and statistical significance test
Demonstrated programming experience in Python, R, Keras, TensorFlow with .NET integration
Acts as a key contributor to all phases of the design and development lifecycle of analytic applications utilizing various technology platforms
Experience developing/implementing analytic solutions in the Amazon or Azure cloud that leverage relational, in-memory, NoSQL, document and/or graph databases
Strong analytical skills, able to translate complex business requirements into sound architectural solutions
Experience designing and implementing data pipeline for analytical model consumption of structured, semi-structured, and/or unstructured data in batch and real-time environments
Experience with APIs, Web Services
Good understanding of dimensional data modeling, data transformation & designing analytical data structures
Good SQL skills, broad exposure to all language constructs
Ability to work in a fast-paced, collaborative team environment
Data Integration / ETL / ELT tools
Excellent written and verbal communication skills and ability to express ideas clearly and concisely
Performs other related duties as directed
Nextpath Career Partners
NextPath is a candidate centric recruiting firm focused on your career goals to help you find your next path! Our team of professional recruiters have over 80 years of combined experience to help you navigate a job change. Our clients span local and national with roles in sales, creative, marketing, and staffing on a direct, contract-to-perm, and contract (project) basis.
If the position above doesn’t appear to fit, we do have a host of clients with roles that could be a match and not every position will be posted. Feel free to reach out to find a better match by emailing your resume to apply@nextpathcp.com for a recruiter to engage.
We are an Equal Opportunity Employer
View all open jobs: http://www.nextpathcp.com
The post Data Scientist (3536) appeared first on NextPath Career Partners.
Show more
Show less","Power BI, Machine Learning, Azure, Azure ML, DevOps, CI/CD, Agile, Git, Azure DevOps, Computer Science, Applied Mathematics, Python, R, Keras, TensorFlow, .NET, Data Integration, ETL, ELT, SQL, Data Modeling, Data Transformation, Data Structures, APIs, Web Services, Data Pipelines, Dimensional Data Modeling, Data Analysis, Data Visualization, Communication","power bi, machine learning, azure, azure ml, devops, cicd, agile, git, azure devops, computer science, applied mathematics, python, r, keras, tensorflow, net, data integration, etl, elt, sql, data modeling, data transformation, data structures, apis, web services, data pipelines, dimensional data modeling, data analysis, data visualization, communication","agile, apis, applied mathematics, azure, azure devops, azure ml, cicd, communication, computer science, data integration, data structures, data transformation, dataanalytics, datamodeling, datapipeline, devops, dimensional data modeling, elt, etl, git, keras, machine learning, net, powerbi, python, r, sql, tensorflow, visualization, web services"
HEDIS Data Analyst (Healthcare) - Remote $100K,Confidential Jobs,"Florida, United States",https://www.linkedin.com/jobs/view/hedis-data-analyst-healthcare-remote-%24100k-at-confidential-jobs-3785885409,2023-12-17,Beverly Hills,United States,Mid senior,Remote,"*Must have HEDIS experience.  Remote potential with HEDIS experience.
The HEDIS Data Analyst works within our Informatics team to identify solutions and meet deadlines associated with HEDIS, Care Gaps, Star Analytics and other highly critical data processes. By conducting data analyses, this position assures accuracy and agility in report creation and data management to drive client satisfaction. In addition, the ability to present data in a meaningful way to clients and business owners is required. Collaboratively working with business owners, developers, and other analysts on a variety of analytical projects, the analyst must demonstrate both excellent communication and technical skills.
Key Responsibilities: Understand complexities associated with highly regulated HEDIS healthcare data reporting processesAdvanced knowledge and experience with Excel, MS-AccessKnowledge and experience with MySQL, SAS, or other related database platformsEffectively partner with internal and external customers to discover needs; independently andproactively offers options and solutionsAnalyzes, researches, and interprets results, variances, data mining and trendsDemonstrate a familiarity with data visualization techniques, as defined by objectives of business usersCreates report summaries and narratives to explain findings and further analysis needed; clearly express recommendations for actionDelivers on expectations and commitments in a timely manner; follows through on agreed upon assignments without deviationPerform other duties, tasks or special assignments, as assigned
Experience:
Bachelor’s Degree required.3 years of experience managing backend HEDIS for healthcare.2 years of experience with working with HEDIS auditors.HEDIS reporting experience required - specifically with HEDIS measures for certificationsAble to perform data profiling and monitor data for changes.Advanced knowledge and experience with Excel, MS-AccessKnowledge and experience with MySQL, SAS, or other related database platforms
Show more
Show less","HEDIS, Data Analyst, Data Mining, Data Processing, Analytics, Databases, Data Accuracy, Communication, Data Visualization, Data Analysis, Reporting, SQL, SAS, MySQL, MSAccess, Excel","hedis, data analyst, data mining, data processing, analytics, databases, data accuracy, communication, data visualization, data analysis, reporting, sql, sas, mysql, msaccess, excel","analytics, communication, data accuracy, data mining, data processing, dataanalytics, databases, excel, hedis, msaccess, mysql, reporting, sas, sql, visualization"
Data Engineer IV - Max Digital (Data Operations),ACV Auctions,"Florida, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-operations-at-acv-auctions-3762875660,2023-12-17,Beverly Hills,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack (SSIS, C#, Web APIs) outages during our operational support window.
Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required.
Leverage DMVs and monitoring tools to ensure system performance; work with data operations and engineering to improve as required.
Ensure existing HADR (availability groups) solution is functional and meets requirements.
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively. This may include being part of the emergency after-hours on-call rotation.
Mentor junior data engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert in SQL Query optimization
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (e.g., MongoDB)
Experience developing Windows services in C#
Experience writing unit and integration testing
Expert SQL and data-layer development experience; OLTP schema design.
Experience using and integrating with cloud services, specifically: AWS RDS, S3, SQS, SNS.
Nice to Have
Experience with Airflow
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, SSIS, C#, Python, AWS RDS, S3, SQS, SNS, MongoDB, ETL, Airflow, DBT, Windows services, OLTP, Database operations, Scalable data services, Data retention, Monitoring tools, High availability, System performance, HADR, Transactional and analytical schema design, DevOps, Code reviews, Documentation, Testing, Cloud services","sql, ssis, c, python, aws rds, s3, sqs, sns, mongodb, etl, airflow, dbt, windows services, oltp, database operations, scalable data services, data retention, monitoring tools, high availability, system performance, hadr, transactional and analytical schema design, devops, code reviews, documentation, testing, cloud services","airflow, aws rds, c, cloud services, code reviews, data retention, database operations, dbt, devops, documentation, etl, hadr, high availability, mongodb, monitoring tools, oltp, python, s3, scalable data services, sns, sql, sqs, ssis, system performance, testing, transactional and analytical schema design, windows services"
InformationTechnology - Data Analyst 3 #: 23-06980,HireTalent - Diversity Staffing & Recruiting Firm,"Florida, United States",https://www.linkedin.com/jobs/view/informationtechnology-data-analyst-3-%23-23-06980-at-hiretalent-diversity-staffing-recruiting-firm-3774600037,2023-12-17,Beverly Hills,United States,Mid senior,Hybrid,"Job Description: Summary: The main function of a database analyst/programmer is to coordinate changes to computer databases, test, and implement the database applying knowledge of database management systems. A typical database analyst/programmer is responsible for planning, coordinating and implementing security measures to safeguard the computer database. Job Responsibilities: Test programs or databases, correct errors and make necessary modifications. Modify existing databases and database management systems or direct programmers and analysts to make changes. Write and code logical and physical database descriptions and specify identifiers of database to management system or direct others in coding descriptions. Work as part of a project team to coordinate database development and determine project scope and limitations. Review project requests describing database user needs to estimate time and cost required to accomplish project. Skills: Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. Ability to work independently and manage one’s time. Basic mentoring skills necessary to provide support and constructive performance feedback. Knowledge of database management software. Education/Experience: Associate's degree in computer programming or equivalent training required. 5-7 years experience required.
Comments for Vendors: Virtual interviews required!
Same requirements/team as #107129, 107127, and 107130. Please do not cross-submit candidates.
Show more
Show less","Database management systems, Database development, Troubleshooting, Coding, Logical and physical database descriptions, Customer service, Mentoring, Software knowledge","database management systems, database development, troubleshooting, coding, logical and physical database descriptions, customer service, mentoring, software knowledge","coding, customer service, database development, database management systems, logical and physical database descriptions, mentoring, software knowledge, troubleshooting"
CDP Data Engineer,Digital Republic Talent,"Florida, United States",https://www.linkedin.com/jobs/view/cdp-data-engineer-at-digital-republic-talent-3774174070,2023-12-17,Beverly Hills,United States,Mid senior,Hybrid,"CDP Data Engineer – Marketing Consultancy – US/Remote - $130,000
Role & Company:
I am working with a global Strategy, Technology & Analytics consultancy who are known for creating digital transformation solutions for their clients.
In this role, you will take the lead on projects and tailor inventive solutions for your clients. This involves building unified customer profiles by extracting, transforming, and loading multiple data sets from various sources, into the CDP. You will also create queries for segmentation and reporting, and build and validate the AEP source, datasets and identity namespaces.
Responsibilities:
· Be a platform expert within AEP
· Demonstrate knowledge across the client’s business, tech stack, and data infrastructure, along with a strong understanding of digital marketing
· Extract, transform, and load customer & marketing data into the platform in an automated and scalable manner
· Build unified customer profiles
· Create essential queries for segmentation, reporting, analysis and the development of ML models within the query service
· Build & validate the AEP source and destination connections, datasets, XDM schemas, and identity namespaces
Required Experience:
· Hands-on experience configuring AEP, including schema creation ingesting data from a variety of sources, configuring identity resolution, and connecting destinations for activation
· 5 years experience building data pipelines
· Experience with Adobe Launch and implementing the AEP WebSDK
· Strong expertise in working with APIs
· Strong expertise with SQL
· Solid grasp of CDPs and the modern data infrastructure
· Experience working with cloud technologies such as AWS, Google Cloud, Azure etc
· Exposure to data warehouse solutions like Amazon Redshift, BigQuery, Snowflake etc
· Preferred experience in an agency with strong consulting & client-facing skills
Show more
Show less","Adobe Experience Platform (AEP), Data extraction, Data transformation, Data loading, Data pipelines, Cloud technologies, Data warehouses, SQL, API, Customer data platforms (CDP), Modern data infrastructure, Adobe Launch, AEP WebSDK, Schema creation, Identity resolution, Data activation, Segmentation, Reporting, Analysis, Machine learning (ML), XDM schemas, Identity namespaces","adobe experience platform aep, data extraction, data transformation, data loading, data pipelines, cloud technologies, data warehouses, sql, api, customer data platforms cdp, modern data infrastructure, adobe launch, aep websdk, schema creation, identity resolution, data activation, segmentation, reporting, analysis, machine learning ml, xdm schemas, identity namespaces","adobe experience platform aep, adobe launch, aep websdk, analysis, api, cloud technologies, customer data platforms cdp, data activation, data extraction, data loading, data transformation, data warehouses, datapipeline, identity namespaces, identity resolution, machine learning ml, modern data infrastructure, reporting, schema creation, segmentation, sql, xdm schemas"
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"Florida, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-engineering-at-acv-auctions-3762880108,2023-12-17,Beverly Hills,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, Data Engineering, ETL, Data Modeling, NoSQL, Cloud Services, Unit Testing, Integration Testing, AWS, RDS, S3, SQS, SNS, MongoDB, Python, C#, Airflow, DBT, Snowflake","sql, data engineering, etl, data modeling, nosql, cloud services, unit testing, integration testing, aws, rds, s3, sqs, sns, mongodb, python, c, airflow, dbt, snowflake","airflow, aws, c, cloud services, data engineering, datamodeling, dbt, etl, integration testing, mongodb, nosql, python, rds, s3, snowflake, sns, sql, sqs, unit testing"
InformationTechnology - Data Analyst 3 #: 23-06979,HireTalent - Diversity Staffing & Recruiting Firm,"Florida, United States",https://www.linkedin.com/jobs/view/informationtechnology-data-analyst-3-%23-23-06979-at-hiretalent-diversity-staffing-recruiting-firm-3774600036,2023-12-17,Beverly Hills,United States,Mid senior,Hybrid,"Job Description: Summary: The main function of a database analyst/programmer is to coordinate changes to computer databases, test, and implement the database applying knowledge of database management systems. A typical database analyst/programmer is responsible for planning, coordinating and implementing security measures to safeguard the computer database. Job Responsibilities: Test programs or databases, correct errors and make necessary modifications. Modify existing databases and database management systems or direct programmers and analysts to make changes. Write and code logical and physical database descriptions and specify identifiers of database to management system or direct others in coding descriptions. Work as part of a project team to coordinate database development and determine project scope and limitations. Review project requests describing database user needs to estimate time and cost required to accomplish project. Skills: Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. Ability to work independently and manage one’s time. Basic mentoring skills necessary to provide support and constructive performance feedback. Knowledge of database management software. Education/Experience: Associate's degree in computer programming or equivalent training required. 5-7 years experience required.
Comments for Vendors: Virtual interviews required!
Same requirements/team as #107129, 107127, and 107130. Please do not cross-submit candidates.
Show more
Show less","Database Management, Database Management Systems, Programming, Testing, Coding, Logical Database Descriptions, Physical Database Descriptions, Project Management, Time Management, Communication, Customer Service, Problem Solving, Mentoring","database management, database management systems, programming, testing, coding, logical database descriptions, physical database descriptions, project management, time management, communication, customer service, problem solving, mentoring","coding, communication, customer service, database management, database management systems, logical database descriptions, mentoring, physical database descriptions, problem solving, programming, project management, testing, time management"
InformationTechnology - Data Analyst 3 #: 23-06978,HireTalent - Diversity Staffing & Recruiting Firm,"Florida, United States",https://www.linkedin.com/jobs/view/informationtechnology-data-analyst-3-%23-23-06978-at-hiretalent-diversity-staffing-recruiting-firm-3774599164,2023-12-17,Beverly Hills,United States,Mid senior,Hybrid,"Job Description: Summary: The main function of a database analyst/programmer is to coordinate changes to computer databases, test, and implement the database applying knowledge of database management systems. A typical database analyst/programmer is responsible for planning, coordinating and implementing security measures to safeguard the computer database. Job Responsibilities: Test programs or databases, correct errors and make necessary modifications. Modify existing databases and database management systems or direct programmers and analysts to make changes. Write and code logical and physical database descriptions and specify identifiers of database to management system or direct others in coding descriptions. Work as part of a project team to coordinate database development and determine project scope and limitations. Review project requests describing database user needs to estimate time and cost required to accomplish project. Skills: Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. Ability to work independently and manage one’s time. Basic mentoring skills necessary to provide support and constructive performance feedback. Knowledge of database management software. Education/Experience: Associate's degree in computer programming or equivalent training required. 5-7 years experience required.
Comments for Vendors: Virtual interviews required!
Same requirements/team as #107129, 107127, and 107130. Please do not cross-submit candidates.
Show more
Show less","Database Management, Coding, Communication, Problem Solving, Customer Service, Interpersonal Skills, Project Management, Time Management, Mentoring, SQL, Database Design","database management, coding, communication, problem solving, customer service, interpersonal skills, project management, time management, mentoring, sql, database design","coding, communication, customer service, database design, database management, interpersonal skills, mentoring, problem solving, project management, sql, time management"
Senior Technical Analyst – Advanced Analytic (Data Engineer),Vancouver Airport Authority,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-technical-analyst-%E2%80%93-advanced-analytic-data-engineer-at-vancouver-airport-authority-3788837665,2023-12-17,Point Roberts,United States,Mid senior,Onsite,"YVR is made up of a team of diverse people who are working collaboratively to Connect BC Proudly to the World. Safety is at the core of everything we do; we’re innovative, fun, and we invest in our people. We’re a BC Top Employer for 14 years standing, with high engagement scores, an abundance of learning and development opportunities, and a holistic approach to wellness. And we’re looking for someone to join our team.
Position Overview
We have an opportunity for a permanent, full-time
Senior Technical Analyst – Advanced Analytics (Data Engineer)
in the Innovation and Technology Group. Reporting to the Manager, Enterprise Advanced Analytics, the Senior Technical Analyst – Advanced Analytic (Data Engineer) is responsible for determining the required strategy and technical architecture, leading technical decisions, leading projects, managing day to day operational activities, guiding other internal and external resources in project tasks and issue resolution, managing vendors, identifying opportunities for improvement, and presenting to senior management. The incumbent is expected to have broad and deep technical and analytical ability within their area of responsibility as well as strong communication skills.
Key Responsibilities Include
Building the pipelines and processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure cloud services (e.g., Azure Databricks, Azure Data Factory, Azure Data Lake Storage) as well as building data products with analytics tools that utilize enterprise datasets to unlock actionable insights to enhance passenger experience and increase operational efficiencies.
Developing processes and documentation for data collection, integration, and consumption as well as contributing to design and development of RESTful APIs that enrich operational applications with additional data sources and insights.
Collaborating and delivering data products for advanced analytics and data science team members that enable them to build and optimize our digital products and machine learning models.
Providing day-to-day support and technical expertise to both technical and non-technical teams including working with stakeholders at all levels to resolve operational issues with data products, and support operation of our data infrastructure.
Identifying, designing, and implementing internal process improvements by automating manual processes, optimizing data delivery, and ensuring data infrastructures, pipelines, and processes are scalable, resilient, and performant.
Collaborating with the Information Security team to implement best practices and recommendations that ensure security of our data sources and data platform.
Key Qualifications Include
At least 7 years’ post degree experience designing, implementing, and supporting enterprise-data architecture and environments for medium to large sized organizations, with at least 5 years of advanced analytics and project management experience (senior level) supplemented by a Bachelor’s degree in Computer Science or a related field; or an equivalent combination of training and experience.
Experience with virtual server platforms, storage devices, computer network resources, cloud computing, project, and vendor management, as well as mentoring junior resources. Familiarity with any of: Azure Functions, Infrastructure as Code, and SQL Server Data Tools is a plus.
Advanced working SQL knowledge and experience working with relational databases.
A strong background in technology with experience in data engineering, data warehouse development, or software development looking to move into the big data environment.
Computer programming skills, Python, Java, JSON, HTML, SQL with experience with data integration using APIs as well as working experience with Azure, Databricks, Denodo, Snowflakes and Qlik would be preferred.
Strong written and oral communication skills, strong project management skills (using waterfall and agile methodologies) and excellent documentation skills, including workflow documentation.
This position is open to both Vancouver Airport Authority employees and external applicants. Previous job performance will be taken into consideration for all internal candidates that apply for this position.
As part of our recruitment process, short-listed candidates may be required to participate in an assessment process.
Salary Range:
$97,599 to $127,555 per annum, based on 37.5-hour work week. This is individually tailored to reflect your unique experience, qualifications and internal equity.
At YVR, your work experience goes beyond the paycheck. We support your personal and professional development, well-being, and a thriving work culture with generous vacation days, extensive health benefits, retirement savings matching, wellness programs, community engagement, commuting support, and continuous learning opportunities. Learn more at https://careers.yvr.ca/benefits
Please note that availability may vary based on employment type and is subject to certain eligibility requirements and potential changes by YVR.
Who We Are
YVR is more than just an airport. We connect our beautiful province and all it has to offer to the world. We are all leaders and trailblazers for change and innovation, so no matter the department or team you’re a part of, the work you do matters.
At YVR, we are flexible in everything we do. We will work together to find ways to deliver customer excellence that helps us all thrive.
Whatever your background and wherever you’re from, you belong at YVR. If you have any questions about accessibility or require any assistance applying, please reach out at careers@yvr.ca
Show more
Show less","SQL, Azure Functions, Infrastructure as Code, Azure, Databricks, Denodo, Snowflakes, Qlik, Python, Java, JSON, HTML, Waterfall, Agile, SQL Server Data Tools, Azure Databricks, Azure Data Factory, Azure Data Lake Storage, RESTful APIs","sql, azure functions, infrastructure as code, azure, databricks, denodo, snowflakes, qlik, python, java, json, html, waterfall, agile, sql server data tools, azure databricks, azure data factory, azure data lake storage, restful apis","agile, azure, azure data factory, azure data lake storage, azure databricks, azure functions, databricks, denodo, html, infrastructure as code, java, json, python, qlik, restful apis, snowflakes, sql, sql server data tools, waterfall"
SUPPLY CHAIN DATA ANALYST,Southern Illinois Healthcare,"Carbondale, IL",https://www.linkedin.com/jobs/view/supply-chain-data-analyst-at-southern-illinois-healthcare-3786595637,2023-12-17,Carbondale,United States,Mid senior,Onsite,"Your Career. Our Company. Together, We Grow.
At Southern Illinois Healthcare (SIH), we realize that in order to provide our surrounding communities with excellent care, we must begin by providing our employees with that same care and appreciation. We offer rich opportunities to develop and grow professionally, an environment of excellence in patient care, and the awareness that everything we accomplish is a direct outgrowth of the superb efforts and dedication of our employees.
As a non-profit system of almost 4000 employees, we have won national acclaim for our cancer, cardiac, stroke, bariatric, breast imaging, and rehabilitation services.
$/hr minimum - $/hr maximum (range increases based on education, experience and certifications)
Responsible for GPO and local contract management within the MMIS system, report creation, data analysis, and MMIS system maintenance.
Role Specific Responsibilities
Builds and edits item master, PAR, template locations and supplier files in the MMIS system.
Creates item files in MMIS system in a timely manner as to support ‘bill only’ patient cases that could include EPIC and HCPCS codes.
Assist Product Analyst with product builds, switches, and inactivation of items for product conversions and manufacturer discontinued items.
Gathers, loads, and maintains, GPO, local, distributor, manufacturer, and vendor contracts in the MMIS system as well as the GPO catalog.
Reviews latest updates, expiring and future contracts from GPO and local agreements and proceeds according to established standard operating procedures.
Maintains a contract database/tracker as to easily see what contracts are up for review/renewal.
Utilizes contract analytic tools and usage history from the MMIS system to determine the appropriate contract tiers for new, expiring, and future agreements.
Works closely with GPO representative to activate contracts and select the most appropriate tier pricing based on usage.
Responsible for contract pricing uploads into the MMIS system.
Utilizes contract tools and available resources to resolve contract and price discrepancies.
Utilizes GPO tracking reports to ensure the most up to date information is populated in the MMIS system.
Prepares usage information spreadsheets for impact analysis and validation per request of GPO.
Attends GPO supply chain meetings for contract updates and required action.
Creates, utilizes, and maintains reports and queries for use in Supply Chain and clinical/non-clinical departments.
Utilizes MMIS data to create personalized reports based on requirements from Supply Chain and end users.
Works closely with Supply Chain IT Analyst to support ongoing maintenance of the MMIS system to ensure the integrity of the data and guarantees updates are done in a timely manner.
Education
Bachelor’s Degree in business or healthcare management or a related clinical field preferred
Associate degree in business or healthcare management or a related clinical field required.
Experience And Skills
Technical Experience 2 years
Show more
Show less","MMIS, EPIC, HCPCS, Databases, Business Intelligence, Data Analysis, Data Validation, Reporting, MS Office Suite, Healthcare Management, Contract Management, Pricing, Supply Chain Management","mmis, epic, hcpcs, databases, business intelligence, data analysis, data validation, reporting, ms office suite, healthcare management, contract management, pricing, supply chain management","business intelligence, contract management, data validation, dataanalytics, databases, epic, hcpcs, healthcare management, mmis, ms office suite, pricing, reporting, supply chain management"
Azure Cloud Data Engineer,JANUS Research Group,"Newport News, VA",https://www.linkedin.com/jobs/view/azure-cloud-data-engineer-at-janus-research-group-3765912971,2023-12-17,Carrollton,United States,Mid senior,Onsite,"This is a contingent opening.
Candidates must have an active Secret clearance and verify, before applying, that you meet the minimum requirements of the position.
Target salary range:
$85000- $125000
. The estimate displayed represents the typical salary range for this position based on experience and other factors.
As a Azure Cloud Data Engineer you will help the Army solve data federation and governance challenges using state-of-the-art cloud solutions. You will work on exciting projects, growing your skills, and work with other dedicated people who are solving mission and organizational needs. This role will work to ensure that business technology solutions align with the long-term vision and goals of AIMD, and with the existing enterprise architecture.
Tasking
Work in a team with amazing clients, partners and other JANUS employees to use cutting-edge technologies to solve challenging enterprise problems.
Design application and data solutions using Azure application and data services.
Work with Application teams to develop and deploy dashboard solutions on the latest cloud and data platforms.
Use your experience working with Azure Data & Storage, Azure Analytics, Databricks, Azure Purview, and other Azure PaaS solutions and COTS software (as needed).
Provide mentorship to clients, partners and other team members.
Drive data quality across the business with advanced/expert experience in data engineering and data warehousing.
Work with internal and external stakeholders to plan, design, develop, implement data integration of various external data sources.
Participates in project requirements reviews, conceptual modeling, discovery processes, technical design reviews, and project meetings in order to provide appropriate guidance that influences toward good business outcomes.
Maintains knowledge and familiarity with current and emerging technologies, best practices, business processes, industry trends, and information security considerations.
Location and Travel Details
This is a remote role with preference to Ft. Eustis, Austin TX Occasional travel maybe required
Security Clearance And Citizenship Requirements
Must have at least an active Secret Clearance.
Qualifications – What You Need To Perform The Job
Bachelor’s degree in Mathematics, Statistics, Business or related fields
Relevant work experience may serve as a substitute for Bachelor's degree
Experience
3-5+ Years’ experience in ETL and Data wrangling.
Effective ETL (Extract/Transform/Load), data shaping and data wrangling skills are critical to success in this role. Knowledge of Azure Data & Storage, Azure Analytics, Databricks, Azure Purview, and other Azure PaaS solutions is preferred but not required, while exceptional knowledge of Microsoft Excel, experience with SQL/Access may serve as a substitute.
Excellent verbal and written communication skills to attend stakeholder meetings, gather complex requirements from business partners, present findings back out, and document work procedures.
Should work well within a team environment and be highly motivated and self-directed.
Mathematical problem solving, statistical, and analytical skills are required.
Knowledge and experience with different types of database structures, joins and data blending techniques.
Developing presentation skills, presents mostly within department and to project teams
JANUS strives to provide opportunities for career growth through training and development. We also offer an attractive comprehensive benefit package to include health and welfare plans and financial products. As part of a total rewards program, employees can benefit from our referral bonus program, and other various employee awards. JANUS Research Group takes pride in our benefit package and rewards program which has earned us the certification of a
Great Place to Work
™
JANUS Research Group provides reasonable accommodation so that qualified applicants with a disability may participate in the selection process. Please advise us of any accommodations you request to express interest in a position by e-mailing: Alisha Pollard, Director of Human Resources at alisha.pollard@janusresearch.com or calling (706) 364-9100. Please state your request for assistance in your message. Only reasonable accommodation requests related to applying for a specific position within JANUS Research Group will be reviewed at the e-mail address and phone number supplied. Thank you for considering a career with JANUS Research Group.
JANUS Research Group participates in the Electronic Employment Verification Program. Please click the E-Verify link below for more information.
E-Verify
JANUS Research Group is an equal opportunity/ affirmative action employer. It is company policy to provide equal opportunity in all areas of employment practice without regard to race, color, religion, sex, sexual orientation, national origin, age, marital status, veteran status, citizenship, or disability.
This contractor and subcontractor shall abide by the requirements of 41 CFR 60-1.4(a), 60-300.5(a) and 60-741.5(a). These regulations prohibit discrimination against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibit discrimination against all individuals based on their race, color, religion, sex, or national origin. Moreover, these regulations require that covered prime contractors and subcontractors take affirmative action to employ and advance in employment qualified individuals without regard to race, color, religion, sex, national origin, protected veteran status or disability.
Show more
Show less","Azure, Azure Data & Storage, Azure Analytics, Databricks, Azure Purview, Microsoft Excel, SQL/Access, ETL, Data wrangling, Database structures, Joins, Data blending, Presentation skills","azure, azure data storage, azure analytics, databricks, azure purview, microsoft excel, sqlaccess, etl, data wrangling, database structures, joins, data blending, presentation skills","azure, azure analytics, azure data storage, azure purview, data blending, data wrangling, database structures, databricks, etl, joins, microsoft excel, presentation skills, sqlaccess"
Business Data Analyst,BayPort Credit Union,"Newport News, VA",https://www.linkedin.com/jobs/view/business-data-analyst-at-bayport-credit-union-3731503471,2023-12-17,Carrollton,United States,Mid senior,Onsite,"Job Details
Description
We are seeking a highly skilled and motivated Credit Union Business Data Analyst to join our team. As a Credit Union Business Data Analyst, you will play a critical role in driving data-driven decision-making processes within our credit union. You will be responsible for collecting, analyzing, and interpreting complex financial and operational data to provide insights that enhance our member experience, streamline operations, and drive business growth. This role requires proficiency in utilizing tools such as Power BI Snowflake SQL, and Python to extract, transform, and visualize data effectively.
Responsibilities Include
Collaborate with cross-functional teams to understand business requirements and objectives, translating them into data analysis and reporting solutions.
Gather, clean, and organize data from various sources.
Design and create interactive and visually appealing dashboards and reports using Power BI, presenting insights to stakeholders in a clear and concise manner.
Conduct business analysis, determine requirements, set up analytic reporting, provide ad hoc training, and diagnose and solve technical problems with the data analytics system and visualization tools used by the credit union staff.
Work with key stakeholders such as member experience, marketing, finance, risk, and IT to develop and deliver insights into member and product performance.
Perform advanced data analysis to identify trends, patterns, and anomalies, extracting actionable insights to support strategic decision-making.
Assist with the development and implementation of statistical models and predictive analytics to forecast trends and assess potential risks and opportunities for the credit union.
Collaborate with IT teams to ensure data security, integrity, and compliance with industry regulations and best practices.
Stay updated with industry trends and advancements in data analytics, bringing innovative ideas to enhance data-driven strategies.
Provide training and support to team members in utilizing data analysis tools, techniques, and best practices.
Assist with the deployment and ongoing maintenance of a data analytics system.
Assist with the development of and adherence to data governance guidelines.
Facilitate the development and implementation of data quality standards, data protection standards, and adoption requirements as related to data governance.
Develop tools and processes to monitor and analyze data integrity.
Provide monthly reports of project and work status.
Work with third party vendors to prioritize and implement data science activities and contribute to future collaborations.
Help advocate for the effective use of data and build a culture of data-informed decision making across the credit union.
Maintain confidentiality of credit union policies and procedures, financial data, and personal information related to credit union activities.
Research machine learning and data analysis using Python, R, or other data tools to help build models that can identify trends and opportunities.
Possess a strong curiosity regarding data and an extreme desire to learn new things.
Perform other duties as assigned when needed.
Qualifications
Knowledge, Skills, and Abilities:
Understanding of data analytics and data science principles.
Understanding of the role of data analytics and technology in a credit union environment.
Ability to seek out and implement new and innovative technologies.
Strong communication skills including written, verbal, and listening.
Ability to articulate ideas to both technical and non-technical audiences.
Ability to handle multiple priorities and handle confidential information.
Experience with PowerBI or other data visualization tools.
Knowledge of SQL and database structure (including SSIS, SSRS, and SSAS)
Interest in artificial intelligence, machine learning, and predictive analytics using R, Python, or other data science tools including offerings by third party vendors.
Strong PC experience including Microsoft Office and Outlook.
Required Education: Bachelor’s degree in Business Information Technology or related field.
Required Experience: 4 – 6 years’ experience in data analytics field.
This description is not an all-inclusive list of the responsibilities, skills, working conditions or essential functions of this job. Management reserves the right to modify, add or remove essential functions as business needs warrant.
We are proud to be an EEO/AA employer M/F/D/V. We maintain a drug-free workplace and reserve the right to perform pre-employment substance abuse testing.
Show more
Show less","Data Analytics, Data Science, Power BI, Snowflake SQL, Python, Data Visualization, Data Governance, Data Quality, Machine Learning, Artificial Intelligence, Predictive Analytics, Statistics, Microsoft Office, SQL, SSIS, SSRS, SSAS, R","data analytics, data science, power bi, snowflake sql, python, data visualization, data governance, data quality, machine learning, artificial intelligence, predictive analytics, statistics, microsoft office, sql, ssis, ssrs, ssas, r","artificial intelligence, data governance, data quality, data science, dataanalytics, machine learning, microsoft office, powerbi, predictive analytics, python, r, snowflake sql, sql, ssas, ssis, ssrs, statistics, visualization"
Supply Chain Data Analyst,Dollar Tree Stores,"Chesapeake, VA",https://www.linkedin.com/jobs/view/supply-chain-data-analyst-at-dollar-tree-stores-3775621537,2023-12-17,Carrollton,United States,Mid senior,Onsite,"Overview:
At Dollar Tree, we strive for exceptional collaboration between talented individuals who exhibit our values and execute our mission. Everything we do starts with our people. We look for people who offer inspiration, innovation and have an internal drive for results.
This position is for a Data Analyst who can manage Supply Chain’s SQL server data, Alteryx workflow (ETL) and Tableau Dashboard development. The candidate will support the manager in the associated governance and partnership with IT. The candidate should be an excellent Alteryx and Tableau modeler who can build from requirement documentation with limited support. Strong analytical mindset to deliver outcomes that are linked to business value and to drive the business forward. The ideal candidate will be able to interpret data science into practical business applications. The candidate should be able to use analytics to optimize next generation enterprise performance methods. Data & Analytics professionals at Dollar Tree have a unique career experience and unparalleled opportunity to grow and advance.
This position is in the SSC headquarters in Chesapeake, VA. 3 days in office and 2 days remote.
Principal Duties & Responsibilities:
Determines best practices and develops actionable insights and recommendations for the current business operations or issues
Performs advanced business analysis using various techniques, e.g. statistical analysis, explanatory and predictive modeling, data mining
Works closely with the internal or external client to understand available data elements, where they reside across various systems
Manages the flow of information from various source systems through the supply chain SQL Server and ultimately into the Supply Chain DataMart, ensuring clean, harmonized and accurate data
Ability to work and communicate with business stakeholders and technical teams while bridging the communication gap between the two
Be the key contact for supply chain data details and interpretation
What you have:
Technical expertise in SQL, Alteryx, Tableau, Python, and other programming languages
Strong knowledge of and experience working with data stores, analyzing large volumes of data using analytical tools, and in building data models.
Knowledge of statistics and experience using statistical packages for analyzing large datasets.
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Strong experience initiating and driving enterprise-wide projects to completion
Skilled in managing multiple requests and maintaining a high-level of responsiveness.
Experience being self-directed in managing projects.
Ability to advise team members and lead matrixed groups.
Articulate the value proposition and business case for enterprise data and analytics solutions
Develop fluent understanding of advancements in technology and innovations happening across digital, analytics, enterprise ecosystem and broader marketplace
Identify, assess, and solve complex business problems for area of responsibility, where analysis of situations or data requires an in- depth evaluation of variable factors
Encourage creative and innovative thinking from team members while helping them to bring their ideas to fruition
Drive an understanding and adherence to the principles of data quality management including metadata, lineage, and business definitions
Build and execute tools to monitor and report on data quality, ensuring high accuracy and quality of our data
Partner with the business to provide consultancy and translate the business needs to design and develop tools, techniques, metrics, and dashboards for insights and data visualization
Build well-managed data solutions, tools, and capabilities to enable self-service frameworks for data consumers
Drive analysis that provides meaningful insights on business strategies
Work collaboratively with appropriate teams to manage security and data governance
Experience:
5-7 years of experience working with data required (preferably in a data warehouse, data management team, or repository)
Project management, business, practice, and people development experience
Education:
Bachelor's degree required (in Data Science, MIS, Engineering preferred)
Master’s degree preferred
Prerequisites:
(Technical)
Expert knowledge of business intelligence reporting solutions (Tableau, Microstrategy, Business Objects, etc.)
Expert knowledge of enterprise data tools and technologies (Alteryx, SQL, SQL Server, etc.)
Working knowledge of analytics techniques and technologies (Statistical modeling, R, Python, etc.)
Advanced working knowledge of Microsoft PowerPoint, MS Excel
Working knowledge of technical documentation
(Interpersonal)
Ability to turn data into insights and stories understandable at various levels of the organization
Ability to work creatively and analytically in a problem-solving environment
Strong interpersonal skills and emphasis on teamwork
Highly effective communication and problem-solving skills
Show more
Show less","SQL, Alteryx, Tableau, Python, Data Stores, Statistical Packages, Statistics, Analytical Skills, Data Modeling, EnterpriseWide Projects, Matrixed Groups, Data Quality Management, Data Visualization, SelfService Frameworks, Business Intelligence Reporting Solutions, Enterprise Data Tools and Technologies, Analytics Techniques and Technologies, Microsoft PowerPoint, MS Excel, Technical Documentation, Data Insights, ProblemSolving, Teamwork, Communication Skills","sql, alteryx, tableau, python, data stores, statistical packages, statistics, analytical skills, data modeling, enterprisewide projects, matrixed groups, data quality management, data visualization, selfservice frameworks, business intelligence reporting solutions, enterprise data tools and technologies, analytics techniques and technologies, microsoft powerpoint, ms excel, technical documentation, data insights, problemsolving, teamwork, communication skills","alteryx, analytical skills, analytics techniques and technologies, business intelligence reporting solutions, communication skills, data insights, data quality management, data stores, datamodeling, enterprise data tools and technologies, enterprisewide projects, matrixed groups, microsoft powerpoint, ms excel, problemsolving, python, selfservice frameworks, sql, statistical packages, statistics, tableau, teamwork, technical documentation, visualization"
Python Data Scientist,Latitude Inc,"Norfolk, VA",https://www.linkedin.com/jobs/view/python-data-scientist-at-latitude-inc-3787721942,2023-12-17,Carrollton,United States,Mid senior,Onsite,"Duties:
Work across functional teams to identify data capture and analysis requirements.
Implement a big data architecture onto NCTE.
Design and build analysis capabilities into the architecture
Collaborate with software engineers on a data-driven predictive maintenance tool
Keep up-to-date on current technologies and applications of big data architectures
Identify opportunities use cases for supervised and unsupervised learning to expand the use of the big data architecture within NCTE
As needed deliver KPIs and automated reporting to teams supporting the NCTE
Required Skills:
Bachelor's degree in technical discipline (i.e. data science, computer science, engineering, mathematics, etc.) and 8-10 years of relevant experience
At least 3 years of experience as a data scientist
Educational requirements may be adjusted for applicable work experience. Work experience may be adjusted for highly specialized knowledge or uniquely applicable experience.
Experience in RESTful web services and/or Object Oriented Programming (OOP) paradigms
Experience with Python, R, or other data science related tools
Experience querying data from SQL databases
Experience with machine learning, artificial intelligence, neural networks (e.g. Tensorflow)
Experience with the Linux operating system
Experience with configuration management tools (e.g. Git, Nexus, Maven)
Experience with the agile software lifecycle
Experience with anomaly detection, time series forecasting, and predictive maintenance
Has a proven ability to learn quickly and works well both independently as well as in a team setting
Desired Skills:
Experience rapidly scaling data storage and processing
Experience with causal analysis methods for root cause analysis
Experience in Modern Java Frameworks and Libraries (e.g. Spring, Guava)
Experience with data visualization
Experience with web frontend frameworks (e.g. React) and accessing REST APIs
Experience in distributed databases, NoSQL, or Graph databases (e.g.Neo4j or MongoDB) a high plus
Experience in streaming and/or batch analytics (e.g. Kafka, Spark, Flink, Storm, MapReduce, Hadoop)
Experience implementing a distributed storage system such as HDFS, HBASE etc.
Experience creating a distributed analytics engine such as DASK or SPARK directly on virtual machines
Must be able to obtain and maintain a Secret clearance. US Citizenship is required
Powered by JazzHR
5eX4VimNN2
Show more
Show less","Data science, Computer science, Engineering, Mathematics, Python, R, SQL, Machine learning, Artificial intelligence, Neural networks, Tensorflow, Linux, Git, Nexus, Maven, Agile software lifecycle, Anomaly detection, Time series forecasting, Predictive maintenance, RESTful web services, Object Oriented Programming (OOP), Spring, Guava, Data visualization, React, REST APIs, Distributed databases, NoSQL, Graph databases, Neo4j, MongoDB, Streaming analytics, Batch analytics, Kafka, Spark, Flink, Storm, MapReduce, Hadoop, HDFS, HBASE, DASK, Secret clearance","data science, computer science, engineering, mathematics, python, r, sql, machine learning, artificial intelligence, neural networks, tensorflow, linux, git, nexus, maven, agile software lifecycle, anomaly detection, time series forecasting, predictive maintenance, restful web services, object oriented programming oop, spring, guava, data visualization, react, rest apis, distributed databases, nosql, graph databases, neo4j, mongodb, streaming analytics, batch analytics, kafka, spark, flink, storm, mapreduce, hadoop, hdfs, hbase, dask, secret clearance","agile software lifecycle, anomaly detection, artificial intelligence, batch analytics, computer science, dask, data science, distributed databases, engineering, flink, git, graph databases, guava, hadoop, hbase, hdfs, kafka, linux, machine learning, mapreduce, mathematics, maven, mongodb, neo4j, neural networks, nexus, nosql, object oriented programming oop, predictive maintenance, python, r, react, rest apis, restful web services, secret clearance, spark, spring, sql, storm, streaming analytics, tensorflow, time series forecasting, visualization"
Data Analyst 1 - 5 with Security Clearance,ClearanceJobs,"Chesapeake, VA",https://www.linkedin.com/jobs/view/data-analyst-1-5-with-security-clearance-at-clearancejobs-3769658338,2023-12-17,Carrollton,United States,Mid senior,Onsite,"TQI Solutions seeks proactive, visionary thought leaders to drive impactful solutions for clients. We value our team, offering work-life balance, excellent benefits, and career growth. Join us and immerse yourself in a rewarding experience like no other. We are currently seeking Data Analysts I - V to join our DTO team. These positions are based in Norfolk, VA. Job Responsibilities: Analyst’s will be actively engaged in various aspects of data management and examination, which includes:
Data Engineering, Data Analytics, and rigorous Data Analysis.
Utilizing and developing expertise in Data Science methodologies.
Creating and interpreting Data Visualizations, preferably with Qlik or Tableau or similar.
Mastering Data & Systems Engineering, particularly in platforms such as Databricks and Trifacta.
Performing Statistical and Business Analysis with a niche focus on Business/Economics Analytics.
Demonstrating a foundational to expert understanding of USN subject matters.
Implementing and understanding transformation processes at a broad scale.
Designing and optimizing User Experiences and Interfaces.
Participating in Marketing, Public Affairs initiatives, and crafting effective Communications.
Applying critical thinking to engage in Complex Problem Solving. Qualifications: US Citizenship Required. Analyst I:
Qualifications: Bachelor's degree is mandatory.
Experience: A minimum of 10 years in related fields. Analyst II:
Qualifications: Bachelor's degree is preferred.
Experience: At least 7 years in pertinent roles. Analyst III:
Qualifications: Bachelor's degree is preferred.
Experience: A minimum of 5 years in related areas. Analyst IV:
Qualifications: Bachelor's degree is preferred.
Experience: At least 3 years in relevant positions. Analyst V:
Qualifications: Associate's degree is preferred.
Experience: Between 0 to 3 years in associated fields
Government-granted security clearance required for all positions. TQI Solutions, Inc. is a Service-Disabled Veteran-Owned Small Business with staff located in VA, MD, DC, NC, HI and CA. We provide Mission and IT Support Services to a wide range of DoD clients. At TQI Solutions, our aim is to surpass customer expectations through results-oriented services and solutions that ensure mission success. We take pride in fully embracing our clients' missions as our own and consistently delivering world-class technical solutions. Our team comprises dedicated problem solvers who prioritize building lasting and mutually trusting relationships with our customers. TQI Solutions is an equal opportunity employer and is committed to fostering a diverse and inclusive workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, pregnancy, national origin, disability, or veteran status.
Show more
Show less","Data Analytics, Data Engineering, Data Science, Data Visualization, Qlik, Tableau, Data & Systems Engineering, Databricks, Trifacta, Statistical Analysis, Business Analysis, Business/Economics Analytics, USN subject matters, Transformation processes, User Experience, User Interface, Marketing, Public Affairs, Communications, Complex Problem Solving, Government security clearance, IT Support Services, DoD clients","data analytics, data engineering, data science, data visualization, qlik, tableau, data systems engineering, databricks, trifacta, statistical analysis, business analysis, businesseconomics analytics, usn subject matters, transformation processes, user experience, user interface, marketing, public affairs, communications, complex problem solving, government security clearance, it support services, dod clients","business analysis, businesseconomics analytics, communications, complex problem solving, data engineering, data science, data systems engineering, dataanalytics, databricks, dod clients, government security clearance, it support services, marketing, public affairs, qlik, statistical analysis, tableau, transformation processes, trifacta, user experience, user interface, usn subject matters, visualization"
Supply Chain Data Analyst - Remote | WFH,Get It Recruit - Transportation,"Chesapeake, VA",https://www.linkedin.com/jobs/view/supply-chain-data-analyst-remote-wfh-at-get-it-recruit-transportation-3770168621,2023-12-17,Carrollton,United States,Mid senior,Remote,"We're on the lookout for a skilled Data Analyst to join our team, focusing on managing and optimizing server data within the Supply Chain, streamlining Alteryx workflows, and crafting insightful Tableau Dashboards. If you're passionate about leveraging data to drive business success and possess exceptional skills, Alteryx, Tableau, and other programming languages, we'd love to hear from you.
Your role will involve collaborating closely with our Supply Chain team, partnering with IT, and utilizing your expertise in Alteryx and Tableau to create impactful solutions independently, translating data science into practical business applications. Your knack for analytics and a strategic mindset will be crucial in delivering outcomes that align with business objectives and propel our operations forward.
Key Responsibilities
Devise best practices and generate actionable insights to enhance current business operations and address challenges.
Conduct advanced business analysis, employing statistical analysis, predictive modeling, and data mining techniques.
Collaborate with internal and external stakeholders to understand available data elements across systems.
Oversee the information flow from diverse source systems through the supply chain Server, ensuring data integrity and accuracy within the Supply Chain DataMart.
Serve as the primary liaison for supply chain data details and interpretation, bridging communication between business stakeholders and technical teams.
Drive the implementation of enterprise-wide projects and manage multiple requests with a high level of responsiveness.
Articulate the value proposition and business case for data and analytics solutions, while staying updated on technological advancements in the field.
What We're Seeking
Proficiency in , Alteryx, Tableau, Python, and other programming languages.
Strong experience with data analysis, data modeling, and handling large datasets.
Sound knowledge of statistics and using statistical packages for data analysis.
Exceptional analytical skills with a keen eye for detail and accuracy.
Proven track record of initiating and leading enterprise-wide projects.
Self-directed with excellent project management abilities.
Effective communication and leadership skills, capable of advising teams and driving innovative thinking.
Qualifications
5-7 years of experience in data-related roles, preferably in data warehousing or data management.
Bachelor's degree in Data Science, MIS, or Engineering (Master’s degree preferred).
Essential Skills
(Technical)
Expertise in business intelligence reporting solutions (e.g., Tableau, Microstrategy).
Advanced knowledge of enterprise data tools and analytics techniques.
Proficiency in Microsoft PowerPoint, MS Excel, and technical documentation.
(Interpersonal)
Ability to transform data into understandable insights at various organizational levels.
Creative and analytical problem-solving skills.
Strong teamwork, interpersonal, and communication abilities.
Join us and be part of a team that values creativity, innovation, and data-driven decision-making. Together, we'll create meaningful impacts through robust data solutions and insights.
Employment Type: Full-Time
Show more
Show less","Data Analysis, Data Management, Alteryx, Tableau, Python, Programming Languages, Statistical Analysis, Predictive Modeling, Data Mining, Data Visualization, Data Interpretation, Project Management, Communication, Leadership, Data Warehousing, Data Science, MIS, Business Intelligence, Enterprise Data Tools, Analytics Techniques, Microsoft PowerPoint, MS Excel, Technical Documentation, Problem Solving, Teamwork, Interpersonal Skills","data analysis, data management, alteryx, tableau, python, programming languages, statistical analysis, predictive modeling, data mining, data visualization, data interpretation, project management, communication, leadership, data warehousing, data science, mis, business intelligence, enterprise data tools, analytics techniques, microsoft powerpoint, ms excel, technical documentation, problem solving, teamwork, interpersonal skills","alteryx, analytics techniques, business intelligence, communication, data interpretation, data management, data mining, data science, dataanalytics, datawarehouse, enterprise data tools, interpersonal skills, leadership, microsoft powerpoint, mis, ms excel, predictive modeling, problem solving, programming languages, project management, python, statistical analysis, tableau, teamwork, technical documentation, visualization"
Business Data Analyst with Security Clearance,ClearanceJobs,"Norfolk, VA",https://www.linkedin.com/jobs/view/business-data-analyst-with-security-clearance-at-clearancejobs-3781966708,2023-12-17,Carrollton,United States,Mid senior,Remote,"Description At InnovaSystems, extraordinary solutions are born through innovative software. A wholly owned subsidiary of Cydecor, InnovaSystems is a leading provider of information technology and enterprise-level solutions to the Department of Defense, federal, state, and local government agencies, delivering a vital service to support our nation's National Security Strategy. We engage organizational effectiveness and readiness resulting in reduced costs, increased effectiveness, and accessibility. As a member of the InnovaSystems team, you will be empowered to do the best work of your career with challenging assignments and collaborative teams that solve real-world problems. Some of our perks & benefits include: Flexible schedule & telecommute options, continuous learning opportunities including technical training and Leadership development programs, comprehensive benefits package, onsite gym/workout facilities, 401k plan w/ company match, competitive salary and a fantastic culture of collaboration.
Successful applicants will be asked to show proof of their U.S. Citizenship. Job Role: InnovaSystems has a need for a Data Analyst for its Navy Readiness Reporting Enterprise - Business Intelligence (NRRE-BI) project team. Using modern analytics toolkits, the successful candidate will work with the product owner, data analysts, and data engineers to deliver dashboards and related data-driven decision support products to the government customer. This work requires someone who can communicate and coordinate with stakeholders, subject matter experts, user experience architects, and the product owner to determine and execute prioritized tasks aligned with customer requirements. The candidate must possess a strong ability to clearly communicate complex analysis and related data requirements orally and in writing while closely adhering to required formats. Will assist in the development and management of requirements during brainstorming, refining and planning activities. Requires a strong commitment to learning requirements development best practices associated with agile development methodologies. Also requires a strong commitment to learn the Navy's business domain. The work will be conducted in person on classified systems with infrequent opportunity for remote work. Due to the nature of our business, the candidate must hold an Active DoD issued Secret Security Clearance upon hire. Skills Required:
Effective written and oral communication skills a must
Ability to communicate with customers, stakeholders, and an agile software development team
Experience researching customer concepts, capturing requirements, and analyzing information is a must
Proficiency in MS Office Suite a must, including Excel
Experience with Power Query M, SQL, report / dashboard features of Microsoft Power BI, IBM Cognos or equivalent analytic toolkits highly desired
Strong commitment to learn the Navy readiness production business domain is a must
Willingness to work on site a must Skills Desired:
Familiarity with SQL queries is a plus
Experience with Scrum or other Agile methods
At least 5 years of relevant experience desired Education:
BS in CS or in related discipline, and 4-6 years experience as a data analyst or similar analysis discipline that required research, analysis, and critical thinking for developing software requirements or technical documentation. Due to the nature of our business, the candidate must hold an Active DoD issued Secret Security Clearance upon hire. Our comprehensive employee benefits offerings include:
Flexible schedule
Company-paid employee development learning and licenses
10 paid holidays
3 weeks (120 hours) of paid leave annually - hours increase after 3 years of service
5% 401k plan w/ company match
Comprehensive benefit package to include health, dental, vision, pet and supplemental insurance plans
Recognition & reward programs including peer-to-peer, service awards, leadership and values awards
Onsite offices with fitness facilities (depending on location)
Social events
InnovaSystems is a proud supporter of community organizations including; Challenged Athletes Foundation (CAF), Support the Enlisted Program (STEP), and Salvation Army InnovaSystems International is an EEO/AA employer M/F/D/V.
Show more
Show less","MS Office Suite, SQL, Power Query M, Business Intelligence, Excel, Reporting, Dashboards, Data Analysis, Scrum, Agile","ms office suite, sql, power query m, business intelligence, excel, reporting, dashboards, data analysis, scrum, agile","agile, business intelligence, dashboard, dataanalytics, excel, ms office suite, power query m, reporting, scrum, sql"
Senior Cloud Data Engineer,BDO USA,"Norfolk, VA",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765471249,2023-12-17,Carrollton,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, AI, Application Development, Cloudbased Data Analytics, SQL, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Azure, AWS, C#, Python, Java, Scala, Tabular Modeling, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, Computer Vision, DataOps, Purview, Delta, Pandas, Spark SQL, Microsoft Fabric, dbt, Terraform, Bicep, Glue, Star Schema, SSIS, SSAS, SSRS, PySpark","data analytics, business intelligence, ai, application development, cloudbased data analytics, sql, data warehousing, data modeling, semantic model definition, star schema construction, azure, aws, c, python, java, scala, tabular modeling, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, computer vision, dataops, purview, delta, pandas, spark sql, microsoft fabric, dbt, terraform, bicep, glue, star schema, ssis, ssas, ssrs, pyspark","ai, ai algorithms, application development, automation tools, aws, azure, azure analysis services, batch data ingestion, bicep, business intelligence, c, cloudbased data analytics, computer vision, data lake medallion architecture, dataanalytics, datamodeling, dataops, datawarehouse, dbt, delta, devops, git, glue, java, linux, machine learning, microsoft fabric, pandas, powerbi, purview, python, scala, semantic model definition, spark, spark sql, sql, ssas, ssis, ssrs, star schema, star schema construction, streaming data ingestion, tabular modeling, terraform"
Business Data Analyst,"Cydecor, Inc.","Norfolk, VA",https://www.linkedin.com/jobs/view/business-data-analyst-at-cydecor-inc-3779280590,2023-12-17,Carrollton,United States,Mid senior,Hybrid,"At InnovaSystems, extraordinary solutions are born through innovative software.
A wholly owned subsidiary of Cydecor, InnovaSystems is a leading provider of information technology and enterprise-level solutions to the Department of Defense, federal, state, and local government agencies, delivering a vital service to support our nation’s National Security Strategy. We engage organizational effectiveness and readiness resulting in reduced costs, increased effectiveness, and accessibility.
As a member of the InnovaSystems team, you will be empowered to do the best work of your career with challenging assignments and collaborative teams that solve real-world problems.
Some of our perks & benefits include:
Flexible schedule & telecommute options, continuous learning opportunities including technical training and Leadership development programs, comprehensive benefits package, onsite gym/workout facilities, 401k plan w/ company match, competitive salary and a fantastic culture of collaboration.
Successful applicants will be asked to show proof of their U.S. Citizenship.
Job Role:
InnovaSystems has a need for a Data Analyst for its Navy Readiness Reporting Enterprise - Business Intelligence (NRRE-BI) project team. Using modern analytics toolkits, the successful candidate will work with the product owner, data analysts, and data engineers to deliver dashboards and related data-driven decision support products to the government customer. This work requires someone who can communicate and coordinate with stakeholders, subject matter experts, user experience architects, and the product owner to determine and execute prioritized tasks aligned with customer requirements. The candidate must possess a strong ability to clearly communicate complex analysis and related data requirements orally and in writing while closely adhering to required formats. Will assist in the development and management of requirements during brainstorming, refining and planning activities. Requires a strong commitment to learning requirements development best practices associated with agile development methodologies. Also requires a strong commitment to learn the Navy's business domain. The work will be conducted in person on classified systems with infrequent opportunity for remote work.
Due to the nature of our business, the candidate must hold an Active DoD issued Secret Security Clearance upon hire.
Skills Required:
Effective written and oral communication skills a must
Ability to communicate with customers, stakeholders, and an agile software development team
Experience researching customer concepts, capturing requirements, and analyzing information is a must
Proficiency in MS Office Suite a must, including Excel
Experience with Power Query M, SQL, report / dashboard features of Microsoft Power BI, IBM Cognos or equivalent analytic toolkits highly desired
Strong commitment to learn the Navy readiness production business domain is a must
Willingness to work on site a must
Skills Desired:
Familiarity with SQL queries is a plus
Experience with Scrum or other Agile methods
At least 5 years of relevant experience desired
Education:
BS in CS or in related discipline, and 4-6 years experience as a data analyst or similar analysis discipline that required research, analysis, and critical thinking for developing software requirements or technical documentation.
Due to the nature of our business, the candidate must hold an Active DoD issued Secret Security Clearance upon hire.
Our comprehensive employee benefits offerings include:
Flexible schedule
Company-paid employee development learning and licenses
10 paid holidays
3 weeks (120 hours) of paid leave annually - hours increase after 3 years of service
5% 401k plan w/ company match
Comprehensive benefit package to include health, dental, vision, pet and supplemental insurance plans
Recognition & reward programs including peer-to-peer, service awards, leadership and values awards
Onsite offices with fitness facilities (depending on location)
Social events
InnovaSystems is a proud supporter of community organizations including; Challenged Athletes Foundation (CAF), Support the Enlisted Program (STEP), and Salvation Army
InnovaSystems International is an EEO/AA employer M/F/D/V.
Show more
Show less","data analytics, data engineering, dashboarding, Power BI, Microsoft Power Query M, SQL, Agile, Scrum, MS Office Suite, Excel","data analytics, data engineering, dashboarding, power bi, microsoft power query m, sql, agile, scrum, ms office suite, excel","agile, dashboard, data engineering, dataanalytics, excel, microsoft power query m, ms office suite, powerbi, scrum, sql"
Senior Metrics & Data Analyst,Epsilon C5I,"Norfolk, VA",https://www.linkedin.com/jobs/view/senior-metrics-data-analyst-at-epsilon-c5i-3678059028,2023-12-17,Carrollton,United States,Mid senior,Hybrid,"Epsilon Systems Solutions, Inc. has a contingent job opportunity for a full time Senior Metrics & Data Analyst in Norfolk, VA.
Summary:
The Senior Metrics & Data Analyst will lead a team dedicated to developing and maintaining the N43 metrics and analysis database and to track and report metrics for CNO availabilities and CMAVs completions.
Duties And Responsibilities
Provide metrics that may require frequent updates to support management decisions and Flag level briefs. The database should have a report generator capability for identified users to print reports without analyst assistance.
Develop, update and maintain CNO Availability and CMAV trend reports. Analyze trend reports and provide recommendations.
Develop and maintain database to track and report metrics for CNO Availability extension root cause analysis once identified.
Develop and maintain database to track and report metrics for CNO Availability growth and new work root cause analysis once identified.
Oversees the database manager for the N43 metrics and analysis database on the CNSL share drive. Ensure the database can update/interface with the Command Readiness Dashboard.
Coordinate with Modernization managers to track and analyze modernization installation operational impact and return on investment (ROI) for tracked alterations. Maintain this data in the N43 metrics and analysis database.
Assign analysis to for team to develop graphs, charts and reports summarizing results.
Generate PowerPoint presentations outlining results and recommended actions.
Required Qualifications
High school diploma/equivalent and four to six years of relevant experience
Must be able to obtain and maintain a DoD Secret Clearance
Two to four years of experience leading a team
Four to six years of experience in database design and management
Four to six years of experience in data analysis support for complex projects
Proficient and experienced with Microsoft Office, especially with Access, Excel and PowerPoint Highly proficient and experienced with Microsoft Office, especially with Access, Excel and PowerPoint.
Proficient and experienced with Visual Basic for Applications (VBA)
Microsoft Technology Associate (MTA) certification required
Pursuant to the various government contractual requirements, all applicants must be U.S. Citizens
ADA Notations
Able to handle prolonged sitting and use of computer.
Office environment.
Epsilon Systems Solutions, Inc. and its subsidiaries are proud to be an Equal Employment Opportunity and Affirmative Action employers, Minority/Female/Disabled/Veterans.
Show more
Show less","Microsoft Office, Access, Excel, PowerPoint, Visual Basic for Applications (VBA), Microsoft Technology Associate (MTA) certification, Database design, Database management, Data analysis, Metrics reporting, Trend analysis, Root cause analysis, PowerPoint presentations","microsoft office, access, excel, powerpoint, visual basic for applications vba, microsoft technology associate mta certification, database design, database management, data analysis, metrics reporting, trend analysis, root cause analysis, powerpoint presentations","access, dataanalytics, database design, database management, excel, metrics reporting, microsoft office, microsoft technology associate mta certification, powerpoint, powerpoint presentations, root cause analysis, trend analysis, visual basic for applications vba"
EW Data Analyst - (IWTG0012.3) with Security Clearance,ClearanceJobs,"Norfolk, VA",https://www.linkedin.com/jobs/view/ew-data-analyst-iwtg0012-3-with-security-clearance-at-clearancejobs-3753468254,2023-12-17,Carrollton,United States,Mid senior,Hybrid,"Our team has a Technical ELINT analyst position available 9/28/2023 with demonstrated experience providing TECHELINT/TECHSIGINT & EW Data systems requirement analysis, technical guidance for data management/interoperability, database design and development, data standardization and maintenance, data migration, and content quality assurance. The position will support the Navy Information Operations Database (NIODB) and the Navy's Electronic Warfare Data (EWD) mission. We are looking for someone who is excited about taking on technical challenges, enjoys working as part of a team, and who can also take on a solo challenge should the need arise. EW Data Analyst position requires on-site support, remote work is not available. Primary responsibilities include:
Liaise with Navy and DoD Electronic Warfare system engineers to document, interpret, and analyze current and fielded EW systems/community data requirements
Research, identify, and validate authoritative information sources to support documented and validated community requirements
Provide Subject Matter Expertise in Navy Technical ELINT (TECHELINT) data products (NGES, EWIR, CED) analysis and National Intelligence Product (ONI, NASIC, NGIC, MSIC, NGA, DIA, NSA) analysis in support of Electronic Warfare systems
Liaise with Software Developers to produce, assess, and validate unique EW data products developed from NIODB in support of Navy and DoD EW systems/community partners
Research, develop, recommend, and document Quality Assurance processes. Analyze derivative data from databases and complete QA verification tasks relative to NIODB
Create detailed technical papers recommending and describing QA processes that will ensure development of intuitive, precise, user friendly, easily integrated, web enabled QA tools Job Requirements
15+ years demonstrated experience in the field of Technical ELINT (TECHELINT) data analysis and authoritative national data sources to include NGES, EWIR, and CED
10+ years demonstrated experience in the field of Electronic Warfare systems, RADAR threat parameter analysis, threat/friendly order of battle (OOB) analysis, Specific Emitter Identification (SEI), and authoritative national data sources to include ONI, NASIC, NGIC, MSIC, NGA, DIA, and NSA
10+ years demonstrated experience in the use of data query and visualization tools to include CA ERwin Data Modeling Software, Data Analysis and Reconciliation Tool (DART), MS Excel, and Oracle SQL Developer
Comprehensive knowledge of data rationalization and standardization efforts to document and cross-correlate EW system data requirements in addition to interpreting highly technical Database Description Documents (DBDD) and System Requirements Specifications (SRS)
Ability to effectively communicate orally and in writing to produce finished products, liaise with current and potential customers, prepare and present briefing documents, charts, graphs, and presentations Additional experience/qualification(s) is desired, but not required:
Qualified Intermediate Technical ELINT (TECHELINT) Analysis Technician or equivalent
Naval Research Laboratory's Interactive Scenario Builder (commonly referred to as BUILDER) Modeling and Simulation application Employees who provide referrals for candidates that are subsequently hired are eligible for a referral bonus. Candidates should indicate the employee who provided the referral upon applying. Terms and conditions apply. Security Clearance Top Secret SCI Job Type Full-time StratasCorp provides a complete compensation package with competitive wages and benefits that include medical, dental, and vision insurance, FSA & HSA accounts, disability and other income protection benefits, life insurance, paid personal time-off benefits, paid holidays, and a 401K Saving Plan with a company matching contribution. StratasCorp is a private corporation established in the Commonwealth of Virginia, is an Equal Employment Opportunity and Affirmative Action employer. This commitment affirms StratasCorp policy to provide equal employment opportunity in accordance with all applicable Equal Employment Opportunity/Affirmative Action laws, directives and regulations to all employees and qualified applicants without regard to race, ethnicity, color, religion, national origin, sex, age, disability status, pregnancy, sexual orientation, gender identity, genetic information, protected veteran status, or any other protected status under Federal, State or Local law.
Show more
Show less","Technical ELINT (TECHELINT) analysis, Data management, Interoperability, Database design, Development, Data standardization, Data migration, Data quality assurance, Navy Information Operations Database (NIODB), Navy's Electronic Warfare Data (EWD), Software development, Data query, Data visualization, Data rationalization, Data standardization, Database Description Documents (DBDD), System Requirements Specifications (SRS)","technical elint techelint analysis, data management, interoperability, database design, development, data standardization, data migration, data quality assurance, navy information operations database niodb, navys electronic warfare data ewd, software development, data query, data visualization, data rationalization, data standardization, database description documents dbdd, system requirements specifications srs","data management, data migration, data quality assurance, data query, data rationalization, data standardization, database description documents dbdd, database design, development, interoperability, navy information operations database niodb, navys electronic warfare data ewd, software development, system requirements specifications srs, technical elint techelint analysis, visualization"
Senior Data Analyst,In Technology Group,"Ilkley, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-in-technology-group-3775364238,2023-12-17,Burnley, United Kingdom,Mid senior,Onsite,"Senior Data Analyst – £45,000 – Ilkley (Hybrid Options)
My client, an award winning Data Analytics provider based in Ilkley are looking to acquire a new Senior Data Analyst for their highly successful team. This role will see you working within an expanding team and collaborating with industry giants on exciting Data Analysis, Data Management, and Category Projects. This position is perfect for a candidate who is keen to use their initiative, is driven, and always eager to develop their skillset further.
This role will be based onsite (at least initially) at my clients site in Ilkley, with hybrid working to be discussed post probatory period.
Responsibilities:
Develop and maintain analytical models and data sets to support business decisions
Utilise data from multiple sources to identify trends and patterns in order to provide insights
Implement data mining techniques to analyse and interpret patterns from data
Develop and maintain reports and dashboards for executives and users
Collaborate with stakeholders to define data requirements
Design and develop data visualisations to communicate insights
Provide technical guidance and training for other data analysts
Develop data models to enable the analysis of complex data
Analyse large volumes of data to identify trends and patterns
Help other or mentor junior team members
Provide in depth customer or buying insights aiding business intelligence / sales teams and improving performance
Manage you own end-to-end reporting projects
Requirements:
4+ years professional experience – desirable areas would be Data, Insights, Research, Marketing, or Commercial Analysis
Advanced experience with SQL, Power BI, & DAX
Transferable skills for the wider team e.g. Python, R, Tableau, Qlik
Knowledge of ETL Processes
Cloud exposure Azure/AWS is a bonus
Client facing or Customer facing skills/experience is highly desirable
Benefits:
Excellent development and growth opportunities
Progression plans
Team building events / Days out
Client trips to New York, Hong Kong, Paris, London, and more
Chance to attend the Annual Trade show in Cannes, South of France
Exceptional workplace culture
If the above role is of interest and you’ve got the required experience, apply with your most up to date CV for immediate consideration and interview!
Alternatively, if you would like to find out more about the position, reach out to me on 0113 526 6892 or at ethan.chesterfield@intechnologygroup.com
Show more
Show less","Data Analysis, Data Management, SQL, Power BI, DAX, Python, R, Tableau, Qlik, ETL Processes, Azure, AWS, Data Visualization, Data Mining, Data Models, Reporting, Dashboards, Clientfacing Skills, Communication, Team Collaboration, Business Intelligence, Sales","data analysis, data management, sql, power bi, dax, python, r, tableau, qlik, etl processes, azure, aws, data visualization, data mining, data models, reporting, dashboards, clientfacing skills, communication, team collaboration, business intelligence, sales","aws, azure, business intelligence, clientfacing skills, communication, dashboard, data management, data mining, data models, dataanalytics, dax, etl, powerbi, python, qlik, r, reporting, sales, sql, tableau, team collaboration, visualization"
"Data Analytics, Insight & Engineering Consultant",Oliver James,"Manchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analytics-insight-engineering-consultant-at-oliver-james-3782352123,2023-12-17,Burnley, United Kingdom,Mid senior,Onsite,"Our client is a true global leader is business consultancy and professional services. Their most profitable and high-growth area sits within the ever-evolving digital audit business, which is the clear global market-leader.
As ever, the demand for data analytics & insight talent for this business unit is high.
Currently, our client wishes to see Manchester or London talent within this area - to work on a hybrid basis - paying c£45,000.
Ideally, you will carry 2-5 years' experience within a data analytics capacity in a commercial environment.
Key drivers of the role:-
Data transformation and modelling (e.g. pandas and scikit-learn in Python);
Data storage and querying (e.g. SQL, Alteryx, Python);
Data visualisation experience (eg PowerBI, Tableau)
Understanding of common data quality issues and they effect they have on machine learning models;
Data cleansing and manipulation for machine learning (e.g. feature engineering);
Knowledge of the current data science software platforms.
To be successful in this role you will need to demonstrate the following:
The ability to come up with creative solutions to complex problems;
Experience working and leading a team and on listed/multinational clients
Exceptional analytical and technical aptitude;
Exceptional attention to detail;
The ability to manage time, prioritise tasks and work under tight deadlines;
Ability to coach and support team members;
Concise and clear communication when presenting and explaining results and findings.
Client relationship-building and management skills
Experience in working in complex environments
Experience in coaching junior colleagues and an interest in developing others
2:1 university degree in any subject (or equivilent) with an interest in data analytics, machine learning, data science, data mining & coding
Show more
Show less","Python, Pandas, ScikitLearn, SQL, Alteryx, PowerBI, Tableau, Data quality, Machine learning, Data cleansing, Data manipulation, Data science, Creative problem solving, Team leadership, Analytical skills, Technical skills, Attention to detail, Time management, Task prioritization, Deadline management, Coaching, Communication, Client relationship building, Complex environments, Junior colleague coaching","python, pandas, scikitlearn, sql, alteryx, powerbi, tableau, data quality, machine learning, data cleansing, data manipulation, data science, creative problem solving, team leadership, analytical skills, technical skills, attention to detail, time management, task prioritization, deadline management, coaching, communication, client relationship building, complex environments, junior colleague coaching","alteryx, analytical skills, attention to detail, client relationship building, coaching, communication, complex environments, creative problem solving, data manipulation, data quality, data science, datacleaning, deadline management, junior colleague coaching, machine learning, pandas, powerbi, python, scikitlearn, sql, tableau, task prioritization, team leadership, technical skills, time management"
Lead Life Sciences Data Quality Engineer,Motion Recruitment,"Waltham, MA",https://www.linkedin.com/jobs/view/lead-life-sciences-data-quality-engineer-at-motion-recruitment-3776423265,2023-12-17,Revere,United States,Mid senior,Remote,"We are working with a leading life sciences management consulting firm that specializes in delivering innovative solutions to complex challenges in the healthcare and biotechnology sectors. Our commitment to excellence and a data-driven approach enables us to empower our clients with actionable insights and strategic guidance.
Job Overview:
We are seeking an experienced Lead Data Quality Engineer to oversee and enhance the integrity, accuracy, and reliability of our data management systems. This role is critical in supporting our consulting projects by ensuring high-quality data for analysis and decision-making.
Key Responsibilities
Lead and manage a team of data quality analysts and engineers.
Design and implement robust data quality frameworks, policies, and procedures.
Collaborate with cross-functional teams to understand data requirements and ensure alignment with business objectives.
Develop and maintain data quality metrics and dashboards for monitoring and reporting.
Identify, analyze, and resolve complex data quality issues.
Provide expertise in data modeling, ETL processes, and data warehousing.
Advise on best practices in data management and quality assurance.
Stay abreast of emerging trends and technologies in data quality and life sciences.
Qualifications
Bachelor’s or Master’s degree in Computer Science, Data Science, Bioinformatics, or a related field.
Minimum of 5 years of experience in data quality management, preferably in the life sciences or healthcare industry.
Strong understanding of data management principles and practices.
Proficiency in SQL, Python, or other data manipulation languages.
Experience with data quality tools and software.
Excellent leadership and team management skills.
Strong analytical, problem-solving, and communication abilities.
What We Offer
A dynamic and collaborative work environment.
Opportunities for professional growth and development.
Competitive salary and benefits package.
Exposure to cutting-edge technologies and methodologies in the life sciences sector.
Posted By:
Phillip Perkins
Show more
Show less","Data Quality, Data Management, Data Integration, ETL, Data Warehousing, Data Modeling, SQL, Python, Data Manipulation Languages, Data Analytics, Data Mining, Data Visualization, Data Security, Data Governance, Big Data, Cloud Computing, Artificial Intelligence, Machine Learning","data quality, data management, data integration, etl, data warehousing, data modeling, sql, python, data manipulation languages, data analytics, data mining, data visualization, data security, data governance, big data, cloud computing, artificial intelligence, machine learning","artificial intelligence, big data, cloud computing, data governance, data integration, data management, data manipulation languages, data mining, data quality, data security, dataanalytics, datamodeling, datawarehouse, etl, machine learning, python, sql, visualization"
"Data Conversion Developer, Senior Associate",PwC,"Boston, MA",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749936653,2023-12-17,Revere,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Azure Data Engineer Associate, Databricks Certified Data Engineer Associate, Python, PySpark, Scala, SQL, Maximo, Database Configuration, Java Customizations, Automation Scripts, Application Designer, IBM DB2, Oracle, Microsoft SQL Server, Maximo's Integration Framework (MIF), XML, JSON, SOAP, RESTful APIs, Azure ADF, AWS Glue, SSIS, DataBricks","azure data engineer associate, databricks certified data engineer associate, python, pyspark, scala, sql, maximo, database configuration, java customizations, automation scripts, application designer, ibm db2, oracle, microsoft sql server, maximos integration framework mif, xml, json, soap, restful apis, azure adf, aws glue, ssis, databricks","application designer, automation scripts, aws glue, azure adf, azure data engineer associate, database configuration, databricks, databricks certified data engineer associate, ibm db2, java customizations, json, maximo, maximos integration framework mif, microsoft sql server, oracle, python, restful apis, scala, soap, spark, sql, ssis, xml"
DevOps (Data Platforms),BigRio,"Boston, MA",https://www.linkedin.com/jobs/view/devops-data-platforms-at-bigrio-3780007607,2023-12-17,Revere,United States,Mid senior,Remote,"DevOps (Data Platforms)
Remote- East Coast Hours
Contract to Hire
Must hold or be eligible for Public Trust Clearance
About BigRio:
BigRio is a remote technology consulting firm headquartered in Boston. We deliver a range of solutions including custom machine learning/AI integrations and data warehousing and processing solutions. Our comprehensive approach serves clients from a variety of industries as a result of our ability to consistently, and quickly deliver cutting-edge and cost-conscious software solutions.
You will join our client's team as a DataOps Engineer who will work with our clients.
About the Job:
The Data Connectivity Team designs, builds, and maintains the integrated platform that securely procures and links critical business data from disparate internal and external sources. This involves assimilating all structured, unstructured, and semi-structured data. Our client needs a Senior DataOps Engineer who will touch all aspects of the data operation, in particular, data infrastructure, to ensure a robust, efficient, and consistent foundation for enterprise consumption & and application development.
Responsibilities:
Work with other engineers/architects and engineers to define, execute, and update core data pipelines while maintaining a high level of availability and transactional correctness.
Migrate Data between cloud platforms.
Minimum Job Qualifications:
Experience supporting production infrastructure on AWS platforms (Experience with GCP optional)
3+ years of AWS production experience
3+ years of Terraform
Experience with Gitlab CI/CD, Helm and Kubernetes
Knowledge of log collection solutions, preferably Elasticsearch
Experience building and optimizing production-grade data pipelines (stream processing and batch) to prepare datasets at scale for data analysis, modeling, and optimization
Demonstrated understanding of fast-paced Agile principles with technical designs, iterative development, and code reviews
Experience building IAC code for the data platform technologies
Preferred Job Qualifications:
Strong experience with DataBricks
Strong Python skills with Airflow experience
Solid knowledge of Python/Looker
Solid experience with Redis
Experience working with PostGres.
Strong SQL experience
Solid experience in using CI/CD pipelines for deployments
Experience in hosting, provisioning and maintaining database servers in cloud environment, preferably AWS and GCP, is a plus
Experience working in the Government Cloud (GovCloud)is a huge plus!
Experience with CDKTF is a plus
Experience with shell scripting such as bash
Equal Opportunity Statement
BigR.io is an equal-opportunity employer. We prohibit discrimination and harassment of any kind based on race, religion, national origin, sex, sexual orientation, gender identity, age, pregnancy, status as a qualified individual with disability, protected veteran status, or other protected characteristic as outlined by federal, state, or local laws. BigR.io makes hiring decisions based solely on qualifications, merit, and business needs at the time. All qualified applicants will receive equal consideration for employment.
Show more
Show less","Machine Learning, AI, Data Warehousing, Data Processing, Public Trust Clearance, DataOps Engineer, Data Infrastructure, Data Pipelines, AWS, GCP, Terraform, Gitlab CI/CD, Helm, Kubernetes, Elasticsearch, DataBricks, Airflow, Python, Looker, Redis, PostGres, SQL, CI/CD Pipelines, Cloud Environment, Government Cloud (GovCloud), CDKTF, Shell Scripting","machine learning, ai, data warehousing, data processing, public trust clearance, dataops engineer, data infrastructure, data pipelines, aws, gcp, terraform, gitlab cicd, helm, kubernetes, elasticsearch, databricks, airflow, python, looker, redis, postgres, sql, cicd pipelines, cloud environment, government cloud govcloud, cdktf, shell scripting","ai, airflow, aws, cdktf, cicd pipelines, cloud environment, data infrastructure, data processing, databricks, dataops engineer, datapipeline, datawarehouse, elasticsearch, gcp, gitlab cicd, government cloud govcloud, helm, kubernetes, looker, machine learning, postgres, public trust clearance, python, redis, shell scripting, sql, terraform"
Expression of Interest: Data Scientist,Fingerprint for Success (F4S),"Boston, MA",https://www.linkedin.com/jobs/view/expression-of-interest-data-scientist-at-fingerprint-for-success-f4s-3787775913,2023-12-17,Revere,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
bm7vA3eIqQ
Show more
Show less","JazzHR, F4S work style assessment","jazzhr, f4s work style assessment","f4s work style assessment, jazzhr"
Senior Software Developer - Enterprise Data Services (.NET),CERES Group,"Boston, MA",https://www.linkedin.com/jobs/view/senior-software-developer-enterprise-data-services-net-at-ceres-group-3581038318,2023-12-17,Revere,United States,Mid senior,Remote,"The Enterprise Data Services team is seeking a senior level software developer to join our development team. You will have the opportunity to work with a talented team designing and developing mission-critical software at our firm.
The team is responsible for delivering data services to support business process and workflows, decision support systems, and reporting capabilities for our internal customers and the firm’s clients. These include web applications, mobile apps, and backend services.
Our developers are responsible for review business needs, providing a technical design, implementation, testing and maintaining software. The Client is looking for creative individuals who are driven to produce great solutions while following standards and industry best practices. These positions require intelligent and self-motivated developers.
As part of an agile team, work closely with business analysts, architects and QA
Participate in planning activities, collaborate and coordinates the efforts across development teams
Provide suggestions and adjustments as needed to deliver quality software within project scope and timeframes
Explore, promote and apply new frameworks and technologies
Present new ideas and technologies for system designs for architectural review
Promote and perform development work consistent with industry standards and best practices
Work on R&D activities and prototypes
Familiar with various design and architectural patterns
Understanding of fundamental design principles for building scalable and reusable application
Where applicable, refactor existing code for performance improvements and compliance with latest standards
Ability to work independently and understand application functionality through reverse engineering code
Perform unit testing and support test automation
Apply continuous integration and continuous delivery techniques
Conduct peer code reviews
Mentor and educate peer developers
Support Tier 1 systems once they are up and running
Qualifications
A Masters or Bachelor’s degree in computer science or equivalent experience
10+ years of professional experience designing, building, and deploying enterprise scale applications and services
Strong CS fundamentals in data structures, algorithms, and complexity analysis
ASP.NET MVC, Web API, .NET 4.5 +, SQL Server 2012 +, ETL
Front-end experience with CSS, jQuery, Angular
Advanced SQL Server skillset that includes writing and optimizing complex queries and performing data analysis
Understanding of relational database design best practices and experience with data modeling
Consistently applies SOLID object-orientated design principals
Demonstrated experience applying unit tests and test automation
Demonstrated experience with Continuous Integration Platforms (such as Jenkins, TeamCity)
Experience preferred with .Net Core and ASP.NET Core
Experience preferred with AWS including EC2 Auto Scaling, SQS, DynamoDB, and CloudFormation
Ability to satisfy complex business requirements through simple design and execution
Possess strong problem-solving and analytical skills
Strong leadership and communication skills
Extreme attention to detail
Experience with mid/back-office financial services operations a plus
Show more
Show less","Software Development, Data Services, Web Applications, Mobile Apps, Backend Services, Business Analysis, Software Architecture, QA, Agile Development, Unit Testing, Test Automation, Continuous Integration, Continuous Delivery, Code Reviews, Mentoring, Tier 1 Support, Computer Science, Data Structures, Algorithms, Complexity Analysis, ASP.NET MVC, Web API, .NET 4.5 +, SQL Server 2012 +, ETL, CSS, jQuery, Angular, SQL Server, Relational Database Design, Data Modeling, SOLID ObjectOriented Design, Unit Tests, Continuous Integration Platforms, Jenkins, TeamCity, .Net Core, ASP.NET Core, AWS, EC2 Auto Scaling, SQS, DynamoDB, CloudFormation, ProblemSolving, Analytical Skills, Leadership, Communication, Attention to Detail, Financial Services Operations","software development, data services, web applications, mobile apps, backend services, business analysis, software architecture, qa, agile development, unit testing, test automation, continuous integration, continuous delivery, code reviews, mentoring, tier 1 support, computer science, data structures, algorithms, complexity analysis, aspnet mvc, web api, net 45, sql server 2012, etl, css, jquery, angular, sql server, relational database design, data modeling, solid objectoriented design, unit tests, continuous integration platforms, jenkins, teamcity, net core, aspnet core, aws, ec2 auto scaling, sqs, dynamodb, cloudformation, problemsolving, analytical skills, leadership, communication, attention to detail, financial services operations","agile development, algorithms, analytical skills, angular, aspnet core, aspnet mvc, attention to detail, aws, backend services, business analysis, cloudformation, code reviews, communication, complexity analysis, computer science, continuous delivery, continuous integration, continuous integration platforms, css, data services, data structures, datamodeling, dynamodb, ec2 auto scaling, etl, financial services operations, jenkins, jquery, leadership, mentoring, mobile apps, net 45, net core, problemsolving, qa, relational database design, software architecture, software development, solid objectoriented design, sql server, sql server 2012, sqs, teamcity, test automation, tier 1 support, unit testing, unit tests, web api, web applications"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Boston, MA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759706899,2023-12-17,Revere,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Data mining, Data cleaning, Data modeling, Data visualization, Pandas, R, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning, Data compliance, Data management","data engineering, data mining, data cleaning, data modeling, data visualization, pandas, r, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data compliance, data management","airflow, aws, azure, bash, data cleaning, data compliance, data engineering, data management, data mining, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm, visualization"
Investment Data Engineer - Lead,MFS Investment Management,"Boston, MA",https://www.linkedin.com/jobs/view/investment-data-engineer-lead-at-mfs-investment-management-3736901044,2023-12-17,Revere,United States,Mid senior,Hybrid,"At MFS, you will find a culture that supports you in doing what you do best. Our employees work together to reach better outcomes, favoring the strongest idea over the strongest individual. We put people first and demonstrate care and compassion for our community and each other. Because what we do matters – to us as valued professionals and to the millions of people and institutions who rely on us to help them build more secure and prosperous futures.
In conjunction with the Investment Data Management Office, the Lead Data Engineer contributes to a long-term strategic initiative to unify and harmonize our investment data. This initiative enables enhanced investment decision making, risk management and client reporting for our multi-asset platform by delivering consistent, timely, accurate and user-friendly data to investors, risk teams and clients.
Are you a hands-on and detailed-oriented individual working on the cutting edge of financial instruments, investment data, and analytics? Are you interested in investment data strategies across a wide variety of traditional and alternative asset classes? Are you a thinker who enjoys devising innovative and flexible business solutions to meet emerging business needs?
The MFS Investment Data Management Office is actively searching for a Lead Data Engineer to implement data engineering and analytics solutions . Primary responsibilities include full implementation and maintenance of data ingestion, data maintenance, data validation and data delivery of investment data. We are looking for someone who thrives in an agile, collaborative, team-based environment, working closely with technology peers across MFS, investment professionals and key vendor partners. This position offers the opportunity to shape the future of investment data at MFS.
Principal Responsibilities
Design, develop, and implement data pipelines to maintain unified data platform for the Investment Data Management Office(IDMO)
Lead and participate in all development activities, develop and implement solutions to meet business requirements that align with program strategic objectives
Responsible for new and on-going development of data pipelines sourcing from internal and external sources
Drive continuous improvement of data quality, resiliency, control, efficiency, and monitoring
Troubleshooting complex system interactions to find the root cause to problems
Partner with platform lead to design, develop, implement and deploy new software components to investment data platform
Partner with data architect to evaluate and finalize the unified data model
Partner with integration architect to upgrade and integrate data ingestion and data delivery tools with the unified data platform
Upgrade and integrate transformation tool, data validation tool and orchestration tools with the unified data platform to implement data engineering, analytical engineering and data maintenance capabilities.
Provide support during unexpected outages
Job Requirements
Bachelor’s degree in Computer Science or related disciplines.
Minimum of 5 years of experience in design, development and building data oriented complex applications.
Deep understanding of Agile SDLC, DevOps and Cloud technologies required, in addition to exposure to multiple, diverse technologies, platforms, and processing environments.
Experience in data integration, data warehouse, data modeling and data analytics architecture and design principles. Knowledge of and experience with Snowflake and other cloud native databases is highly preferred.
Knowledge about various architectures, patterns such as unified data management architecture (UDM), data mesh architecture, event-driven architecture, real-time data flows, non-relational repositories, data virtualization, etc.
Experience with building solutions in the financial services domain with an understanding of financial instruments, transactions, and positions, is desired.
Good interpersonal and communication skills with the ability to lead cross-team collaboration and partnerships across a variety of internal and external constituencies.
Not sure you meet 100% of our job requirements? That’s ok. If you believe that you could excel
#MBLI
At MFS, we are dedicated to building a diverse, inclusive and authentic workplace. If you are excited about this role but your past experience doesn't align perfectly, we encourage you to apply - you might be just the right candidate for this role or others.
What We Offer
Generous time-off provided: including ""Responsible time off"" for many roles, paid company holidays when the US Stock Exchange is closed, plus paid volunteer time
Family Focus: Up to 20 weeks of paid leave for new parents, back-up care program, dependent care flexible spending account, adoption assistance, generous caregiver leave
Health and Welfare: Competitive medical, vision and dental plans, plus tax-free health savings accounts with company contributions
Wellness Programs: Robust wellness webinars, employee assistance program, gym reimbursement through our medical plans, fitness center discounts and more
Life & Disability Benefits: Company-paid basic life insurance and short-term disability
Financial Benefits: 401(k) savings plan, Defined Contribution plan- 15% of base salary invested into the Plan, competitive total compensation programs
MFS is a
hybrid work environment
(remote/onsite) unless otherwise stated in the job posting.
If any applicant is unable to complete an application or respond to a job opening because of a disability, please contact MFS at 617-954-5000 or email talent_acquisition@mfs.com for assistance.
MFS is an
Affirmative Action and Equal Opportunity Employer
and it is our policy to not discriminate against any employee or applicant for employment because of race, color, religion, sex, national origin, age, marital status, sexual orientation, gender identity, genetic information, disability, veteran status, or any other status protected by federal, state or local laws. Employees and applicants of MFS will not be subject to harassment on the basis of their status. Additionally, retaliation, including intimidation, threats, or coercion, because an employee or applicant has objected to discrimination, engaged or may engage in filing a complaint, assisted in a review, investigation, or hearing or have otherwise sought to obtain their legal rights under any Federal, State, or local EEO law is prohibited.
Please see the Know Your Rights: Workplace Discrimination is Illegal document and Pay Transparency Nondiscrimination Provision, linked for your reference.
Show more
Show less","Agile, SDLC, DevOps, Cloud Technologies, Data Integration, Data Warehouse, Data Modeling, Data Analytics, Snowflake, Cloud Native Databases, Unified Data Management Architecture, Data Mesh Architecture, EventDriven Architecture, Realtime Data Flows, NonRelational Repositories, Data Virtualization","agile, sdlc, devops, cloud technologies, data integration, data warehouse, data modeling, data analytics, snowflake, cloud native databases, unified data management architecture, data mesh architecture, eventdriven architecture, realtime data flows, nonrelational repositories, data virtualization","agile, cloud native databases, cloud technologies, data integration, data mesh architecture, data virtualization, dataanalytics, datamodeling, datawarehouse, devops, eventdriven architecture, nonrelational repositories, realtime data flows, sdlc, snowflake, unified data management architecture"
Data Visualization Analyst,"firstPRO, Inc","Boston, MA",https://www.linkedin.com/jobs/view/data-visualization-analyst-at-firstpro-inc-3770548117,2023-12-17,Revere,United States,Mid senior,Hybrid,"FirstPro is now accepting resumes for a Data Visualization Analyst based in Boston, MA. This role will focus on understanding the organization's Business Intelligence needs, and fulfilling those needs through the creation of dashboards, reports and visualizations to answer key data and business questions. This is a permanent, direct-hire role that can offer benefits and a hybrid remote/onsite schedule.
Candidates must be located in the Greater Boston Area as you will be required to work onsite on a weekly basis.
Responsibilities
Contribute to the strategic planning process for business intelligence initiatives, recommend innovative technologies, methods, and industry-leading tools and best practices, design and maintenance of a system to warehouse data and information, including structured and unstructured data sets.
Scale Argos and Tableau platforms through building advanced dashboards and incorporate new and diversified data sources. Actively organize and participate in data literacy, report and dashboard training to key business users. Through storytelling to evolve data analytics for department engagement.
Manages all technical aspects of our Business Intelligence environment, including data transformation and governance, performance monitoring and identification and support of analytics tools.
Use complex data analysis and query, data science and data modeling techniques to develop report and data analytics solutions for decision support. This includes developing data models, engineering solutions in the form of business dashboards, reports and data visualizations integrating with other 3rd party applications.
Partner with academic units and business departments such as Institutional Research, Enrollment Management, Registrar, Finance, HR, Financial Aid and Institutional Advancement to define and execute potential BI initiatives. Analyze data needs, gather report or dashboard requirements, perform data mapping including multiple internal data sources and/or the use of public data, to create BI solutions to solve business issues. This often involves conducting complex root cause analysis to troubleshoot data discrepancies, work with end users to correct business processes, and systematically cleanse inaccurate data sets.
Identify performance metrics and KPIs with relevant business area leaders. Apply innovative visualization and data science techniques to develop interactive dashboards, and to analyze and interpret trends or patterns using a large data population.
Effectively work and collaborate with PMO on BI projects, ensure deliverables are completed on time, and objectives are met.
Manage tables and data views within Oracle database, SQL Server, PostgreSQL and synchronize with data models with corresponding data mapping and appropriate ETL tools. Keep current on the latest technologies, develop proof-of-concept prototypes to advance the BI program.
Commit to professional development by fostering a climate of team learning, cross-training and knowledge base building.
Handle complex data issues with minimal supervision, while escalating critical issues to management using sound judgment. Enforce data quality, knowledge retention, and process consistency through the curation and expansion of the institutional data dictionary/catalog.
Communicate effectively with internal team and external business users through written, verbal and visual storytelling. Develop detailed technical specifications, data mapping and data flow documentations for major data analytics projects.
Requirements
Bachelor’s Degree in Computer Science, Management Information Systems, Business or a similar degree, combined work experience and education as equivalent. Higher Education experience preferred
10 years of experience as data visualization analyst, business intelligence analyst or advanced report writer.
5+ years of Tableau Dashboard development experience.
Experience working with data stewards, data governance, and data standards.
Experience working with Ellucian Banner, Slate, Canvas or Workday preferred
Certification in Tableau or other applicable business intelligence tools preferred
Strong SQL query and Oracle relational databases skills.
Ability to extract diverse datasets, perform data mapping, and ETL data transformation.
Show more
Show less","Data Visualization, Business Intelligence, Dashboards, Reporting, Data Analytics, Data Mining, Data Modeling, Data Warehousing, ETL, Data Governance, Data Quality, Data Cleansing, Root Cause Analysis, Performance Metrics, KPIs, Interactive Dashboards, Trend Analysis, Pattern Recognition, PMO, Oracle, SQL Server, PostgreSQL, Tableau, Ellucian Banner, Slate, Canvas, Workday, Data Dictionary, Data Catalog, Data Storytelling, Technical Specifications, Data Mapping, Data Flow Documentation, Data Stewards, Data Standards","data visualization, business intelligence, dashboards, reporting, data analytics, data mining, data modeling, data warehousing, etl, data governance, data quality, data cleansing, root cause analysis, performance metrics, kpis, interactive dashboards, trend analysis, pattern recognition, pmo, oracle, sql server, postgresql, tableau, ellucian banner, slate, canvas, workday, data dictionary, data catalog, data storytelling, technical specifications, data mapping, data flow documentation, data stewards, data standards","business intelligence, canvas, dashboard, data catalog, data dictionary, data flow documentation, data governance, data mapping, data mining, data quality, data standards, data stewards, data storytelling, dataanalytics, datacleaning, datamodeling, datawarehouse, ellucian banner, etl, interactive dashboards, kpis, oracle, pattern recognition, performance metrics, pmo, postgresql, reporting, root cause analysis, slate, sql server, tableau, technical specifications, trend analysis, visualization, workday"
Senior Data Analyst,Kellton,"Boston, MA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-kellton-3774002092,2023-12-17,Revere,United States,Mid senior,Hybrid,"Title - Senior Data Analyst
Worksite Address: Boston, MA
Duration 6 months with possible extension
Hours per Week: 37.5
This contract position is a hybrid position
Responsibilities/Experience/Qualifications
Primary Responsibilities
Partner with business and technical groups and actively engage in strategic initiatives requiring complex analytic models and solutions.
Independently lead design sessions, perform data cleansing, and implement new data analytics tools to extract business intelligence from a business model with large volumes of data.
Must be able to collect, store and associate different data model from different data sources.
Process data, including data cleansing and transformation, in preparation for data analysis.
Use statistical methods and data visualization tools to analyze data, identify trends and patterns.
Create reports, graphs and other visualizations to present and explain data to business.
Working with business and technical members to ensure data is collected accurately and up-to-date.
Identify business data collection and analysis area of improvement or suggesting changes to current business processes.
Recommend new tools or technologies that can help streamline data analysis.
Able to develop and refine predictive models that can be used to make informed decisions.
Able to work with machine learning algorithms to analyze data is a plus.
Work with business users and technical teams to research, understand, and document technical specifications using common tools such as JIRA, VISIO, SharePoint, MS Office Suite.
Ability to work independently as well as part of a data team in fast paced and dynamic environment.
Qualifications
BS/MS in a relevant field such as mathematics, statistics, or advanced computer science.
8+ years of relevant hands-on experience in Business Intelligence domain working as a Data Analyst/Scientist.
Must have strong analytical skills and be able to use statistical and data analysis tools to interpret data and identify trends and patterns.
Must be strong in data collection and transformation using SQL, Excel, Python as well as analytical tools such as Power BI or Tableau.
Strong communication skills to effectively communicate findings to technical team and business stake holders.
Posse great attention to detail and able to spot patterns in large and complex data sets.
Must be able to work with business, technical teams to learn and understand business and their data.
Able to develop solutions and data insights and transform them into recommendations for decision makers.
Certification in Business Data Analytics Certification preferred, but not required.
Some background in environmental issues, energy, or government experience preferred but not required.
Desired Skills
Must have exceptional verbal and written communication skills.
Must be a quick and enthusiastic learner.
Must be able to recommend best practices for Data Governance.
Effective negotiator with the ability to successfully balance business needs with IT time and resource constraints and arrive at mutually acceptable outcomes.
Comfort working in a fluid environment where visions and requirements may change.
Must be able to work directly with end-users to ensure that the reports meet requirements.
Must be able to direct technical staff with a collaborative, hands-on approach.
Applies a proven customer-first approach when working with others.
Should you be interested, please send a copy of your resume to swapnil.tayade@kelltontech.net along with the following details ASAP.
Full Name
Current Location:
Work Authorization
Hourly rate on C2C/W2:
Earliest Available Date To Start
Date and times available to interview:
Show more
Show less","Data Analysis, Data Cleansing, Data Visualization, Predictive Modeling, Machine Learning, JIRA, VISIO, SharePoint, MS Office Suite, SQL, Excel, Python, Power BI, Tableau, Data Governance, Verbal Communication, Written Communication, Business Needs, IT Time, Resource Constraints","data analysis, data cleansing, data visualization, predictive modeling, machine learning, jira, visio, sharepoint, ms office suite, sql, excel, python, power bi, tableau, data governance, verbal communication, written communication, business needs, it time, resource constraints","business needs, data governance, dataanalytics, datacleaning, excel, it time, jira, machine learning, ms office suite, powerbi, predictive modeling, python, resource constraints, sharepoint, sql, tableau, verbal communication, visio, visualization, written communication"
Data / AI Scientist,BioSpace,"Boston, MA",https://www.linkedin.com/jobs/view/data-ai-scientist-at-biospace-3781519984,2023-12-17,Revere,United States,Mid senior,Hybrid,"By clicking the “Apply” button, I understand that my employment application process with Takeda will commence and that the information I provide in my application will be processed in line with Takeda’s Privacy Notice and Terms of Use. I further attest that all information I submit in my employment application is true to the best of my knowledge.
Job Description
Job Title:
Data / AI Scientist (Hybrid)
Location
: Cambridge, MA
About The Role
We are looking for an experienced and driven Data / AI Scientist to join the Digital Innovation and AI team within Global Manufacturing & Supply / Global Quality Data Digital & Technology (GMS/GQ DD&T). Our team strives to be a leader in digital and outcome-driven innovation amongst our peers in the pharmaceutical industry – we identify capability gaps, design digital solutions, accelerate experimentation, conduct market research, and improve business operations end-to-end.
How You Will Contribute
Work across GMS/GQ to define the technical problem statement, identify hypothesis to test, and develop novel ways to meet goals using modern techniques and tools.
Communicate the analytics approach and how it will address our goals, in a manner that is easy for technical and non-technical audiences to comprehend.
Create repeatable, interpretable, dynamic, and scalable models that can be incorporated into analytic data products.
Leverage open source algorithms and fine-tune them for specific needs.
Improve and experiment with prompt designs (prompt engineering) to improve AI model performance and end user experience.
Develop prompt templates for common use cases and adapt them for multiple applications.
Design test automation, and potential scaling roadmap and methods, set standards, suggest governance to ensure model integrity.
Collaborate, coach, and learn with a growing team of data scientists with varying levels of expertise in the field.
Mentor and lead other data / AI scientists (not necessarily direct reports), providing guidance, expertise, and training
Collaborate with cross-functional teams to set performance goals and review progress.
What You Bring To Takeda
12+ years real-world experience applying expertise in artificial intelligence through use of machine learning and data mining, to design, prototype, and build next-generation analytics.
Industry experience developing, training, and testing of deep learning models in the life-sciences domain.
Hands-on development of AI solutions that comply with sound ethical use of data, government regulations, and industry standards.
Experienced with creating well-structured prompts to instruct AI models and conducting experiments to increase model behavior based on prompt variations.
Experience with Agile product development and expert user of Jira, Confluence, Databricks, Python or R, Spark, and Hive.
Work with other data scientists and developers across multiple geographies; and ability to guide engineers on how to remove technical road-blocks.
Knowledge of public cloud infrastructure
Firm grasp of industry, digital, and artificial intelligence trends and market conditions
Enjoys teaching others who are less knowledgeable about the latest industry trends and technologies
Experience building custom solutions on Microsoft 365 platform is preferred.
Experience in biopharmaceutical or manufacturing industry is preferred.
What Takeda Can Offer You
Comprehensive Healthcare: Medical, Dental, and Vision
Financial Planning & Stability: 401(k) with company match and Annual Retirement Contribution Plan
Health & Wellness programs including onsite flu shots and health screenings
Generous time off for vacation and the option to purchase additional vacation days
Community Outreach Programs and company match of charitable contributions
Family Planning Support
Professional training and development opportunities
Tuition reimbursement
Important Considerations
At Takeda, our patients rely on us to deliver quality products. As a result, we must follow strict rules in our manufacturing facilities to ensure we are not endangering the quality of the product. In this role, you may:
Work in a controlled environments requiring special gowning. Will be required to follow gowning requirements and wear protective clothing over the head, face, hands, feet and body
No make-up, jewelry, contact lenses, nail polish or artificial fingernails may be worn in the manufacturing environment.
Will work in a cold and/or wet environment.
Must be able to work multiple shifts, including weekends.
Non-Exempt Roles only: Must be able to work overtime as required.
May be required to work in a confined area.
Some Clean Room and cool/hot storage conditions.
We have a hybrid working environment
Primary office location for this role will be Lexington / Cambridge MA
We are a global organization with many colleagues in the US Eastern and Central European time zones
More About Us
At Takeda, we are transforming patient care through the development of novel specialty pharmaceuticals and the best patient support programs. Takeda is a patient-focused company to inspire and empower you to grow through life-changing work.
Certified as a Global Top Employer, Takeda offers stimulating careers, encourages innovation, and strives for excellence in everything we do. We foster an inclusive, collaborative workplace, in which our teams are united by an unwavering commitment to deliver Better Health and a Brighter Future to people around the world.
This posting excludes Colorado applicants.
#GMSGQ #ZR1
EEO Statement
Takeda is proud in its commitment to creating a diverse workforce and providing equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, parental status, national origin, age, disability, citizenship status, genetic information or characteristics, marital status, status as a Vietnam era veteran, special disabled veteran, or other protected veteran in accordance with applicable federal, state and local laws, and any other characteristic protected by law.
Locations
Boston, MA
Worker Type
Employee
Worker Sub-Type
Regular
Time Type
Full time
Show more
Show less","Data Science, Artificial Intelligence, Machine Learning, Data Mining, Deep Learning, Agile Development, Jira, Confluence, Databricks, Python, R, Spark, Hive, Public Cloud Infrastructure, Microsoft 365, Biopharmaceutical Industry, Manufacturing Industry","data science, artificial intelligence, machine learning, data mining, deep learning, agile development, jira, confluence, databricks, python, r, spark, hive, public cloud infrastructure, microsoft 365, biopharmaceutical industry, manufacturing industry","agile development, artificial intelligence, biopharmaceutical industry, confluence, data mining, data science, databricks, deep learning, hive, jira, machine learning, manufacturing industry, microsoft 365, public cloud infrastructure, python, r, spark"
"Staff Software Engineer, Data Infrastructure",Verily,"Boston, MA",https://www.linkedin.com/jobs/view/staff-software-engineer-data-infrastructure-at-verily-3780593533,2023-12-17,Revere,United States,Mid senior,Hybrid,"Who We Are
Verily is a subsidiary of Alphabet
that is using a data-driven approach to change the way people manage their health and the way healthcare is delivered. Launched from Google X in 2015, our purpose is to bring the promise of precision health to everyone, every day. We are focused on generating and activating data from a variety of sources, including clinical, social, behavioral and the real world, to arrive at the best solutions for a person based on a comprehensive view of the evidence. Our unique expertise and capabilities in technology, data science and healthcare enable the entire healthcare ecosystem to drive better health outcomes.
Description
As a Staff Software Engineer working on our Data Infrastructure you will report to the Head of Production Engineering and Cloud Operations, and you will help us design and implement our big data and machine learning capabilities on Google Cloud. You will work with data scientists, ML researchers, and software engineers to help them optimize and automate their use of GCP technology. You will also work closely with security and other cloud engineers to incorporate your designs into our automation platform, using technologies like Terraform, Spacelift, and the Atlassian stack. You will also help develop training materials and documentation to enable engineers to quickly adopt our tooling. As an expert in big data and machine learning workloads, you will also help influence architecture and design decisions for data related solutions across the company.
Responsibilities
Work with data science and ML research teams to design and deploy resilient infrastructure to support multiple different big data and ML use cases optimizing for security, cost, and chip availability constraints.
Work with engineering, security and privacy teams to design and maintain Google Cloud infrastructure components to support complex data governance and Access On-Demand frameworks, including GCP IAM permissions, OKTA Integration, and VPC Service control perimeters.
Work with all Verily teams that process data to help design and maintain secure data import and export processes, including integration with customer identity management.
Work with all Verily teams that process data to help design, test, and maintain data backup and recovery plans.
Qualifications
Minimum Qualifications:
Practical experience and/or Bachelor’s degree in Computer Science or a related technical field.
Experience working on and leading large data related projects featuring: ETL/ELT frameworks, metrics stores, infrastructure management, and data security.
Experience working with GCP or AWS big data and ML services like VertexAI, BigQuery, Apache Beam, and PubSub.
Excellent written and verbal communication skills.
Preferred Qualifications
Experience working with Google Cloud data products and Terraform.
Experience working with Ray.io or similar technology.
Experience working with modern ELT tools such as FiveTran, Boomi.
Experience designing, training and service complex ML Models.
Experience writing technical documentation or building code labs.
The US base salary range for this full-time position is $174,000 - $276,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.
Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits.
Why Join Us
Build What’s Vital.
At Verily, you are a part of something bigger. We are a diverse team of builders innovating at the intersection of health and technology—united by a shared spirit of curiosity, resilience and determination to make better health possible for all. This builder mindset means your fingerprints will be on the work that shapes the future of health.
Fulfilling our precision health purpose starts with the health of our Veeps, which is why we offer flexibility, resources, and competitive benefits to support you in your whole-person well being.
Our culture reflects the behaviors that stem from living our values every day in how we Innovate Healthcare and Technology, Gain Velocity as One Verily, and Respect Individuals. As One Verily, we uphold our collective accountability to sustain this culture and to create a VIBE (Verily’s Culture of Inclusion, Belonging, and Equitability) where all Veeps feel included, a sense of belonging, and have opportunities to grow.
If this sounds exciting to you, we would love to hear from you.
You can find out more about our company culture on our LinkedIn Company Page and Verily Careers page.
Show more
Show less","Computer Science, ETL/ELT frameworks, Metrics stores, Infrastructure management, Data security, Google Cloud data products, Terraform, Ray.io, Google Cloud, AWS, BigQuery, Apache Beam, PubSub, VertexAI, FiveTran, Boomi, ML Models, Technical documentation, Code labs","computer science, etlelt frameworks, metrics stores, infrastructure management, data security, google cloud data products, terraform, rayio, google cloud, aws, bigquery, apache beam, pubsub, vertexai, fivetran, boomi, ml models, technical documentation, code labs","apache beam, aws, bigquery, boomi, code labs, computer science, data security, etlelt frameworks, fivetran, google cloud, google cloud data products, infrastructure management, metrics stores, ml models, pubsub, rayio, technical documentation, terraform, vertexai"
Master Data Engineer,Siemens Gamesa,"Shanghai, VA",https://www.linkedin.com/jobs/view/master-data-engineer-at-siemens-gamesa-3734549308,2023-12-17,Abingdon,United States,Mid senior,Onsite,"It takes the brightest minds to be a technology leader. It takes imagination to create green energy for the generations to come. At Siemens Gamesa we make real what matters, join our global team.
Siemens Gamesa has a vision for renewable energy: we believe in the power of nature and technology. Help us to be ready to face the energy challenges of tomorrow and make a green footprint – join the team in creating a better future for us on our planet.
We focus on hiring the best people, wherever they may be in the world. We pride ourselves on the flexibility we offer to our employees and are committed to building a workforce that can grow with the company. Siemens Gamesa is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
In our culture of trust, we focus on empowerment, diversity and continuous learning. Valuing our people is what makes us one global team, with our colleagues’ safety at the heart of our organization.
How To Contribute To Our Vision
Creation of new Blade Products and variants in given quality, time, and cost frame,
Creation of Blade BoMs out of TE specifications and manage product changes deployment, including each plant requirement (materials and documentation) and parameterization in SAP,
Leading engineering change management process for material management in team center and product structures in windchill for logistic view.,
Parameterize SAP according to each plant requirement,
Test and maintain the systems and their interfaces,
Creation of project reports and materials related change notes and material readiness for related SOPs and milestones BOM one pagers,
Close relationship with plant Process engineers for a better definition of the BoM and related documents, and the completion of the Operations Change Notes,
Support plants and EXE with blades gateways,
Support TE, procurement, process engineers, plants and finance departments with content and input related to semi annual budget blade BoMs and cost-outs.
What you need to make a difference?
A passion for renewable energy and a sense for the importance to lead the change. We are also looking for:
University degree in Engineering or equivalent significant experience in manufacturing engineering/ We will also consider newly graduate students with no experience, who will demonstrate a big drive for learning and developing,
Previous experience: Minimum 2 years in a similar position,
Ability to interact effectively with the plant stakeholders involved in the Change Management; Warehouse, Quality, local buyers, Process Engineers, Technology, TPM and PMO,
Windchill (PLM) and Teamcenter (PLM) advanced user profile,
SAP S4 expert user profile, particularly in Material Master, BOMs, Routings, Production and Logistics,
NPI/NPP/NFI project type gates and milestones awareness
MS Office advanced user,
Languages: Fluent in written and spoken English. Knowledge of country of employment's local language also required (Portuguese or Chinese),
Deep knowledge in working with Product Configuration with Engineering and Manufacturing Bill of Materials,
Extensive understanding of how PLM (Windchill and Teamcenter) and ERP(SAP) systems and their interfaces work together.
In Return Of Your Commitment We Offer You
Become a part of our mission for sustainability: Clean energy for generations to come,
A global team of diverse colleagues who share passion for renewable energy,
Trust and empowerment to make your own ideas reality,
Personal and professional development to grow internally within our organization,
Attractive remuneration package,
Local benefits, employee discounts and much more.
Empowering our people
https://www.siemensgamesa.com/sustainability/employees
How do you imagine the future?
https://youtu.be/12Sm678tjuY
Our global team is on the front line of tackling the climate crisis, reducing carbon emissions – the greatest challenge we face.
Severely disabled applicants are encouraged to reach out to us. We have inclusive recruiting processes specifically for severely disabled persons in Portugal and we do our utmost to tailor working spaces to suit your individual needs. In cases of severe disability, applicants who are equally qualified will be given preferential consideration.
Siemens Gamesa is an equal opportunity employer and maintains a work environment that is free from discrimination and where employees are treated with dignity and respect. Employment at Siemens Gamesa is based solely on an individual's merit and qualifications, which are directly related to job competence. Siemens Gamesa does not discriminate against any employee or job applicant on the basis of race, ethnicity, nationality, ancestry, genetic information, citizenship, religion, age, gender, gender identity/expression, sexual orientation, pregnancy, marital status, disability or any other characteristic protected by applicable laws, rules or regulations. We adhere to these principles in all aspects of employment, including recruiting, hiring, training, compensation, promotion and benefits.
We are driven by people - from more than 100 different countries, they build the company we are every day. Our diverse and inclusive culture encourages us to think outside the box, speak without fear, and be bold. We value the flexibility that our smart-working arrangements, our digital disconnection framework and our family-friendly practices bring to the new way of working.
Show more
Show less","Engineering, Manufacturing engineering, Windchill (PLM), Teamcenter (PLM), SAP S4, Material Master, BOMs, Routings, Production, Logistics, NPI/NPP/NFI project type gates and milestones, MS Office, English, Portuguese, Chinese, Product Configuration, Engineering Bill of Materials, Manufacturing Bill of Materials, ERP(SAP)","engineering, manufacturing engineering, windchill plm, teamcenter plm, sap s4, material master, boms, routings, production, logistics, npinppnfi project type gates and milestones, ms office, english, portuguese, chinese, product configuration, engineering bill of materials, manufacturing bill of materials, erpsap","boms, chinese, engineering, engineering bill of materials, english, erpsap, logistics, manufacturing bill of materials, manufacturing engineering, material master, ms office, npinppnfi project type gates and milestones, portuguese, product configuration, production, routings, sap s4, teamcenter plm, windchill plm"
"Sr. Staff Engineer, Data & I/O Design",SK hynix America,"Sacramento, CA",https://www.linkedin.com/jobs/view/sr-staff-engineer-data-i-o-design-at-sk-hynix-america-3785869695,2023-12-17,Davis,United States,Associate,Onsite,"Job Title : Sr. Staff Engineer – Data&IO Design
Location : Sacramento or Santa Clara CA
Job Type : Full-Time
In the rapidly growing NAND Flash space amid the era of big data, this is an exciting time to be at SK hynix NAND America
- come join our NAND Design team as a Senior staff Engineer and work on one of the most advanced 3DNAND technology portfolios in the world.
SK hynix NAND Flash memory design is part of SK hynix NAND Development America for developing advanced 3D NAND products.
Key Responsibilities:
Responsible for designing and developing data & IO design in 3D NAND Flash memory products
Design and build for higher IO speed at the block & transistor level
Design Data/IO circuit and develop circuit solutions that meet the
area, power and performance within process distribution
Create test benches, simulate, and analyze the results
Perform parasitic extraction, run pre and post-layout simulations and analyze results
Work closely with layout designer
Minimum Qualifications:
At least 6 years experience of IO design with Bachelor’s degree
RX, TX, and IO pipeline circuits design and debugging experience
Understanding of circuit layout with layout review and supervision
Understanding of high speed IO operation mechanism
Experience of power/signal integration at high IO speed area
Good knowledge of Device physics at transistor level
Experience with commercial schematic and simulation tools
Benefits:
The base salary range for this full-time position is: $137,000 - $168,000.
Individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training.
Additional compensation may include annual bonuses, discretionary bonuses and benefits.
The benefits include the following.
We provide top-class healthcare benefits including medical, dental, vision, employee assistance plans for employees and their dependents.
We provide multiple leave types, including paid time off, 11 holidays, 12 Happy Fridays (additional days off), and family and medical leaves.
We provide market-competitive retirement programs (401(k) plans) for your future
.
Equal Employment Opportunity:
SKHYA is an Equal Employment Opportunity Employer. We provide equal employment opportunities to all qualified applicants and employees and prohibit discrimination and harassment of any type without regard to race, sex, pregnancy, sexual orientation, religion, age, gender identity, national origin, color, protected veteran or disability status, genetic information or any other status protected under federal, state, or local applicable laws.
Show more
Show less","Data and IO Design, 3D NAND Flash Memory Products, IO Design, RX, TX, IO Pipeline Circuits, Circuit Layout, Layout Review, Supervision, Highspeed IO Operation Mechanism, Power/Signal Integration, High IO Speed Area, Device Physics, Transistor Level, Commercial Schematic, Simulation Tools, Pre and PostLayout Simulations","data and io design, 3d nand flash memory products, io design, rx, tx, io pipeline circuits, circuit layout, layout review, supervision, highspeed io operation mechanism, powersignal integration, high io speed area, device physics, transistor level, commercial schematic, simulation tools, pre and postlayout simulations","3d nand flash memory products, circuit layout, commercial schematic, data and io design, device physics, high io speed area, highspeed io operation mechanism, io design, io pipeline circuits, layout review, powersignal integration, pre and postlayout simulations, rx, simulation tools, supervision, transistor level, tx"
Data Analyst,Adame Services,"Elk Grove, CA",https://www.linkedin.com/jobs/view/data-analyst-at-adame-services-3676687484,2023-12-17,Davis,United States,Mid senior,Onsite,"Title
: Data Analyst
Location
: Elk Grove, CA
Duration
: 12 Months
Must have recent continuous experience in Tableau/Dashboard/SQL for 5 yrs. min
Requirements And Skills
Use Tableau to create visualizations
Use SQL to build tables
Machine learning experience is not required
Skill Sets
Tableau - 5-10 years Is Required
SQL - 5-10 years Is Required
Show more
Show less","Tableau, SQL","tableau, sql","sql, tableau"
Data Analyst,Adame Services,"Elk Grove, CA",https://www.linkedin.com/jobs/view/data-analyst-at-adame-services-3677459368,2023-12-17,Davis,United States,Mid senior,Onsite,"Title
: Data Analyst
Location
: Elk Grove, CA
Duration
: 12 Months
Must have recent continuous experience in Tableau/Dashboard/SQL for 5 yrs. min
Requirements And Skills
Use Tableau to create visualizations
Use SQL to build tables
Machine learning experience is not required
Skill Sets
Tableau - 5-10 years Is Required
SQL - 5-10 years Is Required
Show more
Show less","Tableau, SQL","tableau, sql","sql, tableau"
"Data Analyst 1 - Sacramento, CA, 95828",eStaffing Inc.,"Sacramento, CA",https://www.linkedin.com/jobs/view/data-analyst-1-sacramento-ca-95828-at-estaffing-inc-3655982982,2023-12-17,Davis,United States,Mid senior,Onsite,"Description
Researches, analyzes, consolidates and interprets data using statistical and data analytics methods to create information on business-relevant topics, e.g. market environment, operational process and equipment performance etc.
Acquires data from primary or secondary data sources and maintain databases/data systems.
Operates and optimizes pre-defined tools, applications and data bases/data management systems.
Creates reports and communicates results to various internal and/or external stakeholders (e.g. management, customers).
Show more
Show less","Statistical analysis, Data analytics, Statistical methods, Databases, Data systems, Data management systems, Data interpretation, Data consolidation, Data acquisition, Data optimization, Data communication, Reporting, Management, Customers","statistical analysis, data analytics, statistical methods, databases, data systems, data management systems, data interpretation, data consolidation, data acquisition, data optimization, data communication, reporting, management, customers","customers, data acquisition, data communication, data consolidation, data interpretation, data management systems, data optimization, data systems, dataanalytics, databases, management, reporting, statistical analysis, statistical methods"
Data Analyst - Power Supply / Renewable Energy,CyberCoders,"Sacramento, CA",https://www.linkedin.com/jobs/view/data-analyst-power-supply-renewable-energy-at-cybercoders-3714430360,2023-12-17,Davis,United States,Mid senior,Onsite,"If you are a Data Analyst with power supply or experience in the renewable energy industry, please read on!
Job Title: Data Analyst
Job Location: Sacramento area / Hybrid
Job Type: Full time / Direct-hire
Based in Sacramento, CA, we are a multi-service consulting and engineering firm with a 36 year history of success serving the electric, gas, water, and wastewater utilities sectors, and helping our clients make the most cost-effective and environmentally responsible choices when it comes to energy consumption and output. Due to growth and demand we are in need of a talented Data Analyst to join our team.
Top Reasons to Work with Us
We take great pride in our fast-paced and innovative culture, which allows our team to develop cutting-edge solutions to power our customers' businesses.
We offer flexible working hours, and a stimulating professional environment. Furthermore, we provide a comprehensive benefits package and competitive salary.
What You Will Be Doing
As a Data Analyst, you will use your technical expertise to analyze data, identify trends, and provide recommendations that will improve our operational efficiency and profitability.
You will be responsible for developing and maintaining databases, designing and developing innovative solutions, and providing insights on market design and power supply planning.
Work cross-organizationally with multiple teams across our organization, including teams in other regions and time zones.
Help us continue to build out our Folsom office.
What You Need for this Position
Bachelor's degree in a related field of study preferred
2 to 5+ years of hands-on professional experience
Experience with SQL, Python, or VBA
Effective written and oral communication skills
Ability to manage multiple priorities and deadlines
Nice To Have
Experience working with power supply portfolio management concepts
Experience in the evaluation of solar and storage economics
Experience in wholesale organized market design/policy
What's In It for You
Competitive base salary ($80K to $100K DOE)
Bonus
Health, Dental, Vision
PTO
Sick time
401(K)
Employee Assistance Program (EAP)
Tuition Reimbursement
A great company culture
Enjoyable company events/get-togethers
Much more!
So if this sounds like you and you're interested in joining an incredible organization, please apply today!
Email Your Resume In Word To
Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:
Brennen.Leftwick@CyberCoders.com
Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : BL1-1763577 -- in the email subject line for your application to be considered.***
Brennen Leftwick - Recruiting Manager - CyberCoders
Applicants must be authorized to work in the U.S.
CyberCoders is proud to be an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work
– In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
Show more
Show less","Data Analysis, SQL, Python, VBA, Database Development, Data Visualization, Market Design, Power Supply Planning, Power Supply Portfolio Management, Economics, Wholesale Organized Market Design/Policy","data analysis, sql, python, vba, database development, data visualization, market design, power supply planning, power supply portfolio management, economics, wholesale organized market designpolicy","dataanalytics, database development, economics, market design, power supply planning, power supply portfolio management, python, sql, vba, visualization, wholesale organized market designpolicy"
Data Analyst III,WinMax,"Elk Grove, CA",https://www.linkedin.com/jobs/view/data-analyst-iii-at-winmax-3645143795,2023-12-17,Davis,United States,Mid senior,Onsite,"Title:
Data Analyst III Req#: 17163295
Location: Eik Grove,CA(
This position will be in Elk Grove and will be on the hybrid schedule we are in (T-Th in office)
Contract:6 Month
Job Description
Duties
Strong data and analytic skills, knowledge of excel, numbers and data modeling. Knowledge of or experience working on
subscription businesses a plus. Reporting and presentation skills a must. Looking for a person who has a passion for data
and numbers. The successful candidate will turn data into information and insights that will help support business
decisions. Work closely with the Data Analytics team and cross functionally in marketing to produce monthly & quarterly
data driven marketing reports. Interpret data, analyze results using statistical techniques and provide ongoing reports.
Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical
efficiency and quality. Acquire data from primary or secondary data sources and maintain databases/data systems.
Identify, analyze, and interpret trends or patterns in complex data sets. Filter and ""clean"" data by reviewing computer
reports, printouts, and performance indicators to locate and correct code problems. Work with management to
prioritize business and information needs. Locate and define new process improvement opportunities.
Skills
MacOS & iOS a MUST
Education
Show more
Show less","Data analysis, Excel, Numerical modeling, Data modeling, Statistical analysis, Reporting, Data visualization, Database development, SQL, Python, R, Statistical techniques, Data collection, Data quality, Data interpretation, Trend analysis, Data cleaning, Data integration, MacOS, iOS","data analysis, excel, numerical modeling, data modeling, statistical analysis, reporting, data visualization, database development, sql, python, r, statistical techniques, data collection, data quality, data interpretation, trend analysis, data cleaning, data integration, macos, ios","data cleaning, data collection, data integration, data interpretation, data quality, dataanalytics, database development, datamodeling, excel, ios, macos, numerical modeling, python, r, reporting, sql, statistical analysis, statistical techniques, trend analysis, visualization"
Supply Chain Data Analyst,Siemens,"Sacramento, CA",https://www.linkedin.com/jobs/view/supply-chain-data-analyst-at-siemens-3781700307,2023-12-17,Davis,United States,Mid senior,Onsite,"Siemens Mobility Inc., Rolling Stock Division produces light rail vehicles, electric and diesel locomotives, and passenger coaches. The Supply chain data analyst is a key member of the Order Management team and manages the critical and complex suppliers to ensure on time delivery of open purchase orders to support production demand.
What you will be doing:
You will be responsible for data management, which comprises of analyzing raw and highly complex data from SAP using advanced excel skills and Tableau.
Will develop new reports, analyze existing reports, and monitor key performance indicators.
Manage Siemens supplier- buyer collaboration portal as a super user.
Train suppliers, Siemens’ associates on the portal and act as the point of contact for the entire portal.
Brainstorm ideas to improve the functionalities of the portal and KPIs on a high-level.
Create data validation exceptions for identifying points of failure if any.
Leverage deep understanding of supply chain principles and appropriate analytics techniques to identify bottlenecks and critical control points in Siemens’s supply chain.
Collaborate with Materials Managers, Engineers, and Project Managers to understand Supply chain problems and identify opportunities for improvement.
Translate business problems into manageable analytics projects and lead execution across cross-functional teams.
Ensure continuous supply of required and materials and communicate any supply problems which may pose a risk or impact on business operations.
Lead purchasing activities with manufacturing, planning, sourcing, and engineering departments to solve complicated material supply issues.
Partner with Materials Program Managers, Project Managers, and quality for un-interrupted delivery of parts for production.
Proactively manage suppliers where delivery dates are at risk by escalating within the supplier organization to identify and evaluate opportunities to improve deliveries.
What you will need to be successful in this role:
2 to 5 years of experience in Procurement / SCM in a manufacturing environment.
Bachelor’s degree in a related discipline – Business, Commercial, Manufacturing, or Engineering degree. Master’s preferred.
Proficient MS Excel skills. Well versed other MS Office products (Word, PP & Outlook).
Tenacious and assertive driver who escalates in supplier organization as well as internally.
We would be impressed if you are:
Highly proficient in SAP ERP
Expert in advanced Excel (Macros, conditional logics and other advanced excess skills)
Have strong technical knowledge and understanding of procurement, inventory control, and process control.
Experience working in manufacturing environment.
Why Siemens Mobility?
We are # 9 On Forbes Great Places to Work List
Health and Wellness: Health Insurance, Dental Insurance, Vision Insurance, HSA, Commuter Benefits
Vacation & Time Off: Paid Vacation, Paid Holidays, Personal / Sick Days, Maternity/Paternity Leave,
Financial & Retirement: Savings Plan, Performance Bonus for Key Roles, Merit Increases, Relocation
Professional Development: Tuition Reimbursement, Promote from Within, Mentor Program, Access to Online Courses, Lunch & Learns
Office Life: Flexible Work Hours, Diversity & Inclusion Program
The salary range for this position in Sacramento, CA is $96,800-130,700 per year. Siemens offers a variety of health and wellness benefits to employees. Details regarding our benefits can be found here: www.benefitsquickstart.com. In addition, this position is eligible for time off in accordance with Company policies, including paid sick leave, paid parental leave, PTO (for non-exempt employees) or non-accrued flexible vacation (for exempt employees).
Equal Employment Opportunity Statement
Siemens is an Equal Opportunity and Affirmative Action Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to their race, color, creed, religion, national origin, citizenship status, ancestry, sex, age, physical or mental disability unrelated to ability, marital status, family responsibilities, pregnancy, genetic information, sexual orientation, gender expression, gender identity, transgender, sex stereotyping, order of protection status, protected veteran or military status, or an unfavorable discharge from military service, and other categories protected by federal, state or local law.
EEO is the Law
Applicants and employees are protected under Federal law from discrimination. To learn more, Click here.
Pay Transparency Non-Discrimination Provision
Siemens follows Executive Order 11246, including the Pay Transparency Nondiscrimination Provision. To learn more, Click here.
California Privacy Notice
California residents have the right to receive additional notices about their personal information. To learn more, click here.
Show more
Show less","Supply chain management, Data analysis, SAP, Advanced Excel, Tableau, MS Office Suite, Procurement, Inventory control, Process control, Manufacturing","supply chain management, data analysis, sap, advanced excel, tableau, ms office suite, procurement, inventory control, process control, manufacturing","advanced excel, dataanalytics, inventory control, manufacturing, ms office suite, process control, procurement, sap, supply chain management, tableau"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Elk Grove, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744393674,2023-12-17,Davis,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Science, Business Intelligence, Machine Learning, Python, SQL, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Management, Data Governance, Security, Scalability, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Legal Compliance, Data Classification, Data Retention","data engineering, data science, business intelligence, machine learning, python, sql, airflow, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management, data governance, security, scalability, tdd, pair programming, continuous integration, automated testing, deployment, legal compliance, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data governance, data management, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, machine learning, pair programming, python, scalability, schema design, security, spark, sparkstreaming, sql, storm, tdd"
"Sr. Principal/Principal Engineer, Data & I/O Design",SK hynix America,"Sacramento, CA",https://www.linkedin.com/jobs/view/sr-principal-principal-engineer-data-i-o-design-at-sk-hynix-america-3785874539,2023-12-17,Davis,United States,Mid senior,Onsite,"Job Title : Principal / Sr. Principal Engineer – Data&IO Design
Location : Sacramento or San Jose CA
Job Type : Full-Time
In the rapidly growing NAND Flash space amid the era of big data, this is an exciting time to be at SK hynix NAND America - come join our NAND Design team as a Principal/Senior Principal Engineer and work on one of the most advanced 3D NAND technology portfolios in the world.
SK hynix NAND Flash memory design is part of SK hynix NAND Development America for developing advanced 3D NAND products.
Key Responsibilities:
Responsible for designing and developing data & IO design in 3D NAND Flash memory products
Architect, design and build for higher IO speed at 3D NAND memory
Design Data/IO circuit and develop circuit solutions that meet the area, power and performance within process distribution
Develop advanced High Speed IO design
Lead the front-end/back-end HSIO tests in the post silicon
Work closely with the layout designers to optimize area and performance
Work with several cross-functional teams for resolving product level issues
Mentoring and instructing junior engineers along with ensuring timely product delivery
Minimum Qualifications:
At least 10 years experience of IO design with Bachelor’s degree
RX, TX, and IO pipeline circuits design and debugging experience
Experience with DDR IO interface trainings and Calibrations
Detailed understanding of high speed IO circuits layout
Understanding of high speed IO operation mechanism along with reliability mechanisms
Experience of power/signal integration at high IO speed area
Strong communication skills with ability to convey technical concepts to peers
Benefits:
The base salary range for this full-time position is: $182,000 - $257,000.
Individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training.
Additional compensation may include annual bonuses, discretionary bonuses and benefits.
The benefits include the following.
We provide top-class healthcare benefits including medical, dental, vision, employee assistance plans for employees and their dependents.
We provide multiple leave types, including paid time off, 11 holidays, 12 Happy Fridays (additional days off), and family and medical leaves.
We provide market-competitive retirement programs (401(k) plans) for your future
.
Equal Employment Opportunity:
SKHYA is an Equal Employment Opportunity Employer. We provide equal employment opportunities to all qualified applicants and employees and prohibit discrimination and harassment of any type without regard to race, sex, pregnancy, sexual orientation, religion, age, gender identity, national origin, color, protected veteran or disability status, genetic information or any other status protected under federal, state, or local applicable laws.
Show more
Show less","3D NAND Flash Memory, Data & IO Design, High Speed IO Design, IO Pipeline Circuits, DDR IO Interface, Power/Signal Integration, Layout Optimization, CrossFunctional Team Collaboration, Mentoring and Instruction, Communication Skills","3d nand flash memory, data io design, high speed io design, io pipeline circuits, ddr io interface, powersignal integration, layout optimization, crossfunctional team collaboration, mentoring and instruction, communication skills","3d nand flash memory, communication skills, crossfunctional team collaboration, data io design, ddr io interface, high speed io design, io pipeline circuits, layout optimization, mentoring and instruction, powersignal integration"
Data and Policy Analyst - Statistical Programmer,"Acumen, LLC","Sacramento, CA",https://www.linkedin.com/jobs/view/data-and-policy-analyst-statistical-programmer-at-acumen-llc-3688543688,2023-12-17,Davis,United States,Mid senior,Onsite,"At Acumen, LLC / The SPHERE Institute, Data and Policy Analysts provide analytical support for research and consulting projects for government and private clients, primarily in the area of health policy. This position is responsible for managing and analyzing data, researching and interpreting policy, and communicating findings to multiple audiences.
Data and Policy Analysts perform a wide array of functions as part of the research process. Those applicants interested in focusing on statistical programming help create and report descriptive statistic, conduct econometric and statistical analyses of data using statistical software such as SAS and STATA, interpret study specifications, research findings, and quantitative analyses for relevant audiences, develop research files from large confidential datasets, and perform other duties as assigned.
As we review your materials, we are looking for candidates who best exemplify the following qualifications:
Qualifications Required:
Bachelor’s degree in a quantitative, public policy, or related field or equivalent relevant experience
Prior work experience is not necessary
Good quantitative analytical skills
Good oral and written communication skills
Good organizational skills and strong attention to detail
Familiar with one or more of the following technical competencies: statistics, programming, policy writing, research methods documentation, or project management
Able to demonstrate initiative, critical thinking, and problem-solving
Able to work in a team-oriented environment
Able to prioritize tasks and meet deadlines in a fast-paced environment
Demonstrated aptitude and enthusiasm for learning
Qualifications Desired:
Demonstrates an interest in health and social policy
Master's degree in a quantitative, public policy, or related field or equivalent relevant experience
$50,000 - $110,000 a year
Any application without a cover letter will not be considered. Transcripts and coding samples are not required, however highly preferred and may be requested. We are excited to review your application and look forward to seeing how you may best contribute to our work.
The salary range for this position is $50,000- $110,000.
Please note, this position is available in our Sacramento, CA office. Project teams are comprised of employees who work across all of our offices. Please apply to the office location you would most prefer to work from. Applying to multiple offices for the same role will not increase your likelihood of moving forward in the hiring process.
Please upload all documents requested in the application. We are excited to review your application and look forward to seeing how you can contribute to our mission!
Show more
Show less","Statistical Programming, Descriptive Statistics, Econometrics, Statistical Analysis, SAS, STATA, Data Management, Data Analysis, Policy Writing, Research Methods Documentation, Project Management, Initiative, Critical Thinking, Problem Solving, Teamwork, Prioritization, Deadlines, Aptitude, Health Policy, Social Policy","statistical programming, descriptive statistics, econometrics, statistical analysis, sas, stata, data management, data analysis, policy writing, research methods documentation, project management, initiative, critical thinking, problem solving, teamwork, prioritization, deadlines, aptitude, health policy, social policy","aptitude, critical thinking, data management, dataanalytics, deadlines, descriptive statistics, econometrics, health policy, initiative, policy writing, prioritization, problem solving, project management, research methods documentation, sas, social policy, stata, statistical analysis, statistical programming, teamwork"
RESEARCH DATA SPECIALIST II,Caltrans,"Sacramento County, CA",https://www.linkedin.com/jobs/view/research-data-specialist-ii-at-caltrans-3782081453,2023-12-17,Davis,United States,Mid senior,Onsite,"Equal Opportunity Employer
The State of California is an equal opportunity employer to all, regardless of age, ancestry, color, disability (mental and physical), exercising the right to family care and medical leave, gender, gender expression, gender identity, genetic information, marital status, medical condition, military or veteran status, national origin, political affiliation, race, religious creed, sex (includes pregnancy, childbirth, breastfeeding and related medical conditions), and sexual orientation.
It is an objective of the State of California to achieve a drug-free work place. Any applicant for state employment will be expected to behave in accordance with this objective because the use of illegal drugs is inconsistent with the law of the State, the rules governing Civil Service, and the special trust placed in public servants.
Position Details
Job Code #:
JC-406424
Position #(s):
913-110-5758-XXX
Working Title:
Local Civil Compliance Data Specialist
Classification:
RESEARCH DATA SPECIALIST II
$6,658.00 - $8,334.00 A
Shall Consider:
RESEARCH DATA SPECIALIST I
$6,061.00 - $7,587.00 A
# of Positions:
Multiple
Work Location:
Sacramento County
Telework:
In Office
Job Type:
Permanent, Full Time
Department Information
Caltrans Mission:
Provide a safe and reliable transportation network that serves all people and respects the environment
Caltrans Vision:
A brighter future for all through a world-class transportation network
The Caltrans workforce is made up of diverse and unique individuals who contribute to our organizational success. Caltrans is about celebrating diversity, valuing one another, and recognizing that Caltrans is strong not in spite of the diverse attributes of our workforce, but because of our diversity.
Department Website:
www.dot.ca.gov
Frequently Asked Questions for an Applicant:
http://dot.ca.gov/jobs/docs/faq-ct-applicants-081617.pdf
Director’s EEO Policy
: https://dot.ca.gov/programs/equal-employment-opportunity
Director’s EEO Policy Statement:
https://dot.ca.gov/programs/equal-employment-opportunity
Job Description And Duties
Will Consider Research Data Specialist I
Under the general direction of the Branch Chief of Local Civil Compliance Data Management, a Senior Transportation Planner, the incumbent serves as a Local Civil Rights Data Specialist. This position requires a knowledgeable data specialist with the ability to collect, evaluate, clean, and interpret data from complex data collection systems. The incumbent will support developing and maintaining new and existing data collection systems and databases for the Disadvantage Business Enterprise (DBE), Title VI, and ADA programs. The incumbent must possess working knowledge of structured query language, python, or other computer programming languages along with working knowledge of software such as Microsoft Excel, Access, Power BI, Tableau, and other data visualization tools. In addition, the incumbent assists with the continued development and implementation of policies, programs, and reporting to comply with Federal and State laws related to these programs.
Eligibility for hire may be determined by your score on the Research Data Specialist II Exam. For those who do not have current eligibility (e.g., transfer, permissive reinstatement, or voluntary demotions) and/or who will be new to state civil services employment, you must be on the state examination list to be eligible for these positions. Research Data Specialist II: RDSII: https://www.calcareers.ca.gov/CalHrPublic/Exams/Bulletin.aspx?examCD=8PB40
Eligibility for hire may be determined by your score on the Research Data Specialist I Exam. For those who do not have current eligibility (e.g., transfer, permissive reinstatement, or voluntary demotions) and/or who will be new to state civil services employment, you must be on the state examination list to be eligible for these positions. Research Data Specialist I:
RDSI: https://www.calcareers.ca.gov/CalHrPublic/Exams/Bulletin.aspx?examCD=8PB39
T he Human Resources Contact is available to answer questions regarding the application process. The Hiring Unit Contact is available to answer questions regarding the position.
PARF# 50-4-090 / JC-406424
You will find additional information about the job in the
Duty Statement
.
Special Requirements
Statement of Qualifications (SOQ) required.
The Statement of Qualifications (SOQ) must be no more than two (2) pages and must be submitted along with your State Applications (STD 678), using 12-point font Arial Font, and 0.75” margins. Include “Statement of Qualifications”, name, and job control number centered in the header. Resumes, cover letters and other documents do not replace the SOQ. Applicants who fail to submit the required SOQ as outlined, will be disqualified from the selection process.
In the SOQ, please describe your skills and experience with the following:
Data modeling and implementing databases in a professional organization.
Programming languages (List what languages are you familiar with and how you have used them).
Disadvantaged Business Enterprise (DBE), Title VI, and the Americans with Disabilities Act (ADA).
Possession of Minimum Qualifications will be verified prior to interview and/or appointment. If you are basing your eligibility on education, you must include your unofficial transcript(s)/diploma for verification. Unofficial, original, or official sealed transcripts will be accepted and may be required upon appointment. Applicants with foreign transcripts/degrees must provide a transcript/degree U.S. equivalency report evaluation that indicates the number of units and degree to which the foreign coursework is equivalent. Here is a list of evaluation agencies: https://www.naces.org/members . Please redact birthdates and social security numbers.
Application Instructions
Completed applications and all required documents must be received or postmarked by the Final Filing Date in order to be considered. Dates printed on Mobile Bar Codes, such as the Quick Response (QR) Codes available at the USPS, are not considered Postmark dates for the purpose of determining timely filing of an application.
Final Filing Date: 12/29/2023
Who May Apply
Individuals who are currently in the classification, eligible for lateral transfer, eligible for reinstatement, have list eligibility, are in the process of obtaining list eligibility, or have SROA and/or Surplus eligibility (please attach your letter, if available). SROA and Surplus candidates are given priority; therefore, individuals with other eligibility may be considered in the event no SROA or Surplus candidates apply. Individuals who are eligible for a Training and Development assignment may also be considered for this position(s).
Applications will be screened and only the most qualified applicants will be selected to move forward in the selection process. Applicants must meet the Minimum Qualifications stated in the Classification Specification(s).
How To Apply
Complete Application Packages (including your Examination/Employment Application (STD 678) and applicable or required documents) must be submitted to apply for this Job Posting. Application Packages may be submitted electronically through your CalCareer Account at www.CalCareers.ca.gov. When submitting your application in hard copy, a completed copy of the Application Package listing must be included. If you choose to not apply electronically, a hard copy application package may be submitted through an alternative method listed below:
Address for Mailing Application Packages
You may submit your application and any applicable or required documents to:
Department of Transportation
Attn: Caltrans DHR Contact
Certification Services MS-90
P O Box 168036
Sacramento , CA 95816-8036
Address for Drop-Off Application Packages
You may drop off your application and any applicable or required documents at:
Department of Transportation
Caltrans DHR Contact
Classification and Hiring Unit - ECOS
1727 30th Street, MS 90
Sacramento , CA 95816
Closed on weekends and State Holidays
08:00 AM - 05:00 PM
Required Application Package Documents
The following items are required to be submitted with your application. Applicants who do not submit the required items timely may not be considered for this job:
Current version of the State Examination/Employment Application STD Form 678 (when not applying electronically), or the Electronic State Employment Application through your Applicant Account at www.CalCareers.ca.gov. All Experience and Education relating to the Minimum Qualifications listed on the Classification Specification should be included to demonstrate how you meet the Minimum Qualifications for the position.
Resume is optional. It may be included, but is not required.
Statement of Qualifications -
Statement of Qualifications (SOQ) is required. Please see the Special Requirements section for SOQ instructions.
Applicants requiring reasonable accommodations for the hiring interview process must request the necessary accommodations if scheduled for a hiring interview. The request should be made at the time of contact to schedule the interview. Questions regarding reasonable accommodations may be directed to the EEO contact listed on this job posting.
Show more
Show less","Data modeling, Database implementation, Programming languages, Structured query language, Python, Microsoft Excel, Microsoft Access, Power BI, Tableau, Data visualization tools, Disadvantaged Business Enterprise (DBE), Title VI, Americans with Disabilities Act (ADA)","data modeling, database implementation, programming languages, structured query language, python, microsoft excel, microsoft access, power bi, tableau, data visualization tools, disadvantaged business enterprisnt, data retention, data science teams, data stack, database operations, datamodeling, dbt, development environments, devops engineering, documentation, english communication, etl, etl workflow implementation, feature rollout, integration testing, interfaces, mentoring, message brokers, mongodb, nosql, nosql data stores, olap schema design, oltp schema design, performance optimization, problemsolving, project management, protocols, query optimization, requirements definition, s3, saas web applications, scalable services, snowflake, sns, software engineering, sql, sql development, sql query execution, sqs, ssis, test suites, transactional schema design, troubleshooting, unit testing, web apis",
"Principal Engineer, Data Intelligence & Analytics",Bristol Myers Squibb,"Devens, MA",https://www.linkedin.com/jobs/view/principal-engineer-data-intelligence-analytics-at-bristol-myers-squibb-3715649602,2023-12-17,Massachusetts,United States,Mid senior,Hybrid,"Working with Us
Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams rich in diversity. Take your career farther than you thought possible.
Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us.
Position Summary
The Data Intelligence and Analytics (DI&A) team for Global Biologics provides data engineering and analytics solutions for biologics manufacturing sites at Bristol Myers Squibb. DI&A manages a wide variety of data platforms and works with stakeholders throughout manufacturing to develop analytics assets that aim to deliver more medicines to more patients faster.
The Principal Engineer, DI&A will report to the Director, DI&A and take a lead role in the implementation and management of solutions for significantly complex, un-scoped analytics problems. The position requires an individual who is passionate about the intersection of data intelligence and manufacturing with a desire to push the boundaries of possibility.
Key Responsibilities
Manage cross-functional project teams to scope and prioritize analytics opportunities that originate from manufacturing, partner groups, senior leadership, and DI&A itself.
Hands on development and implementation of analytics tools based on complex user requirements.
Conduct design and code reviews for peers and junior engineers to ensure code is secure, scalable, sustainable and meets/exceeds best practices. Support the growth and success of teammates.
Enhance existing production tools/designs through incremental expansion/simplification. Lead troubleshooting efforts and work to resolve root cause issues related to reliability and performance.
Work independently to overcome challenges related to people, processes, and systems turning these situations into opportunities for everyone involved.
Articulate and present complex information clearly and concisely across all levels. Demonstrate in-depth knowledge and expertise to both build and maintain positive stakeholder relationships.
Align work for project teams with Bristol Myers Squibb business and technology priorities to realize meaningful impact. Drive innovation efforts in the analytics space related to AI and novel platforms.
Lead administration activities for the site Data Warehouse. Flexibility for on-call support as required.
Qualifications & Experience
Bachelor’s degree in computer science or engineering with 6 years of analytics experience. Master’s degree in computer science, engineering or MBA preferred.
At least two years of experience within a manufacturing environment, preferably within the pharmaceutical industry.
Ability to lead and manage projects efficiently and effectively with minimal direction.
Exceptional communication and problem-solving competency with both local and remote teams.
Familiar with common data sources found in manufacturing such as ERP, MES, LIMS, LES, LMS, and process data historians. Aptitude for quickly learning platform database structures and schemas.
Strong competency with Python and key libraries such as Pandas, NumPy, SciPy, TensorFlow, etc. Experience with cloud data science and MLOps platforms such as Domino.
Strong competency with relational databases and query languages such as SQL. Proficient with RDBMS tools such as Microsoft SQL Server Management Studio.
Strong competency with front-end visualization platforms such as Tableau, Spotfire or PowerBI.
Hands on experience utilizing supervised/unsupervised and reinforcement AI algorithms including Decision Trees, Linear Regression, k-NN, and Neural Networks such as CNNs and LLMs.
A desire to transform patient’s lives through science.
If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.
Uniquely Interesting Work, Life-changing Careers
With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in an inclusive culture, promoting diversity in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.
On-site Protocol
Physical presence at the BMS worksite or physical presence in the field is a necessary job function of this role, which the Company deems critical to collaboration, innovation, productivity, employee well-being and engagement, and it enhances the Company culture.
BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.
BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.
BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.
Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.
Show more
Show less","Python, Pandas, NumPy, SciPy, TensorFlow, SQL, Microsoft SQL Server Management Studio, Tableau, Spotfire, PowerBI, Decision Trees, Linear Regression, kNN, Neural Networks, CNNs, LLMs, ERP, MES, LIMS, LES, LMS, Domino","python, pandas, numpy, scipy, tensorflow, sql, microsoft sql server management studio, tableau, spotfire, powerbi, decision trees, linear regression, knn, neural networks, cnns, llms, erp, mes, lims, les, lms, domino","cnns, decision trees, domino, erp, knn, les, lims, linear regression, llms, lms, mes, microsoft sql server management studio, neural networks, numpy, pandas, powerbi, python, scipy, spotfire, sql, tableau, tensorflow"
"Data Engineer, Data Platform",Grammarly,"Massachusetts, United States",https://www.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3689966173,2023-12-17,Massachusetts,United States,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Data Engineering, AWS, Python, Scala, Java, MySQL, NoSQL, SQL, Hadoop, Spark, Kafka, Flink, Data Lakes, APIs, Microservices, System Design, Internal Tools, Open Source, ThirdParty Services, Agile, Scrum, Kanban, Jira, Confluence, Slack, GitHub, GitLab, Bitbucket, Jenkins, Docker, Kubernetes","data engineering, aws, python, scala, java, mysql, nosql, sql, hadoop, spark, kafka, flink, data lakes, apis, microservices, system design, internal tools, open source, thirdparty services, agile, scrum, kanban, jira, confluence, slack, github, gitlab, bitbucket, jenkins, docker, kubernetes","agile, apis, aws, bitbucket, confluence, data engineering, data lakes, docker, flink, github, gitlab, hadoop, internal tools, java, jenkins, jira, kafka, kanban, kubernetes, microservices, mysql, nosql, open source, python, scala, scrum, slack, spark, sql, system design, thirdparty services"
Data Platform Engineer,Harnham,"Boston, MA",https://www.linkedin.com/jobs/view/data-platform-engineer-at-harnham-3750652091,2023-12-17,Massachusetts,United States,Mid senior,Hybrid,"Data Platform Engineer
Hybrid- San Francisco, CA and Seattle, WA, Boston, MA
125,000-145,000K with bonus
This role cannot sponsor at this time.
The Company
I am working with a huge player in the biotech and pharma space who is looking for an experienced Data Platform Engineer to join their team.
THE ROLE
Responsibilities Will Include
Automating various data flows.
Support end-to-end code traceability and data provenance.
Partner with tech to modify tools as needed.
Build and manage reusable components and architecture s designed to make it both fast and easy to build robust, scalable, production-grade data products and services.
Your Skills Include
3+ years of relevant experience.
Experience with distributed data tools ( Spark, Kafka, Hive)
Experience with cloud platforms.
Experience with specialized data architecture.
Demonstrated excellence in writing production Python, Java, Scala, Go, and/or C#/C++
Experience building and designing a DevOps first way of working.
The Benefits
competitive salary plus benefits.
How To Apply
Please register your interest by sending your CV to Kyle Margolies via the Apply link on this page.
Show more
Show less","Data Platform Engineer, Spark, Kafka, Hive, Cloud platforms, Data architecture, Python, Java, Scala, Go, C#/C++, DevOps","data platform engineer, spark, kafka, hive, cloud platforms, data architecture, python, java, scala, go, cc, devops","cc, cloud platforms, data architecture, data platform engineer, devops, go, hive, java, kafka, python, scala, spark"
Data Engineer,Mastermind.com,"Arizona, United States",https://www.linkedin.com/jobs/view/data-engineer-at-mastermind-com-3720396925,2023-12-17,Canyon,United States,Associate,Onsite,"💰 JOIN AN AMAZING COMPANY AND GLOBALLY RECOGNIZED BRAND
🔥 DIVE INTO INTRIGUING PROJECTS AND LEVEL UP YOUR SKILLS
🌎 WORK WITH A FUN TEAM ONSITE IN SCOTTSDALE ARIZONA
Are you ready to embark on an
exciting challenge in the field of data engineering
?
Do you possess a genuine passion for unraveling patterns and
crafting insightful solutions
through data engineering?
Can you envision yourself
delivering impactful outcomes
that align with our company's objectives through data-driven solutions?
If so, we have an opportunity that will ignite your enthusiasm for data engineering and propel your career to new heights.
Mastermind.com, created in partnership by Dean Graziosi and Tony Robbins, is the #1 online platform for people who are looking to market and monetize their knowledge base. Mastermind has a worldwide following and touches lives all over the world.
We are seeking a world-class
Data Engineer
to join our IT team. Here is your chance to help take our company to new heights.
THE ROLE
🛠️ Build tools and solutions that automate complex workflows
🏗️ Create data pipelines and maintain the infrastructure and architecture for data generation, storage, and processing
🔢 Build systems that collect, manage, and convert raw data into usable information
🤝 This is a role that involves solving complex technical issues and working collaboratively with our team. You will report directly to the
Chief Technology Officer
.
💻 Our interview process will include a
technical assessment
and peer code review to assess your technical proficiency.
🏡
This Data Engineer position is based in Phoenix, Arizona, and will require you to work from the Mastermind headquarters in Scottsdale, AZ.
💸 We provide an excellent compensation model based on experience ranging from
$100k-120k.
🗽This opportunity is
only available for USA residents
with valid work authorization.
We DO NOT offer sponsorship or relocation.
REQUIREMENTS
You must have 5 years experience in designing and maintaining MySQL
You must have experience with cloud data warehouses (preferably BigQuery), dbt, or Python experience
You must have SQL development experience
You must have experience with spreadsheet manipulation in SQL
You must be able to write custom queries to answer any question about the data without leaving a SQL environment.
You must have experience querying and manipulating large datasets
Bachelor’s degree in a technical field or equivalent related work experience
Ability to utilize Fivetran and Stitch for extracting and loading data into BigQuery
Strong understanding of database design
Experience with Web APIs and pulling data from them, preferably in Python, is a plus
Excellent problem-solving and communication skills
Experience querying and manipulating large datasets
Experience with data parsing, scripting, and automation
RESPONSIBILITIES
Design and create database objects, such as tables, stored procedures, and views.
Conduct technical research and data profiling on various data sources, utilizing technologies like Python, APIs, and SQL.
Innovatively propose technical data solutions to address business challenges.
Perform dimensional data modeling and optimize database objects for accessibility, performance, and consistency.
Collaborate with business stakeholders to gather and understand data requirements.
Communicate data concepts, reports, KPIs, and other technical subjects in a business-friendly language.
Develop ETL applications using SQL, Python, etc., for data extraction, transformation, and loading.
Document development standards, KPI calculations, business terms, table diagrams, and other relevant information related to data and reports.
Keep up-to-date with emerging technologies and trends in data engineering
Perform other duties as assigned.
PERKS & BENEFITS
💰 Competitive salary and compensation
🏥 Excellent Medical benefits
🎉 EOY Profit Sharing
💼 401(k) administration and matching program
🌱 Incredible opportunities for growth and development
😄 Amazing in-office culture
👥 Become part of a mission-team making a difference!
HOW TO APPLY
Ready to dive into the fascinating realm of data engineering and take your skills to new heights as a
Data Engineer
?
If so, we invite you to join our exceptional team, where you'll have the opportunity to unleash the power of data, shape the future of data-driven decision-making, and embark on a fulfilling career journey.
Click the
""Apply Now
"" button below to take the first step toward an exciting future.
We can't wait to review your application and explore the endless possibilities of working together.
About Mastermind.com
Mastermind.com, created in partnership by Dean Graziosi and Tony Robbins, is the #1 online platform for people who are looking to market and monetize their knowledge base. We are redefining what ""Self-Education"" means to the world.
Mastermind is not just ""another software"" but an all-in-one platform for Education, Entertainment, Implementation & Community. Mastermind serves people worldwide who seek transformation, fulfillment, and success outside the traditional education path.
The Mastermind software empowers and enables you to implement what you learn & actually get paid, in addition providing a community where you are surrounded by like-minded individuals cheering you on to YOUR NEXT LEVEL.
Show more
Show less","Data Engineering, MySQL, BigQuery, dbt, Python, SQL, Data Warehousing, Data Pipelines, ETL, Data Modeling, Data Profiling, Web APIs, Dimensional Data Modeling, KPI","data engineering, mysql, bigquery, dbt, python, sql, data warehousing, data pipelines, etl, data modeling, data profiling, web apis, dimensional data modeling, kpi","bigquery, data engineering, data profiling, datamodeling, datapipeline, datawarehouse, dbt, dimensional data modeling, etl, kpi, mysql, python, sql, web apis"
Project Coordinator/Data Analyst,Prosum,"Arizona, United States",https://www.linkedin.com/jobs/view/project-coordinator-data-analyst-at-prosum-3768721137,2023-12-17,Canyon,United States,Associate,Remote,"3RD PARTIES PLEASE DO NOT APPLY
This opportunity will play a critical role in supporting the PMO organization. It will be responsible for planning, reporting, and ad hoc analysis.
Qualifications:
3+ years of experience in technology project coordination/management.
2+ years of experience utilizing Power BI or Tableau for buildings dashboards and reports.
Experience with current and future state projects, including capacity modeling / scheduling, budgets tracking, assignment of work and review of individual project efforts.
Agile/Scrum
Show more
Show less","Project Coordination, Project Management, Power BI, Tableau, Agile, Scrum, Capacity Modeling, Scheduling, Budget Tracking, Work Assignment, Project Effort Review","project coordination, project management, power bi, tableau, agile, scrum, capacity modeling, scheduling, budget tracking, work assignment, project effort review","agile, budget tracking, capacity modeling, powerbi, project coordination, project effort review, project management, scheduling, scrum, tableau, work assignment"
Senior Salesforce Data Analyst - Data Cloud,Stott and May,"Arizona, United States",https://www.linkedin.com/jobs/view/senior-salesforce-data-analyst-data-cloud-at-stott-and-may-3778897526,2023-12-17,Canyon,United States,Mid senior,Remote,"We are hiring a Senior Salesforce Data Analyst for one of our clients -
This is a full-time, remote role with occasional travel (quarterly), with annual compensation in the range of $70,000-85,000
The role is internally facing and will be responsible for data monitoring, validation, governance and enrichment within Salesforce, enhancing and driving efficiencies for Sales teams and with 3rd party integrations.
Experience Needed:
Bachelor’s Degree
4+ years' experience with data analysis and data governance management
3+ years' experience with Salesforce data management experience
Tableau CRM/Einstein analytics experience
Salesforce Data Cloud experience
AWS data management
Show more
Show less","Salesforce, Data Analysis, Data Governance, Data Monitoring, Data Validation, Data Enrichment, Data Integration, Tableau CRM, Einstein Analytics, Salesforce Data Cloud, AWS Data Management","salesforce, data analysis, data governance, data monitoring, data validation, data enrichment, data integration, tableau crm, einstein analytics, salesforce data cloud, aws data management","aws data management, data enrichment, data governance, data integration, data monitoring, data validation, dataanalytics, einstein analytics, salesforce, salesforce data cloud, tableau crm"
Data Analyst,ZoomInfo,"Vancouver, WA",https://www.linkedin.com/jobs/view/data-analyst-at-zoominfo-3785807752,2023-12-17,Chickasaw,United States,Associate,Hybrid,"At ZoomInfo, we encourage creativity, value innovation, demand teamwork, expect accountability and cherish results. We value your take charge, take initiative, get stuff done attitude and will help you unlock your growth potential. One great choice can change everything. Thrive with us at ZoomInfo.
Are you looking for an opportunity to challenge yourself?
ZoomInfo is looking for a Data Analyst to join our Data Innovation team and improve our data. This position supports our broader Data team and works closely with Data Science, Engineering, Product, and Research. Key aspects of this role include conducting analysis to support the creation of new products, identifying opportunities for product improvement, and working with internal teams to bolster the ZI database.
You’ll be given opportunities to personally and professionally develop as you build your career. We all share in the sense of accomplishment we get from being a part of what we’re building; you will have the opportunity to influence the future success of ZoomInfo’s data assets, developing your technical and analytical skills along the way. You’ll see that collaboration is second nature here, you’ll be greeted by a team of brilliant, talented, and motivated individuals who will help you define your new best, and most of all, you'll have fun!
You are…
Inquisitive - You are curious about ZoomInfo’s product, market and data operations, and eager to develop a thorough understanding of our data and infrastructure. You pursue technical training and development opportunities and strive to build knowledge and skills continuously.
A Critical Thinker - You can exercise good judgment in ambiguous situations. You have strong problem-solving and troubleshooting skills and enjoy solving puzzles. Most importantly, you can apply your analysis to the big-picture problems and opportunities.
A Team Player - You are willing to tackle new challenges and enjoy facilitating and owning cross-functional collaboration.
Responsibilities
Develop a deep and comprehensive understanding of the Person/Contact data and infrastructure we work with.
Diagnose issues in data quality, identify gaps and errors in logic, and evaluate potential data partners.
Summarize and document different aspects of the current state of our data, probing for opportunities, and charting the path forward with solutions to improve the accuracy and volume of the data we serve to our customers.
Help drive implementation to maintain & improve our data and pipeline by collaborating with our Software & Data Engineering, Product Management, and Research counterparts.
Qualifications
Bachelor's Degree in an analytical/research field (i.e. Math, Physics, Engineering, Computer Science, Biology, etc.) and 2+ years of experience working in a data-related capacity, or an equivalent combination of education and experience. An advanced degree is a plus.
Analytical and problem-solving skills - you are interested in making sense of large quantities of data and can see patterns quickly
Innovative – you aren’t afraid to question the status quo with new and inventive solutions to difficult problems
Strong attention to detail
Eager to learn
Strong Interpersonal/Teamwork Skills. This is a must.
An ability to work independently and manage multiple projects at once
Comfortability with statistics
Very comfortable writing SQL queries
Experience with Microsoft Excel (formulas, pivot tables), or data analysis tools
Preferred: Experience with Python (or R), or other coding experience. It is expected any individual who comes in without Python experience directly is willing to learn
The US base salary range for this position is $76,000.00 - $95,000.00 + bonus + benefits.
Actual compensation offered will be based on factors such as the candidate’s work location, qualifications, skills, experience and/or training. Your recruiter can share more information about the specific salary range for your desired work location during the hiring process.
We want our employees and their families to thrive. In addition to comprehensive benefits we offer holistic mind, body and lifestyle programs designed for overall well-being. Learn more about ZoomInfo benefits here.
About Us
ZoomInfo (NASDAQ: ZI) is the trusted go-to-market platform for businesses to find, acquire, and grow their customers. It delivers accurate, real-time data, insights, and technology to more than 35,000 companies worldwide. Businesses use ZoomInfo to increase efficiency, consolidate technology stacks, and align their sales and marketing teams — all in one platform.
ZoomInfo may use a software-based assessment as part of the recruitment process. More information about this tool, including the results of the most recent bias audit, is available here.
ZoomInfo is proud to be an Equal Opportunity employer. We are committed to equal employment opportunities for applicants and employees regardless of sex, race, age, color, national origin, sexual orientation, gender identity, marital status, disability status, religion, protected military or veteran status, medical condition, or any other characteristic or status protected by applicable law. At ZoomInfo, we also consider qualified candidates with criminal histories, consistent with legal requirements.
Show more
Show less","Python, R, SQL, Microsoft Excel, Data Analysis Tools","python, r, sql, microsoft excel, data analysis tools","data analysis tools, microsoft excel, python, r, sql"
Jr. Data Analyst,"Liquid Advertising, Inc.","Ann Arbor, MI",https://www.linkedin.com/jobs/view/jr-data-analyst-at-liquid-advertising-inc-3777094905,2023-12-17,Chickasaw,United States,Associate,Hybrid,"First, a little about us:
Born in 2000 as an ad agency for the video game, entertainment, and technology communities
80 teammates strong across the US, LATAM, and European markets
Our employee turnover has historically been ~8% annually
Ad agency partner for Bethesda Softworks, Square Enix, CD Projekt Red, Riot Forge, Capcom, Sega, and other top gaming and entertainment partners. See our work at www.liquidadvertising.com
This can be a hybrid or remote position based in Michigan
What's this position about?
We are ALWAYS curious about what data are telling us.
Every day we run dozens of digital advertising campaigns, spending millions of dollars, on behalf of our clients—some of the most successful videogame publishers in the industry. Our analytics team helps ensure that we're investing those advertising dollars in the most effective and efficient way possible. Our analytics projects and reporting vary widely in complexity, typically exploring datasets in the millions of rows.
This junior position on our analytics team will show you how to create rich data stories about advertising campaigns—and the video game fans behind them. As the successful candidate, you will use your Python, SQL, and other scripting skills in wrangling data, then visualizing in Tableau to help your teammates and our clients understand the stories in the data.
Requirements
What do we look for?
Ideally, you will be a four-year graduate of a research-heavy academic discipline with some professional analytics experience. Your technical skills will include—
Strength with Python analytics routines
Experience with SQL and other data-focused scripting
Excel formulas and macros with an emphasis on analytical functions
Developing visualizations and narratives, preferably with Tableau dashboard experience
Driving projects from data ingestion to presentation (tell us about this in your application)
Coding experience is not required but highly preferred
Benefits
The perks we offer
Base salary for this role is $50,000-$59,000 per year. Actual salary offered will be based on experience, skillset, and location.
Our people also enjoy remote work options, a generous annual bonus plan, fully paid premiums for comprehensive health insurance, generous paid time off plans, and 100% match on 401k savings.
Our stance
Liquid Advertising is committed to creating an anti-racist, anti-sexist environment. We're building an ad agency where committed and creative people from all backgrounds can do their best work.
No agencies, please. This is a pretty good representation of this position's responsibilities but is not a comprehensive job description. Duties, clients, and team assignments may change as assigned. We regret we cannot consider applicants outside of the United States or those requiring visa sponsorship at this time.
Show more
Show less","Python, SQL, Tableau, Data Visualization, Data Analysis, Analytics, Microsoft Excel, Data Wrangling, Data Storytelling, Data Ingestion, Presentation, DataFocused Scripting, Coding, 401k Savings, Remote Work, Annual Bonus Plan, Comprehensive Health Insurance, Generous Paid Time Off","python, sql, tableau, data visualization, data analysis, analytics, microsoft excel, data wrangling, data storytelling, data ingestion, presentation, datafocused scripting, coding, 401k savings, remote work, annual bonus plan, comprehensive health insurance, generous paid time off","401k savings, analytics, annual bonus plan, coding, comprehensive health insurance, data ingestion, data storytelling, data wrangling, dataanalytics, datafocused scripting, generous paid time off, microsoft excel, presentation, python, remote work, sql, tableau, visualization"
"Software Data Engineer, Java",MassMutual,"Boston, MA",https://www.linkedin.com/jobs/view/software-data-engineer-java-at-massmutual-3645223387,2023-12-17,Chickasaw,United States,Associate,Hybrid,"Job Description
Since 1851, MassMutual’s commitment has always been to help people protect their families, support their communities, and help one another. This is why we want to inspire people to Live Mutual. We are people helping people.
A career with us means you will work alongside exceptional people and be empowered to reach your professional and personal goals. Our employees are the foundation of what makes MassMutual a strong, stable and ethical business. We seek and value unique and varied perspectives and experiences because we believe we are stronger when all voices are heard. We invite you to bring your bright, innovative ideas to MassMutual as we continue to help millions of Americans rely on each other.
Together, we are stronger
What great looks like in this role
Our ideal candidate is someone who enjoys designing, building and delivering complex systems. You love coding and revel in finding elegant solutions to difficult problems. You understand the challenges of handling data at scale and are always looking for opportunities to leverage open-source tools to accelerate development. You are passionate about learning new technologies, are team orientated and a strong communicator.
Objectives of the role
Design, develop and deliver scalable, robust and highly re-usable components using technologies such as Python, Java, AWS serverless (Lambda, Glue), Apache Spark, Apache Kafka and REST
Participate in all aspects of development from design to delivery, acting as both developer and component lead
Interact closely with data users, including data engineers and data scientists to understand & refine requirements
Develop code, unit tests and conducts code reviews
Debug and troubleshoots problems in code and data pipelines
Evaluate and recommend tools, technologies, processes and reference architectures
Identify areas for process improvement, automation and simplification (e.g. use of existing open source technologies)
Collaborate closely with other developers and provide mentorship as appropriate
Collaborate with other peer organizations (e.g., Business Analyst, Data Modeler, QA, technical support, etc.) to prevent and resolve technical issues
Work in Agile development environment, attending daily stand-up meetings and delivering incremental improvements.
Basic Qualifications
Java: 4+ years development
Strong foundation in algorithms, design patterns and how to write performant code
Data: Good understanding of data & data processing tools (e.g. Spark, Kafka, SQL), of relational database technologies and of analytics databases (e.g. Redshift, Vertica, Snowflake)
CI/CD: Experienced with source control and with using CI/CD tools
Testing: Proficient in writing unit, integration and load tests
Communication: Excellent communication, problem solving, organizational and analytical skills
Able to work independently and also to provide leadership to small teams of developers
Bachelor’s degree or equivalent work experience
Preferred Qualifications
Cloud: Experience building with and deploying to cloud platforms such as AWS and leveraging serverless architectures (e.g. Lambda, Glue)
Big Data & Streaming: 2+ years using big data and/or streaming technologies (e.g. Apache Spark, Apache Kafka, Apache Flink)
MassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply. Note: Veterans are welcome to apply, regardless of their discharge status.
If you need an accommodation to complete the application process, please contact us and share the specifics of the assistance you need.
Show more
Show less","Python, Java, AWS, Lambda, Glue, Apache Spark, Apache Kafka, Apache Flink, REST, SQL, Redshift, Vertica, Snowflake, CI/CD, Testing, Agile, Algorithms, Design patterns, Unit testing, Integration testing, Load testing, Analytics, Cloud, Big Data, Streaming","python, java, aws, lambda, glue, apache spark, apache kafka, apache flink, rest, sql, redshift, vertica, snowflake, cicd, testing, agile, algorithms, design patterns, unit testing, integration testing, load testing, analytics, cloud, big data, streaming","agile, algorithms, analytics, apache flink, apache kafka, apache spark, aws, big data, cicd, cloud, design patterns, glue, integration testing, java, lambda, load testing, python, redshift, rest, snowflake, sql, streaming, testing, unit testing, vertica"
"Lead Software Engineer, Full Stack (Enterprise Data)",Jobs for Humanity,"Harrisonburg, VA",https://www.linkedin.com/jobs/view/lead-software-engineer-full-stack-enterprise-data-at-jobs-for-humanity-3768296399,2023-12-17,Plains,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with Capital One to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Capital One
Job Description
Lead Software Engineer, Full Stack (Enterprise Data) Are you passionate about technology and enjoy solving complex business problems in a collaborative and inclusive environment? At Capital One, we have a diverse group of makers, breakers, and doers who solve real problems and meet real customer needs. We are looking for Full Stack Software Engineers who are excited about combining data with emerging technologies. As a Lead Software Engineer, you will play a key role in driving a major transformation at Capital One. You will join our Enterprise Data team as a Lead Software Engineer, where you will provide technical leadership for data management applications that support crucial regulatory processes. What You'll Do: - Lead a portfolio of diverse technology projects and a team of developers with deep experience in distributed microservices and full stack systems. - Stay updated on the latest tech trends, experiment with new technologies, and actively participate in technology communities. Mentor other members of the engineering community. - Collaborate with digital product managers to deliver robust cloud-based solutions that empower millions of Americans financially. - Use programming languages like Python and Go, open source databases, container orchestration services, and various AWS tools and services. - Lead the technical design and implementation for the team, leveraging technologies such as Python, Apache Airflow, Apache Spark, PostgreSQL, and more. - Administer multiple datasets, ensuring compliance with best practices and regulatory requirements. - Mentor team members and provide guidance on technical decisions and issue resolution. - Assist in verifying the accuracy and completeness of third-party data and evaluate new data sources. Basic Qualifications: - Bachelor's Degree. - At least 6 years of software engineering experience. - At least 1 year of experience with cloud computing (AWS, Microsoft Azure, Google Cloud). Preferred Qualifications: - Master's Degree. - 6+ years of experience with Python. - 3+ years of experience with AWS. - 2+ years of experience with Airflow. - 2+ years of experience with Spark. - 2+ years of experience with Presto or Snowflake. - 4+ years of experience in open-source frameworks. - 1+ years of people management experience. - 2+ years of experience in Agile practices. At Capital One, we provide a comprehensive and inclusive set of health, financial, and other benefits that support your total well-being. To learn more, visit our website. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. We value all applicants regardless of their age, sex, race, color, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable laws. We promote a drug-free workplace. If you need an accommodation during the application process, please contact Capital One Recruiting. All information provided will be kept confidential and used only to provide necessary accommodations. For technical support or questions about the recruiting process, please email Careers@capitalone.com. Note: This job posting is specific to the United States. Salary ranges vary by location. No agencies, please. Capital One Financial Corporation is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, positions in the United Kingdom are for Capital One Europe, and positions in the Philippines are for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Python, Go, Apache Airflow, Apache Spark, PostgreSQL, AWS, Cloud Computing, Microservices, Open Source Frameworks, Agile Practices","python, go, apache airflow, apache spark, postgresql, aws, cloud computing, microservices, open source frameworks, agile practices","agile practices, apache airflow, apache spark, aws, cloud computing, go, microservices, open source frameworks, postgresql, python"
Data Analyst,Avani Tech Solutions Private Limited,"Minneapolis, MN",https://www.linkedin.com/jobs/view/data-analyst-at-avani-tech-solutions-private-limited-3774016127,2023-12-17,Lake Forest,United States,Associate,Onsite,"Success Factor knowledge
Schedule : Monday through Friday 8:00-5:00 PM: Hybrid schedule as needed but mainly remote.
Pay : 36/hr
Requirements
The Data Management Generalist 2 will assist to implement HR Data Retention Controls across several HR applications to meet requirements from California Privacy Rights Act and Internal Retention Information Management (RIM) policies. The HR Data Retention Consultant will be working as part of the People Data Governance team (HR) and collaborate with application owners as well as RIM COE, IT and Data Governance team to:
identifies Retention Controls gaps for each of the application.
Assess and define best system solution.
understand and analyze RIM data categorization to be applied to the different data sets involved.
Implement system solution.
Show more
Show less","Success Factor, California Privacy Rights Act, Retention Information Management (RIM), People Data Governance, RIM COE, IT, Data Governance","success factor, california privacy rights act, retention information management rim, people data governance, rim coe, it, data governance","california privacy rights act, data governance, it, people data governance, retention information management rim, rim coe, success factor"
Police Technical Data Analyst,Metropolitan Council of the Twin Cities,"Minneapolis, MN",https://www.linkedin.com/jobs/view/police-technical-data-analyst-at-metropolitan-council-of-the-twin-cities-3781760038,2023-12-17,Lake Forest,United States,Associate,Onsite,"The
Metro Transit Police Department
is one of the fastest growing law enforcement agencies in the state due to the expansion of light rail and Rapid Transit Bus systems. We are a licensed police force committed to the safety of our customers and employees, serving eight counties and approximately 85 cities in the region. There is an authorized strength of 171 full-time officers, 80 part-time officers, 70 community service officers and 34 administrative staff dedicated to one thing: public safety on and near our transit system. Our mission statement is: Safeguarding the transit community with integrity and professionalism while building trust through community partnerships. We strive to provide excellent police service each and every day.
We are committed to maintaining and cultivating a diverse department, reflective of the communities we serve. Nearly half of our force is made up of women and people of color.
How your work would contribute to our organization and the Twin Cities region:
The
Police Technical Data Analyst
utilizes technical skills and critical thinking to construct queries and scripts for the extraction of data in response to crime analysis, data requests, and building dashboards for ongoing reporting. Utilizes multiple programs including Microsoft SQL Server Management Studio and Microsoft Report Builder to create reports that are accurate and understandable. This position works in coordination with the Police Data Analyst creating reports, charts, maps, statistics, presentations, and intelligence bulletins to communicate real time crime trends, hotspots, modes of operation and patterns, as well as communicate information for wanted persons and officer safety.
This posting will establish a six (6) month eligibility list for current and future positions.
Anticipated hiring salary range: $78,582 - $89,190
We offer a competitive salary, excellent benefits and a good work/life balance.
What you would do in this job:
Utilize SQL Programming to build complex scripts and join tables in relational databases.
Compile and configure data and statistics to develop real time crime trends and patterns utilizing various law enforcement databases.
Utilize crime analytics programs to further extract and organize data.
Create and maintain dashboards for reports and queries.
Assist in preparation of weekly analysis of crime statistics (aka CompStat) for the Police Chief and Command Staff.
Prepare quarterly reports for the General Manager that include crime statistics and trends.
Communicate regularly with MTPD Staff to understand the needs of each division and respond by researching and presenting crucial data.
Utilize statistics, investigative alerts, law enforcement databases and photos to prepare intelligence bulletins to enhance data sharing and officer safety.
Gather data from other law enforcement agencies to study and analyze past and existing crime series, patterns, and trends directly impacting MTPD.
Respond to requests for crime data to assist others with information; assist with data requests from Met Council and the Public.
Performs other duties as assigned.
Minimum Qualifications:
Education / Experience Requirements:
Associate degree in Criminal Justice, Information Technology, Mathematics/Statistics, Public Administration or closely related, AND six (6) years of experience to include a combination of information technology, technical skills in data extraction, analysis of law enforcement and/or criminal data or systems and customer service.
OR
Bachelor’s degree in Criminal Justice, Information Technology, Mathematics/Statistics, Public Administration or closely related, AND four (4) years of experience to include a combination of information technology, technical skills in data extraction, analysis of law enforcement and/or criminal data or systems, and customer service.
OR
Master’s degree in Criminal Justice, Information Technology, Mathematics/Statistics, Public Administration or closely related, AND two (2) years of experience to include a combination of information technology, technical skills in data extraction, analysis of law enforcement and/or criminal data or systems and customer service.
COMPLETE JOB POSTING AND APPLY:
https://www.governmentjobs.com/careers/metrocouncil
Show more
Show less","Microsoft SQL Server, Microsoft Report Builder, SQL Programming, Crime Analytics Programs, Statistics, CompStat, Data Extraction, Law Enforcement Databases, Data Presentation, Criminal Data Analysis, Data Visualization, Data Sharing, Crime Pattern Analysis, Crime Mapping, Crime Series Analysis, Intelligence Bulletins","microsoft sql server, microsoft report builder, sql programming, crime analytics programs, statistics, compstat, data extraction, law enforcement databases, data presentation, criminal data analysis, data visualization, data sharing, crime pattern analysis, crime mapping, crime series analysis, intelligence bulletins","compstat, crime analytics programs, crime mapping, crime pattern analysis, crime series analysis, criminal data analysis, data extraction, data presentation, data sharing, intelligence bulletins, law enforcement databases, microsoft report builder, microsoft sql server, sql, statistics, visualization"
Human Capital Data Analyst,Harris,"St Paul, MN",https://www.linkedin.com/jobs/view/human-capital-data-analyst-at-harris-3756689153,2023-12-17,Lake Forest,United States,Associate,Hybrid,"The purpose of your role as a
Human Capital Data Analyst
As a Human Capital Data Analyst, you will be responsible for collecting, analyzing, and interpreting human capital-related data to provide insights, support strategic initiatives, and enhance Human Capital operations. Partnering closely with leaders, this role will directly influence Human Capital and business decisions by performing research and analysis on a broad range of initiatives.
This role can be performed in-office, fully remote, or hybrid. Ability to work in-office or hybrid from our St. Paul, MN office preferred.
Data Collection and Management
Efficiently and accurately integrate new hires into the Human Capital Management System, ensuring a smooth transition into the organization.
Collect and maintain accurate Human Capital data, including employee information, payroll, benefits, performance, and other relevant data.
Ensure data accuracy, completeness, and security, adhering to data privacy regulations.
Collaborate with Human Capital team members and business/division leaders to gather data requirements and address data-related inquiries.
Collaborate with the HC Operations Manager to design and implement Human Capital systems and processes.
Assesses, tests, and evaluates software upgrade options to determine efficiency, reliability, and compatibility with existing systems.
Develops procedures and drafts necessary guidance & training documentation for installation, use, and troubleshooting of Human Capital software.
Data Analysis and Reporting
Analyze Human Capital data to identify trends, patterns, and insights.
Create regular reports, dashboards, and ad-hoc analyses to support Human Capital decision-making and strategic planning.
Partner with Human Capital team members to develop reports for all Human Capital & compliance reporting (e.g., EEO Reporting, ACA reporting).
Write, maintain, and support a variety of queries utilizing Human Capital reporting tools. Develop standard reports for ongoing supporting needs.
Research and set up end-user management dashboards.
Proactively partner with team members to understand pain points, inefficient processes and/or obstacles to developing efficient processes. Research and recommend solutions to create capacity on the team while ensuring end-users also experience efficiencies.
Metrics and KPI Tracking
In collaboration with Human Capital team members, define and track Human Capital metrics and key performance indicators (KPIs) to assess the effectiveness of programs and initiatives.
Provide recommendations based on data analysis to improve processes and outcomes.
Data Visualization
Create visual representations of data to make complex information more accessible and understandable for stakeholders.
Utilize data visualization tools to enhance reporting.
Compliance and Data Security
Ensure data privacy and security compliance, following relevant regulations.
Collaborate with legal and compliance teams to implement data protection policies.
Partner with technology teams to ensure data integrity and optimize Human Capital technology.
Perform periotic data audit to ensure the integrity of data.
What We're Looking For In You
Bachelor’s degree in human resources, business, or related
1+ year of proven experience in data analysis, particularly in an HR or business context
Proficiency in data analytics and reporting tools (e.g., Excel, Power BI)
Strong computer literacy required with advanced skills in Microsoft Excel: Pivot tables, VLOOKUP, and other advanced Excel functions and formulas
Strong analytic, problem-solving, troubleshooting, and consultancy skills
Critical thinker with exceptional ability to model system workflows and processes and identify and resolve gaps
Strong aptitude for learning new software and the flexibility to adapt to diverse system environments
UKG and Smartsheet previous experience a plus
Must be authorized to work in the US without sponsorship.
Your life at Harris
As one of the country's leading mechanical contractors, Harris offers you the best of both worlds: the stability, resources and opportunities of a national company, and the team culture, creative spirit and customer loyalty of a local business. If you thrive on variety and new challenges, we want to meet you!
From stadiums to manufacturing facilities, power plants to hospitals, concert halls to classrooms, we handle projects of all sizes and complexity from multiple regional locations across the country.
Harris Benefits + Compensation
Medical, dental, vision, and life insurance
401K with company match
Vacation time, sick time, and paid holidays
Paid Parental leave
Short-Term Incentive Plan
Visit our Careers Page for additional benefit details: https://www.harriscompany.com/careers/employee-benefits-at-a-glance
Pay Range: $54,282 - $81,423 per year
The actual salary offer will vary by candidate based on a wide range of factors such as specific skills, qualifications, experience, and location.
Show more
Show less","Human Capital Data, Data Collection, Data Management, Data Analysis, Data Visualization, Data Reporting, Data Security, Metrics, KPIs, Data Analytics, Data Modeling, Problem Solving, Troubleshooting, Microsoft Excel, Power BI, UKG, Smartsheet","human capital data, data collection, data management, data analysis, data visualization, data reporting, data security, metrics, kpis, data analytics, data modeling, problem solving, troubleshooting, microsoft excel, power bi, ukg, smartsheet","data collection, data management, data reporting, data security, dataanalytics, datamodeling, human capital data, kpis, metrics, microsoft excel, powerbi, problem solving, smartsheet, troubleshooting, ukg, visualization"
Senior Snowflake Data Engineer,Gullview Technologies,"Minneapolis, MN",https://www.linkedin.com/jobs/view/senior-snowflake-data-engineer-at-gullview-technologies-3646126171,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"Sr. Snowflake Data Engineer
Gullview Technologies is an exciting and rapidly growing technology company focused on taking on the most vital and challenging business and technical challenges our clients face in our highly connected business world today. We have an exceptional focus at Gullview on developing deep and long-lasting relationships (several years and counting) with our clients. This focus enables us to deliver an ongoing continuum of projects and solutions to them with high value, meaningful impact, and predictable performance.
Position Overview
We have a great opportunity to help refine a Snowflake Data Solution for a strategic client of ours as part of their Enterprise Data journey. This is a fantastic opportunity to join our firm, and work with our client in the foundational stages of a Data Practice for Gullview and the Enterprise Data Practice for the client with significant opportunities to influence Enterprise Data and Analytics capabilities.
Initially, this role will be an individual contributor with opportunities of leadership in supporting mission critical systems for our clients.
Focus of Role
The
Sr. Snowflake Data Engineer
will lead a discovery and resolution for our client’s current Snowflake implementation. The talented individual in this role will collaborate with Gullview and Client Leadership, Architects, and Engineers to build out a Snowflake Data Solution focusing on delivering business value. This role will initially be an individual contributor with future opportunities for leadership if desired. You will help with the overall Data Strategy including infrastructure, software, utilities/tools, Public Cloud Solutions, and Business Intelligence Solutions for the organization. This is a Sr. level engineering position, not an architecture role.
Responsibilities
Provide understanding of Snowflake, Data Warehouse, Data Movement, Data Curation, and Data Staging best practices in relation to existing Snowflake implementations
Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Create and contribute to automation of essential processes related to infrastructure solution deployment creating repeatable and robust deployments
Provide Snowflake solutions, including hands on keyboard (implementation)
Overall ownership of the “platform” and supporting essential platform capabilities
Responsible for day-to-day sustainment and configuration of the platform including
System Health and Telemetry
Performance and performance tuning
Data Quality, Protection, and Availability
Responsible for developing and implementing solutions related to Snowflake and preparing data for Business Intelligence
Fully document all solution work including designs and configurations
Play an essential role in troubleshooting data system problems and provide viable solution options
Requirements
Qualifications
Minimum 10 years of experience in IT with focus on Enterprise Data Solutions
Minimum 5 of years of experience in:
Snowflake Data Solutions, hands-on
Data warehousing methodologies and modelling techniques Data migration methods of on-prem to cloud data solutions including ELT/ETL Tools and concepts
Working with Batch and Stream data
SQL, preferrable Snowflake SQL
Massively Parallel Processing (MPP) Analytical Datastores
Experience in Snowflake utilities including SnowSLQ, Snowpipe, Snowlight for handling Streaming data is a plus
Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning, Zero copy clone, time travel and understand how to use these features
Experience in in re-clustering Snowflake data with good understanding of Micro-Partition within Snowflake
Expertise in deploying Snowflake features such as data sharing, events and lake-house patterns
Experience in handling semi-structured data (JSON, XML) in Snowflake
Minimum of 2 years of hands-on experience in Cloud technologies such as
Azure - Blob Storage, Cool Blob Storage, Virtual Machine, Functions, SQL Datawarehouse
Certified Snowflake cloud data warehouse Architect (Desirable)
Should be able to troubleshoot problems across infrastructure, platform and application domains
Must have experience of Agile development methodologies
Strong written communication skills. Is effective and persuasive in both written and oral communication
Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)
Education Requirements
Bachelor’s degree in computer science, engineering, or Management Information Systems, or related IT field
Position Type
Full time employee
Show more
Show less","Snowflake, Data Warehouse, Data Movement, Data Curation, Data Staging, Data Pipeline, Data Transformation, Data Structures, Metadata, Dependency Management, Workload Management, Automation, Infrastructure Solution Deployment, System Health, Telemetry, Performance Tuning, Data Quality, Data Protection, Data Availability, Business Intelligence, Data System Troubleshooting, Enterprise Data Solutions, Data Warehousing Methodologies, Data Modelling Techniques, Data Migration, ELT/ETL Tools, Batch Data, Stream Data, SQL, Snowflake SQL, Massively Parallel Processing, Analytical Datastores, SnowSLQ, Snowpipe, Snowlight, Resource Monitors, RBAC Controls, Virtual Warehouse Sizing, Query Performance Tuning, Zero Copy Clone, Time Travel, MicroPartition, Data Sharing, Events, LakeHouse Patterns, SemiStructured Data, JSON, XML, Cloud Technologies, Azure, Blob Storage, Cool Blob Storage, Virtual Machine, Functions, SQL Datawarehouse, Agile Development Methodologies, Written Communication Skills, Oral Communication Skills, Relational Data Stores, NoSQL Data Stores, Star Schema, Snowflake Schema, Dimensional Modelling, Computer Science, Engineering, Management Information Systems","snowflake, data warehouse, data movement, data curation, data staging, data pipeline, data transformation, data structures, metadata, dependency management, workload management, automation, infrastructure solution deployment, system health, telemetry, performance tuning, data quality, data protection, data availability, business intelligence, data system troubleshooting, enterprise data solutions, data warehousing methodologies, data modelling techniques, data migration, eltetl tools, batch data, stream data, sql, snowflake sql, massively parallel processing, analytical datastores, snowslq, snowpipe, snowlight, resource monitors, rbac controls, virtual warehouse sizing, query performance tuning, zero copy clone, time travel, micropartition, data sharing, events, lakehouse patterns, semistructured data, json, xml, cloud technologies, azure, blob storage, cool blob storage, virtual machine, functions, sql datawarehouse, agile development methodologies, written communication skills, oral communication skills, relational data stores, nosql data stores, star schema, snowflake schema, dimensional modelling, computer science, engineering, management information systems","agile development methodologies, analytical datastores, automation, azure, batch data, blob storage, business intelligence, cloud technologies, computer science, cool blob storage, data availability, data curation, data migration, data modelling techniques, data movement, data pipeline, data protection, data quality, data sharing, data staging, data structures, data system troubleshooting, data transformation, data warehousing methodologies, datawarehouse, dependency management, dimensional modelling, eltetl tools, engineering, enterprise data solutions, events, functions, infrastructure solution deployment, json, lakehouse patterns, management information systems, massively parallel processing, metadata, micropartition, nosql data stores, oral communication skills, performance tuning, query performance tuning, rbac controls, relational data stores, resource monitors, semistructured data, snowflake, snowflake schema, snowflake sql, snowlight, snowpipe, snowslq, sql, sql datawarehouse, star schema, stream data, system health, telemetry, time travel, virtual machine, virtual warehouse sizing, workload management, written communication skills, xml, zero copy clone"
Lead Data Engineer,Steneral Consulting,"Minneapolis, MN",https://www.linkedin.com/jobs/view/lead-data-engineer%C2%A0-at-steneral-consulting-3755527654,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"Lead Data Engineer
Minneapolis/St. Paul - MN -
hybrid position with 2/3 days onsite -
Local Candidates of Minneapolis/St. Paul only
Keep in mind that the position is open to Minneapolis/St. Paul local candidates only, and out of state candidates would not be eligible since the interview stage itself is onsite.
The lead data engineer will design solutions and work with a team of engineers to ensure accurate, consistent, reliable, and sustainable solutions are delivered. The data engineer will also influence data engineering processes that result in highly accurate and trusted data assets for use in Truterra and across the organization. Additionally, the lead data engineer influences stakeholders to ensure investments in data driven solutions are centered on optimizing clearly defined and quantified business outcomes.
Required Education And Experience
Advanced SQL, data engineering & data modeling techniques
Ability to function as a technical lead, working closely with developers and data analysts, as well as hands-on implementation
7+ years of experience building data integration solutions using tools like Informatica, Talend, Mulesoft, Qlik, etc.
Strong experience building out data warehouse and/or data lake
3+ years of experience leading engineering resources
2+ years of experience working with cloud-native data solutions on Microsoft Azure, AWS, or Google Cloud platform
Strong experience leading full lifecycle, large, complex reporting or data engineering efforts
Strong experience in working with heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using various data integration technologies (ETL/ELT, data replication/CDC, message-oriented data movement, API design, etc.)
Experience with DevOps, CI/CD pipelines and automated testing Required Competencies/skills
Implement data structures using standards and best practices in data modeling, ETL/ELT processes, SQL, database, and other technologies
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Thorough experience building and optimizing data pipelines and data sets.
Deep knowledge of the data vault method, model, and architecture
Ability to manage the overall data landscape: meta data, processing patterns, and data quality
A successful history of manipulating, processing and extracting value from large datasets.
Strong working knowledge of message queuing, stream processing, and highly scalable data stores
Ability to obtain a clear understanding of business needs and value, developing a detailed vision for the initiative, mapping out the solution, and guiding its implementation
Develop test-driven solutions that can be deployed quickly and in an automated fashion
Ability to obtain a clear understanding of business needs and value, developing a detailed vision for the initiative, mapping out the solution, and guiding its implementation
Demonstrated ability to collaborate across all levels (Engineers, Management, Architects, etc.) & across all skill sets (Data scientists, Data visualization developers, Salesforce developers etc.) particularly in a Product-oriented culture
Capable of using agile methodology and implementing Continuous Integration/Continuous Delivery (CI/CD) pipelines
Experience working with Databricks (or Spark) & Qlik technologies (Replicate and Compose)
Experience with any scripting languages, preferably Python
Experience working with BI Tools (Power BI, Tableau etc.) to create dashboards and reports
Experience with Snowflake
Experience in Manufacturing or Agriculture industry Preferred
Troubleshooting skills, ability to determine impacts, ability to resolve complex issues, and ability to exercise sound judgment and initiative in challenging situations.
Show more
Show less","SQL, Data modeling, Data engineering, Data integration, Data warehousing, Data lake, Cloudnative data solutions, Microsoft Azure, AWS, Google Cloud platform, Reporting, Data pipelines, Data structures, ETL/ELT, Databases, Data vault, Meta data, Data quality, Message queuing, Stream processing, Scalable data stores, Testdriven solutions, Agile methodology, Continuous Integration/Continuous Delivery (CI/CD), Databricks, Spark, Qlik, Python, BI Tools, Power BI, Tableau, Snowflake, Manufacturing, Agriculture","sql, data modeling, data engineering, data integration, data warehousing, data lake, cloudnative data solutions, microsoft azure, aws, google cloud platform, reporting, data pipelines, data structures, etlelt, databases, data vault, meta data, data quality, message queuing, stream processing, scalable data stores, testdriven solutions, agile methodology, continuous integrationcontinuous delivery cicd, databricks, spark, qlik, python, bi tools, power bi, tableau, snowflake, manufacturing, agriculture","agile methodology, agriculture, aws, bi tools, cloudnative data solutions, continuous integrationcontinuous delivery cicd, data engineering, data integration, data lake, data quality, data structures, data vault, databases, databricks, datamodeling, datapipeline, datawarehouse, etlelt, google cloud platform, manufacturing, message queuing, meta data, microsoft azure, powerbi, python, qlik, reporting, scalable data stores, snowflake, spark, sql, stream processing, tableau, testdriven solutions"
Lead Azure Data Engineer-US,Zortech Solutions,"Minneapolis, MN",https://www.linkedin.com/jobs/view/lead-azure-data-engineer-us-at-zortech-solutions-3731653937,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"Please share your updated resume to pavan@zortechsolutions.ca
Role: Lead Azure Data Engineer
Location: (MN / CT)-Day one Onsite
Duration: 6+ Months
Job Description
Required Skills:
Azure Data Bricks
ETL, Big Data
Marklogic, Talend, MDM, Data Bricks
Snowflake, Data factory, Change Data
Show more
Show less","Azure, Data Bricks, ETL, Big Data, MarkLogic, Talend, MDM, Snowflake, Data Factory, Change Data","azure, data bricks, etl, big data, marklogic, talend, mdm, snowflake, data factory, change data","azure, big data, change data, data bricks, data factory, etl, marklogic, mdm, snowflake, talend"
Senior SSIS/ETL Data Engineer,TekIntegral,"Minneapolis, MN",https://www.linkedin.com/jobs/view/senior-ssis-etl-data-engineer-at-tekintegral-3768989913,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"W2 Rate range 73-75/hr
LOCATION DETAILS -noted below
Only taking USC, GC or H4 EAD, L2 Visa or TN Visa
NOT able to take OPT EAD or CPT
ALL ROLES ARE HYBRID IN THE OFFICE (2-3 DAYS/WEEK) UNLESS REMOTE IS NOTED
Please submit qualified candidates and include FULL LEGAL NAME as it appears on the passport
If your candidate is relocating, need to have confirmation in writing that they will be onsite by Day 1
Senior SSIS/ETL Data Engineer
12 months 3 days in office
Minneapolis, MN or Dallas-Irving, TX
5+ years of experience with SQL Server 2016 (or newer) and SSIS
7+ years of data engineering
Must be capable of leading teams
2+ years of experience working with large scale SQL implementations
Experience designing, developing, and maintaining complex stored procedures, functions, and views using T-SQL
Experience designing, developing, and maintaining ETL processes using SSIS
Should understand how to unit test their product
Hands on experience with Visual Studio to document
Experience in financial or regulatory environments highly preferred
Ideal candidates will have experience creating conceptual, logical, and physical database designs
Preference given to candidates with extensive knowledge and experience with performance monitoring, database and SQL tuning
Bachelor's or Master's in computer science or related field or equivalent work experience
Show more
Show less","SQL Server 2016+, SSIS, ETL Data Engineering, Team Leadership, SQL Implementations, Stored Procedures, Functions, Views, TSQL, ETL Processes, Unit Testing, Visual Studio, Database Design, Performance Monitoring, Database Tuning, Computer Science","sql server 2016, ssis, etl data engineering, team leadership, sql implementations, stored procedures, functions, views, tsql, etl processes, unit testing, visual studio, database design, performance monitoring, database tuning, computer science","computer science, database design, database tuning, etl, etl data engineering, functions, performance monitoring, sql implementations, sql server 2016, ssis, stored procedures, team leadership, tsql, unit testing, views, visual studio"
"Sr Engineer - Big Data Infra (Hadoop, Spark, Linux, Java)",Target,"Brooklyn Park, MN",https://www.linkedin.com/jobs/view/sr-engineer-big-data-infra-hadoop-spark-linux-java-at-target-3781526428,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"Description: The pay range is $83,800.00 - $150,800.00
Pay is based on several factors which vary based on position. These include labor markets and in some instances may include education, work experience and certifications. In addition to your pay, Target cares about and invests in you as a team member, so that you can take care of yourself and your family. Target offers eligible team members and their dependents comprehensive health benefits and programs, which may include medical, vision, dental, life insurance and more, to help you and your family take care of your whole selves. Other benefits for eligible team members include 401(k), employee discount, short term disability, long term disability, paid sick leave, paid national holidays, and paid vacation. Find competitive benefits from financial and education to well-being and beyond at https://corporate.target.com/careers/benefits.
JOIN US AS A SR ENGINEER – BIG DATA INFRA ENGINEERING (HADOOP, SPARK, DRUID)
About Us
As a Fortune 50 company with more than 350k team members worldwide, Target is an iconic brand and one of America's leading retailers. Working at Target means the opportunity to help all families discover the joy of everyday life. Caring for our communities is woven into who we are, and we invest in the places we collectively live, work and play. We prioritize relationships, fuel and develop talent by creating growth opportunities, and succeed as one Target team. At our core, our purpose is ingrained in who we are, what we value, and how we work. It’s how we care, grow, and win together.
The Target High Performance Distributed Computing team creates the platforms and tools to enable our business partners to make great data-based decisions at Target. This team helps to manage hardware and software for large scale distributed computing, frequently angling towards data analytics and Artificial Intelligence/Machine Learning type applications. We help develop the technology that personalizes the guest experience, from product recommendations to relevant ad content. We’re also the source of the data and analytics behind Target's Supply Chain optimization, fraud detection, demand forecasting and metrics to support our stores. We play a key role in identifying the test-and-measure or A/B test opportunities that continuously help Target improve the guest experience, whether they love to shop in stores or at Target.com.
As a Senior Engineer, High Performance Distributed Computing, you’ll have the opportunity to create software solutions using Agile practices and DevOps principles. Your responsibilities will include designing, programming, debugging and supporting high quality, distributed, and large scale software solutions on the latest Big Data tech stack. You will develop software systems using test driven development while employing CI/CD practices. You will partner with other engineers and team members to develop software that meets business needs. You will follow Agile methodology for software development and technical documentation. You will innovate constantly and stay current with latest technologies while staying focused on solving problems at hand.
Core responsibilities are described within this job description. Job duties may change at any time due to business needs.
About You
BS/MS in Computer Science or equivalent experience
3+ years of experience in operating computing clusters and software development
Proven track record in writing code that is correct, maintainable, testable, expressive, easy to change, efficient & fault-tolerant
Extensive understanding of software development and design
Experience building and supporting APIs
Proficiency in at least one of the following languages: Java, Scala, Python
Have familiarity and/or experience with the following technologies:
Hadoop
Hive
ZooKeeper
Oozie
Trino/Presto
Ranger
Spark
Python
R
Druid
Ozone
Elasticsearch / Logstash / Kibana (ELK)
PostgreSQL
Kafka
Experience with distributed and parallel processing, computer architecture, operating systems, synchronization, communication
Experience with modern CI/CD technologies such as Git, Drone, Docker, Artifactory
Understand business fundamentals and how technologies can support business goals along with how to translate business vision into a technical strategy while understanding the financial implications
Have proven interpersonal skills, and problem-solving skills
Strong team player who understands concepts of teamwork and team effectiveness.
Have excellent verbal, written, and presentation communication skills to convey complex technical solutions clearly to an organization
Proven track record in writing code that is correct, maintainable, testable, expressive, easy to change, efficient & fault-tolerant
This position will operate as a Hybrid/Flex for Your Day work arrangement based on Target’s needs. A Hybrid/Flex for Your Day work arrangement means the team member’s core role will need to be performed both onsite at the Target HQ MN location the role is assigned to and virtually, depending upon what your role, team and tasks require for that day. Work duties cannot be performed outside of the country of the primary work location, unless otherwise prescribed by Target. Click here if you are curious to learn more about Minnesota.
Americans With Disabilities Act (ADA)
Target will provide reasonable accommodations with the application process upon your request as required to comply with applicable laws. If you have a disability and require assistance in this application process, please visit your nearest Target store or Supply Chain Facility or reach out to Guest Services at 1-800-440-0680 for additional information.
Show more
Show less","Java, Scala, Python, Hadoop, Hive, ZooKeeper, Oozie, Trino/Presto, Ranger, Spark, R, Druid, Ozone, Elasticsearch, Logstash, Kibana (ELK), PostgreSQL, Kafka, Git, Drone, Docker, Artifactory, CI/CD","java, scala, python, hadoop, hive, zookeeper, oozie, trinopresto, ranger, spark, r, druid, ozone, elasticsearch, logstash, kibana elk, postgresql, kafka, git, drone, docker, artifactory, cicd","artifactory, cicd, docker, drone, druid, elasticsearch, git, hadoop, hive, java, kafka, kibana elk, logstash, oozie, ozone, postgresql, python, r, ranger, scala, spark, trinopresto, zookeeper"
"Lead Engineer - Big Data Platform/Infra (Hadoop, Spark Streaming, Druid)",Target,"Brooklyn Park, MN",https://www.linkedin.com/jobs/view/lead-engineer-big-data-platform-infra-hadoop-spark-streaming-druid-at-target-3741788357,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"Description: The pay range is $109,000.00 - $196,200.00
Pay is based on several factors which vary based on position. These include labor markets and in some instances may include education, work experience and certifications. In addition to your pay, Target cares about and invests in you as a team member, so that you can take care of yourself and your family. Target offers eligible team members and their dependents comprehensive health benefits and programs, which may include medical, vision, dental, life insurance and more, to help you and your family take care of your whole selves. Other benefits for eligible team members include 401(k), employee discount, short term disability, long term disability, paid sick leave, paid national holidays, and paid vacation. Find competitive benefits from financial and education to well-being and beyond at https://corporate.target.com/careers/benefits.
JOIN US AS A LEAD ENGINEER – BIG DATA PLATFORM
About Us
As a Fortune 50 company with more than 350k team members worldwide, Target is an iconic brand and one of America's leading retailers. Working at Target means the opportunity to help all families discover the joy of everyday life. Caring for our communities is woven into who we are, and we invest in the places we collectively live, work and play. We prioritize relationships, fuel and develop talent by creating growth opportunities, and succeed as one Target team. At our core, our purpose is ingrained in who we are, what we value, and how we work. It’s how we care, grow, and win together.
The Target High Performance Distributed Computing team creates the platforms and tools to enable our business partners to make data-based decisions at Target. This team helps to manage hardware and software for large scale distributed computing, frequently angling towards data analytics and Artificial Intelligence/Machine Learning type applications. We help develop the technology that personalizes the guest experience, from product recommendations to relevant ad content. We’re also the source of the data and analytics behind Target's Supply Chain optimization, fraud detection, demand forecasting (DFE) and metrics to support our stores. We play a key role in identifying the test-and-measure or A/B test opportunities that continuously help Target improve the guest experience, whether they love to shop in stores or Target.com.
As a Lead Engineer, you serve as the technical anchor for the engineering team that supports a product. You create, own and are responsible for the application and platform architecture that best serves the product in its functional and non-functional needs. You'll bring innovative ideas and help set the strategy for the future of our platform. You love keeping abreast of the latest industry trends and use them to help you innovate. You have leadership qualities, good judgment, and clear communication skills. If you’re excited to work on a fast-moving, tightly knit team and build solutions to unsolved problems, we want to meet you.
As a Lead Engineer, you’ll take the lead as you…
Understand Target's business and technical environments and assist teams in resolving complex business challenges via current technical solutions by assessing viability/applicability/cost implication through POCs and prototypes.
Collaborate with technical staff and Enterprise Architecture teams in setting technical direction across platform and drive technology lifecycle management and communication of standards/decisions to the engineering team.
Participate in procurement specifications, installation, and maintenance of Target systems.
Lead designing and building the Target platform API with deep focus on non-functional requirements including scalability, availability, performance, etc. while being a strong advocate of extreme agile and DevOps practices across engineers.
About You
BS/MA degree in Computer Science or relevant experience
5+ years of experience in developing software applications
Detailed knowledge of GNU/Linux OS experience w/ administration of production grade services running on Linux servers
Proven track record in writing code that is correct, maintainable, testable, expressive, easy to change, efficient and fault-tolerant
Demonstrated proficiency in Java
Demonstrated knowledge of some of the following concepts:
Operating system architecture, memory management, process scheduling, I/O scheduling
Networking, technologies, latency, bandwidth
Benchmarking, performance debugging, performance monitoring
Limiting-resource identification
Have familiarity and experience with some of the following:
Hadoop (multi-node fully distributed Hadoop clusters)
Spark
HDFS
Hive
ZooKeeper
Ozone
Trino/PrestoSQL
Possess a strong understanding of high-performance, large-scale system architecture design and implementation
Experience with distributed and parallel processing, computer architecture, operating systems, synchronization, communication
Experience with modern CI/CD technologies such as Git, Drone, Docker, Artifactory
Understand business fundamentals and how technologies can support business goals along with how to translate business vision into a technical strategy and financial implications
Strong team player who understands concepts of teamwork and team effectiveness.
Have excellent verbal, written, and presentation communication skills to convey complex technical solutions clearly to an organization
Have excellent planning and organizational skills­­­
This position will operate as a Hybrid/Flex for Your Day work arrangement based on Target’s needs. A Hybrid/Flex for Your Day work arrangement means the team member’s core role will need to be performed both onsite at the Target HQ MN location the role is assigned to and virtually, depending upon what your role, team and tasks require for that day. Work duties cannot be performed outside of the country of the primary work location, unless otherwise prescribed by Target. Click here if you are curious to learn more about Minnesota.
Americans With Disabilities Act (ADA)
Target will provide reasonable accommodations with the application process upon your request as required to comply with applicable laws. If you have a disability and require assistance in this application process, please visit your nearest Target store or Supply Chain Facility or reach out to Guest Services at 1-800-440-0680 for additional information.
Application deadline is : 02/25/2024
Show more
Show less","Linux, Java, Hadoop, Spark, HDFS, Hive, ZooKeeper, Ozone, Trino/PrestoSQL, Distributed processing, Parallel processing, Computer architecture, Operating systems, Synchronization, Communication, CI/CD, Git, Drone, Docker, Artifactory, Business fundamentals, Teamwork, Communication, Planning, Organization","linux, java, hadoop, spark, hdfs, hive, zookeeper, ozone, trinoprestosql, distributed processing, parallel processing, computer architecture, operating systems, synchronization, communication, cicd, git, drone, docker, artifactory, business fundamentals, teamwork, communication, planning, organization","artifactory, business fundamentals, cicd, communication, computer architecture, distributed processing, docker, drone, git, hadoop, hdfs, hive, java, linux, operating systems, organization, ozone, parallel processing, planning, spark, synchronization, teamwork, trinoprestosql, zookeeper"
Sr AI/Data Science Engineer,Medtronic,"Minneapolis, MN",https://www.linkedin.com/jobs/view/sr-ai-data-science-engineer-at-medtronic-3738071997,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"Careers that Change Lives
Engineers and Scientists create our market-leading portfolio of innovations.Join us to make a lasting impact. Help bring the next generation of life-changing medical technology to patients worldwide. Together, we can change healthcare worldwide. At Medtronic, we push the limits of what technology can do to help alleviate pain, restore health and extend life. We challenge ourselves and each other to make tomorrow better than yesterday.Join us to make a lasting impact.
Medtronic is looking for an Sr. AI Data Science Engineer to join the Manufacturing Process Optimization Team.As a part of Global Operations and Supply Chain (GO&SC) - Operations Innovation, the Process Optimization team will deliver strategic manufacturing process performance improvements across the Medtronic enterprise.The Sr. AI/Data Science Engineer will work between the strategic programs in Process Optimization, the Digital Technology Team, and the Information Technology (IT) Team to develop, coordinate and deliver advanced analytics, data process solutions, and data insights to help steer our strategic work.
A Day in the Life
Responsibilities may include the following and other duties may be assigned.
Supports key analytics for OEE, SPC, Yield, and Capacity Modeling.Work with program leaders to identify business problems and propose data analytics solutions.
Help identify and develop cutting edge methods for data mining to develop new insights.
Liaison between various business, functional and/or technical development teams.Specifically, works between the Digital Technology team, the Information Technology (IT) team, and the Analytics team to help enable and grow existing data architecture platform(s).
Work to understand manufacturing process and equipment, understand machine requirements for data communication. Develop value stream, process, machine data hierarchy to enable data reporting requirements.
Develop and manage automated data subscriptions within the analytics process.
Enable standard digital capabilities for asset intelligence.Drive continuous improvements in operations through digitizing data, creating analytics and visualization, and providing data insights.
Ensure data hand-off between Process Optimization and Digital, IT teams are seamless with well-defined and standardized schema to ensure successful data visualization.
Focus on data accuracy and data governance for key analytics.Be the go-to person on the Process Optimization team, to work between the program leaders and the Digital, IT, Analytics teams.
Trouble shooting – Lead and/or assist various trouble shooting activities in Digital Technology and Analytics.Work between key stake holders in IT, OT, and Projects to ensure timely resolution of issues.
Develops and communicates descriptive, diagnostic, predictive and prescriptive insights/algorithms.
Manage workflow within and between multiple domain environments including testing development and production.
Experience managing teams for programming and implements efficiencies, performs testing and debugging.
Completes documentation and procedures for requirements, training, installation, and maintenance.
After data set and dashboard development: Adapts machine learning to areas such as virtual reality, augmented reality, artificial intelligence, robotics, and other products that allow users to have an interactive experience (or possible in sites with SPC tool in place)
Can work with large scale agnostic frameworks, data analysis systems and modeling environments.
Work within existing Quality System requirements to ensure data compliance.
There is no relocation assistance available for this role
Must Have: Minimum Requirements
Bachelors degree required
Minimum of 4 years of relevant experience, or advanced degree with a minimum of 2 years relevant experience
Nice to Have
Experience in developing Digital Analytics, Statistical Methods, Visualization Tools, and Analytics Insights.
Experience in relevant data-oriented technology solutions, for example Ignition, Spark, Python, SQL, PowerBI
Experience in Controls Engineering, Machine Learning, or similar.
Experience in Project Management, Governance, and Change Management.
Strong problem solver, strong analytical skills.Working understanding of industrial statistics.
Requirements Development - Able to formulate clear requirements contingencies, identify and manage risk and execute program objective autonomously.
Familiar with IT architecture, infrastructure, data engineering to empower data driven decision making.
Experience Driving business strategy through digital tools and dashboards.
Working knowledge of Industry 4.0 operational technology applications (MES/SCADA/IIOT/PLC/Controls)
TECHNICAL SPECIALIST CAREER STREAM
An individual contributor with responsibility in our technical functions to advance existing technology or introduce new technology and therapies.Formulates, delivers and/or manages projects assigned and works with other stakeholders to achieve desired results.May act as a mentor to colleagues or may direct the work of other lower-level professionals.The majority of time is spent delivering R&D, systems or initiatives related to new technologies or therapies – from design to implementation - while adhering to policies, using specialized knowledge and skills.
DIFFERENTIATING FACTORS
Autonomy: Seasoned individual contributor.
Works independently under limited supervision to determine and develop approach to solutions.
Coaches and reviews the work of lower-level specialists; may manage projects / processes.
Organizational Impact: May be responsible for entire projects or processes within job area.
Contributes to the completion of work group objectives, through building relationships and consensus to reach agreements on assignments.
Innovation and Complexity: Problems and issues faced are difficult, and may require understanding of multiple issues, job areas or specialties.
Makes improvements of processes, systems, or products to enhance performance of the job area.
Analysis provided is in-depth in nature and often provides recommendations on process improvements.
Communication and Influence: Communicates with senior internal and external customers and vendors.
Exchange information of facts, statuses, ideas, and issues to achieve objective, and influence decision-making.
Leadership and Talent Management: May provide guidance, coaching and training to other employees within job area.
Required Knowledge and Experience: Requires advanced knowledge of job area combining breadth and depth, typically obtained through advanced education combined with experience.
About Medtronic
Together, we can change healthcare worldwide. At Medtronic, we push the limits of what technology, therapies and services can do to help alleviate pain, restore health and extend life. We challenge ourselves and each other to make tomorrow better than yesterday. It is what makes this an exciting and rewarding place to be.
We want to accelerate and advance our ability to create meaningful innovations - but we will only succeed with the right people on our team. Let’s work together to address universal healthcare needs and improve patients’ lives. Help us shape the future.
A commitment to our employees lives at the core of our values. We recognize their contributions. They share in the success they help to create. We offer a wide range of benefits, resources, and competitive compensation plans designed to support you at every career and life stage. Learn more about our benefits
here
.
This position is eligible for a short-term incentive plan. Learn more about Medtronic Incentive Plan (MIP) on page 6
here
.
The provided base salary range is used nationally (except in certain CA locations). The rate offered is compliant with federal/local regulations and may vary by experience, certification/education, market conditions, location, etc.
Physical Job Requirements
The physical demands described within the Responsibilities section of this job description are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. For Office Roles: While performing the duties of this job, the employee is regularly required to be independently mobile. The employee is also required to interact with a computer, and communicate with peers and co-workers. Contact your manager or local HR to understand the Work Conditions and Physical requirements that may be specific to each role. (ADA-United States of America)
Show more
Show less","OEE, SPC, Yield, Capacity Modeling, Python, SQL, PowerBI, Ignition, Spark, Industrial Statistics, Digital Analytics, Statistical Methods, Visualization Tools, Analytics Insights, Controls Engineering, Machine Learning, Project Management, Governance, Change Management, Data Engineering, Virtual Reality, Augmented Reality, Artificial Intelligence, Robotics, MES, SCADA, IIOT, PLC, Controls, Business Strategy","oee, spc, yield, capacity modeling, python, sql, powerbi, ignition, spark, industrial statistics, digital analytics, statistical methods, visualization tools, analytics insights, controls engineering, machine learning, project management, governance, change management, data engineering, virtual reality, augmented reality, artificial intelligence, robotics, mes, scada, iiot, plc, controls, business strategy","analytics insights, artificial intelligence, augmented reality, business strategy, capacity modeling, change management, controls, controls engineering, data engineering, digital analytics, governance, ignition, iiot, industrial statistics, machine learning, mes, oee, plc, powerbi, project management, python, robotics, scada, spark, spc, sql, statistical methods, virtual reality, visualization tools, yield"
"Lead Data Analyst, Power BI",Strive Health,"Minneapolis, MN",https://www.linkedin.com/jobs/view/lead-data-analyst-power-bi-at-strive-health-3744884924,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"What We Strive For
Strive Health is built for purpose — to transform a broken kidney care system. We are fundamentally changing the lives of kidney disease patients through early identification, engagement and comprehensive coordinated care. Strive’s model is driven by a high-touch care team that integrates with local providers and spans the entire care journey from chronic kidney disease through end-stage kidney disease, leveraging comparative and predictive data and analytics to identify at-risk patients. Strive Health’s interventions significantly reduce the rate of emergent dialysis crash, cut inpatient utilization and significantly improve patient outcomes and experience. Come join our journey as we create THE destination for top talent in the healthcare community and set a new standard for how kidney care should be done.
Why We’re Worth the Application
We Strive for excellence and were recognized as one of America’s Best Startup Employers for 2023 by Forbes, Circle of Excellence, American Society of Transplantation, 2023, Best Places to Work – Denver, Comparably.com, 2022, Best Places to Work – Denver, Built in Colorado, 2022
We derive innovation and ideas from through authentic diversity intentionally building a team that represents the populations we serve in partnership with our Employee Resource Groups:
Strive Forward - LGBTQ
Underrepresented Minorities
Women and Allies
We care and support our Strivers within and beyond work to feel fully charged and empowered through our generous wellbeing offerings including:
Flexible time off
Companywide wellbeing days
Volunteer time off
Leave packages including a sabbatical, parental leave and eight weeks paid for living donor
Professional development
A dedicated certified financial planner
Headspace, Carrot Fertility and Gympass for all Strivers
We like to have fun by celebrating our successes as a team through team building, company gatherings, trivia, wellbeing raffles, pajama days, a companywide book club and more.
We value tenacity to help us overcome obstacles with grit and determination to deliver compassionate kidney care.
Lead Data Analyst, Power BI
We are looking for a Lead Data Analyst, PowerBI with responsibility to provide support to Finance. Individual will help with PowerBI implementation, system enhancements, documentation, training, and automation of complex models, reporting and analytics. Role will be responsible for maintaining the application for reliable performance, performing analysis, identifying issues, and working with business to solution and drive improvements. With the company being in start-up mode, this role needs to operate in a constantly changing environment where the future will look vastly different than the present.
Essential Functions
Working in ERP systems and related finance systems/technologies and reporting environment.
Detailed oriented individual with ability to rapidly learn and adjust to business needs.
Experience with PowerBI Roadmap Implementation – (Establishing Processes, defining Use cases/business requirements, Testing, project managing change management, and driving Adoption)
Prior experience with building Data Governance, Data Architecture and Strategy
Be a SME and train end users on functionality of PowerBI capabilities and work cross functionally with Finance, Data and Engineering team.
Ability to coordinate upgrades and application releases and data structure enhancements, and proactively defines areas for improvement for data flows and analytical needs.
Minimum Qualifications
Bachelor’s degree in business, accounting, finance, or information systems
Minimum 7+ years’ progressive experience in systems and finance industry
3+ years in Financial/Analytics System implementation and post go live support.
Preferred Qualifications
Knowledge of Excel, Financial and Reporting systems such as SAP, Hyperion, Tableau
Data Strategy and Visualization tools and working with large data sets.
Microsoft Office Suite
Experience in working in Matrix environment and adaptable to change.
Proven experience in implementing PowerBI complex models, robust reporting, and actionable analytics.
Dynamic understanding of business process in finance domain
Experience with Implementing PowerBI application, Testing.
Successful Traits
Embodies Strive’s core values: Care for Others First, Resilience, Tenacity, Innovation, and Fun
Strong communication and high emotional intelligence – ability to navigate relationships and work across multiple domains.
Resourcefulness and willingness to learn and gain new information.
Adaptable to change and comfortable with ambiguity.
Self-Starter with excellent interpersonal communication, problem solving skills and comfortable dealing with ambiguity.
Proven performance working in collaborative team environment and ability to work independently.
Ability to work in a fast-paced environment and manage competing deadlines.
Loves a good challenge and will naturally thrive in a changing environment.
Annual Salary Range:
$93,400.00-$116,700.00
Strive Health offers competitive compensation and benefits, including Health insurance, Dental insurance, Vision insurance, 401k Retirement Plan with Employer Match, Life and Accidental Death & Dismemberment insurance, Disability insurance, Health Savings Account, Flexible Spending Account, paid company holidays, in addition to Vacation Time Off. An annual performance bonus, determined by company and individual performance, is available for many roles as aligned to Strive Health guidelines.
Strive Health is an equal opportunity employer and drug free workplace. At this time Strive Health is unable to provide work visa sponsorship. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Please apply even if you feel you do not meet all qualifications. If you require reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please direct your inquiries to talentacquisition@strivehealth.com
Show more
Show less","PowerBI, ERP systems, Financial systems, Excel, SAP, Hyperion, Tableau, Data Strategy, Visualization tools, Microsoft Office Suite, Matrix environment, Dynamic understanding of business process in finance domain, Data Governance, Data Architecture, Data Strategy","powerbi, erp systems, financial systems, excel, sap, hyperion, tableau, data strategy, visualization tools, microsoft office suite, matrix environment, dynamic understanding of business process in finance domain, data governance, data architecture, data strategy","data architecture, data governance, data strategy, dynamic understanding of business process in finance domain, erp systems, excel, financial systems, hyperion, matrix environment, microsoft office suite, powerbi, sap, tableau, visualization tools"
Principal Data Engineer,Dice,"Minneapolis, MN",https://www.linkedin.com/jobs/view/principal-data-engineer-at-dice-3786296193,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, CS Solutions, Inc., is seeking the following. Apply via Dice today!
Responsibilities:
Develop, implement, and maintain leading-edge analytic systems, taking complicated problems and building simple frameworks
Identify trends and opportunities for growth through analysis of complex data sets
Evaluate organizational methods and provide source-to-target mappings and information-model specification documents for data sets
Create best-practice reports based on data mining, analysis, and visualization
Evaluate internal systems for efficiency, problems, and inaccuracies, developing and maintaining protocols for handling, processing, and cleaning data
Work directly with management and users to gather requirements, provide status updates, and build relationships
Work closely with Business and project managers to understand and maintain focus on their analytical needs, including identifying critical metrics and KPIs, and deliver actionable insights to relevant decision-makers
Proactively analyze data to answer key questions from stakeholders or out of self-initiated curiosity with an eye for what drives business performance, investigating and communicating areas for improvement in efficiency and productivity
Create and maintain rich interactive visualizations through data interpretation and analysis integrating various reporting components from multiple data sources
Define and implement data acquisition and integration logic, selecting appropriate combination of methods and tools within defined technology stack to ensure optimal scalability and performance of the solution
Develop and maintain databases by acquiring data from primary and secondary sources, and build scripts that will make our data evaluation process more flexible or scalable across data sets
Skills and Qualifications:
6-8 years’ experience data as a data engineer
Experience in azure 9or expertise in any other cloud expertise)
Experience is Spark, Python
Must have financial Industry experience ,especially with investments
Proven analytic skills, including mining, evaluation, analysis, and visualization
Technical writing experience in relevant areas, including queries, reports, and presentations
Strong SQL and MS Excel skills with the ability to learn other analytic tools
Prior experience with Analytics , model design and segmentation techniques
Strong programming experience with ETL
Practical experience in statistical analysis through the use of statistical packages
Proven success in a collaborative, team-oriented environment
Principal Data Engineer
Show more
Show less","Data engineering, Data analysis, Data mining, Data visualization, Data integration, Data modeling, Data acquisition, Data cleaning, Statistical analysis, SQL, MS Excel, Python, Spark, Azure, Financial industry knowledge, Investment industry knowledge, Analytics, Model design, Segmentation techniques, Agile, ETL","data engineering, data analysis, data mining, data visualization, data integration, data modeling, data acquisition, data cleaning, statistical analysis, sql, ms excel, python, spark, azure, financial industry knowledge, investment industry knowledge, analytics, model design, segmentation techniques, agile, etl","agile, analytics, azure, data acquisition, data cleaning, data engineering, data integration, data mining, dataanalytics, datamodeling, etl, financial industry knowledge, investment industry knowledge, model design, ms excel, python, segmentation techniques, spark, sql, statistical analysis, visualization"
Staff Data Engineer,Recruiting from Scratch,"Minneapolis, MN",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744388835,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Modeling, Schema Design","python, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, data warehouses, etl, tdd, pair programming, continuous integration, automated testing, deployment, modeling, schema design","airflow, automated testing, continuous integration, data warehouses, deployment, docker, etl, helm, kafka, kubernetes, modeling, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Minneapolis, MN",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748832217,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Continuous Integration, Pair Programming, automated testing, Kafka, Storm, SparkStreaming, dimensional data modeling, Data Warehouses, ETL, legal compliance, data management tools, data classification, retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, continuous integration, pair programming, automated testing, kafka, storm, sparkstreaming, dimensional data modeling, data warehouses, etl, legal compliance, data management tools, data classification, retention","airflow, automated testing, continuous integration, data classification, data management tools, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, retention, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Minneapolis, MN",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744389969,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Realtime streaming, TDD, Automation, Continuous delivery, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, SQL, Data Warehouses, ETL, Kafka, Storm, SparkStreaming, Dimensional data modeling, Data governance, Agile engineering practices, Pair Programming, Continuous Integration, Automated testing, Deployment","data engineering, realtime streaming, tdd, automation, continuous delivery, python, snowflake, airflow, kubernetes, docker, helm, spark, sql, data warehouses, etl, kafka, storm, sparkstreaming, dimensional data modeling, data governance, agile engineering practices, pair programming, continuous integration, automated testing, deployment","agile engineering practices, airflow, automated testing, automation, continuous delivery, continuous integration, data engineering, data governance, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, realtime streaming, snowflake, spark, sparkstreaming, sql, storm, tdd"
Sr Data Engineer,General Mills,"Minneapolis, MN",https://www.linkedin.com/jobs/view/sr-data-engineer-at-general-mills-3783682108,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"Employer: General Mills, Inc.
Job Title: Senior Data Engineer (multiple positions)
Job Requisition: #26018 | 20330.287.4
Job Location: 1 General Mills Blvd, Minneapolis, MN 55426 | Telecommuting up to 100% of time allowed.
Job Type: Full Time
Duties
Work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions that will generate insights from our connected data, enabling General Mills to advance the data-driven decision-making capabilities of our enterprise. Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals. Solve complex data problems to deliver insights that helps our business to achieve their goals. Create data products for analytics and data scientist team members to improve their productivity. Advise, consult, mentor and coach other data and analytic professionals on data standards and practices. Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions. Lead evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve our productivity as a team. Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes. Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives. Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics.
Telecommuting up to 100% of time allowed.
Requirements
Bachelor’s degree in Computer Science, Computer Information Systems, Engineering or a related field and 5 years of post-baccalaureate, progressively responsible experience in the job offered or 5 years of post-baccalaureate, progressively responsible experience in data engineering or architecture OR
Master’s degree in Computer Science, Computer Information Systems, Engineering or a related field and 1 year of experience in job offered or 1 year of experience in data engineering or architecture.
Must have 1 year of experience in each of the following:
SQL and data analysis
Python or Scala
Developing and maintaining data warehouses in big data solutions
Big Data development experience using Hadoop
At least 2 of the following: Hive, BigQuery, Impala OR Spark
Kafka
Tableau, Power BI, Looker or Shiny
Background check and drug testing required.
Contact: Apply online at https://careers.generalmills.com/careers/ Please refer to job requisition number- #26018 | 20330.287.4.
The salary range for this position $140,358-$174,600. At General Mills we strive for each employee’s pay at any point in their career to reflect their experiences, performance and skills for their current role. The salary range for this role represents the numerous factors considered in the hiring decision including, but not limited to, education, skills, work experience, certifications, etc. As such, pay for the successful candidate(s) could fall anywhere within the stated range. Beyond base salary, General Mills offers a competitive Total Rewards package focusing on your overall well-being. We are proud to offer a foundation of health benefits, retirement and financial wellbeing, time off programs, wellbeing support and perks. Benefits may vary by role, country, region, union status, and other employment status factors. You may also be eligible to participate in an annual incentive program. An incentive award, if any, depends on various factors, including, individual and organizational performance.
Company Overview
We exist to make food the world loves. But we do more than that. Our company is a place that prioritizes being a force for good, a place to expand learning, explore new perspectives and reimagine new possibilities, every day. We look for people who want to bring their best — bold thinkers with big hearts who challenge one other and grow together. Because becoming the undisputed leader in food means surrounding ourselves with people who are hungry for what’s next.
Show more
Show less","SQL, Python, Scala, Hadoop, Apache Hive, Apache BigQuery, Apache Impala, Apache Spark, Apache Kafka, Tableau, Power BI, Looker, Shiny, ETL, Data Warehousing, Data Analytics, Data Pipelines, Data Architecture, Machine Learning, Artificial Intelligence, Computer Vision, Statistics","sql, python, scala, hadoop, apache hive, apache bigquery, apache impala, apache spark, apache kafka, tableau, power bi, looker, shiny, etl, data warehousing, data analytics, data pipelines, data architecture, machine learning, artificial intelligence, computer vision, statistics","apache bigquery, apache hive, apache impala, apache kafka, apache spark, artificial intelligence, computer vision, data architecture, dataanalytics, datapipeline, datawarehouse, etl, hadoop, looker, machine learning, powerbi, python, scala, shiny, sql, statistics, tableau"
Sr Data Engineer,General Mills,"Minneapolis, MN",https://www.linkedin.com/jobs/view/sr-data-engineer-at-general-mills-3783678783,2023-12-17,Lake Forest,United States,Mid senior,Onsite,"Employer: General Mills, Inc.
Job Title: Sr. Data Engineer (multiple positions)
Job Requisition: #26017 | 20330.291.8
Job Location: 1 General Mills Blvd, Minneapolis, MN 55426 | Telecommuting 100% of the time is permitted.
Job Type: Full Time
Duties
Work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions that will generate insights from our connected data, enabling General Mills to advance the data-driven decision-making capabilities of our enterprise. Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals. Solve complex data problems to deliver insights that helps our business to achieve their goals. Create data products for analytics and data scientist team members to improve their productivity. Advice, consult, mentor and coach other data and analytic professionals on data standards and practices. Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions. Lead evaluation, implementation and deployment of emerging tools & process for analytic data engineering to improve our productivity as a team. Develop and deliver communication & education plans on analytic data engineering capabilities, standards, and processes. Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives. Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics.
Telecommuting 100% of the time is permitted.
Requirements
Bachelor’s Degree in Computer Science, Computer Information Systems, Information Technology and Management, Engineering or related field and 5 years of post-baccalaureate, progressively responsible experience in data engineering or architecture.
OR
Master’s degree in Computer Science, Computer Information Systems, Information Technology and Management, Engineering or related field and 1 year of experience in data engineering or architecture.
Must have 1 year of experience in each of the following:
SQL and data analysis
Python, Scala or Java
Developing and maintaining data warehouses in cloud or other large scale data platforms
Big Data development experience using Hadoop with any of the following: Hive, BigQuery, SQL, Impala OR Spark
Designing, creating, coding, and supporting an ETL solution, including at least one of the following: Talend Studio, Kafka, Jira, SAP Data Services, SAP or HANA.
Tableau, Power BI, Looker or Shiny
Data and analytics including at least one of the following: dimensional modeling, ETL, reporting tools, data governance, data warehousing or structured and unstructured data
Background check and drug testing required.
Contact: Apply online at https://careers.generalmills.com/careers/ Please refer to job requisition number- #26017 | 20330.291.8.
The salary range for this position $129,147-174,600. At General Mills we strive for each employee’s pay at any point in their career to reflect their experiences, performance and skills for their current role. The salary range for this role represents the numerous factors considered in the hiring decision including, but not limited to, education, skills, work experience, certifications, etc. As such, pay for the successful candidate(s) could fall anywhere within the stated range. Beyond base salary, General Mills offers a competitive Total Rewards package focusing on your overall well-being. We are proud to offer a foundation of health benefits, retirement and financial wellbeing, time off programs, wellbeing support and perks. Benefits may vary by role, country, region, union status, and other employment status factors. You may also be eligible to participate in an annual incentive program. An incentive award, if any, depends on various factors, including, individual and organizational performance.
Company Overview
We exist to make food the world loves. But we do more than that. Our company is a place that prioritizes being a force for good, a place to expand learning, explore new perspectives and reimagine new possibilities, every day. We look for people who want to bring their best — bold thinkers with big hearts who challenge one other and grow together. Because becoming the undisputed leader in food means surrounding ourselves with people who are hungry for what’s next.
Show more
Show less","Data engineering, Data architecture, ETL, Python, Scala, Java, Hadoop, Hive, BigQuery, SQL, Impala, Spark, Talend Studio, Kafka, Jira, SAP Data Services, SAP, HANA, Tableau, Power BI, Looker, Shiny, Dimensional modeling, Data governance, Data warehousing, Structured data, Unstructured data","data engineering, data architecture, etl, python, scala, java, hadoop, hive, bigquery, sql, impala, spark, talend studio, kafka, jira, sap data services, sap, hana, tableau, power bi, looker, shiny, dimensional modeling, data governance, data warehousing, structured data, unstructured data","bigquery, data architecture, data engineering, data governance, datawarehouse, dimensional modeling, etl, hadoop, hana, hive, impala, java, jira, kafka, looker, powerbi, python, sap, sap data services, scala, shiny, spark, sql, structured data, tableau, talend studio, unstructured data"
Senior Data Engineer,When I Work,"Minneapolis, MN",https://www.linkedin.com/jobs/view/senior-data-engineer-at-when-i-work-3758250587,2023-12-17,Lake Forest,United States,Mid senior,Remote,"When I Work is a remote first company. We are open to hiring candidates in the continental US and Ontario, Canada. If an onsite location is important to you in your search, you are welcome to work from our Minneapolis HQ office.
Who We Are
We help hourly teams get shift done.
At When I Work, everything we do starts with a mission to make shift work awesome. We deliver on that mission by making every piece of hourly workforce management - scheduling, time tracking, shift trading, team messaging, and more - easy and straightforward for managers and employees alike.
The Data and RevOps team at When I Work is a group of inquisitive and driven individuals who love solving problems using data. We have built a best-in-class data environment and fuel insights throughout the organization on our product and customers. We work collaboratively together, invest in our processes and tooling, and move slow to move fast. We focus on projects that will have a big impact to the company and work to enable anyone to be a savvy data user.
What You'll Do
Over the last few years we have been building out a best-in-class data environment that we've used to transform When I Work into a data-driven company. You will be a key contributor in continuing to grow and mature this environment as well as develop and sustain data projects that will have significant impact on our company and our users.
Proactively identify opportunities to improve and update data platform infrastructure and research new technologies and strategies
Design, build and implement tools aimed at allowing business users to collect and analyze data in an efficient and effective way
Design, build and maintain integrations between our internal data platform and 3rd party tools utilized throughout the company
Develop and manage ETLs and data pipelines
Create data products for consumption by internal When I Work team members
Be part of a team that owns all aspects of its service delivery -- from cloud infrastructure, to application code, to operations
Our Technology Stack
We use a lot of different technologies to get the job done, and each member of our team brings their own mix of technology experience. If you have familiarity with even a few of these (or equivalents), you could make a valuable contribution: Python, Go, SQL, Terraform, Jupyter, Git, GitLab, Spark, Flink, Presto, Kafka, MySQL, NoSQL, Kubernetes, DBT, Prefect, Airflow, lots of AWS(EC2, EKS, Lambda, S3, RDS, DynamoDB, Aurora, Redshift, Athena, EMR, CloudSearch, Kinesis, API Gateway).
Who You Are
You are a programmer who is excited by data and its endless possible use cases. Someone that enjoys creating tools and infrastructure to empower your peers. Collaboration and teamwork are a must, but you also have the ability to work independently when needed to get things done. Above all, you are driven to learn and a motivated problem solver who wants to help tackle the new and interesting challenges that we encounter as a fast-growing startup.
Experience And Skills Needed
3+ years of experience in data engineering
You have strong programming fundamentals
You are comfortable with agile software processes
You have experience with multiple programming languages (Python, SQL, etc.)
Comfortable working with APIs/Webhooks
You have significant experience working with structured and unstructured data
You have significant experience with cloud computing environments and infrastructure
You are a proponent of DevOps and enthusiastic about DataOps
You practice empathy and kindness, and you look to help others
What Would Be Awesome To Have
Advanced Python and data package (Numpy, Pandas, etc.) skills
You are comfortable with different data modeling techniques and have experience with a data warehouse platform (Redshift, Snowflake, etc.)
You have experience working with message queues and event buses to collect and process data in near real time
Understanding & perspective on data catalogs and schema registries
What's In It For You
Professional development allowance
Paid parental leave
Medical benefits - employee premiums paid 100% by When I Work
Dental benefits- employee premiums paid 100% by When I Work
Paid vacation and holidays
Flexible work environment
401K Match
Remote first culture including home office set-up stipend and ongoing telecommuter stipend
Casual dress code
Dynamic and dedicated team
We believe actions speak louder than words. Every encounter with our people and products should be memorable and helpful. Challenges are exciting, failure is how we learn, and we all have an entrepreneurial spirit. Building an inclusive and equitable workplace isn't lip service. We invest our time and our money in organizations that are not only working to diversify the current jobscape, but also investing in the future of talent. We're motivated by a strong, innovative, and passionate work culture and we're constantly searching for ways to improve and get shift done.
Whether you're a perfect match or not, if it sounds like a good fit, we encourage you to apply.
The tech industry is notorious for its lack of diverse representation, and we're aware of the research showing that historically underrepresented groups are less likely to apply to a job if they don't believe that they meet all of the criteria. Are you hesitant to submit an application because you're not sure if you check every box? Apply anyway! We would love to hear from you and figure out what you can add to the culture here at When I Work.
We'd love to talk to you! Please submit the following to apply:
Resume (including months/years of employment for each position).
Cover letter including:
an overview of your existing experience
a convincing reason why you'd like to work at When I Work.
Must already be authorized to work in the United States or Canada on a full-time basis for any employer.
Show more
Show less","Python, Go, SQL, Terraform, Jupyter, Git, GitLab, Spark, Flink, Presto, Kafka, MySQL, NoSQL, Kubernetes, DBT, Prefect, Airflow, AWS, EC2, EKS, Lambda, S3, RDS, DynamoDB, Aurora, Redshift, Athena, EMR, CloudSearch, Kinesis, API Gateway, Agile, DevOps, DataOps, Empathy, Kindness, Numpy, Pandas, Data warehouse platform, Redshift, Snowflake, Message queues, Event buses, Data catalogs, Schema registries","python, go, sql, terraform, jupyter, git, gitlab, spark, flink, presto, kafka, mysql, nosql, kubernetes, dbt, prefect, airflow, aws, ec2, eks, lambda, s3, rds, dynamodb, aurora, redshift, athena, emr, cloudsearch, kinesis, api gateway, agile, devops, dataops, empathy, kindness, numpy, pandas, data warehouse platform, redshift, snowflake, message queues, event buses, data catalogs, schema registries","agile, airflow, api gateway, athena, aurora, aws, cloudsearch, data catalogs, data warehouse platform, dataops, dbt, devops, dynamodb, ec2, eks, empathy, emr, event buses, flink, git, gitlab, go, jupyter, kafka, kindness, kinesis, kubernetes, lambda, message queues, mysql, nosql, numpy, pandas, prefect, presto, python, rds, redshift, s3, schema registries, snowflake, spark, sql, terraform"
Senior Data Engineer,Emergent Staffing,"Hennepin County, MN",https://www.linkedin.com/jobs/view/senior-data-engineer-at-emergent-staffing-3782574784,2023-12-17,Lake Forest,United States,Mid senior,Remote,"This is a direct-hire, fully remote opportunity with our client. Candidates must be able to work in the US without sponsorship.**
Join the team to contribute to the development and maintenance of our data analytics infrastructure and data warehouse architecture. We seek a skilled Data Engineer with a strong emphasis on SQL expertise and data warehouse architecture. Your role will focus on designing and architecting data warehouses in the Azure space, playing a crucial role in optimizing data infrastructure.
Responsibilities
Design, develop, and maintain scalable data pipelines and ETL processes for various data sources with a strong emphasis on SQL.
Build a framework of repeatable solutions for efficient data pipelines.
Work hands-on in an agile, cloud-centric data warehousing and reporting platform, emphasizing architecture and design.
Collaborate with stakeholders to translate requirements into effective insights via reports and visualizations.
Implement data models, database schemas, and transformation logic for efficient data storage and retrieval, with a focus on data warehouse architecture.
Perform data cleansing, quality assurance, and validation for accuracy and reliability.
Optimize data infrastructure, query performance, and data processing workflows within the Azure Microsoft technology stack.
Ensure data security, privacy, and compliance with regulations and best practices.
Monitor and troubleshoot data pipeline issues, resolve anomalies, and perform root cause analysis.
Stay updated on emerging trends and technologies in business intelligence and data analytics, especially in the Azure space.
Maintain query performance and tuning for cost optimization.
Requirements
Proven experience as a Data Engineer with a strong emphasis on SQL, large-scale data pipelines, and data warehouse architecture.
Solid understanding of data warehouse concepts, dimensional modeling, and schema design principles.
Experience with data extraction from systems like SAP, Salesforce, and Azure SQL Server.
Familiarity with Azure data integration and orchestration tools (e.g., Data Factory, Databricks, Synapse Analytics).
Knowledge of Azure analytics services for advanced data analytics and insights.
Experience with CI/CD tools for managing and deploying data integration code.
Strong analytical and problem-solving skills with attention to detail.
Excellent communication and collaboration skills.
Significant experience developing data pipelines using ETL/ELT tools.
Experience with cloud data lake/warehouse solutions and data integration tools.
Experience building complex Analytics and Reporting solutions (e.g., Power BI, Tableau, Looker).
Nice to have: experience with SAP.
Our Vetting Process
At Emergent Staffing, we work hard to find candidates who are the right fit for our clients. Here are the steps of our vetting process for this position:
Application (5 minutes)
Online Assessment (40 minutes)
Initial Phone Interview (30-45 minutes)
2-3 Interviews with the Client
Job Offer!
#EmergentStaffing
Show more
Show less","Data Engineering, SQL, Data Warehouse Architecture, Azure, Data Pipelines, ETL, Data Modeling, Dimensional Modeling, Schema Design, SAP, Salesforce, Azure SQL Server, Data Factory, Databricks, Synapse Analytics, Azure Analytics Services, CI/CD, Data Integration, Data Lake, Data Warehouse, Analytics, Reporting, Power BI, Tableau, Looker","data engineering, sql, data warehouse architecture, azure, data pipelines, etl, data modeling, dimensional modeling, schema design, sap, salesforce, azure sql server, data factory, databricks, synapse analytics, azure analytics services, cicd, data integration, data lake, data warehouse, analytics, reporting, power bi, tableau, looker","analytics, azure, azure analytics services, azure sql server, cicd, data engineering, data factory, data integration, data lake, data warehouse architecture, databricks, datamodeling, datapipeline, datawarehouse, dimensional modeling, etl, looker, powerbi, reporting, salesforce, sap, schema design, sql, synapse analytics, tableau"
Senior Data Analytics Engineer,When I Work,"Minneapolis, MN",https://www.linkedin.com/jobs/view/senior-data-analytics-engineer-at-when-i-work-3785765740,2023-12-17,Lake Forest,United States,Mid senior,Remote,"When I Work is a remote first company. We are open to hiring candidates in the continental US and Ontario, Canada. If an onsite location is important to you in your search, you are welcome to work from our Minneapolis HQ office.
Who We Are
We help hourly teams get shift done.
At When I Work, everything we do starts with a mission to make shift work awesome. We deliver on that mission by making every piece of hourly workforce management - scheduling, time tracking, shift trading, team messaging, and more - easy and straightforward for managers and employees alike.
The Data and RevOps team at When I Work is a group of inquisitive and driven individuals who love solving problems using data. We have built a best-in-class data environment and fuel insights throughout the organization on our product and customers. We work collaboratively together, invest in our processes and tooling, and move slow to move fast. We focus on projects that will have a big impact to the company and work to enable anyone to be a savvy data user.
What You'll Do
Over the last few years we have been building out a best-in-class data environment that we've used to transform When I Work into a data-driven company. You will be a key contributor extracting value from this environment as well as develop and sustain data projects and analysis that will have significant impact to our company and our users.
Proactively identify opportunities to build out solutions or use internally stored data to provide value to the business
Design and implement scalable ETLs
Own end to end design and implementation of analytical project scope to turn raw data into actionable insight
Work closely with internal teams across the business (Product, GTM, etc.) to understand their analytical needs and help empower them to find success in their initiatives through data
Clearly articulate analytical findings to both technical and non-technical stakeholders
Work with data visualization tools to help end users answer pertinent questions
Make use of statistical modeling / machine learning in order to advance business outcomes
Who You Are
As a Senior Data Analytics Engineer, you are eager to use data to solve business problems and guide data-driven decision making across an entire organization. You enjoy the end to end process of data analytics. You are a maker at heart who enjoys being in the code while also uncovering new product and business opportunities through whatever methods and tools are right for the task. You enjoy collaborating with a team, but you also have the ability to work independently when needed to get things done. Above all, you are broadly curious, driven to learn and a motivated problem solver who wants to help tackle the new and interesting challenges that we encounter as a fast-growing startup.
Experience And Skills Needed
3+ years of experience in data science or data analytics
You have strong programming fundamentals
You are comfortable with agile, collaborative coding processes
You can distinguish signal from noise in complex data sets
You have experience with Python or another, similar programming language
You have solid, hygienic SQL skills
You have experience modeling data to support both consistent and generic usage patterns
You have significant experience with both structured and unstructured data
Experience modeling data within OLAP relational data stores
You have familiarity with cloud computing environments and infrastructure
You have experience taking technical analytics work and presenting in a form that non-technical users are able to glean the significance of your findings
You practice empathy and kindness, and you look to help others
What Would Be Awesome To Have
Advanced Python and data package (Numpy, Pandas, etc.) skills
You are a proponent of DevOps and enthusiastic about DataOps
You are comfortable with different data modeling techniques (Star Schema, Snowflake, DV2, etc.)
Have experience with a data warehouse platform (Redshift, Snowflake, etc.)
Experience with S3 based data lake and parquet file format
What's In It For You
Professional development allowance
Paid parental leave
Medical benefits - employee premiums paid 100% by When I Work
Dental benefits- employee premiums paid 100% by When I Work
Paid vacation and holidays
Flexible work environment
401K Match
Remote first culture including home office set-up stipend and ongoing telecommuter stipend
Casual dress code
Dynamic and dedicated team
We believe actions speak louder than words. Every encounter with our people and products should be memorable and helpful. Challenges are exciting, failure is how we learn, and we all have an entrepreneurial spirit. Building an inclusive and equitable workplace isn't lip service. We invest our time and our money in organizations that are not only working to diversify the current jobscape, but also investing in the future of talent. We're motivated by a strong, innovative, and passionate work culture and we're constantly searching for ways to improve and get shift done.
Whether you're a perfect match or not, if it sounds like a good fit, we encourage you to apply.
The tech industry is notorious for its lack of diverse representation, and we're aware of the research showing that historically underrepresented groups are less likely to apply to a job if they don't believe that they meet all of the criteria. Are you hesitant to submit an application because you're not sure if you check every box? Apply anyway! We would love to hear from you and figure out what you can add to the culture here at When I Work.
We'd love to talk to you! Please submit the following to apply:
Resume (including months/years of employment for each position).
Cover letter including:
an overview of your existing experience
a convincing reason why you'd like to work at When I Work.
Must already be authorized to work in the United States or Canada on a full-time basis for any employer.
Show more
Show less","Data Analytics, Data Science, ETL, Python, SQL, Modeling, Data Warehousing, Data Visualization, Statistical Modeling, Machine Learning, AWS, DataOps, Data Lake, Parquet, Redshift, Snowflake","data analytics, data science, etl, python, sql, modeling, data warehousing, data visualization, statistical modeling, machine learning, aws, dataops, data lake, parquet, redshift, snowflake","aws, data lake, data science, dataanalytics, dataops, datawarehouse, etl, machine learning, modeling, parquet, python, redshift, snowflake, sql, statistical modeling, visualization"
Data Engineer - Scala(U.S. remote),Railroad19,"Minneapolis, MN",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782290882,2023-12-17,Lake Forest,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, EMR, S3, Restful APIs, Relational databases, Nonrelational databases, Computer science, Computer engineering","scala, spark, aws, emr, s3, restful apis, relational databases, nonrelational databases, computer science, computer engineering","aws, computer engineering, computer science, emr, nonrelational databases, relational databases, restful apis, s3, scala, spark"
"Data Conversion Developer, Senior Associate",PwC,"Minneapolis, MN",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749938485,2023-12-17,Lake Forest,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Data Analysis, Data Cleansing, Data Conversion, Data Extraction, Data Integration, Data Loading, Data Migration, Data Mining, Data Modeling, Data Quality, Data Transformation, Database Administration, Database Design, Database Development, Database Management, Database Programming, ETL, Maximo, PowerPlant, Python, PySpark, Scala, SQL, SSIS, XML, JSON, Data Engineering, Data Architecture, Programming Languages, Azure ADF, AWS Glue, Azure Data Engineer Associate, Databricks Certified Data Engineer Associate","data analysis, data cleansing, data conversion, data extraction, data integration, data loading, data migration, data mining, data modeling, data quality, data transformation, database administration, database design, database development, database management, database programming, etl, maximo, powerplant, python, pyspark, scala, sql, ssis, xml, json, data engineering, data architecture, programming languages, azure adf, aws glue, azure data engineer associate, databricks certified data engineer associate","aws glue, azure adf, azure data engineer associate, data architecture, data conversion, data engineering, data extraction, data integration, data loading, data migration, data mining, data quality, data transformation, dataanalytics, database administration, database design, database development, database management, database programming, databricks certified data engineer associate, datacleaning, datamodeling, etl, json, maximo, powerplant, programming languages, python, scala, spark, sql, ssis, xml"
Data Engineer,DataDrive,"Minneapolis, MN",https://www.linkedin.com/jobs/view/data-engineer-at-datadrive-3779394597,2023-12-17,Lake Forest,United States,Mid senior,Remote,"Role Overview
As a Data Engineer, you will be part of a dynamic environment within our delivery function, helping clients solve complex business problems with data and delivering on automated data pipelines, reporting data sources, ELT and data modeling solutions, and ad-hoc training & support. We seek someone who genuinely enjoys connecting with people, developing data automation and pipelines with modern technologies, learning how data delivers actionable value within our clients’ organizations, and managing multiple cloud warehousing environments.
The primary responsibility will be supporting our managed analytics service clients through their data-driven journey and delivering measurable business value through data modeling, API integration, SQL scriptings, data pipeline development, and robust technical documentation. The data engineer will bridge the important gap between data applications and insightful business reports.
We have a fun, remote-first culture that celebrates individual differences. We believe in and are committed to creating a diverse, equitable, and inclusive workplace. We value those who embrace adventure, subscribe to life-long learning, and take ownership and pride in their work. In our professional-yet-relaxed environment, we want everyone to enjoy what they do, seek balance outside of work, and lean into an entrepreneurial mindset to help us and our clients grow in their analytics capabilities.
What You’ll Be Doing as a Data Engineer
Participate in building our data platform from the ground up by exploring new technologies and vendors within our cloud-first environment
Embrace an incremental learning and improvement mindset to always explore how to optimize
Thrive in an ambiguous environment and be well-organized, creative, and a strong listener
Be collaborative, with an optimistic style to problem-solving, relentlessly seek opportunities to go the extra mile for others
Communicate with internal and external audiences - presenting through strong written and verbal communication that resonates with audiences
Support requirements gathering and note-taking during client interactions - including new data insights requests and ongoing client project check-ins
Troubleshoot and support resolution of client support tickets & data issues related to pipeline monitoring and data accuracy
Maintain multiple cloud data environments focused on best practices in user permissions, data security, and content organization
Align with our closely-held company values
Day-to-Day Responsibilities
Monitor, optimize, and integrate batch data source pipelines into Snowflake databases using ELT vendors (e.g. Fivetran, Airbyte) and Python integrations to databases and APIs
Build and maintain dbt pipelines to transform source data into reporting data models for our visualization team
Set up easy-to-use data sources in Tableau for reporting developers to build out dashboards
Code orchestration pipelines leveraging Python to connect data ingestion, Snowflake, dbt, Tableau
Design and implement Data Ops practices to make our lives easier using Github actions, dbt testing, and/or data observability/monitoring tools
Identify where we can incrementally improve our platform and build the automation to accomplish it
Stay informed on constantly evolving cloud warehousing and ELT ecosystems
Commit to strong application SDLC concepts during development
In a growing small company rooted in analytics, change is the only constant. A candidate for this role should be adaptable and excited about the opportunity to ‘wear many hats’ as every member plays an influential role in making DataDrive an amazing place to work and grow together. We want you to embrace the adventure!
The Data Engineer role is a full-time, salaried role. We are unable to sponsor visas at this time.
What We Look For
4+ years professional experience in building cloud data pipelines and data warehousing within a cloud data warehouse (ideally Snowflake)
High proficiency and experience working with SQL and Python
Experience in designing data models to support reporting and business outcomes
Experience or a strong desire to learn AWS, Snowflake, and dbt
Familiarity with Prefect/Airflow, Fivetran, Tableau, Terraform, Github Actions
Ability to self-manage to requirements, anticipate needs, and perform necessary quality assurance steps to ensure high-quality deliverables
Comfortable handling ambiguity & managing multiple assignments
Strong ability to communicate complex ideas effectively (verbal and written) with people at all levels of the organization, including senior management
Additional ‘nice-to-have’ knowledge and experience includes
Office productivity tools (e.g. Google Workspace)
Project management tools (e.g. Monday.com)
Familiarity with Tableau Cloud and administration best practices
Master’s degree in data analytics-related field
We recognize that there is no such thing as a perfect candidate. We embrace professional and personal growth - so however you identify and no matter what your experience level, background, or education is, please apply if this role would make you excited to come to work every day!
Anticipated Schedule & Workplace Expectations
DataDrive is a remote-first company that embraces a flexible, virtual workplace. We expect employees to be generally available for meetings, collaboration, client communication, and support Monday - Friday during normal business hours (9a - 4p CST).
To maintain connection in a virtual world with our team and clients, employees are expected to:
Maintain regular access to high-speed Internet connections
Turn on video/webcam for the majority of meetings
Dress appropriately for their day (e.g. business casual for external audiences)
About DataDrive
Founded in 2017, DataDrive is a fast-growing managed analytics service provider that delivers both cloud reporting platforms and ongoing training, adoption, and growth of our clients’ data strategy and maturity. DataDrive’s unique offering is a fully-managed reporting platform and fractional data team that scales with our clients - providing an opportunity to define and evolve a data strategy that creates a measurable impact beyond traditional consulting engagements.
DataDrive offers a team-oriented environment where one can develop their skills and work directly with some of the most talented analytics professionals in the business. We are a connected group who work hard, live well, care for others, & celebrate as a team.
We help our clients and ourselves win by showing up with the following values:
Embrace the Adventure
Win as a Team
Grow through Curiosity
Own the Outcome
DataDrive has been recognized as a ‘Best Place to Work’ by Minneapolis Business Journal.
Show more
Show less","Data engineering, Cloudfirst environment, ELT vendors, SQL, Python, dbt, Tableau, Github actions, Data observability/monitoring tools, AWS, Snowflake, Prefect/Airflow, Fivetran, Terraform, Google Workspace, Monday.com, Tableau Cloud, Business casual","data engineering, cloudfirst environment, elt vendors, sql, python, dbt, tableau, github actions, data observabilitymonitoring tools, aws, snowflake, prefectairflow, fivetran, terraform, google workspace, mondaycom, tableau cloud, business casual","aws, business casual, cloudfirst environment, data engineering, data observabilitymonitoring tools, dbt, elt vendors, fivetran, github actions, google workspace, mondaycom, prefectairflow, python, snowflake, sql, tableau, tableau cloud, terraform"
Analytics Consultant (Dataiku),phData,"Minneapolis, MN",https://www.linkedin.com/jobs/view/analytics-consultant-dataiku-at-phdata-3775779344,2023-12-17,Lake Forest,United States,Mid senior,Remote,"Join phData, a dynamic and innovative leader in the modern data stack. We partner with major cloud data platforms like Snowflake, AWS, Azure, GCP, Fivetran, and dbt to deliver cutting-edge services and solutions. We're committed to helping global enterprises overcome their toughest data challenges. Even though we're growing extremely fast, we maintain a casual, exciting work environment. We hire top performers and allow you the autonomy to deliver results.
4x Snowflake Partner of the Year (2020, 2021, 2022, 2023)
#1 Partner in Snowflake Advanced Certifications
600+ Expert Cloud Certifications (Fivetran, dbt, Sigma)
7x Best Places to Work
Inc 5000 Fastest Growing US Companies (2020-2023)
Overview:
We’re looking for a talented Senior Analytics Consultant to help our customers gain tangible value from their data. Our consulting services emphasize analytics enablement, data visualization, data preparation, and data science. We specialize in Tableau, PowerBI, Sigma, Alteryx, KNIME, Power Platform and Snowflake.
Responsibilities:
Deliver project-based consulting engagements to help clients develop analytical solutions; activities include requirements gathering, prototyping, analytics engineering, and visualization.
Manage clients expectations by leading weekly status reports to clients; proactively soliciting client feedback by running working sessions
Create reporting and data visualization solutions
Analyze, troubleshoot and / or tune product performance or deployment issues when required
Support or assist with backlog support and development
Identify and recommend functionality and solutions that would meet and enhance user experience
Required Experience:
3+ years of relevant experience and/or demonstrated expertise in data analytics and data visualization platforms
Expertise in Dataiku, plus if paired with other tools such as Alteryx, KNIME, or Matillion
Proven experience to create scalable, durable analytics solutions
Eagerness to learn new data visualization tools
Demonstrated expertise in data fluency and communicating to business users
Knowledge of SQL and experience leveraging data warehouse, data marts, and/or data models
Exceptional customer facing skills, including but not limited to communication and project management
Strong problem solving skills with a passion for learning and mastering new technologies, techniques, and procedures
Preferred Experience:
1+ years of Consulting experience
Experience with data visualization, ideally in one of the following: Tableau, Power BI, Sigma
Experience with Snowflake
Experience with Python and/or R scripting
Why phData? We offer?
Remote-First Work Environment
Casual, award-winning small-business work environment
Collaborative culture that prizes autonomy, creativity, and transparency
Competitive comp, excellent benefits, 4 weeks PTO plus 10 Holidays (and other cool perks)
Accelerated learning and professional development through advanced training and certifications
Show more
Show less","Snowflake, AWS, Azure, GCP, Fivetran, dbt, Sigma, Tableau, PowerBI, Alteryx, KNIME, Power Platform, Dataiku, Matillion, SQL, Data warehouse, Data marts, Data models, Python, R scripting, Consulting, Data visualization, Data fluency, Communication, Project management, Problem solving","snowflake, aws, azure, gcp, fivetran, dbt, sigma, tableau, powerbi, alteryx, knime, power platform, dataiku, matillion, sql, data warehouse, data marts, data models, python, r scripting, consulting, data visualization, data fluency, communication, project management, problem solving","alteryx, aws, azure, communication, consulting, data fluency, data marts, data models, dataiku, datawarehouse, dbt, fivetran, gcp, knime, matillion, power platform, powerbi, problem solving, project management, python, r scripting, sigma, snowflake, sql, tableau, visualization"
Staff Data Engineer,RVO Health,"Minneapolis, MN",https://www.linkedin.com/jobs/view/staff-data-engineer-at-rvo-health-3787307266,2023-12-17,Lake Forest,United States,Mid senior,Hybrid,"AT A GLANCE
RVO Health is looking for a talented Staff Data Engineer to join our team! As a Staff Data Engineer at RVO Health, you will have the chance to build technology that drives real improvements to consumer health outcomes and has the potential to have widespread impact across the healthcare industry. You will design, develop, test, and maintain big data pipelines for ingestion, segmentation, and reporting to drive our vision!
What You'll Do
Provide technology ownership for data solutions for projects that the team has been tasked with.
Work with a cross functional team of business analysts, architects, engineers, data analysts and data scientists to formulate both business and technical requirements.
Design and build data pipelines from various data sources to a target data warehouse using batch data load strategies utilizing cutting edge cloud technologies.
Conceptualizing and generating infrastructure that allows data to be accessed and analyzed effectively.
Documenting database designs that include data models, metadata, ETL specifications and process flows for business data project integrations.
Perform periodic code reviews and test plans to ensure data quality and integrity.
Provide input into strategies as they drive the team forward with delivery of business value and technical acumen.
Execute on proof of concepts, where appropriate, to help improve our technical processes.
Conduct Technical assessments and mentor junior team members
What We're Looking For
7-10 years of Data Engineering experience
4+ years of experience working on Spark (RDDs / Data Frames / Dataset API) using Scala/Python to build and maintain complex ETL pipelines.
4+ years of experience in the big data space
Experience in translating business requirements into technical data solutions on a large scale.
2+ years of experience working on AWS (Kinesis / Kafka / S3 / RedShift) or Azure.
Able to research and troubleshoot potential issues presented by stakeholders within the data ecosystem.
Experience with GitHub and CI/CD processes
Experience with Compute technologies like EMR and Databricks
Experience working job orchestration (eg., Airflow / AWS Step Function)
Experience with Data Modeling, Data warehousing
Working with Kubernetes is a plus
Strong analytical and interpersonal skills.
Enthusiastic, highly motivated and ability to learn quickly.
Able to work through ambiguity in a fast-paced, dynamically changing business environment.
Ability to manage multiple tasks at the same time with minimal supervision.
Pursuant to various state Fair Pay Acts, below is a summary of compensation elements for this role at the company. The following benefits are provided by RVO Health, subject to eligibility requirements.
Starting Salary: $150,000 - $200,000
Note actual salary is based on geographic location, qualifications and experience
Access to a Free Udemy for Business subscription—thousands of hours of learning content on hundreds of different subjects at your fingertips
Health Insurance Coverage (medical, dental, and vision)
Life Insurance
Short and Long-Term Disability Insurance
Flexible Spending Accounts
Paid Time Off
Holiday Pay
401(k) with match
Employee Assistance Program
Paid Parental Bonding Benefit Program
Who We Are
Founded in 2022, RVO Health is a new healthcare platform of digital media brands, services and technologies focused on building relationships with people throughout their health & wellness journey. We meet people where they are in their personal health journeys and connect them with both the information and the care they need. RVO Health was created by joining teams from both Red Ventures and UnitedHealth Group’s Optum Health. Together we’re focused on delivering on our vision of a stronger and healthier world.
RVO Health is comprised of Healthline Media (Healthline, Medical News Today, Psych Central, Greatist and Bezzy), Healthgrades, FindCare and PlateJoy; Optum Perks, Optum Store and the virtual coaching platforms Real Appeal, Wellness Coaching, and QuitForLife.
We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program.
RVO Health is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at RVO Health is based solely on a person's merit and qualifications.
We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodations@rvohealth.com.
RVO Health Privacy Policy: https://rvohealth.com/legal/privacy
Show more
Show less","Data Engineering, Spark, Scala, Python, ETL, Data Warehousing, AWS, Azure, Kinesis, Kafka, S3, RedShift, EMR, Databricks, Airflow, AWS Step Functions, Kubernetes, GitHub, CI/CD","data engineering, spark, scala, python, etl, data warehousing, aws, azure, kinesis, kafka, s3, redshift, emr, databricks, airflow, aws step functions, kubernetes, github, cicd","airflow, aws, aws step functions, azure, cicd, data engineering, databricks, datawarehouse, emr, etl, github, kafka, kinesis, kubernetes, python, redshift, s3, scala, spark"
Data Developer Principal - Investment Systems,Steneral Consulting,"Minneapolis, MN",https://www.linkedin.com/jobs/view/data-developer-principal-investment-systems-at-steneral-consulting-3766055159,2023-12-17,Lake Forest,United States,Mid senior,Hybrid,"Share 2 profiles only
Local candidates only with proof
USC/GC only
Insurance Investment Experience Is Must
Data Developer Principal - Investment Systems
Location: Greater Minneapolis-St. Paul Area
Position: - Investment Systems – Data Principal - Contract to Hire
Start Date: ASAP
Location:
Local candidates only! Candidate will be expected to be onsite at least 3 days/week
Must be local to Minneapolis, MN
Investment Systems – Data Principal
(Database Developer)
The Role
Responsible for assisting the front, middle and back-office investment teams with technology data system improvements, process improvements, project implementations and daily run activities. Accountable for driving increased efficiencies and improved access to data leveraged for investment decision making.
Essential Job Functions
Translate user requirements and identify new features. (20%)
Modify databases according to requests and perform tests. (10%)
Develop stable, reliable and effective databases. Ensure all database programs meet company and performance requirements. (50%)
Research, solve database usage issues and malfunctions, and suggest new database products, services and protocols. (20%)
Experience And Education
6-7 years of experience required: Finance or investment systems knowledge; Experience managing large-scale projects; SQL knowledge. Azure Cloud DB knowledge and programming expertise.
Four-year degree required: Finance, Business Administration, MIS/programming or other related field, specialized training or equivalent work experience.
Preferred Qualifications
8-10 years of experience preferred: Technical experience or support of – PAM investment accounting system, Black Rock Aladdin, and Bloomberg.
Advanced degree preferred.
Licensing/Certification/Other: CFA and or relevant technical certifications preferred.
Show more
Show less","SQL, Azure Cloud DB, PAM investment accounting system, Black Rock Aladdin, Bloomberg, Finance, Investment systems, Project management, Database development, Data analysis, CFA, MIS, Programming","sql, azure cloud db, pam investment accounting system, black rock aladdin, bloomberg, finance, investment systems, project management, database development, data analysis, cfa, mis, programming","azure cloud db, black rock aladdin, bloomberg, cfa, dataanalytics, database development, finance, investment systems, mis, pam investment accounting system, programming, project management, sql"
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"Minneapolis, MN",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-engineering-at-acv-auctions-3762880100,2023-12-17,Lake Forest,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","Data Engineering, SQL, ETL, SSIS, C#, Web APIs, Data modeling, Storage, Message brokers, Protocols, Interfaces, Unit testing, Integration testing, DBT, Cloud Services, AWS RDS, S3, SQS, SNS, NoSQL, MongoDB, OLTP, OLAP, Airflow, Snowflake","data engineering, sql, etl, ssis, c, web apis, data modeling, storage, message brokers, protocols, interfaces, unit testing, integration testing, dbt, cloud services, aws rds, s3, sqs, sns, nosql, mongodb, oltp, olap, airflow, snowflake","airflow, aws rds, c, cloud services, data engineering, datamodeling, dbt, etl, integration testing, interfaces, message brokers, mongodb, nosql, olap, oltp, protocols, s3, snowflake, sns, sql, sqs, ssis, storage, unit testing, web apis"
Sr. Data and Software Engineer,Ovative Group,"Minneapolis, MN",https://www.linkedin.com/jobs/view/sr-data-and-software-engineer-at-ovative-group-3762353291,2023-12-17,Lake Forest,United States,Mid senior,Hybrid,"About Ovative Group
Ovative Group is the premier independent media and measurement firm in the United States. We help change-makers, in fast-growing, customer-centric organizations across industries reinvent their marketing and measurement programs. We leverage our media, measurement, and consulting capabilities to help brands like Coach, Kate Spade, Stuart Weitzman, Facebook, The Home Depot, CVS, Disney, and UnitedHealth Group transform their media and marketing programs. Our proprietary approach to measuring and optimizing marketing investment decisions, Enterprise Marketing Return (EMR), is disrupting the industry and setting the gold standard for customer and marketing strategy, activation, and measurement.
Recognized eight consecutive years on Star Tribune’s list of Top 150 Workplaces and five years on Inc. 5000’s list of the fastest-growing private companies in America, we pride ourselves in always overdelivering for our clients, our teams, and our communities.
About The Role
We are seeking a Sr. Data & Software Engineer to join our rapidly growing product and engineering development team. Our company specializes in enterprise-level media measurement and optimization across various industries. In this role, you will take a leadership position within a cross-functional team responsible for creating, optimizing, and maintaining scalable software and data solutions to enhance performance, stability, and scalability. You will play a pivotal role in guiding the team and working closely with stakeholders throughout the entire software development lifecycle, from concept to deployment.
The ideal candidate will bring extensive experience in iterative development practices, deep knowledge of version control (e.g., GitHub), and both conceptual and pragmatic problem-solving skills. Your outstanding attention to detail and strong written and oral communication skills will enable you to work directly with a variety of users, understanding their objectives and translating them into technical requirements and solutions. You will have the opportunity to mentor junior team members, contribute to innovative ideas, and drive the adoption of cutting-edge technologies within the team and foster your own professional growth.
Responsibilities
:
Lead the design, development, testing, and deployment of robust software solutions that meet business and technical goals.
Lead effort in identifying opportunities for automation with a focus on the operational stability of software applications and systems
Engage directly with Product Managers and stakeholders to understand their business goals, gather requirements, and translate into detailed, actionable technical requirements
Research, write and edit documentation and technical requirements, including software designs, evaluation plans, test results, technical manuals and formal recommendations and reports
Evaluate trade-offs between correctness, robustness, performance and customer impact to ensure we build the right solutions that scale
Mentor and level up the skills of your teammates through collaboration and by sharing your expertise
Ensure adherence to and advancement of software and product engineering best practices, including code quality, documentation, testing, security, deployment, and performance.
Evaluate and integrate new technologies, tools, and frameworks to optimize the software development process.
Troubleshoot complex software issues and provide effective solutions.
Provide guidance, mentorship, and contribute to architectural decisions, code reviews, and project direction.
Foster a culture of continuous learning to keep your team current with emerging technology and software engineering trends.
Requirements
:
5+ years of relevant data & software engineering development experience
Highly proficient working with ETL/ELT tooling for large data sets (e.g., Airbyte)
Highly proficient utilizing SQL, Python, and command line
Experience with cloud-based platforms (i.e. GCP, AWS)
Preferred
:
Experience working with marketing, analytics and customer data
Experience working with APIs for data retrieval
Experience working with data warehouses and big data tools (e.g., BigQuery, Databricks)
Experience creating data/table architecture
Experience implementing QA processes and QA automation
Experience integrating data models within software
Pay Transparency
At Ovative, we offer a transparent view into three core components of your total compensation package: Base Salary, Annual Bonus, and Benefits. The salary range for this position below is inclusive of an annual bonus. Actual offers are made with consideration for relevant experience and anticipated impact. Additional benefits information is provided below.
For our Manager positions, our compensation ranges from $79,000 to $132,000, which is inclusive of a 20% bonus.
Benefits Of Working At Ovative Group
We provide strong, competitive, holistic benefits that understand the importance of your life inside and out of work.
Culture:
Culture matters and we’ve been recognized as a Top Workplace for eight years running because of it. We demand trust and transparency from each other. We believe in doing the hard and complicated work others put off. We’re open in communication and floor plan. We’re flat – our interns sit next to VPs, our analysts work closely with senior leaders, and our CEO interacts with every single person daily. Put together, these elements help foster an environment where smart people can support each other in performing to their highest potential.
Compensation and Insurance:
We strive to hire and retain the best talent. Paying fair, competitive compensation, with a large bonus incentive, and phenomenal health insurance is an important part of this mix.
We’re rewarded fairly and when the company performs well, we all benefit.
Tangible amenities we enjoy:
Access to all office spaces in MSP, NYC, and CHI
Frequent, paid travel to our Minneapolis headquarters for company events, team events, and in-person collaboration with teams.
Flexible paid vacation policy
401k match program
Top-notch health insurance options
Monthly stipend for your mobile phone and data plan
Sabbatical program
Charitable giving via our time and a financial match program
Shenanigan’s Day
Working at Ovative won’t be easy, but if you like getting your hands dirty, driving results, and being surrounded by the best talent, it’ll be the most rewarding job you’ll ever have. If you think you can make us better, we want to hear from you!
Show more
Show less","Data Engineering, Software Engineering, ETL/ELT tooling, Airbyte, SQL, Python, Cloud platforms, GCP, AWS, API integration, Data warehouses, BigQuery, Databricks, Data architecture, QA processes, QA automation, Data models","data engineering, software engineering, etlelt tooling, airbyte, sql, python, cloud platforms, gcp, aws, api integration, data warehouses, bigquery, databricks, data architecture, qa processes, qa automation, data models","airbyte, api integration, aws, bigquery, cloud platforms, data architecture, data engineering, data models, data warehouses, databricks, etlelt tooling, gcp, python, qa automation, qa processes, software engineering, sql"
Senior Cloud Data Engineer,BDO USA,"Minneapolis, MN",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765472150,2023-12-17,Lake Forest,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, Azure, AWS, SQL, Data Warehousing, Data Modeling, Star Schema Construction, Cloud Data Analytics, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, Computer Vision, .Net, Qlik, Tableau, Synapse, IoT, Microsoft SQL Server, SSIS, SSAS, SSRS, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, Athena, Data Pipeline, Glue, Snowflake, Databricks, AWS Lake Formation, Redshift, Kinesis, QuickSight, SageMaker, S3, UiPath","data analytics, business intelligence, artificial intelligence, application development, azure, aws, sql, data warehousing, data modeling, star schema construction, cloud data analytics, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, computer vision, net, qlik, tableau, synapse, iot, microsoft sql server, ssis, ssas, ssrs, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql, athena, data pipeline, glue, snowflake, databricks, aws lake formation, redshift, kinesis, quicksight, sagemaker, s3, uipath","ai algorithms, application development, artificial intelligence, athena, automation tools, aws, aws lake formation, azure, azure analysis services, batch data ingestion, bicep, business intelligence, c, cloud data analytics, computer vision, data lake medallion architecture, data ops, data pipeline, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops, git, glue, iot, java, kinesis, linux, machine learning, microsoft fabric, microsoft sql server, net, pandas, powerbi, purview, python, qlik, quicksight, redshift, s3, sagemaker, scala, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, streaming data ingestion, synapse, tableau, tabular modeling, terraform, uipath"
Sr. Data and Software Engineer,Ovative Group,"Minneapolis, MN",https://www.linkedin.com/jobs/view/sr-data-and-software-engineer-at-ovative-group-3762351451,2023-12-17,Lake Forest,United States,Mid senior,Hybrid,"About Ovative Group
Ovative Group is the premier independent media and measurement firm in the United States. We help change-makers, in fast-growing, customer-centric organizations across industries reinvent their marketing and measurement programs. We leverage our media, measurement, and consulting capabilities to help brands like Coach, Kate Spade, Stuart Weitzman, Facebook, The Home Depot, CVS, Disney, and UnitedHealth Group transform their media and marketing programs. Our proprietary approach to measuring and optimizing marketing investment decisions, Enterprise Marketing Return (EMR), is disrupting the industry and setting the gold standard for customer and marketing strategy, activation, and measurement.
Recognized eight consecutive years on Star Tribune’s list of Top 150 Workplaces and five years on Inc. 5000’s list of the fastest-growing private companies in America, we pride ourselves in always overdelivering for our clients, our teams, and our communities.
About The Role
We are seeking a Sr. Data & Software Engineer to join our rapidly growing product and engineering development team. Our company specializes in enterprise-level media measurement and optimization across various industries. In this role, you will take a leadership position within a cross-functional team responsible for creating, optimizing, and maintaining scalable software and data solutions to enhance performance, stability, and scalability. You will play a pivotal role in guiding the team and working closely with stakeholders throughout the entire software development lifecycle, from concept to deployment.
The ideal candidate will bring extensive experience in iterative development practices, deep knowledge of version control (e.g., GitHub), and both conceptual and pragmatic problem-solving skills. Your outstanding attention to detail and strong written and oral communication skills will enable you to work directly with a variety of users, understanding their objectives and translating them into technical requirements and solutions. You will have the opportunity to mentor junior team members, contribute to innovative ideas, and drive the adoption of cutting-edge technologies within the team and foster your own professional growth.
Responsibilities
:
Lead the design, development, testing, and deployment of robust software solutions that meet business and technical goals.
Lead effort in identifying opportunities for automation with a focus on the operational stability of software applications and systems
Engage directly with Product Managers and stakeholders to understand their business goals, gather requirements, and translate into detailed, actionable technical requirements
Research, write and edit documentation and technical requirements, including software designs, evaluation plans, test results, technical manuals and formal recommendations and reports
Evaluate trade-offs between correctness, robustness, performance and customer impact to ensure we build the right solutions that scale
Mentor and level up the skills of your teammates through collaboration and by sharing your expertise
Ensure adherence to and advancement of software and product engineering best practices, including code quality, documentation, testing, security, deployment, and performance.
Evaluate and integrate new technologies, tools, and frameworks to optimize the software development process.
Troubleshoot complex software issues and provide effective solutions.
Provide guidance, mentorship, and contribute to architectural decisions, code reviews, and project direction.
Foster a culture of continuous learning to keep your team current with emerging technology and software engineering trends.
Requirements
:
5+ years of relevant data & software engineering development experience
Highly proficient working with ETL/ELT tooling for large data sets (e.g., Airbyte)
Highly proficient utilizing SQL, Python, and command line
Experience with cloud-based platforms (i.e. GCP, AWS)
Preferred
:
Experience working with marketing, analytics and customer data
Experience working with APIs for data retrieval
Experience working with data warehouses and big data tools (e.g., BigQuery, Databricks)
Experience creating data/table architecture
Experience implementing QA processes and QA automation
Experience integrating data models within software
Pay Transparency
At Ovative, we offer a transparent view into three core components of your total compensation package: Base Salary, Annual Bonus, and Benefits. The salary range for this position below is inclusive of an annual bonus. Actual offers are made with consideration for relevant experience and anticipated impact. Additional benefits information is provided below.
For our Manager positions, our compensation ranges from $79,000 to $132,000, which is inclusive of a 20% bonus.
Benefits Of Working At Ovative Group
We provide strong, competitive, holistic benefits that understand the importance of your life inside and out of work.
Culture:
Culture matters and we’ve been recognized as a Top Workplace for eight years running because of it. We demand trust and transparency from each other. We believe in doing the hard and complicated work others put off. We’re open in communication and floor plan. We’re flat – our interns sit next to VPs, our analysts work closely with senior leaders, and our CEO interacts with every single person daily. Put together, these elements help foster an environment where smart people can support each other in performing to their highest potential.
Compensation and Insurance:
We strive to hire and retain the best talent. Paying fair, competitive compensation, with a large bonus incentive, and phenomenal health insurance is an important part of this mix.
We’re rewarded fairly and when the company performs well, we all benefit.
Tangible amenities we enjoy:
Access to all office spaces in MSP, NYC, and CHI
Frequent, paid travel to our Minneapolis headquarters for company events, team events, and in-person collaboration with teams.
Flexible paid vacation policy
401k match program
Top-notch health insurance options
Monthly stipend for your mobile phone and data plan
Sabbatical program
Charitable giving via our time and a financial match program
Shenanigan’s Day
Working at Ovative won’t be easy, but if you like getting your hands dirty, driving results, and being surrounded by the best talent, it’ll be the most rewarding job you’ll ever have. If you think you can make us better, we want to hear from you!
Show more
Show less","SQL, Python, Command line, ETL/ELT tooling, Airbyte, Cloudbased platforms, GCP, AWS, Marketing, Analytics, Customer data, APIs, Data retrieval, Data warehouses, Big data tools, BigQuery, Databricks, Data/table architecture, QA processes, QA automation, Data models, Software","sql, python, command line, etlelt tooling, airbyte, cloudbased platforms, gcp, aws, marketing, analytics, customer data, apis, data retrieval, data warehouses, big data tools, bigquery, databricks, datatable architecture, qa processes, qa automation, data models, software","airbyte, analytics, apis, aws, big data tools, bigquery, cloudbased platforms, command line, customer data, data models, data retrieval, data warehouses, databricks, datatable architecture, etlelt tooling, gcp, marketing, python, qa automation, qa processes, software, sql"
Lead Data Analyst with MySQL & Java,Zortech Solutions,"Minneapolis, MN",https://www.linkedin.com/jobs/view/lead-data-analyst-with-mysql-java-at-zortech-solutions-3750896895,2023-12-17,Lake Forest,United States,Mid senior,Hybrid,"Role: Lead Data Analyst with MySQL & Java
Location: Minneapolis, MN/Hybrid/Remote
Duration: 6+ Months
Experience: 10+ years
Must Have Skills
Data Analyst.
MySQL.
Java.
Support expertise
Show more
Show less","Data Analyst, MySQL, Java","data analyst, mysql, java","dataanalytics, java, mysql"
"IT Staff Engineer, Data Architect - Legence",Legence,"Minneapolis, MN",https://www.linkedin.com/jobs/view/it-staff-engineer-data-architect-legence-at-legence-3774672925,2023-12-17,Lake Forest,United States,Mid senior,Hybrid,"Legence , a Blackstone portfolio company, is an Energy Transition Accelerator™ that provides advisory services and implementation focused on financing, designing, building, and servicing complex systems in mission-critical and high performance facilities. With five decades of expertise in the built environment, Legence has a proven track record of reducing carbon emissions, implementing renewables, lowering utility costs through efficiency consumption, and making systems run better at unmatched speed and scale.
This is an IT staff engineer role that is responsible for enterprise data architecture, design, standards, modeling, and data management.
This role will include collaboration with multiple stakeholders, both business and IT. This role may include 20% of travel.
Responsibilities:
Build data models for database structures, analytics, and AI applications.
Design, document, build, and implement database architectures and applications.
Develop measures that ensure data accuracy, integrity, and accessibility.
Monitor, refine, and report enterprise data management system performance.
Develop and enforce enterprise database development standards.
Develop enterprise data strategy in collaboration with IT and company management.
Create and maintain an inventory of the enterprise data.
Build and maintain enterprise data pipelines.
Maintain data architecture procedures and artifacts in our enterprise repository.
Skills:
Relational database management systems, SQL Server, Oracle, PostgreSQL
Data modeling, migration, and visualization
Proficiency with SQL and modern programming languages
Data management and reporting technologies
Experience with both structured and unstructured data
Information management and data processing
Enterprise resource planning systems experience
Machine learning, predictive modeling, and natural language processing
Power BI experience
Hadoop and MapReduce experience is a plus.
Very good communication skills, including technical writing.
Applied mathematics and statistics skills.
Education:
Bachelor’s degree in computer science, management information systems, mathematics, statistics, or related field.
Experience:
At least 5 years in data architect/analyst/engineer roles in a large enterprise setting.
We are unable to provide immigration sponsorship for this position.
Health And Welfare Benefits
Health and Welfare
Medical
Dental
Vision
Prescription drug
Employee assistance program
Personal Benefits
Paid vacation
Company-paid holidays
Sick leave
Bereavement leave
Jury duty
Financial Benefits
401(k) retirement savings plan
Company-paid long-term disability insurance
AD&D insurance
Life insurance
Contingent Employment Statement
Offers of employment for this role may be contingent upon successfully passing a background check and/or drug screen. Execution of screens will vary based on role requirements and Company policy. All background checks and drug screens will be done in accordance with applicable federal, state, or local law.
Equal Employment Opportunity Employer
Legence and its affiliate companies are proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, religion, sex (including pregnancy, gender identity, and sexual orientation), parental status, national origin, age, disability, genetic information (including family medical history), political affiliation, military service, other non-merit-based factors, and any other characteristic protected under applicable local, state or federal laws and regulations.
EEO is the Law
Reasonable Accommodations
If you require assistance applying online, email ta@wearelegence.com . Please include a description of the specific accommodations you are requesting as well as the job title and requisition number of the position for which you are applying. If you are selected for an interview, please notify your recruiter of your accommodation needs. All efforts to provide reasonable accommodations will be made.
To all recruitment agencies
Legence and its affiliate companies do not accept unsolicited agency resumes. Do not forward resumes to our career’s alias or employees of Legence and/or its affiliate companies. Legence and/or its affiliate companies are not responsible for any fees related to unsolicited resumes. Any third-party recruiting agreements for Legence and its affiliate companies may only be executed by Legence Holdings LLC’s CHRO or Director of Talent Acquisition, without exception. All others are done without proper authorization and will not be honored. We will not be responsible for any fees under any third-party recruiting agreement not executed by said authority.
Pay Transparency Nondiscrimination Provision
Legence and its affiliate companies will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)
Apply Now
Show more
Show less","SQL Server, Oracle, PostgreSQL, SQL, Programming languages, Data management, Reporting technologies, Structured data, Unstructured data, Information management, Data processing, Enterprise resource planning systems, Machine learning, Predictive modeling, Natural language processing, Power BI, Hadoop, MapReduce, Communication skills, Technical writing, Applied mathematics, Statistics, Data modeling, Data pipelines, Data architecture, Data migration, Data visualization","sql server, oracle, postgresql, sql, programming languages, data management, reporting technologies, structured data, unstructured data, information management, data processing, enterprise resource planning systems, machine learning, predictive modeling, natural language processing, power bi, hadoop, mapreduce, communication skills, technical writing, applied mathematics, statistics, data modeling, data pipelines, data architecture, data migration, data visualization","applied mathematics, communication skills, data architecture, data management, data migration, data processing, datamodeling, datapipeline, enterprise resource planning systems, hadoop, information management, machine learning, mapreduce, natural language processing, oracle, postgresql, powerbi, predictive modeling, programming languages, reporting technologies, sql, sql server, statistics, structured data, technical writing, unstructured data, visualization"
"Lead Engineer - Big Data Platform/Infra (Hadoop, Spark Streaming, Druid)",Jobs for Humanity,"Brooklyn Park, MN",https://www.linkedin.com/jobs/view/lead-engineer-big-data-platform-infra-hadoop-spark-streaming-druid-at-jobs-for-humanity-3784529677,2023-12-17,Lake Forest,United States,Mid senior,Hybrid,"Company Description
Jobs for Humanity is partnering with Target to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Target
Job Description
Job Title: Lead Engineer - Big Data Platform** **Location:** 7000 Target Pkwy N, Brooklyn Park, Minnesota, United States, 55445 **Salary Range:** $109,000.00 - $196,200.00 **Benefits:** At Target, we care about your well-being and offer comprehensive health benefits including medical, vision, dental, and life insurance. We also provide financial benefits like 401(k), employee discount, short and long term disability, paid sick leave, national holidays, and vacation. Learn more about our competitive benefits at [https://corporate.target.com/careers/benefits](https://corporate.target.com/careers/benefits). --- **About Target:** Target is a Fortune 50 company and one of America's leading retailers, with over 350,000 team members worldwide. We prioritize caring for our communities and creating growth opportunities for our employees. Our purpose is to make everyday life joyful for all families. --- **About the Role:** As a Lead Engineer in the High Performance Distributed Computing team at Target, you will play a crucial role in creating platforms and tools for data-based decision making. You will work on large scale distributed computing, focusing on data analytics and artificial intelligence/machine learning applications. Your work will personalize the guest experience, optimize supply chain, detect fraud, and improve overall guest satisfaction. --- **Responsibilities:** - Understand Target's business and technical environments, and help resolve complex challenges through technical solutions. - Collaborate with technical staff and Enterprise Architecture teams to set technical direction and communicate standards to the engineering team. - Participate in procurement, installation, and maintenance of Target systems. - Design and develop the Target platform API, prioritizing non-functional requirements like scalability and performance. - Promote extreme agile and DevOps practices among the engineering team. --- **Qualifications:** - Bachelor's or Master's degree in Computer Science or relevant experience. - 5+ years of software application development experience. - Detailed knowledge of GNU/Linux operating system administration. - Proficiency in Java programming. - Familiarity with concepts like operating system architecture, networking, benchmarking, and resource management. - Experience with technologies like Hadoop, Spark, HDFS, Hive, ZooKeeper, Ozone, and Trino/PrestoSQL. - Strong understanding of high-performance, large-scale system architecture design. - Familiarity with modern CI/CD technologies like Git, Docker, and Artifactory. - Ability to translate business goals into technical strategies. --- **Work Arrangement:** This position will operate as a Hybrid/Flex for Your Day work arrangement. This means that the core role will require both onsite and virtual work, depending on the tasks and team requirements. Work duties cannot be performed outside of the country of the primary work location. --- **Americans with Disabilities Act (ADA):** Target is committed to providing reasonable accommodations during the application process as required by applicable laws. If you have a disability and need assistance, please contact your nearest Target store or Supply Chain Facility, or reach out to Guest Services at 1-800-440-0680. --- Please let us know if you have any questions or require further information. We look forward to considering your application for this exciting opportunity!
Show more
Show less","Java, Linux, Hadoop, Spark, HDFS, Hive, ZooKeeper, Ozone, Trino/PrestoSQL, Git, Docker, Artifactory, DevOps, Operating system architecture, Networking, Benchmarking, Resource management, Highperformance largescale system architecture design, CI/CD technologies","java, linux, hadoop, spark, hdfs, hive, zookeeper, ozone, trinoprestosql, git, docker, artifactory, devops, operating system architecture, networking, benchmarking, resource management, highperformance largescale system architecture design, cicd technologies","artifactory, benchmarking, cicd technologies, devops, docker, git, hadoop, hdfs, highperformance largescale system architecture design, hive, java, linux, networking, operating system architecture, ozone, resource management, spark, trinoprestosql, zookeeper"
PeopleSoft HR Data Analyst,Diverse Lynx,"Minneapolis, MN",https://www.linkedin.com/jobs/view/peoplesoft-hr-data-analyst-at-diverse-lynx-3764424171,2023-12-17,Lake Forest,United States,Mid senior,Hybrid,"We are in need of a PeopleSoft HR data analyst to support the daily reconciliation process between PeopleSoft HR and Oracle HCM. Understanding of PeopleSoft HR data and tables, review, and analysis of large data extracts in Excel, identify issues and find data resolution approaches. Loading data to PeopleSoft HR an Oracle HCM Core HR with the focus on PeopleSoft HR.
The Role is predominantly a People Soft HR skill set with Excel analysis skills. Oracle HCM Cloud HR is a secondary skill set as the consultant will gain exposure to the Oracle module but should be content working in PeopleSoft HR for the majority of the role.
5 – 6 years PeopleSoft HR Experience
5- 6 years working in Data analysis using Excel
Less than 1 year Oracle HCM experience
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","PeopleSoft HR, Oracle HCM Cloud HR, Excel, Data analysis","peoplesoft hr, oracle hcm cloud hr, excel, data analysis","dataanalytics, excel, oracle hcm cloud hr, peoplesoft hr"
Tech Lead (Database Analyst),"ICONIC Infosys, Inc","Minneapolis, MN",https://www.linkedin.com/jobs/view/tech-lead-database-analyst-at-iconic-infosys-inc-3781156314,2023-12-17,Lake Forest,United States,Mid senior,Hybrid,"This position is for a Sr. Data engineer with primary role is Oracle database administration and must have enthusiastic in gaining experience with other DBMS technologies like MySQL, SQL Server, DB2, Postgres.
Role And Responsibilities
8+ Years experience in database administration, operations, and production support
Candidate must have excellent knowledge in resolving performance tuning issues.
Candidate must have experience in Monitoring, back up/ Recovery Management, DB Provisioning/de-provisioning, DB refresh/restore, HA/DRE, Patching, migration, replication and upgrades Understanding of Incident Management, Problem Management and Change Management Experience in ticketing tools such as Service Now.
Ability to document solutions and train operational teams.
Experience in administering MySQL database is a plus.
Experience with other DBMS platform(SQL Server/DB2/Postgress) will be an added advantage.
Experience in MySQL, Oracle Enterprise Manager (OEM) Cloud Control, SQL Server, Postgres, DB2 is a plus.
Excellent written and verbal communication.
Flexible, team player, get-it-done personality.
Ability to multi-task and context-switch effectively between different activities and teams Certifications in this area is plus.
Key Skills
Oracle DBA Oracle database administration, operations and production support Logical approach to work and ability to prioritize the tasks.
Experience in database and application performance tuning Experience in Monitoring, back up/ Recovery Management, DB Provisioning/de-provisioning, DB refresh/restore, HA/DRE, Patching, migration, replication, and upgrades.
Understanding of Incident Management, Problem Management and Change Management Experience in MySQL, Oracle Enterprise Manager (OEM) Cloud Control, SQL Server, Postgres, DB2 is a plus.
Show more
Show less","Oracle, MySQL, SQL Server, DB2, Postgres, Data administration, Performance tuning, Monitoring, Back up/Recovery Management, DB Provisioning/Deprovisioning, DB refresh/restore, HA/DRE, Patching, Migration, Replication, Upgrades, Incident Management, Problem Management, Change Management, Ticketing tools, MySQL Enterprise Manager (OEM) Cloud Control, Logical approach to work, Prioritization, Multitasking","oracle, mysql, sql server, db2, postgres, data administration, performance tuning, monitoring, back uprecovery management, db provisioningdeprovisioning, db refreshrestore, hadre, patching, migration, replication, upgrades, incident management, problem management, change management, ticketing tools, mysql enterprise manager oem cloud control, logical approach to work, prioritization, multitasking","back uprecovery management, change management, data administration, db provisioningdeprovisioning, db refreshrestore, db2, hadre, incident management, logical approach to work, migration, monitoring, multitasking, mysql, mysql enterprise manager oem cloud control, oracle, patching, performance tuning, postgres, prioritization, problem management, replication, sql server, ticketing tools, upgrades"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Provo, UT",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744391818,2023-12-17,Heber City,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Agile Engineering, ETL, Kafka, Storm, SparkStreaming, Data Modeling, Schema Design, Data Warehousing","data engineering, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, agile engineering, etl, kafka, storm, sparkstreaming, data modeling, schema design, data warehousing","agile engineering, airflow, data engineering, datamodeling, datawarehouse, docker, etl, helm, kafka, kubernetes, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Java Developer/Data Engineer (Hybrid),ConsultNet,"Pleasant Grove, UT",https://www.linkedin.com/jobs/view/senior-java-developer-data-engineer-hybrid-at-consultnet-3762469787,2023-12-17,Heber City,United States,Mid senior,Onsite,"Senior Java Developer/Data Engineer (Hybrid)
Utah County, UT
Contract to Hire
Contract Rate: $65-$75 per hour
Salary Target: $120 – 140k/year DOE
Our client is in search of a Senior Backend Java developer to innovate and develop solutions around the processing of large data sets. This position will require deep knowledge of data pipelines to build custom solutions using Java, Linux and Apache NiFi in a legacy monolith and transitioning them to a more open source, microservice environment.
Required Skills
5&plus; years' experience in Java (v8&plus;)
Experience with Apache NiFi and ETL
Experience in MySQL as well as NoSQL Environments (Mongo/Elastic)
Experience working in large data ingestion environments
Deep Linux experience
Bachelors Degree or higher in computer science
Bonus Skills
Python development experience
Experience with Docker, Kubernetes
Experience moving a monolith to micro services
Be a part of the ConsultNet difference. As a leading national provider of IT staffing and solutions, ConsultNet delivers exceptional services to startup, midmarket and Fortune 1000 companies across North America. Since 1996, we've partnered with clients to create rewarding opportunities for our consultants, successfully building teams that have surefire results.
In the past two years alone, we have placed more than 1,500 consultants in contract, contract-to-hire, or direct placement opportunities. We understand communication is key to finding the right job that matches your skills and career goals. For us, it's not just the work that we do; it's how we do the work. Our breadth of offerings extends to multiple IT positions in major markets throughout the country, see more at - www.consultnet.com
Show more
Show less","Java, Apache NiFi, ETL, MySQL, NoSQL, MongoDB, Elasticsearch, Linux, Docker, Kubernetes, Python","java, apache nifi, etl, mysql, nosql, mongodb, elasticsearch, linux, docker, kubernetes, python","apache nifi, docker, elasticsearch, etl, java, kubernetes, linux, mongodb, mysql, nosql, python"
Senior Data Engineer,Juniper Square,"Vancouver, British Columbia, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-juniper-square-3705283924,2023-12-17,Burnaby, Canada,Mid senior,Remote,"About Juniper Square
Our mission is to unlock the full potential of private markets. Privately owned assets like commercial real estate, private equity, and venture capital make up half of our financial ecosystem yet remain inaccessible to most people. We are digitizing these markets, and as a result, bringing efficiency, transparency, and access to one of the most productive corners of our financial ecosystem. If you care about making the world a better place by making markets work better through technology – all while contributing as a member of a values-driven organization – we want to hear from you.
GP Experience
Juniper Square serves two sides of the private capital markets, the investment managers (GPs) and the investors (LPs). The GP eXperience team (i.e., GPX) is responsible for Juniper Square’s product offering for General Partners (GPs). This is our core product that enables all other innovation at Juniper Square as we unlock and improve the world’s private capital markets. Our platform handles billions of dollars of transactions each month and we are actively expanding into additional private asset classes such as Venture Capital& Private Equity. Come help us innovate in fundraising, reporting, asset-ownership mapping, and more.
The Team
The Data Engineering team is responsible for Data pipelines that serve multiple types of customers including internal Juniper Square users for Business Intelligence and GPs for Analytics on their data. We support the ability for these customers to create and manage their custom dashboards. We also support the ability for other Product Engineering teams to add metrics to track product usage for the features they launch into production.
About Your Role
Juniper Square is growing rapidly, and our data needs are growing even faster, so we’re growing our Data Engineering Team. As a Senior Data Engineer your role will be pivotal to evolving our existing data and reporting experiences. You’ll build out pipelines to gather data from multiple sources and make it available for analysis. You will shape both internal and external analytics products to help guide business-critical decisions, enhance their workflows, and improve decision-making.
What You’ll Do
Design and implement sophisticated data models in SQL.
Work closely with the other Software Engineers to ensure sound, scalable implementation.
Act as a technical expert on our team regarding all things data, especially as the data team grows and evolves.
Introduce new technologies to evolve and enhance our data pipeline capabilities.
Document data models, architectural decisions and data dictionaries to enable collaboration, maintainability and usability of our analytics platforms and code.
Assist with governance, guidance, code reviews, and access controls so that we maintain consistency, quality, and business confidentiality as we scale analytics access across the company and to customers.
Externally: learn our application data schema, and develop a fluency in how to transform it to enhance customer’s decision-making with data.
Internally: guide product and development teams, advising on instrumentation and laying development foundations for product usage reporting.
Fulfill projects with minimal guidance but with an appropriate sense of when and how to collaborate with others.
Build scalable, highly performant infrastructure for delivering clear business insights from a variety of raw data sources.
Qualifications
Bachelor's degree in Computer Science, or equivalent work experience
5+ years of experience building ETL (Extraction Transform Load) or ELT (Extraction Load Transform) pipelines from scratch
Strong command of relational databases (Postgresql preferred), data modeling and database design
Strong command of Python and experience using Python for Data Pipelines
Experience with cloud based services (AWS RDS preferred)
Experience developing on (or administering) BI / data visualization platforms (ex. Looker, Tableau, PowerBI, Mode, Data Studio, Domo, QlikView etc.).
Basic understanding of data warehouses such as Amazon Redshift, Google BigQuery, Snowflake etc.
Demonstrated history of translating data into clear and actionable narratives and communicating opportunities and challenges relevant to stakeholders.
You must be flexible and adaptable—you will be operating in a fast-paced startup environment.
At Juniper Square, we believe building a diverse workforce and an inclusive culture makes us a better company. If you think this job sounds like a fit, we encourage you to apply even if you don’t meet all the qualifications.
Benefits
Compensation for this position includes a base salary, equity, and a variety of benefits. The U.S. base salary range for this role is $160,000 - $200,000 and the Canadian base salary range for this role is $200,000 to $250,000 CAD. Actual base salaries will be based on candidate-specific factors, including experience, skillset, and location, and local minimum pay requirements as applicable. Your recruiter can provide further details.
Competitive salary and meaningful equity
Health, dental, and vision care for you and your family
Unlimited vacation policy and paid holidays
Generous paid family leave, medical leave, and bereavement leave policies
401k retirement savings plan
Healthcare FSA and commuter benefits programs
Freedom to customize your work and technology setup as you see fit
Professional development stipend
Monthly work from home wellness stipend while we're all remote
Mental wellness coverage including live coaching and therapy sessions
Home office productivity allowance to help create an ideal work from home setup
#Juniper-US
#Juniper-Canada
Show more
Show less","Python, Postgresql, ETL/ELT Pipelines, Data Modeling, Data Warehouses, Cloud Services, BI/Data Visualization, AWS RDS, Looker, Tableau, PowerBI, Mode, Data Studio, Domo, QlikView, Amazon Redshift, Google BigQuery, Snowflake","python, postgresql, etlelt pipelines, data modeling, data warehouses, cloud services, bidata visualization, aws rds, looker, tableau, powerbi, mode, data studio, domo, qlikview, amazon redshift, google bigquery, snowflake","amazon redshift, aws rds, bidata visualization, cloud services, data studio, data warehouses, datamodeling, domo, etlelt pipelines, google bigquery, looker, mode, postgresql, powerbi, python, qlikview, snowflake, tableau"
Data Engineer -Lead,Diverse Lynx,"Baskin, LA",https://www.linkedin.com/jobs/view/data-engineer-lead-at-diverse-lynx-3764423350,2023-12-17,Louisiana,United States,Mid senior,Onsite,"Data Engineer – Lead (
Bigdata Production support
)
Baskin, NJ (Day 1 onsite)
Job Description
Responsibilities:
Monitor production jobs for P1 system (Data Management and Reporting – DMR).
Debug and restart in the event of production failures.
Acknowledge, respond and resolve production issues.
Understand change requests / enhancements requests.
Design and developed data loading strategies.
Build, develop, testing shared components that will be used across modules.
Extract source data files, stage, transform and load into EERA system (PostgreSQL SQL)
Ingest data from Mainframe system using Kafka and load into EERA.
Extract source data files, stage, transform and load into BDS system (Hive based)
Use Apache Spark for large data processing integrated with functional programming language Scala
Create programs to continuously listen in requests from consumption layer, generate data extracts and publish it via email.
On call support during weekend
Handshake with offshore team on daily / need basis.
Mandatory Skills
Scala, Spark, Hadoop, Kafka
Postgres SQL
Hive
Airflow (or Control M / Oozie)
Kubernetes (or equivalent containerization tool)
Agile
Strong architecture, design & coding skills
Experience in production operations and support
Experience in distributed databases
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Scala, Spark, Hadoop, Kafka, PostgreSQL, Hive, Airflow, Kubernetes, Architecture, Coding, Production operations, Distributed databases, Python","scala, spark, hadoop, kafka, postgresql, hive, airflow, kubernetes, architecture, coding, production operations, distributed databases, python","airflow, architecture, coding, distributed databases, hadoop, hive, kafka, kubernetes, postgresql, production operations, python, scala, spark"
Data Engineer Lead (Bigdata Production support),Diverse Lynx,"Baskin, LA",https://www.linkedin.com/jobs/view/data-engineer-lead-bigdata-production-support-at-diverse-lynx-3764427003,2023-12-17,Louisiana,United States,Mid senior,Onsite,"Title: Data Engineer – Lead (Bigdata Production support)
Location: Baskin, NJ (Day 1 onsite)
Client: Wipro
Contract
Job Description
Responsibilities:
Monitor production jobs for P1 system (Data Management and Reporting – DMR).
Debug and restart in the event of production failures.
Acknowledge, respond and resolve production issues.
Understand change requests / enhancements requests.
Design and developed data loading strategies.
Build, develop, testing shared components that will be used across modules.
Extract source data files, stage, transform and load into EERA system (PostGres SQL)
Ingest data from Mainframe system using Kafka and load into EERA.
Extract source data files, stage, transform and load into BDS system (Hive based)
Use Apache Spark for large data processing integrated with functional programming language Scala
Create programs to continuously listen in requests from consumption layer, generate data extracts and publish it via email.
On call support during weekend
Handshake with offshore team on daily / need basis.
Mandatory Skills
Scala, Spark, Hadoop, Kafka
Postgres SQL
Hive
Airflow (or Control M / Oozie)
Kubernetes (or equivalent containerization tool)
Agile
Strong architecture, design & coding skills
Experience in production operations and support
Experience in distributed databases
Nice To Have Skills
Java as some of the UI applications are built in Java. Resource should know Java fundamentals.
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Scala, Spark, Hadoop, Kafka, PostGres SQL, Hive, Airflow, Control M, Oozie, Kubernetes, Agile, Java","scala, spark, hadoop, kafka, postgres sql, hive, airflow, control m, oozie, kubernetes, agile, java","agile, airflow, control m, hadoop, hive, java, kafka, kubernetes, oozie, postgres sql, scala, spark"
Data Engineer Lead (Bigdata Production support),Diverse Lynx,"Baskin, LA",https://www.linkedin.com/jobs/view/data-engineer-lead-bigdata-production-support-at-diverse-lynx-3764422701,2023-12-17,Louisiana,United States,Mid senior,Onsite,"Title
Data Engineer Lead (Bigdata Production support)
Duration
9&plus; month
Location
Baskin, NJ (Day 1 onsite) - Pls share candidate consent of day 1 onsite.
Mandatory Skills
Provide on-call support between 5 PM and 9 PM EST.
No of Open Positions
1
Job Description
Responsibilities:
Monitor production jobs for P1 system (Data Management and Reporting DMR).
Debug and restart in the event of production failures.
Acknowledge, respond and resolve production issues.
Understand change requests / enhancements requests.
Design and developed data loading strategies.
Build, develop, testing shared components that will be used across modules.
Extract source data files, stage, transform and load into EERA system ( PostGres SQL)
Ingest data from Mainframe system using Kafka and load into EERA.
Extract source data files, stage, transform and load into BDS system (Hive based)
Use Apache Spark for large data processing integrated with functional programming language Scala
Create programs to continuously listen in requests from consumption layer, generate data extracts and publish it via email.
On call support during weekend
Handshake with offshore team on daily / need basis.
Mandatory Skills
Scala, Spark, Hadoop, Kafka
Postgres SQL
Hive
Airflow (or Control M / Oozie)
Kubernetes (or equivalent containerization tool)
Agile
Strong architecture, design & coding skills
Experience in production operations and support
Experience in distributed databases
Nice To Have Skills
Java as some of the UI applications are built in Java. Resource should know Java fundamentals.
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Scala, Spark, Hadoop, Kafka, Postgres SQL, Hive, Airflow, Kubernetes, Agile, Distributed databases, Java","scala, spark, hadoop, kafka, postgres sql, hive, airflow, kubernetes, agile, distributed databases, java","agile, airflow, distributed databases, hadoop, hive, java, kafka, kubernetes, postgres sql, scala, spark"
Clinical Data Analyst,Tulane University,"New Orleans, LA",https://www.linkedin.com/jobs/view/clinical-data-analyst-at-tulane-university-3642106774,2023-12-17,Louisiana,United States,Mid senior,Onsite,"Summary
The Clinical Data Analyst pulls information from multiple data sources across departments and creates accurate, comprehensive data sets for end-users. He/she must be adept at combining data sets from multiple systems and presenting the aggregate data using a variety of industry standards. He/she will provide oversight and day to day monitoring of clinical activities including reviewing and managing professional billing activities, RVU reports and coding benchmarking. The Clinical Data Analyst will also work with other university partners to fulfill data requests as needed. The Clinical Data Analyst assists with data collection, conducts data analysis, and develops reports, and works closely with department leadership to evaluate clinical activity compared to budget and project.
Required Knowledge, Skills, And Abilities
Experience working with physician practice management.
Knowledge of ICD-10, CPT coding
Knowledge of Federal Regulations with regard to physician billing
Knowledge and understanding of relative value units related to physician billing systems.
Knowledge of HMO, PPO, Commercial, and Governmental Payor Policies & Guidelines
Excellent interpersonal and organizational skills
Ability to self motivate and organize activities
Ability to pro-actively work independently and focus/complete a given assignment
Required Education And/or Experience
Bachelor’s Degree and one year of experience analyzing data systems
OR
High School Diploma/equivalent and eight years of experience analyzing data systems.
Preferred Qualifications
Knowledge of electronic medical record systems such as eCW, Meditech, EPIC, CPRS, etc.
Familiar with academic medicine practices and programs
Experience working in patient care setting or business office of a patient care setting
Ability to collect, analyze, and maintain data from a variety of sources in both the quantitative and qualitative fields through, but not limited to the following sources – local surveys, national surveys, electronic medical record databases, and focus groups
Show more
Show less","Data Analysis, Data Collection, Data Visualization, Data Management, Statistical Analysis, Data Mining, Data Modeling, Data Cleaning, Data Integration, Data Presentation, Data Interpretation, Data Reporting, Physician Practice Management, ICD10 Coding, CPT Coding, Federal Regulations, Relative Value Units, HMO, PPO, Commercial Payors, Governmental Payors, Interpersonal Skills, Organizational Skills, SelfMotivation, Proactive Work Style, Independence, Ability to Focus and Complete Assignments, Electronic Medical Record Systems, eCW, Meditech, EPIC, CPRS, Academic Medicine Practices, Patient Care Setting, Business Office of a Patient Care Setting","data analysis, data collection, data visualization, data management, statistical analysis, data mining, data modeling, data cleaning, data integration, data presentation, data interpretation, data reporting, physician practice management, icd10 coding, cpt coding, federal regulations, relative value units, hmo, ppo, commercial payors, governmental payors, interpersonal skills, organizational skills, selfmotivation, proactive work style, independence, ability to focus and complete assignments, electronic medical record systems, ecw, meditech, epic, cprs, academic medicine practices, patient care setting, business office of a patient care setting","ability to focus and complete assignments, academic medicine practices, business office of a patient care setting, commercial payors, cprs, cpt coding, data cleaning, data collection, data integration, data interpretation, data management, data mining, data presentation, data reporting, dataanalytics, datamodeling, ecw, electronic medical record systems, epic, federal regulations, governmental payors, hmo, icd10 coding, independence, interpersonal skills, meditech, organizational skills, patient care setting, physician practice management, ppo, proactive work style, relative value units, selfmotivation, statistical analysis, visualization"
Lead Data Engineer Lead (Bigdata Production support),Diverse Lynx,"Baskin, LA",https://www.linkedin.com/jobs/view/lead-data-engineer-lead-bigdata-production-support-at-diverse-lynx-3764422670,2023-12-17,Louisiana,United States,Mid senior,Onsite,"Role: Lead Data Engineer – Lead (Bigdata Production support)
Location: Baskin, NJ (Day 1 onsite)
Type: Contract
Job Description
Responsibilities:
Monitor production jobs for P1 system (Data Management and Reporting – DMR).
Debug and restart in the event of production failures.
Acknowledge, respond and resolve production issues.
Understand change requests / enhancements requests.
Design and developed data loading strategies.
Build, develop, testing shared components that will be used across modules.
Extract source data files, stage, transform and load into EERA system (PostGres SQL)
Ingest data from Mainframe system using Kafka and load into EERA.
Extract source data files, stage, transform and load into BDS system (Hive based)
Use Apache Spark for large data processing integrated with functional programming language Scala
Create programs to continuously listen in requests from consumption layer, generate data extracts and publish it via email.
On call support during weekend
Handshake with offshore team on daily / need basis.
Mandatory Skills
Scala, Spark, Hadoop, Kafka
Postgres SQL
Hive
Airflow (or Control M / Oozie)
Kubernetes (or equivalent containerization tool)
Agile
Strong architecture, design & coding skills
Experience in production operations and support
Experience in distributed databases
Nice To Have Skills
Java as some of the UI applications are built in Java. Resource should know Java fundamentals.
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Scala, Spark, Hadoop, Kafka, PostGRES SQL, Hive, Airflow, Kubernetes, Agile, Java, SQL","scala, spark, hadoop, kafka, postgres sql, hive, airflow, kubernetes, agile, java, sql","agile, airflow, hadoop, hive, java, kafka, kubernetes, postgres sql, scala, spark, sql"
"Data Engineer Lead (Bigdata Production support)@Baskin, NJ (Day 1 onsite) - hybrid",Diverse Lynx,"Baskin, LA",https://www.linkedin.com/jobs/view/data-engineer-lead-bigdata-production-support-%40baskin-nj-day-1-onsite-hybrid-at-diverse-lynx-3764422679,2023-12-17,Louisiana,United States,Mid senior,Onsite,"Title
Data Engineer Lead (Bigdata Production support)
Location
Baskin, NJ (Day 1 onsite) - hybrid
Job Description
Responsibilities:
Monitor production jobs for P1 system (Data Management and Reporting DMR).
Debug and restart in the event of production failures.
Acknowledge, respond and resolve production issues.
Understand change requests / enhancements requests.
Design and developed data loading strategies.
Build, develop, testing shared components that will be used across modules.
Extract source data files, stage, transform and load into EERA system ( PostGres SQL)
Ingest data from Mainframe system using Kafka and load into EERA.
Extract source data files, stage, transform and load into BDS system (Hive based)
Use Apache Spark for large data processing integrated with functional programming language Sca la
Create programs to continuously listen in requests from consumption layer, generate data extracts and publish it via email.
On call support during weekend
Handshake with offshore team on daily / need basis.
Mandatory Skills
Scala, Spark, Hadoop, Kafka
Postgres SQL
Hive
Airflow (or Control M / Oozie)
Kubernetes (or equivalent containerization tool)
Agile
Strong architecture, design & coding skills
Experience in production operations and support
Experience in distributed databases
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Data Engineering, Big Data, Production Support, Monitoring, Debugging, Data Loading Strategies, Apache Spark, Scala, Hadoop, Kafka, PostgreSQL, Hive, Apache Airflow, Kubernetes, Agile, Software Architecture, Coding, Linux","data engineering, big data, production support, monitoring, debugging, data loading strategies, apache spark, scala, hadoop, kafka, postgresql, hive, apache airflow, kubernetes, agile, software architecture, coding, linux","agile, apache airflow, apache spark, big data, coding, data engineering, data loading strategies, debugging, hadoop, hive, kafka, kubernetes, linux, monitoring, postgresql, production support, scala, software architecture"
Sr. Data Analyst - Quality Consultant,CVS Health,"Kenner, LA",https://www.linkedin.com/jobs/view/sr-data-analyst-quality-consultant-at-cvs-health-3768788223,2023-12-17,Louisiana,United States,Mid senior,Onsite,"Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.
Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
Position Summary
Aetna Better Health of Louisiana, a CVS Health company, is a trusted health partner in the local Louisiana communities we serve. We provide a full array of innovative services that enhance overall wellness and improve everyday life for our members. At Aetna Better Health of Louisiana, we value professional development and career growth. You will work along other colleagues who align on Heart at Work behaviors and bringing your heart to every moment of health. We will support you all the way!
This is a fulltime teleworker opportunity in Louisiana. If candidate is living within Louisiana Instate travel may be required based on business needs; including travel to the Louisiana City office.
The Sr. Data Analyst, Quality Consultant works closely with business partners to identify and improve key business processes and improve member experience. Serves as an operations champion through measuring and monitoring the KPIs and effectiveness of operational processes that impact customer satisfaction, cost management, and operational efficiency. You will evaluate reporting and management dashboards, perform data analysis, develop workflows, and offer support for assigned initiatives that impact the delivery of products and services to internal and external customers.
Applies critical and analytical methods and procedures to identify root causes to recommend, assist in implementing and measuring durable solutions.
Ability to understand healthcare data operations and systems
Experience with data manipulation, data visualization, and presentation.
Act as liaison between internal business units to facilitate new business process plans.
Manage multiple initiatives with multiple customer priorities and successfully meet targeted deadlines.
Communication with all levels of management and relevant stakeholders to encourage internal problem solving.
Collaborate with management to create new processes and document them for future use.
Drive process improvements and effectively manage change with demonstrated ability to challenge status quo.
Proactively identify inefficiencies and process improvement opportunities.
Presentation of findings to both small groups and larger audiences.
Performs other duties as assigned.
Required Qualifications
2 years experience programming using SharePoint, Tableau and other similar programs.
3 years of business experience leading analyses and initiatives with track record of business impact.
2 years of experience of programming experience with SQL or SAS (writing advance queries).
2 years of experience with data visualization and interpretation of data to other stakeholders in a digestible way.
Strong knowledge of advanced analytics tools and languages to analyze large data sets from multiple data sources.
2+ years’ experience using personal computer, keyboard navigation, navigating multiple systems and applications; and using MS Office Suite applications.
Experience with Healthcare analytics or managed care experience.
3+ years’ experience working with data, data analytics or data management systems.
Demonstrated ability to facilitate cross-functional process improvement teams.
Ability to manage multiple initiatives with multiple customer priorities and successfully meet targeted deadlines.
Strong capability to communicate with all levels of management and large groups to achieve desired outcomes.
Must possess strong presentation and communication skills, verbal and written.
Ability to proactively identify inefficiencies and process improvement opportunities.
Ability to collaborate with small teams and cross functionally across business units.
Must possess reliable transportation and be willing and able to travel in-state up to 10% of the time. Mileage is reimbursed per our company expense reimbursement policy.
Preferred Qualifications
Experience with QuickBase, or PowerBI.
Medicaid experience.
Knowledge of data management systems.
Bachelor’s degree preferred.
3+ years of project management skills and experience.
Education
Bachelor’s Degree in health informatics, information technology, computer science, statistics, applied mathematics, or equivalent relevant work experience.
Pay Range
The typical pay range for this role is:
$43,700.00 - $90,000.00
This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.
In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Companypolicies.
For more detailed information on available benefits, please visitjobs.CVSHealth.com/benefits
CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services throughColleagueRelations@CVSHealth.comIf you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.
Show more
Show less","SharePoint, Tableau, SQL, SAS, Data analytics, Data management, Data visualization, Microsoft Office Suite, PowerBI, QuickBase, Medicaid, Health informatics, Information technology, Computer science, Statistics, Applied mathematics","sharepoint, tableau, sql, sas, data analytics, data management, data visualization, microsoft office suite, powerbi, quickbase, medicaid, health informatics, information technology, computer science, statistics, applied mathematics","applied mathematics, computer science, data management, dataanalytics, health informatics, information technology, medicaid, microsoft office suite, powerbi, quickbase, sas, sharepoint, sql, statistics, tableau, visualization"
Data Engineer - Scala(U.S. remote),Railroad19,"New Orleans, LA",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782291811,2023-12-17,Louisiana,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark 2.4, AWS, EMR, S3, RESTful APIs, Relational Databases, NonRelational Databases, Hadoop","scala, spark 24, aws, emr, s3, restful apis, relational databases, nonrelational databases, hadoop","aws, emr, hadoop, nonrelational databases, relational databases, restful apis, s3, scala, spark 24"
Senior Principal Consultant – Data and Analytics,Genesys,"Louisiana, United States",https://www.linkedin.com/jobs/view/senior-principal-consultant-%E2%80%93-data-and-analytics-at-genesys-3781016466,2023-12-17,Louisiana,United States,Mid senior,Remote,"Build something new with a world-class team.
At Genesys, we allow our employees to make their mark by entrusting them to make decisions and do what they’ve been hired to do: their very best. Your potential is waiting; why are you?
About Genesys:
GENESYS® powers more than 25 billion of the world’s best customer experiences each year.
In GENESYS, Innovations team builds solutions that enable our customers to get more value from their Genesys platforms. The Innovations team’s primary job is to build new, innovative applications using AI and analytics, with the goal of helping our customers effectively and efficiently deliver experience as a service (XaaS).
Summary: We are seeking a highly skilled and experienced Contact Center Data and Analytics Consultant to join our team. The ideal candidate will have a strong background in data analysis, data visualization, data engineering, and business intelligence. This is a customer facing role and requires the ability to present to both technical and business executive audiences.
Responsibilities:
Present Genesys data and analytics technology and vision to executives and technical contributors at prospects and customers
Work hands-on with prospects and customers to demonstrate and communicate the value of Genesys data and analytics apps throughout the sales cycle, from demo to proof of concept to design and implementation
Immerse yourself in the ever-evolving industry of contact center and experience management SaaS, maintaining a deep understanding of competitive and complementary technologies and vendors and how to position our analytics apps in relation to them
Collaborate with Product Management, Engineering, and Saloes to continuously improve Genesys data and analytics apps and marketing
Requirements:
8+ years direct experience with Genesys Engage Infomart or GCXI or NICE CX Analytics; strong understanding of Contact Center and Customer Experience data and metrics
Strong proficiency in SQL, data modeling, and data engineering
Familiarity with scripting languages for data engineering
Outstanding presenting skills to both technical and executive audiences, whether impromptu on a whiteboard or using presentations and demos.
Hands-on expertise with SQL and SQL analytics
Ability to connect a customer’s specific business problems to Genesys analytics solutions
Experience with data visualization tools such as Tableau or Power BI
Preferred Qualifications:
Experience with Snowflake
Experience with Elastic (ELK stack)
Experience with data governance and data management best practices.
Experience working with remote engineering teams
Familiar with software development and project management frameworks
Experience in consulting or developing products and solutions with contact center technologies like Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya, etc.
Compensation:
This role has a market-competitive salary with an anticipated base compensation range listed below. Actual salaries will vary depending on a candidate’s experience, qualifications, skills, and location. This role might also be eligible for a commission or performance-based bonus opportunities.
$121,500.00 - $238,400.00
Benefits:
Medical, Dental, and Vision Insurance.
Telehealth coverage
Flexible work schedules and work from home opportunities
Development and career growth opportunities
Open Time Off in addition to 10 paid holidays
401(k) matching program
Adoption Assistance
Fertility treatments
More details about our company benefits can be found at the following link: https://mygenesysbenefits.com
If a Genesys employee referred you, please use the link they sent you to apply.
About Genesys:
Every year, Genesys orchestrates billions of remarkable customer experiences for organizations in more than 100 countries. Through the power of our cloud, digital and AI technologies, organizations can realize Experience as a Service™ our vision for empathetic customer experiences at scale. With Genesys, organizations have the power to deliver proactive, predictive, and hyper personalized experiences to deepen their customer connection across every marketing, sales, and service moment on any channel, while also improving employee productivity and engagement. By transforming back-office technology to a modern revenue velocity engine Genesys enables true intimacy at scale to foster customer trust and loyalty. Visit www.genesys.com.
Reasonable Accommodations:
If you require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you or someone you know may complete the Reasonable Accommodations Form for assistance. Please use the Candidate field in the dropdown menu to ensure a timely response.
This form is designed to assist job seekers who seek reasonable accommodation for the application process. Submissions entered for non-accommodation-related issues, such as following up on an application or submitting a resume, may not receive a response.
Genesys is an equal opportunity employer committed to equity in the workplace. We evaluate qualified applicants without regard to race, color, age, religion, sex, sexual orientation, gender identity or expression, marital status, domestic partner status, national origin, genetics, disability, military and veteran status, and other protected characteristics.
Please note that recruiters will never ask for sensitive personal or financial information during the application phase.
Show more
Show less","Data analysis, Data visualization, Data engineering, Business intelligence, AI, Analytics, SQL, Data modeling, Scripting languages, Tableau, Power BI, Snowflake, Elastic (ELK stack), Data governance, Data management, Software development, Project management, Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya","data analysis, data visualization, data engineering, business intelligence, ai, analytics, sql, data modeling, scripting languages, tableau, power bi, snowflake, elastic elk stack, data governance, data management, software development, project management, genesys engage, genesys cloud, nice, cisco, avaya","ai, analytics, avaya, business intelligence, cisco, data engineering, data governance, data management, dataanalytics, datamodeling, elastic elk stack, genesys cloud, genesys engage, nice, powerbi, project management, scripting languages, snowflake, software development, sql, tableau, visualization"
Data Systems Engineer,CyberCoders,"Baton Rouge, LA",https://www.linkedin.com/jobs/view/data-systems-engineer-at-cybercoders-3779635170,2023-12-17,Louisiana,United States,Mid senior,Hybrid,"Job Title
: Data Systems Engineer
Job Location
: Baton Rouge, LA OR Houston, TX (mainly remote; however, candidates MUST be local to Baton Rouge or Houston)
Salary
: $70-100K DOE (benefits, time off, etc.)
Requirements
Bachelor's degree in Computer Science, Engineering, or related field (or equivalent work experience).
3+ years of recent professional experience developing SQL queries and ETL processes.
Experience with Microsoft T-SQL, .NET, Visual Studio, and/or Oracle is preferred, but not required.
For over 60 years, we have provided solutions for industrial valve/instrumentation applications in the oil/gas industry. Due to recent growth, we are seeking a talented Data Systems Engineer to join our team. This position requires a Bachelor's degree in Computer Science, Engineering, or related field (or equivalent work experience) and 3+ years of recent professional experience developing SQL queries and ETL processes. Experience with Microsoft T-SQL, .NET, Visual Studio, and/or Oracle is preferred, but not required.
What You Will Be Doing
Developing solutions to support business analytics/data processing needs.
Designing, developing, and maintaining data lake/warehouse.
Implementing reports/data extracts and data sources for visualization tools (ex. Tableau).
What You Need for this Position
Bachelor's degree in Computer Science, Engineering, or related field (or equivalent work experience).
3+ years of recent professional experience developing SQL queries and ETL processes.
Preferred Qualifications
Azure
T-SQL
.NET, SSRS, SSIS, Excel
Visual Studio, SQL Managment Studio
Oracle PL/SQL
What's In It for You
Competitive Base Salary ($70-100K DOE)
Medical, Dental, Vision Insurance
Time Off
So, if you are a Data Systems Engineer with SQL and ETL experience, please apply today!
Email Your Resume In Word To
Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:
Brittany.Owen@CyberCoders.com
Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : BO4-1777444 -- in the email subject line for your application to be considered.***
Brittany Owen - Executive Recruiter - CyberCoders
Applicants must be authorized to work in the U.S.
CyberCoders is proud to be an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work
– In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.
Show more
Show less","SQL, ETL, TSQL, .NET, SSRS, SSIS, Excel, Visual Studio, Oracle PL/SQL, Azure, Tableau, SQL Managment Studio, Data lake, Data warehouse","sql, etl, tsql, net, ssrs, ssis, excel, visual studio, oracle plsql, azure, tableau, sql managment studio, data lake, data warehouse","azure, data lake, datawarehouse, etl, excel, net, oracle plsql, sql, sql managment studio, ssis, ssrs, tableau, tsql, visual studio"
Data Analyst,Steneral Consulting,"Ramsey, NJ",https://www.linkedin.com/jobs/view/data-analyst-at-steneral-consulting-3758496996,2023-12-17,Montrose,United States,Associate,Onsite,"100% onsite role, must be local to NJ and under 60 mins commute
need LinkedIn
Need 10/10 comms
Sage 100 ERP migrating to FactoryMaster (do not need experience w/ this)
They need a person to come in a clean up the data
Advanced Excel
Any ERP Experience
Data Cleanse
After cleanse they will do migration to FactoryMaster
Show more
Show less","Sage 100 ERP, FactoryMaster, Advanced Excel, ERP, Data Cleanse","sage 100 erp, factorymaster, advanced excel, erp, data cleanse","advanced excel, data cleanse, erp, factorymaster, sage 100 erp"
DATA ANALYST,Steneral Consulting,"Ramsey, NJ",https://www.linkedin.com/jobs/view/data-analyst-at-steneral-consulting-3755501845,2023-12-17,Montrose,United States,Associate,Onsite,"100% onsite role, must be local to NJ and under 60 mins commute
need LinkedIn
Need 10/10 comms
Sage 100 ERP migrating to FactoryMaster (do not need experience w/ this)
They need a person to come in a clean up the data
Advanced Excel
Sage 100 background
Data Cleanse
After cleanse they will do migration to FactoryMaster
Show more
Show less","Sage 100, FactoryMaster, Advanced Excel, Data Cleanse","sage 100, factorymaster, advanced excel, data cleanse","advanced excel, data cleanse, factorymaster, sage 100"
Data Scientist (USA),Trexquant Investment LP,"Stamford, CT",https://www.linkedin.com/jobs/view/data-scientist-usa-at-trexquant-investment-lp-3590312323,2023-12-17,Montrose,United States,Associate,Onsite,"As a member of the Data team at Trexquant, you will be involved in parsing and analyzing large data sets, working on discovering and obtaining new sources of data, and collaborating with the Alpha and Strategy Research team to build predictive machine learning models.
Responsibilities
Explore and learn about a wide range of data sets that are used to develop signals for systematic quantitative strategies
Develop a framework to automatically download and monitor hundreds of data sources that are vital to our trading and research
Create data visualizations to gain insight on large data sets, such as order-by-order tick data, present findings and results
Research and implement machine learning techniques to identify patterns in large data sets and create new derived variables
Requirements
A degree in a technical discipline (computer science, mathematics, statistics, physics, etc.)
Experience with statistical analysis and managing of large data sets
Knowledge of Linux, Bash, Python, and SQL Database
Ability to work independently and take projects to completion, quickly learn new systems, think creatively and pay attention to details
Benefits
Competitive compensation, with opportunity for outstanding monetary success
Work in a collaborative and friendly environment, participate in decision-making process for research direction, and have opportunity to lead on new ideas
Comprehensive benefits including healthcare and insurance
Show more
Show less","Data analysis, Machine learning, Data visualization, Statistical analysis, Data management, Linux, Bash, Python, SQL, Quantitative strategies, Data sources, Orderbyorder tick data, Predictive machine learning models, Derived variables","data analysis, machine learning, data visualization, statistical analysis, data management, linux, bash, python, sql, quantitative strategies, data sources, orderbyorder tick data, predictive machine learning models, derived variables","bash, data management, data sources, dataanalytics, derived variables, linux, machine learning, orderbyorder tick data, predictive machine learning models, python, quantitative strategies, sql, statistical analysis, visualization"
Future Opportunity- Data Engineering Consultant,Avanade,"Stamford, CT",https://www.linkedin.com/jobs/view/future-opportunity-data-engineering-consultant-at-avanade-3781780504,2023-12-17,Montrose,United States,Associate,Onsite,"Job Description
RECRUITING FOR THE FUTURE
This posting is for a future job opportunity.
To support the needs of our clients, Avanade is actively recruiting and interviewing for the role outlined below, as we continue to build on another year of growth in our business. Now, with more than 60,000 employees around the globe, we are positioning our organization to effectively prepare for the future. What does this mean for you? We encourage you to apply and interview for this role without the need to decide now. This allows you to connect with leaders and hiring managers at a pace that works for you and when roles become available, you will be the first know.
Data Engineering Consultant
Our talented Data and AI Practice is made up of globally recognized experts- and there's room for more analytical and ambitious data professionals. If you're passionate about helping clients make better data-driven decisions to tackle their most complex business issues. let's talk. Take your skills to a new level and launch a career where you can can truly do what matters.
Come join us
As a Data Engineer within our Data and AI practice, you’ll design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.
Together we do what matters.
What You'll Do
Use your knowledge to plan and deliver data warehouses and storage
Take part in crafting and running bespoke data services for individual projects
Stay up to date with business best practice in using and retrieving data
Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining
Customize storage and extraction, metadata, and information repositories
Build and use effective metrics and monitoring processes
Help to develop business intelligence tools
Craft and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
Skills And Experiences
Strong knowledge of Python, Spark, and T-SQL
Microsoft Fabric/Synapse, Purview and Azure Databricks, PowerBI
Database, storage, collection and aggregation models, techniques, and technologies – and how to apply them in business
Ability to use technology to aim business problems using one or more Microsoft Analytics services for building data pipelines, data streams, and system integration
About You
Characteristics that can spell success for this role:
Advanced knowledge in entity and relationship extraction from unstructured data
Understanding of database table indexing, with emphasis on clustered column store tables
Proficient in data handling, understanding, analysis and interpretation of results
Experience with SQL technologies: Databricks (Spark), OR Azure Synapse
Data security: Must be able to demonstrate an understanding of data security at rest and in transit
Scale: Deliver performance at scale, and accurately anticipate system enhancement needs
Data integrity: Data manipulation, error identification and handling, and modeling
Enjoy your career
Some of the best things about working at Avanade
People-first culture that supports innovation and encourages people to move forward
Turn your ideas to human impact by cultivating a team that will help clients unlock what’s next
Opportunities to innovate in the Microsoft platform while charting your own career path
On-the-job growth through hackathons, innovation contests, and other expressions of our passion for innovating with purpose
Real-time access to technical and skilled resources globally
Find out more about some of our benefits here .
A great place to work
As you bring your skills and abilities to Avanade, you’ll get distinctive experiences, limitless learning, and ambitious growth in return. As we continue to build our diverse and inclusive culture, we become even more innovative and creative, helping us better serve our clients and our communities. You’ll join a community of smart, supportive collaborators to lift, mentor, and guide you, but to also lean on your expertise . You get a company purpose-built for business-critical, leading-edge technology solutions, committed to improving the way humans work, interact, and live. It’s all here, so take a closer look!
We work hard to provide an inclusive, diverse culture with a deep sense of belonging for all our employees. Visit our Inclusion & Diversity page.
Create a future for our people that focuses on
Expanding your thinking
Experimenting courageously
Learning and pivoting
Inspire greatness in our people by
Empowering every voice
Encouraging boldness
Celebrating progress
Accelerate the impact of our people by
Amazing the client
Prioritizing what matters
Acting as one
Learn more
To learn more about the types of projects our Data & AI team works on check out these case studies:
thyssenkrupp Materials Services uses data to help strike a delicate operational balance
What matters to SSE Renewables is innovating for a sustainable future
Hachette UK enables employees with machine learning-driven contract searches
Marston Holdings identifies significant cloud savings
Interested in knowing what’s going on inside Avanade? Check out our blogs:
Avanade Insights – exchange ideas that drive tomorrow’s innovation
Inside Avanade – explore what life is like working at Avanade
Compensation for roles at Avanade varies depending on a wide array of factors including but not limited to the specific office location, role, skill set and level of experience. As required by local law, Avanade provides a reasonable range of compensation for roles that may be hired in New York as set forth below and information on benefits offered is here.
Range of Starting Pay for role:
New York: $121,500 – $ 143,000
About Avanade
Avanade is the leading provider of innovative digital, cloud and advisory services, industry solutions and design-led experiences across the Microsoft ecosystem. Every day, our 59,000 professionals in 26 countries make a genuine human impact for our clients, their employees and their customers.
We have been recognized as Microsoft’s Global SI Partner of the Year more than any other company. With the most Microsoft certifications (60,000+) and 18 (out of 18) Gold-level Microsoft competencies, we are uniquely positioned to help businesses grow and solve their toughest challenges.
We are a people first company, committed to providing an inclusive workplace where employees feel comfortable being their authentic selves. As a responsible business, we are building a sustainable world and helping people from underrepresented communities fulfil their potential.
Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com .
Show more
Show less","Python, Spark, TSQL, Microsoft Fabric/Synapse, Purview, Azure Databricks, PowerBI, Data warehouses, Data storage, Data services, Data analysis, Data mining, Data security, Data integrity, Entity extraction, Relationship extraction, Database indexing, Data handling, Data interpretation, SQL technologies, Databricks, Azure Synapse, Data manipulation, Error identification, Data modeling","python, spark, tsql, microsoft fabricsynapse, purview, azure databricks, powerbi, data warehouses, data storage, data services, data analysis, data mining, data security, data integrity, entity extraction, relationship extraction, database indexing, data handling, data interpretation, sql technologies, databricks, azure synapse, data manipulation, error identification, data modeling","azure databricks, azure synapse, data handling, data integrity, data interpretation, data manipulation, data mining, data security, data services, data storage, data warehouses, dataanalytics, database indexing, databricks, datamodeling, entity extraction, error identification, microsoft fabricsynapse, powerbi, purview, python, relationship extraction, spark, sql technologies, tsql"
IQVIA Data Analyst,SUN PHARMA,"Hawthorne, NY",https://www.linkedin.com/jobs/view/iqvia-data-analyst-at-sun-pharma-3778536401,2023-12-17,Montrose,United States,Associate,Hybrid,"Job description:
Summary:
This position will support and collaborate with the department and other cross functional departments (sales, art department, regulatory, supply chain, portfolio, etc.) in the development, review, and maintenance of marketing tools, competitor market intelligence, hub services, competitive product intelligence, product launch materials, executive and customer presentations and databases. In addition will be responsible for helping with the coordination of major trade shows each year.
Further, the candidate will utilize these tools for market analysis and to formulate go forward recommendations to support the organizations sales and marketing functions related to the continuous growth of Taro’s product portfolio.
Additionally, the Senior Analyst will assist with the development and management of product strategies to maximize revenue growth and ensure launch target goals are achieved, including revenue and desired market share.
The ideal candidate will have a background consistent with managing multiple projects and be able to work closely with cross functional teams to develop and maintain valuable marketing and reporting information.
Duties and Responsibilities:
Work with marketing team and other cross functional departments to provide sales with the necessary marketing tools, materials, and support to more efficiently and effectively sell Taro generic products.
Data review, organization, and presentation
Analyze historical sales trends, IQVIA prescription trends, market intelligence, and other pertinent information to formulate launch strategies
Provide pro-active and in-depth financial analysis of the products
Database / website maintenance
Supply chain / product launch coordination
Work with cross functional departments to collect data and monitor market opportunities
Market research and reporting
Provide proactive and in-depth financial analysis of product or market-specific business activity
Responsible for managing, analyzing and tracking commercial portfolio’s performance to budget. Identify challenges and underperforming molecules and make strategic adjustment recommendations to increase sales performance.
Review monthly performance of Generic business. Work cross-functionally with finance, supply chain and sales to identify areas for improvement and present results from implemented strategies.
Responsible for generating and forecasting the Generics Sales Budget plan while working cross-functionally with finance, sales, supply chain, and pricing & contracts.
Assist Pricing team with maximizing margin retention while dealing with constant pricing pressure from accounts.
Quarterly Analysis of market trends, and Taro’s portfolio. Identify price and unit growth trends, identify key drivers, and make strategic recommendations to adjust to market dynamics.
Account based strategic analytics.
Product based strategic analytics.
Work with cross functional departments to implement new strategies that will resolve current limitations, improve processes and increase sales.
Quarterly tracking and reporting of Taro’s market share KPOs.
Responsible for supporting cross functional departments with strategic ad hoc analytics to drive sales.
Responsible for generating and forecasting the expense budget plan for department of Sales & Marketing.
Responsible for generating Monthly Performance Review deck. Reporting on Taro’s financial monthly performance. Reporting on top positive and negative molecule drivers, identifying key wins and losses, and any impactful market share changes.
Work closely with supply chain to understand supply limitations and make recommendations on strategies to limit backorders and maximize potential sales.
Qualifications/Education:
BS in business, marketing or related discipline.
Strong orientation to detail and ability to grasp new concepts
Ability to analyze and interpret IQVIA data
Strong organization, communication and presentation skills
Ability to meet tight deadlines in an environment of competing priorities
Ability to build strong internal and external relationships, foster an environment of teamwork and collaboration.
Previous experience within the generic pharmaceutical industry strongly preferred.
Subject matter expert on Microsoft Office applications
The presently-anticipated base compensation pay range for this position is $95,500 to $119,000. Actual base compensation may vary based on a number of factors, including but not limited to geographical location and experience. In addition, this position is part of the Annual Performance Bonus Plan.  Employees are eligible to participate in Company employee benefit programs which include medical, dental and vision coverage; life insurance; disability insurance; 401(k) savings plan; flexible spending accounts; and the employee assistance program. Employees also receive various paid time off benefits, including vacation time and sick time.
The compensation and benefits described above are subject to the terms and conditions of any governing plans, policies, practices, agreements, or other materials or documents as in effect from time to time, including but not limited to terms and conditions regarding eligibility. If hired, employee will be in an “at-will position” and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, Company, or individual department/team performance, and market factors.
The preceding job description has been designed to indicate the general nature and level of work performed by employees within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees as assigned to this job. Nothing herein shall preclude the employer from changing these duties from time to time and assigning comparable duties or other duties commensurate with the experience and background of the incumbent(s).
EEO Notice:
We provide equal employment opportunities for all current employees and applicants for employment. This policy means that no one will be discriminated against because of race, religion, creed, color, national origin, nationality, citizenship, ancestry, sex, age, marital status, physical or mental disability, affectional or sexual orientation, military or veteran status, generic predisposing characteristics or any other basis prohibited by law.
Notice to Agency and Search Firm Representatives:
Sun Pharmaceuticals (Sun) is not accepting unsolicited resumes from agencies and/or search firms for this job posting. Resumes submitted to any Sun employee by a third party agency and/or search firm without a valid written & signed search agreement, will become the sole property of Taro. No fee will be paid if a candidate is hired for this position as a result of an unsolicited agency or search firm referral.
Show more
Show less","Data analysis, Presentation skills, Market research, Financial analysis, Microsoft Office Suite, IQVIA, Sales and marketing, Product management, Product launch coordination, Crossfunctional collaboration, Communication skills, Teamwork, Time management, Strategic thinking, Business intelligence, Project management, Database management, Forecasting, Market share analysis, Pricing strategy, Supply chain management, Backorder management, Sales budgeting, Expense budgeting, Performance review, Microsoft Office applications","data analysis, presentation skills, market research, financial analysis, microsoft office suite, iqvia, sales and marketing, product management, product launch coordination, crossfunctional collaboration, communication skills, teamwork, time management, strategic thinking, business intelligence, project management, database management, forecasting, market share analysis, pricing strategy, supply chain management, backorder management, sales budgeting, expense budgeting, performance review, microsoft office applications","backorder management, business intelligence, communication skills, crossfunctional collaboration, dataanalytics, database management, expense budgeting, financial analysis, forecasting, iqvia, market research, market share analysis, microsoft office applications, microsoft office suite, performance review, presentation skills, pricing strategy, product launch coordination, product management, project management, sales and marketing, sales budgeting, strategic thinking, supply chain management, teamwork, time management"
"Urgent Hiring :: SAP DATA CONVERSION ANALYST – SUCCESSFACTORS-- :: STAMFORD, CT (1 Days Onsite each week)-Locals",Steneral Consulting,"Stamford, CT",https://www.linkedin.com/jobs/view/urgent-hiring-sap-data-conversion-analyst-%E2%80%93-successfactors-stamford-ct-1-days-onsite-each-week-locals-at-steneral-consulting-3750850206,2023-12-17,Montrose,United States,Associate,Hybrid,"1 day oniste each week
Must be local to NY/NJ./CT
Need valid LinkedIn
Looking SAP Data conversion analyst.
SAP HR and SuccessFactors Employee Central, Data Migration
EC Configuration (Data Models, XML, Foundation Objects, Workflow, Data Imports, Permissions, Reporting (Adhoc & ORD), Data Migrations
SuccessFactors Reporting
Data Mappings from SAP HR
Expert in SAP Testing and QA processes in multiple modules across industry verticals
SuccessFactors WFA module
Expert in Data compare and Data validation reports in SQL
Data migration tool DSP (Data Stewardship Platform)
Expert in Data Load Templates and Load Sequence
Managed Data Loads via SuccessFactors Templates, Managed Data Cleansing Activities
Create reports based on Data fetched via Boomi and create reports to validate Data
Show more
Show less","SAP, SAP HR, SuccessFactors Employee Central, Data Migration, EC Configuration, Data Models, XML, Foundation Objects, Workflow, Data Imports, Permissions, Reporting, Adhoc, ORD, SuccessFactors Reporting, Data Mappings, SAP Testing, QA processes, SuccessFactors WFA module, Data compare, Data validation reports, SQL, Data migration tool DSP, Data Stewardship Platform, Data Load Templates, Load Sequence, Managed Data Loads, SuccessFactors Templates, Data Cleansing Activities, Boomi","sap, sap hr, successfactors employee central, data migration, ec configuration, data models, xml, foundation objects, workflow, data imports, permissions, reporting, adhoc, ord, successfactors reporting, data mappings, sap testing, qa processes, successfactors wfa module, data compare, data validation reports, sql, data migration tool dsp, data stewardship platform, data load templates, load sequence, managed data loads, successfactors templates, data cleansing activities, boomi","adhoc, boomi, data cleansing activities, data compare, data imports, data load templates, data mappings, data migration, data migration tool dsp, data models, data stewardship platform, data validation reports, ec configuration, foundation objects, load sequence, managed data loads, ord, permissions, qa processes, reporting, sap, sap hr, sap testing, sql, successfactors employee central, successfactors reporting, successfactors templates, successfactors wfa module, workflow, xml"
Senior Data Engineer,Harnham,"Suffern, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-harnham-3773998800,2023-12-17,Montrose,United States,Mid senior,Onsite,"Senior Data Engineer
6 Month Contract-to-Hire
Suffern, NY - Hybrid
Our client is seeking a Data Engineer who excels at creating data pipelines, as well as refining developmental systems, aiding in their transition from Alteryx to the Google Cloud landscape.
Responsibilities
Develop and maintain scalable and fault-tolerant data infrastructure. Ensure the architecture supports the demands and future growth of the company.
Create, deploy, and manage ETL processes. Oversee real-time and batch data processing.
Guide our migration from Alteryx to a code-centric ETL framework, minimizing disruptions and maximizing efficiency, assisting the company's transition to the Google Cloud, with a specific emphasis on BigQuery integration
Collaborate to understand database requirements, ensuring optimal performance, resilience, and reliability.
Analyze current systems to identify bottlenecks and opportunities for enhancements, driving the evolution of our data infrastructure.
Implement best practices and solutions to ensure data security, privacy, and compliance with relevant regulations.
Work closely with data scientists, analysts, and other stakeholders to deliver on shared goals and projects.
The Ideal Candidate Will Have
2-5+ years of demonstrable experience as a data engineer or in a similar capacity
In-depth knowledge of Google Cloud Platform (GCP) and BigQuery
Strong proficiency in programming languages commonly used in data engineering, such as Python, SQL, or Scala
A deep understanding of distributed system design, data modeling, and schema design
Knowledge of data governance and data quality management principles, privacy regulations, and compliance standards like GDPR, CCPA, etc.
Show more
Show less","Data Engineering, ETL, Data Pipelines, Google Cloud Platform (GCP), BigQuery, Python, SQL, Scala, Distributed System Design, Data Modeling, Schema Design, Data Governance, Data Quality Management, Privacy Regulations, Compliance Standards (GDPR CCPA)","data engineering, etl, data pipelines, google cloud platform gcp, bigquery, python, sql, scala, distributed system design, data modeling, schema design, data governance, data quality management, privacy regulations, compliance standards gdpr ccpa","bigquery, compliance standards gdpr ccpa, data engineering, data governance, data quality management, datamodeling, datapipeline, distributed system design, etl, google cloud platform gcp, privacy regulations, python, scala, schema design, sql"
Senior Python Data Engineer,Luxoft,"New City, NY",https://www.linkedin.com/jobs/view/senior-python-data-engineer-at-luxoft-3773779493,2023-12-17,Montrose,United States,Mid senior,Onsite,"Project Description:
We are looking for an experienced Python Data Engineer to expand our Systematic Data Platform team.
The team develops data pipelines for a top-tier Hedge Fund.
Responsibilities:
Design, develop, and maintain data pipelines
Develop data APIs
Analyze and organize raw data.
Conduct complex data analysis and report on results.
Mandatory Skills Description:
3+ years of data engineering
Solid python skills
Strong SQL
Nice-to-Have Skills:
Data Science
Statistics
Data Modelling
Compensation for NY is 100000-180000 USD Gross per Year
Show more
Show less","Python, Data Engineering, Data Pipelines, SQL, Data Science, Statistics, Data Modelling","python, data engineering, data pipelines, sql, data science, statistics, data modelling","data engineering, data modelling, data science, datapipeline, python, sql, statistics"
Senior Data Engineer,The Shade Store,"Port Chester, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-the-shade-store-3768607483,2023-12-17,Montrose,United States,Mid senior,Onsite,"Job Details
Job Location
Port Chester NY - Port Chester, NY
Description
Position:
Senior Data Engineer (Contractor or Full Time)
Location: Currently Remote / Port Chester, NY
Position Reports to: SVP, Development
About The Shade Store
At The Shade Store, we have handcrafted the finest Shades, Blinds and Drapery for 75 years. We believe designing beautiful custom window treatments should be an effortless experience, so we offer outstanding services to help our customers every step of the way, from inspiration to installation.
WHY WORK AT THE SHADE STORE
We set out to create a company culture that is enjoyable, rewarding, and where there is continuous upward mobility and growth opportunity. If you work hard, give the company your all, use good judgment, and have a positive attitude then the sky’s the limit. In return, there are numerous perks and benefits including:
Full time highly competitive salary
Medical Benefits
401k Available and we match up to 4% of your contributions
Life Insurance & Short Term Disability Coverage provided at no charge
THE POSITION: Senior Data Engineer
As The Shade Store continues our aggressive growth, it has become ever more apparent that we need to continue to grow and scale this very important team. We’re committed to building an excellent team of collaborators and innovators. Our team is ambitious and passionate; the environment is close-knit, fast paced and fun. We are a company that values doers over talkers and mentors over managers. If you work well with people and most importantly have the technical skill set to help support the business in the functions described above -- then we would love to hear from you.
The position will report directly to the SVP, Development and will be part of the Development team responsible for building and scaling our existing data and reporting operational functions that support and inform strategic decision making for The Shade Store. The role requires hands-on technical data skills, and good communication skills for collaboration with the rest of the development team and business stakeholders in other departments.
RESPONSIBILITIES:
Design, build, develop, deploy and maintain applications using the Looker and Snowflake business intelligence platforms.
Develop and optimize large-scale batch and real-time data pipelines that ingest structured and unstructured data from a variety of sources.
Experience loading and unloading data with different file types (such as CSV, JSON, XML, etc)
Hands-on experience with LookML and SnowSQL (writing procedures)
Strong experience with SQL language and data warehousing architecture
Develop and maintain pipelines using DBT
Experience with ETL technologies and integrations
Experience using code versioning and build processes (GIT)
Able to perform data quality checks and validation rules and various stages of processing.
Develop reporting solutions to challenging complex business problems.
Work closely with the business stakeholder team and shared development resources across all phases of projects
Lead and/or participate in project efforts using an Agile development methodology.
Collaborate with QA to develop comprehensive and appropriate test strategies for each release
POSITION REQUIREMENTS:
Bachelor’s degree in computer science, Engineering or a related subject, or equivalent experience
5+ years in a data engineer or SQL-heavy data analyst role
Track record of success in building new data engineering processes with a wide range of data sets from various sources.
Demonstrated ability to work across disparate teams to achieve consensus on key business decisions.
Expert-level SQL skills.
Advanced knowledge with Looker, DBT, Snowflake and/or equivalent tools.
Willingness to roll up your sleeves and fix problems in a hands-on manner.
Intellectual curiosity and research abilities.
Demonstrated knowledge of modern SDLC, Agile/Scrum, Kanban etc.
Ability to organize and manage multiple tasks and priorities
Strong communication skills with the ability to cross collaborate amongst internal departments
Self-motivated and willing to ""do what it takes"" to get the job done
NICE TO HAVES:
Experience building on AWS
Expert level with basic tools like Excel
MySQL tuning and database optimization
THE SHADE STORE offer is contingent upon:
Proof of legal authorization to work in the United States for The Shade Store, which will be confirmed by E-Verify within three business days of your hire date
The base salary range for this position is $160-$180k, commensurate with experience.
The Shade Store provides equal employment opportunities to all employees and applicants without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Show more
Show less","Data Engineering, Looker, Snowflake, SQL, DBT, ETL, Git, Agile, Kanban, SDLC, AWS, Excel, MySQL","data engineering, looker, snowflake, sql, dbt, etl, git, agile, kanban, sdlc, aws, excel, mysql","agile, aws, data engineering, dbt, etl, excel, git, kanban, looker, mysql, sdlc, snowflake, sql"
Senior Data Analyst,Stanwich Energy,"Greenwich, CT",https://www.linkedin.com/jobs/view/senior-data-analyst-at-stanwich-energy-3764426996,2023-12-17,Montrose,United States,Mid senior,Onsite,"Senior Data Analyst
Join us at Stanwich Energy, a rapidly expanding energy consulting firm, as we embark on a journey to revolutionize data-driven decision-making in the energy sector. We are seeking a passionate and driven Senior Data Analyst to spearhead the transformation of our extensive datasets into actionable insights, intuitive dashboards, and comprehensive reports.
Role Overview:
As a pivotal figure within our organization, you will be entrusted with leveraging our vast and meticulously organized SQL database to create sophisticated reporting tools. You'll play a critical advisory role, collaborating with major commercial and industrial clients, harnessing smart meter IoT data, market insights, financial information, sustainability metrics, and more.
Key Requirements:
Proficiency in Building Dynamic Reporting Tools:
Demonstrated experience in architecting, deploying, and maintaining advanced reporting tools that harness live data from production SQL databases.
Expertise in Business Intelligence Tools: Mastery
in utilizing business intelligence tools such as PowerBI, Looker, Tableau, or similar platforms to craft impactful visualizations and insightful reports.
Excel Proficiency:
Adeptness in Excel, including strong Power Query and VBA skills for solving diverse problems and constructing agile tools.
Advanced SQL Skills:
Mastery of intricate SQL queries (MySQL, MS SQL, PostgreSQL) involving join, group by, having, window functions, etc.
Analytical Prowess:
Strong analytical skills coupled with a knack for problem-solving.
Detail-Oriented and Committed to Quality:
An unwavering commitment to precision, quality, and continuous improvement in all data-related endeavors.
Preferred Skills:
Educational Background:
Bachelor’s degree in Mathematics, Statistics, Computer Science, Information Systems, Engineering, or a related field. A Master’s degree is advantageous.
Programming Languages
: Experience in Python or R for data analysis purposes.
Energy Market Insight:
Prior exposure to the energy market, particularly in electricity, sustainability, and natural gas domains.
Data Visualization:
A passion for transforming intriguing data into captivating visualizations.
Project Management:
Previous experience in project management.
Advanced SQL Proficiency:
Ability in table design, stored procedures, indexing, etc.
Keenness for Technological Advancements
: Enthusiasm for staying updated on the latest trends and tools in automation.
Join Us:
Be part of a dynamic team driving innovation in the energy sector. If you’re excited about utilizing data to create impactful solutions and drive strategic decisions, we welcome you to apply and contribute to the future of energy analytics.
Show more
Show less","SQL, PowerBI, Looker, Tableau, Excel, Power Query, VBA, Python, R, MySQL, MS SQL, PostgreSQL, Mathematics, Statistics, Computer Science, Information Systems, Engineering","sql, powerbi, looker, tableau, excel, power query, vba, python, r, mysql, ms sql, postgresql, mathematics, statistics, computer science, information systems, engineering","computer science, engineering, excel, information systems, looker, mathematics, ms sql, mysql, postgresql, power query, powerbi, python, r, sql, statistics, tableau, vba"
Azure Sr. Data Engineer,Tata Consultancy Services,"Allendale, NJ",https://www.linkedin.com/jobs/view/azure-sr-data-engineer-at-tata-consultancy-services-3772658048,2023-12-17,Montrose,United States,Mid senior,Onsite,"Job Description
Good Knowledge of Data Brick lakehouse and Azure DataLake concept
Knowledge of Data Bricks delta concept– Delta live tables (DLT)
Strong hands-on experience in ELT– pipeline development using Azure Data factory and Databricks Autoloader, Notebook scripting and Azure Synapse Activity Copy, Data Flow Task
Strong knowledge of metadata-driven data pipeline, metadata management, dynamic logic
In-depth knowledge of data storage solutions, including Azure Data Lake Storage (ADLS), and Azure Serverless SQL Pool.
Experience with data transformation using Spark, and SQL technologies.
Solid understanding of design patterns, and best practices of the cloud stack.
Experience with code management and version control using Git or similar tools.
Strong problem-solving and debugging skills in ETL workflows and data pipelines.
Strong understanding of Azure Data bricks and Azure Synapse internals – features and capabilities.
Knowledge of Azure DevOps and continuous integration and deployment (CI/CD) process.
Knowledge of data quality and data profiling techniques, with experience in data validation and data cleansing.
Show more
Show less","Databricks Lakehouse, Azure Data Lake, Databricks Delta, ELT, Azure Data Factory, Databricks Autoloader, Azure Synapse Activity Copy, Data Flow Task, Metadatadriven data pipeline, Metadata management, Dynamic logic, Azure Data Lake Storage (ADLS), Azure Serverless SQL Pool, Spark, SQL, Design patterns, Cloud stack, Git, ETL, Data pipelines, Azure Data Bricks, Azure Synapse, Azure DevOps, Continuous integration and deployment (CI/CD), Data quality, Data profiling, Data validation, Data cleansing","databricks lakehouse, azure data lake, databricks delta, elt, azure data factory, databricks autoloader, azure synapse activity copy, data flow task, metadatadriven data pipeline, metadata management, dynamic logic, azure data lake storage adls, azure serverless sql pool, spark, sql, design patterns, cloud stack, git, etl, data pipelines, azure data bricks, azure synapse, azure devops, continuous integration and deployment cicd, data quality, data profiling, data validation, data cleansing","azure data bricks, azure data factory, azure data lake, azure data lake storage adls, azure devops, azure serverless sql pool, azure synapse, azure synapse activity copy, cloud stack, continuous integration and deployment cicd, data flow task, data profiling, data quality, data validation, databricks autoloader, databricks delta, databricks lakehouse, datacleaning, datapipeline, design patterns, dynamic logic, elt, etl, git, metadata management, metadatadriven data pipeline, spark, sql"
"Analyst, Data Scientist",PepsiCo,"Purchase, NY",https://www.linkedin.com/jobs/view/analyst-data-scientist-at-pepsico-3781906511,2023-12-17,Montrose,United States,Mid senior,Onsite,"Overview
This role will support technical development within Artificial Intelligence/Machine Learning for PBNA (PepsiCo Beverages North America) Advanced Analytics and the Global IBP (Integrated Business Planning) program. This will include design, implement, and maintain digital solutions for PBNA sector. The role is responsible for:
Set up and manage our AI development and facilitate production infrastructure.
Help AI product managers and business stakeholders understand the potential and limitations of AI when planning new products.
Build data ingest and data transformation infrastructure.
Identify transfer learning opportunities and new training datasets.
Build AI models from scratch and help product managers and stakeholders understand results.
Deploy AI models into production.
Create APIs and help business customers put results of your AI models into operations.
Keep current of latest AI research relevant to PBNA business domain.
Responsibilities
Technical subject matter expert for Advanced Analytics and digital solution (AI, ML, Advanced Analytics, NLP)
Develop proof-of-concept demos in a SAFe Agile team
Identify transfer learning opportunities and new training datasets.
Build and deploy AI models for PBNA
Assess new capabilities within AI/ML, shape business demand to leverage digital capabilities
Qualifications
2+ years java development.
5+ years project management.
3+ years people management.
Bachelor’s/Masters/ Phd degree in Computer Science, MIS, Business Management, or related field required.
7+ years proven experience in programming and development projects.
Hands-on experience with software development and system administration.
Technical Skills
At least 7+ years hands-on programming experience working on enterprise products.
Demonstrated proficiency in multiple programming languages with a strong foundation in a statistical platform such as Python, R, SAS, or MatLab.
Experience building AI models in platforms such as Keras, TensorFlow, or Theano.
Demonstrated commitment to learning about AI through your own initiatives through courses, books, or side projects.
Non-technical Skills
Excellent drive.
Ability to work in ambiguous situations.
Problem-solving aptitude.
Strong communication skills.
Understand sometimes ambiguous, needs, and translate to clear, aligned requirements.
Compensation & Benefits:
The expected compensation range for this position is between $74,800 - $125,250
Location confirmed job-related skills and experience will be considered in setting actual starting salary
Bonus based on performance and eligibility target payout is 8% of annual salary paid out annually
Paid time off subject to eligibility, including paid parental leave, vacation, sick, and bereavement
In addition to salary, PepsiCo offers a comprehensive benefits package to support our employees and their families, subject to elections and eligibility: Medical, Dental, Vision, Disability, Health and Dependent Care Reimbursement Accounts, Employee Assistance Program (EAP), Insurance (Accident, Group Legal, Life), Defined Contribution Retirement Plan
EEO Statement
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.
PepsiCo is an Equal Opportunity Employer: Female / Minority / Disability / Protected Veteran / Sexual Orientation / Gender Identity
If you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.
Please view our Pay Transparency Statement
Show more
Show less","Artificial Intelligence, Machine Learning, Advanced Analytics, Natural Language Processing, Java, Project Management, People Management, Python, R, SAS, MatLab, Keras, TensorFlow, Theano, Software Development, System Administration","artificial intelligence, machine learning, advanced analytics, natural language processing, java, project management, people management, python, r, sas, matlab, keras, tensorflow, theano, software development, system administration","advanced analytics, artificial intelligence, java, keras, machine learning, matlab, natural language processing, people management, project management, python, r, sas, software development, system administration, tensorflow, theano"
Data Analyst 3,eStaffing Inc.,"Tarrytown, NY",https://www.linkedin.com/jobs/view/data-analyst-3-at-estaffing-inc-3718182858,2023-12-17,Montrose,United States,Mid senior,Onsite,"Description
Description:
The Senior Biostatistician will be responsible for providing technical expertise in biostatistics and to lead statistical evaluations of development and clinical validation studies for new in vitro diagnostics devices developed by ### Healthineers Laboratory Diagnostics.
The role will interact cross-functionally with members of Clinical Affairs, Data Management, Medical Affairs, Regulatory Affairs and Global Assay Development to ensure high quality of data and analysis results used in regulatory submissions.
The role will leverage knowledge and experience in the areas of statistical techniques and programming to support product development goals within the Laboratory Diagnostics Division. This is an ideal role for candidates who are highly motivated, detail-oriented, have strong problem solving and scientific thinking skills, a collaborative mindset, and enthusiasm for topics related to biostatistics, data analysis, programming, presentation of data, and biological science. A willingness to share this enthusiasm with colleagues and to have fun is a must for our team.
The position will report to the Senior Director of Pre-Market Biostatistics.
Responsibilities
Responsible for consulting on the design of clinical trials and statistical analysis methodology for studies directed towards the development, verification, validation, and monitoring of in vitro diagnostics devices developed by ### Healthineers Laboratory Diagnostics.
Recommends appropriate statistical methodology for the evaluation of analytical and clinical study data.
Interprets and evaluates test data and results of complex studies and develops appropriate recommendations.
Perform detailed review of clinical study protocols and development plans to ensure that the appropriate sample size and statistical methods are specified to support study endpoints.
Authors statistical sections of protocols for development-phase projects
Ensure that clinical monitoring plans, data management plans, and statistical analysis plans are harmonized and ensure delivery of high quality data and analysis results for regulatory submissions
Leads discussions and interacts with members of the Data Management and Clinical Affairs teams to align statistical endpoints of the clinical study to data collection, monitoring, and analysis activities.
Authors Statistical Analysis Plans (SAP) for clinical studies, maintains plan through life cycle of study, and executes analyses per plan.
Develops statistical analysis programs to merge complex data structures from multiple databases, create analysis data set (ADS) and generate tables, listings, and figures (TLF) for reports.
Participate in development of database clinical trial data specifications, including eCRF design, user requirements, edit rules/checks, query logic, and data validations. Ensures data specifications align with statistical analysis requirements and goals.
Provides complex specifications and directions to clinical data analysts for creation of ADS and/or TLFs.
Required Knowledge/Skills, Education, And Experience
Minimum educational background of MS in statistics/biostatistics or other quantitative field. Exceptional candidates with backgrounds bridging biological science and biostatistics will be considered.
At least 6 years of experience in biostatistics supporting development of medical diagnostics/devices. Candidate needs to demonstrate progressive experience in consulting and serving as lead consulting biostatistician on product development projects.
Assay Migration Experience strongly desired.
Proficiency in statistical programming (SAS, R) and query of relational database systems (SQL). Certifications in SAS programming a plus.
Experience with extraction of data and reporting within Clinical Data Management systems (e.g. Medidata RAVE, Medrio, Oracle Clinical). Experience with Open Clinica preferred.
Strong knowledge of statistical theory, experimental design and clinical trial methodologies, linear and nonlinear modeling, categorical and non-parametric methods, survival analysis, and associated sample size calculations.
Knowledge of diagnostic clinical trial statistics is required.
Ability to understand scientific questions and formulate statistical and data-analytic methods to provide solution to novel problems.
Familiar with FDA guidelines and CLSI guidelines applicable to medical diagnostics is a plus.
Experience with analysis of complex data from multiple source.
Ability to work independently and in project teams. Highly motivated and driven to exceed expectations.
Strong communication skills in English, written and spoken. Strong ability for public speaking in audiences with mixed backgrounds. Able to distill complex statistical concepts in simpler terms to generalized audiences
This position will be remotely based.
Show more
Show less","Biostatistics, Statistical programming, SAS, R, SQL, Data management, Clinical data management, Medidata RAVE, Medrio, Oracle Clinical, Open Clinica, Statistical theory, Experimental design, Clinical trial methodologies, Linear and nonlinear modeling, Categorical and nonparametric methods, Survival analysis, Sample size calculations, Diagnostic clinical trial statistics, FDA guidelines, CLSI guidelines, Data analysis, Communication skills, Public speaking","biostatistics, statistical programming, sas, r, sql, data management, clinical data management, medidata rave, medrio, oracle clinical, open clinica, statistical theory, experimental design, clinical trial methodologies, linear and nonlinear modeling, categorical and nonparametric methods, survival analysis, sample size calculations, diagnostic clinical trial statistics, fda guidelines, clsi guidelines, data analysis, communication skills, public speaking","biostatistics, categorical and nonparametric methods, clinical data management, clinical trial methodologies, clsi guidelines, communication skills, data management, dataanalytics, diagnostic clinical trial statistics, experimental design, fda guidelines, linear and nonlinear modeling, medidata rave, medrio, open clinica, oracle clinical, public speaking, r, sample size calculations, sas, sql, statistical programming, statistical theory, survival analysis"
Sr. BODS (Data Services) Developer (Hybrid),Benjamin Moore,"Montvale, NJ",https://www.linkedin.com/jobs/view/sr-bods-data-services-developer-hybrid-at-benjamin-moore-3760323398,2023-12-17,Montrose,United States,Mid senior,Onsite,"At Benjamin Moore, we empower our team members to achieve their goals and make a positive impact in our communities. We offer a rewarding and inspiring work environment that fosters creativity, collaboration, and a strong sense of camaraderie. Our culture of excellence and transparency encourages our colleagues to bring their authentic selves and unique perspectives/ideas every day. With 140 years of rich history behind our brand, we know that our people are the driving force behind our success. We believe in investing in our colleagues by offering work-life balance, competitive/ benefits, ongoing learning/continuing education, and skill development. Through a positive and engaging workplace, we facilitate growth, development, and fulfillment for all.
Join us and be a part of a brand that inspires creativity, innovation, and passion in support of locally-owned stores around the world.
Click here to see how you can paint your future!
The SAP BODS Consultant is responsible for keeping the SAP BODS system running smoothly and building and supporting data integration solutions. They should have experience with SAP HANA modeling, which involves creating and managing data models in SAP HANA using Calculation Views, dimensions, and SQL Scripting.
Job Function
Install, configure, and maintain SAP BODS servers and repositories
Manage user roles and permissions
Create and manage data sources and targets
Monitor and troubleshoot BODS jobs
Perform backup and recovery of BODS repositories
Implement performance tuning techniques
Deploy code across Dev, QA, and Production environments using central repositories.
Responsible for developing BODS workflows/dataflows/mapping.
Good knowledge of various BODS transformations including DQ-like address cleansing, data cleansing, and Match.
Possess strong knowledge/experience of HANA View Modeling, using calculation views and performance tuning.
Experience with SQL and procedures in Native HANA.
Loading delta records on both source and target level CDC on type 1, type 2, and type 3 SCD.
Must be a resourceful problem solver who possesses creativity in resolving problems
Possess a project-oriented approach to development work
Ability to work independently and drive work to completion
Ability to manage a heavy workload, and achieve results while meeting project deadlines with minimal supervision
Must be able to perform quality testing and beta testing of the applications that are delivered.
Identify, define, and deliver in parallel complex programs and projects/enhancements in a matrix environment.
Translates business goals into appropriate solutions while assessing the feasibility and optimization of the solution.
Work in Planning, Developing, Implementing, and Managing Data Warehouse Project Deliverables, Design Documents, and High-Level Data Mapping for SAP BODS ETL Specifications.
Recognize BODS-related risks/issues that need escalation and arrange for appropriate assistance.
Support Activities
Production Support
Must be able to analyze and troubleshoot data related issues (joins etc) in a timely manner and best practices of the company
Identify continuous process improvement opportunities and operationalize solutions for same.
Maintain SAP BODS Coding best practices and standards. Ensure that the best practices are enforced and followed during the development process.
Assist business and functional leads with day-to-day development, support, and troubleshooting.
Education
Bachelor's degree in Business (or Management), Computer Science, Engineering, or related discipline, or equivalent work experience is required
Experience
At least 5 years of experience in Business Objects Data Services (Admin, workflows. Mapping, dataflows, Data integrator/Data Quality/Platform transformations etc.)
At least 5 years of experience in Data warehousing.
At least 5 years delivering complex data management engagements including all phases of the project life cycle from scoping and planning, requirements gathering, design, development, testing, and go live phases.
Significant amount of experience in SAP HANA Modelling (calculation views)
Strong knowledge of data modeling and star schemas.
Working knowledge and experience with SQL language. 4+ Years.
A good understanding of best practices such as naming conventions in creating BODS jobs.
Experience working with a large data warehouse a plus
Experience in connecting BODS with a REST/SOAP API is a plus.
Experience in extracting semi-structured data like JSON is a plus
Have strong analytical skills and Strong process orientation with significant attention to detail
Effective interpersonal skills with the ability to participate in and lead ""Virtual"" teams
Compensation Philosophy
At Benjamin Moore, our brand represents excellence, and we strive to provide a comprehensive total rewards package to match. In addition to competitive base salary, every exempt and non-exempt role in our organization is eligible for a performance-based annual raise and bonus in recognition of their efforts that contribute to the success of our organization. We conduct regular pay audits, using both external market data and internal comparisons to ensure that our employees are compensated fairly and equitably. During our annual compensation review, we implement merit, equity, and promotion increases after a full calibration across all roles. We believe that benefits should connect you to the support you need when it matters most. In addition to our monetary compensation package, Benjamin Moore provides a full range of benefits that are personalized to help support you physically, financially, and emotionally, both through the big milestones and in your everyday life.
Diversity, Equity & Inclusion
At Benjamin Moore, we believe diversity of culture, talent, and varying perspectives is key to a collaborative, innovative, and successful business. We are committed to driving change within our organization through purposeful Diversity, Equity & Inclusion (DE&I) efforts, while also focusing on our customers and communities in which we serve. Our DE&I efforts allow each of us to authentically live our corporate values of Openness, Integrity, Community, Excellence and Safety. Our Social Impact programming of strategic local and national partnerships, in-kind donations, volunteerism, and more expands our ability to make a difference in the lives of all of our stakeholders. We hope you will join us and become an advocate for diversity and inclusion here at Benjamin Moore.
At Benjamin Moore & Co, we don’t just accept difference — we celebrate it, we support it, and we thrive because it benefits our employees, our products, and our community. We are an equal opportunity employer and value diversity. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Benjamin Moore is proud to be an equal opportunity employer.
Student Loan Repayment Assistance Program
Benjamin Moore provides Student Loan Repayment Assistance Program to support eligible active employees who graduated from an accredited post-secondary educational institution. The Repayment Program is intended to contribute to reducing employee’s student loans. The Company follows all rules and regulations concerning the taxability of student loan repayments provided under applicable law.
EOE
Benjamin Moore & Co. is an equal-opportunity employer that is committed to a culture of inclusion and diversity. We do not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, disability, national origin, veteran status, or any other basis covered by appropriate law. Our commitment to these principles means all employment decisions are made based on qualifications, merit, and business need.
Show more
Show less","SAP BODS, SAP HANA, Data Integration, Data Warehousing, SQL, Calculation Views, Dimensions, Data Sources, Data Targets, Data Quality, Data Cleansing, Data Mapping, ETL, Data Modeling, Star Schemas, REST API, SOAP API, JSON, Data Analysis, Data Architecture, Data Governance, Data Management, Problem Solving, Project Management, Communication Skills, Teamwork, Attention to Detail, Analytical Skills, Creativity, Innovation, Business Intelligence, Data Visualization, Data Security, Cloud Computing, Big Data, Machine Learning, Artificial Intelligence","sap bods, sap hana, data integration, data warehousing, sql, calculation views, dimensions, data sources, data targets, data quality, data cleansing, data mapping, etl, data modeling, star schemas, rest api, soap api, json, data analysis, data architecture, data governance, data management, problem solving, project management, communication skills, teamwork, attention to detail, analytical skills, creativity, innovation, business intelligence, data visualization, data security, cloud computing, big data, machine learning, artificial intelligence","analytical skills, artificial intelligence, attention to detail, big data, business intelligence, calculation views, cloud computing, communication skills, creativity, data architecture, data governance, data integration, data management, data mapping, data quality, data security, data sources, data targets, dataanalytics, datacleaning, datamodeling, datawarehouse, dimensions, etl, innovation, json, machine learning, problem solving, project management, rest api, sap bods, sap hana, soap api, sql, star schemas, teamwork, visualization"
Medical Data Analyst,Montefiore St. Luke's Cornwall,"Newburgh, NY",https://www.linkedin.com/jobs/view/medical-data-analyst-at-montefiore-st-luke-s-cornwall-3748499751,2023-12-17,Montrose,United States,Mid senior,Onsite,"Pay Rate
: $20-$24 per hour
Position Summary
: Develop and analyze medical data related to patient flow functions included but not limited to: Length of Stay, re-admission data, Volume indicators, and compliance with both internal and external benchmarks. Assists with automating regular reports. Identifies trends in the data that will be useful in making organizational changes.
Education/Training
: Associates degree required; Bachelors preferred. Ability to read, write and communicate in English.
Experience
: Minimum of one (1) year experience in an acute care hospital with basic understanding of medical terminology. Strong experience with Excel, Word, Access, and PowerPoint applications required. Experience with EPIC, Allscripts, MIDAS preferred. Ability to think critically, analyze data, and develop comprehensive reports
Montefiore St. Luke's Cornwall
(MSLC) is a not-for-profit hospital dedicated to serving the health care needs of those in the Hudson Valley. In January 2018, St. Luke's Cornwall Hospital officially partnered with the Montefiore Health System, making MSLC part of the leading organization in the country for population health management. With dedicated staff, modern facilities and state-of-the-art treatment, Montefiore St. Luke's Cornwall is committed to meeting the needs of the community and continuing to aspire to excellence.
We are proud to be on Becker's Hospital Review as one of the 150 Great Places to Work in Healthcare, as well as being a Certified Great Place to Work.
Visit https://www.greatplacetowork.com/certified-company/1379468 to learn what makes MSLC an exceptional place to work.
Show more
Show less","Data analysis, Medical data, Patient flow functions, Length of Stay, Readmission data, Volume indicators, Automation, Reporting, Trend analysis, Excel, Word, Access, PowerPoint, EPIC, Allscripts, MIDAS, Critical thinking, Data analysis, Report writing","data analysis, medical data, patient flow functions, length of stay, readmission data, volume indicators, automation, reporting, trend analysis, excel, word, access, powerpoint, epic, allscripts, midas, critical thinking, data analysis, report writing","access, allscripts, automation, critical thinking, dataanalytics, epic, excel, length of stay, medical data, midas, patient flow functions, powerpoint, readmission data, report writing, reporting, trend analysis, volume indicators, word"
Data Analyst 3,"APN Software Services, Inc.","Tarrytown, NY",https://www.linkedin.com/jobs/view/data-analyst-3-at-apn-software-services-inc-3734014600,2023-12-17,Montrose,United States,Mid senior,Onsite,"100% Remote role
12+ months
Description
The Senior Biostatistician will be responsible for providing technical expertise in biostatistics and to lead statistical evaluations of development and clinical validation studies for new in vitro diagnostics devices developed by Company Laboratory Diagnostics.
The role will interact cross-functionally with members of Clinical Affairs, Data Management, Medical Affairs, Regulatory Affairs and Global Assay Development to ensure high quality of data and analysis results used in regulatory submissions.
The role will leverage knowledge and experience in the areas of statistical techniques and programming to support product development goals within the Laboratory Diagnostics Division. This is an ideal role for candidates who are highly motivated, detail-oriented, have strong problem solving and scientific thinking skills, a collaborative mindset, and enthusiasm for topics related to biostatistics, data analysis, programming, presentation of data, and biological science. A willingness to share this enthusiasm with colleagues and to have fun is a must for our team.
The position will report to the Senior Director of Pre-Market Biostatistics.
Responsibilities
Responsible for consulting on the design of clinical trials and statistical analysis methodology for studies directed towards the development, verification, validation, and monitoring of in vitro diagnostics devices developed by Company Laboratory Diagnostics.
Recommends appropriate statistical methodology for the evaluation of analytical and clinical study data.
Interprets and evaluates test data and results of complex studies and develops appropriate recommendations.
Perform detailed review of clinical study protocols and development plans to ensure that the appropriate sample size and statistical methods are specified to support study endpoints.
Authors statistical sections of protocols for development-phase projects
Ensure that clinical monitoring plans, data management plans, and statistical analysis plans are harmonized and ensure delivery of high quality data and analysis results for regulatory submissions
Leads discussions and interacts with members of the Data Management and Clinical Affairs teams to align statistical endpoints of the clinical study to data collection, monitoring, and analysis activities.
Authors Statistical Analysis Plans (SAP) for clinical studies, maintains plan through life cycle of study, and executes analyses per plan.
Develops statistical analysis programs to merge complex data structures from multiple databases, create analysis data set (ADS) and generate tables, listings, and figures (TLF) for reports.
Participate in development of database clinical trial data specifications, including eCRF design, user requirements, edit rules/checks, query logic, and data validations. Ensures data specifications align with statistical analysis requirements and goals.
Provides complex specifications and directions to clinical data analysts for creation of ADS and/or TLFs.
Required Knowledge/Skills, Education, And Experience
Minimum educational background of MS in statistics/biostatistics or other quantitative field. Exceptional candidates with backgrounds bridging biological science and biostatistics will be considered.
At least 6 years of experience in biostatistics supporting development of medical diagnostics/devices. Candidate needs to demonstrate progressive experience in consulting and serving as lead consulting biostatistician on product development projects.
Assay Migration Experience strongly desired.
Proficiency in statistical programming (SAS, R) and query of relational database systems (SQL). Certifications in SAS programming a plus.
Experience with extraction of data and reporting within Clinical Data Management systems (e.g. Medidata RAVE, Medrio, Oracle Clinical). Experience with Open Clinica preferred.
Strong knowledge of statistical theory, experimental design and clinical trial methodologies, linear and nonlinear modeling, categorical and non-parametric methods, survival analysis, and associated sample size calculations.
Knowledge of diagnostic clinical trial statistics is required.
Ability to understand scientific questions and formulate statistical and data-analytic methods to provide solution to novel problems.
Familiar with FDA guidelines and CLSI guidelines applicable to medical diagnostics is a plus.
Experience with analysis of complex data from multiple source.
Ability to work independently and in project teams. Highly motivated and driven to exceed expectations.
Strong communication skills in English, written and spoken. Strong ability for public speaking in audiences with mixed backgrounds. Able to distill complex statistical concepts in simpler terms to generalized audiences
This position will be remotely based.
Show more
Show less","Biostatistics, Statistical Programming, SAS, R, SQL, Medical Diagnostics, Clinical Trials, Statistical Theory, Experimental Design, Clinical Trial Methodologies, Linear and Nonlinear Modeling, Categorical and NonParametric Methods, Survival Analysis, Sample Size Calculations, Diagnostic Clinical Trial Statistics, FDA Guidelines, CLSI Guidelines, Data Analysis, Project Teams, Communication Skills, Public Speaking, Medidata RAVE, Medrio, Oracle Clinical, Open Clinica","biostatistics, statistical programming, sas, r, sql, medical diagnostics, clinical trials, statistical theory, experimental design, clinical trial methodologies, linear and nonlinear modeling, categorical and nonparametric methods, survival analysis, sample size calculations, diagnostic clinical trial statistics, fda guidelines, clsi guidelines, data analysis, project teams, communication skills, public speaking, medidata rave, medrio, oracle clinical, open clinica","biostatistics, categorical and nonparametric methods, clinical trial methodologies, clinical trials, clsi guidelines, communication skills, dataanalytics, diagnostic clinical trial statistics, experimental design, fda guidelines, linear and nonlinear modeling, medical diagnostics, medidata rave, medrio, open clinica, oracle clinical, project teams, public speaking, r, sample size calculations, sas, sql, statistical programming, statistical theory, survival analysis"
Senior Data Analyst,Actalent,"Allendale, NJ",https://www.linkedin.com/jobs/view/senior-data-analyst-at-actalent-3787961949,2023-12-17,Montrose,United States,Mid senior,Onsite,"Description
Provides biostatistical consultation to clients or colleagues with manufacturing data analysis strategies
Builds capabilities and supports multiple regions on data analytics to establish process parameter ranges or identify trends.
Supports preparation of CPV (Continued Process Verification) plans or protocols for client processes.
Collects, trends, and interprets data to support root cause analysis and trending for CPV and APR (Annual Product Review) that are required for clinical and commercial manufacturing.
Develops data analysis modules and tools for Tech Transfer activities to establish critical process parameter specifications or ranges
Prepares statistical data packages for data monitoring committees, regulatory agencies, managers, or clients
Develops analytics to enhance business development opportunities
Additional Skills & Qualifications
Minimum BS/BA in statistics, computer science, mathematics, engineering and 3+ years of related experience
Knowledge of SAS programming and/or other statistical software
Knowledge and understanding of scientific approach and methodologies and the principles, concepts, methods, and standards of statistical processes.
Ability to apply a range of advanced statistical techniques in support of scientific research studies and/or experiments.
Strong focus and quality and timely delivery of work
Good computer skills and Microsoft Office knowledge
Thorough knowledge and understanding of cGMP and regulatory requirements preferred
Experience Level
Expert Level
Diversity, Equity & Inclusion
At Actalent, Diversity And Inclusion Are a Bridge Towards The Equity And Success Of Our People. DE&I Are Embedded Into Our Culture Through
Hiring diverse talent
Maintaining an inclusive environment through persistent self-reflection
Building a culture of care, engagement, and recognition with clear outcomes
Ensuring growth opportunities for our people
Actalent is an equal opportunity employer.
About Actalent
Actalent connects passion with purpose. We help visionary companies advance their engineering and science initiatives through access to specialized experts that drive scale, innovation, and speed to market. With a network of almost 30,000 engineering and sciences consultants and more than 4,500 clients across the U.S., Canada, Asia, and Europe, Actalent serves many of the Fortune 500. An operating company of Allegis Group, the global leader in talent solutions, Actalent launched as a new specialized engineering and sciences services and workforce solutions brand in 2021.
Show more
Show less","Statistical analysis, Data analytics, SAS programming, Statistical software, Data interpretation, Statistical techniques, Microsoft Office, cGMP, Regulatory requirements, Statistics, Computer science, Mathematics, Engineering","statistical analysis, data analytics, sas programming, statistical software, data interpretation, statistical techniques, microsoft office, cgmp, regulatory requirements, statistics, computer science, mathematics, engineering","cgmp, computer science, data interpretation, dataanalytics, engineering, mathematics, microsoft office, regulatory requirements, sas programming, statistical analysis, statistical software, statistical techniques, statistics"
Senior Data Analyst,Actalent,"Allendale, NJ",https://www.linkedin.com/jobs/view/senior-data-analyst-at-actalent-3787962743,2023-12-17,Montrose,United States,Mid senior,Onsite,"Description
Provides biostatistical consultation to clients or colleagues with manufacturing data analysis strategies
Builds capabilities and supports multiple regions on data analytics to establish process parameter ranges or identify trends.
Supports preparation of CPV (Continued Process Verification) plans or protocols for client processes.
Collects, trends, and interprets data to support root cause analysis and trending for CPV and APR (Annual Product Review) that are required for clinical and commercial manufacturing.
Develops data analysis modules and tools for Tech Transfer activities to establish critical process parameter specifications or ranges
Prepares statistical data packages for data monitoring committees, regulatory agencies, managers, or clients
Develops analytics to enhance business development opportunities
Additional Skills & Qualifications
Minimum BS/BA in statistics, computer science, mathematics, engineering and 3+ years of related experience
Knowledge of SAS programming and/or other statistical software
Knowledge and understanding of scientific approach and methodologies and the principles, concepts, methods, and standards of statistical processes.
Ability to apply a range of advanced statistical techniques in support of scientific research studies and/or experiments.
Strong focus and quality and timely delivery of work
Good computer skills and Microsoft Office knowledge
Thorough knowledge and understanding of cGMP and regulatory requirements preferred
Diversity, Equity & Inclusion
At Actalent, Diversity And Inclusion Are a Bridge Towards The Equity And Success Of Our People. DE&I Are Embedded Into Our Culture Through
Hiring diverse talent
Maintaining an inclusive environment through persistent self-reflection
Building a culture of care, engagement, and recognition with clear outcomes
Ensuring growth opportunities for our people
Actalent is an equal opportunity employer.
About Actalent
Actalent connects passion with purpose. We help visionary companies advance their engineering and science initiatives through access to specialized experts that drive scale, innovation, and speed to market. With a network of almost 30,000 engineering and sciences consultants and more than 4,500 clients across the U.S., Canada, Asia, and Europe, Actalent serves many of the Fortune 500. An operating company of Allegis Group, the global leader in talent solutions, Actalent launched as a new specialized engineering and sciences services and workforce solutions brand in 2021.
Show more
Show less","Biostatistical consultation, Data analytics, Data analysis strategies, Process parameter ranges, Trends, CPV (Continued Process Verification) plans, Data monitoring committees, Statistical data packages, SAS programming, Computer science, Mathematics, Engineering, cGMP, Regulatory requirements","biostatistical consultation, data analytics, data analysis strategies, process parameter ranges, trends, cpv continued process verification plans, data monitoring committees, statistical data packages, sas programming, computer science, mathematics, engineering, cgmp, regulatory requirements","biostatistical consultation, cgmp, computer science, cpv continued process verification plans, data analysis strategies, data monitoring committees, dataanalytics, engineering, mathematics, process parameter ranges, regulatory requirements, sas programming, statistical data packages, trends"
Marketing Data Analyst,Anteriad,"Rye Brook, NY",https://www.linkedin.com/jobs/view/marketing-data-analyst-at-anteriad-3770685985,2023-12-17,Montrose,United States,Mid senior,Onsite,"Come Join Our Team At Anteriad
and innovate the way B2B marketers make data-driven business decisions.
Why Join Our Intelligence & Analytics Team?
Anteriad is seeking a
Marketing Data Analyst
(MDA) to join our Intelligence and Analytics team on-site in our Rye Brook, NY headquarters. This is an incredible opportunity for an intelligent, energetic, and self-motivated individual to play a vital role within a growing Client Service and Support organization.
This position is not eligible for employer-visa sponsorship. This is an on-site position and local candidates will only be considered.
The MDA will support senior team members in executing key project deliverables as well as recurring analytical & reporting responsibilities. Database analysis, predictive analytics/modeling & heavy SQL/Excel work are required for this position. The successful candidate is a resourceful self-starter who is comfortable working independently as well as collaboratively in a small, fast paced environment. Above all, the MDA must possess
curiosity
as to why direct response campaigns “work” and an unwavering dedication to personal growth & development. The MDA will be expected to straddle the line between strategic thinking & tactical execution. In order to fit our culture, they must be prepared to work hard & be willing to attempt many different solutions until the best resolution is found.
Anteriad means “always moving forward” and we apply that to our company culture by tirelessly promoting an environment that allows our employees to thrive:
Flexible PTO
Training & development with unlimited access to Cornerstone Learning System
Steady, full-time role
Mix of collaborative & independent work
Community outreach via Anteriad Cares - Encouraging staff to take time to volunteer
Professional mentoring program - Career guidance from leadership
Employee Resource Groups - Collaborate with others that share your passions!
Great benefits for you and your family
Benefits We Bring To You:
Comprehensive medical (choice of 3 plans), dental and vision coverage
Company paid short-term disability, long term disability and life insurance
Optional supplemental life, accident and critical illness insurance plans
401K with company match
Flexible PTO and generous paid holiday schedule
Fully paid primary caregiver leave (12 weeks) & parental bonding leave (2 weeks)
Free Apple Fitness
Free Peloton Fitness
What You’ll Do:
Be an active data-story teller
Account Support for several accounts
Liaison between company, client(s) and contracted vendors
Effective communicator, adept with communication at all levels
Responsible for flawless execution of projects in support of the Analytics team
What You’ll Bring:
Required:
0-2 years professional work experience
Quantitative College degree (Computer Science, Math, Business Data/Analytics, Marketing, Finance or similar)
Strong communication (written and verbal), analytical, organizational, problem solving and computer skills.
Data manipulation and coding knowledge fundamentals via SQL, Python
Ability to multi-task & Detail oriented
Strong time-management skills
Self Starter/Self Motivated
Ability to learn new skills quickly
Ability to work well in a team environment
Bonus:
Certification or Class Completion In Google Analytics, Excel
SQL Data Coding Experience, Certification or Class Completion
This role is not eligible for employer visa sponsorship
Our Values:
Lead & Learn We lead with unrivaled vision, innovation and execution, always learning and embracing new ways of doing things to stay out in front
Collaborate & Celebrate We build great things when we work together as one Anteriad team, celebrating our achievements – both great and small – along the way
Innovate & Inspire We are always looking for bold new ways to exceed the expectations of our customers and to inspire each other to even greater success
Do More & Do Good We go above and beyond in the service of our clients and colleagues, and the communities where we live
Show more
Show less","SQL, Python, Google Analytics, Excel, Cornerstone Learning System, Predictive Analytics, Modeling, Data Manipulation, Coding","sql, python, google analytics, excel, cornerstone learning system, predictive analytics, modeling, data manipulation, coding","coding, cornerstone learning system, data manipulation, excel, google analytics, modeling, predictive analytics, python, sql"
Lead Data Engineer,XO Health Inc.,"Stamford, CT",https://www.linkedin.com/jobs/view/lead-data-engineer-at-xo-health-inc-3779647775,2023-12-17,Montrose,United States,Mid senior,Remote,"XO Health believes healthcare is fixable. Become part of the community changing the face of the industry.
XO Health is the first health plan designed by and for self-insured employers that delivers a more unified health experience for everyone – from those who receive care, to those who deliver it, to those who pay for it.
We are growing a multi-disciplinary team of diverse and digitally empowered employees ready to rebuild trust in healthcare through comprehensive and unified transformation.
About the Role:
The Lead Data Engineer is a critical role in our Data Engineering team who will work closely with Product Managers, Data Scientists and Software Engineers to support product launches and roadmaps by building the data architecture that informs and drives insight. In this role, you'll influence technology strategies, ensure that the technological solutions are aligned with the company's business needs and bring to life data and how it can impact positive healthcare outcomes.
The perfect candidate will have strong data infrastructure and data architecture skills, great understanding and experience with health plan data, a proven track record of technical leadership, strong operational skills to drive efficiency and speed, strong project management skills, and a vision for how data can be an enabler.
In This Role, You will:
Possess a hardworking ""can-do"" mindset with focus on the collective success of the scrum team to iteratively deliver high quality products to enable the XO Health business architecture, intelligence, and creation of intellectual property.
Collaborates with the Product Owner, Project Manager / Business Analyst, Subject Matter Experts and Development team to define and analyze user stories tracked in Jira.
Collaborate in design, development, and implementation of software/data solutions for using modern cloud native, API first technology stack.
Build cross-functional relationships with Data Scientists, Product Managers and Software Engineers, and PMO to understand data needs and deliver on those needs.
Drive the design, building, and launching of new data models and data pipelines in production.
Continually enhance full delivery pipeline through automation, expanded yet increasingly efficient test coverage, ultimately optimizing time-to-market and quality.
Participate in innovation in the team through continuous learning and building prototyping complex, cross platform business solutions.
Drive data quality across the product vertical and related business areas.
Support the delivery of high impact dashboards and data visualizations.
Define and manage SLAs for all data sets and processes running in production.
Mentor new or junior team members and participate in code reviews.
We're Looking for People Who Have:
A bachelor's or master's degree in a technical or business discipline, or equivalent experience.
5+ years of related data engineering, data science and/or business intelligence experience.
Extensive knowledge of contemporary frameworks, data/software engineering languages, diverse and emerging technologies.
3+ years data architecture experience.
3+ years development experience in at least one object-oriented language (Python, R, Java, etc.).
Hands on experience in building cloud native applications using AWS platform.
Strong experience building processes and architecture in AWS Stack and Snowflake.
Progressive experience with SQL and related database technologies.
Progressive experience with a variety of data management tools and technologies, and related tools, data visualization and data extraction and transformation tools.
Hands on experience in Python (ETL tools - Pandas/NumPy, Python unit testing etc.).
Experience with business intelligence tools and reporting solutions (PowerBI, Tableau).
Experience with build/deploy automation & DevOps frameworks (CI/CD, Bamboo, GitHub Actions, pipeline-as-code, AWS CDK).
Familiarity and experience in building and consuming APIs (REST, API Gateway, etc.) in an environment that uses multiple external applications.
Strong testing and version control methodology.
Must have strong problem-solving, analytical and in-depth research skills.
Possess the ability to communicate effectively with internal and external partners, both orally and written.
Experience using DBT is preferred.
Full compensation packages are based on candidate experience and relevant certifications.
$140,000—$160,000 USD
XO Health is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. XO Health promotes a drug-free workplace.
Show more
Show less","Data Engineering, Data Architecture, Data Pipelines, Data Models, Data Quality, Dashboarding, Data Visualization, SQL, Python, AWS, Snowflake, ETL, Unit Testing, PowerBI, Tableau, CI/CD, Bamboo, GitHub Actions, API Gateway, REST, Version Control, DBT","data engineering, data architecture, data pipelines, data models, data quality, dashboarding, data visualization, sql, python, aws, snowflake, etl, unit testing, powerbi, tableau, cicd, bamboo, github actions, api gateway, rest, version control, dbt","api gateway, aws, bamboo, cicd, dashboard, data architecture, data engineering, data models, data quality, datapipeline, dbt, etl, github actions, powerbi, python, rest, snowflake, sql, tableau, unit testing, version control, visualization"
Data Analyst 3 - Remote,eStaffing Inc.,"Tarrytown, NY",https://www.linkedin.com/jobs/view/data-analyst-3-remote-at-estaffing-inc-3637418371,2023-12-17,Montrose,United States,Mid senior,Remote,"Description
Responsibilities:
Provide consulting support to develop combined People & Insights strategy, support and assist in organizing and facilitating strategy workshops
Analyze existing people analytics and insights solutions
Develop reporting standards and define KPI/metrics and measurements
Partner with different stakeholders and support the setup of HR IT landscape (data lake, BI platforms)
Develop data analytics and reporting across different HR systems (e.g. Workday & Prism; Talent management; Learning platform; Employee survey etc.) focusing on Data extraction, transformation, modelling, analysis, visualization and presentation
Experience & Skills
3-5 yr of experience in the field of data analytics
Solid experience in Data engineering; Data visualization; Business intelligence; Statistical modelling; Data storytelling
Practical knowledge and experience working with BI platforms (e.g. PowerBI, Qlik), Data warehouse; Data lake (e.g. Prism, Snowflake);
Good stakeholder management and communication skills, collaborating across different HR functions
Comfortable in working in a fast-paced environment, leading parts of a project independently and proactive driving of responsibilities
Show more
Show less","Data Analytics, Data Engineering, Data Visualization, Business Intelligence, Statistical Modelling, Data Storytelling, BI Platforms (PowerBI Qlik), Data Warehouse, Data Lake (Prism Snowflake), Stakeholder Management, Communication Skills, Project Leadership, Proactive Driving of Responsibilities","data analytics, data engineering, data visualization, business intelligence, statistical modelling, data storytelling, bi platforms powerbi qlik, data warehouse, data lake prism snowflake, stakeholder management, communication skills, project leadership, proactive driving of responsibilities","bi platforms powerbi qlik, business intelligence, communication skills, data engineering, data lake prism snowflake, data storytelling, dataanalytics, datawarehouse, proactive driving of responsibilities, project leadership, stakeholder management, statistical modelling, visualization"
Data Engineer - Scala(U.S. remote),Railroad19,"Stamford, CT",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782886060,2023-12-17,Montrose,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, Building Restful APIs, AWS, EMR, S3, Relational and nonrelational databases","scala 212, spark 24, building restful apis, aws, emr, s3, relational and nonrelational databases","aws, building restful apis, emr, relational and nonrelational databases, s3, scala 212, spark 24"
Data Manager - Remote,eStaffing Inc.,"Tarrytown, NY",https://www.linkedin.com/jobs/view/data-manager-remote-at-estaffing-inc-3732677622,2023-12-17,Montrose,United States,Mid senior,Remote,"Description
REMOTE POSITION
The Data Manager oversees the studies to drive innovative database building, data collection, tracking and review solutions from complex, diverse sources to ensure timely delivery of high-quality data in a regulated and changing global clinical trial environment.
The role involves interfacing with statisticians, clinical affairs representatives, and external partners and vendors to design, configure, and test the clinical database systems. The CDM will also be involved in troubleshooting any issues on behalf of the end users and maintaining data standards.
The position will report to the Director of Data Management within Medical, Biostatistical and Design Quality (MBDQ).
Duties & Responsibilities
Conduct Study Preparation Meeting (SPM)
Create and Validate Data Management Report Specifications
Work with Database Engineer to create the EDC database specification
Upload preliminary testing data into database
Conduct User Acceptance Testing (UAT)
Create CRF Completion Guidelines
Create and sign-off on the Data Management Plan (DMP)
Drive the use of data standards (i.e., CDISC)
Participate in Study Element Team Meetings
Train end users on eDC system
Grant User Access for eDC system
Adds and/or updates external users' ids in GAMA
Contributes to departmental process improvement initiatives
Required Knowledge/Skills, Education, And Experience
Master's degree in a science related field
At least three (5) years data management and/or related work experience in a medical device or pharmaceutical industry/company
Working knowledge within areas of Good Clinical Practices, Good Clinical Data Management Practices, Design and Conduct of Clinical Trials/Studies, and related regulatory requirements and terminology
Experience working within a Clinical EDC system (e.g., OpenClincia, Medrio, Medidata Rave)
Exposure to clinical data coding classification systems, e.g., MedDRA, LOINC, WHO-Drug, Data Standardization (CDISC STDM &/or ADAM), Meta-Data Repository
Programming knowledge Python or R is desired
Strong interpersonal, organizational, and communication skills
Excellent attention to detail and problem-solving skills
Ability to work effectively work in a team setting
Show more
Show less","Data Management, Database Building, Data Collection, Data Tracking, Data Review, Clinical Trial Environment, Statistical Analysis, Clinical Affairs, External Partners, Vendors, Clinical Database Systems, Data Standards, User Acceptance Testing, CRF Completion Guidelines, Data Management Plan, CDISC, Study Element Team Meetings, eDC System, GAMA, Process Improvement, Science, Medical Device, Pharmaceutical Industry, Good Clinical Practices, Good Clinical Data Management Practices, Design and Conduct of Clinical Trials, Regulatory Requirements, Terminology, Clinical EDC System, OpenClinica, Medrio, Medidata Rave, Clinical Data Coding Classification Systems, MedDRA, LOINC, WHODrug, Data Standardization, CDISC STDM, ADAM, MetaData Repository, Programming, Python, R, Interpersonal Skills, Organizational Skills, Communication Skills, Attention to Detail, ProblemSolving Skills, Teamwork","data management, database building, data collection, data tracking, data review, clinical trial environment, statistical analysis, clinical affairs, external partners, vendors, clinical database systems, data standards, user acceptance testing, crf completion guidelines, data management plan, cdisc, study element team meetings, edc system, gama, process improvement, science, medical device, pharmaceutical industry, good clinical practices, good clinical data management practices, design and conduct of clinical trials, regulatory requirements, terminology, clinical edc system, openclinica, medrio, medidata rave, clinical data coding classification systems, meddra, loinc, whodrug, data standardization, cdisc stdm, adam, metadata repository, programming, python, r, interpersonal skills, organizational skills, communication skills, attention to detail, problemsolving skills, teamwork","adam, attention to detail, cdisc, cdisc stdm, clinical affairs, clinical data coding classification systems, clinical database systems, clinical edc system, clinical trial environment, communication skills, crf completion guidelines, data collection, data management, data management plan, data review, data standardization, data standards, data tracking, database building, design and conduct of clinical trials, edc system, external partners, gama, good clinical data management practices, good clinical practices, interpersonal skills, loinc, meddra, medical device, medidata rave, medrio, metadata repository, openclinica, organizational skills, pharmaceutical industry, problemsolving skills, process improvement, programming, python, r, regulatory requirements, science, statistical analysis, study element team meetings, teamwork, terminology, user acceptance testing, vendors, whodrug"
Data Manager - -Remote,eStaffing Inc.,"Tarrytown, NY",https://www.linkedin.com/jobs/view/data-manager-remote-at-estaffing-inc-3628156741,2023-12-17,Montrose,United States,Mid senior,Remote,"Description
REMOTE POSITION
The Data Manager oversees the studies to drive innovative database building, data collection, tracking and review solutions from complex, diverse sources to ensure timely delivery of high-quality data in a regulated and changing global clinical trial environment.
The role involves interfacing with statisticians, clinical affairs representatives, and external partners and vendors to design, configure, and test the clinical database systems. The CDM will also be involved in troubleshooting any issues on behalf of the end users and maintaining data standards.
The position will report to the Director of Data Management within Medical, Biostatistical and Design Quality (MBDQ).
Duties & Responsibilities
Conduct Study Preparation Meeting (SPM)
Create and Validate Data Management Report Specifications
Work with Database Engineer to create the EDC database specification
Upload preliminary testing data into database
Conduct User Acceptance Testing (UAT)
Create CRF Completion Guidelines
Create and sign-off on the Data Management Plan (DMP)
Drive the use of data standards (i.e., CDISC)
Participate in Study Element Team Meetings
Train end users on eDC system
Grant User Access for eDC system
Adds and/or updates external users' ids in GAMA
Contributes to departmental process improvement initiatives
Required Knowledge/Skills, Education, And Experience
Master's degree in a science related field
At least three (5) years data management and/or related work experience in a medical device or pharmaceutical industry/company
Working knowledge within areas of Good Clinical Practices, Good Clinical Data Management Practices, Design and Conduct of Clinical Trials/Studies, and related regulatory requirements and terminology
Experience working within a Clinical EDC system (e.g., OpenClincia, Medrio, Medidata Rave)
Exposure to clinical data coding classification systems, e.g., MedDRA, LOINC, WHO-Drug, Data Standardization (CDISC STDM &/or ADAM), Meta-Data Repository
Programming knowledge Python or R is desired
Strong interpersonal, organizational, and communication skills
Excellent attention to detail and problem-solving skills
Ability to work effectively work in a team setting
Show more
Show less","Data Management, Database Building, Data Collection, Data Tracking, Data Review, Clinical Trials, Statistical Analysis, Clinical Database Systems, Data Standards, CDISC, eDC Systems, OpenClincia, Medrio, Medidata Rave, Clinical Data Coding, MedDRA, LOINC, WHODrug, Data Standardization, CDISC STDM, ADAM, MetaData Repository, Programming, Python, R, Interpersonal Skills, Organizational Skills, Communication Skills, Attention to Detail, ProblemSolving Skills, Teamwork","data management, database building, data collection, data tracking, data review, clinical trials, statistical analysis, clinical database systems, data standards, cdisc, edc systems, openclincia, medrio, medidata rave, clinical data coding, meddra, loinc, whodrug, data standardization, cdisc stdm, adam, metadata repository, programming, python, r, interpersonal skills, organizational skills, communication skills, attention to detail, problemsolving skills, teamwork","adam, attention to detail, cdisc, cdisc stdm, clinical data coding, clinical database systems, clinical trials, communication skills, data collection, data management, data review, data standardization, data standards, data tracking, database building, edc systems, interpersonal skills, loinc, meddra, medidata rave, medrio, metadata repository, openclincia, organizational skills, problemsolving skills, programming, python, r, statistical analysis, teamwork, whodrug"
Lead Data Statistician,Jackson Lewis P.C.,"Harrison, NY",https://www.linkedin.com/jobs/view/lead-data-statistician-at-jackson-lewis-p-c-3755012097,2023-12-17,Montrose,United States,Mid senior,Remote,"Focused on labor and employment law since 1958, Jackson Lewis P.C.’s 950+ attorneys located in major cities nationwide consistently identify and respond to new ways workplace law intersects business. We help employers develop proactive strategies, strong policies and business-oriented solutions to cultivate high-functioning workforces that are engaged, stable and diverse, and share our clients' goals to emphasize inclusivity and respect for the contribution of every employee.
The Firm is ranked in the First Tier nationally in the category of Labor and Employment Litigation, as well as in both Employment Law and Labor Law on behalf of Management, in the U.S. News - Best Lawyers® “Best Law Firms”.
Job Summary
The Lead Data Statistician role in the Wage & Hour Data Analytics group provides support on various projects, including but not limited to, single plaintiff, class action, collective action, and DOL audit client exposure analyses, largely based on client timekeeping and payroll data. This individual will work autonomously to handle complex attorney/client issues and assist in overseeing the work of analysts.
Essential Functions
Regularly work as primary contact with attorneys, clients, and team manager to determine strategies on projects involving but not limited to class action data analytics.
Provide support on Practice Group initiatives, marketing efforts and pitches
Assess client exposure under various litigation scenarios – class action, collective action, etc.
Provide clear and concise communications regarding the analysis processes and results – both written and oral – to attorneys and client contacts, with the ability to explain complex topics in an easy-to-understand manner.
Develop methods and strategies to reach the end goal of a project based on various types/formats of raw client data.
Work with attorneys and analysts for analysis revisions, often on-the-fly during mediation proceedings.
Develop strong and effective relationships with clients
Execute assigned tasks in a self-directed, proactive manner, while actively managing own time and quality of work
Appropriately identify areas of discrete code and/or analytical methods that should be utilized for project analysis.
Expand scope of data tools employed by analysis team, including R, in conjunction with data and process integration, which is ongoing.
Ability to lead a project, as needed, from data intake and methodology development to analysis completion.
Qualifications/Skills Required
Minimum three years of experience directly working with Wage & Hour topics and data (timekeeping and payroll data especially) preferred, ideally within a legal or HR context.
Direct experience with California Wage & Hour laws a large plus.
Expert working knowledge with Excel, especially with basic and intermediate Excel formulas (vlookups, sumifs, etc.). VBA experience a large plus.
Ability to use Excel formulas and mathematics to manipulate raw data into dollar-figure exposure estimates.
Critical thinking and problem-solving skills with attention to detail. Experience in R, SQL, Python, other statistical programs a plus.
Ability to write programs/macros to automate repetitive statistical and data tasks.
Strong organizational and communication skills.
Ability to determine client and project needs and independently design and prepare data and analyses
Ability to multi-task and manage multiple projects, from start to finish, with shifting priority levels.
Willingness to conform to standardized group practices.
Educational Requirements
Master’s degree in applied statistics, mathematics, econometrics or a similar field or commensurate experience or equivalent level of training.
For Dallas, TX, the expected hourly wage range for this position is between $38.00/hour – $51.00/hour. The actual compensation will be determined based on experience and other factors permitted by law.
In accordance with the Colorado Equal Pay for Equal Work Act, the expected salary range for this Colorado position is between $38.00/hour – $51.00/hour. The actual compensation will be determined based on experience and other factors permitted by law.
Jackson Lewis offers a competitive benefits package that includes:
medical, dental, vision, life and disability insurance
401(k) Retirement Plan
Flexible Spending & Health Savings Account
firm-paid holidays, vacation, and sick time
For California and NY State, the expected salary range for this position is between $38/hour and $51/hour. The actual compensation will be determined based on experience and other factors permitted by law.
Jackson Lewis understands that embracing our differences makes us a stronger, better firm. We appreciate the importance of having a workforce that reflects the various communities in which we work, and we strive to create an inclusive environment where diverse employees want to work and where they can flourish professionally. In furtherance of our culture, all qualified applicants will receive consideration for employment without regard to race, national origin, gender, age, religion, disability, sexual orientation, veteran status, marital status or any other characteristics protected by law.
Show more
Show less","Wage & Hour laws, Excel, Basic and intermediate Excel formulas (vlookups sumifs etc.), VBA, Statistical programs (R SQL Python), Mathematics, Critical thinking, Problemsolving, Data analysis, Data manipulation, Automation, Data visualization, Communication, Project management, Applied statistics, Econometrics","wage hour laws, excel, basic and intermediate excel formulas vlookups sumifs etc, vba, statistical programs r sql python, mathematics, critical thinking, problemsolving, data analysis, data manipulation, automation, data visualization, communication, project management, applied statistics, econometrics","applied statistics, automation, basic and intermediate excel formulas vlookups sumifs etc, communication, critical thinking, data manipulation, dataanalytics, econometrics, excel, mathematics, problemsolving, project management, statistical programs r sql python, vba, visualization, wage hour laws"
Lead Data Statistician,Jackson Lewis P.C.,"White Plains, NY",https://www.linkedin.com/jobs/view/lead-data-statistician-at-jackson-lewis-p-c-3755011183,2023-12-17,Montrose,United States,Mid senior,Remote,"Focused on labor and employment law since 1958, Jackson Lewis P.C.’s 950+ attorneys located in major cities nationwide consistently identify and respond to new ways workplace law intersects business. We help employers develop proactive strategies, strong policies and business-oriented solutions to cultivate high-functioning workforces that are engaged, stable and diverse, and share our clients' goals to emphasize inclusivity and respect for the contribution of every employee.
The Firm is ranked in the First Tier nationally in the category of Labor and Employment Litigation, as well as in both Employment Law and Labor Law on behalf of Management, in the U.S. News - Best Lawyers® “Best Law Firms”.
Job Summary
The Lead Data Statistician role in the Wage & Hour Data Analytics group provides support on various projects, including but not limited to, single plaintiff, class action, collective action, and DOL audit client exposure analyses, largely based on client timekeeping and payroll data. This individual will work autonomously to handle complex attorney/client issues and assist in overseeing the work of analysts.
Essential Functions
Regularly work as primary contact with attorneys, clients, and team manager to determine strategies on projects involving but not limited to class action data analytics.
Provide support on Practice Group initiatives, marketing efforts and pitches
Assess client exposure under various litigation scenarios – class action, collective action, etc.
Provide clear and concise communications regarding the analysis processes and results – both written and oral – to attorneys and client contacts, with the ability to explain complex topics in an easy-to-understand manner.
Develop methods and strategies to reach the end goal of a project based on various types/formats of raw client data.
Work with attorneys and analysts for analysis revisions, often on-the-fly during mediation proceedings.
Develop strong and effective relationships with clients
Execute assigned tasks in a self-directed, proactive manner, while actively managing own time and quality of work
Appropriately identify areas of discrete code and/or analytical methods that should be utilized for project analysis.
Expand scope of data tools employed by analysis team, including R, in conjunction with data and process integration, which is ongoing.
Ability to lead a project, as needed, from data intake and methodology development to analysis completion.
Qualifications/Skills Required
Minimum three years of experience directly working with Wage & Hour topics and data (timekeeping and payroll data especially) preferred, ideally within a legal or HR context.
Direct experience with California Wage & Hour laws a large plus.
Expert working knowledge with Excel, especially with basic and intermediate Excel formulas (vlookups, sumifs, etc.). VBA experience a large plus.
Ability to use Excel formulas and mathematics to manipulate raw data into dollar-figure exposure estimates.
Critical thinking and problem-solving skills with attention to detail. Experience in R, SQL, Python, other statistical programs a plus.
Ability to write programs/macros to automate repetitive statistical and data tasks.
Strong organizational and communication skills.
Ability to determine client and project needs and independently design and prepare data and analyses
Ability to multi-task and manage multiple projects, from start to finish, with shifting priority levels.
Willingness to conform to standardized group practices.
Educational Requirements
Master’s degree in applied statistics, mathematics, econometrics or a similar field or commensurate experience or equivalent level of training.
For Dallas, TX, the expected hourly wage range for this position is between $38.00/hour – $51.00/hour. The actual compensation will be determined based on experience and other factors permitted by law.
In accordance with the Colorado Equal Pay for Equal Work Act, the expected salary range for this Colorado position is between $38.00/hour – $51.00/hour. The actual compensation will be determined based on experience and other factors permitted by law.
Jackson Lewis offers a competitive benefits package that includes:
medical, dental, vision, life and disability insurance
401(k) Retirement Plan
Flexible Spending & Health Savings Account
firm-paid holidays, vacation, and sick time
For California and NY State, the expected salary range for this position is between $38/hour and $51/hour. The actual compensation will be determined based on experience and other factors permitted by law.
Jackson Lewis understands that embracing our differences makes us a stronger, better firm. We appreciate the importance of having a workforce that reflects the various communities in which we work, and we strive to create an inclusive environment where diverse employees want to work and where they can flourish professionally. In furtherance of our culture, all qualified applicants will receive consideration for employment without regard to race, national origin, gender, age, religion, disability, sexual orientation, veteran status, marital status or any other characteristics protected by law.
Show more
Show less","Labor law, Employment law, Class action data analytics, Excel, VBA, R, SQL, Python, Statistics, Data analysis, Data manipulation, Automation, Communication, Multitasking, Project management, Mathematics","labor law, employment law, class action data analytics, excel, vba, r, sql, python, statistics, data analysis, data manipulation, automation, communication, multitasking, project management, mathematics","automation, class action data analytics, communication, data manipulation, dataanalytics, employment law, excel, labor law, mathematics, multitasking, project management, python, r, sql, statistics, vba"
Data Architect,Arcadis,"Fair Lawn, NJ",https://www.linkedin.com/jobs/view/data-architect-at-arcadis-3749503138,2023-12-17,Montrose,United States,Mid senior,Remote,"Arcadis is the world's leading company delivering sustainable design, engineering, and consultancy solutions for natural and built assets.
We are more than 36,000 people, in over 70 countries, dedicated to improving quality of life. Everyone has an important role to play. With the power of many curious minds, together we can solve the world’s most complex challenges and deliver more impact together.
Overview:
The Information Systems Data Architect will be part of part of a design and development that includes database developers, data architects, data analysts and process automation team. The mission will be to develop and maintain application integration, Data Warehouse by using on-premises MS SQL Server, Azure SQL Managed instance, Azure SQL Database, SSIS, Azure data factory, Azure storage account, Azure App Service, and other data management tools.
Responsibilities:
Lead Data Warehouse design, ETL architecture design, security profile management, data share design and other data architecture application for internal users and corporate transactional applications. Also act as other billable sector data architect consultant.
Act as application solutions architect, lead and design corporation internal application by using Microsoft C#, Azure Web App Service, Web jobs, Azure function App Service.
Admin and monitor corporate service Azure Cloud resources. Based on architecture design, create resources for corporate service application, monitoring resources status and billing status, manage resources access privilege, Suggest new functions in resources for development team.
Act as scrum master in corporate service development teams. Manage sprint by hosting daily standup meeting and bi-weekly sprint meeting, check development activities fit to agile methodology and continuously helping improve teams’ productivity.
Manage a high dynamic development team, and prioritize multiple tasks by working with various stakeholders, maximize team resources utilization to meet each project deadline within limited resources.
Admin Azure DevOps, manage users, user access level, user privileges, processes and its rules, repos for DW, Applications, ETL, etc. Sprints planning, executing, and reviewing.
Act as company Power BI Admin, manage all kinds of Power BI license (such as pro license, premium capacity, embedded capacity) companywide, research and learn Power BI admin parameter in Power BI portal and implement proper setup. Manage embedded Power BI solutions by providing training and guide to development team. Manage Power BI gateways which are used by Power BI, Power Automate.
Qualifications:
Please ensure to provide a description of the projects you have worked on within your resume.
A University bachelor’s degree in computer science with programming experience, or a degree in a related discipline.
Minimum 3 to 5 years of related Data Warehouse, ETL, application integration design and development experience with Microsoft Azure SQL database, data factory, storage, app service, synapse analytics, data lake, network, Azure resource management.
Possess excellent communication, presentation, and leadership skills and be able to work in a dynamic environment with rapidly changing environment. Demonstrated ability to achieve goals by leading team in an innovative and fast paced environment.
Strong experience with SSIS package and pipeline design, development, deployment, monitoring and scheduling.
Strong experience with MS SQL Server design, development, deployment, performance tuning, monitoring.
Strong experience with Azure DevOps in user, project, repo, wiki management. Can observe and improve current process.
Strong experience with Power BI license and parameter admin, can provide user proper Power BI license and proper guide to start with, can research and setup proper parameters under Power BI admin portal, can manage Power BI gateway by applying patch, observing performance, resolving bottle neck.
Strong experience with application integration between corporate service applications such as HRIS, CRM, ERP, CPM by using C# and Azure App Service.
MS Certified: Azure Solution Architect Expert and/or MS Certified: Azure Data Engineer Associate will be an asset.
Previous working experience with extracting data from an ERP application, HRIS application, CRM application is highly preferred.
Why Arcadis?
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills-based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You’ll do meaningful work, and no matter what role, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark, on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Join Arcadis. Create a Legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We are an equal opportunity and affirmative action employer. Women, minorities, people with disabilities and veterans are strongly encouraged to apply. We are dedicated to a policy of non-discrimination in employment on any basis including race, creed, color, religion, national origin, sex, age, disability, marital status, sexual orientation, gender identity, citizenship status, disability, veteran status, or any other basis prohibited by law.
Arcadis offers benefits for full time and part time positions. These benefits include medical, dental, and vision, EAP, 401K, STD, LTD, AD&D, life insurance, paid parental leave, reward & recognition program and optional benefits including wellbeing benefits, adoption assistance and tuition reimbursement. We offer seven paid holidays and potentially up to two floating holidays per calendar year depending on start date, and 15 days PTO that accrue per year. The salary range for this position is $95000 - $110000.
Show more
Show less","MS SQL Server, Azure SQL Managed instance, Azure SQL Database, SSIS, Azure data factory, Azure storage account, Azure App Service, Microsoft C#, Azure Web App Service, Web jobs, Azure function App Service, Scrum master, Agile methodology, Azure DevOps, Power BI, Power Automate, Power BI gateways, Data Warehouse, ETL, Data architecture, Application integration, Synapse analytics, Data lake, Azure resource management, SSIS package, Pipeline design, Azure DevOps, User project repo wiki management, Application integration, C#, Azure App Service, Azure Solution Architect Expert, Azure Data Engineer Associate, ERP application, HRIS application, CRM application","ms sql server, azure sql managed instance, azure sql database, ssis, azure data factory, azure storage account, azure app service, microsoft c, azure web app service, web jobs, azure function app service, scrum master, agile methodology, azure devops, power bi, power automate, power bi gateways, data warehouse, etl, data architecture, application integration, synapse analytics, data lake, azure resource management, ssis package, pipeline design, azure devops, user project repo wiki management, application integration, c, azure app service, azure solution architect expert, azure data engineer associate, erp application, hris application, crm application","agile methodology, application integration, azure app service, azure data engineer associate, azure data factory, azure devops, azure function app service, azure resource management, azure solution architect expert, azure sql database, azure sql managed instance, azure storage account, azure web app service, c, crm application, data architecture, data lake, datawarehouse, erp application, etl, hris application, microsoft c, ms sql server, pipeline design, power automate, power bi gateways, powerbi, scrum master, ssis, ssis package, synapse analytics, user project repo wiki management, web jobs"
Senior Data Engineer,Cephas Consultancy Services Private Limited,"Stamford, CT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-cephas-consultancy-services-private-limited-3577425683,2023-12-17,Montrose,United States,Mid senior,Hybrid,"Positions: 1 Total Experience: 5 – 10 years Our Client, A global integrated media and entertainment company, is seeking an Senior Engineer, Data Engineering Location:Stamford, CT/ Hybrid Job Summary: The Company has an Enterprise Data Warehouse and continues to focus on improving the data-driven decision-making and automation processes. This role will be reporting to the Vice President, Data Platform Engineering and supervise the team of ETL Developers, Data Engineers, QA, and Data analysts who build, enhance and optimize ETL (Extract, Transform, Load) jobs, implement data lake and warehouse solutions and manage their day-to-day tasks. The Senior Manager will be responsible for the continued success of the data warehouse release cycle process, prioritizing items for inclusion, and ensuring those tasks are completed per schedule. The candidate must have strong data lake, data warehouse, data pipeline design/development, data modeling, SQL, AWS Cloud, and Big Data Technologies experience. You will have the opportunity to contribute to end-to-end platform design for our cloud architecture and work multi-functionally with operations, data science, and the business segments to build batch and real-time data solutions. The role will be part of the Data Platform Engineering team supporting Company Corporate Marketing, Products, Content, and Consumer business units. This is also a hands-on position, requiring extensive experience developing and maintaining complex data warehouse and data transformation processes on AWS cloud as well as good decision making, leadership, time management, prioritization, and escalation skills are required to ensure all objectives are met
Key Responsibilities: Oversee the on-going development of the enterprise data warehouse using inputs from the business end-user’s perspective to produce timely and accurate information for management decision making and planning Supervise the EDW/ETL team, including the design and development of the data warehouse Evaluate EDW/ETL team members and provide input into the performance planning and evaluation process Work with business stakeholders and data SMEs to clarify requirements and business rules Responsible for the prompt, courteous, and efficient management of all support requests to the Data Warehouse team, including ad-hoc data analysis requests Oversee efforts to design, develop, implement, and validate ETL/ELT processes Architect analytical data layers (marts, aggregates, reporting, semantic layer) and define methods of building and consuming data (views, tables, extracts, caching) leveraging CI/CD approaches with tools such as Python and dbt. Provide expertise and guidance in the creation and refinement of agile technical stories for data architecture and development Work with Data Management to establish governance processes around meta-data to ensure an integrated definition of data for enterprise information, and to ensure the accuracy, validity, and reusability of metadata Assist in developing long-term strategies and capacity planning for meeting future data warehouse and data automation needs Experience in Delivering solutions using Agile methodologies and supporting tools like Jira Experience in building CI/CD pipelines on AWS and Google Cloud Conduct POC’s for Data Integration tools, continuously assessing, automating, and evolving data Integration roadmap Responsible for creating and maintaining inbound and outbound Data Delivery SLA’s Ability to implement PII data policies and principles to ensure data is secured, acceptable and accurate Document and maintain all the processes and procedures pertaining to ETL and data warehouse usage/development of the organization, including a data dictionary Education & Technical Experience Requirements Bachelors in computer science, science, or similar technical degree 10+ years of hands-on experience in building data pipeline using traditional ETL tech stack and big data technologies 5+ years of experience with traditional ETL/ELT technologies such as Talend, Informatica, SSIS, MPP, and NoSQL Databases with the recent experience on AWS cloud technologies 3+ years of experience with AWS services including S3, Lambda, Redshift, EMR, and RDS databases such as MySQL 3+ years of Expertise in Snowflake data modeling, ELT using Snowflake SQL or Modern Data Replication tools (i.e., Five Tran), Snowflake Store Procedures / UDF / advanced SQL scripting, and standard Data Lake / Data Warehouse concepts. 3+ years of experience with Hadoop, Hive, PySpark, Airflow, and Streaming Technologies (Kinesis/Kafka) 3+ years of experience in programming languages such as Python, R, Java, C# 3+ years of team management experience Experience designing, deploying and supporting production cloud services in AWS ecosystem including EC2, VPC, SQS, RDS, ELB, EBS, S3, KMS, Redshift Experience in AWS MLOPS tools and understanding of the data science model development processes Solid experience in data modeling, star/snowflake schemas design/development, and dimensional modeling Proficient in Data warehousing theory, practice and combined with strong hands-on experience in data warehousing, frameworks development, and data architecture Hands-on experience in managing and delivering high-performance ETL solutions Must have experience in implementing the end to end data warehousing solution Excellent communication and collaboration skills Able to sell concepts and designs/benefits to multiple audiences Able to work within a cross-functional team environment with people from multiple business units, vendors, countries, and cultures. Experience in .NET, C#, Java, Web Development is a plus Experience in Social Media Datasets such as Twitter, YouTube, Facebook, Instagram is a plus Experience in Google Clickstream, DFP, or Adobe Analytics datasets is a plus Experience in dealing with the Media content subscription-based datasets is a plus AWS Cloud Certification is a plus Experience in Media & Entertainment industry is a plus
Show more
Show less","Data Engineering, Data Modeling, SQL, AWS Cloud, Big Data Technologies, Data Lake, Data Warehouse, ETL (Extract Transform Load), Data Pipeline Design/Development, Python, dbt, Apache Spark, Snowflake, Hadoop, Hive, PySpark, Airflow, Streaming Technologies, Kinesis/Kafka, Java, C#, .NET, Web Development, AWS MLOPS tools, Data Science, Star/Snowflake Schemas, Dimensional Modeling, Data Warehousing, Frameworks Development, Data Architecture, HighPerformance ETL Solutions, EndtoEnd Data Warehousing, Communication Skills, Collaboration Skills, CrossFunctional Team Environment, Media & Entertainment Industry","data engineering, data modeling, sql, aws cloud, big data technologies, data lake, data warehouse, etl extract transform load, data pipeline designdevelopment, python, dbt, apache spark, snowflake, hadoop, hive, pyspark, airflow, streaming technologies, kinesiskafka, java, c, net, web development, aws mlops tools, data science, starsnowflake schemas, dimensional modeling, data warehousing, frameworks development, data architecture, highperformance etl solutions, endtoend data warehousing, communication skills, collaboration skills, crossfunctional team environment, media entertainment industry","airflow, apache spark, aws cloud, aws mlops tools, big data technologies, c, collaboration skills, communication skills, crossfunctional team environment, data architecture, data engineering, data lake, data pipeline designdevelopment, data science, datamodeling, datawarehouse, dbt, dimensional modeling, endtoend data warehousing, etl extract transform load, frameworks development, hadoop, highperformance etl solutions, hive, java, kinesiskafka, media entertainment industry, net, python, snowflake, spark, sql, starsnowflake schemas, streaming technologies, web development"
Lead Data Engineer,Zortech Solutions,"Tarrytown, NY",https://www.linkedin.com/jobs/view/lead-data-engineer-at-zortech-solutions-3667476705,2023-12-17,Montrose,United States,Mid senior,Hybrid,"Role : Lead Data Engineer
Location : Tarrytown NY 10591 (Hybrid role 3 days onsite 2 days WFH)
Duration: 6-12+ Months
Job Description
Must have AWS , Apache Airflow , Pyspark , Redshift
Candidate should have 12+ years of experience in Data Engineering
Designing, creating, testing and maintaining the complete data management & processing systems.
Working closely with the stakeholders & solution architect.
Ensuring architecture meets the business requirements.
Building highly scalable, robust & fault-tolerant systems.
Taking care of the complete ETL process.
Knowledge of Hadoop ecosystem and different frameworks inside it HDFS, YARN, MapReduce, Apache Pig, Hive, Flume, Sqoop, ZooKeeper, Oozie, Impala and Kafka
Must have knowledge and working experience in Real-time processing Framework (Apache Spark), PySpark and in AWS Redshift
Must have experience on SQL-based technologies (e.g. MySQL/ Oracle DB) and NoSQL technologies (e.g. Cassandra and MongoDB)
Should have Python/Scala/Java Programming skills
Discovering data acquisitions opportunities
Finding ways & methods to find value out of existing data.
Improving data quality, reliability & efficiency of the individual components & the complete system.
Creating a complete solution by integrating a variety of programming languages & tools together.
Creating data models to reduce system complexities and hence increase efficiency & reduce cost.
Introducing new data management tools & technologies into the existing system to make it more efficient.
Setting & achieving individual as well as the team goal.
Problem solving mindset working in agile environment
Show more
Show less","AWS, Apache Airflow, Pyspark, Redshift, Hadoop, HDFS, YARN, MapReduce, Apache Pig, Hive, Flume, Sqoop, ZooKeeper, Oozie, Impala, Kafka, SQL, MySQL, Oracle DB, NoSQL, Cassandra, MongoDB, Python, Scala, Java","aws, apache airflow, pyspark, redshift, hadoop, hdfs, yarn, mapreduce, apache pig, hive, flume, sqoop, zookeeper, oozie, impala, kafka, sql, mysql, oracle db, nosql, cassandra, mongodb, python, scala, java","apache airflow, apache pig, aws, cassandra, flume, hadoop, hdfs, hive, impala, java, kafka, mapreduce, mongodb, mysql, nosql, oozie, oracle db, python, redshift, scala, spark, sql, sqoop, yarn, zookeeper"
IQVIA Data Analyst,EPM Scientific,"Westchester County, NY",https://www.linkedin.com/jobs/view/iqvia-data-analyst-at-epm-scientific-3778576478,2023-12-17,Montrose,United States,Mid senior,Hybrid,"The ideal candidate will use their passion for big data and analytics to provide insights to the business covering a range of topics. This is a highly collaborative role that will manage various projects and work cross-functionally to main marketing and reporting information across the departments.
Responsibilities
Analyze IQVIA prescription trends, sales trends, market intelligence, and more to create launch strategies
Review monthly performance of Generic and generate and forecast Generics Sales Budget plan, as we all working cross-functionally across various departments to identify areas of improvement
Provide financial analysis of products
Collect data and monitor / website management
Qualifications
3+ years Data Analysis experience
1-3+ years Pharma industry experience
IQVIA data experience
Experience with Data Mining- IMS data
**(This opportunity is unable to provide sponsorship at this time).
If the above is something you'd be interested in learning more about, please don't hesitate to apply.
Show more
Show less","Data Analysis, Pharma industry, IQVIA data, Data Mining, IMS data","data analysis, pharma industry, iqvia data, data mining, ims data","data mining, dataanalytics, ims data, iqvia data, pharma industry"
Senior Cloud Data Engineer,BDO USA,"Valhalla, NY",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765470285,2023-12-17,Montrose,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, AI Algorithms, Machine Learning, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, SQL, Data Definition Language (DDL), Data Manipulation Language (DML), Views, Functions, Stored Procedures, Performance Tuning, C#, Python, Java, Scala, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, Automation Tools, Computer Vision, UiPath, Alteryx, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SSIS, SSAS, SSRS, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, ai algorithms, machine learning, data warehousing, data modeling, semantic model definition, star schema construction, sql, data definition language ddl, data manipulation language dml, views, functions, stored procedures, performance tuning, c, python, java, scala, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, automation tools, computer vision, uipath, alteryx, aws, redshift, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, qlik, athena, data pipeline, glue, star schema, data modeling, ssis, ssas, ssrs, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql","ai algorithms, alteryx, athena, automation tools, aws, aws lake formation, azure analysis services, batch data ingestion, bicep, business intelligence, c, computer vision, data definition language ddl, data lake medallion architecture, data manipulation language dml, data ops, data pipeline, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops, functions, git, glue, java, kinesis, linux, machine learning, microsoft fabric, pandas, performance tuning, powerbi, purview, python, qlik, quicksight, redshift, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema, star schema construction, stored procedures, streaming data ingestion, terraform, uipath, views"
Senior Cloud Data Engineer,BDO USA,"Stamford, CT",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765469443,2023-12-17,Montrose,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, AI, Application Development, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics solutions, SQL, DDL, DML, Views, Functions, Stored procedures, Performance Tuning, Azure, AWS, C#, Python, Java, Scala, Tabular modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps deployment technologies, Linux, Data Lake Medallion Architecture, Batch data ingestion, Streaming data ingestion, Data lake, AI Algorithms, Machine Learning, Automation tools, UiPath, Alteryx, Computer Vision based AI technologies, Professionalism, Autonomy, Verbal Communication, Written Communication, Organizational Skills, Project Deadlines, Multitasking, Deadlinedriven Environment, Team Environment, Professional Development, Relationship Building, Synapse, IoT, Tableau, .Net, Qlik, RedShift, RPA, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Athena, Data Pipeline, Glue, SSIS, SSAS, SSRS, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, ai, application development, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics solutions, sql, ddl, dml, views, functions, stored procedures, performance tuning, azure, aws, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops deployment technologies, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, data lake, ai algorithms, machine learning, automation tools, uipath, alteryx, computer vision based ai technologies, professionalism, autonomy, verbal communication, written communication, organizational skills, project deadlines, multitasking, deadlinedriven environment, team environment, professional development, relationship building, synapse, iot, tableau, net, qlik, redshift, rpa, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, athena, data pipeline, glue, ssis, ssas, ssrs, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql","ai, ai algorithms, alteryx, application development, athena, automation tools, autonomy, aws, aws lake formation, azure, azure analysis services, batch data ingestion, bicep, business intelligence, c, cloud data analytics solutions, computer vision based ai technologies, data lake, data lake medallion architecture, data ops, data pipeline, dataanalytics, databricks, datamodeling, datawarehouse, dbt, ddl, deadlinedriven environment, delta, devops deployment technologies, dml, functions, git, glue, iot, java, kinesis, linux, machine learning, microsoft fabric, multitasking, net, organizational skills, pandas, performance tuning, powerbi, professional development, professionalism, project deadlines, purview, python, qlik, quicksight, redshift, relationship building, rpa, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, stored procedures, streaming data ingestion, synapse, tableau, tabular modeling, team environment, terraform, uipath, verbal communication, views, written communication"
Senior Software Engineer - Data Strategy (NYC-Hybrid),Rad Hires,"West Nyack, NY",https://www.linkedin.com/jobs/view/senior-software-engineer-data-strategy-nyc-hybrid-at-rad-hires-3747283789,2023-12-17,Montrose,United States,Mid senior,Hybrid,"Role
The Senior Software Engineer position in the Data Strategy team presents a chance to create and execute data products for the world's biggest and most reputable reinsurance brokerage. Data Strategy has a “start-up style” mandate (within a $2 billion company) to enhance the acquisition, storage, analysis, fidelity, and monetization of client, internal, and third-party data across the organization. This innovation spans our petabyte-scale insured assets, including property, business, marine, and aviation entities, and their associated risks, such as hurricanes, wildfires, cyber-attacks, and wars, in a financial and economic context.
As a member of the Data Strategy group, the Senior Software Engineer will work with fellow data and web engineers, data scientists, product managers, business analysts, and stakeholders from other internal groups to design and improve data-centric projects with the dual mandate of (1) increasing the efficiency of the data collection and analysis process across the company and (2) driving the monetization of data via newly designed and existing products for the company’s reinsurance clients. The Senior Software Engineer will be the head facilitator on multiple innovative initiatives and will have ownership over the design, development, and delivery of projects requiring direct reporting to senior-level management in both business and technical groups.
Leadership Responsibilities
Work with a product manager as technical lead of a team of ~5 engineers, data scientists, and analysts to design, scope, and oversee work in an Agile environment.
Manage junior data and web engineers, focusing on productivity, quality, and professional development.
Partner with the head of Data Strategy and other senior engineers to create and evangelize best-in-class engineering competency and tooling within the organization.
Enforce strong development standards across the team through code reviews, automated testing, and monitoring.
Establish strong relationships with internal clients as an engineering representative for data strategy.
Contribute to the overall Data Strategy vision and execution via quarterly planning and executive committee reporting.
Partner regularly improving engineering recruiting process for the required skillsets and resourcing demands.
Learn the complex business of reinsurance to coach data technologists and execute the team's initiatives more effectively.
Software Engineer Responsibilities
Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting, and new data products.
Innovate new ways to leverage large and small datasets to drive revenue via the development of new products with the Data Strategy team, as well as the enhancement of existing products.
Architect engineering solutions using the latest cloud technologies in a process that spans hypothesis-validating prototypes to large-scale production data products, ensuring internal security and regulatory compliance.
Design solutions that account for unstructured data and document management system(s), including ingesting, tracking, parsing, analyzing, and summarizing documents at scale.
Perform exploratory and goal-oriented data analyses to understand and validate the requirements of data products and help create product roadmaps.
Develop, implement, and deploy front-ends and APIs, which may involve business intelligence dashboards, data pipelines, machine learning algorithms, and file ingestion mechanisms.
Work closely with data scientists, data engineers, web engineers, PMs, and other stakeholders to design & develop products.
Keep current on the latest trends and innovations in data technology and how these trends apply to the company's business and data strategy.
Required Qualifications
5-8+ years of relevant experience in data-focused software engineering
Master’s Degree or Ph.D. in data science, computer science, or related quantitative field such as applied mathematics, statistics, engineering or operations research, or equivalent experience
Experience in Python and familiarity with OOP and functional programming principles
Strong knowledge of SQL and familiarity with the high-level properties of modern data stores.
Strong understanding of the contemporary SDLC, including dev/QC/prod environments, unit/integration/UA testing, CI/CD, etc.
Experience building and maintaining CI/CD pipelines with tools such as Azure DevOps, GitLab, Travis, Jenkins, etc.
At least two and ideally all of the following sets of experience:
Data Engineering
2+ years’ experience with data engineering
Extensive experience with (py)Spark, Python, JSON, and SQL
Experience integrating data from semi-structured and unstructured sources
Knowledge of various industry-leading SQL and NoSQL database systems
Backend Web
2+ years of backend/full-stack web engineering
Experience working with Python-based server-side web frameworks like FastAPI or Django
Experience with complex backends involving multiple data stores, asynchronous worker queues, pub-sub messaging, and the like
Knowledge of cloud-based web deployments (AWS/Azure/GCP, Kubernetes, auto-scaling, etc.)
Experience with one or more major frontend frameworks (React strongly preferred)
Data Science/Analytics
2+ years of data analysis, AI, or data science work
Experience with data cleaning, enrichment, and reporting to business users
Experience selecting, training, validating, and deploying machine-learning models
Experience with or strong interest in learning about LLMs in a productized context
Experience working in an Agile environment to facilitate the quick and effective fulfillment of group goals
Good interpersonal and communication skills for establishing and maintaining sound internal relationships, working well as part of a team, and for presentations and discussions
Strong analytical skills and intellectual curiosity (interest in the meaning and usefulness of the data), as demonstrated through academic experience or work assignments
Excellent English verbal and writing skills for complex communications with company colleagues in all departments and levels of the organization, including communicating technical concepts to a non-technical audience
Good ability to prioritize workload according to volume, urgency, etc., and to deliver on required projects in a timely fashion
Preferred Qualifications
Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks
Experience with web scraping and crowdsourcing technologies
Experience with Databricks and optimizing Spark clusters
Experience architecting web ecosystems from the ground up, including monolith vs. microservice decisions, caching technologies, security integrations, etc.
Experience working with data visualization dashboarding tools (PowerBI, Tableau)
Insurance domain knowledge or strong interest in developing it
Experience with the MS Azure cloud environment
Show more
Show less","Data Engineering, Python, OOP, Functional Programming, SQL, Data Stores, Software Development Life Cycle, Continuous Integration/Continuous Deployment, Azure DevOps, GitLab, Travis, Jenkins, Spark, JSON, NoSQL, Backend Web Development, Python, FastAPI, Django, Asynchronous Worker Queues, PubSub Messaging, Kubernetes, AutoScaling, React, Data Science/Analytics, Data Cleaning, Data Enrichment, Machine Learning, Agile, Interpersonal Skills, Communication Skills, Analytical Skills, Intellectual Curiosity, English, Prioritization, Timeliness, Entity Resolution, Streaming Technologies, ELT/ETL, Web Scraping, Crowdsourcing, Databricks, Microservices, Caching, Security, Data Visualization, PowerBI, Tableau, Insurance, Azure","data engineering, python, oop, functional programming, sql, data stores, software development life cycle, continuous integrationcontinuous deployment, azure devops, gitlab, travis, jenkins, spark, json, nosql, backend web development, python, fastapi, django, asynchronous worker queues, pubsub messaging, kubernetes, autoscaling, react, data scienceanalytics, data cleaning, data enrichment, machine learning, agile, interpersonal skills, communication skills, analytical skills, intellectual curiosity, english, prioritization, timeliness, entity resolution, streaming technologies, eltetl, web scraping, crowdsourcing, databricks, microservices, caching, security, data visualization, powerbi, tableau, insurance, azure","agile, analytical skills, asynchronous worker queues, autoscaling, azure, azure devops, backend web development, caching, communication skills, continuous integrationcontinuous deployment, crowdsourcing, data cleaning, data engineering, data enrichment, data scienceanalytics, data stores, databricks, django, eltetl, english, entity resolution, fastapi, functional programming, gitlab, insurance, intellectual curiosity, interpersonal skills, jenkins, json, kubernetes, machine learning, microservices, nosql, oop, powerbi, prioritization, pubsub messaging, python, react, security, software development life cycle, spark, sql, streaming technologies, tableau, timeliness, travis, visualization, web scraping"
Senior Software Engineer - Data Strategy (NYC-Hybrid),Rad Hires,"Stamford, CT",https://www.linkedin.com/jobs/view/senior-software-engineer-data-strategy-nyc-hybrid-at-rad-hires-3747282937,2023-12-17,Montrose,United States,Mid senior,Hybrid,"Role
The Senior Software Engineer position in the Data Strategy team presents a chance to create and execute data products for the world's biggest and most reputable reinsurance brokerage. Data Strategy has a “start-up style” mandate (within a $2 billion company) to enhance the acquisition, storage, analysis, fidelity, and monetization of client, internal, and third-party data across the organization. This innovation spans our petabyte-scale insured assets, including property, business, marine, and aviation entities, and their associated risks, such as hurricanes, wildfires, cyber-attacks, and wars, in a financial and economic context.
As a member of the Data Strategy group, the Senior Software Engineer will work with fellow data and web engineers, data scientists, product managers, business analysts, and stakeholders from other internal groups to design and improve data-centric projects with the dual mandate of (1) increasing the efficiency of the data collection and analysis process across the company and (2) driving the monetization of data via newly designed and existing products for the company’s reinsurance clients. The Senior Software Engineer will be the head facilitator on multiple innovative initiatives and will have ownership over the design, development, and delivery of projects requiring direct reporting to senior-level management in both business and technical groups.
Leadership Responsibilities
Work with a product manager as technical lead of a team of ~5 engineers, data scientists, and analysts to design, scope, and oversee work in an Agile environment.
Manage junior data and web engineers, focusing on productivity, quality, and professional development.
Partner with the head of Data Strategy and other senior engineers to create and evangelize best-in-class engineering competency and tooling within the organization.
Enforce strong development standards across the team through code reviews, automated testing, and monitoring.
Establish strong relationships with internal clients as an engineering representative for data strategy.
Contribute to the overall Data Strategy vision and execution via quarterly planning and executive committee reporting.
Partner regularly improving engineering recruiting process for the required skillsets and resourcing demands.
Learn the complex business of reinsurance to coach data technologists and execute the team's initiatives more effectively.
Software Engineer Responsibilities
Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting, and new data products.
Innovate new ways to leverage large and small datasets to drive revenue via the development of new products with the Data Strategy team, as well as the enhancement of existing products.
Architect engineering solutions using the latest cloud technologies in a process that spans hypothesis-validating prototypes to large-scale production data products, ensuring internal security and regulatory compliance.
Design solutions that account for unstructured data and document management system(s), including ingesting, tracking, parsing, analyzing, and summarizing documents at scale.
Perform exploratory and goal-oriented data analyses to understand and validate the requirements of data products and help create product roadmaps.
Develop, implement, and deploy front-ends and APIs, which may involve business intelligence dashboards, data pipelines, machine learning algorithms, and file ingestion mechanisms.
Work closely with data scientists, data engineers, web engineers, PMs, and other stakeholders to design & develop products.
Keep current on the latest trends and innovations in data technology and how these trends apply to the company's business and data strategy.
Required Qualifications
5-8+ years of relevant experience in data-focused software engineering
Master’s Degree or Ph.D. in data science, computer science, or related quantitative field such as applied mathematics, statistics, engineering or operations research, or equivalent experience
Experience in Python and familiarity with OOP and functional programming principles
Strong knowledge of SQL and familiarity with the high-level properties of modern data stores.
Strong understanding of the contemporary SDLC, including dev/QC/prod environments, unit/integration/UA testing, CI/CD, etc.
Experience building and maintaining CI/CD pipelines with tools such as Azure DevOps, GitLab, Travis, Jenkins, etc.
At least two and ideally all of the following sets of experience:
Data Engineering
2+ years’ experience with data engineering
Extensive experience with (py)Spark, Python, JSON, and SQL
Experience integrating data from semi-structured and unstructured sources
Knowledge of various industry-leading SQL and NoSQL database systems
Backend Web
2+ years of backend/full-stack web engineering
Experience working with Python-based server-side web frameworks like FastAPI or Django
Experience with complex backends involving multiple data stores, asynchronous worker queues, pub-sub messaging, and the like
Knowledge of cloud-based web deployments (AWS/Azure/GCP, Kubernetes, auto-scaling, etc.)
Experience with one or more major frontend frameworks (React strongly preferred)
Data Science/Analytics
2+ years of data analysis, AI, or data science work
Experience with data cleaning, enrichment, and reporting to business users
Experience selecting, training, validating, and deploying machine-learning models
Experience with or strong interest in learning about LLMs in a productized context
Experience working in an Agile environment to facilitate the quick and effective fulfillment of group goals
Good interpersonal and communication skills for establishing and maintaining sound internal relationships, working well as part of a team, and for presentations and discussions
Strong analytical skills and intellectual curiosity (interest in the meaning and usefulness of the data), as demonstrated through academic experience or work assignments
Excellent English verbal and writing skills for complex communications with company colleagues in all departments and levels of the organization, including communicating technical concepts to a non-technical audience
Good ability to prioritize workload according to volume, urgency, etc., and to deliver on required projects in a timely fashion
Preferred Qualifications
Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks
Experience with web scraping and crowdsourcing technologies
Experience with Databricks and optimizing Spark clusters
Experience architecting web ecosystems from the ground up, including monolith vs. microservice decisions, caching technologies, security integrations, etc.
Experience working with data visualization dashboarding tools (PowerBI, Tableau)
Insurance domain knowledge or strong interest in developing it
Experience with the MS Azure cloud environment
Show more
Show less","Agile, OOP, SQL, Python, Spark, Django, FastAPI, React, AWS, Azure, GCP, Kubernetes, Data Engineering, Backend Web, Data Science/Analytics, Machine Learning, AI, Data Analysis, Entity Resolution, Streaming Technologies, ELT/ETL Frameworks, Web Scraping, Crowdsourcing, Databricks, Data Visualization, PowerBI, Tableau, Insurance","agile, oop, sql, python, spark, django, fastapi, react, aws, azure, gcp, kubernetes, data engineering, backend web, data scienceanalytics, machine learning, ai, data analysis, entity resolution, streaming technologies, eltetl frameworks, web scraping, crowdsourcing, databricks, data visualization, powerbi, tableau, insurance","agile, ai, aws, azure, backend web, crowdsourcing, data engineering, data scienceanalytics, dataanalytics, databricks, django, eltetl frameworks, entity resolution, fastapi, gcp, insurance, kubernetes, machine learning, oop, powerbi, python, react, spark, sql, streaming technologies, tableau, visualization, web scraping"
Sr Data Analyst (Data Warehouse),Damco Solutions,"Purchase, NY",https://www.linkedin.com/jobs/view/sr-data-analyst-data-warehouse-at-damco-solutions-3768039489,2023-12-17,Montrose,United States,Mid senior,Hybrid,"Title
: Sr Data Analyst (Data Warehouse)
Location
: Purchase, NY or Florham Park, NJ; (Day 1 onsite with Hybrid model)
Required Skills
Guidewire Policy center
Datawarehouse
Informatica
TECHNICAL QUALIFICATIONS
10 + years of data analysis experience extracting OLTP data specifically for a data warehouse
5 + years P&C Insurance subject matter experience (Guidewire Policy center)
Advanced SQL knowledge working with extracting data from OLTP applications
Extensive knowledge of Data Warehousing, ETL and BI Architectures, concepts and frameworks
Capable of creating and tuning Semantic layer Reporting Views
Capable of facilitating data discovery sessions involving business subject matter experts
Knowledge of RDBMS platforms (e.g. SQL Server, DB2) with experience in generating DDL
Experience in Guidewire policy Center
Show more
Show less","Guidewire Policy Center, Data Warehouse, Informatica, SQL, ETL, BI Architectures, Semantic Layer Reporting Views, RDBMS, DDL","guidewire policy center, data warehouse, informatica, sql, etl, bi architectures, semantic layer reporting views, rdbms, ddl","bi architectures, datawarehouse, ddl, etl, guidewire policy center, informatica, rdbms, semantic layer reporting views, sql"
Treasury Data Analyst,Coda Search│Staffing,"Stamford, CT",https://www.linkedin.com/jobs/view/treasury-data-analyst-at-coda-search%E2%94%82staffing-3784575637,2023-12-17,Montrose,United States,Mid senior,Hybrid,"The Treasury Data Analyst plays a vital role within the treasury team, reporting directly to the Treasurer. Key responsibilities include daily cash flow reporting, forecasting, analysis of credit facilities, and the automation of data and reports from various systems. Engages in treasury-related projects for Castleton Commodities International entities.
Responsibilities:
Generate daily cash flow reports and consolidate forecasts for multiple entities across various currencies. Collaborate with teams such as trade settlements, cash operations, credit, futures clearing, and business development for accurate forecasts.
Design and implement custom solutions to enhance reporting and data analysis for Treasury management, business performance reviews, and financial planning.
Manage data sets supporting financial reporting, including foreign exchange exposure, working capital, counterparty exposures, and cash positions.
Prepare monthly borrowing base reports, enhancing processes for timeliness and reconciliation. Support audits related to monthly treasury reports.
Provide timely analysis for daily, monthly, and quarterly reporting packages.
Collaborate with corporate accounting to ensure accurate recording of treasury-related activities.
Analyze FX cash positions and forecasts to meet cash needs while minimizing exposure to currency market fluctuations.
Support bank account management and provide detailed cash position reports.
Qualifications:
Bachelor’s Degree in Accounting, Finance, Economics, Management Information Systems, or related field.
1-5 years in a Data Analyst, Data Management, or Treasury-related role preferred.
Proficient in database management, SQL, VBA coding; familiarity with Alteryx and Power Automate tools preferred.
Strong MS Excel, Word, and PowerPoint skills.
Excellent analytical and communication skills, both verbal and written.
Highly organized, able to work independently with an energetic and determined attitude.
Detail-oriented with strong problem-solving skills.
Ability to manage multiple activities concurrently.
General knowledge of corporate finance and accounting.
Show more
Show less","Data Analysis, Financial Reporting, Treasury Management, Data Management, SQL, VBA, Alteryx, Power Automate, MS Excel, MS Word, MS PowerPoint, Database Management, Financial Planning, Foreign Exchange, Counterparty Exposure, Cash Position, Forecasting, Cash Flow Reporting, Budgeting, Corporate Finance, Accounting","data analysis, financial reporting, treasury management, data management, sql, vba, alteryx, power automate, ms excel, ms word, ms powerpoint, database management, financial planning, foreign exchange, counterparty exposure, cash position, forecasting, cash flow reporting, budgeting, corporate finance, accounting","accounting, alteryx, budgeting, cash flow reporting, cash position, corporate finance, counterparty exposure, data management, dataanalytics, database management, financial planning, financial reporting, forecasting, foreign exchange, ms excel, ms powerpoint, ms word, power automate, sql, treasury management, vba"
Data Scientist,Links Technology Solutions,"Tarrytown, NY",https://www.linkedin.com/jobs/view/data-scientist-at-links-technology-solutions-3779304002,2023-12-17,Montrose,United States,Mid senior,Hybrid,"Links Technology Solutions is seeking a passionate and experienced Data Scientist to join our client's team in Tarrytown, NY, and contribute to cutting-edge research and development in the pharmaceutical industry. This hybrid role offers the flexibility of 3 days onsite per week, allowing you to make a real impact while maintaining a healthy work-life balance.
Responsibilities of the Data Scientist:
Develop, evaluate, and optimize AI/ML models
to solve critical pharma challenges.
Improve data-driven processes
through machine learning and feature engineering.
Mine insights from diverse data sources
(internal & external) to enhance analytical capabilities.
Refine data collection procedures
for building robust predictive systems.
Ensure data integrity and quality
through advanced cleaning and validation techniques.
Qualifications of the Data Scientist:
Bachelor's degree (or equivalent)
in Statistics, Applied Mathematics, or related field.
7+ years of experience
in data science, including
pharma/biotech experience a MUST
.
Strong proficiency in AI/ML
(e.g., supervised & unsupervised learning) and statistical analysis.
Advanced skills in data mining, pattern recognition, and predictive modeling
.
Command of Excel, PowerPoint, Tableau, SQL, and programming languages
(e.g., Python, R, SAS).
Benefits of the Data Scientist:
Competitive hourly rate ($70-$80/hr)
with long-term contract potential.
Comprehensive benefits package
including medical/dental/vision and 401k.
2 weeks paid vacation
to recharge and explore the vibrant Hudson Valley.
Hybrid work model
offering flexibility and a supportive team environment.
Show more
Show less","Data Science, AI/ML, Machine Learning, Feature Engineering, Data Mining, Pattern Recognition, Predictive Modeling, Statistical Analysis, Data Cleaning, Data Validation, Data Integrity, Statistics, Applied Mathematics, Excel, PowerPoint, Tableau, SQL, Python, R, SAS","data science, aiml, machine learning, feature engineering, data mining, pattern recognition, predictive modeling, statistical analysis, data cleaning, data validation, data integrity, statistics, applied mathematics, excel, powerpoint, tableau, sql, python, r, sas","aiml, applied mathematics, data cleaning, data integrity, data mining, data science, data validation, excel, feature engineering, machine learning, pattern recognition, powerpoint, predictive modeling, python, r, sas, sql, statistical analysis, statistics, tableau"
"Associate, Treasury Data Analyst",Castleton Commodities International,"Stamford, CT",https://www.linkedin.com/jobs/view/associate-treasury-data-analyst-at-castleton-commodities-international-3770792350,2023-12-17,Montrose,United States,Mid senior,Hybrid,"The Treasury Data Analyst will report to the Treasurer and is an integral part of the treasury team. The primary responsibilities of this position include daily cash flow reporting and forecasting, reporting and analysis associated with the company’s primary credit facilities, automation of data and reports sourced from multiple trade and accounting systems, and engagement in treasury-related projects for Castleton Commodities International entities
Responsibilities:
Prepare daily cash flow reports and aggregate cash flow forecasts for multiple corporate entities in multiple currencies. Coordinate the collection of information from various functions and company data sets to ensure the cash forecast is highly accurate. Requires engagement with trade settlements, cash operations, credit, futures clearing and business development teams.
Develop and implement custom solutions to enhance and improve reporting and data analysis for Treasury management, business performance reviews, annual budgeting and associated financial planning.
Coordinate and maintain data sets used to support financial reporting requirements, which can include foreign exchange exposure, working capital usage, counterparty exposures, and changes in cash and liquidity positions.
Prepare monthly borrowing base reports required under the company’s lines of credit, including detailed schedules and analysis of inventory, accounts receivable, mark-to-market gains, and affiliate transactions. Enhance the reporting process to improve timeliness and reconciliation of associated financial data. Support additional diligence and audit requests in connection with monthly treasury reports.
Provide timely and accurate analysis and reporting for variety of daily, monthly, and quarterly reporting packages.
Work closely with corporate accounting to ensure that treasury-related activities are recorded in an accurate and timely manner.
Analyze FX cash positions and forecasting to ensure FX cash needs are met while minimizing exposure due to currency market fluctuations.
Support bank account management and detailed reporting of cash positions.
Qualifications:
Bachelor’s Degree in Accounting, Finance, Economics, Management Information Systems, or a related field of study.
1-5 years in a Data Analyst, Data Management, or Treasury related role preferred.
Demonstrated capability in database management, SQL and VBA coding; familiarity with Alteryx and Power Automate tools preferred.
Proficiency in MS Excel, Word, and PowerPoint required.
Strong analytical and communication skills, including verbal and written skills. Highly organized and ability to work independently.
Energetic and determined attitude.
Detail-oriented.
Problem-solving skills.
Ability to simultaneously manage multiple activities to completion.
General knowledge of corporate finance and accounting.
Employee Programs & Benefits:
CCI offers competitive benefits and programs to support our employees, their families and local communities. These include:
Competitive comprehensive medical, dental, retirement and life insurance benefits
Employee assistance & wellness programs
Parental and family leave policies
CCI in the Community: Each office has a Charity Committee and as a part of this program employees are allocated 2 days annually to volunteer at the selected charities.
Charitable contribution match program
Tuition assistance & reimbursement
Quarterly Innovation & Collaboration Awards
Employee discount program, including access to fitness facilities
Competitive paid time off
Continued learning opportunities
Visit https://www.cci.com/careers/life-at-cci/# to learn more!
Show more
Show less","SQL (Structured Query Language), VBA (Visual Basic for Applications), Alteryx (Data Analytics Software), Power Automate (Data Integration and Automation Software), MS Excel, MS Word, MS PowerPoint, Tableau, Data Management, Database Management, Data Analysis, Financial Analysis, Cash Flow Forecasting, Corporate Finance, Accounting, Cash Management, Risk Management, Treasury Management, Business Performance Analysis, Foreign Exchange, Counterparty Exposure, Audit","sql structured query language, vba visual basic for applications, alteryx data analytics software, power automate data integration and automation software, ms excel, ms word, ms powerpoint, tableau, data management, database management, data analysis, financial analysis, cash flow forecasting, corporate finance, accounting, cash management, risk management, treasury management, business performance analysis, foreign exchange, counterparty exposure, audit","accounting, alteryx data analytics software, audit, business performance analysis, cash flow forecasting, cash management, corporate finance, counterparty exposure, data management, dataanalytics, database management, financial analysis, foreign exchange, ms excel, ms powerpoint, ms word, power automate data integration and automation software, risk management, sql structured query language, tableau, treasury management, vba visual basic for applications"
"Looking Forward To Hearing From You Onsite Role San Antonio, TX Fraud- Data Analyst",Accuro,"San Antonio, TX",https://www.linkedin.com/jobs/view/looking-forward-to-hearing-from-you-onsite-role-san-antonio-tx-fraud-data-analyst-at-accuro-3742070482,2023-12-17,San Antonio,United States,Associate,Onsite,"Greetings from Accuro Group,
My name is Bharat Gautam, and I am working with Accuro Group.
If I am unable to answer your call, kindly Send me a message. On LinkedIn: linkedin.com/in/bharat-gautam-645546252
Job Title: Fraud- Data Analyst
Work Location: San Antonio, TX (Onsite)
implementation partner: TCS
Check processing is a must with debit card fraud. Fraud analysts investigate theft and fraud within transactions on behalf of a company or a financial institution.
Fraud analysts examine data and fraudulent activity to brainstorm new techniques to help prevent future fraudulent activity from happening.
A fraud analyst is someone who investigates fraudulent activity related to financial transactions and accounts on behalf of a bank or a financial institution.
They monitor and analyze the data to detect and resolve fraud cases such as identity theft, forgery, or unauthorized usage of cards.
They also strive to minimize potential risks to the institution and its clients. Their responsibilities may differ depending on the type of field.
Behind every resume is a human being with hopes, dreams and a family
Early response is appreciated....
Regards
Bharat Gautam | Sr.Technical Recruiter
Desk:+1 919-364-1229 *164
EMAIL Bharat.g@accurogroup.com
linkedin.com/in/bharat-gautam-645546252
Show more
Show less","Data Analysis, Fraud Detection, Financial Transactions, Risk Management, Identity Theft, Forgery, Unauthorized Card Usage, Debit Card Fraud","data analysis, fraud detection, financial transactions, risk management, identity theft, forgery, unauthorized card usage, debit card fraud","dataanalytics, debit card fraud, financial transactions, forgery, fraud detection, identity theft, risk management, unauthorized card usage"
Data Center Facilities Operator,JLL,"San Antonio, TX",https://www.linkedin.com/jobs/view/data-center-facilities-operator-at-jll-3775808568,2023-12-17,San Antonio,United States,Associate,Onsite,"The Data Center Operations Engineer is responsible for delivery of best practice systems and problem resolution on all data center electrical and mechanical infrastructure (UPS, MV electrical systems, generators, cooling systems etc.)
Location: San Antonio TX
Principal Duties and Responsibilities
Task will include but not be limited to:
Responsible for maintaining, monitoring, and performing preventive maintenance and continuous operation of all building systems to maintain 100% Up-time including: fire/life safety, mechanical systems such as (HVAC, chillers, crac, crah, plumbing, controls), electrical including emergency backup systems such as (lighting, UPS, ATS, STS, PDU, generators, primary switchgear, power distribution, transformers), and hot water systems. Monitors operation, adjusts, and maintains refrigeration, chilled water, and air conditioning equipment; boilers, and ventilating and water heaters; pumps, valves, piping, and filters; other mechanical and electrical equipment. Must record readings and make and adjust where necessary to ensure proper operation of equipment.
Requires the ability to analyze the operation of various systems, determine the cause of any problems/malfunctions and take corrective action as required.
Comply with departmental policy for the safe storage, usage, and disposal of hazardous materials. Maintains a clean and safe workplace.
Learn and understand the data center site in-order to manage incidents and events that put the critical systems at risk.
Work order management, including CMMS, Vendor Management, and Customer Facing Tickets.
Understanding and complying with emergency escalation procedures.
Perform additional job duties as required.
Minimum Requirements:
Preferred to have hands-on experience working in a data center/critical facility, including UPS.
Systems, emergency generators, and switchgears.
High School diploma or GED equivalent
2+ years related work experience.
Working knowledge of computer applications including Word and Excel.
Demonstrated verbal/written communication skills.
Preferred Requirements:
Corrigo Experience.
MCIM / Salesforce Experience.
Zendesk Experience.
Service Now Experience.
Received EPA 608.
Trained in NFPA70E.
PHYSICAL WORK ABILITIES AND REQUIREMENTS
:
This position requires frequent walking, climbing, bending, kneeling, lifting, stooping, and working/extending overhead, including:
Walking large, campus-like settings.
Lifting a minimum of 50 lbs.
Climbing stairs and navigating rooftops to access equipment.
Using ladders up to 30 ft and working from heights.
Ability to Climb a ladder with a 300-lb weight limit.
Must be able to work different schedules.
Must be able to work Holidays.
Must be able to respond to site emergencies.
Personalized benefits that support personal well-being and growth:
JLL recognizes the impact that the workplace can have on your wellness, so we offer a supportive culture and comprehensive benefits package that prioritizes mental, physical and emotional health. Some of these benefits, include:
401(k) plan with matching company contributions
Comprehensive Medical, Dental & Vision Care
FMLA at 100% of salary after 1 year of employment.
Paid Time Off and Company Holidays.
Compensated for Holidays Worked.
15% Pay differential for Night Shift Employment.
Looking for a job that values YOU beyond the checkboxes? Don't let requirements hold you back. At our company, we're all about embracing new perspectives and unique talents. So, if you're someone who's ready to bring your A-game and show us what you're made of, we want to meet you! Apply now and let your skills and passion shine through. Be bold, be yourself, and let's create something extraordinary together!
Potential pay is $32-$36 based on experience
Show more
Show less","Data Center Operations, Electrical Systems, Mechanical Systems, UPS, Generators, HVAC, Chillers, Crac, Crah, Plumbing, Controls, ATS, STS, PDU, Primary Switchgear, Power Distribution, Transformers, Hot Water Systems, Refrigeration, Air Conditioning Equipment, Boilers, Ventilating, Water Heaters, Pumps, Valves, Piping, Filters, CMMS, Vendor Management, Customer Facing Tickets, Corrigo, MCIM, Salesforce, Zendesk, Service Now, EPA 608, NFPA70E, Microsoft Word, Microsoft Excel","data center operations, electrical systems, mechanical systems, ups, generators, hvac, chillers, crac, crah, plumbing, controls, ats, sts, pdu, primary switchgear, power distribution, transformers, hot water systems, refrigeration, air conditioning equipment, boilers, ventilating, water heaters, pumps, valves, piping, filters, cmms, vendor management, customer facing tickets, corrigo, mcim, salesforce, zendesk, service now, epa 608, nfpa70e, microsoft word, microsoft excel","air conditioning equipment, ats, boilers, chillers, cmms, controls, corrigo, crac, crah, customer facing tickets, data center operations, electrical systems, epa 608, filters, generators, hot water systems, hvac, mcim, mechanical systems, microsoft excel, microsoft word, nfpa70e, pdu, piping, plumbing, power distribution, primary switchgear, pumps, refrigeration, salesforce, service now, sts, transformers, ups, valves, vendor management, ventilating, water heaters, zendesk"
Senior Staff Data Engineer,"SADA, An Insight company","San Antonio, TX",https://www.linkedin.com/jobs/view/senior-staff-data-engineer-at-sada-an-insight-company-3743771158,2023-12-17,San Antonio,United States,Mid senior,Onsite,"Join SADA as a Senior Staff Data Engineer, Corporate!
Your Mission
As a Senior Staff Data Engineer, Corporate at SADA, you will have the opportunity to work with big data and emerging Google Cloud technologies to drive corporate services. You will have an opportunity to design, develop, and maintain the best Enterprise Data Warehouse solution to fit our corporate needs. You will be interacting with all of our business units and Google Cloud subject matter experts.
From transforming business requirements, solution architecture, data modeling, architecting, ETL, metadata, and business continuity, you will have the opportunity to work collaboratively with architects and other engineers to recommend, prototype, build, and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and covering a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes, and data warehouses.
You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as guide client-facing technical discussions for established projects.
Pathway to Success
#BeOneStepAhead: At SADA, we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.
Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers and the consultative polish you bring to customer interactions.
As you continue to execute successfully, we will build a customized development plan together that takes you through the engineering or management growth tracks.
Expectations
Internal Facing - You will interact with internal customers and stakeholders regularly, sometimes daily, other times weekly/bi-weekly. Expectations will be to capture requirements and deliver solutions suitable for corporate divisions.
Onboarding/Training - The first several weeks of onboarding are dedicated to learning and will include learning materials/assignments and compliance training, and meetings with relevant individuals. Details of the timeline are shared closer to the start date.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Mastery in the following domain area:
Data warehouse modernization: building complete data warehouse solutions on BigQuery, including technical architectures, star/snowflake schema designs, query optimization, ETL/ELT pipelines, and reporting/analytic tools. Must have expert-level experience working with Google's batch or streaming data processing solutions (such as BigQuery, Dataform, and BI Engine)
Proficiency in the following domain areas:
Big Data: managing Hadoop clusters (all included services), troubleshooting cluster operation issues, migrating Hadoop workloads, architecting solutions on Hadoop, experience with NoSQL data stores like Cassandra and HBase, building batch/streaming ETL pipelines with frameworks such as Spark, Spark Streaming, and Apache Beam, and working with messaging systems like Pub/Sub, Kafka and RabbitMQ.
Data Catalog: Managing Data Catalogs, definitions, and data lineage.
Data Quality: Must have experience with DataForm, or other DQ solutions.
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for minimizing downtime. It may involve conversion between relational and NoSQL data stores, or vice versa
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale
4+ years of experience with Data modeling, SQL, ETL, Data Warehousing, and Data Lakes
4+ years experience in writing production-grade data solutions (relational and NoSQL)in an enterprise-class RDBMS
2+ years of experience with enterprise-class Business Intelligence tools such as Looker, PowerBI, Tableau, etc.
Mastery in writing software in Python
Experience writing software in one or more languages, such as Javascript, Java, R, or Go
Experience with systems monitoring/alerting, capacity planning, and performance tuning
Hands-on experience building frontend applications with React
Hands-on experience with CI/CD solutions (Cloud Build / Terraform)
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc.)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Ability to balance and prioritize multiple conflicting requirements with great attention to detail
Excellent verbal/written communication & data presentation skills, including the ability to succinctly summarize key findings and effectively communicate with both business and technical teams
About SADA An Insight Company
Values:
SADA stands for inclusion, fairness, and doing the right thing. From our very beginning, we've championed a diverse workplace where we support and learn from each other, amplifying the impact we make with our customers. We're proud that our teams are composed of contributors who represent a wide array of backgrounds, experiences, abilities, and perspectives. We are an equal opportunity employer. Our five core values are the foundation of everything we do:
Make Them Rave
Be Data Driven
Think One Step Ahead
Drive Purposeful Impact
Do The Right Thing
Work with the Best
: SADA has been the largest Google Cloud partner in North America since 2016 and, for the sixth year in a row, has been named a Google Global Partner of the Year . This year, SADA was named a Google Cloud Global Partner of the year 2023. SADA has also been awarded Best Place to Work year after year by the Business Intelligence Group and Inc. Magazine, and was recognized as a Niche Player in the 2023 Gartner® Magic Quadrant™ for Public Cloud IT Transformation Services.
Benefits
: Unlimited PTO, paid parental leave, competitive and attractive compensation, performance-based bonuses, paid holidays, generous medical, dental, vision plans, life, short and long-term disability insurance, 401K/RRSP with match, as well as Google-certified training programs and a professional development stipend.
Business Performance:
SADA has been named to the INC 5000 Fastest-Growing Private Companies list for the last 10+ years in a row, garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers list for the past 5 years. The overall culture continues to evolve with engineering at its core:
3200+ projects completed, 4000+ customers served, 10K+ workloads, and 30M+ users migrated to the cloud.
To request reasonable accommodation to participate in the job application or interview process, contact careers@sada.com. SADA complies with federal and state/provincial disability laws and makes reasonable accommodations for applicants and candidates with disabilities.
Show more
Show less","Google Professional Data Engineer Certified, Google Cloud Platform, Google's batch or streaming data processing solutions (BigQuery Dataform BI Engine), Hadoop clusters, Cassandra, HBase, Spark, Spark Streaming, Apache Beam, Pub/Sub, Kafka, RabbitMQ, Data Catalogs, DataForm, Data migration, Data modeling, SQL, ETL, Data Warehousing, Data Lakes, Looker, PowerBI, Tableau, Python, Javascript, Java, R, Go, React, Cloud Build / Terraform, CloudSQL, Spanner, Cloud Storage, Dataflow, Dataproc, Bigtable, Dataprep, Composer, IoT architectures, Machine learning models, Statistics, Data presentation, Verbal communication, Written communication","google professional data engineer certified, google cloud platform, googles batch or streaming data processing solutions bigquery dataform bi engine, hadoop clusters, cassandra, hbase, spark, spark streaming, apache beam, pubsub, kafka, rabbitmq, data catalogs, dataform, data migration, data modeling, sql, etl, data warehousing, data lakes, looker, powerbi, tableau, python, javascript, java, r, go, react, cloud build terraform, cloudsql, spanner, cloud storage, dataflow, dataproc, bigtable, dataprep, composer, iot architectures, machine learning models, statistics, data presentation, verbal communication, written communication","apache beam, bigtable, cassandra, cloud build terraform, cloud storage, cloudsql, composer, data catalogs, data lakes, data migration, data presentation, dataflow, dataform, datamodeling, dataprep, dataproc, datawarehouse, etl, go, google cloud platform, google professional data engineer certified, googles batch or streaming data processing solutions bigquery dataform bi engine, hadoop clusters, hbase, iot architectures, java, javascript, kafka, looker, machine learning models, powerbi, pubsub, python, r, rabbitmq, react, spanner, spark, spark streaming, sql, statistics, tableau, verbal communication, written communication"
Senior Data Engineer,Tata Consultancy Services,"San Antonio, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-tata-consultancy-services-3779622237,2023-12-17,San Antonio,United States,Mid senior,Onsite,"Job Title
Senior Data Engineer
Technical/Functional Skills
Primary – Cloud Based ETL Tool, DBT, Snowflake (Cloud DB), Strong SQL, Unix/Python, Control-M, Service Now, RPA
Experience Required
8-10 yrs
Roles & Responsibilities
Role Description
Analyze requirements and existing resources to Propose, create ETL designs and database objects
Work with project and business analyst leads in order to develop and clari fy in-depth technical requirements including logical and physical data modeling activities
Design and implement ETL processes for data transactions related to Enterprise Data Warehouse, Operational Data Store (ODS), and other data structures to support our Business Intelligence operations
Develops, enhances, debugs, supports, maintains and tests software applications that support business units or supporting functions using IBM Infosphere Data Stage ETL or any other cloud based ETL tool both ETL and ELT approaches. These application program solutions may involve diverse development platforms, software, hardware, technologies and tools.
Must have hands-on on Snowflake development environment with all SQL operations. Must be aware of ELT approach as well.
Participates in the design, development and implementation of complex applications, often using IBM Infosphere Information Server (IIS) products like Data Stage, Quality Stage on a Linux Grid environment. Control-M/Scheduling tools.
Required Skills:10+ Yrs Relevant IT software experience (Technical) in ETL Datastage or any other cloud based ETL Tool development Experience with databases like Snowflake (Cloud DB), Oracle, Netezza, MS SQL Server 2012+, DB2 and MS Access
Experience with job automation & scheduling software (Control-M) Strong ability to write SQL queries
Desired Skills:
Familiar with Snowflake (Cloud DB), DBT,
Python, UNIX, Windows, File transfer utilities, process flow creation, ETL technologies, Hadoop, ServiceNow, RPA
Good to have Skills: Snow-Pro Certified. Service Now Certified,.Strong SQL, Strong conceptual understanding of core DW Concepts including different approaches/methodologies. dbt (data build tool) experience is an added advantage
Show more
Show less","CloudBased ETL Tool, DBT, Snowflake, SQL, Unix, Python, ControlM, Service Now, RPA, ETL Datastage, Oracle, Netezza, MS SQL Server, DB2, MS Access, Job automation & scheduling software, Hadoop, File transfer utilities, Process flow creation, ETL technologies, DW Concepts, dbt (data build tool)","cloudbased etl tool, dbt, snowflake, sql, unix, python, controlm, service now, rpa, etl datastage, oracle, netezza, ms sql server, db2, ms access, job automation scheduling software, hadoop, file transfer utilities, process flow creation, etl technologies, dw concepts, dbt data build tool","cloudbased etl tool, controlm, db2, dbt, dbt data build tool, dw concepts, etl datastage, etl technologies, file transfer utilities, hadoop, job automation scheduling software, ms access, ms sql server, netezza, oracle, process flow creation, python, rpa, service now, snowflake, sql, unix"
Senior Data Engineer - Snowflake,Tata Consultancy Services,"San Antonio, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-snowflake-at-tata-consultancy-services-3752006423,2023-12-17,San Antonio,United States,Mid senior,Onsite,"Job Title - Sr. Data Engineer - Snowflake
Job Type - Full Time
Location - San Antonio, TX or Plano, Tx (Onsite Role)
Experience required - 5+ Years
Roles & Responsibility
Analyze, develop, refactor, fix, test, review and deploy functionality, and bug fixes in ETL that moves data between Snowflake data layers.
Database and Query tuning, diagnosis, and resolution of performance issues leveraging ELT and push-down if required.
Use and improve ETL frameworks, continuous data quality frameworks and other automation in data pipeline.
Service data availability SLOs and attend triage meetings to engage with Security, Infrastructure and Workload management teams to issue resolutions.
Compliance to Agile Jira SDLC controls, Service Now Change & Incident management, and Data Ops Gitlab CI/CD pipeline.
Participate in daily standups, lead design reviews and offshore coordination.
Show more
Show less","Snowflake, ETL, ELT, Agile Jira SDLC, Service Now, Gitlab CI/CD, Data Ops","snowflake, etl, elt, agile jira sdlc, service now, gitlab cicd, data ops","agile jira sdlc, data ops, elt, etl, gitlab cicd, service now, snowflake"
Lead Data Integration Engineer,Jefferson Frank,"San Antonio, TX",https://www.linkedin.com/jobs/view/lead-data-integration-engineer-at-jefferson-frank-3785117346,2023-12-17,San Antonio,United States,Mid senior,Onsite,"Role & Responsibilities
Architects, designs, and implements integration solutions on the MuleSoft platform based on functional and technical requirements.
Creates and maintains solution design documentation.
Provides guidance and leadership to junior team members.
Acts as a trusted advisor and expert on the MuleSoft platform, promoting security and performance.
Develops and evaluates productivity and teamwork to ensure Legendary Customer Service.
Models and promotes the use of Values Based Leadership tools to align with company values.
Defines systems integrations and design standards for scalable and flexible solutions.
Manages relationships with key clients and identifies new opportunities.
Preferred Skills & Qualifications
Hands-on experience with MuleSoft's CloudHub, DataWeave, Anypoint MQ, and deploying/managing Mule flows to CloudHub.
Strong experience in Application Integration Architecture, API and Microservices architecture, and Solution Design using SOA/EAI solutions.
Knowledge of integrating with Cloud/SaaS applications, APIs, SDK of packaged applications, and legacy systems, ideally including Salesforce, MS Dynamics, and Data warehouse integration.
Experience in setting up and configuring on-premise/cloud-based infrastructures.
Proficiency in implementing security aspects, including API security, authentication, authorization, message & transport level security.
Experience working with API Management tools like MuleSoft API Manager.
Well-versed in configuring VPC and dedicated load balancer on the Anypoint platform.
Familiarity with DevOps stack (CI & CD) and other dependency management and build tools.
Show more
Show less","MuleSoft, CloudHub, DataWeave, Anypoint MQ, SOA/EAI, Application Integration Architecture, API, Microservices, Cloud/SaaS applications, Salesforce, MS Dynamics, Data warehouse, VPC, API Manager, DevOps, CI/CD, Dependency management, Build tools","mulesoft, cloudhub, dataweave, anypoint mq, soaeai, application integration architecture, api, microservices, cloudsaas applications, salesforce, ms dynamics, data warehouse, vpc, api manager, devops, cicd, dependency management, build tools","anypoint mq, api, api manager, application integration architecture, build tools, cicd, cloudhub, cloudsaas applications, datawarehouse, dataweave, dependency management, devops, microservices, ms dynamics, mulesoft, salesforce, soaeai, vpc"
Health Care Data Analyst (Full-Time),University Health,"San Antonio, TX",https://www.linkedin.com/jobs/view/health-care-data-analyst-full-time-at-university-health-3786637236,2023-12-17,San Antonio,United States,Mid senior,Onsite,"Position Summary/Responsibilities
Responsible for research, data collection and analysis of health plan data to identify areas for improving cost savings, utilization management, satisfaction, and market share. Serves as a subject matter expert and coach to other CFHP analysts. Acts as primary health plan resource to analyze and improve organizational performance. Identifies areas for improvement, documents business processes, researches best practices, and assists process owners to implement change and monitor performance. Interacts extensively with other CFHP departments.
Education/Experience
Bachelor's degree in Business, Statistics or related field is required. Two years experience conducting data analysis in a managed care environment is preferred. Eight (8) years experience in a relational database environment (MS SQL Server 2005/2008 or Oracle) including applicable knowledge of Windows and Unix operating systems may be substituted for the educational requirement. Proficiency in Excel, Access, PowerPoint, and Word is required. Experience with SQL or Oracle databases, and ESRI Business Analyst is preferred.
PI234089132
Show more
Show less","SQL, Oracle, MS SQL Server, Windows, Unix, Excel, Access, PowerPoint, Word, ESRI Business Analyst, Data analysis, Business process improvement, Change management, Performance monitoring","sql, oracle, ms sql server, windows, unix, excel, access, powerpoint, word, esri business analyst, data analysis, business process improvement, change management, performance monitoring","access, business process improvement, change management, dataanalytics, esri business analyst, excel, ms sql server, oracle, performance monitoring, powerpoint, sql, unix, windows, word"
LEAD ANALYST - LEAD ENGINEER - PRINCIPAL ENGINEER - Systems Engineer/Data Science Engineer,Southwest Research Institute,"San Antonio, TX",https://www.linkedin.com/jobs/view/lead-analyst-lead-engineer-principal-engineer-systems-engineer-data-science-engineer-at-southwest-research-institute-3739152874,2023-12-17,San Antonio,United States,Mid senior,Onsite,"Who We Are:
Join Our Team! The Tactical Aerospace Department is a premier supplier for aerospace quality technology insertion on new and legacy DoD systems. This includes avionics, ground systems, and cutting edge third generation AI/ML for DoD platforms.
Objectives of this Role:
Develop System and LRU Architectures; this includes Data Science/Analytics techniques to design data pipelines that utilizes second/third generation AI/ML.
Work collaboratively across a multi-disciplinary engineering teams to establish hardware, firmware, and AI/ML requirements for complex flight worthy avionics hardware.
Interact directly with clients to establish system requirements and architectures to meet client expectations.
Provide technical project leadership throughout each program.
Utilize Model Based Systems Engineering (MBSE) processes to support system designs/architectures.
Support department management in specific marketing activities, proposal development of embedded avionics programs, and in the development of product/technical roadmaps.
Daily and Monthly Responsibilities:
Perform as a System Engineer over embedded avionics programs.
Lead a multi-disciplinary team to develop, integrate, and test complex avionics; this includes electrical, mechanical, firmware, data pipelines, data science, and AI/ML.
Meet with internal and external customers to establish system technical requirements and facilitate development of specification documents and system architectures.
Lead trade analyses within and across the relevant domains.
Work with stakeholders to allocate functions to components/systems and develop those allocations into requirements for the engineers, designers, and managers to design, develop, integrate and test.
Requirements:
Requires a Bachelor of Science in Electrical Engineering, Data Engineering, Data Science, Data Analytics or related with directly related experience
Shown experience in complex electronic avionics systems.
Shown experience with System Safety Analysis, FMEA, Hardware architectures, avionic I/O , latency analysis, and MBSE.
8 years: Experience developing, deriving, and allocating tiered Systems Engineering requirements in Avionics applications including data pipelines and tiered Data Science applications
Experience developing Architectures and basic/Intermediate experience with MBSE
Broad, basic understanding of engineering activities/disciplines in avionics development (e.g. Systems, Hardware Development, Firmware, Integration, Testing, Bid and Proposal efforts, CONOPS, Certification, System Safety, FMEA, Lab activities, etc)
A valid/clear driver's license is required
Special Requirements:
Applicant selected will be subject to a government security investigation and must meet eligibility requirements for access to classified information. Applicant must be a U.S. citizen.
Job Locations: San Antonio, Texas
Show more
Show less","Avionics, Ground Systems, AI/ML, Data Science/Analytics, Hardware, Firmware, MBSE, System Engineering, Electrical Engineering, Data Engineering, System Safety Analysis, FMEA, Hardware Architectures, Avionic I/O, Latency Analysis, Systems Engineering Requirements, Data Pipeline Architecture, Data Science Applications, Bid and Proposal Efforts, CONOPS, Certification, Lab Activities","avionics, ground systems, aiml, data scienceanalytics, hardware, firmware, mbse, system engineering, electrical engineering, data engineering, system safety analysis, fmea, hardware architectures, avionic io, latency analysis, systems engineering requirements, data pipeline architecture, data science applications, bid and proposal efforts, conops, certification, lab activities","aiml, avionic io, avionics, bid and proposal efforts, certification, conops, data engineering, data pipeline architecture, data science applications, data scienceanalytics, electrical engineering, firmware, fmea, ground systems, hardware, hardware architectures, lab activities, latency analysis, mbse, system engineering, system safety analysis, systems engineering requirements"
Staff Data Engineer,Recruiting from Scratch,"San Antonio, TX",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744395172,2023-12-17,San Antonio,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Deployment, Streamprocessing systems, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data warehouses, ETL, Data management tools, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"San Antonio, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748828478,2023-12-17,San Antonio,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Data Science, Kubernetes, Docker, Helm, Spark, pySpark, Kafka, Storm, SparkStreaming, ETL, Data Warehouses, Airflow","python, sql, snowflake, data science, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, etl, data warehouses, airflow","airflow, data science, data warehouses, docker, etl, helm, kafka, kubernetes, python, snowflake, spark, sparkstreaming, sql, storm"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"San Antonio, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744393506,2023-12-17,San Antonio,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, automated testing, Kafka, Storm, SparkStreaming, dimensional data modeling, ETL, legal compliance, data classification, retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, dimensional data modeling, etl, legal compliance, data classification, retention","airflow, automated testing, continuous integration, data classification, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, retention, snowflake, spark, sparkstreaming, sql, storm, tdd"
Human Resources Compensation and Data Analyst,CAPTRUST,"San Antonio, TX",https://www.linkedin.com/jobs/view/human-resources-compensation-and-data-analyst-at-captrust-3775815711,2023-12-17,San Antonio,United States,Mid senior,Onsite,"WHO are we looking for?
The
Human Resources Compensation and Data Analyst
will oversee the firm’s compensation function and day-to-day operations of data processing. The analyst will research job requirements and evaluate job positions to ensure the company is competitive in the areas of salaries and be responsible for data production, report generation, and dashboards to ensure the timely and accurate processing of data. Additionally, the analyst will provide leadership for math heavy functional areas such as benefits design and analysis and project coordination for the human resource information system (HRIS).
Responsibilities
Benchmark jobs against survey data and other market intelligence to determine competitive compensation ranges for each position
Prepare and maintain job classifications and salary scales
Prepare and present summary reports of job analysis and compensation analysis information
Gather data from market-based compensation surveys; use spreadsheet and data analysis techniques to assess results and market trends
Evaluate and implement job analysis instruments and materials
Interview and survey employees and managers to gather and document job, organizational, and occupational information including duties, responsibilities, and skills required by each job
Assesses jobs and their respective duties to determine classification as exempt or nonexempt and appropriate salary range
Oversee and manage assigned project and/or department’s data
Coordinate the production of data and ensure timely and accurate processing
Ensure integrity of data being processed is maintained, and field data-related questions
Build PowerBI dashboards for various department initiatives and for executives
Serve as the point person for data-related tasks on an assigned project including data requests for requests for proposals (RFPs)
Participate in the HRIS function in managing the day-to-day activities to support the HR, payroll and other interrelated functions
Primary financial analyst for benefits design
Perform other related duties as assigned
Qualifications
Minimum Qualifications:
Bachelor’s degree in a related field (Mathematics, Finance or Computer Science)
Minimum of two to three years’ experience in compensation and analyzing complex data
Desired Qualifications/Skills
Strong analytical and problem-solving skills
Expert in Microsoft Excel
Detail oriented
Excellent organizational and time management skills
Strong verbal and written communication skills
Ability to prioritize tasks
Proficient in Microsoft Office Suite or related software
Ability to explain technical information in understandable language to nontechnical personnel
WHAT can you expect from your career at CAPTRUST?
Our colleagues, like our clients, tend to stay with CAPTRUST for years. There’s a reason for it; it’s a great culture in which to work and grow. We all work together, each of us motivating those around us with our commitment to high standards. At CAPTRUST, expect a fully stocked break room, fun employee events, and a quality team surrounding you with opportunities for personal growth.
Our Employee Benefits Package shows how much we value our team. Some benefits include:
Employee ownership opportunities
Brick Bonus success sharing program
Comprehensive health coverage + Virgin Pulse wellness platform
401(k) program with a 5% employer match + financial planning for colleagues
WHERE will you be working?
The position can be filled at any of the following office locations:
400 North Tampa St #1800 | Tampa, FL 33602
4200 West 115th Street #210 | Leawood, KS 66211
4201 Congress St #160 | Charlotte, NC 28209
4208 Six Forks Rd #1700 | Raleigh, NC 27609
OR
700 North Saint Mary's St #100 | San Antonio, TX 78205
HOW do we build a world class organization one brick at a time?
We make it a priority to hire those who have a commitment to service, a real interest in other people, and a passion to continuously improve. Simply put: the difference at CAPTRUST is the quality of our people and depth of our bench. If you are ready to make your mark, we want to talk to you.
Are you the next brick?
To get it done the CAPTRUST Way, an individual should exhibit the following characteristics:
Ability to build successful, collaborative, and trusting relationships
Instinctive aptitude for consistently creating accurate, concise, respectful, and easy-to-understand verbal and written communications conveying complex information
A strong sense of urgency about getting work done and solving problems to achieve results that benefit our clients and colleagues, even when faced with challenges
Inherent desire to give back to our communities and enrich the lives of those around us
An other-centered mindset
Integrity through maintaining objectivity
EEO/Diversity Statement
At CAPTRUST, we are committed to building and maintaining a diverse workforce and inclusive work environment where ALL colleagues feel authentically seen, respected, and supported.
CAPTRUST is committed to providing employment opportunities without regard to race, color, age, sex, sexual orientation, familial status, religious creed, national origin, ancestry, medical condition, marital status and registered domestic partner status, citizenship status, military and veteran status, disability, protected medical condition, genetic information, or any other status protected by law. CAPTRUST makes all employment decisions without regard to these protected statuses and does not tolerate harassment or discrimination. #hybrid #mid-senior
Show more
Show less","Data Analysis, Microsoft Excel, PowerBI, Compensation and Benefits, Job Classification, Market Research, HRIS, Project Coordination, Microsoft Office Suite, Communication Skills, Problem Solving, Analytical Skills, Time Management, Organizational Skills","data analysis, microsoft excel, powerbi, compensation and benefits, job classification, market research, hris, project coordination, microsoft office suite, communication skills, problem solving, analytical skills, time management, organizational skills","analytical skills, communication skills, compensation and benefits, dataanalytics, hris, job classification, market research, microsoft excel, microsoft office suite, organizational skills, powerbi, problem solving, project coordination, time management"
Data Scientist,Archetype Permanent Solutions,"San Antonio, TX",https://www.linkedin.com/jobs/view/data-scientist-at-archetype-permanent-solutions-3787919070,2023-12-17,San Antonio,United States,Mid senior,Onsite,"We are seeking a highly skilled and analytical Data Scientist to join our team. The ideal candidate will be adept at using large data sets to find opportunities for product and process optimization, and using models to test the effectiveness of different courses of action. This role will require both a strong technical background in data analysis and the ability to communicate findings to non-technical teams and stakeholders.
Key Responsibilities:
Work with stakeholders to identify opportunities for leveraging company data to drive business solutions.
Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques, and business strategies.
Assess the effectiveness and accuracy of new data sources and data gathering techniques.
Develop custom data models and algorithms to apply to data sets.
Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting, and other business outcomes.
Coordinate with different functional teams to implement models and monitor outcomes.
Develop processes and tools to monitor and analyze model performance and data accuracy.
Qualifications:
Bachelor’s or Master’s degree in Statistics, Mathematics, Computer Science, or another quantitative field.
2 years of experience in a Data Scientist or Data Analyst role.
Strong problem-solving skills with an emphasis on product development.
Experience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets.
Experience working with and creating data architectures.
Knowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.
Excellent written and verbal communication skills for coordinating across teams.
Preferred Skills:
Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, Gurobi, MySQL, etc.
Experience analyzing data from 3rd party providers: Google Analytics, Site Catalyst, Coremetrics, Adwords, Crimson Hexagon, Facebook Insights, etc.
Experience with AWS, Azure, or another cloud service.
Experience visualizing/presenting data for stakeholders using: Periscope, Business Objects, D3, ggplot, etc.
Benefits:
Competitive salary and benefits package.
Dynamic and innovative work environment.
Opportunities for professional growth and development.
[Other company-specific benefits like remote work options, flexible schedules, wellness programs, etc.]
How to Apply:
Please submit your resume, cover letter, and any relevant work samples
Powered by JazzHR
3tBUY1AUTb
Show more
Show less","Data Science, Data Analysis, Statistics, Mathematics, Computer Science, R, Python, SQL, Data Modeling, Machine Learning, Clustering, Decision Tree Learning, Artificial Neural Networks, Map/Reduce, Hadoop, Hive, Spark, Gurobi, MySQL, Google Analytics, Site Catalyst, Coremetrics, Adwords, Crimson Hexagon, Facebook Insights, AWS, Azure, Periscope, Business Objects, D3, ggplot","data science, data analysis, statistics, mathematics, computer science, r, python, sql, data modeling, machine learning, clustering, decision tree learning, artificial neural networks, mapreduce, hadoop, hive, spark, gurobi, mysql, google analytics, site catalyst, coremetrics, adwords, crimson hexagon, facebook insights, aws, azure, periscope, business objects, d3, ggplot","adwords, artificial neural networks, aws, azure, business objects, clustering, computer science, coremetrics, crimson hexagon, d3, data science, dataanalytics, datamodeling, decision tree learning, facebook insights, ggplot, google analytics, gurobi, hadoop, hive, machine learning, mapreduce, mathematics, mysql, periscope, python, r, site catalyst, spark, sql, statistics"
"Data Conversion Developer, Senior Associate",PwC,"San Antonio, TX",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749937559,2023-12-17,San Antonio,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Maximo, PowerPlant, Azure Data Engineer Associate, Databricks Certified Data Engineer Associate, Computer Science, Information Technology, Data analysis, Data extraction, Data transformation, Data loading, SQL, ETL tools, Azure ADF, AWS Glue, SSIS, DataBricks, Data cleansing, Python, PySpark, Scala","maximo, powerplant, azure data engineer associate, databricks certified data engineer associate, computer science, information technology, data analysis, data extraction, data transformation, data loading, sql, etl tools, azure adf, aws glue, ssis, databricks, data cleansing, python, pyspark, scala","aws glue, azure adf, azure data engineer associate, computer science, data extraction, data loading, data transformation, dataanalytics, databricks, databricks certified data engineer associate, datacleaning, etl tools, information technology, maximo, powerplant, python, scala, spark, sql, ssis"
Expression of Interest: Data Scientist,Fingerprint for Success (F4S),"San Antonio, TX",https://www.linkedin.com/jobs/view/expression-of-interest-data-scientist-at-fingerprint-for-success-f4s-3787771606,2023-12-17,San Antonio,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
SuIE1TJG4w
Show more
Show less","F4S work style assessment, Predictive analytics, Team motivations, Behaviors, Performance, JazzHR","f4s work style assessment, predictive analytics, team motivations, behaviors, performance, jazzhr","behaviors, f4s work style assessment, jazzhr, performance, predictive analytics, team motivations"
"Bus Analyst I Data,Rept&Vis",H-E-B,"San Antonio, TX",https://www.linkedin.com/jobs/view/bus-analyst-i-data-rept-vis-at-h-e-b-3779272578,2023-12-17,San Antonio,United States,Mid senior,Hybrid,"The H-E-B Planning & Analysis Team develops and maintains budgets and financial systems while providing current, reliable financial data, analysis, and technical info. To help make essential Corporate planning decisions, our Analysts apply data-wrangling skills and business acumen to identify and tackle business problems. As a Business Analyst - Data, Reporting & Visualization, you'll support Product Manager work with data analysis, requirements documentation, and reporting and visualization.
Once you're eligible, you'll become an Owner in the company, so we're looking for commitment, hard work, and focus on quality and Customer service. 'Partner-owned' means our most important resources--People--drive the innovation, growth, and success that make H-E-B The Greatest Omnichannel Retailing Company.
Do you have a:
HEART FOR PEOPLE... skills to communicate technical data to non-technical people?
HEAD FOR BUSINESS... visualization skills?
PASSION FOR RESULTS... drive to explore / map data and processes?
We are looking for:
a related degree or comparable formal training, certification, or work experience
2+ years of experience in business intelligence report development
What is the work?
Analytics / Reporting & Documentation:
Collaborates with Product Owners and stakeholders to document business processes and detailed requirements for Data Engineers and Report Developers
Applies understanding of project vision from Product Owner; assists in coordinating efforts with a cross-functional team
Assists Product Owners in building out project roadmaps
Learns to serve as data SME in specific subject area(s); applies understanding of data flow across all systems that support the business process; supports mapping of business process to systems and data to support Visualization
Supports Data Engineers in translating product into solutions
Explores / profiles new data sets to understand scenarios and anomalies
Accesses / organizes data; builds out analyses to support / propose solutions for business problems
Creates simple reports and visualization as part of data exploration and analysis
Performs quality assurance / user acceptance testing for Data Solution or Report Development solutions
Provides ongoing support for products built by the vertical scrum team and across verticals
Provides ongoing reporting and subject area training for H-E-B BI users
What is your background?
A related degree or comparable formal training, certification, or work experience
2+ years of experience in business intelligence report development
Experience on H-E-B data engineering, financial analysis, or merchandising teams (a plus)
Do you have what it takes to be a fit as an H-E-B Business Analyst - Data, Reporting & Visualization?
Strong working knowledge of SQL
Strong data visualization skills
Ability to extract, explore, and profile data
Ability to communicate with all levels of stakeholders
Can you...
Function in a fast-paced, retail, office environment
Work extended hours, as needed
11-2020
Show more
Show less","Data Wrangling, SQL, Data Visualization, Business Intelligence, Report Development, Data Analysis, Requirements Documentation, Project Roadmaps, Data Mapping, Data Profiling, Data Exploration, Data Exploration, Data Solution Testing, User Acceptance Testing, Data Training, Financial Analysis, Merchandising","data wrangling, sql, data visualization, business intelligence, report development, data analysis, requirements documentation, project roadmaps, data mapping, data profiling, data exploration, data exploration, data solution testing, user acceptance testing, data training, financial analysis, merchandising","business intelligence, data exploration, data mapping, data profiling, data solution testing, data training, data wrangling, dataanalytics, financial analysis, merchandising, project roadmaps, report development, requirements documentation, sql, user acceptance testing, visualization"
Lead Data Integration Engineer- EIT,HOLT CAT,"San Antonio, TX",https://www.linkedin.com/jobs/view/lead-data-integration-engineer-eit-at-holt-cat-3661328150,2023-12-17,San Antonio,United States,Mid senior,Hybrid,"As the Lead Data Integration Engineer you will be part of a team responsible for delivering cloud data management solutions to our customers.
Integration engineers are an integral part of the Data Solutions team and primarily responsible for implementing solutions that integrate applications across an enterprise. They are the trusted advisor to client’s technology teams and bring passion for solving complex business problems by designing and building reusable integrations.
The Lead Data Integration Engineer will support solution architects, business analysts and data engineers on system implementations and ensure optimal data delivery. They will work with stakeholders to define non-functional requirements and partner with solution architects to develop the solution architecture. This role will lead a team of onshore and offshore/nearshore engineers responsible for building integrations and other required automation. They must be self-directed and comfortable supporting the integration needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of designing, optimizing or even re-designing our client’s data architecture to support next generation of products and transformation initiatives.
The incumbent in this position is expected to model the following practices daily: 1) Demonstrate alignment with the company's mission and core business values; 2) Collaborate with key internal/external resources; 3) Participate in ongoing self- development.
Essential Functions:
Develops, evaluates, and influences effective and consistent productivity and teamwork to ensure the delivery of Legendary Customer Service (LCS)
Models, promotes, reinforces, and rewards the consistent use of HOLT’s Values Based Leadership (VBL) tools, models, and processes to ensure alignment with our Vision, Values, and Mission
Defines systems integrations, design patterns and development standards to support cross-functional, multi-system solutions that are scalable and flexible to meet current and future needs of the organization
Analyzes and translates business requirements using frameworks into components of a modernized solution
Architects, designs, develops, and implements small to large scale integration solutions in MuleSoft platform based on functional and technical requirements
Creates architectural deliverables that clearly communicate design and solution
Designs and develops automated solutions in accordance with MuleSoft and enterprise leading practices and design principles
Participates in design reviews to ensure they meet automation policies and design principles
Authors and maintains solution design documentation
Develops efficient, well-structured, reusable, and scalable automation processes and integrations
Performs thorough code-reviews based on high engineering standards and writes unit and integration tests based on chosen DevOps frameworks
Analyzes and resolves automation software issues whenever required
Identifies and communicates risks associated with integration solutions and process automation candidates
Provides guidance to junior resources on best practices and development techniques for automated processes
Leads one or more team members consisting of cross functional, global, and virtual groups; may need to supervise staff and assign responsibility to other team members.
Develop and maintain relationships with key client leadership
Works with Business Development Manager (BDM) (Salesperson) to identify new opportunities
Engages in multiple short-term strategic consulting engagements and develop new opportunities
Manages the development of case studies and project summaries of each project delivered related to the service offering(s)
Acts as trusted advisor and expert on MuleSoft platform promoting security and performance
Works safely always and adheres to all applicable safety policies; complies with all company policies, procedures, and standards
Performs other duties as assigned
Knowledge, Skills, and Abilities:
Experience creating and maintaining domain diagrams, architecture frameworks, design patterns and standards to support various work streams
Strong experience in the Application Integration Architecture, API and Microservices architecture, Solution Design, Development using SOA/EAI solutions, API Led Architectures, creation of API design specifications, and RAML creation
Experience integrating with Cloud/SaaS applications, APIs, SDK of packaged applications and legacy Ideally have Salesforce, MS Dynamics, and Data warehouse integration experience
Hands on experience on MuleSoft's CloudHub, DataWeave, Anypoint MQ and deploying/managing Mule flows to CloudHub
Experience setting up and configuring on-premise/cloud-based infrastructures
Experience in implementing security aspects including API security, authentication, authorization, message & transport level security
Experience in API Management tools using MuleSoft API Manager or others
Well versed in configuring VPC and dedicated load balancer on Anypoint platform
Good knowledge on DevOps stack (CI & CD) and other dependency management and build tools
Experience working with API testing Tools like SOAPUI, postman
Experience with High-Availability, Fault-Tolerance, Performance Testing and Tuning parameters
Well versed with agile methodologies and source control (Bitbucket, GitHub, ADO)
A desire to work as part of a growing, fast-paced, and highly flexible team
Be comfortable working in a matrix environment and foster motivation within the project team to meet tight deadlines
Possess the ability to manage workload, manage multiple priorities, and manage conflicts with customers/employees/managers, as applicable
Excellent problem solving and project management skills; experienced in both Agile and waterfall methodologies
Education and Experience:
High School diploma or equivalent required; Bachelor’s degree in Information Technology, or related field preferred
8+ years of experience in delivering enterprise complex systems integrations and intelligent automations required
6+ years of demonstrated hands-on experience with ESB platforms such as Talend, Workato, Boomi, MuleSoft, Informatica or similar products required
Strong working experience with SQL/PLSQL and relational databases such as Oracle, MS SQL Server, and NoSQL databases required
Established enterprise integration infrastructure, supporting ESB, messaging and SLA monitoring tools required
Experience with messaging infrastructure, preferably Azure Service Bus and with Storage like Azure Blobs or Data Lake preferred
Experience with ETL and Web Services based integrations with expert level knowledge of developing APIs using SOAP and REST architecture styles and data interchange formats like XML, JSON, etc. required
Experience working in an Agile environment preferred
Preferred Certifications
Active MuleSoft, Salesforce or Azure
MuleSoft Certified Developer and MuleSoft Certified Integration Architect
Supervisory Responsibilities:
This position directs and manages the positions within assigned division. Responsibilities include, but are not limited to interviewing, hiring, and training employees; planning, assigning, and directing work; coaching and development; appraising performance; rewarding and educating employees; resolving conflicts.
Travel:
Up to 20% with occasionally overnight stay
Physical Requirements:
This position involves extended periods in a stationary position; additionally, occasional movement inside the office to access office machinery, file cabinets,
This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets and fax machines
Work Environment
This job is generally performed in a professional office environment
Frequently works at fast pace with unscheduled interruptions
Disclaimer:
Please note that the above statements are intended to describe the general nature and level of work being performed by employees assigned to this classification. They are not to be interpreted as an exhaustive list of all responsibilities, duties, and skills required of the incumbents so classified. All incumbents may be required to perform duties outside of their normal responsibilities, as needed.
Show more
Show less","Data Integration, MuleSoft, CloudHub, DataWeave, Anypoint MQ, VPC, API Manager, Bitbucket, GitHub, ADO, SOAPUI, High Availability, Fault Tolerance, Performance Testing, Agile, SQL, PLSQL, Oracle, MS SQL Server, NoSQL, Azure Service Bus, Azure Blobs, Azure Data Lake, REST, XML, JSON","data integration, mulesoft, cloudhub, dataweave, anypoint mq, vpc, api manager, bitbucket, github, ado, soapui, high availability, fault tolerance, performance testing, agile, sql, plsql, oracle, ms sql server, nosql, azure service bus, azure blobs, azure data lake, rest, xml, json","ado, agile, anypoint mq, api manager, azure blobs, azure data lake, azure service bus, bitbucket, cloudhub, data integration, dataweave, fault tolerance, github, high availability, json, ms sql server, mulesoft, nosql, oracle, performance testing, plsql, rest, soapui, sql, vpc, xml"
Data Center Energy Marshal 2,Ascendion,"San Antonio, TX",https://www.linkedin.com/jobs/view/data-center-energy-marshal-2-at-ascendion-3782503747,2023-12-17,San Antonio,United States,Mid senior,Hybrid,"About Ascendion
Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next.
Ascendion | Engineering to elevate life
We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:
Build the coolest tech for world’s leading brands
Solve complex problems – and learn new skills
Experience the power of transforming digital engineering for Fortune 500 clients
Master your craft with leading training programs and hands-on experience
Experience a community of change makers!
Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.
About the Role:
The Energy Marshal is an assurance role that helps ensure the vendors’ and contractors’ Energy Isolation Program meets or exceeds company safety policy requirements as stood up by the Authorizing Energy Marshal.
Energy Isolation Management is a significant part of construction and commissioning efforts. To ensure focus and compliance for all aspects and types of energy isolation a Energy Marshal is required for each campus or project.
This critical role implements learnings, provides consistency, and drives rigor into company's energy isolation program. The Energy Marshal is a companies representative who oversees and assures the Authorizing Energy Marshal (GC) and supplemental vendor support comply with the overall Energy Isolation Program Management on a datacenter construction campus.
Job Title: Data Center Energy Marshal 2
Key Responsibilities:
The Energy Marshal is an assurance role that helps ensure the vendors’ and contractors’ Energy Isolation Program meets or exceeds any Company's safety policy requirements and is stood up by the Authorizing Energy Marshal.
Utilize SPS-101 Energy Isolation SPS and SPS-101 Energy Isolation Guidebook as the baseline and foundation of the Energy Isolation Program that is implemented throughout the campus and project.
Establishes and manages an Energy Isolation assurance process and works with site teams to close any gaps identified in assurance audits. Alignment Checklist SPS 101 Energy Isolation is provided as a reference / guideline
Participate in High Risk Activity (HRA) planning meetings associated with Energy Isolation.
Ensures approval process for proposed Energy Isolation procedures is in place.
Work with the Authorizing Energy Marshal to establish an Energy Isolation Permit process.
Ensures SoWs, MOPs, and Scripts are reviewed, and all sources of energy are identified.
Helps determine if a group or individual LOTO will be required. Establishes with the Authorizing Energy Marshal a centralized LOTO with all site entities.
Ensures process is established that verifies qualifications, training, and PPE of personnel performing the work.
Delegates alternates that are qualified to assume role during multi-shift and peak energization time frames.
Acts in coordination with the Authorizing Energy Marshal as a subject matter expert in all Energy Isolation incident investigations in area of responsibility.
Serves as an independent authority for stopping all unsafe work practices regarding Energy Isolation.
Skills:
Ability to audit site practices against written standards as part of assurance role
DC construction
Good ability to effectively communicate complex technical solutions and concepts to engineers and non-engineers
Location: San Antonio, TX
Salary Range: The salary for this position is between $176800– $197600 annually. Factors that may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.
Benefits: The Company offers the following benefits for this position, subject to applicable eligibility requirements: Medical insurance, Dental insurance, Vision insurance, 401(k) retirement plan, long-term disability insurance, short-term disability insurance, and 5 personal days accrued each calendar year. The Paid time off benefits meets the paid sick and safe time laws that pertain to the City/ State, 10-15 days of paid vacation time, 6 paid holidays, and 1 floating holiday per calendar year, Ascendion Learning Management System.
Want to change the world? Let us know.
Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!
Show more
Show less","Energy Isolation Management, SPS101 Energy Isolation SPS, SPS101 Energy Isolation Guidebook, Energy Isolation Assurance Process, Alignment Checklist SPS 101 Energy Isolation, High Risk Activity (HRA) Planning Meetings, Energy Isolation Permit Process, SoWs, MOPs, Scripts, LOTO, PPE, Energy Isolation Incident Investigations, DC Construction, Technical Communication","energy isolation management, sps101 energy isolation sps, sps101 energy isolation guidebook, energy isolation assurance process, alignment checklist sps 101 energy isolation, high risk activity hra planning meetings, energy isolation permit process, sows, mops, scripts, loto, ppe, energy isolation incident investigations, dc construction, technical communication","alignment checklist sps 101 energy isolation, dc construction, energy isolation assurance process, energy isolation incident investigations, energy isolation management, energy isolation permit process, high risk activity hra planning meetings, loto, mops, ppe, scripts, sows, sps101 energy isolation guidebook, sps101 energy isolation sps, technical communication"
Senior Staff AI Data Engineer,Recruiting from Scratch,"San Antonio, TX",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087720,2023-12-17,San Antonio,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML/DL, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Statistical Analysis, Visualization, Big Data Technologies, Apache Airflow, Kubernetes, Apache Spark, Python, Java, SQL, Git, Data Pipelines, AWS, GCP, Azure, NLP, Conversational AI, Recommender Systems, Microservices, Kafka, Storm, Machine Learning","data engineering, mldl, data mining, data cleaning, data normalization, data modeling, statistical analysis, visualization, big data technologies, apache airflow, kubernetes, apache spark, python, java, sql, git, data pipelines, aws, gcp, azure, nlp, conversational ai, recommender systems, microservices, kafka, storm, machine learning","apache airflow, apache spark, aws, azure, big data technologies, conversational ai, data cleaning, data engineering, data mining, data normalization, datamodeling, datapipeline, gcp, git, java, kafka, kubernetes, machine learning, microservices, mldl, nlp, python, recommender systems, sql, statistical analysis, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"San Antonio, TX",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759708638,2023-12-17,San Antonio,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Airflow, Applied ML, AWS, Azure, Bash, Big data, Cloud environment, Computer Science, Data engineering, Data governance, Data pipelines, Data processing, Data science, Data visualization, Docker, DynamoDB, ETL, Git, GCP, Helm, Java, Kafka, Kubernetes, KubeFlow, LLMs, Machine learning, Mathematics, Microservices, NLP, NoSQL, Orchestration frameworks, Pandas, Physics, Python, R, Relational databases, Salesforce, Snowflake, SQL, Spark, SparkStreaming, Storm, Technology leadership, Text data, Version control","airflow, applied ml, aws, azure, bash, big data, cloud environment, computer science, data engineering, data governance, data pipelines, data processing, data science, data visualization, docker, dynamodb, etl, git, gcp, helm, java, kafka, kubernetes, kubeflow, llms, machine learning, mathematics, microservices, nlp, nosql, orchestration frameworks, pandas, physics, python, r, relational databases, salesforce, snowflake, sql, spark, sparkstreaming, storm, technology leadership, text data, version control","airflow, applied ml, aws, azure, bash, big data, cloud environment, computer science, data engineering, data governance, data processing, data science, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, llms, machine learning, mathematics, microservices, nlp, nosql, orchestration frameworks, pandas, physics, python, r, relational databases, salesforce, snowflake, spark, sparkstreaming, sql, storm, technology leadership, text data, version control, visualization"
Data Analyst,Marathon TS,"San Antonio, TX",https://www.linkedin.com/jobs/view/data-analyst-at-marathon-ts-3772601903,2023-12-17,San Antonio,United States,Mid senior,Hybrid,"Marathon TS is seeking a
Senior Data Analyst
to join a contract with a federal government client in support of an important mission.
This
position requires employees to be located in the Huntsville, AL area
in order to report to work on-site at Redstone Arsenal.
Responsibilities
Provide highly complex data mining, statistical analysis, trend analysis, and causal analysis.
Perform as a technical lead responsible for monitoring and providing monthly contract reports and deliverables.
Responsible for integrating multiple disciplines in an operations research team and translating applicable methods into language and application understandable by operational managers throughout the organization.
Review and provide quality control methods on products.
Prepare and update training materials to ensure newly assigned personnel gain an understanding of key analytic tools, procedures, and methodologies used.
Take structured and unstructured data and distill the information into a cohesive analytical product for a contracting functional business area audience.
Support pattern analysis methods to formulate recommendations to operational managers based upon exploiting patterns in the past, current, and anticipated operational environment.
Education and Experience
8 plus years in a technical field
BA/BS required
Required Skills
Have experience advising senior DoD decision makers on methodologies, results, and conclusions from applied operations research.
Have experience with the primary tools used for research services include, but are not limited to, Logistics Management Program, Vantage, Virtual Contracting Enterprise, General Fund Enterprise Business System (
GFEBS
), SAP Business Objects/Web Intelligence Reports, Microsoft SharePoint, Army-specific contract writing systems (Procurement Desktop Defense (
PD2
) and Procurement Automated Data and Document System (
PADDS
)), and various Government and Commercial business process automation systems.
Personnel should have strong data manipulation and problem-solving skills.
Must have strong technical skills in areas such as statistics, programming languages like R or
Python
,
SQL
(Structured Query Language), data visualization, and data cleaning and preparation.
Have good communication skills and problem-solving ability.
Security Clearance
Active Secret clearance is required
Show more
Show less","Data mining, Statistical analysis, Trend analysis, Causal analysis, Operations research, Quality control, Data visualization, Data cleaning, Python, SQL, R, SAP Business Objects, Microsoft SharePoint, Procurement Desktop Defense, Procurement Automated Data and Document System, GFEBS, Logistics Management Program, Vantage, Virtual Contracting Enterprise","data mining, statistical analysis, trend analysis, causal analysis, operations research, quality control, data visualization, data cleaning, python, sql, r, sap business objects, microsoft sharepoint, procurement desktop defense, procurement automated data and document system, gfebs, logistics management program, vantage, virtual contracting enterprise","causal analysis, data cleaning, data mining, gfebs, logistics management program, microsoft sharepoint, operations research, procurement automated data and document system, procurement desktop defense, python, quality control, r, sap business objects, sql, statistical analysis, trend analysis, vantage, virtual contracting enterprise, visualization"
Senior Cloud Data Engineer,BDO USA,"San Antonio, TX",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765469454,2023-12-17,San Antonio,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, SQL, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics Solutions, Azure, AWS, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, Data Lake, AI Algorithms, Machine Learning, Automation Tools, UiPath, Alteryx, Computer Vision based AI technologies","data analytics, business intelligence, artificial intelligence, application development, sql, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics solutions, azure, aws, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, data lake, ai algorithms, machine learning, automation tools, uipath, alteryx, computer vision based ai technologies","ai algorithms, alteryx, application development, artificial intelligence, automation tools, aws, azure, azure analysis services, batch data ingestion, business intelligence, c, cloud data analytics solutions, computer vision based ai technologies, data lake, data lake medallion architecture, dataanalytics, datamodeling, datawarehouse, devops, git, java, linux, machine learning, microsoft fabric, powerbi, python, scala, semantic model definition, sql, star schema construction, streaming data ingestion, tabular modeling, uipath"
Data Scientist (Active TS Clearance),Motion Recruitment,"San Antonio, TX",https://www.linkedin.com/jobs/view/data-scientist-active-ts-clearance-at-motion-recruitment-3737589734,2023-12-17,San Antonio,United States,Mid senior,Hybrid,"We are working with a Federal consulting company that is transforming the nations defense and national security with their AI platforms. This company works with various Federal agencies and will need this person to work on-site 2 to 3 days a week.
Candidates should hold at least an active Top Secret clearance.
Requirements
Over 3 years of experience in data science
Expertise in Python
Experience with MySQL and Oracle
Experience with machine learning
Bonus to have PhD in Sciences, Mathematics, or Engineering
Offer
Competitive salary
Annual Bonus
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
Paid Time Off (PTO)
401(k)
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Sean Thompson
Show more
Show less","Python, Machine Learning, MySQL, Oracle, Data Science","python, machine learning, mysql, oracle, data science","data science, machine learning, mysql, oracle, python"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Newyorkuniversity,"Yellowknife, Northwest Territories, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-newyorkuniversity-3750805522,2023-12-17,Yellowknife, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
Powered by Webbtree
Show more
Show less","Data analysis, Statistics, Advanced statistical techniques, R, Python, SQL, Tableau, Power BI, Data visualization, Data modeling, Algorithms, A/B testing, Data quality, Data cleansing, Data manipulation, Data reporting, Data presentation, Datadriven decision making, Business intelligence, ETL, Data management","data analysis, statistics, advanced statistical techniques, r, python, sql, tableau, power bi, data visualization, data modeling, algorithms, ab testing, data quality, data cleansing, data manipulation, data reporting, data presentation, datadriven decision making, business intelligence, etl, data management","ab testing, advanced statistical techniques, algorithms, business intelligence, data management, data manipulation, data presentation, data quality, data reporting, dataanalytics, datacleaning, datadriven decision making, datamodeling, etl, powerbi, python, r, sql, statistics, tableau, visualization"
Population Health Data Analyst Sr/UKHC,UK HealthCare,"Lexington, KY",https://www.linkedin.com/jobs/view/population-health-data-analyst-sr-ukhc-at-uk-healthcare-3780209127,2023-12-17,Kentucky,United States,Mid senior,Onsite,"Posting Details
Job Title
Population Health Data Analyst Sr/UKHC
Requisition Number
RE41879
Working Title
Population Health Data Analyst Senior
Department Name
H3997:EVPHA Information Technology
Work Location
Lexington, KY
Grade Level
12
Salary Range
$62,400-111,634/year
Type of Position
Staff
Position Time Status
Full-Time
Required Education
BA
Click here for more information about equivalencies:
https://hr.uky.edu/employment/working-uk/equivalencies
Required Related Experience
5 yrs
Required License/Registration/Certification
Epic Cogito & Healthy Planet (Population Health) within 90 days
Physical Requirements
Mobility to work from several locations depending on the business need. Regularly sitting at a computer workstation for extended periods and repetitive motions.
Shift
Monday through Friday; 8:00am – 5:00pm (40 hrs/week) with occasional weekends and off shifts as related to Information Technology support needs.
Job Summary
This position will enable expanded population health management, reporting, and analysis including but not limited to Epic registries and registry metrics. This position will build capacity and subject matter expertise in risk-based contracting and provide metrics related to screening, preventive care, and outcome metrics for population health.
Skills / Knowledge / Abilities
none
Does this position have supervisory responsibilities?
No
Preferred Education/Experience
BS with healthcare experience using population health tools from Epic or other systems preferred.
Deadline to Apply
12/21/2023
University Community of Inclusion
The University of Kentucky is committed to a diverse and inclusive workforce by ensuring all our students, faculty, and staff work in an environment of openness and acceptance. We strive to foster a community where people of all backgrounds, identities, and perspectives can feel secure and welcome. We also value the well-being of each of our employees and are dedicated to creating a healthy place to work, learn and live. In the interest of maintaining a safe and healthy environment for our students, employees, patients and visitors the University of Kentucky is a Tobacco & Drug Free campus.
As an Equal Opportunity Employer, we strongly encourage veterans, individuals with disabilities, women, and all minorities to consider our employment opportunities.
Any candidate offered a position may be required to pass pre-employment screenings as mandated by University of Kentucky Human Resources. These screenings may include a national background check and/or drug screen.
#HospitalGrade
Show more
Show less","Data Analysis, Population Health, Reporting, Epic, Registries, RiskBased Contracting, Screening, Preventive Care, Outcome Metrics","data analysis, population health, reporting, epic, registries, riskbased contracting, screening, preventive care, outcome metrics","dataanalytics, epic, outcome metrics, population health, preventive care, registries, reporting, riskbased contracting, screening"
Senior Principal Consultant – Data and Analytics,Genesys,"Kentucky, United States",https://www.linkedin.com/jobs/view/senior-principal-consultant-%E2%80%93-data-and-analytics-at-genesys-3781014608,2023-12-17,Kentucky,United States,Mid senior,Remote,"Build something new with a world-class team.
At Genesys, we allow our employees to make their mark by entrusting them to make decisions and do what they’ve been hired to do: their very best. Your potential is waiting; why are you?
About Genesys:
GENESYS® powers more than 25 billion of the world’s best customer experiences each year.
In GENESYS, Innovations team builds solutions that enable our customers to get more value from their Genesys platforms. The Innovations team’s primary job is to build new, innovative applications using AI and analytics, with the goal of helping our customers effectively and efficiently deliver experience as a service (XaaS).
Summary: We are seeking a highly skilled and experienced Contact Center Data and Analytics Consultant to join our team. The ideal candidate will have a strong background in data analysis, data visualization, data engineering, and business intelligence. This is a customer facing role and requires the ability to present to both technical and business executive audiences.
Responsibilities:
Present Genesys data and analytics technology and vision to executives and technical contributors at prospects and customers
Work hands-on with prospects and customers to demonstrate and communicate the value of Genesys data and analytics apps throughout the sales cycle, from demo to proof of concept to design and implementation
Immerse yourself in the ever-evolving industry of contact center and experience management SaaS, maintaining a deep understanding of competitive and complementary technologies and vendors and how to position our analytics apps in relation to them
Collaborate with Product Management, Engineering, and Saloes to continuously improve Genesys data and analytics apps and marketing
Requirements:
8+ years direct experience with Genesys Engage Infomart or GCXI or NICE CX Analytics; strong understanding of Contact Center and Customer Experience data and metrics
Strong proficiency in SQL, data modeling, and data engineering
Familiarity with scripting languages for data engineering
Outstanding presenting skills to both technical and executive audiences, whether impromptu on a whiteboard or using presentations and demos.
Hands-on expertise with SQL and SQL analytics
Ability to connect a customer’s specific business problems to Genesys analytics solutions
Experience with data visualization tools such as Tableau or Power BI
Preferred Qualifications:
Experience with Snowflake
Experience with Elastic (ELK stack)
Experience with data governance and data management best practices.
Experience working with remote engineering teams
Familiar with software development and project management frameworks
Experience in consulting or developing products and solutions with contact center technologies like Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya, etc.
Compensation:
This role has a market-competitive salary with an anticipated base compensation range listed below. Actual salaries will vary depending on a candidate’s experience, qualifications, skills, and location. This role might also be eligible for a commission or performance-based bonus opportunities.
$121,500.00 - $238,400.00
Benefits:
Medical, Dental, and Vision Insurance.
Telehealth coverage
Flexible work schedules and work from home opportunities
Development and career growth opportunities
Open Time Off in addition to 10 paid holidays
401(k) matching program
Adoption Assistance
Fertility treatments
More details about our company benefits can be found at the following link: https://mygenesysbenefits.com
If a Genesys employee referred you, please use the link they sent you to apply.
About Genesys:
Every year, Genesys orchestrates billions of remarkable customer experiences for organizations in more than 100 countries. Through the power of our cloud, digital and AI technologies, organizations can realize Experience as a Service™ our vision for empathetic customer experiences at scale. With Genesys, organizations have the power to deliver proactive, predictive, and hyper personalized experiences to deepen their customer connection across every marketing, sales, and service moment on any channel, while also improving employee productivity and engagement. By transforming back-office technology to a modern revenue velocity engine Genesys enables true intimacy at scale to foster customer trust and loyalty. Visit www.genesys.com.
Reasonable Accommodations:
If you require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you or someone you know may complete the Reasonable Accommodations Form for assistance. Please use the Candidate field in the dropdown menu to ensure a timely response.
This form is designed to assist job seekers who seek reasonable accommodation for the application process. Submissions entered for non-accommodation-related issues, such as following up on an application or submitting a resume, may not receive a response.
Genesys is an equal opportunity employer committed to equity in the workplace. We evaluate qualified applicants without regard to race, color, age, religion, sex, sexual orientation, gender identity or expression, marital status, domestic partner status, national origin, genetics, disability, military and veteran status, and other protected characteristics.
Please note that recruiters will never ask for sensitive personal or financial information during the application phase.
Show more
Show less","SQL, Data visualization, Data analytics, Data modeling, Data engineering, Data management, AI, Tableau, Power BI, Snowflake, Elastic (ELK stack), Software development, Project management, Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya","sql, data visualization, data analytics, data modeling, data engineering, data management, ai, tableau, power bi, snowflake, elastic elk stack, software development, project management, genesys engage, genesys cloud, nice, cisco, avaya","ai, avaya, cisco, data engineering, data management, dataanalytics, datamodeling, elastic elk stack, genesys cloud, genesys engage, nice, powerbi, project management, snowflake, software development, sql, tableau, visualization"
Departmental Analyst 9-12 - SIA Compliance and Data Governance Bureau,State of Michigan,"Lansing, MI",https://www.linkedin.com/jobs/view/departmental-analyst-9-12-sia-compliance-and-data-governance-bureau-at-state-of-michigan-3782766104,2023-12-17,East Lansing,United States,Mid senior,Onsite,"Job Description
The MDHHS mission is to provide opportunities, services, and programs that promote a healthy, safe, and stable environment for residents to be self-sufficient. We are committed to ensuring a diverse workforce and a work environment whereby all employees are treated with dignity, respect and fairness. For more information, please visit our MDHHS Diversity, Equity, and Inclusion Plan.
Departmental Analyst 9-P11 (E Level)
This position serves, in an entry to experienced level capacity, as an analyst, supporting a wide range of Bureau functions including compliance with state and federal regulations and department procedures; privacy, security, and data governance; liaison with business program project teams and overall business support.
Position Description
Departmental Analyst 12 (A level)
This position in concert with the approved senior standards supports a wide range of complex Compliance and Data Governance Bureau functions including: compliance with federal regulations and department procedures; development of training, outreach, and communications; Integrated Service Area (ISA) and Business support. The position is responsible for completing reports for federal agencies. Provides guidance and serves as a resource for internal audits. Provides analysis and recommendations to supervisor on security related matters. The individual assigned serves as the subject matter expert for supporting agency businesses and programs on best practices for privacy, security and data governance. The position provides expert level support to the agency and conducts investigations related to privacy and security incidents. Manages implementation, tracking and reporting of security related activities. Provides advanced level guidance to business on business, functional and technical requirements related to privacy, security and data governance. The position leads various activities related to documenting and reviewing security plans and data classification of systems. Also provides subject matter expertise with establishment of interconnection sharing agreements and data sharing agreements requiring detailed understanding of laws and regulations related to data. Handles complex privacy, security and data governance inquiries. Advanced level of knowledge related to the various privacy and security frameworks utilized within the agency.
Position Description
Job Specification
To Be Considered For This Position You Must
Apply for this position online via NEOGOV; click on ""Apply"" in the job posting for instructions on submitting your electronic application. Hard copy applications are not accepted.
Relevant experience and/or education referred to in the supplemental questions must be documented in the resume, transcript and/or application to allow for accurate screening.
Attach a resume identifying specific experience and dates of employment. Dates of employment should include month and year and hours per week.
Attach a cover letter.
If applicable, attach a copy of an official transcript(s). We accept scanned copies of official transcripts. We do not accept web-based, internet, or copies of unofficial transcripts. Official transcripts provide the name of the institution, confirmation that a degree was awarded and on what date, and the registrar's signature.
Failure to complete any of the above items may result in your application not being considered. See instructions for attaching files here:
Instructions
Required Education And Experience
Education
Possession of a bachelor's degree in any major.
Experience
Departmental Analyst 9
No specific type or amount is required.
Departmental Analyst 10
One year of professional experience.
Departmental Analyst P11
Two years of professional experience, including one year of experience equivalent to the intermediate (10) level in state service.
Departmental Analyst 12
Three years of professional experience, including one year of experience equivalent to the experienced (P11) level in state service.
Additional Requirements And Information
This position is being filled as either a Departmental Analyst 9-P11 or Departmental Analyst 12. The specific level will depend on the selected candidate.
This position will work remotely. Occasional travel to the office located at Grand Tower Building, 235 S. Grand Ave., Lansing, MI may be required. Selected candidates who have been approved to work remotely must complete that work within Michigan. Candidates should confirm work location and schedule at the time of interview.
A secure work location that allows privacy and prevents distractions.
A high-speed internet connection of at least 25 Mbps download and 5 Mbps upload.
Suitable lighting, furniture, and utilities.
Your application for any position does not guarantee you will be contacted by the Department/Agency for further consideration. Only those applicants interviewed will be notified of the results.
As a Condition of Employment:
this position requires successful completion of a background investigation and a criminal records check.
If you previously held status in this classification and departed within the last three (3) years, please contact Human Resources regarding your interest in a potential reinstatement. Reinstatement is not guaranteed or required.
The Department of Health & Human Services reserves the right to close this posting prior to its original end date once a sufficient number of applications have been received.
For information about this specific position, please email MDHHSVacancies@michigan.gov . Please reference the job posting number in subject line.
Follow us on LinkedIn for more job opportunities!
MDHHSJobs #MDHHSJobs #Veteranfriendly #CareersWithPurpose #CommunityHeroes
MDHHS is proud to be a Michigan Veteran’s Affairs Agency (MVAA) Gold Level Veteran-Friendly Employer.
Show more
Show less","Compliance, Privacy, Security, Data governance, Liaison, Business program project teams, Business support, Reporting, Internal audits, Investigations, Security plans, Data classification, Interconnection sharing agreements, Data sharing agreements, Laws, Regulations, Education, Experience, Professional experience, Work remotely, Highspeed internet connection, Secure work location, Background investigation, Criminal records check, Reinstatement, LinkedIn, MDHHSJobs, #MDHHSJobs, #Veteranfriendly, #CareersWithPurpose, #CommunityHeroes, MVAA, Gold Level VeteranFriendly Employer","compliance, privacy, security, data governance, liaison, business program project teams, business support, reporting, internal audits, investigations, security plans, data classification, interconnection sharing agreements, data sharing agreements, laws, regulations, education, experience, professional experience, work remotely, highspeed internet connection, secure work location, background investigation, criminal records check, reinstatement, linkedin, mdhhsjobs, mdhhsjobs, veteranfriendly, careerswithpurpose, communityheroes, mvaa, gold level veteranfriendly employer","background investigation, business program project teams, business support, careerswithpurpose, communityheroes, compliance, criminal records check, data classification, data governance, data sharing agreements, education, experience, gold level veteranfriendly employer, highspeed internet connection, interconnection sharing agreements, internal audits, investigations, laws, liaison, linkedin, mdhhsjobs, mvaa, privacy, professional experience, regulations, reinstatement, reporting, secure work location, security, security plans, veteranfriendly, work remotely"
"Behavioral Health CRM Analyst (Local Candidates Only) (4+ Years of CRM Experience, Proficiency in Data Analysis & Reporting Required)",Dice,"Lansing, MI",https://www.linkedin.com/jobs/view/behavioral-health-crm-analyst-local-candidates-only-4%2B-years-of-crm-experience-proficiency-in-data-analysis-reporting-required-at-dice-3786286980,2023-12-17,East Lansing,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Quantam Solutions, is seeking the following. Apply via Dice today!
Quantam Solutions provides IT solutions and consulting for the State of Michigan. We offer a competitive hourly wage, health benefits, paid time off, and a 401(k) plan. We're currently seeking a CRM Analyst (Business Analyst 5) for the State of Michigan's Center for Shared Solutions. The selected candidate must be able to start working onsite in Lansing, Michigan. A hybrid work schedule is available.
POSITION DESCRIPTION:
This position functions as a Behavioral Health Customer Relationship Management (BH CRM) analyst to support the development, implementation, monitoring, and continued improvements of complex strategic initiatives. These initiatives include, but are not limited to, the Behavioral Health Customer Relationship Management (BH CRM) business processes, housed in the Behavioral Health Customer Relationship Management (BH CRM) system. This position coordinates and communicates with project teams, other Behavioral Health Customer Relationship Management (BH CRM) staff, business owners, leadership, IT, and contractors to assist with operationalizing Behavioral Health Customer Relationship Management (BH CRM) business processes into the Behavioral Health Customer Relationship Management (BH CRM) system. This position will provide level 1 support to users, assisting with triage of tickets and requests, and communicating status updates to users. Additionally, this role involves creating program guidance, job aids, webinar content, and web-based trainings, as well as coordinating technical assistance learning sessions as directed. Furthermore, this position will also monitor program and account data within the Behavioral Health Customer Relationship Management (BH CRM) system.
DUTIES & RESPONSIBILITIES:
Reviews, analyzes, and evaluates business systems and user needs.
Triage user requests and tickets, assign priority and severity, and communicate effectively to all stakeholders.
Maintain and prioritize CRM inbox communications.
Draft and send communications to users and specific audiences related to business process upgrades, timelines, system changes, or anything relevant to the system.
Formulates systems to parallel overall business strategies.
Prepares solution options, risk identification, and financial analyses such as cost/benefit, ROI, buy/build, etc.
Writes detailed description of user needs, program functions, and steps required to develop or modify computer programs.
Prepare and document Functional and Technical Specifications for reporting and data warehouse work. Assist with business warehouse/intelligence support and enhancements.
Assist with developing and drafting RFPs.
Assist in deployment and management of end-user reporting tools and platforms.
Work with IT and business project teams to understand reporting and data warehousing requirements and propose solutions.
Document and provide knowledge transfer to the rest of the Enterprise Reporting Team for all solutions.
Reviews, analyzes, and evaluates business systems and user needs.
Formulates systems to parallel overall business strategies.
Relies on limited experience and judgment to plan and accomplish goals.
Performs a variety of tasks.
Works under general supervision; typically reports to a project manager.
Attends all BH CRM meetings as appropriate and assigned.
QUALIFICATIONS/REQUIREMENTS:
Experienced with business process reengineering and identifying new applications of technology to business problems to make business more effective.
Familiar with industry standards (including Legacy, Core, and Emerging technologies), business process mapping, and reengineering.
A certain degree of creativity and latitude is required.
Has knowledge of commonly used concepts, practices, and procedures within a particular field.
Familiar with relational database concepts, and client-server concepts.
Ability to organize, evaluate, and present information effectively.
Ability to analyze, synthesize, and evaluate a variety of data for use in program development and analysis.
Other required skills are technologically savvy; strong training and technical assistance skills; excellent oral and written communication skills; and highly organized and detail oriented.
Required skills in the use of all necessary software necessary to execute stated duties and responsibilities (includes the suite of Microsoft Office applications such as Word, Excel, PowerPoint, Outlook).
Ability to meet competing deadlines.
Strong data management skills.
Experience in process improvement, project management, and data management.
Behavioral Health CRM Analyst (Local Candidates Only) (4+ Years of CRM Experience, Proficiency in Data Analysis & Reporting Required)
Show more
Show less","CRM Analyst, Behavioral Health, Microsoft Office Suite (Word Excel PowerPoint Outlook), SQL, Data Analysis, Reporting, Project Management, Process Improvement, Data Management, Business Process Reengineering, Business Process Mapping, Industry Standards, Relational Database Concepts, ClientServer Concepts, Training and Technical Assistance, Communication Skills, Organization, Evaluation, Presentation Skills, Competing Deadlines, Data Management Skills","crm analyst, behavioral health, microsoft office suite word excel powerpoint outlook, sql, data analysis, reporting, project management, process improvement, data management, business process reengineering, business process mapping, industry standards, relational database concepts, clientserver concepts, training and technical assistance, communication skills, organization, evaluation, presentation skills, competing deadlines, data management skills","behavioral health, business process mapping, business process reengineering, clientserver concepts, communication skills, competing deadlines, crm analyst, data management, data management skills, dataanalytics, evaluation, industry standards, microsoft office suite word excel powerpoint outlook, organization, presentation skills, process improvement, project management, relational database concepts, reporting, sql, training and technical assistance"
Project Management Data Analyst (PMA),BS&A Software,"Lansing, MI",https://www.linkedin.com/jobs/view/project-management-data-analyst-pma-at-bs-a-software-3787924484,2023-12-17,East Lansing,United States,Mid senior,Remote,"Project Management Data Analyst
We are seeking a talented and detail oriented Project Management Data Analyst to join our team. You will play a crucial role in collecting, analyzing, converting, and integrating data from various third-party data sources into our software solutions. This role requires a high degree of technical competence and creativity, as well as excellent organizational and communication skills.
A successful Project Management Data Analyst will utilize and build extensive technical expertise and interpersonal skills to ensure seamless data transformation and system integration for our clients. This position requires proficiency in handling technical challenges, excellent ability to prioritize, and a strong commitment to delivering exceptional customer service.
If you are looking for a challenging and rewarding opportunity to utilize your skills, we'd love to hear from you!
Key Responsibilities:
Analyze and verify the integrity of converted data, comparing it with incumbent solution in relation to BS&A Software
Documenting and resolving issues, risks, and actions related to the conversion
Assist in developing and maintaining comprehensive documentation, including data mappings, data dictionaries, and technical specifications
Design and run conversion verification routines from a variety of competitive software products
Develop/test/execute data conversion verification routines within an established conversion framework in a timely and accurate manner
Develop an in-depth understanding of multiple BS&A modules and their database layout
Collaborate with fellow BS&A team members, including ETL Software Development Specialists, Project Managers, Implementation & Training Services Specialists, and customers to ensure successful data conversion
Qualifications:
Bachelor's degree in Computer Science, Information Systems, a related field, and/or relevant work experience
Experience (2-3 years) with SQL programming languages
Experience (2-3 years) with Microsoft SQL Server Management Studio
Knowledge of data architecture, database structure, and data modeling techniques
Strong problem-solving and decision-making skills, particularly with data validation
Ability to efficiently manage multiple tasks, projects, and deadlines simultaneously while maintaining a high level of accuracy and attention to detail
Demonstrated skill in prioritizing competing priorities to ensure successful project execution
Excellent communication skills, both written and verbal, with an ability to convey complex data concepts to non-technical team members
At BS&A, we love our team members and offer a highly competitive compensation and benefit package. Our benefit package includes:
This Role has the ability to be Remote
Competitive pay
Health Insurance – BCBS of Michigan – Employer-paid premium
Health Savings Plan – Employer contributes 75%
Dental Insurance – Employer-paid premium
Vision Insurance – Employer-paid premium
Retirement – 401(k) – Employer-paid
Retirement – 401(k) – Employer matches 50% of team member contribution
Paid Parental Leave
Disability Insurance – Employer-paid premium
Life Insurance – Employer-paid premium
Generous PTO and Holiday Time
Company-sponsored events
BS&A Software uses E-Verify as part of the I-9 process to verify the work eligibility of all new hires.
BS&A Software provides Equal Employment Opportunity to all employees and applicants for employment without regard to race, color, religion, gender identity or expression, sex, sexual orientation, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state, and local laws. BS&A Software complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.
If you need accommodation or special assistance to navigate our website or to complete your application, please send an e-mail with your request to
Powered by JazzHR
ZdfqQ6IBpj
Show more
Show less","SQL, Microsoft SQL Server Management Studio, Data architecture, Database structure, Data modeling, Problemsolving, Decisionmaking, Project management, Data validation, Data conversion, Data integration, Project execution, Communication, ETL, BS&A Software","sql, microsoft sql server management studio, data architecture, database structure, data modeling, problemsolving, decisionmaking, project management, data validation, data conversion, data integration, project execution, communication, etl, bsa software","bsa software, communication, data architecture, data conversion, data integration, data validation, database structure, datamodeling, decisionmaking, etl, microsoft sql server management studio, problemsolving, project execution, project management, sql"
Senior Data Engineer,"Verticalmove, Inc",Greater Boston,https://www.linkedin.com/jobs/view/senior-data-engineer-at-verticalmove-inc-3764334255,2023-12-17,Boston,United States,Mid senior,Onsite,"ABOUT OUR COMPANY:
Verticalmove is among the 2023 Inc. 5000 list of the fastest-growing private companies in America! We build digital transformation, product, and software engineering teams...
Verticalmove is a respected Digital Transformation and Technical Recruitment services company with more than 20 years of proven results, delivering solutions to Fortune 500 companies and start-up organizations by finding top professional talent before their competitors can.
About The Role:
As a part of our team, you will play a pivotal role in evangelizing and constructing Data Products that simplify critical Machine Learning (ML) and Analytics products. Your role will enrich the customer experience, streamline marketing operations, and contribute to the creation of high-quality data marts. Collaborating with data engineering and platform teams, you will lead the design, implementation, and operation of substantial big data pipelines and tools.
As a member of our team, your responsibilities will include:
Designing, building, and implementing data pipelines and products to support ML and Analytical use cases.
Collaborating closely with product managers, engineers, data scientists, and analysts to fulfill property data needs and create outstanding datasets.
Identifying opportunities to enhance and support existing data processes.
Contributing to shared tooling and infrastructure to facilitate self-service solutions and expedite customer onboarding.
Qualifications:
Over 5 years of software development experience using Python, Scala, Java, and demonstrated leadership in designing and implementing scalable, reliable services and workflows/pipelines using tools such as Airflow, Hive, Spark, Kafka, EMR, or equivalents.
A degree in Computer Science or a related technical field or equivalent work experience.
Expertise in establishing and upholding high standards in pipeline monitoring, data validation, testing, etc.
Extensive experience in automating data engineering processes (DataOps).
A genuine passion for data engineering, analytics, and distributed systems.
Strong interpersonal skills and a deep commitment to collaborative work across organizational boundaries.
Comfort in transforming informal customer requirements into well-defined problem definitions, resolving ambiguity, and navigating challenging objectives.
Enthusiasm for mentoring, coaching, onboarding, and leading fellow team members.
We value diverse backgrounds and experiences. If you possess transferable skills or relevant experiences, we encourage you to apply.
Required Education: Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
Show more
Show less","Python, Scala, Java, Data Pipelines, Machine Learning, Analytics, Data Marts, Airflow, Hive, Spark, Kafka, EMR, Software Development, Computer Science, Pipeline Monitoring, Data Validation, Testing, DataOps, Data Engineering, Distributed Systems, Mentoring, Coaching, Onboarding","python, scala, java, data pipelines, machine learning, analytics, data marts, airflow, hive, spark, kafka, emr, software development, computer science, pipeline monitoring, data validation, testing, dataops, data engineering, distributed systems, mentoring, coaching, onboarding","airflow, analytics, coaching, computer science, data engineering, data marts, data validation, dataops, datapipeline, distributed systems, emr, hive, java, kafka, machine learning, mentoring, onboarding, pipeline monitoring, python, scala, software development, spark, testing"
Data Analyst,FreightPlus,"Quincy, MA",https://www.linkedin.com/jobs/view/data-analyst-at-freightplus-3778655459,2023-12-17,Boston,United States,Mid senior,Onsite,"FreightPlus is an industry provider of data-driven transportation management, offering businesses customized and fully tailored managed transportation solutions in a boutique environment where clients get the individual attention they deserve. FreightPlus combines first class customer service with innovative technology and industry best practices to help mid-size and growing companies work efficiently in the $800B domestic transportation market. The Company is ranked #184 in the most recent Inc. 5000 list of fastest growing companies. Visit FreightPlus.io for more information.
Location: Quincy, MA
The Data Analyst is responsible for working with our product and technology teams to find trends in data sets and to develop algorithms to help make raw data more useful to the enterprise. This role requires a significant set of technical skills, including a deep knowledge of database design and multiple programming languages. Communication skills are critical when working with various departments across the organization.
Responsibilities
The Data Analyst is responsible for:
Interfacing with business and sales customers to gather and present data
Enabling effective decision making by retrieving and aggregating data from multiple sources and compiling it into a digestible and actionable format
Analyzing and solving business problems with focus on understanding root causes and driving forward-looking opportunities
Designing new metrics and enhance existing metrics to support the future state of business processes and ensure sustainability
Identifying ways to improve data reliability, efficiency and quality
Planning and conducting research to answer industry and business questions
Communicating complex analysis and insights to stakeholders and business leaders, verbally, in writing and through data reports
Requirements
Bachelor's degree in a quantitative field (e.g., Statistics, Mathematics, Computer Science)
4+ years of professional experience in analytics, business analysis or comparable consumer analytics position
Strong proficiency in data analysis tools and programming languages such as SQL, Python, or R
Demonstrated experience with data warehousing, data modeling concept and building new data warehouse schema
Experience with a relational database platform (e.g.; MySQL, SQL Server, PostgreSQL) and NoSQL databases such as MongoDB, Cassandra, or Redis
Experience defining requirements and using data and metrics to draw business insights
Experience with data visualization tools (e.g., Tableau, Power BI) and proficiency in creating meaningful visualizations
Proven problem-solving skills, attention to detail, and exceptional organizational skills
Ability to deal with ambiguity and competing objectives in a fast-paced environment
Strong operational business understanding, including potential impact of business decisions on various internal/external stakeholders
Benefits
The compensation package includes a competitive salary, performance bonuses, health & dental benefits, and unlimited PTO.
Show more
Show less","Data Analysis, Programming Languages, SQL, Python, R, Data Warehousing, Data Modeling, NoSQL Databases, MongoDB, Cassandra, Redis, Tableau, Power BI","data analysis, programming languages, sql, python, r, data warehousing, data modeling, nosql databases, mongodb, cassandra, redis, tableau, power bi","cassandra, dataanalytics, datamodeling, datawarehouse, mongodb, nosql databases, powerbi, programming languages, python, r, redis, sql, tableau"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Willowbrook, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742678357,2023-12-17,Oswego,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, MLOps, Apache Beam, Spark, Scala, PySpark, AWS, Azure, GCP, Kafka, CI/CD, DevOps, Infrastructure as Code (IaC), ETL, ELT, Machine Learning, AI/ML, VR, NFT","python, mlops, apache beam, spark, scala, pyspark, aws, azure, gcp, kafka, cicd, devops, infrastructure as code iac, etl, elt, machine learning, aiml, vr, nft","aiml, apache beam, aws, azure, cicd, devops, elt, etl, gcp, infrastructure as code iac, kafka, machine learning, mlops, nft, python, scala, spark, vr"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Burr Ridge, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742673891,2023-12-17,Oswego,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Machine Learning, Machine Learning Operations (MLOps), AWS, Azure, GCP, Spark, Scala, PySpark, DevOps, CI/CD, IaC (Infrastructure as Code), Apache Beam, Kafka, ETL tools","python, machine learning, machine learning operations mlops, aws, azure, gcp, spark, scala, pyspark, devops, cicd, iac infrastructure as code, apache beam, kafka, etl tools","apache beam, aws, azure, cicd, devops, etl tools, gcp, iac infrastructure as code, kafka, machine learning, machine learning operations mlops, python, scala, spark"
